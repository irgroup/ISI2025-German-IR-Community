<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,137.71,112.05,336.58,15.12;1,474.29,108.59,5.85,11.96">Overview of the TREC 2020 Fair Ranking Track *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,149.03,144.53,68.02,10.48"><forename type="first">Asia</forename><forename type="middle">J</forename><surname>Biega</surname></persName>
							<email>asia.biega@acm.org</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Montréal</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,129.84,200.32,106.38,10.48"><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Ekstrand</surname></persName>
							<email>michaelekstrand@boisestate.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Boise State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,130.33,256.11,105.41,10.48"><forename type="first">Sebastian</forename><surname>Kohlmeier</surname></persName>
							<email>sebastiank@allenai.org</email>
							<affiliation key="aff2">
								<orgName type="institution">Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,382.52,144.53,74.79,10.48"><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
							<email>diazf@acm.org</email>
							<affiliation key="aff3">
								<orgName type="institution">Montreal Institute for Learning Algorithms</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,379.35,200.32,81.13,10.48"><forename type="first">Sergey</forename><surname>Feldman</surname></persName>
							<email>sergey@allenai.org</email>
							<affiliation key="aff4">
								<orgName type="institution">Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,137.71,112.05,336.58,15.12;1,474.29,108.59,5.85,11.96">Overview of the TREC 2020 Fair Ranking Track *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7142ACD4882A81EEB1AA72FF40A57B86</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For 2020, we again adopted an academic search task, where we have a corpus of academic article abstracts and queries submitted to a production academic search engine. The central goal of the Fair Ranking track is to provide fair exposure to different groups of authors (a group fairness framing). We recognize that there may be multiple group definitions (e.g. based on demographics, stature, topic) and hoped for the systems to be robust to these. We expected participants to develop systems that optimize for fairness and relevance for arbitrary group definitions, and did not reveal the exact group definitions until after the evaluation runs were submitted.</p><p>The track contains two tasks, reranking and retrieval, with a shared evaluation.</p><p>Rerank runs sorted a query-dependent list of documents to simultaneously provide fairness and relevance.</p><p>Retrieval runs returned 100-item rankings from the corpus in response to a query string.</p><p>The track organizers provided a sequence of queries, each accompanied by a varying-size set of documents. Both tasks used the same queries; participants were asked not to use the test queries' rerank sets as a component of their retrieval model training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Protocol</head><p>For our fair ranking evaluation, we provided participants with a sequence Q of queries accompanied by unordered sets of documents to rank. The document sets are of varying size. For each request (query q and set of documents D q ), participants provided a ranked list of the documents from D q . For the retrieval task, q is the set of all documents in our corpus and participants were asked to return a fixed set of documents. The final system output is a sequence of rankings for each query. Algorithm 1 presents a pseudocode of the evaluation protocol.</p><p>The rankings produced in response to queries in the sequence were to balance two goals: (1) be relevant to the consumers and (2) be fair to the producers.</p><p>Algorithm 1 Evaluation protocol ∀q, D q ∈ Q, Π q ← {} for q, D q ∈ Q do π ← System(q, D q ) Π q ← Π q ∪ {π} end for return {Π q } i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " h A 2 B M j c r 0 b t V k 5 q Y 1 / H O r 4 x O 9 4 Q = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e i F 4 8 t 2 F p o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o r e N U M W y x W M S q E 1 C N g k t s G W 4 E d h K F N A o E P g T j 2 5 n / 8 I R K 8 1 j e m 0 m C f k S H k o e c U W O l J u + X K 2 7 V n Y O s E i 8 n F c j R 6 J e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S r l W 9 i 2 q t e V m p 3 </p><formula xml:id="formula_0" coords="2,234.24,189.14,134.33,97.63">+ R x F O E E T u E c P L i C O t x B A 1 r A A O E Z X u H N e X R e n H f n Y 9 F a c P K Z Y / g D 5 / M H 0 M u M 8 Q = = &lt; / l a t e x i t &gt; p(s|⇡ i ) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 8 F U k Z E T W d P j z G v c C G I w 9 4 Q Q A 4 9 I = " &gt; A A A B 8 X i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R a h X s p u K + i x 6 M V j B f u B 7 V K y a b Y N z W Z D k h X K 2 n / h x Y M i X v 0 3 3 v w 3 p u 0 e t P X B w O O 9 G W b m B Z I z b V z 3 2 8 m t r W 9 s b u W 3 C z u 7 e / s H x c O j l o 4 T R W i T x D x W n Q B r y p m g T c M M p x 2 p K I 4 C T t v B + G b m t x + p 0 i w W 9 2 Y i q R / h o W A h I 9 h Y 6 U G W 9 V N P s j 4 7 7 x d L b s W d A 6 0 S L y M l y N D o F 7 9 6 g 5 g k E R W G c K x 1 1 3 O l 8 V O s D C O c T g u 9 R F O J y R g P a d d S g S O q / X R + 8 R S d W W W A w l j Z E g b N 1 d 8 T K Y 6 0 n k S B 7 Y y w G e l l b y b + 5 3 U T E 1 7 5 K R M y M V S Q x a I w 4 c j E a P Y + G j B F i e E T S z B R z N 6 K y A g r T I w N q W B D 8 J Z f X i W t a s W r V a p 3 F 6 X 6 d R Z H H k 7 g F M r g w S X U 4 R Y a 0 A Q C A p 7 h F d 4 c 7 b w 4 7 8 7 H o j X n Z D P H 8 A f O 5 w 8 h u 5 C P &lt; / l a t e x i t &gt; s &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 8 r 6 d W O v d y f g m P O H E a n D S Q r m g c i s = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e i F 4 8 t 2 F p o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o r e N U M W y x W M S q E 1 C N g k t s G W 4 E d h K F N A o E P g T j 2 5 n / 8 I R K 8 1 j e m 0 m C f k S H k o e c U W O l p u 6 X K 2 7 V n Y O s E i 8 n F c j R 6 J e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S r l W 9 i 2 q t e V m p 3 + R x F O E E T u E c P L i C O t x B A 1 r A A O E Z X u H N e X R e n H f n Y 9 F a c P K Z Y / g D 5 / M H 3 / O M + w = = &lt; / l a t e x i t &gt; (1 p(s|⇡ i )) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n D n Q 6 d N v D 4 f d f s l y M y 1 P d l e P j K E = " &gt; A A A B / X i c b V D L S s N A F J 3 4 r P U V H z s 3 g 0 V o F 5 a k C r o s u n F Z w T 6 g C W E y n b R D Z y Z h Z i L U W P w V N y 4 U c e t / u P N v n L Z Z a O u B C 4 d z 7 u X e e 8 K E U a U d 5 9 t a W l 5 Z X V s v b B Q 3 t 7 Z 3 d u 2 9 / Z a K U 4 l J E 8 c s l p 0 Q K c K o I E 1 N N S O d R B L E Q 0 b a 4 f B 6 4 r f v i V Q 0 F n d 6 l B C f o 7 6 g E c V I G y m w D 7 0 + 4 h y V 3 d O k r B 6 9 h A a 0 U g n s k l N 1 p o C L x M 1 J C e R o B P a X 1 4 t x y o n Q m C G l u q 6 T a D 9 D U l P M y L j o p Y o k C A 9 R n 3 Q N F Y g T 5 W f T 6 8 f w x C g 9 G M X S l N B w q v 6 e y B B X a s R D 0 8 m R H q h 5 b y L + 5 3 V T H V 3 6 G R V J q o n A s 0 V R y q C O 4 S Q K 2 K O S Y M 1 G h i A s q b k V 4 g G S C G s T W N G E 4 M 6 / v E h a t a p 7 V q 3 d n p f q V 3 k c B X A E j k E Z u O A C 1 M E N a I A m w O A B P I N X 8 G Y 9 W S / W u / U x a 1 2 y 8 p k D 8 A f W 5 w / e h 5 Q y &lt; / l a t e x i t &gt; i + 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " V 6 J z O i R y k N n s E U N a K m 3 l z 7 + C r K 0 = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S I I Q k m q o M e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I J / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O y u r a + s b m 4 W t 4 v b O 7 t 5 + 6 e C w q e N U M W y w W M S q H V C N g k t s G G 4 E t h O F N A o E t o L R 7 d R v P a H S P J a P Z p y g H 9 G B 5 C F n 1 F j p g Z 9 7 v V L Z r b g z k G X i 5 a Q M O e q 9 0 l e 3 H 7 M 0 Q m m Y o F p 3 P D c x f k a V 4 U z g p N h N N S a U j e g A O 5 Z K G q H 2 s 9 m p E 3 J q l T 4 J Y 2 V L G j J T f 0 9 k N N J 6 H A W 2 M 6 J m q B e 9 q f i f 1 0 l N e O 1 n X C a p Q c n m i 8 J U E B O T 6 d + k z x U y I 8 a W U K a 4 v Z W w I V W U G Z t O 0 Y b g L b 6 8 T J r V i n d R q d 5 f l m s 3 e R w F O I Y T O A M P r q A G d 1 C H B j A Y w D O 8 w p s j n B f n 3 f m Y t 6 4 4 + c w R / I H z + Q O n E Y 1 h &lt; / l a t e x i t &gt; (1 p(s|⇡ i+1 )) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v p L K K e g z K x Y U G Z c n D M d L R H s / V C c = " &gt; A A A C A X i c b V D L S g M x F M 3 U V 6 2 v U T e C m 8 E i t I h l p g q 6 L L p x W c E + o D M M m T T T h i a Z k G S E M t a N v + L G h S J u / Q t 3 / o 3 p Y 6 G t B y 4 c z r m X e + + J B C V K u + 6 3 l V t a X l l d y 6 8 X N j a 3 t n f s 3 b 2 m S l K J c A M l N J H t C C p M C c c N T T T F b S E x Z B H F r W h w P f Z b 9 1 g q k v A 7 P R Q 4 Y L D H S U w Q 1 E Y K 7 Q O / B x m D J e 9 U l N S D L 0 i Y k R N v V C 6 H d t G t u B M 4 i 8 S b k S K Y o R 7 a X 3 4 3 Q S n D X C M K l e p 4 r t B B B q U m i O J R w U 8 V F h A N Y A 9 3 D O W Q Y R V k k w 9 G z r F R u k 6 c S F N c O x P 1 9 0 Q G m V J D F p l O B n V f z X t j 8 T + v k + r 4 M s g I F 6 n G H E 0 X x S l 1 d O K M 4 3 C 6 R G K k 6 d A Q i C Q x t z q o D y V E 2 o R W M C F 4 8 y 8 v k m a 1 4 p 1 V q r f n x d r V L I 4 8 O A R H o A Q 8 c A F q 4 A b U Q Q M g 8 A i e w S t 4 s 5 6 s F + v d + p i 2 5 q z Z z D 7 4 A + v z B 5 U K l a 4 = &lt; / l a t e x i t &gt; p(s|⇡ i+1 ) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 a Z V l c 3 y U w f g Z s 3 B m f S Y U 5 m N 3 i Y = " &gt; A A A B 9 X i c b V B N S w M x E J 3 1 s 9 a v q k c v w S J U h L J b B T 0 W v X i s Y D + g X U s 2 z b a h 2 W x I s k p Z + z + 8 e F D E q / / F m / / G t N 2 D t j 4 Y e L w 3 w 8 y 8 Q H K m j e t + O 0 v L K 6 t r 6 7 m N / O b W 9 s 5 u Y W + / o e N E E V o n M Y 9 V K 8 C a c i Z o 3 T D D a U s q i q O A 0 2 Y w v J 7 4 z Q e q N I v F n R l J 6 k e 4 L 1 j I C D Z W u p c l / d S R r J u y U 2 9 8 0 i 0 U 3 b I 7 B V o k X k a K k K H W L X x 1 e j F J I i o M 4 V j r t u d K 4 6 d Y G U Y 4 H e c 7 i a Y S k y H u 0 7 a l A k d U + + n 0 6 j E 6 t k o P h b G y J Q y a q r 8 n U h x p P Y o C 2 x l h M 9 D z 3 k T 8 z 2 s n J r z 0 U y Z k Y q g g s 0 V h w p G J 0 S Q C 1 G O K E s N H l m C i m L 0 V k Q F W m B g b V N 6 G 4 M 2 / v E g a l b J 3 V q 7 c n h e r V 1 k c O T i E I y i B B x d Q h R u o Q R 0 I K H i G V 3 h z H p 0 X 5 9 3 5 m L U u O d n M A f y B 8 / k D y n u S C w = = &lt; / l a t e x i t &gt;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Evaluation</head><p>Unlike previous TREC tracks, systems were to return multiple rankings for each query, as they might in response to different impressions of the same query text. At evaluation time, we measured measure expected exposure of groups over rankings produced for each given query <ref type="bibr" coords="2,352.57,386.83,9.96,9.96" target="#b0">[1]</ref>.</p><p>Given a sequence of queries Q and associated system rankings, we evaluated systems according to fair exposure of authors and relevance of documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Measuring Author Exposure for a Single Query</head><p>In order to measure exposure, we adopt the browsing model underlying the Expected Reciprocal Rank metric [? ]. Given a static ranking π in response to a query impression, the exposure of author a is,</p><formula xml:id="formula_1" coords="2,147.24,489.90,190.57,33.92">e π a = n i=1   γ i-1 i-1 j=1 (1 -p(s|π j ))   I(π i ∈ D a )</formula><p>n number of documents in ranking π D a documents including a as an author π i document at position i γ continuation probability (fixed to 0 for the final position in the ranking) p(s|d) probability of stopping given user examined d</p><p>We present a graphical depiction of this model in Figure <ref type="figure" coords="2,321.95,627.28,3.87,9.96" target="#fig_0">1</ref>.</p><p>We used a discounting factor γ = 0.5, and assumed p(s|d) = f (r d ), where r d is the relevance of the document d and f is a monotonic transform of that relevance into a probability of being satisfied.</p><p>In order to compute the expected exposure for a, we consider the set of all rankings presented by the system for that query Π q ,</p><formula xml:id="formula_2" coords="3,278.72,96.89,261.28,22.99">e a = π∈Πq e π a<label>(1)</label></formula><p>The target expected exposure for a query is derived from Equation 1 assuming a policy that randomizes amongst all permutations whose relevance monotonically degrades with rank <ref type="bibr" coords="3,410.23,143.46,9.96,9.96" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Measuring Group Exposure for a Single Query</head><p>Assume that each author is assigned to exactly one of |G| groups. Let A g be the set of all authors in group g. The group expected exposure is defined as,</p><formula xml:id="formula_3" coords="3,278.72,224.80,261.28,21.37">E g = a∈Ag e a<label>(2)</label></formula><p>We define the target group expected exposure E * g as Equation 2 using the individual target expected exposure (Section 2.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Expected Exposure Metric</head><p>We evaluated systems using the per-query difference in system group expected exposure and target group expected exposure,</p><formula xml:id="formula_4" coords="3,243.70,348.17,296.31,37.67">∆ G (E) =   g∈G (E g -E * g ) 2   1 2<label>(3)</label></formula><p>We averaged per-query metrics to compute the summary metric for the run.</p><p>3 Data</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Input</head><p>Three main inputs were made available to participants: the corpus of articles to search, the example group definition file to help them develop and test their solutions, and the queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Corpus</head><p>The full corpus for this track was the Semantic Scholar (S2) Open Corpus from the Allen Institute for Artificial Intelligence. It can be downloaded from http://api.semanticscholar.org/corpus/, and consists of 186 1GB data files. Each file is compressed JSON, where each line is a JSON object describing one paper.</p><p>The following data are available for most papers:</p><formula xml:id="formula_5" coords="4,86.95,146.70,63.96,64.98">• S2 Paper ID • DOI • Title • Abstract</formula><p>• Authors (resolved to author IDs)</p><p>• Inbound and outbound citations (resolved to S2 paper IDs)</p><p>We provide tools for subsetting the corpus at https://github.com/fair-trec/fair-trec-tools. These tools were used to create the subset we released to participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Example Group Definition Data</head><p>For training, we provided the file fair-TREC-sample-author-groups.csv containing group ids for authors in the S2 corpus. This group definition was not our final group definition, but was intended to help groups get started on the task.</p><p>This CSV file contains two columns:</p><p>1. The author column has the S2 ID of the author.</p><p>2. The gid column has the author's group identifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Queries</head><p>Query data. The query data was obtained from searches that occurred on the Semantic Scholar<ref type="foot" coords="4,500.53,426.29,3.97,6.97" target="#foot_0">1</ref> website between Feb 14, 2020 and April 27, 2020. The data consisted of session id, query text, result papers from the first 3 pages (30 results), and result clicks. Sessions with more than 25 unique queries were excluded, after which only sessions with at least 1 result paper click and no more than 250 result paper clicks were included.</p><p>Query-document relevance. We estimated the relevance of different documents to queries based on the click data described above. We computed the query-document relevance as a weighted average of the number of clicks on a given document over all impressions of a given query-document pairs present in the data. For weighting, we used ranking position propensity scores estimated by the Semantic Scholar from their system data. Relevance scores were converted to binary based on a manually selected threshold.</p><p>Query filtering. Because of the exhaustive annotation process that required annotating group memberships of all document authors, we then sampled a smaller number of queries to construct evaluation sequences. We released 200 training and 200 evaluation queries. For both the training and evaluation data, these queries were selected first by random sampling, and then by a number of filtering steps. More specifically,</p><p>• To help remove known-item queries, we included only queries with at least two relevant documents and excluded queries with more than 4 words.</p><p>• We further manually cleaned the sample to remove any known-item queries, queries containing people's names, and queries with offensive and sensitive keywords.</p><p>Query sequences. Since the evaluation this year focused on individual queries, each query sequence consisted of repetitions of a single query. We had 200 sequences, each consisting of a 100 repetitions of a query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Output</head><p>For each query sequence, participants submitted a JSON file where each line is a JSON object (a dictionary) containing their ranking results:</p><p>• &lt;sequence id&gt;.&lt;query number in sequence&gt; ('q num')</p><p>• &lt;query id&gt; (to look up in query file) ('qid')</p><p>• An ordered list of document IDs (of the documents to be re-ranked for the query) ('ranking')</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Annotations</head><p>NIST assessors annotated returned papers with the country in which each author was operating (based on their affiliation data in the paper manuscript), along with institution type (academic, industry, nonprofit, government, etc.). Not all papers were able to be annotated. These are the known reasons a paper may not have annotations:</p><p>• It has a large author list (&gt; 10). We excluded such long papers because there were not very many of them, and large-team papers require special treatment in how we consider their author lists, particularly when authors may be from different groups.</p><p>• Some papers did not have an accessible source with sufficient affiliation information to provide annotations (e.g. no available PDF file, and a paper information page that either did not contain affiliation details or was not accessible from the annotation interface).</p><p>• Some papers may not provide sufficient information to determine an author's affiliation location.</p><p>All documents in the candidate sets for the rerank tests were annotated, along with many of the documents in the corpus for the retrieval task. Table <ref type="table" coords="5,114.55,625.49,4.98,9.96" target="#tab_0">1</ref> shows a summary of the collected annotation data, after merging and integrating data sources. For these statistics, to aggregate each paper's authors into a single economic designation for the paper, we considered a paper to be from an advanced or developing economy if all authors' locations had the same economic designation; otherwise, we list it as a 'mixed' economy paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overall Eval Candidates</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Documents</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Group Definitions</head><p>Group definition accompanying the training data. To help participants get started, we provided a file containing group membership definitions for authors in the S2 corpus. This definition was based on author h-indices. This definition was not used in the final evaluation, but was meant as a starting point for system development. For each author, the data consisted of:</p><p>• the author's S2 ID,</p><p>• the author's group identifier.</p><p>Authors were split into 2 groups, based on the value of their h-index.</p><p>Group definitions for evaluation. Our primary evaluation was based on the NIST assessors' country annotations. We combined these annotations with economic development levels from the International Monetary Fund. With this definition, the fairness target is to ensure fair exposure for papers written in countries with more-and less-developed economies.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Submitted runs</head><p>This year, 6 different teams submitted a total of 28 runs, (23 runs for the reranking task and 5 runs for the retrieval task). Some of the approaches included:</p><p>• weighted reranking methods that optimized the KL-divergence of the output group distributions to target group distributions estimated using external Google Scholar data (team InfoSeeking),</p><p>• ranking fusion methods where the documents were ranked by the BM25 scores of queries matched to different document parts (title, abstract) and the individual ranking weights shift throughout a query sequence (team MacEwan),</p><p>• including authors from all groups at the top of the ranking with the groups determined by a clustering algorithm on the authorship graph; an approach that randomizes the output of a learning-to-rank algorithm based on the predicted relevance and optionally includes the publication year as a feature (team MTG),</p><p>• randomization of the outputs of rankings based on the textual content of documents and externally trained word embeddings, with an optional parametrized readjustments to match a target group exposure distribution (team NLE),</p><p>• a static method that does keep track of exposure in between rankings based on a learning-to-rank algorithm with a custom objective balancing fairness and relevance (team UMD),</p><p>• a two-stage approach where the first stage is based on standard retrieval methods, and the second stage uses reranking based on membership in authorship communities detected using graph embedding methods (team UoGTr).</p><p>Notably, novel ideas as compared to the last year's runs included estimating group membership using automatically detected authorship communities, randomization of the outputs, including the publication year as a feature, and incorporation of external resources (Google Scholar data and word embeddings trained on the Bing corpus).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation</head><p>We present the results for reranking and retrieval in Tables <ref type="table" coords="8,346.87,481.78,4.98,9.96" target="#tab_1">2</ref> and<ref type="table" coords="8,377.13,481.78,3.87,9.96" target="#tab_2">3</ref>, sorted by ∆ G . Although the run descriptions were not sufficient to draw many general conclusions, the top runs all used external resources (e.g. public embeddings) and multiple permutations per query (e.g. amortization or randomization).</p><p>We can decompose ∆ G into relevance and disparity components <ref type="bibr" coords="8,368.77,517.64,9.96,9.96" target="#b0">[1]</ref>,</p><formula xml:id="formula_6" coords="8,254.98,539.68,285.02,51.70">disparity = g∈G E 2 g (4) relevance = g∈G E g × E * g (5)</formula><p>This allows us to plot each run on disparity-relevance axes which often reflects a trade-off between disparity and relevance. We present results in Figure <ref type="figure" coords="8,271.74,614.35,3.87,9.96" target="#fig_1">2</ref>. In general, we would like runs to lie close to the top left corner. Although the top performing runs from NLE had relatively high relevance, the strong ∆ G was more attributable to exhibiting less disparity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,247.11,307.62,117.78,9.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Attention model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,72.00,642.03,468.00,9.96;7,72.00,653.98,118.31,9.96"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Disparity and relevance results for the reranking task. Lower disparity values are better. Higher relevance values are better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,186.79,481.81,238.42,128.08"><head>Table 1 :</head><label>1</label><figDesc>Annotation outcome summary.</figDesc><table coords="5,186.79,481.81,238.42,103.65"><row><cell></cell><cell>-</cell><cell>4,693</cell></row><row><cell>Annotated Documents</cell><cell>4,381</cell><cell>2,114</cell></row><row><cell>Have Country Data</cell><cell>4,160</cell><cell>2,008</cell></row><row><cell>Advanced Econ Papers</cell><cell>3,374</cell><cell>1,609</cell></row><row><cell>Developing Econ Papers</cell><cell>543</cell><cell>272</cell></row><row><cell>Mixed Econ Papers</cell><cell>243</cell><cell>127</cell></row><row><cell>Advanced Econ Authors</cell><cell>10,679</cell><cell>5,250</cell></row><row><cell>Developing Econ Authors</cell><cell>2,317</cell><cell>1,187</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,72.00,72.90,468.00,318.64"><head>Table 2 :</head><label>2</label><figDesc>Reranking results. Runs ordered in increasing expected exposure (Equation3). Smaller values are better.</figDesc><table coords="6,248.81,72.90,114.39,285.33"><row><cell>run</cell><cell>∆ G</cell></row><row><cell>NLE META 9 1</cell><cell>0.428</cell></row><row><cell>NLE META 99 1</cell><cell>0.429</cell></row><row><cell cols="2">NLE META PKL 0.433</cell></row><row><cell>NLE TEXT 9 1</cell><cell>0.438</cell></row><row><cell>NLE TEXT 99 1</cell><cell>0.442</cell></row><row><cell>UoGTrBComFu</cell><cell>0.475</cell></row><row><cell>LM-rel-groups</cell><cell>0.580</cell></row><row><cell>LM-relevance</cell><cell>0.601</cell></row><row><cell>MacEwan-base</cell><cell>0.722</cell></row><row><cell>UoGTrComRel</cell><cell>0.798</cell></row><row><cell>LM-relev-year</cell><cell>0.811</cell></row><row><cell>UoGTrBComRel</cell><cell>0.832</cell></row><row><cell>MacEwan-norm</cell><cell>0.850</cell></row><row><cell>UoGTrBComPro</cell><cell>0.851</cell></row><row><cell>UW bm25</cell><cell>0.875</cell></row><row><cell>UoGTrBRel</cell><cell>0.886</cell></row><row><cell cols="2">UW Kr r60g20c20 0.895</cell></row><row><cell>umd relfair ltr</cell><cell>0.907</cell></row><row><cell cols="2">UW Kr r25g25c50 0.916</cell></row><row><cell>UW Kr r0g0c100</cell><cell>0.948</cell></row><row><cell>UW Kr r0g100c0</cell><cell>0.999</cell></row><row><cell>LM-rel-year-100</cell><cell>1.046</cell></row><row><cell>Deltr-gammas</cell><cell>1.067</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,72.00,586.42,468.00,33.87"><head>Table 3 :</head><label>3</label><figDesc>The evaluation itself uses individual author-level annotations; the exposure a mixed-economy paper receives counts towards both developing and advanced economy exposure. Under this definition, authors are split into two groups. Retrieval results. Runs ordered in increasing expected exposure (Equation3). Smaller values are better.</figDesc><table coords="7,248.82,97.97,114.35,70.14"><row><cell>run</cell><cell>∆ G</cell></row><row><cell>UW t bm25</cell><cell>0.748</cell></row><row><cell cols="2">UW Kt r80g10c10 0.769</cell></row><row><cell cols="2">UW Kt r60g20c20 0.770</cell></row><row><cell cols="2">UW Kt r25g25c50 0.821</cell></row><row><cell>UW Kt r0g0c100</cell><cell>1.056</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,87.24,685.24,131.26,6.64"><p>https://www.semanticscholar.org</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Paper and Author Data</head><p>The paper and author metadata CSV files provide summary information for papers and their authors in the rerank set. There are three files: paper metadata.csv contains basic paper information: ID, title, year, venue, and the number of citations.</p><p>author metadata.csv contains author information: ID, name, citation count, paper count, and H-index. authors for papers.csv contains the author list for each paper: paper ID, author ID, and position. These files do not contain abstracts. Creating a usable index requires the corpus in the next section.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="9,87.50,96.31,452.50,9.96;9,87.50,108.27,452.51,9.96;9,87.50,120.22,452.50,9.96;9,87.50,132.18,452.50,9.96;9,87.50,145.52,81.22,8.30" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,404.32,96.31,135.68,9.96;9,87.50,108.27,102.15,9.96">Evaluating stochastic rankings with expected exposure</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Ekstrand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Biega</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<idno type="DOI">10.1145/3340531.3411962</idno>
		<ptr target="https://doi.org/10.1145/3340531.3411962" />
	</analytic>
	<monogr>
		<title level="m" coord="9,213.97,108.93,326.04,8.74;9,87.50,120.22,151.20,9.96">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management, CIKM &apos;20</title>
		<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management, CIKM &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="275" to="284" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
