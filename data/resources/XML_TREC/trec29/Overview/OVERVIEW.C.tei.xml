<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,77.21,84.23,458.29,15.44">CAsT 2020: The Conversational Assistance Track Overview</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,180.11,109.24,68.19,11.96"><forename type="first">Jeffrey</forename><surname>Dalton</surname></persName>
							<email>jeff.dalton@glasgow.ac.uk</email>
						</author>
						<author>
							<persName coords="1,258.49,109.24,76.68,11.96"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
							<email>chenyan.xiong@microsoft.com</email>
						</author>
						<author>
							<persName coords="1,366.35,109.24,61.77,11.96"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
							<email>callan@cs.cmu.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Glasgow 1</orgName>
								<address>
									<addrLine>Microsoft Research 2</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,77.21,84.23,458.29,15.44">CAsT 2020: The Conversational Assistance Track Overview</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6A2C17687DCAE752D6997E12E294A754</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>CAsT 2020 is the second year of the Conversational Assistance Track and builds on the lessons from the first year. Teams tried a wide range of techniques to address conversational search challenges. Some methods used proven techniques such as query difficulty prediction and query expansion. Given the text understanding challenges in the task, teams also used traditional NLP models that incorporate coreference resolution. One important development was the application of generative query models and ranking models using pre-trained neural language models. The results showed that both traditional and neural techniques provided complementary effectiveness.</p><p>Based on participant feedback, CAsT 2020 task is similar to 2019. The task is identify relevant passages for conversational queries that evolve through a trajectory of a discussion on a topic. For consistency, the corpus is unchanged with it including both MS MARCO passage and Wikipedia (Complex Answer Retrieval) passages. This stability enables the community to explore the challenges and best practices in a consistent setup and to accumulate high quality annotation labels on more topics for long-term use.</p><p>The biggest change for CAsT 2020 is that utterances refer to previous responses given by a system (e.g., see turn 8 in Table <ref type="table" coords="1,275.78,388.26,2.88,8.97" target="#tab_0">1</ref>). In constrast, in 2019 turns could only refer to information mentioned in previous user utterances. This make the setup more realistic by eliminating the need to restate information presented in a previous result. It also expands the contextual information that systems can, and sometimes must, use to understand a turn. To allow for repeatability for new systems the dependence on previous results is done in a controlled manner. A single canonical result is selected as the response for each turn, as detailed in Section 2. Section 5 describes how participating systems handle this additional dependency. Another minor change to make the conversations realistic compared with 2019, the topics in 2020 no longer include a title or description -the information need unfolds only through the turns.</p><p>To add additional realism the topics in 2020 are based on real user needs mined from information seeking sessions in Bing sessions <ref type="bibr" coords="1,75.63,552.64,9.52,8.97" target="#b5">[6]</ref>. Sessions are manually reviewed and filtered to ensure they have meaningful trajectories and are then manually rewritten by an organizer to allow them to be used for reference. The organizer manually construct 2020 topics from these derived sessions. The topics reflect the organizers' vision of user behavior for conversational search systems of the future, while also being grounded in real information needs and current search behavior. We provide details on topic construction in Section 2.</p><p>Year two had strong participation from more than a dozen teams worldwide. The results show that the second year of CAsT is more Oh, IP addresses are considered PII? What is the full range of personal data? 5</p><p>How do big companies adapt to GDPR? 6 OK. Tell me about the privacy issues in social networks. <ref type="bibr" coords="1,323.78,308.71,4.17,8.97" target="#b6">7</ref> What do they get in return for their privacy? 8</p><p>What are the symptoms of that addiction? challenging than the first year. While similar in structure, the 2020 topics have greater complexity. We find that turns requiring previous result context to be particularly challenging.</p><p>The long-term vision of CAsT remains unchanged: to support natural conversations between a person and a search engine to satisfy information needs and support complex information tasks. The first two years of CAsT focused on retrieving relevant short content from passages. Future iterations will evolve this setup and continue to challenge and advance research in CIS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">TASK, DATA, AND RESOURCES</head><p>The core of the CAsT 2020 task is unchanged from CAsT 2019. The goal of the task is to satisfy a user's complex information need expressed through multi-turn conversational queries/utterances (ùë¢) for each turn ùëá = {ùë¢ 1 , ...ùë¢ ùëñ ...ùë¢ ùëõ }, by retrieving and ranking passages from MS MARCO <ref type="bibr" coords="1,406.39,521.39,10.49,8.97" target="#b0">[1]</ref> and Wikipedia -the Complex Answer Retrieval (CAR) corpora <ref type="bibr" coords="1,407.95,532.35,9.37,8.97" target="#b3">[4]</ref>. The CAsT 2019 overview and dataset paper provide detail about the this setup <ref type="bibr" coords="1,468.26,543.31,9.33,8.97" target="#b1">[2,</ref><ref type="bibr" coords="1,479.83,543.31,6.22,8.97" target="#b2">3]</ref>.</p><p>CAsT 2020 has twenty five information needs (topics) with an average length of 8.6 utterances, for a total of 216 turns. In comparison, the CAsT 2020 topics are slightly shorter than the 2019, which averaged 9.5 turns. An example topic is shown in Table <ref type="table" coords="1,546.75,587.14,3.07,8.97" target="#tab_0">1</ref>.</p><p>The rest of this section focuses on the two major changes for the second year: the more realistic information needs derived from commercial search logs, and the possible dependence of a query on a previous system response.</p><p>Information Needs. A majority of the topics are based on multiturn information-seeking sessions from a commercial search engine. The reference sessions are constructed via the following steps.</p><p>(1) Filter the raw web search sessions to conversational-alike search sessions, using the procedure to obtain about 20,000 "QA-Gen" sessions as described in Rosset et al. <ref type="bibr" coords="1,514.33,699.95,9.39,8.97" target="#b5">[6]</ref>.</p><p>(2) One organizer manually examines the QA-Gen sessions and selects approximately two thousand diverse topics and nontrivial topics with non-personal information for privacy reasons.</p><p>(3) The sessions provide inspiration for the organizer to manually write several hundred derived conversational-like adhoc search sessions, with realistic and challenging information needs.</p><p>The result of this step is synthetic web sessions inspired by one or more real complex web sessions.</p><p>The organizers use the manually written web sessions to build natural language conversational topics. The same guidelines from year one are used to ensure that topics are complex (requiring multiple rounds of elaboration), diverse (across different information categories), open-domain (not requiring expert domain knowledge to access), and answerable (sufficient coverage in the corpus).</p><p>Result Dependence. A major difference in year two is the added possible dependence on the system responses of previous turns. When interacting with a conversational assistant, a user often not only refers to their past queries, but also the assistant's responses earlier in the conversation.</p><p>To simulate this, the organizers built a baseline search system (discussed below) that uses standard BM25 retrieval followed by a BERT-based reranker. The organizers used the baseline system when curating queries to check the number and nature of answer passages. One challenge is that the baseline did not include an automatic query rewriter, particularly one that used previous result context and so organizers used their best judgment to construct manual queries.</p><p>When curating the next turn in the conversational sequence (ùë¢ ùëó ), the organizers examine the results of an earlier conversational turn (ùë¢ ùëñ , ùëñ &lt; ùëó), often manually rewriting the earlier query ùë¢ ùëñ until a reasonable response is provided by the system. Then the next query (ùë¢ ùëó ) may be written with contextual dependence on queries and/or responses from previous turns. There is no explicit guidelines on the number of queries that should depend on previous questions or answers, however the majority of turns have some dependence on previous utterances or responses. The resulting dataset has approximately a quarter of the turns depend on a previous system response.</p><p>Canonical Response for Reusability. The interactive nature of the response dependence makes it necessary to include a fixed system's response as part of benchmark. Otherwise it would be nearly impossible to resolve this dependence because of differences in system responses.</p><p>For the system response the organizers provide a single canonical passage response, the response provided by a hypothetical agent. All turns have a response. In most cases the canonical response is sampled from the top three results of the baseline ranking. Note that the organizers provide full rankings for the baseline systems.</p><p>To identify a passage that continues the conversation in a natural direction the organizers sometimes identified passages outside the baseline results; either deeper or from alternative query formulations. Participants are not informed about the source of the canonical response or whether the canonical response is referenced in later turns.</p><p>There are two versions of canonical responses that correspond to two categories of submission:</p><p>The automatic canonical response is sampled from the top results by the baseline system for queries rewritten by an automatic conversation query rewriter built for CAsT 2019 <ref type="bibr" coords="2,496.93,130.76,9.39,8.97" target="#b6">[7]</ref>.</p><p>The manual canonical response is sampled from the baseline system for the manually rewritten query, those released as the oracle context-free queries. Systems that use the manual canonical responses are considered "manual" runs in the participating guideline, as the responses are for oracle manual queries.</p><p>Although it organizers preferred responses returned by the baseline system, it was not required. When the response passage is not returned at an appropriate rank by the canonical query, the organizers insert it into the ranking for both the manual and the automatic canonical responses. We discuss these elements in more depth later.</p><p>Baseline System. The initial ranking is produced by Solr using BM25 (k1=1.2, b=0.75), distributed (multi-partition) idfs, stopword removal, and KStem stemming. Based on preliminary experiments, it reranks the top 500 documents using a BERT-base reranker with a publicly available Hugging Face software<ref type="foot" coords="2,472.64,304.12,3.38,7.27" target="#foot_0">1</ref> model. It is fine-tuned using 12.8M query-passage pairs from the MS-MARCO passage ranking corpus, following Nogueira, et al. <ref type="bibr" coords="2,470.19,328.02,10.44,8.97" target="#b4">[5]</ref> (batch size=32, gradients accumulated over 4 time steps, passages truncated to 256 tokens). The search engine is available for interactive search by track participants. Although not formally benchmarked, most queries returned results in under five seconds.</p><p>Other Data Resources. The organizers release similar resources released as year one <ref type="bibr" coords="2,395.94,393.77,9.44,8.97" target="#b1">[2,</ref><ref type="bibr" coords="2,407.84,393.77,7.42,8.97" target="#b2">3]</ref> that include the canonical results and baseline runs. In addition, the organizers release the contextual dependence labels for queries and results developed during topic curation. In each turn the corresponding curator manually notes which query or response from a previous turn is referred to if a contextual dependency exists. These labels are verified by at least one other organizer. In some cases the references may be ambiguous -to a query, result, or combination. The earliest reference to a concept in the conversation is used. We use this fine-grained annotation to analyze the influence of contextual dependencies for participating runs in Section 5 and believe they can also be be useful for future research.</p><p>Evaluation. The evaluation of CAsT 2020 follows CAsT 2019. Refer to the previous year for details <ref type="bibr" coords="2,453.98,536.24,9.33,8.97" target="#b1">[2,</ref><ref type="bibr" coords="2,465.55,536.24,6.22,8.97" target="#b2">3]</ref>.</p><p>The overlap among submissions is significantly higher in year two than in year one. This allowed assessors to judge deeper for some topics. All document pools are formed from all four runs across all participants. The pools for thirteen topics (81, 83, 84, 85, 87, 88, 90, 92, 95, 98, 99, 101, 102) use documents from ranks 1-10, as last year. The pools for eleven topics (82, 86, 89, 91, 93, 94, 96, 97, 103, 104, 105) include documents from ranks 1-15. Topic 100 use depth 15 for all queries except 100_8, which use depth 10. This variation enables us to analyze the effect of the pool size on recall.</p><p>Table <ref type="table" coords="2,349.18,645.83,4.09,8.97" target="#tab_1">2</ref> shows that the deeper pool increases the average number of judged documents per query by 47%, from 145 to 213. The number of relevant documents per query increases by about 33% for each level of relevance. This difference includes relevance levels 2-4 that have few relevant documents. It suggests that judging four runs per participant up to depth 10 documents misses some relevant documents.</p><p>We advise researchers to monitor whether returned results are assessed because judgments may be incomplete. The queries that have deeper pools may more complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Result Dependence Considerations</head><p>The introduction of response dependence more closely models the challenges faced in conversational search systems, but the offline nature of this benchmark and our goal to ensure re-usability introduces challenges and limitations, which we highlight in this section.</p><p>Canonical response limitations. Adding result dependence presented challenges developing topics, particularly when providing canonical responses for all turns. A canonical response is always provided -regardless of its relevance and whether it is referred to in subsequent turns. However, due to the nature of how they are constructed the results that are referred to later are more likely to be relevant because they are building block for discussion.</p><p>Further, artificially adding results not present in the baseline response list is problematic. For both manual and automatic versions the ground truth response dependence is hidden, embedded in the provided responses. But where there is dependence the results much be shared to keep the automatic and manual conversations consistent. As a result, there is the the potential to implicitly indicate result dependence in an artificial way if different setups are compared.</p><p>Systems using the canonical responses have access to data that systems without access do not. Systems using canonical results may have a limited implicit advantage. As a result, we separate comparison of systems that use canonical results from those that only use previous utterances and recommend this be clearly stated when reporting future results. Due to the setup, systems that use a response are likely to require access to canonical responses.</p><p>Possible bias towards responses from the baseline. By design, all the contextual dependencies on responses came from the baseline system used to curate the topic. The dependent responses are manually selected by the organizers and are frequently relevant to the previous turn. The dependency on these baseline system's output may make it challenging for other systems that have different behavior.</p><p>Separate evaluation for automatic canonical. To ensure a fair comparison of systems this year we separate runs that use the automatic canonical responses in their inference and testing. As previously described, they have additional access to data that may partially indicate the relevance of some results. Although, we note that systems are still automatic and have far less information than systems using the manually resolved utterances (and results).</p><p>There are three categorizes in this year's runs, based on the data they used in the testing phrase:</p><p>(1) Manual: Runs that use the manually rewritten (resolved) context-free queries, and/or manual canonical responses. (2) Automatic-Canonical: Automatic runs that use the provided automatic canonical system responses. (3) Automatic: Runs that only use the provided raw conversational queries, automatic baseline results, automatic query rewrites, and/or other data source that do only contain manual or automatic-canonical information.</p><p>We also suggest that future research make this clear distinction to avoid confusion and for fair comparison. We envision that importance of dependence on results will increase and new methods of incorporating it in the benchmark in a reusable way will evolve in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PARTICIPANTS</head><p>CAsT received 51 run submissions from 15 teams shown in Table <ref type="table" coords="3,553.56,355.37,3.01,8.97" target="#tab_2">3</ref>. When submitting, we asked the participants to provide metadata describing certain properties of their runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Submitted team descriptions</head><p>Below are brief summaries of approaches from each participant. Teams are listed in alphabetical order.</p><p>‚Ä¢ ASCFDA -We first use a T5-based conversational query rewriting (CQR) model to solve the co-reference problem of queries. Then rewritten queries are exploited to do further expansion via three query expansion methods: RM3 from Anserini, keyword distillation method, and sentence-based query extraction. With the expanded queries, we retrieve candidate passages with BM25. A T5-based reranker is used to rerank the retrieved passages. ‚Ä¢ CMU-LTI -The pipeline consisted of two stages. The first stage used a BERT-based classifier (trained with weak supervision) to de-contextualize the input by selecting relevant terms from the dialog history. The selected terms were appended to the input question to create a query that Indri for initial retrieval. The second stage used BERT for reranking. It began by creating two types of queries for each question within a conversation using the BERT-based classifier used in the first stage. The first query consisted of relevant terms from the dialog history whereas the second query consisted of relevant terms from the top 5 passages (retrieved in the first round of retrieval). The queries were individually used to rerank passages. Finally, a fusion over the reranked list was performed to produce the final rankings. ‚Ä¢ DUTH -Our approach consists of two main steps: i) linguistic analysis and ii) query reformulation. We apply the AllenNLP co-reference resolution model to every query of each conversational session. Then, we use the SpaCy model for part-of-speech tagging, and keyword extraction from the current and the previous turns to be used as context. In the second step, we reformulate the initial query into a weighted query by keeping the keywords from the current turn and adding conversational context from previous turns before retrieval using Indri. The goal is for the conversational context of previous turns to have less impact than the keywords from the current turn while adding informational value. ‚Ä¢ GRILL -We leverage leverage query re-writing by finetuning a BART model on a full context generation objective for automatic runs. This is integrated into a multi-stage reranking pipeline with mono &amp; duo BERT for point-wise and pair-wise scoring, both for auto and manual runs. ‚Ä¢ h2oloo -We use a multi-stage pipeline for conversational search comprised of a query reformulation model followed by a two-stage retrieve-and-rerank text ranking model. Our query reformulation model is based on T5, fine-tuned on the conversational question rewriting dataset: CANARD. As for our two-stage text ranking model, we had a dense-sparse hybrid retrieval model followed by a T5 reranker, and the ranking model is fine-tuned on MS MARCO passage ranking dataset. During inference, we have different schemes to extract sentences from system responses, and the extracted sentences are treated as part of historical context for query reformulation.</p><p>‚Ä¢ HBKU -Passages were retrieved by implementing a multistage retrieval pipeline inspired by last year's winning solution with the addition of a pseudo-relevance feedback step where the query is expanded using top-k retrieved passages. Passages were re-ranked using a pre-trained BERT re-ranker. ‚Ä¢ HPCLab-CNR -Our utterance rewriting techniques enrich the raw utterances with the context extracted from the previous utterances. Two of our approaches are completely unsupervised, while the other two rely on utterances manually classified by human assessors. These approaches also employ the canonical responses for the automatically rewritten utterances provided by the organizers. ‚Ä¢ ielab -We investigated two methods to improve both the retrieval and re-ranking stages of a conversational IR system. The first method focused on query adaptation, which extracted context from the first query only to expand all subsequent queries for a conversational session. The second method utilized the text-to-text transfer transformer (T5) as a scoring function within query likelihood model (QLM) for re-ranking passages. All submissions employed Anserini (with BM25 and RM3) for passage retrieval. ‚Ä¢ nova-search -Our submission is based on a three-stage pipeline composed of: 1) query rewriting, 2) passage retrieval, and 3) passage re-ranking. Query rewriting is performed using a T5 model fine-tuned on the conversational query rewriting task using the CANARD dataset. Retrieval is carried out by a Language Model with Dirichlet smoothing (LMD). Passage re-ranking uses a BERT model fine-tuned on MS MARCO on a relevance classification task. ‚Ä¢ POLYU_SOME -We use a 2-stage conversational search architecture, including the initial rule-based retrieval and BERT-based re-ranking. ‚Ä¢ UMass_CIIR -Our submission investigates the use of unsupervised query expansion. We submit two runs, in the first run we study the RM3 formulation for each turn in the conversation. In our second, we explore if the top retrieved passages from previous turns can be improve recall for the current turn.</p><p>‚Ä¢ USI -The system first performs query expansion by concatenating raw, relevant previous utterances, as predicted by an independent model trained on CAsTUR, with the current one. Initial ranking is performed by BM25, followed by ALBERT re-ranker trained on MS MARCO passage ranking task. Modifications of the approach include methods for using context: 1) feeding the previous utterance and its top-1 response to the model alongside the current one; 2) feeding up to 3 relevant utterances to the model and performing an attentive-sum to aggregate context information. Our last run uses automatically rewritten queries without context. ‚Ä¢ Uva.ILPS -Our main run (quretecQR) uses the QuReTeC term classification model applied on raw utterance and automatic canonical responses. For passage retrieval, we use Anserini with BERT-based reranker. The retrieval score is combined with the score of a reader model trained on the MRQA dataset reformulated as a binary classification task. ‚Ä¢ WaterlooClarke -Our runs this year were based solely on the raw utterances. For each utterance, we used two different query generation methods, executed these queries against the collection, and merged the top-500 passages produced by each query into a single pool. We then re-ranked this pool using BERT. ‚Ä¢ WLU -Our approach first uses Indri search engine to retrieve top 1000 sample answer paragraphs as the candidate set. Then a hierarchical session-based learning model with BERT encoding further matches how a candidate document is related to the sequence of utterances and their sample answer paragraphs.</p><p>Most teams used a multi-step pipeline consisting of: 1) conversational rewriting (optionally using the responses), 2) passage retrieval using traditional IR model, and 3) reranking with a finetuned neural language model. Almost all teams leverage pre-trained Transformer-based language models for rewriting (GPT-2, BART, T5) and ranking (BERT, ALBERT, T5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OVERALL RESULTS</head><p>In this section we present results of the submitted runs. We also report three organizer baselines (prefixed org) that are included in the pooling and are available in the CAsT Github repository.</p><p>The main results are turn-level macro-averaged response effectiveness. We use four standard evaluation measures: Recall, Mean Average Precision (MAP@1000), Mean Reciprocal Rank (MRR), and Normalized Discounted Cumulative Gain (NDCG@1000). Continuing with Year one, the primary measure is NDCG@3 to focus on high-precision and quality responses in the top ranks. The other evaluation measures are computed at the standard 1000 cutoff. For the official results we threshold using a relevance cutoff of two as positive, because the value of one is marginal in the guidelines. We distinguish between three types of runs: automatic, automaticcanonical, and manual. Automatic runs use the provided test topics raw or with automatic rewriting. Automatic-canonical runs use the test topics as well as the provided canonical system responses. Manual runs use the the manually rewritten (resolved) queries, where coreference and result dependence are removed to create clear and unambiguous utterances, and also optionally the canonical responses provided (the result dependence is resolved in the manual query). Automatic run results. Table <ref type="table" coords="6,181.91,257.62,4.25,8.97" target="#tab_3">4</ref> shows the results for the 33 automatic runs. The median NDCG@3 score of all automatic runs is 0.304. The organizer provided GPT2 + BERT baseline is slightly below the median. For the best runs, there is a three-way tie between the top automatic runs. All of the top runs leverage neural language models for ranking (T5, Albert, and others). Further, all of the top five systems use T5-based re-rankers trained on MS MARCO. As we analyze in more detail below, generative sequence-to-sequence query rewriting with pre-trained language models (T5, BART) is widely used by the top performing runs. There also appears to be a strong relationship between system effectiveness in NDCG@3 and Recall of the overall run. This indicates that rewriting and first-pass retrieval phases are important for overall end-to-end effectiveness.</p><p>Automatic-canonical results. There are 9 automatic runs that utilize the provided canonical results. Table <ref type="table" coords="6,221.29,411.04,4.25,8.97" target="#tab_4">5</ref> shows the results on all measures. The median NDCG@3 is 0.331, higher than the automatic runs. The best performing run for canonical and automatic overall is h2oloo_RUN2. It is the only run to outperform the best non-canonical runs. It is also notably the only automatic run to outperform the organizer's manual BERT baseline. It uses rules to extract sentences from the canonical and retrieved results and incorporate them into the query reformulation process.</p><p>Manual run results. Table <ref type="table" coords="6,166.05,498.72,4.09,8.97" target="#tab_5">6</ref> shows the results for the 9 manual runs. The median run has an NDCG@3 value of 0.410. This is higher than the organizer provided SDM run, but below the organizer BERT baseline. The best performing run is grill_bmDuo with an NDCG@3 value of 0.530. It leverages a pairwise DuoBERT reranker on top mono-BERT results with a BM25 baseline. Similar to automatic runs, all of the top performing runs use a pre-trained language model for reranking. It's noteworthy that the gap between the best manual and automatic runs shrunk significantly in 2020 compared with 2019. The gap between manual and automatic systems is: 24% in the median; 16% for the best automatic, and 8% for the best automatic-canonical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results by turn depth</head><p>We now study how the systems performed as the conversation progresses and turn depth increases. Turns beyond ten (up to 13 for some topics) are truncated due to small sample size. The sample size for depth 9 is 10 queries and depth 10 is 6 queries. The results in Figure <ref type="figure" coords="6,91.09,699.74,4.25,8.97" target="#fig_0">1</ref> show the average NDCG@3 at each turn depth. To focus on strong systems the figure only shows data for runs that outperform the organizer baselines -auto_bertbase for automatic and canonical -automatic (20), canonical runs (4) and manual_sdm for manual (6); organizer runs are not included.</p><p>The results show that all systems perform well at the start of a topic. For automatic and canonical runs there is a sharp approximately 30-40% drop for turns two through four compared with the first turn. There is also a drop for manual systems, but it is smaller -17-30%. This indicates that these queries are harder as the conversation progresses. For automatic runs, the decreasing effectiveness trend continues, with it approximately halved at the end compared with the first turn. The automatic-canonical results also decrease, but not as much as the automatic runs.</p><p>In contrast, the results for manual runs show a different pattern. Effectiveness dips at turn 2 (by 30%), but then recovers and remains (mostly) stable. For all classes we observe a small decrease in effectiveness around turns 7 and eight, possibly due to topic shifts. The gap between manual and automatic systems appears to widen as the turn depth increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results by Topic</head><p>Figure <ref type="figure" coords="6,343.45,529.31,4.16,8.97">2</ref> provides a per-topic analysis comparing the three classes of systems across topics. It uses data from all submitted runs. The results show that the the topic difficulty varies widely across topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS ANALYSES</head><p>We encouraged the groups to submit metadata along with their runs. We designed a questionnaire with Yes/No questions in three categories: Query Understanding Methods/Data, Training/Retrieval Models Methods/Data, and the Utilization of Context information. Each question asks the teams to provide whether their run uses a specific type of resource or technique. This section discusses the impact of the varying approaches and resource usage on system effectiveness. We use all automatic runs -both automatic and automatic-canonical. This year we focus on the use of context. A complete breakdown of methods and their influence is included in Appendix A. Results by Topic </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Conversational Context</head><p>A key challenge in conversational search is to infer information needs through the use of the conversation history. Year two represents a shift from the first year in that the title and description are not provided. In addition, some turns explicitly rely on result context provided in the single canonical retrieved result (provided for 'canonical' runs). Below are the questions on context usage.</p><p>(1) Previous Raw. The method uses the previous raw utterances before the current turn.</p><p>(2) Previous automatic. The method uses the previously automatically rewritten queries in the topic. (3) Previous manual. The method uses the previous manual utterances for the topic. (4) Canonical result. The method uses the previous canonical results provided for the topic. (5) Automatic result. The method uses the provided automatic baseline results for the topic. (6) Manual results. The method uses the previous manual baseline results for the topic.</p><p>The results in Figure <ref type="figure" coords="7,408.35,523.84,4.25,8.97">3</ref> show the impact. It shows that almost all runs used the previous turn history (86%). Despite this, on average there is a 31% decrease over teams that did not use them. The runs that did not use the previous turns instead used the provided automatically rewritten queries and performed competitively. The biggest gain is for runs that use the manual canonical results. We note that these results rely on the (noisy) team self-assigned metadata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Explicit Query and Turn Dependence</head><p>In this section we study the effect of context. In the organizers' dependence annotation, there are 118 queries that depend on a query from a previous turn in the conversation. There are 56 queries that depend on results from previous turns. Note that a turn may depend on multiple previous queries, results, or a combination of both.  Figure <ref type="figure" coords="8,90.03,426.63,4.25,8.97" target="#fig_3">4</ref> shows the breakdown of these dependencies by turn depth is shown. It shows source turn that is referenced. Unsurprisingly, the figure shows that there is a strong dependence on the first turn. But, it also highlights a significant number of dependencies to turns deeper in the conversation. Further analysis shows a strong dependence on the previous turn, with over two thirds of the query dependencies being to the immediate previous turn. Less than a fifth of the query dependencies are hard, defined to be references not to the first or immediate preceding turn.</p><p>Table <ref type="table" coords="8,86.98,525.26,4.25,8.97" target="#tab_6">7</ref> shows the results broken down by different types of conversational context. Some turns (for example all first turns) do not rely on a previous turn at all. We label these None and they represent approximately 20% of turns. Others only depend on previous utterances Query. Over half of all turns depend on a previous utterance with the majority of these being the first turn. We also further split the dependence into a Query hard subset (approximately 10% of turns) where there is a dependence on a previous query that is not the first turn and not the immediate preceding turn. The bottom two rows focus on result dependence. The Result type has 27% of turns and Result Hard has 2% of turns. Similar to queries, the hard variant for results is result dependence beyond the first or immediate preceding turn.</p><p>The results show that turns without dependence are the easiest and systems perform the best on this subset (the best run is h2oloo_RUN2 with a mean NDCG@3 value of 0.571 on this subset). Turns with a query dependence perform in the middle -between turns without dependence and all turns. Notably, the hard variant performs only slightly worse than all query dependencies.</p><p>The most significant observation is that turns with result dependence perform much worse than those with query dependence (a 40% relative reduction compared with query dependence). We also observe that systems that use the results (canonical) perform only slightly better than those that don't. On the result dependence queries the h2oloo_RUN2 performs the best with a NDCG@3 value of 0.403 (vs 0.571 on queries without context). The only automatic run that outperforms the canonical mean (and median) on this subset of queries is h2oloo_RUN3 (NDCG@3 is 0.291). Interestingly, the best performing automatic run h2oloo_RUN4 performs worse on this subset (0.267). This indicates that methods to result dependence is an area that needs improvement for the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>The second year of TREC CAsT continued developing resources for studying conversational information seeking and added to the community's understanding of the topic. Conversations became a little more natural, and contextual information became more complex. The number of assessed conversations and queries increased, providing further training and evaluation data.</p><p>‚Ä¢ Conversational Language Understanding. Instead of traditional NLP pipelines this year we observe the dominance and effectiveness of sequence-to-sequence neural rewriting methods introduced in Y1. In particular, the training of query rewrite models with T5 and BART is now feasible with available datasets (CAsT Y1 or CANARD). ‚Ä¢ Conversational Context. The results on the manual runs show that clean context has the potential to maintain or even improve effectiveness over the course of the conversation as an information need unfolds. This year we find the most effective automatic run is able to use result context, but methods are still primitive and most simply ignoring context. We find that result dependence is more challenging for current systems than query dependence. ‚Ä¢ Ranking. The use of pre-trained neural language models for ranking is the de facto standard for the most effective systems. We observe that recent advances to larger models and ones that go beyond pointwise ranking. ‚Ä¢ Automatic vs Manual. The gap between automatic and manual methods shrunk for the median systems. This indicates that automatic rewriting methods made significant gains. However, automatic methods show a significant degradation in the effectiveness as turn depth increases. There remains a large gap between automatic and manual systems at deeper turn depths.</p><p>After the success of year two we look forward to year three. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A META-ANALYSES</head><p>We provide additional analysis of the results across metadata dimensions for reference purposes and comparison with previous analyses. These include all automatic runs -both automatic and automatic-canonical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Query Understanding</head><p>The query understanding methods show the techniques used in understanding the conversational queries, including but not limited  to query rewriting, term re-weighting, query expansion, coreference resolution, and others. The first set of questions is on query understanding methods.</p><p>(1) None. Whether no query understanding method is used.</p><p>(2) Supervision. Whether the method uses any training data.</p><p>(3) Rules. Whether the method uses heuristic rules. (4) NLP Toolkit. Whether the method uses a standard NLP toolkit.</p><p>(5) Entity Linking. Whether the method uses entity linking. (6) Coreference. Whether the method uses coreference resolution. (7) Expansion. Whether the method performs query expansion. (8) Rewriting. Whether the method performs generative query rewriting. (9) Other. Whether the method uses other understanding methods.</p><p>Figure <ref type="figure" coords="9,353.47,477.53,4.17,8.97" target="#fig_4">5</ref> shows the use and effect of varying query understanding methods. Runs that did not use query understanding had the biggest loss, approximately 34% compared with those that did. The most commonly used approach is a query expansion with approximately 60% of runs, but the effect was mixed with runs using it performing approximately 26% worse than teams that did not. The approach that appears to have the greatest positive effect is the use of generative rewriting models, 30% of runs with a gain of 43% relative to those that did not. Entity linking also shows a positive 12% gain, but was used by only one run.</p><p>The second set of query understanding focuses on the datasets used to train query understanding and rewriting methods, including CAsT Y1, and others.</p><p>(1) CAsT Y1. Whether the method uses data provided in CAsT Y1 (2) MARCO Sessions. Whether the method uses data provided in the MARCO Conversational Sessions dataset. (3) Y1 rewrites. Whether the method is trained on CAsT Y1 manual query rewrites. (4) Y2 automatic. Whether the method uses the CAsT Y2 automatically rewritten queries.  (5) Y2 Manual. Whether the method uses CAsT Y2 manually rewritten queries. (6) External. Whether the method uses an external dataset. The results in Figure <ref type="figure" coords="10,145.32,526.17,4.25,8.97" target="#fig_5">6</ref> show the datasets used. It shows that using other forms of data resulted in a relative 15% decrease. In contrast, external datasets (such as CANARD) are commonly used and show a relative 51% gain. Note that external datasets were interpreted by some groups to include language models pre-trained on external collections (e.g. T5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Retrieval and Ranking</head><p>We also looked at the effect of different ranking methods and datasets used for training.</p><p>We first look at ranking methods. The first set of questions focused on the retrieval method.</p><p>(1) Unsupervised. Whether any training data has been used. (2) Dense retrieval. Whether a vector-based model is used for retrieval. (3) Entities. Whether an entity-knowledge graph is used. (4) Document Expansion. Whether document expansion is used.</p><p>(5) L2R. Whether the method uses a learning to rank method. (6) Neural. Whether the method uses a neural ranking model <ref type="bibr" coords="10,328.87,224.97,9.51,8.97" target="#b6">(7)</ref> Pre-trained LM. Whether the method uses a pre-trained language model (e.g. BERT).</p><p>The fraction of each technique used and their influence for automatic runs is shown in Figure <ref type="figure" coords="10,437.78,263.28,3.11,8.97">7</ref>. The result is clear -teams use pre-trained neural language models heavily with 71% of runs using them for ranking. However, we observe that their effect is varies widely and is quite mixed, with an average of 18% decrease -despite them being used in all of the top performing systems. There was no method that had a positive effect overall, which is surprising.</p><p>We now examine the training data used to train / fine-tune the models used. The results are shown in Figure <ref type="figure" coords="10,450.89,504.30,3.13,8.97">8</ref>. It shows that the majority (55%) of runs used data from MS MARCO and/or the Deep Learning track, followed by CAsT Y1 (14%). Overall, runs trained on DL / MS MARCO had a decrease in effectiveness of approximately 24%. Inspecting the run descriptions, it appears that most of the Train Other runs actually used MS MARCO and the metadata is incorrectly reported. The remaining runs are trained on other QA datasets (notably MRQA).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,326.53,253.63,223.09,7.70"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: NDCG@3 at varying conversation turn depth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,313.70,344.36,15.57,5.90;7,94.34,212.52,5.90,28.76;7,112.18,320.46,8.85,5.90;7,112.18,280.59,8.85,5.90;7,112.18,240.72,8.85,5.90;7,112.18,200.85,8.85,5.90;7,112.18,160.98,8.85,5.90;7,112.18,121.11,8.85,5.90;7,131.18,327.37,382.30,5.90;7,252.11,128.55,28.31,5.90;7,299.89,128.55,28.33,5.90;7,348.74,128.55,20.89,5.90"><head></head><label></label><figDesc>84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 Automatic Canonical Manual</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,214.41,378.55,183.18,7.70"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: NDCG@3 aggregated for each topic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,53.80,253.63,240.25,7.70;8,53.80,264.58,233.90,7.70"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Statistics on the source of contextual information in query and response (used by later turns) by turn depth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="9,53.80,263.16,240.25,7.70;9,53.80,274.12,234.44,7.70"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Query understanding method influences on system effectiveness for automatic and automatic canonical runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="9,317.96,263.16,241.85,7.70;9,317.96,274.12,227.14,7.70"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Query understanding data influences on system effectiveness for automatic and automatic canonical runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="10,53.80,251.23,241.85,7.70;10,53.80,262.18,156.81,7.70"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: Retrieval method influences on system effectiveness for automatic and canonical runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="10,328.87,356.39,230.85,8.97;10,342.36,367.35,69.38,8.97;10,328.87,378.31,230.85,8.97;10,342.36,389.27,30.84,8.97;10,328.87,400.23,221.95,8.97;10,328.87,411.19,229.34,8.97;10,342.36,422.15,141.74,8.97;10,328.87,433.10,229.34,8.97;10,342.36,444.06,125.16,8.97;10,328.87,455.02,230.85,8.97;10,342.36,465.98,131.21,8.97;10,328.87,476.94,229.51,8.97;10,342.36,487.90,31.37,8.97"><head>( 1 ) 2 )</head><label>12</label><figDesc>Auto baseline. Whether the method uses the provided automatic baseline run. (Manual baseline. Whether the method uses the manual baseline run. (3) CAsT Y1. Whether the model is trained on CAsT Y1 data. (4) DL / MARCO Training. Whether the method is trained with Deep Learning or MS MARCO dataset. (5) Natural Questions. Whether the model is trained using the Google Natural Questions dataset. (6) QUAC. Whether the model is trained on the Question Answering in Context (QUAC) dataset. (7) Other Training. Whether the method is trained with other datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,323.78,154.18,230.12,119.65"><head>Table 1 :</head><label>1</label><figDesc>CAsT 2020 Topic 91.</figDesc><table coords="1,323.78,176.40,230.12,97.43"><row><cell cols="2">Title: GDPR</cell></row><row><cell cols="2">Description: learn about GDPR, the privacy issues in social</cell></row><row><cell cols="2">networks, and the addiction of it.</cell></row><row><cell cols="2">Turn Conversation Utterances</cell></row><row><cell>1</cell><cell>What is the purpose of GDPR?</cell></row><row><cell>2</cell><cell>What is different compared to previous legislation?</cell></row><row><cell>3</cell><cell>What are the privacy implications of those technolo-</cell></row><row><cell></cell><cell>gies?</cell></row><row><cell>4</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,317.66,85.73,242.07,121.65"><head>Table 2 :</head><label>2</label><figDesc>The effect of pool depth on the average number of judged documents per query, averaged over 106 queries. Both pools were formed from up to 4 runs per team.</figDesc><table coords="3,358.86,130.96,158.44,76.42"><row><cell cols="4">Label Depth 10 Depth 15 Increase</cell></row><row><cell>0</cell><cell>125</cell><cell>186</cell><cell>49%</cell></row><row><cell>1</cell><cell>9</cell><cell>12</cell><cell>33%</cell></row><row><cell>2</cell><cell>6</cell><cell>8</cell><cell>33%</cell></row><row><cell>3</cell><cell>2</cell><cell>3</cell><cell>50%</cell></row><row><cell>4</cell><cell>3</cell><cell>4</cell><cell>33%</cell></row><row><cell>Total</cell><cell>145</cell><cell>213</cell><cell>47%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,130.23,85.73,351.54,289.67"><head>Table 3 :</head><label>3</label><figDesc>Participants and their runs.</figDesc><table coords="4,130.23,108.97,351.54,266.43"><row><cell>Group</cell><cell>Run ID</cell><cell cols="2">Run Type Group</cell><cell>Run ID</cell><cell>Run Type</cell></row><row><cell>ASCFDA</cell><cell>ASCFDA_baseline</cell><cell>automatic</cell><cell>ielab</cell><cell>ielab-bertAQ</cell><cell>automatic</cell></row><row><cell>ASCFDA</cell><cell>ASCFDA_d2q_emb</cell><cell>automatic</cell><cell>ielab</cell><cell>ielab-bertPRFAQ</cell><cell>automatic</cell></row><row><cell>ASCFDA</cell><cell>ASCFDA_qa</cell><cell>manual</cell><cell>ielab</cell><cell>ielab-bm25AQ</cell><cell>automatic</cell></row><row><cell>ASCFDA</cell><cell>ASCFDA_rm3</cell><cell>automatic</cell><cell>ielab</cell><cell cols="2">ielab-bm25T5QLM manual</cell></row><row><cell>CMU-LTI</cell><cell>PAS_BQUERY_MER</cell><cell>automatic</cell><cell>nova-search</cell><cell>AUTO_BERT100</cell><cell>automatic</cell></row><row><cell>CMU-LTI</cell><cell>PAS_RQUERY_MER</cell><cell>automatic</cell><cell>nova-search</cell><cell>AUTO_T5_RRF</cell><cell>automatic</cell></row><row><cell>CMU-LTI</cell><cell cols="2">PASO_BQUERO_MER automatic</cell><cell>nova-search</cell><cell>T5_BERT100</cell><cell>automatic</cell></row><row><cell>DUTH</cell><cell>duth</cell><cell>automatic</cell><cell>POLYU_SOME</cell><cell>man_polyu1</cell><cell>manual</cell></row><row><cell>DUTH</cell><cell>duth_arq</cell><cell>automatic</cell><cell>POLYU_SOME</cell><cell>raw_polyu1</cell><cell>automatic</cell></row><row><cell>DUTH</cell><cell>duth_manual</cell><cell>manual</cell><cell>UMass_CIIR</cell><cell>umass_4prev_rm3</cell><cell>automatic</cell></row><row><cell>grill</cell><cell>grill_bmDuo</cell><cell>manual</cell><cell>UMass_CIIR</cell><cell>umass_curr_rm3</cell><cell>manual</cell></row><row><cell>grill</cell><cell>grill_ctxDuo</cell><cell>manual</cell><cell>USI</cell><cell>castur_albert</cell><cell>automatic</cell></row><row><cell>grill</cell><cell>grill_duoBART</cell><cell>automatic</cell><cell>USI</cell><cell>hist_attention</cell><cell>canonical</cell></row><row><cell>grill</cell><cell>grill_fuseDuo</cell><cell>manual</cell><cell>USI</cell><cell>hist_concat</cell><cell>canonical</cell></row><row><cell>h2oloo</cell><cell>h2oloo_RUN1</cell><cell>canonical</cell><cell>USI</cell><cell>rewrite_albert</cell><cell>automatic</cell></row><row><cell>h2oloo</cell><cell>h2oloo_RUN2</cell><cell>canonical</cell><cell>UvA.ILPS</cell><cell>baselineQR</cell><cell>automatic</cell></row><row><cell>h2oloo</cell><cell>h2oloo_RUN3</cell><cell>automatic</cell><cell>UvA.ILPS</cell><cell>humanQR</cell><cell>manual</cell></row><row><cell>h2oloo</cell><cell>h2oloo_RUN4</cell><cell>automatic</cell><cell>UvA.ILPS</cell><cell>quretecNoRerank</cell><cell>canonical</cell></row><row><cell>HBKU</cell><cell>HBKU_t2_1v1</cell><cell>automatic</cell><cell>UvA.ILPS</cell><cell>quretecQR</cell><cell>canonical</cell></row><row><cell>HBKU</cell><cell>HBKU_t2_1v2</cell><cell>automatic</cell><cell cols="2">WaterlooClarke WatACBase</cell><cell>automatic</cell></row><row><cell>HBKU</cell><cell>HBKU_t5_1v1</cell><cell>automatic</cell><cell cols="2">WaterlooClarke WatACBaseRe</cell><cell>automatic</cell></row><row><cell>HBKU</cell><cell>HBKU_t5_1v2</cell><cell>automatic</cell><cell cols="2">WaterlooClarke WatACGPT2Re</cell><cell>automatic</cell></row><row><cell cols="2">HPCLab-CNR HPCLab-CNR-run1</cell><cell>canonical</cell><cell cols="2">WaterlooClarke WatACReAll</cell><cell>automatic</cell></row><row><cell cols="2">HPCLab-CNR HPCLab-CNR-run2</cell><cell>automatic</cell><cell>WLU</cell><cell cols="2">WLU_ManUttOnly manual</cell></row><row><cell cols="2">HPCLab-CNR HPCLab-CNR-run3</cell><cell>canonical</cell><cell>WLU</cell><cell cols="2">WLU_RawUttOnly automatic</cell></row><row><cell cols="2">HPCLab-CNR HPCLab-CNR-run4</cell><cell>automatic</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,317.66,85.73,241.63,312.45"><head>Table 4 :</head><label>4</label><figDesc>Automatic response retrieval results. The first four metrics use cutoff@1000. Binary relevance metrics, Recall, MAP, and MRR, use relevance scale ‚â• 2 as positive.</figDesc><table coords="5,321.46,130.23,233.24,267.94"><row><cell>Group</cell><cell>Run</cell><cell cols="4">Recall MAP MRR NDCG NDCG@3</cell></row><row><cell>h2oloo</cell><cell>h2oloo_RUN4</cell><cell>0.633</cell><cell>0.302 0.593</cell><cell>0.526</cell><cell>0.458</cell></row><row><cell>ASCFDA</cell><cell>ASCFDA_baseline</cell><cell>0.587</cell><cell>0.279 0.597</cell><cell>0.494</cell><cell>0.458</cell></row><row><cell>ASCFDA</cell><cell>ASCFDA_d2q_emb</cell><cell>0.588</cell><cell>0.278 0.595</cell><cell>0.493</cell><cell>0.458</cell></row><row><cell>h2oloo</cell><cell>h2oloo_RUN3</cell><cell>0.639</cell><cell>0.296 0.582</cell><cell>0.522</cell><cell>0.452</cell></row><row><cell>ASCFDA</cell><cell>ASCFDA_rm3</cell><cell>0.612</cell><cell>0.274 0.584</cell><cell>0.500</cell><cell>0.451</cell></row><row><cell>grill</cell><cell>grill_duoBART</cell><cell>0.506</cell><cell>0.190 0.520</cell><cell>0.403</cell><cell>0.398</cell></row><row><cell>USI</cell><cell>rewrite_albert</cell><cell>0.507</cell><cell>0.189 0.495</cell><cell>0.389</cell><cell>0.339</cell></row><row><cell cols="2">WaterlooClarke WatACReAll</cell><cell>0.617</cell><cell>0.204 0.458</cell><cell>0.435</cell><cell>0.337</cell></row><row><cell cols="2">WaterlooClarke WatACGPT2Re</cell><cell>0.523</cell><cell>0.191 0.435</cell><cell>0.393</cell><cell>0.325</cell></row><row><cell>CMU-LTI</cell><cell>PAS_BQUERY_MER</cell><cell>0.511</cell><cell>0.174 0.461</cell><cell>0.381</cell><cell>0.325</cell></row><row><cell>CMU-LTI</cell><cell cols="2">PASO_BQUERO_MER 0.492</cell><cell>0.174 0.451</cell><cell>0.378</cell><cell>0.323</cell></row><row><cell>UvA.ILPS</cell><cell>baselineQR</cell><cell>0.238</cell><cell>0.129 0.427</cell><cell>0.261</cell><cell>0.319</cell></row><row><cell>CMU-LTI</cell><cell>PAS_RQUERY_MER</cell><cell>0.523</cell><cell>0.170 0.450</cell><cell>0.382</cell><cell>0.318</cell></row><row><cell>HBKU</cell><cell>HBKU_t5_1v2</cell><cell>0.494</cell><cell>0.176 0.445</cell><cell>0.379</cell><cell>0.313</cell></row><row><cell>HBKU</cell><cell>HBKU_t2_1v2</cell><cell>0.495</cell><cell>0.177 0.440</cell><cell>0.377</cell><cell>0.309</cell></row><row><cell>HBKU</cell><cell>HBKU_t5_1v1</cell><cell>0.494</cell><cell>0.171 0.428</cell><cell>0.374</cell><cell>0.307</cell></row><row><cell>nova-search</cell><cell>AUTO_BERT100</cell><cell>0.521</cell><cell>0.151 0.422</cell><cell>0.364</cell><cell>0.304</cell></row><row><cell>nova-search</cell><cell>AUTO_T5_RRF</cell><cell>0.547</cell><cell>0.151 0.420</cell><cell>0.377</cell><cell>0.302</cell></row><row><cell>nova-search</cell><cell>T5_BERT100</cell><cell>0.502</cell><cell>0.148 0.416</cell><cell>0.353</cell><cell>0.301</cell></row><row><cell>-</cell><cell>org_auto_bertbase</cell><cell>0.308</cell><cell>0.134 0.408</cell><cell>0.284</cell><cell>0.300</cell></row><row><cell cols="2">WaterlooClarke WatACBaseRe</cell><cell>0.524</cell><cell>0.175 0.425</cell><cell>0.384</cell><cell>0.298</cell></row><row><cell>HBKU</cell><cell>HBKU_t2_1v1</cell><cell>0.495</cell><cell>0.169 0.418</cell><cell>0.369</cell><cell>0.296</cell></row><row><cell>HPCLab-CNR</cell><cell>HPCLab-CNR-run4</cell><cell>0.489</cell><cell>0.164 0.421</cell><cell>0.359</cell><cell>0.292</cell></row><row><cell>USI</cell><cell>castur_albert</cell><cell>0.475</cell><cell>0.160 0.423</cell><cell>0.356</cell><cell>0.281</cell></row><row><cell>HPCLab-CNR</cell><cell>HPCLab-CNR-run2</cell><cell>0.508</cell><cell>0.154 0.401</cell><cell>0.360</cell><cell>0.275</cell></row><row><cell>POLYU_SOME</cell><cell>raw_polyu1</cell><cell>0.503</cell><cell>0.128 0.408</cell><cell>0.346</cell><cell>0.265</cell></row><row><cell>UMass_CIIR</cell><cell>umass_4prev_rm3</cell><cell>0.548</cell><cell>0.114 0.332</cell><cell>0.342</cell><cell>0.211</cell></row><row><cell>DUTH</cell><cell>duth_arq</cell><cell>0.524</cell><cell>0.096 0.278</cell><cell>0.257</cell><cell>0.167</cell></row><row><cell cols="2">WaterlooClarke WatACBase</cell><cell>0.527</cell><cell>0.086 0.236</cell><cell>0.295</cell><cell>0.151</cell></row><row><cell>DUTH</cell><cell>duth</cell><cell>0.422</cell><cell>0.087 0.235</cell><cell>0.311</cell><cell>0.144</cell></row><row><cell>ielab</cell><cell>ielab-bm25AQ</cell><cell>0.445</cell><cell>0.078 0.195</cell><cell>0.246</cell><cell>0.133</cell></row><row><cell>ielab</cell><cell>ielab-bertAQ</cell><cell>0.445</cell><cell>0.052 0.147</cell><cell>0.217</cell><cell>0.079</cell></row><row><cell>ielab</cell><cell>ielab-bertPRFAQ</cell><cell>0.350</cell><cell>0.043 0.134</cell><cell>0.179</cell><cell>0.078</cell></row><row><cell>WLU</cell><cell>WLU_RawUttOnly</cell><cell>0.279</cell><cell>0.010 0.059</cell><cell>0.111</cell><cell>0.022</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,317.66,418.27,242.15,123.97"><head>Table 5 :</head><label>5</label><figDesc>Automatic systems using automatic canonical result context. The first four metrics use cutoff@1000. Binary relevance metrics use relevance scale ‚â• 2 as positive.</figDesc><table coords="5,321.65,462.87,232.86,79.37"><row><cell>Group</cell><cell>Run</cell><cell cols="4">Recall MAP MRR NDCG NDCG@3</cell></row><row><cell>h2oloo</cell><cell>h2oloo_RUN2</cell><cell>0.705</cell><cell>0.326 0.621</cell><cell>0.575</cell><cell>0.493</cell></row><row><cell>h2oloo</cell><cell>h2oloo_RUN1</cell><cell>0.705</cell><cell>0.284 0.576</cell><cell>0.549</cell><cell>0.444</cell></row><row><cell>grill</cell><cell>grill_fuseDuo</cell><cell>0.770</cell><cell>0.243 0.584</cell><cell>0.536</cell><cell>0.444</cell></row><row><cell>UvA.ILPS</cell><cell>quretecQR</cell><cell>0.264</cell><cell>0.147 0.476</cell><cell>0.283</cell><cell>0.340</cell></row><row><cell cols="3">HPCLab-CNR HPCLab-CNR-run3 0.561</cell><cell>0.193 0.449</cell><cell>0.422</cell><cell>0.331</cell></row><row><cell cols="3">HPCLab-CNR HPCLab-CNR-run1 0.545</cell><cell>0.181 0.434</cell><cell>0.403</cell><cell>0.313</cell></row><row><cell>USI</cell><cell>hist_concat</cell><cell>0.475</cell><cell>0.160 0.424</cell><cell>0.354</cell><cell>0.281</cell></row><row><cell>USI</cell><cell>hist_attention</cell><cell>0.475</cell><cell>0.125 0.340</cell><cell>0.321</cell><cell>0.214</cell></row><row><cell>UvA.ILPS</cell><cell>quretecNoRerank</cell><cell>0.264</cell><cell>0.081 0.262</cell><cell>0.216</cell><cell>0.171</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,53.50,85.73,242.15,145.62"><head>Table 6 :</head><label>6</label><figDesc>Manual retrieval results. These runs used the manually resolved queries and/or manual canonical results. The first four metrics use cutoff@1000. Binary relevance metrics use relevance scale ‚â• 2 as positive.</figDesc><table coords="6,57.29,141.18,233.27,90.16"><row><cell>Group</cell><cell>Run</cell><cell cols="4">Recall MAP MRR NDCG NDCG@3</cell></row><row><cell>grill</cell><cell>grill_bmDuo</cell><cell>0.747</cell><cell>0.302 0.684</cell><cell>0.571</cell><cell>0.530</cell></row><row><cell>grill</cell><cell>grill_ctxDuo</cell><cell>0.770</cell><cell>0.338 0.655</cell><cell>0.607</cell><cell>0.507</cell></row><row><cell>UvA.ILPS</cell><cell>humanQR</cell><cell>0.395</cell><cell>0.241 0.640</cell><cell>0.414</cell><cell>0.498</cell></row><row><cell>-</cell><cell cols="2">org_manual_bertbase 0.498</cell><cell>0.252 0.652</cell><cell>0.451</cell><cell>0.479</cell></row><row><cell>ASCFDA</cell><cell>ASCFDA_qa</cell><cell>0.632</cell><cell>0.282 0.593</cell><cell>0.513</cell><cell>0.466</cell></row><row><cell>ielab</cell><cell>ielab-bm25T5QLM</cell><cell>0.526</cell><cell>0.231 0.581</cell><cell>0.447</cell><cell>0.410</cell></row><row><cell cols="2">POLYU_SOME man_polyu1</cell><cell>0.693</cell><cell>0.215 0.547</cell><cell>0.488</cell><cell>0.398</cell></row><row><cell>-</cell><cell>org_manual_sdm</cell><cell>0.770</cell><cell>0.195 0.479</cell><cell>0.493</cell><cell>0.321</cell></row><row><cell>UMass_CIIR</cell><cell>umass_curr_rm3</cell><cell>0.560</cell><cell>0.130 0.364</cell><cell>0.358</cell><cell>0.247</cell></row><row><cell>DUTH</cell><cell>duth_manual</cell><cell>0.691</cell><cell>0.153 0.380</cell><cell>0.419</cell><cell>0.243</cell></row><row><cell>WLU</cell><cell>WLU_ManUttOnly</cell><cell>0.723</cell><cell>0.037 0.148</cell><cell>0.287</cell><cell>0.067</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,53.50,287.61,243.39,119.26"><head>Table 7 :</head><label>7</label><figDesc>Dependence results on automatic and canonical runs. We report the average across runs outperforming the organizer's baseline org_auto_bertbase.</figDesc><table coords="8,58.78,332.84,238.11,74.02"><row><cell cols="4">Dependence Turns Auto. NDCG@3 Canon. NDCG@3</cell></row><row><cell>All turns</cell><cell>208</cell><cell>0.360</cell><cell>0.384</cell></row><row><cell>None</cell><cell>45</cell><cell>0.463</cell><cell>0.473</cell></row><row><cell>Query</cell><cell>118</cell><cell>0.379</cell><cell>0.387</cell></row><row><cell>Query (hard)</cell><cell>22</cell><cell>0.375</cell><cell>0.329</cell></row><row><cell>Result</cell><cell>56</cell><cell>0.226</cell><cell>0.285</cell></row><row><cell>Result (hard)</cell><cell>5</cell><cell>0.217</cell><cell>0.235</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,320.88,701.38,65.90,6.97"><p>https://huggingface.co/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">ACKNOWLEDGMENTS</head><p>We thank <rs type="person">Vaibhav Kumar</rs> and <rs type="person">Carlos Gemmell</rs> for their work in developing topics. <rs type="person">Vaibhav Kumar</rs> created the BERT reranker that generated canonical rankings. We thank <rs type="person">Cameron VandenBerg</rs> for setting up the Solr instances that generated the initial rankings. We thank <rs type="person">Shi Yu</rs> for running the automatic query rewriter on Year 2 topics. We thank the Deep Learning track organizers for helping align the two tracks. We also are deeply thankful for Ellen Voorhees' experience, patience, and persistence in running the assessment process. Finally we thank all our participants.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="9,65.99,435.73,229.23,6.97;9,65.99,443.70,228.05,6.97;9,65.99,451.67,201.84,6.97" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mc-Namara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09268</idno>
		<title level="m" coord="9,180.47,443.70,113.58,6.97;9,65.99,451.67,87.82,6.97">Ms marco: A human generated machine reading comprehension dataset</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,65.99,459.64,228.24,6.97;9,65.99,467.61,228.06,6.97;9,65.99,475.58,227.99,6.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,158.16,459.64,136.06,6.97;9,65.99,467.61,24.47,6.97">Cast 2019: The conversational assistance track overview</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,104.51,467.61,189.54,6.97;9,65.99,475.58,11.06,6.97">The Twenty-Eighth Text REtrieval Conference Proceedings (TREC 2019</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="9,65.99,483.55,228.05,6.97;9,65.99,491.52,229.23,6.97;9,65.99,499.49,229.13,6.97;9,65.75,507.46,33.84,6.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,187.29,483.55,106.75,6.97;9,65.99,491.52,56.77,6.97">Cast-19: A dataset for conversational information seeking</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,136.76,491.52,158.46,6.97;9,65.99,499.49,182.04,6.97">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1985" to="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,65.99,515.43,228.05,6.97;9,65.99,523.40,228.05,6.97;9,65.99,531.37,131.06,6.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,203.38,515.43,90.67,6.97;9,65.99,523.40,24.03,6.97">Trec complex answer retrieval overview</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dietz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Gamari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,103.92,523.40,190.13,6.97;9,65.99,531.37,13.82,6.97">The Twenty-Seventh Text REtrieval Conference Proceedings (TREC 2018)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="500" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,65.99,539.35,229.13,6.97;9,65.99,547.32,83.29,6.97" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="9,175.52,539.35,116.09,6.97">Multi-stage document ranking with BERT</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno>CoRR abs/1910.14424</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,65.99,555.29,228.88,6.97;9,65.99,563.26,228.05,6.97;9,65.78,571.23,136.84,6.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,65.99,563.26,172.57,6.97">Leading conversational search by suggesting useful questions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,252.34,563.26,41.70,6.97;9,65.78,571.23,59.77,6.97">Proceedings of The Web Conference</title>
		<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1160" to="1170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,65.99,579.20,228.06,6.97;9,65.99,587.17,201.33,6.97" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05009</idno>
		<title level="m" coord="9,236.19,579.20,57.85,6.97;9,65.99,587.17,87.20,6.97">Few-shot generative conversational query rewriting</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
