<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,104.61,114.94,402.78,15.12">Overview of the TREC 2020 Health Misinformation Track</title>
				<funder>
					<orgName type="full">University of Waterloo</orgName>
				</funder>
				<funder>
					<orgName type="full">Natural Sciences and Engineering Research Council of Canada</orgName>
					<orgName type="abbreviated">NSERC</orgName>
				</funder>
				<funder ref="#_N8nWJms">
					<orgName type="full">European Union</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.00,148.84,104.73,10.48"><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,188.31,148.84,73.16,10.48"><forename type="first">Maria</forename><surname>Maistro</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,273.04,148.84,56.03,10.48"><forename type="first">Saira</forename><surname>Rizvi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,340.64,148.84,89.45,10.48"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,464.04,148.84,71.23,10.48"><forename type="first">Guido</forename><surname>Zuccon</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Queensland</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,104.61,114.94,402.78,15.12">Overview of the TREC 2020 Health Misinformation Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3DE6F3C189E8DEB921DF61EE7D116D8B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>TREC 2020 was the second year for the Health Misinformation track, which was named the Decision Track in 2019 <ref type="bibr" coords="1,143.43,298.49,10.91,9.57" target="#b0">[1]</ref>. Information retrieval using document collections that contain misinformation are problematic. When a search engine returns documents that contain misinformation, users may have difficulty discerning correct from incorrect information and the incorrect information can lead to incorrect decisions <ref type="bibr" coords="1,178.60,339.14,10.91,9.57" target="#b4">[5]</ref>. Decisions regarding health-related topics can be consequential, and as such we want search engines that enable users to make correct decisions. The track is designed to address the problem of health misinformation in three areas: 1) adhoc retrieval, 2) the total recall of misinformation in the collection, and 3) the evaluation of retrieval in the presence of misinformation.</p><p>The 2020 Health Misinformation track had both a recall task and an adhoc task for participants. With the onset of the coronavirus pandemic (COVID-19), the track organizers selected a document collection of news from the Common Crawl<ref type="foot" coords="1,275.58,432.03,4.23,6.99" target="#foot_0">1</ref> that covered the first four months of 2020. The track's topics were all related to COVID-19 and posed as questions such as "Can gargling salt water prevent COVID-19?" For both tasks, NIST assessors judged documents' usefulness for answering a topic's question, and if judged to be useful, assessors then recorded if the document contained a specific answer to the question and then judged the credibility of the document. We evaluated recall runs on their ability to find all documents containing incorrect information (misinformation). For adhoc runs, we measured their ability to return useful, correct, and credible information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Topics</head><p>The COVID-19 pandemic has highlighted the dangers that uncontrolled proliferation of misinformation can have on consumer health. Therefore, the topics for this track focused on the consumer health search domain relevant to COVID-19, i.e., people seeking medical advice online. Unlike other TREC tracks, the assessors did not create the topics and were instead provided with topics that included title, description, answer, narrative, and evidence fields. Figure <ref type="figure" coords="1,445.26,628.20,5.45,9.57" target="#fig_0">1</ref> shows an example of a topic.</p><p>The final topics were shortlisted from a collection of 74 candidate topics that were collected primarily using WHO mythbusters page 2 and Harvard Medical School page on treatments for COVID-19 3 . Rest of the topics were created using well known fact-checking websites (Snopes, etc). Each topic was assessed on its quality by ensuring that the corpus contains at least one example of a negative and a positive document. The final list was created by filtering out topics for which misinformation did not exist in the corpus, and the topics that were less prevalent on the internet. The topic were provided in an XML file 4 .</p><p>The title field of each topic is built as a pair of treatment and disease, where for TREC 2020, the disease is always COVID-19. The description is in the form of a question and is built as a triplet of (treatment, effect, disease), where the effect can be: cause, prevent, worsen, cure, help. Only these terms were used, resulting in descriptions of form: "Can X Y COVID-19?", where X is a treatment and Y is one of the five effect terms. For example, "Can vitamin D cure COVID-19?" The answer field is either"yes" or "no". The participants were asked to consider this field as the ground truth for the task. However, since the answer corresponds to the topic writer's best understanding of medical consensus at the time of topic creation, it is not medical advice, and should not be taken as truth outside of the context of this track. For the total recall task, participants were asked to identify all documents that contradicted this field. For the adhoc task, the participants were to return the most credible and complete information supporting the answer field for each topic. Finally, the evidence field contains the URL of a page from the open Web that was used to determine this answer. This page may or may not be part of the corpus. The narrative and evidence fields were created to assist the assessors in document judgment and were not to be used by the participants to generate automatic runs. All other fields were allowed to be used.</p><p>During the the topic creation, there was cut-and-paste error resulting in a duplicate topic. The duplicate topics were 9 and 17, Convalescent plasma COVID-19. However, this error did not otherwise jeopardize the integrity of the results. &lt;topics&gt; &lt;topic&gt; &lt;number&gt;0&lt;/number&gt; &lt;query&gt;ibuprofen COVID-19&lt;/query&gt; &lt;description&gt;Can ibuprofen worsen COVID-19?&lt;/description&gt; &lt;alignment&gt;no&lt;/alignment&gt; &lt;narrative&gt;Ibuprofen is an anti-inflammatory drug used to reduce fever and treat pain or inflammation. Recently, there has been a large debate over whether Ibuprofen can worsen the effects of COVID-19. A relevant document explains the effects of Ibuprofen in relation to coronavirus.&lt;/narrative&gt; &lt;/topic&gt; &lt;topic&gt; ... &lt;/topic&gt; &lt;/topics&gt; </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Document Collection</head><p>The track used the documents found in the CommonCrawl News crawl from January 1, 2020 to April 30, 2020. The crawl contains news articles from websites all over the world. The crawl is distributed in the Web ARChive (WARC) format. Each document in a WARC file contains a WARC header and the raw data from the crawl. The WARC files contain a mixture of records. The only records we used had a WARC-Type of "response". Some participants returned nonresponse WARC records, e.g. request records. The collection contains non-English documents, and all non-English documents were considered to be non-relevant regardless of their content. NIST provided the collection also in WET format <ref type="foot" coords="3,280.98,192.75,4.23,6.99" target="#foot_2">5</ref> . The WET files contain only extracted content from the web pages, i.e. they parse the HTML and extract the text content. The four months of the CommonCrawl New Crawl is available via Amazon S3 at URIs: s3://commoncrawl/crawl-data/CC-NEWS/2020/01 s3://commoncrawl/crawl-data/CC-NEWS/2020/02 s3://commoncrawl/crawl-data/CC-NEWS/2020/03 s3://commoncrawl/crawl-data/CC-NEWS/2020/04</p><p>The "docno" or document identifier used was the WARC-Record-ID field for the WARC files and the WARC-Refers-To field for the WET files. Only unique id was used, for example, from the field value of &lt;urn:uuid:49ecaf74-b1aa-4563-83a0-c81cece0e284&gt;, only the id was used: 49ecaf74-b1aa-4563-83a0-c81cece0e284.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Tasks Overview</head><p>The track had two tasks: 1) Total Recall and 2) AdHoc Retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task 1: Total Recall</head><p>For the total recall task, the goal is to identify all the documents conveying incorrect information for a specific set of topics. Documents contradicting the topic's answer are assumed to be misinformation. Participants were to identify all documents in a collection that promulgate, promote, and/or support that misinformation. For example, for the topic "Can Ibuprofen worsen COVID-19", participants were to identify all documents indicating that Ibuprofen can worsen COVID-19. Documents making this claim for the purposes of debunking it are not misinformation. Participants submitted runs that ranked documents according to the likelihood that they promulgate misinformation. Up to 10,000 documents per topic could be submitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Task 2: AdHoc Retrieval</head><p>For the ad-hoc retrieval task, the goal is to design a ranking model that promotes credible and correct information over incorrect information. For a given topic, participants were to return relevant, credible, and correct information that will help searchers make correct decisions. Participants were to assume that the statement included in the topic description is correct or not, based on the answer field, even if they knew current medical or other evidence suggests otherwise. Runs were allowed to contain a maximum of 1,000 documents per topic.</p><p>Note that this task is more than simply a new definition of what is relevant. There are multiple types of results: useful and correct and credible, useful and correct but not credible, etc. as well as incorrect and non-useful documents. It is important that search results avoid containing incorrect results. In place of notions of correctness, the credibility of the information source is useful, and useful and credible information is preferred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Submitted Runs</head><p>Six groups submitted a total of 23 runs to the total recall task. Eight groups submitted 51 runs to the adhoc retrieval task. The UWaterlooMDS group submitted two baseline runs to each task on the behalf of the track organizers: bm25 desc and bm25 title. These runs used Anserini's default BM25 implementation with the WET files to retreive documents using either the topic description or the topic title as the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>Runs were evaluated by using two pieces of software: 1) an extension of trec eval<ref type="foot" coords="4,475.38,271.96,4.23,6.99" target="#foot_3">6</ref> to compute multiple aspect measures <ref type="bibr" coords="4,196.68,287.46,11.52,9.57" target="#b3">[4]</ref> and 2) a script<ref type="foot" coords="4,284.15,285.51,4.23,6.99" target="#foot_4">7</ref> to compute the compatibility measure <ref type="bibr" coords="4,480.94,287.46,11.52,9.57" target="#b2">[3,</ref><ref type="bibr" coords="4,496.57,287.46,7.68,9.57" target="#b1">2]</ref>. In all cases, we derive a qrels file to use with the specific measure from the original NIST qrels files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">qrels (query-relevance files)</head><p>NIST used the track's relevance assessing guidelines<ref type="foot" coords="4,326.53,348.70,4.23,6.99" target="#foot_5">8</ref> to generate the track's qrels. The format adopted for NIST qrels file is as follows: topic_id 0 doc_id usefulness-judgment answer-judgment credibility-judgment where the columns are space separated. Documents were assessed by NIST assessors with respect to 3 criteria, which were recorded in the NIST qrels as follows:</p><p>• Usefulness: does the document contain material that the search user might find useful in answering the topic's question? Usefulness was assessed on a binary scale: 0 if the document is not useful in answering the question and 1 otherwise. This is column 4 of the qrels file.</p><p>• Answer : does the document answer to the question in the description field? If so, is the answer yes or no? The answer will be assessed with 3 values: 0 means that the document does not answer the question, 1 means that the answer provided by the document is "yes", and 2 means that the answer is "yes". A value of -1 means that NIST made no judgment. This is column 5 of the qrels file.</p><p>• Credibility: how credible is the document? Credibility was assessed on a binary scale, where 0 stands for not credible and 1 stands for credible. A value of -1 means that NIST made no judgment. This is column 6 of the qrels file.</p><p>Notes:</p><p>• When a document was judged as not useful, it was not judged for its answer nor for its credibility. In some cases, a useful document was accidentally not judged for its answer or credibility, i.e. a "skip".</p><p>• Some participants submitted docids that were not WARC doc types of "response". While not explained clearly in the track guidelines, only WARC records of type "response" should have been used. In almost all cases, when an assessor was given a non-response docid to judge, it was judged "not useful". Rather than confuse matters by including non-response documents in the qrels, these qrels contain only the judgements for docs of type "response".</p><p>• There are four missing topics: 33, 35, 36, and 48. NIST ran out of time and was not able to judge all topics.</p><p>• Topics 9 and 17 are duplicate topics, but have different judgments. We accidentally duplicated the topics during process of consolidating our selected topics and converting them to XML.</p><p>Mapping to Correctness: Before computing any evaluation measure, we map answer labels to correctness labels. Correctness labels are obtained from the topic answer field and the answer labels provided by the assessors: a document is correct if it contains an answer that matches the topic's given answer. A document is incorrect if it contains an incorrect answer. For a document to be correct or incorrect, it has to be useful. Observe that "not correct" does not necessarily mean "incorrect". A not useful document is neither correct nor incorrect, since a not useful document is off topic. Likewise, if a document does not contain an answer or is not judged for "answer", it is neither correct nor incorrect. For example, in Figure <ref type="figure" coords="5,165.17,339.04,5.45,9.57" target="#fig_0">1</ref> the topic's given answer is "no". If a document for that topic has an answer of "no", then it is correct; if it has an answer of "yes", then it is incorrect; and if it has no answer, then it is not correct. Thus, the final correctness qrels will be as follows:</p><p>topic_id 0 doc_id usefulness-judgment correctness-judgment credibility-judgment where the answer label is replaced by correctness. Correctness can assume the values: -1 for not judged documents; 0 for incorrect documents; 1 for correct documents; 2 for documents that do not provide an answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Derived qrels</head><p>We take the NIST qrels and the topics and generate derived qrels for the various evaluation measures. For the derived qrels, we only include "relevant" documents, for not all topics contain results that meet the success criteria of a "relevant" document in the derived qrels. By excluding topics without "relevant" documents, the effectiveness measures are only computed over topics for which runs could feasible get a non-zero score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Graded / preference levels</head><p>For the compatibility measure, we converted the 3 aspects judged for documents (usefulness, correctness and credibility) into a basic preference ordering or graded relevance values as reported in Table <ref type="table" coords="5,102.00,617.19,4.24,9.57" target="#tab_0">1</ref>.</p><p>It is tempting to use the above scores to compute nDCG, but that ignores the incorrect information. A better solution is to create a set of helpful and harmful qrels. We create helpful qrels by taking all scores greater then zero. The scores define preference levels with a higher scoring document being preferred to a lower scoring documents. To create the harmful qrels, we use only the absolute value of the negative scores. Thus, the most harmful documents are those that are judged to be useful, incorrect, and credible. Useful, no answer or no judgment for answer, credible 1 2 or -1</p><formula xml:id="formula_0" coords="6,72.00,141.19,444.87,20.27">1 1</formula><p>Useful, no answer or no judgment, not credible or no judgment 1 2 or -1 0 or -1 0 Not useful, ignore answer and credibility. 0 ---1</p><p>Useful, incorrect, not credible or no judgment 1 0 0 or -1 -2</p><p>Useful, incorrect, credible 1 0 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Binary Relevance</head><p>We created a series of qrels files in the standard qrels format for binary relevance effectiveness measures. We made the following variations to allow us to use nDCG to evaluate runs in terms of a single aspect as well as combinations:</p><p>• Usefulness. Ignores answer correctness and document credibility.</p><p>• Useful and correct. Note that a document cannot be judged correct unless it is judged useful.</p><p>• Useful and credible. Note that a document cannot be judged credible unless it is judged useful.</p><p>• Useful and correct and credible.</p><p>• Incorrect. A document is incorrect if it is useful and contains and answer that is opposite of the topic's given answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Multiple Aspect qrels</head><p>We created both two aspect and three aspect qrels. For the three aspects qrels, this is the same as the correctness qrels file except that the correctness column is mapped to 1 if the document's answer matches the topic's, and to 0 otherwise (no distinction for not judged or no answer); and the credibility column is the same except that a -1 (not judged) is mapped to 0 (not credible).</p><p>The two aspect qrels are the same but only consider usefulness and one of the other two aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Task 1: Total Recall</head><p>For the total recall task, we use the binary qrels where a document is "relevant" if it is incorrect. Using these qrels, we compute R-precision, which is equivalent to R-recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Task 2: AdHoc Retrieval</head><p>For ad-hoc retrieval, the primary method is to use compatibility of results with helpful and harmful results. For secondary analysis purposes, we computed nDCG using the binary qrels for each of the aspects and the conjunction of all three. Finally, we computed the CAM of all three aspects using mean average precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>Four topics were not assessed by NIST: 33, 35, 36, and 48. Therefore, they are excluded from the following analyses. Likewise, for some versions of the derived qrels, there are no "relevant" documents for some topics, and thus different measures are often computed over different number of topics. Tables <ref type="table" coords="7,123.06,154.06,5.45,9.57" target="#tab_2">2</ref> and<ref type="table" coords="7,153.02,154.06,5.45,9.57" target="#tab_3">3</ref> report results for the adhoc task. Figure <ref type="figure" coords="7,357.99,154.06,5.45,9.57" target="#fig_1">2</ref> plots adhoc runs' compatibility with helpful and harmful results. For two runs with the same level of compatibility with helpful results, the run with the lower compatibility with harmful results is to be preferred. Thus the h2oloo.m8 run stands out for being both very helpful and with some of the least harm. Table <ref type="table" coords="7,463.36,194.70,5.45,9.57">4</ref> reports results for the total recall task.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,139.70,617.85,332.61,9.57"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of topic for the Health Misinformation Track 2020</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,72.00,608.00,467.99,9.57;8,72.00,621.55,462.48,9.57"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Adhoc results: Compatibility of runs with helpful and harmful results. A good run is helpful and not harmful. For a given level of helpfulness, a run with less harm is to be preferred.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,72.00,74.03,464.82,75.55"><head>Table 1 :</head><label>1</label><figDesc>Preference ordering for documents which are mapped to graded relevance labels.</figDesc><table coords="6,72.00,100.74,80.62,8.39"><row><cell>Score Description</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,72.00,608.00,467.99,23.12"><head></head><label></label><figDesc>A good run is helpful and not harmful. For a given level of helpfulness, a run with less harm is to be preferred.</figDesc><table coords="9,115.25,73.96,381.49,630.80"><row><cell></cell><cell></cell><cell></cell><cell>CAM</cell><cell cols="2">Compatibility</cell></row><row><cell>Group</cell><cell>Run</cell><cell>Type</cell><cell cols="3">MAP help harm help-harm</cell></row><row><cell>h2oloo</cell><cell>h2oloo.m8</cell><cell>auto</cell><cell cols="2">0.253 0.490 0.016</cell><cell>0.474</cell></row><row><cell>h2oloo</cell><cell>h2oloo.m5</cell><cell>auto</cell><cell cols="2">0.319 0.549 0.080</cell><cell>0.469</cell></row><row><cell>h2oloo</cell><cell>h2oloo.m3</cell><cell>auto</cell><cell cols="2">0.292 0.511 0.075</cell><cell>0.436</cell></row><row><cell>h2oloo</cell><cell>h2oloo.m7</cell><cell>auto</cell><cell cols="2">0.222 0.449 0.015</cell><cell>0.434</cell></row><row><cell>h2oloo</cell><cell>h2oloo.m9</cell><cell>auto</cell><cell cols="2">0.297 0.502 0.075</cell><cell>0.427</cell></row><row><cell>h2oloo</cell><cell>h2oloo.m10</cell><cell>auto</cell><cell cols="2">0.286 0.483 0.065</cell><cell>0.418</cell></row><row><cell>h2oloo</cell><cell>h2oloo.m4</cell><cell>auto</cell><cell cols="2">0.297 0.466 0.120</cell><cell>0.346</cell></row><row><cell>h2oloo</cell><cell>h2oloo.m2</cell><cell>auto</cell><cell cols="2">0.273 0.440 0.113</cell><cell>0.327</cell></row><row><cell>Webis</cell><cell>cn-kq-td</cell><cell cols="3">manual 0.080 0.334 0.052</cell><cell>0.282</cell></row><row><cell>KU</cell><cell>adhoc run3</cell><cell>auto</cell><cell cols="2">0.250 0.401 0.121</cell><cell>0.280</cell></row><row><cell>KU</cell><cell>adhoc run13</cell><cell>auto</cell><cell cols="2">0.249 0.387 0.109</cell><cell>0.278</cell></row><row><cell>Webis</cell><cell>cn-kq-t-td</cell><cell cols="3">manual 0.086 0.331 0.054</cell><cell>0.277</cell></row><row><cell>Webis</cell><cell>cn-m-title</cell><cell cols="3">manual 0.136 0.357 0.082</cell><cell>0.275</cell></row><row><cell>KU</cell><cell>adhoc run5</cell><cell>auto</cell><cell cols="2">0.249 0.398 0.125</cell><cell>0.273</cell></row><row><cell>KU</cell><cell>adhoc run4</cell><cell>auto</cell><cell cols="2">0.278 0.410 0.139</cell><cell>0.271</cell></row><row><cell>KU</cell><cell>adhoc run7</cell><cell>auto</cell><cell cols="2">0.278 0.402 0.136</cell><cell>0.267</cell></row><row><cell>KU</cell><cell>adhoc run6</cell><cell>auto</cell><cell cols="2">0.284 0.414 0.149</cell><cell>0.266</cell></row><row><cell>KU</cell><cell>adhoc run2</cell><cell>auto</cell><cell cols="2">0.280 0.407 0.145</cell><cell>0.263</cell></row><row><cell>KU</cell><cell>adhoc run9</cell><cell>auto</cell><cell cols="2">0.249 0.347 0.089</cell><cell>0.258</cell></row><row><cell>KU</cell><cell>adhoc run12</cell><cell>auto</cell><cell cols="2">0.212 0.339 0.083</cell><cell>0.257</cell></row><row><cell>KU</cell><cell>adhoc run11</cell><cell>auto</cell><cell cols="2">0.282 0.399 0.146</cell><cell>0.253</cell></row><row><cell>KU</cell><cell>adhoc run8</cell><cell>auto</cell><cell cols="2">0.272 0.385 0.134</cell><cell>0.251</cell></row><row><cell>NLM</cell><cell>NLM CTM R1</cell><cell>auto</cell><cell cols="2">0.119 0.315 0.066</cell><cell>0.250</cell></row><row><cell>KU</cell><cell>adhoc run1</cell><cell>auto</cell><cell cols="2">0.237 0.368 0.120</cell><cell>0.248</cell></row><row><cell>h2oloo</cell><cell>h2oloo.m1</cell><cell>auto</cell><cell cols="2">0.237 0.368 0.120</cell><cell>0.248</cell></row><row><cell>KU</cell><cell>adhoc run10</cell><cell>auto</cell><cell cols="2">0.263 0.374 0.126</cell><cell>0.247</cell></row><row><cell cols="2">UWaterlooMDS bm25 desc</cell><cell>auto</cell><cell cols="2">0.236 0.366 0.125</cell><cell>0.241</cell></row><row><cell>vohcolab</cell><cell>vohbm25</cell><cell>auto</cell><cell cols="2">0.237 0.340 0.112</cell><cell>0.228</cell></row><row><cell>RealSakaiLab</cell><cell>RSL BM25LM</cell><cell>auto</cell><cell cols="2">0.138 0.257 0.045</cell><cell>0.212</cell></row><row><cell>NLM</cell><cell cols="2">NLM BNU T5 CTM auto</cell><cell cols="2">0.083 0.266 0.061</cell><cell>0.205</cell></row><row><cell>RealSakaiLab</cell><cell>RSL BM25LMC</cell><cell>auto</cell><cell cols="2">0.148 0.268 0.070</cell><cell>0.198</cell></row><row><cell>NLM</cell><cell>NLM CTM R2</cell><cell>auto</cell><cell cols="2">0.051 0.223 0.036</cell><cell>0.186</cell></row><row><cell>Webis</cell><cell>cn-descr-2</cell><cell>auto</cell><cell cols="2">0.131 0.318 0.133</cell><cell>0.185</cell></row><row><cell>CiTIUS</cell><cell>CiTIUSSimRelAdh</cell><cell cols="3">manual 0.079 0.238 0.060</cell><cell>0.178</cell></row><row><cell>NLM</cell><cell cols="2">NLM BNU ENS NLI auto</cell><cell cols="2">0.037 0.192 0.019</cell><cell>0.173</cell></row><row><cell>RealSakaiLab</cell><cell>RSL BM25LC</cell><cell>auto</cell><cell cols="2">0.139 0.239 0.067</cell><cell>0.173</cell></row><row><cell>NLM</cell><cell>NLM E4</cell><cell>auto</cell><cell cols="2">0.061 0.205 0.033</cell><cell>0.172</cell></row><row><cell>RealSakaiLab</cell><cell>RSL BM25</cell><cell>auto</cell><cell cols="2">0.139 0.217 0.048</cell><cell>0.169</cell></row><row><cell cols="2">UWaterlooMDS bm25 title</cell><cell>auto</cell><cell cols="2">0.139 0.217 0.048</cell><cell>0.169</cell></row><row><cell>NLM</cell><cell>NLM E3</cell><cell>auto</cell><cell cols="2">0.051 0.197 0.030</cell><cell>0.167</cell></row><row><cell>NLM</cell><cell>NLM BNU E GH</cell><cell>auto</cell><cell cols="2">0.089 0.246 0.080</cell><cell>0.166</cell></row><row><cell>Webis</cell><cell>cn-title-2</cell><cell>auto</cell><cell cols="2">0.114 0.240 0.082</cell><cell>0.158</cell></row><row><cell>NLM</cell><cell>NLM TME NLIR</cell><cell>auto</cell><cell cols="2">0.033 0.167 0.013</cell><cell>0.154</cell></row><row><cell>Webis</cell><cell>cn-ax-rer</cell><cell>auto</cell><cell cols="2">0.111 0.234 0.080</cell><cell>0.153</cell></row><row><cell>CiTIUS</cell><cell>CiTIUSCrdRelAdh</cell><cell>auto</cell><cell cols="2">0.036 0.172 0.048</cell><cell>0.125</cell></row><row><cell>Webis</cell><cell>cn-kq</cell><cell cols="3">manual 0.044 0.182 0.061</cell><cell>0.121</cell></row><row><cell>CiTIUS</cell><cell>CiTIUSSimAdh</cell><cell cols="3">manual 0.025 0.121 0.035</cell><cell>0.086</cell></row><row><cell>NLM</cell><cell>NLM TME GH</cell><cell>auto</cell><cell cols="2">0.086 0.153 0.068</cell><cell>0.085</cell></row><row><cell>NLM</cell><cell>NLM TME</cell><cell>auto</cell><cell cols="2">0.086 0.150 0.069</cell><cell>0.082</cell></row><row><cell>vohcolab</cell><cell>vohcolabEvSim</cell><cell cols="3">manual 0.057 0.086 0.025</cell><cell>0.062</cell></row><row><cell>CiTIUS</cell><cell>CiTIUSCrdAdh</cell><cell>auto</cell><cell cols="2">0.004 0.059 0.008</cell><cell>0.050</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,126.82,718.82,353.52,9.57"><head>Table 2 :</head><label>2</label><figDesc>Adhoc run results with CAM MAP and Compatibility measures.</figDesc><table coords="10,373.01,73.96,96.90,8.74"><row><cell>nDCG on binary qrels</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,195.09,718.82,216.97,9.57"><head>Table 3 :</head><label>3</label><figDesc>Adhoc run results with binary qrels.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,88.59,690.57,259.90,7.47"><p>https://commoncrawl.org/2016/10/news-dataset-available/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,88.59,701.53,444.98,7.47"><p>https://www.who.int/emergencies/diseases/novel-coronavirus-2019/advice-for-public/myth-busters</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="3,88.59,713.88,224.96,8.12"><p>https://ir.nist.gov/trec-hmi/ (password protected)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3" coords="4,88.59,692.60,274.02,7.47"><p>https://github.com/trec-health-misinfo/Trec_eval_extension</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4" coords="4,88.59,703.56,245.78,7.47"><p>https://github.com/trec-health-misinfo/Compatibility</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5" coords="4,88.59,714.52,335.71,7.47"><p>https://trec-health-misinfo.github.io/docs/AssessingGuidelines-2020.pdf</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgments</head><p>Thanks to <rs type="person">Mustafa Abualsaud</rs>, <rs type="person">Kamyar Ghajar</rs>, <rs type="person">Linh Nhi Phan Minh</rs>, <rs type="person">Amir Vakili Tahami</rs>, <rs type="person">Nicole Yan</rs>, and <rs type="person">Dake Zhang</rs>. This work was supported in part by the <rs type="funder">Natural Sciences and Engineering Research Council of Canada (NSERC)</rs>, the facilities of <rs type="institution">Compute Canada</rs>, the <rs type="funder">University of Waterloo</rs>, and the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 research and innovation programme</rs> under the <rs type="grantName">Marie Sk lodowska-Curie</rs> grant agreement No. <rs type="grantNumber">893667</rs>. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsors.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_N8nWJms">
					<idno type="grant-number">893667</idno>
					<orgName type="grant-name">Marie Sk lodowska-Curie</orgName>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="7,88.97,407.00,451.03,9.57;7,88.97,420.55,451.03,9.57;7,88.97,434.10,73.39,9.57" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,440.42,407.00,99.59,9.57;7,88.97,420.55,87.79,9.57">Overview of the trec 2019 decision track</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Abualsaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maistro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,197.57,420.55,337.65,9.57">The Twenty-Eigth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
	<note>TREC 2019) Proceedings</note>
</biblStruct>

<biblStruct coords="7,88.97,456.61,451.03,9.57;7,88.97,470.16,451.03,9.57;7,88.97,483.71,120.06,9.57" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,309.98,456.61,143.46,9.57">Offline evaluation without gain</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vtyurina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,473.77,456.61,66.24,9.57;7,88.97,470.16,451.03,9.57;7,88.97,483.71,12.73,9.57">Proceedings of the 2020 ACM SIGIR on International Conference on Theory of Information Retrieval, ICTIR &apos;20</title>
		<meeting>the 2020 ACM SIGIR on International Conference on Theory of Information Retrieval, ICTIR &apos;20</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="185" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.97,506.23,451.04,9.57;7,88.97,519.78,451.03,9.57;7,88.97,533.32,288.40,9.57" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,339.74,506.23,200.26,9.57;7,88.97,519.78,88.93,9.57">Offline evaluation by maximum similarity to an ideal ranking</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vtyurina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,201.48,519.78,338.52,9.57;7,88.97,533.32,181.07,9.57">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management, CIKM &apos;20</title>
		<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management, CIKM &apos;20</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="225" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.97,555.84,451.03,9.57;7,88.97,569.39,451.03,9.57;7,88.97,582.94,385.20,9.57" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,296.59,555.84,243.41,9.57;7,88.97,569.39,76.06,9.57">Evaluation Measures for Relevance and Credibility in Ranked Lists</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Simonsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Larsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,192.46,569.39,347.54,9.57;7,88.97,582.94,101.81,9.57">Proceedings of the ACM SIGIR International Conference on Theory of Information Retrieval</title>
		<meeting>the ACM SIGIR International Conference on Theory of Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.97,605.45,451.03,9.57;7,88.97,619.00,451.03,9.57;7,88.97,632.55,451.03,9.57;7,88.97,646.10,155.36,9.57" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,411.39,605.45,128.61,9.57;7,88.97,619.00,431.66,9.57">The Positive and Negative Influence of Search Results on People&apos;s Decisions About the Efficacy of Medical Treatments</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">A</forename><surname>Pogacar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ghenai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,88.97,632.55,451.03,9.57;7,88.97,646.10,48.03,9.57">Proceedings of the ACM SIGIR International Conference on Theory of Information Retrieval, ICTIR &apos;17</title>
		<meeting>the ACM SIGIR International Conference on Theory of Information Retrieval, ICTIR &apos;17</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
