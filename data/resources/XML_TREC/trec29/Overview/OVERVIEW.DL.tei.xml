<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,101.29,99.57,409.45,14.93">OVERVIEW OF THE TREC 2020 DEEP LEARNING TRACK</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,162.73,162.69,57.27,8.64"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
							<email>nickcr@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft AI &amp; Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,228.97,162.69,57.27,8.64"><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
							<email>bmitra@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft AI &amp; Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,300.45,162.69,57.28,8.64"><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
							<email>emine.yilmaz@ucl.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,383.57,162.69,61.71,8.64"><forename type="first">Daniel</forename><surname>Campos</surname></persName>
							<email>dcampos3@illinois.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,101.29,99.57,409.45,14.93">OVERVIEW OF THE TREC 2020 DEEP LEARNING TRACK</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8C4B329FC357189BBB13DFF13B8D8706</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This is the second year of the TREC Deep Learning Track, with the goal of studying ad hoc ranking in the large training data regime. We again have a document retrieval task and a passage retrieval task, each with hundreds of thousands of human-labeled training queries. We evaluate using singleshot TREC-style evaluation, to give us a picture of which ranking methods work best when large data is available, with much more comprehensive relevance labeling on the small number of test queries. This year we have further evidence that rankers with BERT-style pretraining outperform other rankers in the large data regime.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning methods, where a computational model learns an intricate representation of a large-scale dataset, yielded dramatic performance improvements in speech recognition and computer vision <ref type="bibr" coords="1,394.06,403.87,79.09,8.64" target="#b8">[LeCun et al., 2015]</ref>. When we have seen such improvements, a common factor is the availability of large-scale training data <ref type="bibr" coords="1,422.79,414.78,69.38,8.64" target="#b5">[Deng et al., 2009</ref><ref type="bibr" coords="1,492.17,414.78,47.83,8.64;1,72.00,425.69,47.34,8.64" target="#b2">, Bellemare et al., 2013]</ref>. For ad hoc ranking in information retrieval, which is a core problem in the field, we did not initially see dramatic improvements in performance from deep learning methods. This led to questions about whether deep learning methods were helping at all <ref type="bibr" coords="1,222.00,447.51,77.70,8.64">[Yang et al., 2019a]</ref>. If large training data sets are a factor, one explanation for this could be that the training sets were too small.</p><p>The TREC Deep Learning Track, and associated MS MARCO leaderboards <ref type="bibr" coords="1,390.37,474.81,76.43,8.64" target="#b1">[Bajaj et al., 2016]</ref>, have introduced human-labeled training sets that were previously unavailable. The main goal is to study information retrieval in the large training data regime, to see which retrieval methods work best.</p><p>The two tasks, document retrieval and passage retrieval, each have hundreds of thousands of human-labeled training queries. The training labels are sparse, with often only one positive example per query. Unlike the MS MARCO leaderboards, which evaluate using the same kind of sparse labels, the evaluation at TREC uses much more comprehensive relevance labeling. Each year of TREC evaluation evaluates on a new set of test queries, where participants submit before the test labels have even been generated, so the TREC results are the gold standard for avoiding multiple testing and overfitting. However, the comprehensive relevance labeling also generates a reusable test collections, allowing reuse of the dataset in future studies, although people should be careful to avoid overfitting and overiteration.</p><p>The main goals of the Deep Learning Track in 2020 have been: 1) To provide large reusable training datasets with associated large scale click dataset for training deep learning and traditional ranking methods in a large training data regime, 2) To construct reusable test collections for evaluating quality of deep learning and traditional ranking methods, 3) To perform a rigorous blind single-shot evaluation, where test labels don't even exist until after all runs are submitted, to compare different ranking methods, and 4) To study this in both a traditional TREC setup with end-to-end retrieval and in a re-ranking setup that matches how some models may be deployed in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task description</head><p>The track has two tasks: Document retrieval and passage retrieval. Participants were allowed to submit up to three runs per task, although this was not strictly enforced. Submissions to both tasks used the same set of 200 test queries.</p><p>In the pooling and judging process, NIST chose a subset of the queries for judging, based on budget constraints and with the goal of finding a sufficiently comprehensive set of relevance judgments to make the test collection reusable. This led to a judged test set of 45 queries for document retrieval and 54 queries for passage retrieval. The document queries are not a subset of the passage queries.</p><p>When submitting each run, participants indicated what external data, pretrained models and other resources were used, as well as information on what style of model was used. Below we provide more detailed information about the document retrieval and passage retrieval tasks, as well as the datasets provided as part of these tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Document retrieval task</head><p>The first task focuses on document retrieval, with two subtasks: (i) Full retrieval and (ii) top-100 reranking.</p><p>In the full retrieval subtask, the runs are expected to rank documents based on their relevance to the query, where documents can be retrieved from the full document collection provided. This subtask models the end-to-end retrieval scenario.</p><p>In the reranking subtask, participants were provided with an initial ranking of 100 documents, giving all participants the same starting point. This is a common scenario in many real-world retrieval systems that employ a telescoping architecture <ref type="bibr" coords="2,121.42,268.28,86.44,8.64" target="#b9">[Matveeva et al., 2006</ref><ref type="bibr" coords="2,207.86,268.28,77.87,8.64" target="#b17">, Wang et al., 2011]</ref>. The reranking subtask allows participants to focus on learning an effective relevance estimator, without the need for implementing an end-to-end retrieval system. It also makes the reranking runs more comparable, because they all rerank the same set of 100 candidates.</p><p>The initial top-100 rankings were retrieved using Indri <ref type="bibr" coords="2,293.50,306.48,92.97,8.64" target="#b16">[Strohman et al., 2005]</ref> on the full corpus with Krovetz stemming and stopwords eliminated.</p><p>Judgments are on a four-point scale:</p><p>[3] Perfectly relevant: Document is dedicated to the query, it is worthy of being a top result in a search engine.</p><p>[2] Highly relevant: The content of this document provides substantial information on the query.</p><p>[1] Relevant: Document provides some information relevant to the query, which may be minimal.</p><p>[0] Irrelevant: Document does not provide any useful information about the query.</p><p>For metrics that binarize the judgment scale, we map document judgment levels 3,2,1 to relevant and map document judgment level 0 to irrelevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Passage retrieval task</head><p>Similar to the document retrieval task, the passage retrieval task includes (i) a full retrieval and (ii) a top-1000 reranking tasks.</p><p>In the full retrieval subtask, given a query, the participants were expected to retrieve a ranked list of passages from the full collection based on their estimated likelihood of containing an answer to the question. Participants could submit up to 1000 passages per query for this end-to-end retrieval task.</p><p>In the top-1000 reranking subtask, 1000 passages per query were provided to participants, giving all participants the same starting point. The sets of 1000 were generated based on BM25 retrieval with no stemming as applied to the full collection. Participants were expected to rerank the 1000 passages based on their estimated likelihood of containing an answer to the query. In this subtask, we can compare different reranking methods based on the same initial set of 1000 candidates, with the same rationale as described for the document reranking subtask.</p><p>Judgments are on a four-point scale:</p><p>[3] Perfectly relevant: The passage is dedicated to the query and contains the exact answer.</p><p>[2] Highly relevant: The passage has some answer for the query, but the answer may be a bit unclear, or hidden amongst extraneous information.</p><p>[1] Related: The passage seems related to the query but does not answer it.</p><p>[0] Irrelevant: The passage has nothing to do with the query.</p><p>For metrics that binarize the judgment scale, we map passage judgment levels 3,2 to relevant and map document judgment levels 1,0 to irrelevant. In the case of passage retrieval, the positive label indicates that the passage contains an answer to a query. In the case of document retrieval, we transferred the passage-level label to the corresponding source document that contained the passage. We do this under the assumption that a document with a relevant passage is a relevant document, although we note that our document snapshot was generated at a different time from the passage dataset, so there can be some mismatch. Despite this, machine learning models trained with these labels seem to benefit from using the labels, when evaluated using NIST's non-sparse, non-transferred labels. This suggests the transferred document labels are meaningful for our TREC task.</p><p>This year for the document retrieval task, we also release a large scale click dataset, The ORCAS data, constructed from the logs of a major search engine <ref type="bibr" coords="3,225.25,564.12,85.98,8.64" target="#b4">[Craswell et al., 2020]</ref>. The data could be used in a variety of ways, for example as additional training data (almost 50 times larger than the main training set) or as a document field in addition to title, URL and body text fields available in the original training data.</p><p>For each task there is a corresponding MS MARCO leaderboard, using the same corpus and sparse training data, but using sparse data for evaluation as well, instead of the NIST test sets. We analyze the agreement between the two types of test in Section 4.</p><p>Table <ref type="table" coords="3,96.71,640.54,4.98,8.64" target="#tab_0">1</ref> and Table <ref type="table" coords="3,146.43,640.54,4.98,8.64">2</ref> provide descriptive statistics for the dataset derived from MS MARCO and the ORCAS dataset, respectively. More details about the datasets-including directions for download-is available on the TREC 2020 Deep Learning Track website<ref type="foot" coords="3,189.79,660.69,3.49,6.05" target="#foot_0">1</ref> . Interested readers are also encouraged to refer to <ref type="bibr" coords="3,395.81,662.36,74.44,8.64" target="#b1">[Bajaj et al., 2016]</ref> for details on the original MS MARCO dataset.  Figure <ref type="figure" coords="4,101.25,359.97,3.88,8.64">1</ref>: NDCG@10 results, broken down by run type. Runs of type "nnlm", meaning they use language models such as BERT, performed best on both tasks. Other neural network models "nn" and non-neural models "trad" had relatively lower performance this year. More iterations of evaluation and analysis would be needed to determine if this is a general result, but it is a strong start for the argument that deep learning methods may take over from traditional methods in IR applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and analysis</head><p>Submitted runs The TREC 2020 Deep Learning Track had 25 participating groups, with a total of 123 runs submitted across both tasks.</p><p>Based run submission surveys, we manually classify each run into one of three categories:</p><p>• nnlm: if the run employs large scale pre-trained neural language models, such as BERT <ref type="bibr" coords="4,459.95,509.96,80.05,8.64" target="#b6">[Devlin et al., 2018]</ref> or XLNet <ref type="bibr" coords="4,148.81,520.87,78.96,8.64">[Yang et al., 2019b]</ref> • nn: if the run employs some form of neural network based approach-e.g., <ref type="bibr" coords="4,404.67,536.88,135.33,8.64">Duet [Mitra et al., 2017, Mitra and</ref><ref type="bibr" coords="4,107.87,547.79,63.64,8.64">Craswell, 2019]</ref> or using word embeddings <ref type="bibr" coords="4,282.37,547.79,79.93,8.64" target="#b7">[Joulin et al., 2016]</ref>-but does not fall into the "nnlm" category</p><p>• trad: if the run exclusively uses traditional IR methods like BM25 <ref type="bibr" coords="4,373.29,563.79,93.59,8.64" target="#b14">[Robertson et al., 2009]</ref> and RM3 <ref type="bibr" coords="4,508.45,563.79,31.54,8.64;4,107.87,574.70,70.97,8.64" target="#b0">[Abdul-Jaleel et al., 2004]</ref>.</p><p>We placed 70 (57%) runs in the "nnlm" category, 13 (10%) in the "nn" category, and the remaining 40 (33%) in the "trad" category. In 2019, 33 (44%) runs were in the "nnlm" category, 20 (27%) in the "nn" category, and the remaining 22 (29%) in the "trad" category. While there was a significant increase in the total number of runs submitted compared to last year, we observed a significant reduction in the fraction of runs in the "nn" category.</p><p>We further categorize runs based on subtask:</p><p>• rerank: if the run reranks the provided top-k candidates, or</p><p>• fullrank: if the run employs their own phase 1 retrieval system.</p><p>We find that only 37 (30%) submissions fall under the "rerank" category-while the remaining 86 (70%) are "fullrank". Table <ref type="table" coords="4,124.08,713.51,4.98,8.64" target="#tab_1">3</ref> breaks down the submissions by category and task.</p><p>Overall results Our main metric in both tasks is Normalized Discounted Cumulative Gain (NDCG)-specifically, NDCG@10, since it makes use of our 4-level judgments and focuses on the first results that users will see. To get a picture of the ranking quality outside the top-10 we also report Average Precision (AP), although this binarizes the judgments. For comparison to the MS MARCO leaderboard, which often only has one relevant judgment per query, we report the Reciprocal Rank (RR) of the first relevant document on the NIST judgments, and also using the sparse leaderboard judgments.</p><p>Some of our evaluation is concerned with the quality of the top-k results, where k = 100 for the document task and k = 1000 for the passage task. We want to consider the quality of the top-k set without considering how they are ranked, so we can see whether improving the set-based quality is correlated with an improvement in NDCG@10. Although we could use Recall@k as a metric here, it binarizes the judgments, so we instead use Normalized Cumulative Gain (NCG@k) <ref type="bibr" coords="5,117.77,190.05,81.00,8.64" target="#b15">[Rosset et al., 2018]</ref>. NCG is not supported in trec_eval. For trec_eval metrics that are correlated, see Recall@k and NDCG@k.</p><p>The overall results are presented in Table <ref type="table" coords="5,244.95,217.35,4.98,8.64" target="#tab_2">4</ref> for document retrieval and Table <ref type="table" coords="5,390.47,217.35,4.98,8.64" target="#tab_3">5</ref> for passage retrieval. These tables include multiple metrics and run categories, which we now use in our analysis.</p><p>Neural vs. traditional methods. The first question we investigated as part of the track is which ranking methods work best in the large-data regime. We summarize NDCG@10 results by run type in Figure <ref type="figure" coords="5,440.25,262.69,3.74,8.64">1</ref>.</p><p>For document retrieval runs (Figure <ref type="figure" coords="5,223.32,279.07,8.48,8.64">1a</ref>) the best "trad" run is outperformed by "nn" and "nnlm" runs by several percentage points, with "nnlm" also having an advantage over "nn". We saw a similar pattern in our 2019 results. This year we encouraged submission of a variety of "trad" runs from different participating groups, to give "trad" more chances to outperform other run types. The best performing run of each category is indicated, with the best "nnlm" and "nn" models outperforming the best "trad" model by 23% and 11% respectively.</p><p>For passage retrieval runs (Figure <ref type="figure" coords="5,205.84,339.10,8.85,8.64">1b</ref>) the gap between the best "nnlm" and "nn" runs and the best "trad" run is larger, at 42% and 17% respectively. One explanation for this could be that vocabulary mismatch between queries and relevant results is greater in short text, so neural methods that can overcome such mismatch have a relatively greater advantage in passage retrieval. Another explanation could be that there is already a public leaderboard, albeit without test labels from NIST, for the passage task. (We did not launch the document ranking leaderboard until after our 2020 TREC submission deadline.) In passage ranking, some TREC participants may have submitted neural models multiple times to the public leaderboard, so are relatively more experienced working with the passage dataset than the document dataset.</p><p>In query-level win-loss analysis for the document retrieval task (Figure <ref type="figure" coords="5,355.83,431.85,4.15,8.64">2</ref>) the best "nnlm" model outperforms the best "trad" run on 38 out of the 45 test queries (i.e., 84%). Passage retrieval shows a similar pattern in Figure <ref type="figure" coords="5,489.87,442.76,3.74,8.64">3</ref>. Similar to last year's data, neither task has a large class of queries where the "nnlm" model performs worse.</p><p>End-to-end retrieval vs. reranking. Our datasets include top-k candidate result lists, with 100 candidates per query for document retrieval and 1000 candidates per query for passage retrieval. Runs that simply rerank the provided candidates are "rerank" runs, whereas runs that perform end-to-end retrieval against the corpus, with millions of potential results, are "fullrank" runs. We would expect that a "fullrank" run should be able to find a greater number of relevant candidates than we provided, achieving higher NCG@k. A multi-stage "fullrank" run should also be able to optimize the stages jointly, such that early stages produce candidates that later stages are good at handling.</p><p>According to Figure <ref type="figure" coords="5,156.79,548.12,3.74,8.64" target="#fig_2">4</ref>, "fullrank" did not achieve much better NDCG@10 performance than "rerank" runs. In fact, for the passage retrieval task, the top two runs are of type "rerank". While it was possible for "fullrank" to achieve better NCG@k, it was also possible to make NCG@k worse, and achieving significantly higher NCG@k does not seem necessary to achieve good NDCG@10.</p><p>Specifically, for the document retrieval task, the best "fullrank" run achieves 5% higher NDCG@10 over the best "rerank' run; whereas for the passage retrieval task, the best "fullrank" run performs slightly worse (0.3% lower NDCG@10) compared to the best "rerank' run.</p><p>Similar to our observations from Deep Learning Track 2019, we are not yet seeing a strong advantage of "fullrank" over "rerank". However, we hope that as the body of literature on neural methods for phase 1 retrieval (e.g., <ref type="bibr" coords="5,503.71,646.36,36.28,8.64;5,72.00,657.27,43.21,8.64" target="#b3">[Boytsov et al., 2016</ref><ref type="bibr" coords="5,115.21,657.27,84.04,8.64" target="#b20">, Zamani et al., 2018</ref><ref type="bibr" coords="5,199.25,657.27,75.74,8.64">, Mitra et al., 2019</ref><ref type="bibr" coords="5,274.99,657.27,94.88,8.64" target="#b13">, Nogueira et al., 2019]</ref>) grows, we would see a larger number of runs with deep learning as an ingredient for phase 1 in future editions of this TREC track.</p><p>Effect of ORCAS data Based on the descriptions provided, ORCAS data seems to have been used by six of the runs (ndrm3-orc-full, ndrm3-orc-re, uogTrBaseL17, uogTrBaseQL17o, uogTr31oR, relemb_mlm_0_2). Most runs seem to be make use of the ORCAS data as a field, with some runs using the data as an additional training dataset as well.  what is chronometer who invented it where is the show shameless filmed how long does it take to remove wisdom tooth when did family feud come out? average annual income data analyst what medium do radio waves travel through how much money do motivational speakers make average wedding dress alteration cost average salary for dental hygienist in nebraska nnlm trad Figure 2: Comparison of the best "nnlm" and "trad" runs on individual test queries for the document retrieval task. Queries are sorted by difference in mean performance between "nnlm" and "trad" runs. Queries on which "nnlm" wins with large margin are at the top. what carvedilol used for where is the show shameless filmed does mississippi have an income tax who is aziz hashim difference between a company's strategy and business model is define: geon why is pete rose banned from hall of fame why do hunters pattern their shotguns? can fever cause miscarriage early pregnancy nnlm trad Figure <ref type="figure" coords="9,101.54,708.13,3.88,8.64">3</ref>: Comparison of the best "nnlm" and "trad" runs on individual test queries for the passage retrieval task. Queries are sorted by difference in mean performance between "nnlm" and "trad" runs. Queries on which "nnlm" wins with large margin are at the top.  the performance of different runs on the document and passage retrieval tasks, respectively. Figure <ref type="figure" coords="10,465.69,408.28,7.37,8.64">(c</ref>) and (d) plot the NCG@100 and NCG@1000 metrics for the same runs for the two tasks, respectively. The runs are ordered by their NDCG@10 performance along the x-axis in all four plots. We observe, that the best run under the "fullrank" setting outperforms the same under the "rerank" setting for both document and passage retrieval tasks-although the gaps are relatively smaller compared to those in Figure <ref type="figure" coords="10,257.88,451.91,3.74,8.64">1</ref> Most runs used the ORCAS data for the document retrieval task, with relemb_mlm_0_2 being the only run using the ORCAS data for the passage retrieval task. This year it was not necessary to use ORCAS data to achieve the highest NDCG@10. However, when we compare the performance of the runs that use the ORCAS dataset with those that do not use the dataset within the same group, we observe that usage of the ORCAS dataset always led to an improved performance in terms of NDCG@10, with maximum increase being around 0.0513 in terms of NDCG@10. This suggests that the ORCAS dataset is providing additional information that is not available in the training data. This could also imply that even though the training dataset provided as part of the track is very large, deep models are still in need of more training data.</p><p>NIST labels vs. Sparse MS MARCO labels. Our baseline human labels from MS MARCO often have one known positive result per query. We use these labels for training, but they are also available for test queries. Although our official evaluation uses NDCG@10 with NIST labels, we now compare this with reciprocal rank (RR) using MS MARCO labels. Our goal is to understand how changing the labeling scheme and metric affects the overall results of the track, but if there is any disagreement we believe the NDCG results are more valid, since they evaluate the ranking more comprehensively and a ranker that can only perform well on labels with exactly the same distribution as the training set is not robust enough for use in real-world applications, where real users will have opinions that are not necessarily identical to the preferences encoded in sparse training labels.</p><p>Figure <ref type="figure" coords="10,100.50,702.61,4.98,8.64" target="#fig_4">5</ref> shows the agreement between the results using MS MARCO and NIST labels for the document retrieval and passage retrieval tasks. While the agreement between the evaluation setup based on MS MARCO and TREC seems  reasonable for both tasks, agreements for the document ranking task seems to be lower (Kendall correlation of 0.46) than agreements for the passage task (Kendall correlation of 0.69). This value is also lower than the correlation we observed for the document retrieval task for last year.</p><p>In Table <ref type="table" coords="11,107.14,528.01,4.98,8.64" target="#tab_5">6</ref> we show how the agreement between the two evaluation setups varies across task and run type. Agreement on which are the best neural network runs is high, but correlation for document trad runs is close to zero.</p><p>One explanation for this low correlation could be use of the ORCAS dataset. ORCAS was mainly used in the document retrieval task, and could bring search results more in line with Bing's results, since Bing's results are what may be clicked. Since MS MARCO sparse labels were also generated based on top results from Bing, we would expect to see some correlation between ORCAS runs and MS MARCO labels (and Bing results). By contrast, NIST judges had no information about what results were retrieved or clicked in Bing, so may have somewhat less correlation with Bing's results and users.</p><p>In Figure <ref type="figure" coords="11,112.14,626.24,4.98,8.64">6</ref> we compare the results from the two evaluation setups when the runs are split based on the usage of the ORCAS dataset. Our results suggest that runs that use the ORCAS dataset did perform somewhat better based on the MS MARCO evaluation setup. While the similarities between the ORCAS dataset and the MS MARCO labels seem to be one reason for the mismatch between the two evaluation results, it is not enough to fully explain the 0.03 correlation in Table6. Removing the ORCAS "trad" runs only increases the correlation to 0.13. In the future we plan to further analyze the possible reasons for this poor correlation, which could also be related to 1) the different metrics used in the two evaluation setups (RR vs. NDCG@10), 2) the different sensitivity of the datasets due to the different number of queries and number of documents labelled per query), or 3) difference in relevance labels provided by NIST assessors vs. labels derived from clicks. Figure <ref type="figure" coords="12,101.23,278.79,3.88,8.64">6</ref>: This year it was not necessary to use ORCAS data to achieve the highest NDCG@10. ORCAS runs did somewhat better on the leaderboard metric RR (MS), which uses different labels from the other metrics. This may indicate an alignment between the Bing user clicks in ORCAS with the labeled MS MARCO results, which were also generated by Bing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>The TREC 2020 Deep Learning Track has provided two large training datasets, for a document retrieval task and a passage retrieval task, generating two ad hoc test collections with good reusability. The main document and passage training datasets in 2020 were the same as those in 2019. In addition, as part of the 2020 track, we have also released a large click dataset, the ORCAS dataset, which was generated using the logs of the Bing search engine.</p><p>For both tasks, in the presence of large training data, this year's non-neural network runs were outperformed by neural network runs. While usage of the ORCAS dataset seems to help improve the performance of the systems, it was not necessary to use ORCAS data to achieve the highest NDCG@10.</p><p>We compared reranking approaches to end-to-end retrieval approaches, and in this year's track there was not a huge difference, with some runs performing well in both regimes. This is another result that would be interesting to track in future years, since we would expect that end-to-end retrieval should perform better if it can recall documents that are unavailable in a reranking subtask.</p><p>This year the number of runs submitted for both tasks have increased compared to last year. In particular, number of non-neural runs have increased. Hence, test collections generated as part of this year's track may be more reusable compared to last year since these test collections may be fairer towards evaluating the quality of unseen non-neural runs. We note that the number of "nn" runs also seems to be smaller this year. We will continue to encourage a variety of approaches in submission, to avoid converging too quickly on one type of run, and to diversify the judging pools.</p><p>Similar to last year, in this year's track we have two types of evaluation label for each task. Our official labels are more comprehensive, covering a large number of results per query, and labeled on a four point scale at NIST. We compare this to the MS MARCO labels, which usually only have one positive result per query. While there was a strong correlation between the evaluation results obtained using the two datasets for the passage retrieval task, the correlation for the document retrieval task was lower. Part of this low correlation seems to be related to the usage of the ORCAS dataset (which is generated using similar dataset as the one used to generate the MS MARCO labels) by some runs, and evaluation results based on MS MARCO data favoring these runs. However, our results suggest that while the ORCAS dataset could be one reason for the low correlation, there might be other reasons causing this reduced correlation, which we plan to explore as future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="10,80.42,204.35,6.41,7.63;10,80.42,177.86,6.41,7.63;10,80.42,151.37,6.41,7.63;10,80.42,124.87,6.41,7.63;10,80.42,98.38,6.41,7.63;10,80.42,71.88,6.41,7.63;10,71.88,132.06,7.63,21.23;10,107.22,82.46,33.31,7.63;10,116.25,96.74,31.23,7.63;10,279.54,77.05,15.24,7.63;10,279.54,82.96,13.16,7.63;10,88.88,221.29,195.57,7.77;10,319.10,204.35,6.41,7.63;10,319.10,177.86,6.41,7.63;10,319.10,151.37,6.41,7.63;10,319.10,124.87,6.41,7.63;10,319.10,98.38,6.41,7.63;10,319.10,71.88,6.41,7.63;10,310.56,132.06,7.63,21.23;10,352.44,80.64,33.31,7.63;10,345.89,86.35,31.23,7.63;10,518.22,77.05,15.24,7.63;10,518.22,82.96,13.16,7.63;10,331.05,221.29,188.59,7.77;10,80.42,363.48,8.98,7.63;10,80.42,346.92,8.98,7.63;10,80.42,330.36,8.98,7.63;10,80.42,313.80,8.98,7.63;10,80.42,297.24,8.98,7.63;10,80.42,280.68,8.98,7.63;10,80.42,264.12,8.98,7.63;10,80.42,247.56,8.98,7.63;10,80.42,231.00,8.98,7.63;10,71.88,291.45,7.63,20.69;10,279.55,236.16,15.24,7.63;10,279.55,242.08,13.16,7.63;10,89.88,380.42,193.57,7.77;10,319.10,363.47,6.41,7.63;10,319.10,336.98,6.41,7.63;10,319.10,310.49,6.41,7.63;10,319.10,283.99,6.41,7.63;10,319.10,257.50,6.41,7.63;10,319.10,231.00,6.41,7.63;10,310.56,290.16,7.63,23.25;10,518.22,236.17,15.24,7.63;10,518.22,242.08,13.16,7.63;10,329.80,380.41,191.08,7.77"><head></head><label></label><figDesc>NCG@1000 for runs on the passage retrieval task</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="10,72.00,397.19,468.00,8.82;10,72.00,408.28,468.00,8.64;10,72.00,419.19,468.00,8.64;10,72.00,429.78,468.00,8.96;10,72.00,441.00,468.00,8.64;10,72.00,451.91,468.00,8.64;10,72.00,462.82,361.66,8.64"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Analyzing the impact of "fullrank" vs. "rerank" settings on retrieval performance. Figure (a) and (b) show the performance of different runs on the document and passage retrieval tasks, respectively. Figure(c) and (d) plot the NCG@100 and NCG@1000 metrics for the same runs for the two tasks, respectively. The runs are ordered by their NDCG@10 performance along the x-axis in all four plots. We observe, that the best run under the "fullrank" setting outperforms the same under the "rerank" setting for both document and passage retrieval tasks-although the gaps are relatively smaller compared to those in Figure1. If we compare Figure (a) with (c) and Figure (b) with (d), we do not observe any evidence that the NCG metric is a good predictor of NDCG@10 performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="10,72.00,397.19,468.00,8.82;10,72.00,408.28,468.00,8.64;10,72.00,419.19,468.00,8.64;10,72.00,429.78,468.00,8.96;10,72.00,441.00,468.00,8.64;10,72.00,451.91,468.00,8.64;10,72.00,462.82,361.66,8.64"><head></head><label></label><figDesc>Figure4: Analyzing the impact of "fullrank" vs. "rerank" settings on retrieval performance. Figure (a) and (b) show the performance of different runs on the document and passage retrieval tasks, respectively. Figure(c) and (d) plot the NCG@100 and NCG@1000 metrics for the same runs for the two tasks, respectively. The runs are ordered by their NDCG@10 performance along the x-axis in all four plots. We observe, that the best run under the "fullrank" setting outperforms the same under the "rerank" setting for both document and passage retrieval tasks-although the gaps are relatively smaller compared to those in Figure1. If we compare Figure (a) with (c) and Figure (b) with (d), we do not observe any evidence that the NCG metric is a good predictor of NDCG@10 performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="11,72.00,430.27,468.00,8.64;11,72.00,440.86,468.00,8.96;11,72.00,451.77,387.14,8.96"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Leaderboard metrics agreement analysis. For document runs, the agreement between the leaderboard metric RR (MS) and the main TREC metric NDCG@10 is lower this year. The Kendall correlation is τ = 0.46, compared to τ = 0.69 in 2019. For the passage task, we see τ = 0.69 in 2020, compared to τ = 0.68 in 2019.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,72.00,80.12,468.00,383.50"><head>Table 1 :</head><label>1</label><figDesc>Summary of statistics on TREC 2020 Deep Learning Track datasets.</figDesc><table coords="3,72.00,94.62,468.00,369.00"><row><cell></cell><cell>Document task</cell><cell cols="2">Passage task</cell></row><row><cell>Data</cell><cell cols="3">Number of records Number of records</cell></row><row><cell>Corpus</cell><cell>3, 213, 835</cell><cell></cell><cell>8, 841, 823</cell></row><row><cell>Train queries</cell><cell>367, 013</cell><cell></cell><cell>502, 939</cell></row><row><cell>Train qrels</cell><cell>384, 597</cell><cell></cell><cell>532, 761</cell></row><row><cell>Dev queries</cell><cell>5, 193</cell><cell></cell><cell>6, 980</cell></row><row><cell>Dev qrels</cell><cell>5, 478</cell><cell></cell><cell>7, 437</cell></row><row><cell>2019 TREC queries</cell><cell>200 → 43</cell><cell></cell><cell>200 → 43</cell></row><row><cell>2019 TREC qrels</cell><cell>16, 258</cell><cell></cell><cell>9, 260</cell></row><row><cell>2020 TREC queries</cell><cell>200 → 45</cell><cell></cell><cell>200 → 54</cell></row><row><cell>2020 TREC qrels</cell><cell>9, 098</cell><cell></cell><cell>11, 386</cell></row><row><cell cols="4">Table 2: Summary of ORCAS data. Each record in the main file (orcas.tsv) indicates a click between a query (Q)</cell></row><row><cell cols="4">and a URL (U), also listing a query ID (QID) and the corresponding TREC document ID (DID). The run file is the</cell></row><row><cell cols="4">top-100 using Indri query likelihood, for use as negative samples during training.</cell></row><row><cell>Filename</cell><cell cols="3">Number of records Data in each record</cell></row><row><cell>orcas.tsv</cell><cell></cell><cell>18.8M</cell><cell>QID Q DID U</cell></row><row><cell cols="2">orcas-doctrain-qrels.tsv</cell><cell>18.8M</cell><cell>QID DID</cell></row><row><cell cols="2">orcas-doctrain-queries.tsv</cell><cell>10.4M</cell><cell>QID Q</cell></row><row><cell>orcas-doctrain-top100</cell><cell></cell><cell>983M</cell><cell>QID DID score</cell></row><row><cell>3 Datasets</cell><cell></cell><cell></cell></row><row><cell cols="4">Both tasks have large training sets based on human relevance assessments, derived from MS MARCO. These are</cell></row><row><cell cols="4">sparse, with no negative labels and often only one positive label per query, analogous to some real-world training data</cell></row><row><cell>such as click logs.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,70.34,80.12,445.79,253.90"><head>Table 3 :</head><label>3</label><figDesc>Summary of statistics of runs for the two retrieval tasks at the TREC 2020 Deep Learning Track.</figDesc><table coords="4,70.34,93.04,398.14,240.98"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Document retrieval Passage retrieval</cell></row><row><cell></cell><cell></cell><cell cols="2">Number of groups</cell><cell>14</cell><cell>14</cell></row><row><cell></cell><cell></cell><cell cols="2">Number of total runs</cell><cell>64</cell><cell>59</cell></row><row><cell></cell><cell></cell><cell cols="2">Number of runs w/ category: nnlm</cell><cell>27</cell><cell>43</cell></row><row><cell></cell><cell></cell><cell cols="2">Number of runs w/ category: nn</cell><cell>11</cell><cell>2</cell></row><row><cell></cell><cell></cell><cell cols="2">Number of runs w/ category: trad</cell><cell>26</cell><cell>14</cell></row><row><cell></cell><cell></cell><cell cols="2">Number of runs w/ category: rerank</cell><cell>19</cell><cell>18</cell></row><row><cell></cell><cell></cell><cell cols="2">Number of runs w/ category: fullrank</cell><cell>45</cell><cell>41</cell></row><row><cell></cell><cell>0.8 0.9</cell><cell></cell><cell>nnlm nn trad</cell></row><row><cell>NDCG@10</cell><cell>0.6 0.7</cell><cell>best nnlm run best nn run</cell><cell>best trad run</cell></row><row><cell></cell><cell>0.5</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.4</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,72.00,92.77,468.00,611.10"><head>Table 4 :</head><label>4</label><figDesc>Document retrieval runs. RR (MS) is based on MS MARCO labels. All other metrics are based on NIST labels. Rows are sorted by NDCG@10.</figDesc><table coords="6,113.75,118.11,382.51,585.76"><row><cell>run</cell><cell>group</cell><cell>subtask</cell><cell cols="2">neural RR (MS)</cell><cell cols="3">RR NDCG@10 NCG@100</cell><cell>AP</cell></row><row><cell>d_d2q_duo</cell><cell>h2oloo</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.4451 0.9476</cell><cell>0.6934</cell><cell>0.7718 0.5422</cell></row><row><cell>d_d2q_rm3_duo</cell><cell>h2oloo</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.4541 0.9476</cell><cell>0.6900</cell><cell>0.7769 0.5427</cell></row><row><cell>d_rm3_duo</cell><cell>h2oloo</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.4547 0.9476</cell><cell>0.6794</cell><cell>0.7498 0.5270</cell></row><row><cell>ICIP_run1</cell><cell>ICIP</cell><cell>rerank</cell><cell>nnlm</cell><cell cols="2">0.3898 0.9630</cell><cell>0.6623</cell><cell>0.6283 0.4333</cell></row><row><cell>ICIP_run3</cell><cell>ICIP</cell><cell>rerank</cell><cell>nnlm</cell><cell cols="2">0.4479 0.9667</cell><cell>0.6528</cell><cell>0.6283 0.4360</cell></row><row><cell>fr_doc_roberta</cell><cell>BITEM</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.3943 0.9365</cell><cell>0.6404</cell><cell>0.6806 0.4423</cell></row><row><cell>ICIP_run2</cell><cell>ICIP</cell><cell>rerank</cell><cell>nnlm</cell><cell cols="2">0.4081 0.9407</cell><cell>0.6322</cell><cell>0.6283 0.4206</cell></row><row><cell>roberta-large</cell><cell>BITEM</cell><cell>rerank</cell><cell>nnlm</cell><cell cols="2">0.3782 0.9185</cell><cell>0.6295</cell><cell>0.6283 0.4199</cell></row><row><cell>bcai_bertb_docv</cell><cell>bcai</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.4102 0.9259</cell><cell>0.6278</cell><cell>0.6604 0.4308</cell></row><row><cell>ndrm3-orc-full</cell><cell>MSAI</cell><cell cols="2">fullrank nn</cell><cell cols="2">0.4369 0.9444</cell><cell>0.6249</cell><cell>0.6764 0.4280</cell></row><row><cell>ndrm3-orc-re</cell><cell>MSAI</cell><cell>rerank</cell><cell>nn</cell><cell cols="2">0.4451 0.9241</cell><cell>0.6217</cell><cell>0.6283 0.4194</cell></row><row><cell>ndrm3-full</cell><cell>MSAI</cell><cell cols="2">fullrank nn</cell><cell cols="2">0.4213 0.9333</cell><cell>0.6162</cell><cell>0.6626 0.4069</cell></row><row><cell>ndrm3-re</cell><cell>MSAI</cell><cell>rerank</cell><cell>nn</cell><cell cols="2">0.4258 0.9333</cell><cell>0.6162</cell><cell>0.6283 0.4122</cell></row><row><cell>ndrm1-re</cell><cell>MSAI</cell><cell>rerank</cell><cell>nn</cell><cell cols="2">0.4427 0.9333</cell><cell>0.6161</cell><cell>0.6283 0.4150</cell></row><row><cell>mpii_run2</cell><cell>mpii</cell><cell>rerank</cell><cell>nnlm</cell><cell cols="2">0.3228 0.8833</cell><cell>0.6135</cell><cell>0.6283 0.4205</cell></row><row><cell>bigIR-DTH-T5-R</cell><cell>QU</cell><cell>rerank</cell><cell>nnlm</cell><cell cols="2">0.3235 0.9119</cell><cell>0.6031</cell><cell>0.6283 0.3936</cell></row><row><cell>mpii_run1</cell><cell>mpii</cell><cell>rerank</cell><cell>nnlm</cell><cell cols="2">0.3503 0.9000</cell><cell>0.6017</cell><cell>0.6283 0.4030</cell></row><row><cell>ndrm1-full</cell><cell>MSAI</cell><cell cols="2">fullrank nn</cell><cell cols="2">0.4350 0.9333</cell><cell>0.5991</cell><cell>0.6280 0.3858</cell></row><row><cell>uob_runid3</cell><cell>UoB</cell><cell>rerank</cell><cell>nnlm</cell><cell cols="2">0.3294 0.9259</cell><cell>0.5949</cell><cell>0.6283 0.3948</cell></row><row><cell>bigIR-DTH-T5-F</cell><cell>QU</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.3184 0.8916</cell><cell>0.5907</cell><cell>0.6669 0.4259</cell></row><row><cell>d_d2q_bm25</cell><cell>anserini</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.3338 0.9369</cell><cell>0.5885</cell><cell>0.6752 0.4230</cell></row><row><cell>TUW-TKL-2k</cell><cell cols="2">TU_Vienna rerank</cell><cell>nn</cell><cell cols="2">0.3683 0.9296</cell><cell>0.5852</cell><cell>0.6283 0.3810</cell></row><row><cell>bigIR-DH-T5-R</cell><cell>QU</cell><cell>rerank</cell><cell>nnlm</cell><cell cols="2">0.2877 0.8889</cell><cell>0.5846</cell><cell>0.6283 0.3842</cell></row><row><cell>uob_runid2</cell><cell>UoB</cell><cell>rerank</cell><cell>nnlm</cell><cell cols="2">0.3534 0.9100</cell><cell>0.5830</cell><cell>0.6283 0.3976</cell></row><row><cell>uogTrQCBMP</cell><cell>UoGTr</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.3521 0.8722</cell><cell>0.5791</cell><cell>0.6034 0.3752</cell></row><row><cell>uob_runid1</cell><cell>UoB</cell><cell>rerank</cell><cell>nnlm</cell><cell cols="2">0.3124 0.8852</cell><cell>0.5781</cell><cell>0.6283 0.3786</cell></row><row><cell>TUW-TKL-4k</cell><cell cols="2">TU_Vienna rerank</cell><cell>nn</cell><cell cols="2">0.4097 0.9185</cell><cell>0.5749</cell><cell>0.6283 0.3749</cell></row><row><cell>bigIR-DH-T5-F</cell><cell>QU</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.2704 0.8902</cell><cell>0.5734</cell><cell>0.6669 0.4177</cell></row><row><cell>bl_bcai_multfld</cell><cell>bl_bcai</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.2622 0.9195</cell><cell>0.5629</cell><cell>0.6299 0.3829</cell></row><row><cell>indri-sdmf</cell><cell>RMIT</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.3431 0.8796</cell><cell>0.5597</cell><cell>0.6908 0.3974</cell></row><row><cell>bcai_classic</cell><cell>bcai</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.3082 0.8648</cell><cell>0.5557</cell><cell>0.6420 0.3906</cell></row><row><cell>longformer_1</cell><cell>USI</cell><cell>rerank</cell><cell>nnlm</cell><cell cols="2">0.3614 0.8889</cell><cell>0.5520</cell><cell>0.6283 0.3503</cell></row><row><cell>uogTr31oR</cell><cell>UoGTr</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.3257 0.8926</cell><cell>0.5476</cell><cell>0.5496 0.3468</cell></row><row><cell>rterrier-expC2</cell><cell>bl_rmit</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.3122 0.8259</cell><cell>0.5475</cell><cell>0.6442 0.3805</cell></row><row><cell>bigIR-DT-T5-R</cell><cell>QU</cell><cell>rerank</cell><cell>nnlm</cell><cell cols="2">0.2293 0.9407</cell><cell>0.5455</cell><cell>0.6283 0.3373</cell></row><row><cell>uogTrT20</cell><cell>UoGTr</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.3787 0.8711</cell><cell>0.5453</cell><cell>0.5354 0.3692</cell></row><row><cell>RMIT_DFRee</cell><cell>RMIT</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.2984 0.8756</cell><cell>0.5431</cell><cell>0.6979 0.4087</cell></row><row><cell>rmit_indri-fdm</cell><cell>bl_rmit</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.2779 0.8481</cell><cell>0.5416</cell><cell>0.6812 0.3859</cell></row><row><cell>d_d2q_bm25rm3</cell><cell>anserini</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.2314 0.8147</cell><cell>0.5407</cell><cell>0.6831 0.4228</cell></row><row><cell>rindri-bm25</cell><cell>bl_rmit</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.3302 0.8572</cell><cell>0.5394</cell><cell>0.6503 0.3773</cell></row><row><cell>bigIR-DT-T5-F</cell><cell>QU</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.2349 0.9060</cell><cell>0.5390</cell><cell>0.6669 0.3619</cell></row><row><cell>bl_bcai_model1</cell><cell>bl_bcai</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.2901 0.8358</cell><cell>0.5378</cell><cell>0.6390 0.3774</cell></row><row><cell>bl_bcai_prox</cell><cell>bl_bcai</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.2763 0.8164</cell><cell>0.5364</cell><cell>0.6405 0.3766</cell></row><row><cell>terrier-jskls</cell><cell>bl_rmit</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.3190 0.8204</cell><cell>0.5342</cell><cell>0.6761 0.4008</cell></row><row><cell>rmit_indri-sdm</cell><cell>bl_rmit</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.2702 0.8470</cell><cell>0.5328</cell><cell>0.6733 0.3780</cell></row><row><cell>rterrier-tfidf</cell><cell>bl_rmit</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.2869 0.8241</cell><cell>0.5317</cell><cell>0.6410 0.3734</cell></row><row><cell>BIT-run2</cell><cell>BIT.UA</cell><cell cols="2">fullrank nn</cell><cell cols="2">0.2687 0.8611</cell><cell>0.5283</cell><cell>0.6061 0.3466</cell></row><row><cell>RMIT_DPH</cell><cell>RMIT</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.3117 0.8278</cell><cell>0.5280</cell><cell>0.6531 0.3879</cell></row><row><cell>d_bm25</cell><cell>anserini</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.2814 0.8521</cell><cell>0.5271</cell><cell>0.6453 0.3791</cell></row><row><cell>d_bm25rm3</cell><cell>anserini</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.2645 0.8541</cell><cell>0.5248</cell><cell>0.6632 0.4006</cell></row><row><cell>BIT-run1</cell><cell>BIT.UA</cell><cell cols="2">fullrank nn</cell><cell cols="2">0.3045 0.8389</cell><cell>0.5239</cell><cell>0.6061 0.3466</cell></row><row><cell>rterrier-dph</cell><cell>bl_rmit</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.3033 0.8267</cell><cell>0.5226</cell><cell>0.6634 0.3884</cell></row><row><cell>rterrier-tfidf2</cell><cell>bl_rmit</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.3010 0.8407</cell><cell>0.5219</cell><cell>0.6287 0.3607</cell></row><row><cell cols="2">uogTrBaseQL17o bl_uogTr</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.4233 0.8276</cell><cell>0.5203</cell><cell>0.6028 0.3529</cell></row><row><cell>uogTrBaseL17o</cell><cell>bl_uogTr</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.3870 0.7980</cell><cell>0.5120</cell><cell>0.5501 0.3248</cell></row><row><cell>rterrier-dph_sd</cell><cell>bl_rmit</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.3243 0.8296</cell><cell>0.5110</cell><cell>0.6650 0.3784</cell></row><row><cell>BIT-run3</cell><cell>BIT.UA</cell><cell cols="2">fullrank nn</cell><cell cols="2">0.2696 0.8296</cell><cell>0.5063</cell><cell>0.6072 0.3267</cell></row><row><cell>uogTrBaseDPHQ</cell><cell>bl_uogTr</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.3459 0.8052</cell><cell>0.5052</cell><cell>0.6041 0.3461</cell></row><row><cell>uogTrBaseQL16</cell><cell>bl_uogTr</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.3321 0.7930</cell><cell>0.4998</cell><cell>0.6030 0.3436</cell></row><row><cell>uogTrBaseL16</cell><cell>bl_uogTr</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.3062 0.8219</cell><cell>0.4964</cell><cell>0.5495 0.3248</cell></row><row><cell>uogTrBaseDPH</cell><cell>bl_uogTr</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.3179 0.8415</cell><cell>0.4871</cell><cell>0.5490 0.3070</cell></row><row><cell>nlm-bm25-prf-2</cell><cell>NLM</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.2732 0.8099</cell><cell>0.4705</cell><cell>0.5218 0.2912</cell></row><row><cell>nlm-bm25-prf-1</cell><cell>NLM</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.2390 0.8086</cell><cell>0.4675</cell><cell>0.4958 0.2720</cell></row><row><cell>mpii_run3</cell><cell>mpii</cell><cell>rerank</cell><cell>nnlm</cell><cell cols="2">0.1499 0.6388</cell><cell>0.3286</cell><cell>0.6283 0.2587</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,72.00,90.75,468.00,615.05"><head>Table 5 :</head><label>5</label><figDesc>Passage retrieval runs. RR (MS) is based on MS MARCO labels. All other metrics are based on NIST labels.</figDesc><table coords="7,85.73,105.23,438.30,600.57"><row><cell>run</cell><cell>group</cell><cell cols="3">subtask neural RR (MS)</cell><cell cols="3">RR NDCG@10 NCG@1000</cell><cell>AP</cell></row><row><cell>pash_r3</cell><cell>PASH</cell><cell>rerank</cell><cell>nnlm</cell><cell cols="2">0.3678 0.9147</cell><cell>0.8031</cell><cell>0.7056 0.5445</cell></row><row><cell>pash_r2</cell><cell>PASH</cell><cell>rerank</cell><cell>nnlm</cell><cell cols="2">0.3677 0.9023</cell><cell>0.8011</cell><cell>0.7056 0.5420</cell></row><row><cell>pash_f3</cell><cell>PASH</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.3506 0.8885</cell><cell>0.8005</cell><cell>0.7255 0.5504</cell></row><row><cell>pash_f1</cell><cell>PASH</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.3598 0.8699</cell><cell>0.7956</cell><cell>0.7209 0.5455</cell></row><row><cell>pash_f2</cell><cell>PASH</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.3603 0.8931</cell><cell>0.7941</cell><cell>0.7132 0.5389</cell></row><row><cell>p_d2q_bm25_duo</cell><cell>h2oloo</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.3838 0.8798</cell><cell>0.7837</cell><cell>0.8035 0.5609</cell></row><row><cell>p_d2q_rm3_duo</cell><cell>h2oloo</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.3795 0.8798</cell><cell>0.7821</cell><cell>0.8446 0.5643</cell></row><row><cell>p_bm25rm3_duo</cell><cell>h2oloo</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.3814 0.8759</cell><cell>0.7583</cell><cell>0.7939 0.5355</cell></row><row><cell>CoRT-electra</cell><cell cols="3">HSRM-LAVIS fullrank nnlm</cell><cell cols="2">0.4039 0.8703</cell><cell>0.7566</cell><cell>0.8072 0.5399</cell></row><row><cell>RMIT-Bart</cell><cell>RMIT</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.3990 0.8447</cell><cell>0.7536</cell><cell>0.7682 0.5121</cell></row><row><cell>pash_r1</cell><cell>PASH</cell><cell>rerank</cell><cell>nnlm</cell><cell cols="2">0.3622 0.8675</cell><cell>0.7463</cell><cell>0.7056 0.4969</cell></row><row><cell>NLE_pr3</cell><cell>NLE</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.3691 0.8440</cell><cell>0.7458</cell><cell>0.8211 0.5245</cell></row><row><cell>pinganNLP2</cell><cell>pinganNLP</cell><cell>rerank</cell><cell>nnlm</cell><cell cols="2">0.3579 0.8602</cell><cell>0.7368</cell><cell>0.7056 0.4881</cell></row><row><cell>pinganNLP3</cell><cell>pinganNLP</cell><cell>rerank</cell><cell>nnlm</cell><cell cols="2">0.3653 0.8586</cell><cell>0.7352</cell><cell>0.7056 0.4918</cell></row><row><cell>pinganNLP1</cell><cell>pinganNLP</cell><cell>rerank</cell><cell>nnlm</cell><cell cols="2">0.3553 0.8593</cell><cell>0.7343</cell><cell>0.7056 0.4896</cell></row><row><cell>NLE_pr2</cell><cell>NLE</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.3658 0.8454</cell><cell>0.7341</cell><cell>0.6938 0.5117</cell></row><row><cell>NLE_pr1</cell><cell>NLE</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.3634 0.8551</cell><cell>0.7325</cell><cell>0.6938 0.5050</cell></row><row><cell>1</cell><cell cols="2">nvidia_ai_apps rerank</cell><cell>nnlm</cell><cell cols="2">0.3709 0.8691</cell><cell>0.7271</cell><cell>0.7056 0.4899</cell></row><row><cell>bigIR-BERT-R</cell><cell>QU</cell><cell>rerank</cell><cell>nnlm</cell><cell cols="2">0.4040 0.8562</cell><cell>0.7201</cell><cell>0.7056 0.4845</cell></row><row><cell>fr_pass_roberta</cell><cell>BITEM</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.3580 0.8769</cell><cell>0.7192</cell><cell>0.7982 0.4990</cell></row><row><cell>bigIR-DCT-T5-F</cell><cell>QU</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.3540 0.8638</cell><cell>0.7173</cell><cell>0.8093 0.5004</cell></row><row><cell>rr-pass-roberta</cell><cell>BITEM</cell><cell>rerank</cell><cell>nnlm</cell><cell cols="2">0.3701 0.8635</cell><cell>0.7169</cell><cell>0.7056 0.4823</cell></row><row><cell>bcai_bertl_pass</cell><cell>bcai</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.3715 0.8453</cell><cell>0.7151</cell><cell>0.7990 0.4641</cell></row><row><cell>bigIR-T5-R</cell><cell>QU</cell><cell>rerank</cell><cell>nnlm</cell><cell cols="2">0.3574 0.8668</cell><cell>0.7138</cell><cell>0.7056 0.4784</cell></row><row><cell>2</cell><cell cols="3">nvidia_ai_apps fullrank nnlm</cell><cell cols="2">0.3560 0.8507</cell><cell>0.7113</cell><cell>0.7447 0.4866</cell></row><row><cell>bigIR-T5-BERT-F</cell><cell>QU</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.3916 0.8478</cell><cell>0.7073</cell><cell>0.8393 0.5101</cell></row><row><cell>bigIR-T5xp-T5-F</cell><cell>QU</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.3420 0.8579</cell><cell>0.7034</cell><cell>0.8393 0.5001</cell></row><row><cell>nlm-ens-bst-2</cell><cell>NLM</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.3542 0.8203</cell><cell>0.6934</cell><cell>0.7190 0.4598</cell></row><row><cell>nlm-ens-bst-3</cell><cell>NLM</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.3195 0.8491</cell><cell>0.6803</cell><cell>0.7594 0.4526</cell></row><row><cell>nlm-bert-rr</cell><cell>NLM</cell><cell>rerank</cell><cell>nnlm</cell><cell cols="2">0.3699 0.7785</cell><cell>0.6721</cell><cell>0.7056 0.4341</cell></row><row><cell>relemb_mlm_0_2</cell><cell>UAmsterdam</cell><cell>rerank</cell><cell>nnlm</cell><cell cols="2">0.2856 0.7677</cell><cell>0.6662</cell><cell>0.7056 0.4350</cell></row><row><cell>nlm-prfun-bert</cell><cell>NLM</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.3445 0.8603</cell><cell>0.6648</cell><cell>0.6927 0.4265</cell></row><row><cell>TUW-TK-Sparse</cell><cell>TU_Vienna</cell><cell>rerank</cell><cell>nn</cell><cell cols="2">0.3188 0.7970</cell><cell>0.6610</cell><cell>0.7056 0.4164</cell></row><row><cell>TUW-TK-2Layer</cell><cell>TU_Vienna</cell><cell>rerank</cell><cell>nn</cell><cell cols="2">0.3075 0.7654</cell><cell>0.6539</cell><cell>0.7056 0.4179</cell></row><row><cell>p_d2q_bm25</cell><cell>anserini</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.2757 0.7326</cell><cell>0.6187</cell><cell>0.8035 0.4074</cell></row><row><cell>p_d2q_bm25rm3</cell><cell>anserini</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.2848 0.7424</cell><cell>0.6172</cell><cell>0.8391 0.4295</cell></row><row><cell>bert_6</cell><cell>UAmsterdam</cell><cell>rerank</cell><cell>nnlm</cell><cell cols="2">0.3240 0.7386</cell><cell>0.6149</cell><cell>0.7056 0.3760</cell></row><row><cell>CoRT-bm25</cell><cell cols="3">HSRM-LAVIS fullrank nnlm</cell><cell cols="2">0.2201 0.8372</cell><cell>0.5992</cell><cell>0.8072 0.3611</cell></row><row><cell>CoRT-standalone</cell><cell cols="3">HSRM-LAVIS fullrank nnlm</cell><cell cols="2">0.2412 0.8112</cell><cell>0.5926</cell><cell>0.6002 0.3308</cell></row><row><cell>bl_bcai_mdl1_vt</cell><cell>bl_bcai</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.1854 0.7037</cell><cell>0.5667</cell><cell>0.7430 0.3380</cell></row><row><cell>bcai_class_pass</cell><cell>bcai</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.1999 0.7115</cell><cell>0.5600</cell><cell>0.7430 0.3374</cell></row><row><cell>bl_bcai_mdl1_vs</cell><cell>bl_bcai</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.1563 0.6277</cell><cell>0.5092</cell><cell>0.7430 0.3094</cell></row><row><cell>indri-fdm</cell><cell>bl_rmit</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.1798 0.6498</cell><cell>0.5003</cell><cell>0.7778 0.2989</cell></row><row><cell>terrier-InL2</cell><cell>bl_rmit</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.1864 0.6436</cell><cell>0.4985</cell><cell>0.7649 0.3135</cell></row><row><cell>terrier-BM25</cell><cell>bl_rmit</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.1631 0.6186</cell><cell>0.4980</cell><cell>0.7572 0.3021</cell></row><row><cell>DLH_d_5_t_25</cell><cell>RMIT</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.1454 0.5094</cell><cell>0.4935</cell><cell>0.8175 0.3199</cell></row><row><cell>indri-lmds</cell><cell>bl_rmit</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.1250 0.5866</cell><cell>0.4912</cell><cell>0.7741 0.2961</cell></row><row><cell>indri-sdm</cell><cell>bl_rmit</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.1600 0.6239</cell><cell>0.4822</cell><cell>0.7726 0.2870</cell></row><row><cell>p_bm25rm3</cell><cell>anserini</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.1495 0.6360</cell><cell>0.4821</cell><cell>0.7939 0.3019</cell></row><row><cell>p_bm25</cell><cell>anserini</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.1786 0.6585</cell><cell>0.4796</cell><cell>0.7428 0.2856</cell></row><row><cell>bm25_bert_token</cell><cell>UAmsterdam</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.1576 0.6409</cell><cell>0.4686</cell><cell>0.7169 0.2606</cell></row><row><cell>terrier-DPH</cell><cell>bl_rmit</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.1420 0.5667</cell><cell>0.4671</cell><cell>0.7353 0.2758</cell></row><row><cell cols="2">TF_IDF_d_2_t_50 RMIT</cell><cell cols="2">fullrank trad</cell><cell cols="2">0.1391 0.5317</cell><cell>0.4580</cell><cell>0.7722 0.2923</cell></row><row><cell>small_1k</cell><cell>reSearch2vec</cell><cell>rerank</cell><cell>nnlm</cell><cell cols="2">0.0232 0.2785</cell><cell>0.2767</cell><cell>0.7056 0.2112</cell></row><row><cell>med_1k</cell><cell>reSearch2vec</cell><cell>rerank</cell><cell>nnlm</cell><cell cols="2">0.0222 0.2720</cell><cell>0.2708</cell><cell>0.7056 0.2081</cell></row><row><cell>DoRA_Large_1k</cell><cell>reSearch2vec</cell><cell>rerank</cell><cell>nnlm</cell><cell cols="2">0.0208 0.2740</cell><cell>0.2661</cell><cell>0.7056 0.2072</cell></row><row><cell>DoRA_Small</cell><cell>reSearch2vec</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.0000 0.1287</cell><cell>0.0484</cell><cell>0.0147 0.0088</cell></row><row><cell>DoRA_Med</cell><cell>reSearch2vec</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.0000 0.1075</cell><cell>0.0431</cell><cell>0.0147 0.0087</cell></row><row><cell>DoRA_Large</cell><cell>reSearch2vec</cell><cell cols="2">fullrank nnlm</cell><cell cols="2">0.0000 0.1111</cell><cell>0.0414</cell><cell>0.0146 0.0079</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="11,72.00,80.12,468.00,337.74"><head>Table 6 :</head><label>6</label><figDesc>Leaderboard metrics breakdown. The Kendall agreement (τ ) of NDCG@10 and RR (MS) varies across task and run type. Agreement on the best neural network runs is high, but agreement on the best document trad runs is very low. We do not list the agreement for passage nn runs since there are only two runs.</figDesc><table coords="11,249.78,116.44,109.96,62.28"><row><cell cols="3">run type docs passages</cell></row><row><cell>nnlm</cell><cell>0.83</cell><cell>0.76</cell></row><row><cell>nn</cell><cell>0.96</cell><cell>-</cell></row><row><cell>trad</cell><cell>0.03</cell><cell>0.67</cell></row><row><cell>all</cell><cell>0.46</cell><cell>0.69</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,88.14,714.52,241.51,7.47"><p>https://microsoft.github.io/TREC-2020-Deep-Learning</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,72.00,702.61,468.00,8.64;12,81.96,713.51,247.80,8.64" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="12,149.90,713.51,149.82,8.64">Umass at trec 2004: Novelty and hard</title>
		<author>
			<persName coords=""><forename type="first">Nasreen</forename><surname>Abdul-Jaleel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Leah</forename><surname>Larkey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Courtney</forename><surname>Wade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.00,75.48,468.00,8.64;13,81.96,86.39,458.04,8.64;13,81.96,97.12,193.32,8.82" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="13,276.80,86.39,263.20,8.64;13,81.96,97.30,26.39,8.64">Ms marco: A human generated machine reading comprehension dataset</title>
		<author>
			<persName coords=""><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Mcnamara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09268</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,72.00,112.19,468.00,8.64;13,81.96,122.92,361.60,8.82" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,349.07,112.19,190.93,8.64;13,81.96,123.10,106.67,8.64">The arcade learning environment: An evaluation platform for general agents</title>
		<author>
			<persName coords=""><forename type="first">Yavar</forename><surname>Marc G Bellemare</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joel</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,196.21,122.92,165.52,8.59">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="253" to="279" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.00,137.99,468.00,8.64;13,81.96,148.72,458.04,8.82;13,81.96,159.63,181.48,8.82" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,325.38,137.99,214.62,8.64;13,81.96,148.90,66.56,8.64">Off the beaten path: Let&apos;s replace term-based retrieval with k-nn search</title>
		<author>
			<persName coords=""><forename type="first">Leonid</forename><surname>Boytsov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yury</forename><surname>Malkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,170.39,148.72,369.61,8.59;13,81.96,159.63,49.37,8.59">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 25th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1099" to="1108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.00,174.70,468.00,8.64;13,81.96,185.44,336.08,8.82" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="13,433.11,174.70,106.89,8.64;13,81.96,185.61,168.92,8.64">Orcas: 18 million clicked query-document pairs for analyzing search</title>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bodo</forename><surname>Billerbeck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05324</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,72.00,200.51,468.00,8.64;13,81.96,211.24,213.97,8.82" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,365.53,200.51,174.47,8.64;13,81.96,211.42,32.70,8.64">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,133.13,211.24,45.82,8.59">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.00,226.31,468.00,8.64;13,81.96,237.04,308.14,8.82" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="13,357.40,226.31,182.60,8.64;13,81.96,237.22,140.77,8.64">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,72.00,252.11,468.00,8.64;13,81.96,262.84,159.58,8.82" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="13,363.85,252.11,172.51,8.64">Bag of tricks for efficient text classification</title>
		<author>
			<persName coords=""><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01759</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,72.00,277.74,410.51,8.82" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,281.44,277.92,54.75,8.64">Deep learning</title>
		<author>
			<persName coords=""><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,343.68,277.74,25.54,8.59">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.00,292.81,468.00,8.64;13,81.96,303.54,458.04,8.82;13,81.96,314.45,213.28,8.82" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,390.09,292.81,149.92,8.64;13,81.96,303.72,50.79,8.64">High accuracy retrieval with multiple nested ranker</title>
		<author>
			<persName coords=""><forename type="first">Irina</forename><surname>Matveeva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timo</forename><surname>Burkard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andy</forename><surname>Laucius</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Leon</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,149.30,303.54,390.69,8.59;13,81.96,314.45,92.44,8.59">Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 29th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="437" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.00,329.35,468.00,8.82;13,81.96,340.43,22.42,8.64" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Mitra</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.07666</idno>
		<title level="m" coord="13,212.95,329.52,184.11,8.64">An updated duet model for passage re-ranking</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,72.00,355.33,468.00,8.64;13,81.96,366.06,243.97,8.82" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,282.36,355.33,257.64,8.64;13,81.96,366.24,74.42,8.64">Learning to match using local and distributed representations of text for web search</title>
		<author>
			<persName coords=""><forename type="first">Mitra</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,174.75,366.06,44.44,8.59">Proc. WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1291" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.00,381.13,468.00,8.64;13,81.96,391.86,458.04,8.82;13,81.96,402.77,100.17,8.82" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="13,485.78,381.13,54.23,8.64;13,81.96,392.04,393.00,8.64">Incorporating query term independence assumption for efficient retrieval and ranking using deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">Corby</forename><surname>Bhaskar Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emine</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Yilmaz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03693</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,72.00,417.66,468.00,8.82;13,81.96,428.57,134.95,8.82" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="13,340.73,417.84,166.85,8.64">Document expansion by query prediction</title>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08375</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,72.00,443.47,468.00,8.82;13,81.96,454.38,239.97,8.82" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="13,246.70,443.65,234.19,8.64">The probabilistic relevance framework: Bm25 and beyond</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,490.68,443.47,49.32,8.59;13,81.96,454.38,151.96,8.59">Foundations and Trends R in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="333" to="389" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.00,469.45,468.00,8.64;13,81.96,480.18,347.47,8.82" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,396.79,469.45,143.21,8.64;13,81.96,480.36,150.65,8.64">Optimizing query evaluations using reinforcement learning for web search</title>
		<author>
			<persName coords=""><forename type="first">Corby</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Damien</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mitra</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,250.98,480.18,46.72,8.59">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1193" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.00,495.25,468.00,8.64;13,81.96,505.98,458.04,8.82;13,81.96,517.07,59.65,8.64" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="13,358.25,495.25,181.76,8.64;13,81.96,506.16,78.39,8.64">Indri: A language model-based search engine for complex queries</title>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Howard</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,179.22,505.98,268.88,8.59">Proceedings of the International Conference on Intelligent Analysis</title>
		<meeting>the International Conference on Intelligent Analysis</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.00,531.79,468.00,8.82;13,81.96,542.70,458.04,8.82;13,81.96,553.78,89.54,8.64" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="13,259.25,531.97,213.55,8.64">A cascade ranking model for efficient ranked retrieval</title>
		<author>
			<persName coords=""><forename type="first">Lidan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,490.65,531.79,49.35,8.59;13,81.96,542.70,427.58,8.59">Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval</title>
		<meeting>the 34th international ACM SIGIR conference on Research and development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.00,568.68,468.00,8.64;13,81.96,579.41,439.25,8.82" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="13,283.02,568.68,256.99,8.64;13,81.96,579.59,237.52,8.64">Critically examining the &quot;neural hype&quot;: Weak baselines and the additivity of effectiveness gains from neural ranking models</title>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kuang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,338.33,579.41,46.72,8.59">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1129" to="1132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.00,594.48,468.00,8.64;13,81.96,605.21,385.34,8.82" xml:id="b19">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Xlnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m" coord="13,491.87,594.48,48.13,8.64;13,81.96,605.39,212.98,8.64">Generalized autoregressive pretraining for language understanding</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,72.00,620.28,468.00,8.64;13,81.96,631.01,458.03,8.82;13,81.96,642.10,22.42,8.64" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="13,446.00,620.28,94.01,8.64;13,81.96,631.19,292.07,8.64">From neural re-ranking to neural ranking: Learning a sparse representation for inverted indexing</title>
		<author>
			<persName coords=""><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,394.52,631.01,46.29,8.59">Proc. CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="497" to="506" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
