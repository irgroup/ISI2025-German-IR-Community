<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,163.32,83.23,285.81,15.44">Overview of TREC OpenSearch 2017</title>
				<funder ref="#_AsnQwbf">
					<orgName type="full">Netherlands Organisation for Scienti c Research (NWO)</orgName>
				</funder>
				<funder ref="#_uQA4pVp">
					<orgName type="full">European Community</orgName>
				</funder>
				<funder ref="#_hZdsVHQ #_mJZM8jv #_xJ3rmpd #_nQVgwGF #_cZN4ZzP">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_tSJkurg">
					<orgName type="full">Ahold Delhaize</orgName>
				</funder>
				<funder ref="#_TJqbxxD">
					<orgName type="full">Microsoft Research Ph</orgName>
				</funder>
				<funder>
					<orgName type="full">Elsevier</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,107.67,113.33,69.36,10.60"><forename type="first">Rolf</forename><surname>Jagerman</surname></persName>
							<email>rolf.jagerman@uva.nl</email>
						</author>
						<author>
							<persName coords="1,269.18,113.33,74.64,10.60"><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
							<email>krisztian.balog@uis.no</email>
						</author>
						<author>
							<persName coords="1,436.95,113.33,67.63,10.60"><forename type="first">Phillip</forename><surname>Schaer</surname></persName>
							<email>phillip.schaer@th-koeln.de</email>
						</author>
						<author>
							<persName coords="1,102.67,183.71,78.16,10.60"><forename type="first">Johann</forename><surname>Schaible</surname></persName>
							<email>johann.schaible@gesis.org</email>
						</author>
						<author>
							<persName coords="1,243.94,183.71,124.13,10.60"><forename type="first">Narges</forename><surname>Tavakolpoursaleh</surname></persName>
							<email>narges.tavakolpoursaleh@gesis.org</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Stavenger Stavenger</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">TH Köln (University of Applied Sciences</orgName>
								<address>
									<settlement>Köln</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">GESIS -Leibniz Institute for the Social Sciences</orgName>
								<address>
									<settlement>Köln</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">GESIS -Leibniz Institute for the Social Sciences</orgName>
								<address>
									<settlement>Köln</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Maarten de Rijke</orgName>
								<orgName type="institution">University of Amsterdam Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,163.32,83.23,285.81,15.44">Overview of TREC OpenSearch 2017</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3F5012357733383A3B8D925383BACFD3</idno>
					<idno type="DOI">10.1145/nnnnnnn.nnnnnnn</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we provide an overview of the TREC 2017 OpenSearch track. The OpenSearch track provides researchers the opportunity to have their retrieval approaches evaluated in a live setting with real users. We focus on the academic search domain with the Social Science Open Access Repository (SSOAR) search engine and report our results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The goal of Information Retrieval (IR) is to help people nd information. Experiments on IR methods conducted in the setting of community-based benchmarking e orts have traditionally been done using either datasets made by professional assessors or using simulated users. This type of set up helps the repeatability and reproducibility of experiments. A drawback, however, is that these types of experiments largely abstract away the user. Furthermore, there are several limitations: <ref type="bibr" coords="1,157.91,457.69,9.32,7.95" target="#b0">(1)</ref> obtaining relevance judgements by professional assessors does not scale and is expensive, (2) relevance may change over time and is not a static concept, and (3) relevance judgements may not accurately portray the intents of the real users. A way to over-come these limitations is to use online evaluation, where we observe users in situ and measure metrics such as click-through rate, time-to-success, abandonment, etc.</p><p>Unfortunately, access to real users is reserved for owners of online properties with a large and active user-base. There are considerable engineering and logistic challenges in setting up a search service and attracting a large user-base. As a result, there is a gap between the evaluation methods of researchers who have access to real users and those who do not. The aim of TREC OpenSearch is to bridge this gap and make evaluation using real users open to all researchers: Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).</p><p>"Open Search is a new evaluation paradigm for IR. The experimentation platform is an existing search engine. Researchers have the opportunity to replace components of this search engine and evaluate these components using interactions with real, unsuspecting users of this search engine" <ref type="bibr" coords="1,425.45,319.22,9.39,7.95" target="#b1">[2]</ref>.</p><p>In this paper we give a brief overview of the results of the OpenSearch track during 2017. We rst provide a description of the living labs methodology in Section 2. Next, we discuss the academic search use-case at SSOAR in Section 3. We present the outcomes of the track in Section 4. Finally, we conclude the overview in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">LIVING LABS METHODOLOGY</head><p>We consider the living labs methodology in a search engine setting: Users submit queries and obtain a ranked list of documents. An overview of the living-labs setup that we use is displayed in Figure <ref type="figure" coords="1,554.32,436.32,4.09,7.95">1</ref> and will now be described in more detail.</p><p>A real search engine extracts a set of queries Q from their logs. The queries are chosen in such a way that it is likely that they will be issued again in the future. For each query q there is also a corresponding set of candidate documents D q . The sites submit this information to the living-labs API<ref type="foot" coords="1,453.70,500.77,3.38,6.45" target="#foot_1">1</ref>  <ref type="bibr" coords="1,459.54,502.92,10.43,7.95" target="#b0">[1]</ref> (step 1 in Figure <ref type="figure" coords="1,531.01,502.92,2.88,7.95">1</ref>), after which it can be downloaded by the participants of the track (step 2 in Figure <ref type="figure" coords="1,353.02,524.84,2.94,7.95">1</ref>).</p><p>Participants are asked to submit their ranked lists of documents for each of the queries submitted by the sites. They are free to use any ranking algorithms they deem appropriate. In essence this task boils down to an ad-hoc document retrieval task (restricted to the candidate set D q ). The ranked lists produced by the participant's method are then submitted to the living-labs API (step 3 in Figure <ref type="figure" coords="1,550.94,590.59,2.88,7.95">1</ref>).</p><p>When a user of the site issues a query q ∈ Q, the site will rst obtain an experimental ranking from one of the participants via the living-labs API. This ranking is then interleaved, using the teamdraft interleaving algorithm <ref type="bibr" coords="1,420.74,634.43,9.29,7.95" target="#b2">[3]</ref>, with the production ranking. This produces the SERP that is shown to the user (step 4 in Figure <ref type="figure" coords="1,550.59,645.38,3.00,7.95">1</ref>). The user interacts with this SERP by clicking on some of the entries. The clicks are recorded and submitted back to the API in the form of feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TREC OpenSearch API</head><p>(1) submit collection of queries and documents (4) request an experimental ranking from a participant when a head query is issued</p><p>(2) get collection of queries and documents (3) submit ranked list for each query</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SSOAR Participant</head><p>Figure <ref type="figure" coords="2,82.48,217.91,3.45,7.70">1</ref>: An overview of TREC OpenSearch. SSOAR provides their collection of queries and documents to the OpenSearch API. Participants download this collection and then compute and submit their ranked lists. Whenever a user on SSOAR issues one of the selected queries, SSOAR requests an experimental ranking from a random participant and interleaves it with their production ranking to produce the SERP.</p><p>The team-draft interleaving algorithm attributes clicks to either the production ranking or the participant's ranking. By counting the number of times a click is attributed to either one of these, we can infer a winner or a tie for each impression (i.e., observation of the result list upon issuing of a query).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Changes Since Last Edition</head><p>To improve the track we have made numerous improvements and modi cations to the API code since the 2016 edition. Compared to the 2016 edition of TREC OpenSearch, there are four new major features:</p><p>2.1.1 Interleaving endpoint. The API has a new endpoint for interleaving. This endpoint allows sites to submit their production ranking and obtain an interleaved ranked list. This reduces engineering overhead for participating sites, as they will not have to implement the interleaving algorithm themselves but can instead rely on the one provided by the API.</p><p>2.1.2 Authentication. We switched to the HTTP Basic authentication scheme to authenticate communication with the API. In previous years we would authenticate users by checking the API key that was provided in the URL. The new authentication scheme follows the HTTP Basic authentication standard and is more secure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Multiple Runs.</head><p>Participants are now able to submit up to ve runs. Impression tra c is still distributed fairly amongst all participants, i.e. having multiple runs does not give you more impressions. Although this feature was much requested in the previous edition of the track, it was not used by the participants featured in this paper.</p><p>2.1.4 Load-balancing. In previous editions, tra c was distributed uniformly at random. We removed the random component and replaced it with a load-balancer that prioritizes participants with the least amount of impressions so far. Unfortunately this load-balancer caused an issue with one participant's run. Their runs were not displayed during the competition, because they were activated very early and had gathered too many impressions in the time leading up the start of the round. During the real round the load-balancer tried to compensate for these impressions by prioritizing the other participants. This behavior was unintentional and subsequently xed after uncovering the problem. Unfortunately the round had already concluded by this point, so we are not able to provide results to this participant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ACADEMIC SEARCH AT SSOAR</head><p>The participating site for TREC OpenSearch 2017 is the Social Science Open Access Repository (SSOAR) <ref type="foot" coords="2,469.37,355.73,3.38,6.45" target="#foot_2">2</ref> , which generously provided us with a collection of queries and documents to be ranked, and opened up their service to the experimental rankings made by the participants.</p><p>SSOAR is an open access document repository that is based on the Solr-based software DSpace<ref type="foot" coords="2,434.83,410.52,3.38,6.45" target="#foot_3">3</ref> which is one of the state-of-theart repository systems in the open access community. SSOAR is developed and maintained at GESIS, Cologne, Germany. It contains over 43,500 full text documents from the social sciences and neighboring areas. Each document is annotated with a rich set of metadata, mostly including descriptors and classi cation information. Around 60,000 unique visitors visit SSOAR and download more than 213,000 PDF full texts per month. Both numbers are cleaned from search engine accesses using the enterprise web tracking software E-Tracker.</p><p>The queries and candidate documents were extracted from the live system. We decided to include more than 1000 queries (instead of 100) to allow generating more impressions and hopefully more clicks as well. In detail, we used the most frequent 1200 head queries from a one year log le dump of SSOAR. Several example queries are displayed in Table <ref type="table" coords="2,430.94,577.05,3.13,7.95" target="#tab_0">1</ref>. After a manual ltering process that erased some obvious data gibberish, 1165 queries remained. The queries were split into 500 test and 665 training queries. The training/test split was non-uniform by mistake. In particular, we took the top frequent queries as test queries and the remaining as training queries. Unfortunately, this type of split leads to training and test queries with very di erent characteristics and is something that should be avoided in the future.</p><p>The items in the set of candidate documents consist of a title and additional content descriptions including some of the rich metadata extracted from the DSpace system. We included information like abstracts, author names, publishers, language, volume and issue numbers, subjects, and if available thesaurus descriptors. 4 Additionally, the original document in SSOAR comprises a persistent identi er and the original document ID, such that even more document metadata-that is available in English as well as in German-can be extracted from the OAI-PMH interface of SSOAR. 5  An example document is displayed in Listing 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Changes Since Last Edition</head><p>In contrast to the 2016 edition of the OpenSearch track, we did not include queries gathered from the discovery interface of SSOAR, 6 as these turned out to produce mostly tie results. The reason behind this is that the general pattern of these discovery queries is to start the search process with a selection of a topic from a topical classi cation of the social sciences. Subsequently, users tend to drill down their search by using facets like publication year or authors. Such drill down searches have the e ect that they are not covered by the top 100 documents precomputed by the teams participating in OpenSearch. Therefore, these queries produce a vast amount of ties that do not help to evaluate the di erent participating systems.</p><p>The technical infrastructure of SSOAR to provide the Living Lab functionality was expanded. In last year's OpenSearch track, we implemented the Living Lab functionality directly on the server hosting the SSOAR live system. This led to various performance issues of the live system due to communication time-outs and the performance-heavy feedback submitted back to the Living Labs API. To address these issues, we set up and con gured an own server for the Living Lab functionality. The server was used to extract the head queries and candidate documents from the SSOAR log les, to communicate with the Living Labs API when a head query was triggered, as well as to record, compute, and submit the feedback to the Living Labs API. As a result, if one of these tasks were to cause any problems, the functionality of the SSOAR live system would not be a ected. The rest of the technical infrastructure is the same as last year. For more technical details please check the corresponding section in <ref type="bibr" coords="3,146.73,488.96,9.39,7.95" target="#b1">[2]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID</head><p>Query string ssoar-q43 migration ssoar-q115 bilateral relations ssoar-q289 labor sozialwissenschaft ssoar-q376 brexit ssoar-q482 gruppendynamik ssoar-q699 alkohol ssoar-q803 migration und gesundheit 4 http://lod.gesis.org/thesoz/en.html 5 http://www.ssoar.info/OAIHandler/request?verb=Identify# 6 http://www.ssoar.info/ssoar/discover Listing 1: Example SSOAR Document { "docid": "ssoar-d10466", "content": { "abstract": "Plausibilit\u00e4t spielt in allen Wissenschafts... "author": "Reszke, Paul", "available": "2015-12-14T11:20:34Z", "description": "Published Version", "identifier": "urn:nbn:de:0168-ssoar-455901", "issued": "2015", "language": "de", "publisher": "DEU", "subject": "10200", "type": "collection article" }, "creation _ time": "2017-06-15T17:04:07.403+0200", "site _ id": "ssoar", "title": "Linguistic-philosophical investigations of plausibility: pat... }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS</head><p>In this section we present the results of running the TREC OpenSearch track in 2017. We report on a single evaluation round, which ran during August 1st-August 31st 2017. The main results are displayed in Table <ref type="table" coords="3,349.89,276.47,3.11,7.95" target="#tab_1">2</ref>. For each participating system we show the number of impressions, clicks, wins, ties, losses and the outcome. Outcome is our main evaluation metric, summarizing the performance of each system against the production ranker, and is computed as follows:</p><formula xml:id="formula_0" coords="3,383.06,322.87,175.14,20.32">Outcome = #Wins #Wins + #Losses .<label>(1)</label></formula><p>Out of the participating teams, Gesis was the winning team with the highest outcome score.</p><p>Unfortunately, the number of clicks is insu cient to draw statistically signi cant conclusions. We perform the sign test where our null-hypothesis is that there is no preference, i.e. each system has a 50% chance to win. The p-values we found were 0.61, 0.99 and 0.61 for teams Gesis, Webis and ICTNET respectively. These numbers tell us that the observed di erences are not statistically signi cant and we would need to collect more data. Assuming the ratio of wins and losses stays the same, we would need to collect about 9 months worth of data to declare a winner with a p-value &lt; 0.05. We plot the number of impressions distributed across queries in Figure <ref type="figure" coords="3,343.29,591.87,3.04,7.95">3</ref>. The impressions follow a power law distribution; several queries are responsible for many of the impressions, while most queries are only issued a handful of times. The distribution of clicks follows a similar distribution as is displayed in Figure <ref type="figure" coords="3,515.88,624.75,3.07,7.95" target="#fig_1">4</ref>. We note that the click-through rate for this round was particularly low. Out of the many thousands of impressions only a few dozen resulted in a click. A further analysis of the tra c shows that some queries were issued on a regular interval. In particular we see in Figure <ref type="figure" coords="3,354.19,679.54,4.25,7.95" target="#fig_0">2</ref> that query ssoar-q1 is issued exactly every 5 minutes. This tells us that some automated process, such as a crawler or a bot, is requesting this query. However, since this query also 08:00 10:00 12:00 14:00 16:00 18:00 ssoar-q1 ssoar-q5 ssoar-q9 ssoar-q10 ssoar-q40 ssoar-q266 ssoar-q277 ssoar-q473 SSOAR queries on Aug 4 2017  occasionally resulted in real clicks, we cannot easily distinguish the automated tra c from the real human tra c. Naively removing this query would also remove valuable click data, so we decided to leave this data in.</p><p>We are running an extra round during October 2017 to obtain more clicks. Furthermore, this extra round o ers the team who experienced a problem with the new tra c load-balancer (see Section 2.1.4) a chance to compete with the other teams. At the time of writing, this extra round is still underway.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper we present our results from the TREC 2017 OpenSearch track. The infrastructure for the OpenSearch track was kept largely the same as the 2016 version of the track, with some minor modications and improvements. The results show that tra c for this round was low and clicks were extremely sparse. This makes it di cult to draw statistically signi cant conclusions.</p><p>Online evaluation remains an important part of IR. The necessity of evaluation using real users is becoming increasingly apparent with the development of new technologies such as conversational assistants <ref type="bibr" coords="4,357.04,369.81,9.52,7.95" target="#b3">[4]</ref>. We can no longer rely solely on Cran eld-style evaluations. With our work we hope to have made the rst steps towards an online evaluation platform that is open to all researchers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,53.80,217.49,504.67,8.77;4,53.45,229.51,255.11,7.70"><head>Figure 2 :Figure</head><label>2</label><figDesc>Figure 2: Query frequency over time on August 4th 2017. Each bar indicates when a query was issued. Notice that ssoar-q1 was issued exactly every 5 minutes, indicating a crawler or bot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,53.80,575.42,240.25,7.70"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The number of clicks distributed over the queries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,121.89,527.89,103.77,7.70"><head>Table 1 :</head><label>1</label><figDesc>Example queries.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,324.77,488.43,226.32,71.04"><head>Table 2 :</head><label>2</label><figDesc>Outcome of TREC OpenSearch 2017 for SSOAR.</figDesc><table coords="3,326.93,514.34,222.31,45.14"><row><cell></cell><cell cols="6">Imps Clicks Wins Ties Losses Outcome</cell></row><row><cell>Gesis</cell><cell>3658</cell><cell>31</cell><cell>9</cell><cell>2</cell><cell>6</cell><cell>0.6</cell></row><row><cell>Webis</cell><cell>3662</cell><cell>30</cell><cell>6</cell><cell>3</cell><cell>7</cell><cell>0.462</cell></row><row><cell cols="2">ICTNET 3191</cell><cell>44</cell><cell>6</cell><cell>4</cell><cell>9</cell><cell>0.4</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0" coords="1,53.80,685.98,214.60,6.23;1,53.32,694.79,240.32,6.19;1,53.80,702.76,92.94,6.19"><p>Proceedings of the Twenty-Sixth Text REtrieval Conference (TREC 2017). NIST, © 2017 Copyright held by the owner/author(s). 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 DOI: 10.1145/nnnnnnn.nnnnnnn</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1" coords="1,320.88,702.76,113.13,6.19"><p>https://bitbucket.com/living-labs/ll-api/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2" coords="2,321.00,694.35,64.35,6.19"><p>http://www.ssoar.info/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3" coords="2,321.00,702.76,100.71,6.19"><p>http://www.dspace.org/introducing</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank <rs type="person">Anne Schuth</rs> and <rs type="person">Peter Dekker</rs> for their help in developing and setting up the infrastructure for <rs type="institution">TREC OpenSearch</rs>. Additionally we are thankful to SSOAR for sharing their queries, documents and users. We also want to thank all the participating teams. This research was supported by <rs type="funder">Ahold Delhaize</rs>, Amsterdam Data Science, the <rs type="grantName">Bloomberg Research Grant program</rs>, the <rs type="programName">Criteo Faculty Research Award program</rs>, <rs type="funder">Elsevier</rs>, the <rs type="funder">European Community</rs>'s <rs type="programName">Seventh Framework Programme</rs> (<rs type="programName">FP7/2007-2013)</rs> under grant agreement nr 312827 (VOX-Pol), the <rs type="funder">Microsoft Research Ph</rs>.D. program, the <rs type="institution">Netherlands Institute for Sound and Vision</rs>, the <rs type="funder">Netherlands Organisation for Scienti c Research (NWO)</rs> under project nrs <rs type="grantNumber">612.001.116</rs>, <rs type="grantNumber">HOR-11-10</rs>, <rs type="grantNumber">CI-14-25</rs>, <rs type="grantNumber">652.002.001</rs>, <rs type="grantNumber">612.001.551</rs>, <rs type="grantNumber">652.001.003</rs>, and <rs type="person">Yandex. All</rs> content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_tSJkurg">
					<orgName type="grant-name">Bloomberg Research Grant program</orgName>
					<orgName type="program" subtype="full">Criteo Faculty Research Award program</orgName>
				</org>
				<org type="funding" xml:id="_uQA4pVp">
					<orgName type="program" subtype="full">Seventh Framework Programme</orgName>
				</org>
				<org type="funding" xml:id="_TJqbxxD">
					<orgName type="program" subtype="full">FP7/2007-2013)</orgName>
				</org>
				<org type="funding" xml:id="_AsnQwbf">
					<idno type="grant-number">612.001.116</idno>
				</org>
				<org type="funding" xml:id="_hZdsVHQ">
					<idno type="grant-number">HOR-11-10</idno>
				</org>
				<org type="funding" xml:id="_mJZM8jv">
					<idno type="grant-number">CI-14-25</idno>
				</org>
				<org type="funding" xml:id="_xJ3rmpd">
					<idno type="grant-number">652.002.001</idno>
				</org>
				<org type="funding" xml:id="_nQVgwGF">
					<idno type="grant-number">612.001.551</idno>
				</org>
				<org type="funding" xml:id="_cZN4ZzP">
					<idno type="grant-number">652.001.003</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="4,334.39,613.21,223.94,6.19;4,334.14,621.13,224.06,6.23;4,334.39,629.10,222.05,6.23" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,483.52,613.21,74.81,6.19;4,334.14,621.18,72.74,6.19">Head First: Living Labs for Ad-hoc Search Evaluation</title>
		<author>
			<persName coords=""><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liadh</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anne</forename><surname>Schuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,419.17,621.13,139.04,6.23;4,334.39,629.10,184.33,6.23">Proc. of the 23rd ACM International Conference on Conference on Information and Knowledge Management (CIKM &apos;14</title>
		<meeting>of the 23rd ACM International Conference on Conference on Information and Knowledge Management (CIKM &apos;14</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1815" to="1818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,334.39,637.12,223.81,6.19;4,334.39,645.09,223.81,6.19;4,334.39,653.01,224.51,6.23;4,334.39,660.98,118.62,6.23" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,493.57,645.09,64.63,6.19;4,334.39,653.06,139.33,6.19">Overview of the TREC 2016 Open Search track: Academic Search Edition</title>
		<author>
			<persName coords=""><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anne</forename><surname>Schuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Dekker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Narges</forename><surname>Tavakolpoursaleh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Philipp</forename><surname>Schaer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Po-Yu</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">Lee</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,485.75,653.01,73.15,6.23;4,334.39,660.98,116.26,6.23">Proceedings of the Twenty-Fifth Text REtrieval Conference (TREC &apos;16)</title>
		<meeting>the Twenty-Fifth Text REtrieval Conference (TREC &apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,334.39,669.00,224.99,6.19;4,334.39,676.92,223.81,6.23;4,334.39,684.89,126.64,6.23" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,541.32,669.00,18.06,6.19;4,334.39,676.97,168.65,6.19">Largescale validation and analysis of interleaved search evaluation</title>
		<author>
			<persName coords=""><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Filip</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,508.23,676.92,49.97,6.23;4,334.39,684.89,84.81,6.23">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,334.39,692.91,223.81,6.19;4,334.39,700.83,154.40,6.23" xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Julia</forename><surname>Kiseleva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04524</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Evaluating Personal Assistants on Mobile devices. arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
