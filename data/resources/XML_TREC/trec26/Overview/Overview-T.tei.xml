<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,185.42,112.60,241.17,14.93">TREC 2017 Tasks Track Overview</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,75.63,144.91,97.65,10.37"><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,183.74,144.91,68.74,10.37"><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,262.94,144.91,86.66,10.37"><forename type="first">Rishabh</forename><surname>Mehrotra</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,360.07,144.91,69.39,10.37"><forename type="first">Ben</forename><surname>Carterette</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Delaware</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,439.91,144.91,68.73,10.37"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,274.46,158.86,58.59,10.37"><forename type="first">Peter</forename><surname>Bailey</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,185.42,112.60,241.17,14.93">TREC 2017 Tasks Track Overview</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3A6126A05892979D866580C735DE4827</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Research in Information Retrieval has traditionally focused on serving the best results for a single query, ignoring the reasons (or the task) that might have motivated the user to submit that query. Often times search engines are used to complete complex tasks; achieving these tasks with current search engines requires users to issue multiple queries. For example, booking travel to a location such as London could require the user to submit various queries such as flights to London, hotels in London, points of interest around London, etc.</p><p>Standard evaluation mechanisms focus on evaluating the quality of a retrieval system in terms of the topical relevance of the results retrieved, completely ignoring the fact that user satisfaction mainly depends on the usefulness of the system in helping the user complete the actual task that led the user issue the query. The TREC Tasks Track is an attempt in devising mechanisms for evaluating quality of retrieval systems in terms of (1) how well they can understand the underlying task that led the user submit a query, and (2) how useful they are for helping users complete their tasks.</p><p>A number of application areas, beyond task-based retrieval, could benefit from such an evaluation framework and the constructed test collections, including, but not limited to, digital assistants tasks, query suggestions, session-based retrieval, exploratory search, entity-based search.</p><p>In this paper, we first summarize the three categories of evaluation mechanisms used in the track and briefly describe the corpus, topics, and tasks that comprise the test collection. We then give an overview of the runs submitted to the 2017 Tasks Track and present the evaluation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Evaluation Goals</head><p>This year we kept the same evaluation goals as in 2015 and 2016, namely: (1) Task understanding, (2) Task completion, and (3) Ad-hoc retrieval.</p><p>Participants were provided with a set of 50 queries, together with the Freebase ID <ref type="foot" coords="1,409.98,571.14,3.49,6.05" target="#foot_0">1</ref> for each entity in these queries. For instance, the following query was one of the 50 provided to participants: &lt;task id = "1"&gt; &lt;query&gt;build a hydroponic garden&lt;/query&gt; &lt;freebase entity = "1"&gt; &lt;id&gt;/m/03nwn&lt;/id&gt; &lt;text&gt;hydroponic&lt;/text&gt; &lt;/freebase&gt; &lt;freebase entity = "2"&gt; &lt;id&gt;/m/0bl0l&lt;/id&gt; &lt;text&gt;garden&lt;/text&gt; &lt;/freebase&gt; &lt;/task&gt;</p><p>The queries were manually constructed by the organizers of the track so that they have the following two properties: (a) they represent an action-oriented task, and (b) the task can be decomposed in a sequence of subtasks.</p><p>The corpus of the track remained the ClueWeb12 document collection. To alleviate the burden of indexing the collection on participants, the category B subset was indexed by Dr. Guido Zuccon and his team in Elastic 5.3 using stemming and stopping. The default retrieval algorithm was used, i.e. BM25. The Elastic Instance was deployed onto Microsoft Azure Platform (and is supported by a Microsoft Azure Research Award). Participants could query the API by sending CURL requests or writing a script. The spam scores from Waterloo Spam Rankings (http: //www.mansci.uwaterloo.ca/~msmucker/cw12spam/) were also provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task Understanding</head><p>The goal of task understanding is to test whether systems can understand the task and the possible subtasks users might be trying to complete given a query. For each query, participants were asked to submit a ranked list of up to 100 key phrases that represent the set of all subtasks the user needs to complete so that a complete coverage of potential subtasks is achieved, while avoiding redundancy.</p><p>Evaluating the coverage and relevance of the key-phrases submitted by the participants requires that a set of "gold standard" subtasks is identified in advance. These gold standard subtasks were constructed by the organizers, but were not be provided to the participants until the evaluation results were disclosed (i.e. post completion of track).</p><p>In order to achieve high coverage of tasks, tasks were developed by examining the corresponding to the query pages of WikiHow and the query suggestions provided by commercial search engines. Further, a task named "other" was added in the case participants submitted a relevant subtask not covered by the gold standard set. An example of a constructed topic can be seen below: build a hydroponic garden [How do I construct a hydroponic garden and grow plants?]</p><p>• What are the different types of hydroponic systems to choose from, their pros and cons?</p><p>• What hydroponic system are used in commercial greenhouses?</p><p>• What material is needed to build a hydroponic system myself?</p><p>• Which plants can be grown in a hydroponics system?</p><p>• How long does it take to grow them?</p><p>• What nutrients to test the water for?</p><p>• How to prepare the seeds?</p><p>• How often to water the plants?</p><p>• What plant food to add and how often?</p><p>• What are the light and temperature requirements?</p><p>Given the gold standard tasks, each key phrase submitted by the participants were judged by NIST assessors with respect to each of the gold standard tasks by using a three level judging scheme:</p><p>• Highly relevant (2): The key phrase completely describes the task and could be used as a query to a search engine to complete the task.</p><p>• Relevant (1):</p><p>The key phrase somehow describes the task but not fully, it can be used as a query to achieve the task but there are better queries than that.</p><p>• Non-relevant (0): The key phrase is not relevant to the task and cannot be used to complete it.</p><p>Similar to 2015 and 2016, the quality of each ranked list was evaluated using diversity metrics such as ERR-IA [1] and α-NDCG <ref type="bibr" coords="3,129.32,428.22,10.58,8.64" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Task Completion</head><p>The aim of this evaluation goal is to test the usefulness of a retrieval system in helping users complete their search task. Participants had to retrieve a ranked list of up to 1000 documents, to be evaluated by a three level "usefulness" judging scheme (key, useful, and not useful), and by four level "topical relevance" judging scheme (highly relevant, relevant, not relevant, and spam).</p><p>In 2017 no team participated in task completion hence no test collection was constructed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Adhoc Retrieval</head><p>For comparison purposes, we continued to have a traditional Web ad-hoc evaluation mechanism this year as well <ref type="bibr" coords="3,525.90,568.79,10.58,8.64" target="#b1">[2]</ref>.</p><p>Participants were asked to submit a ranked list of up to 1000 documents for each topic. Participants were provided a short description of query along with query text and Freebase ID of entities in query.</p><p>In 2017 no team participated in task completion hence no test collection was constructed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Participants and Runs</head><p>This year four runs were submitted in total from two participating groups: Beijing University of Technology (BJUT), and Siena College Institute for AI (SienaCLTeam). All the runs were task understanding runs, so only the key phrase qrels was constructed.</p><p>4 Evaluation Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task Understanding</head><p>Task understanding runs were evaluated depth-30 pools of key phrases. Each key-phrase was labeled using judging guidelines described in Section 2.1. This year 3623 keywords were evaluated across 50 queries, of which 1787 were non-relevant, 1586 were relevant to at least one task, and 623 were highly relevant to at least one task by the assessors.</p><p>We evaluate each task understanding submission with α-NDCG@20 and ERR-IA@20, where ERR-IA@20 is the primary metric. We choose depth-20 so that numbers are comparable (to some extent) with the previous two years of the Tasks track. For each run, we report the average α-NDCG@20 and ERR-IA@20 for all topics. Participant results for task understanding are shown in 1, where evaluation results are sorted on ERR-IA@20 in descending order. This year 4 runs were submitted compared to 12 submitted in 2016 <ref type="bibr" coords="4,368.86,324.44,11.62,8.64" target="#b2">[3]</ref> and to 11 submitted in 2015 <ref type="bibr" coords="4,504.22,324.44,10.58,8.64" target="#b3">[4]</ref>. The maximum ERR@20 and α-NDCG@20 in 2015 were 0.471 and 0.573 respectively, in 2016 they were 0.57 and 0.70, respectively, while this year they are 0.51 (SienaCLTeamRun1) and 0.64 (BJUT_1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis on Task Understanding from 201to 2017</head><p>In this section we present a few results that span the three years of the TREC Tasks Track, and in particular the Task Understanding task. Figure <ref type="figure" coords="4,184.98,416.20,4.98,8.64" target="#fig_0">1</ref> displays the α-nDCG@20 scores for all the participating runs over the three years. To the best of our knowledge none of the participants froze their developed system to reuse it in consequent years, hence it is not easy to reach any conclusions from the presented results.</p><p>What has been apparent from the three years of the Task Understanding task is that it is hard for assessors to judge the relevance of a key-phrase to a task. Further, the collection of judged key-phrases may not even be a re-usable one given that in 2015 73% of the assessed key-phrases were unique, in 2016 30% and in 2017 50%, which indicates that different systems return by and large different key-phrases, hence future systems will probably return key-phrases for which no label is available. Hence, the one question that we attempted to answer was whether we can pivot on document labels to evaluate key-phrases.</p><p>To answer the afore-mentioned question we built an Indri index using Krovetz stemmer and removing stopwords, and employed the standard Indri Language Model with Dirichlet smoothing (µ = 5000). Each produced key-phrase was run against the index and the top-k results where considered. If a returned document was relevant (or useful) for a task then the key-phrase was considered relevant (useful) for that task. Hence, we constructed a key-phrase qrel using the document qrel. Figure <ref type="figure" coords="4,204.41,571.62,4.98,8.64" target="#fig_1">2</ref> demonstrates the correlation between system ERR-IA@20 performance using the assessors key-phrase qrels and the key-phrase qrels built on the document qrels on the basis of relevance (left) and usefulness (right). As one can observe there is a clear correlation between the scores, however in both cases this correlation, as measured by Kendall's τ is less than 0.8, which is considered a rather strong correlation in the field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>The TREC 2017 Tasks Track ran for a third year to build test collections to evaluate retrieval systems on relevance and usefulness of retrieved documents for a given user search task. We organized the track with three tasks: task  understanding, completion and ad-hoc retrieval. We observe a significant drop in number of participants and the number of runs submitted this year, with 4 runs submitted in task understanding, while task completion and ad-hoc retrieval received no runs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,72.00,96.84,468.00,8.96;5,72.00,109.11,21.85,8.64;5,142.20,139.11,327.60,183.06"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: α-nDCG@20 on Task Understanding for all participating systems over 2015 (blue), 2016 (green) and 2017 (red).</figDesc><graphic coords="5,142.20,139.11,327.60,183.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,72.00,388.53,468.00,8.64;5,72.00,400.49,162.17,8.64;5,75.43,430.48,229.32,226.39"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Correlation between system performance using the key-phrase qrels built on the document qrels on the basis of relevance (left) and usefulness (right).</figDesc><graphic coords="5,75.43,430.48,229.32,226.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,169.10,224.58,273.79,80.37"><head>Table 1 :</head><label>1</label><figDesc>Task understanding results</figDesc><table coords="4,169.10,246.58,273.79,58.37"><row><cell>Group</cell><cell>Run</cell><cell cols="2">ERR-IA@20 αNDCG@20</cell></row><row><cell>BJUT</cell><cell>BJUT_1</cell><cell>0.452</cell><cell>0.628</cell></row><row><cell>BJUT</cell><cell>BJUT_2</cell><cell>0.469</cell><cell>0.644</cell></row><row><cell cols="2">SienaCLTeam SienaCLTeamRun1</cell><cell>0.317</cell><cell>0.502</cell></row><row><cell cols="2">SienaCLTeam SienaCLTeamRun2</cell><cell>0.511</cell><cell>0.624</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,86.35,604.80,186.50,5.61"><p>https://developers.google.com/freebase/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,88.60,155.28,451.40,8.64;6,88.60,167.05,451.40,8.82;6,88.60,179.19,22.42,8.64" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,354.78,155.28,181.21,8.64">Expected reciprocal rank for graded relevance</title>
		<author>
			<persName coords=""><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metlzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Grinspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,99.68,167.05,342.24,8.59">Proceedings of the 18th ACM conference on Information and knowledge management</title>
		<meeting>the 18th ACM conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="621" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,88.60,199.11,451.41,8.64;6,88.60,210.89,145.08,8.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,346.22,199.11,174.83,8.64">Overview of the trec-2012 microblog track</title>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,88.60,210.89,21.48,8.59">TREC</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">2012</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,88.60,230.99,451.41,8.64;6,88.60,242.95,208.79,8.64" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="6,119.89,242.95,147.76,8.64">Overview of the trec 2015 tasks track</title>
		<author>
			<persName coords=""><forename type="first">Manisha</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rishabh</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Bailey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,88.60,262.87,451.40,8.64;6,88.60,274.83,177.49,8.64" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,380.27,262.87,159.73,8.64;6,88.60,274.83,147.76,8.64">Nick Craswell, and Rishabh Mehrotra. Overview of the trec 2015 tasks track</title>
		<author>
			<persName coords=""><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manisha</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
