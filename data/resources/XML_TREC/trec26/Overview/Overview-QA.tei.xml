<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,82.17,149.24,430.93,15.15;1,200.45,171.16,194.37,15.15">Overview of the Medical Question Answering Task at TREC 2017 LiveQA</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,115.19,205.08,109.13,10.52"><forename type="first">Asma</forename><forename type="middle">Ben</forename><surname>Abacha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">U.S. National Library of Medicine</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,237.27,205.08,103.15,10.52"><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Emory University</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>GA (</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,353.37,205.08,74.50,10.52"><forename type="first">Yuval</forename><surname>Pinter</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,452.03,205.08,28.06,10.52;1,243.25,219.03,104.04,10.52"><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">U.S. National Library of Medicine</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,82.17,149.24,430.93,15.15;1,200.45,171.16,194.37,15.15">Overview of the Medical Question Answering Task at TREC 2017 LiveQA</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B1A95B438D12E7D3E4E36A6E67104F9A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an overview of the medical question answering task organized at the TREC 2017 LiveQA track. The task addresses the automatic answering of consumer health questions received by the U.S. National Library of Medicine. We provided both training question-answer pairs, and test questions with reference answers 1 . All questions were manually annotated with the main entities (foci) and question types. The medical task received eight runs from five participating teams. Different approaches have been applied, including classical answer retrieval based on question analysis and similar question retrieval. In particular, several deep learning approaches were tested, including attentional encoder-decoder networks, long short-term memory networks and convolutional neural networks. The training datasets were both from the open domain and the medical domain. We discuss the obtained results and give some insights for future research in medical question answering.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The LiveQA track at TREC started in 2015 <ref type="bibr" coords="1,292.20,533.17,11.52,9.57" target="#b1">[2]</ref> focusing on answering user questions in real time. The medical QA task was introduced in 2017 based on questions received by the U.S. National Library of Medicine (NLM).</p><p>The NLM is the world's largest biomedical library, conducting research, development, and training in biomedical informatics and health information technology. The NLM receives more than 100,000 requests a year, including over 10,000 consumer health questions. The medical task at TREC 2017 LiveQA was organized in the scope of the CHQA project 2 which addresses the classification of customers' requests and the automatic answering of Consumer Health Questions (CHQs).</p><p>CHQs cover a wide range of questions on diseases, drugs, or medical procedures (e.g. Information, Treatment, Comparison, Cause, Usage, Tapering). The question below presents a concrete example of a CHQ looking for treatments of "retinitis pigmentosa":</p><p>• Example: Subject: ClinicalTrials.gov -Compliment. Message: Hi I have retinitis pigmentosa for 3years. Im suffering from this disease.</p><p>Please intoduce me any way to treat mg eyes such as stem cell....I am 25 years old and I have only central vision. Please help me. Thank you Several efforts at the NLM focused on the construction of relevant resources by manually annotating relevant question elements such as the foci and question types <ref type="bibr" coords="2,456.56,276.30,11.52,9.57" target="#b7">[8,</ref><ref type="bibr" coords="2,469.40,276.30,7.68,9.57" target="#b8">9]</ref>. Other research efforts tackled the automatic analysis of consumer health questions <ref type="bibr" coords="2,456.87,289.85,11.52,9.57" target="#b3">[4,</ref><ref type="bibr" coords="2,470.34,289.85,8.49,9.57" target="#b6">7,</ref><ref type="bibr" coords="2,480.77,289.85,13.94,9.57" target="#b10">11,</ref><ref type="bibr" coords="2,496.67,289.85,12.73,9.57" target="#b12">13]</ref>.</p><p>A closely related research area addresses health care-related questions in the context of community-based question answering <ref type="bibr" coords="2,262.76,316.95,16.97,9.57" target="#b9">[10,</ref><ref type="bibr" coords="2,281.15,316.95,13.94,9.57" target="#b14">15,</ref><ref type="bibr" coords="2,296.51,316.95,12.73,9.57" target="#b16">17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head><p>The medical task focuses on providing automatic answers to medical questions. Participants were challenged with retrieving relevant answers to consumer health questions. Two more examples of such questions are presented below. The first CHQ asks about a Problem ("abetalipoproteimemia") and includes more than one subquestion (Diagnosis and Management). The second CHQ includes one subquestion asking about the Ingredients of a Drug (Kapvay).</p><p>• CHQ 1: Subject: abetalipoproteimemia Message: hi, I would like to know if there is any support for those suffering with abetalipoproteinemia? I am not diagnosed but have had many test that indicate I am suffering with this, keen to learn how to get it diagnosed and how to manage, many thanks</p><p>• CHQ 2: Subject: ingredients in Kapvay Message: Is there any sufites sulfates sulfa in Kapvay? I am allergic.</p><p>Question Analysis. One of the main approaches to question answering is extracting the relevant question elements that can lead to correct answers, such as the question focus and type <ref type="bibr" coords="2,107.10,673.80,10.91,9.57" target="#b4">[5]</ref>. Other approaches rely on retrieving similar or equivalent questions which were previously answered <ref type="bibr" coords="3,180.91,109.86,16.00,9.57" target="#b11">[12]</ref>. Consumer health questions may contain multiple foci and question types. Users can also describe general and background information such as their medical history before asking their questions, which increases the number of potentially irrelevant medical entities mentioned in the question.</p><p>Answer Retrieval. If the question contains more than one subquestion, complete answers should cover all subquestions.</p><p>For the medical domain, we suggested the use of trusted medical websites to find relevant answers such as Pubmed abstracts and NIH websites (e.g. ninds.nih.gov, rarediseases. info.nih.gov, cancer.gov). In LiveQA'17, participants were free to use other answer sources such as Quora, Wikipedia or medical websites where doctors answer online questions (e.g. icliniq.com).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Training Datasets</head><p>We provided two training sets with 634 pairs of medical questions and answers. We also provided additional annotations for the Question Focus and the Question Type used to define each subquestion. Training questions cover four categories of foci (Disease, Drug, Treatment and Exam) and 23 question types (e.g. Treatment, Cause, Indication, dosage).</p><p>The first training dataset consists of 388 (sub)question-answer pairs corresponding to 200 NLM questions. Figure <ref type="figure" coords="3,216.63,398.93,5.45,9.57" target="#fig_0">1</ref> presents an example from this training dataset.</p><p>Each question is divided into one or more subquestion(s). Each subquestion has one or more answer(s). QA pairs were constructed from FAQs on trusted websites of the U.S. National Institutes of Health (NIH). Candidate question-answer pairs were retrieved using automatic matching between the CHQs and the FAQs based on the focus and the question type. The QA pairs retained for training are the manually validated pairs from the candidate set.</p><p>The second training dataset consists of 246 question-answer pairs corresponding to 246 NLM questions. Answers were retrieved manually by librarians using PubMed and web search engines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Test Dataset</head><p>Test Questions. The test set consists of 104 NLM questions. The subquestion, focus and type annotations were not provided to the participants. For each medical question, participants were tasked to retrieve a correct answer for each subquestion. If the question includes more than one subquestion, answers should be ranked according to the order of the subquestions.</p><p>We selected the test questions to cover a wide range of question types (26) and have a slightly different distribution than the training questions in order to evaluate the scalability of the proposed systems. Section 4.1 describes in more details the question types and the Reference Answers. For each test question, we manually collected one or more reference answer(s) from trusted sources such as NIH websites. NIST assessors created question paraphrases/interpretations after reading both the original questions and the reference answers. They used the paraphrases with the reference answers to judge the participants' answers. Below is an example of a consumer health question with the associated reference answer:</p><p>• Question Subject: Can cancer spread through blood contact • Question Message: Sir, after giving an insulin injection to my uncle who is a cancer patient the needle accidentally pined my finger. Is there a problem for me? Plz reply.</p><p>• Reference Answer: A healthy person cannot "catch" cancer from someone who has it.</p><p>There is no evidence that close contact or things like sex, kissing, touching, sharing meals, or breathing the same air can spread cancer from one person to another. Cancer cells from one person are generally unable to live in the body of another healthy person. A healthy person's immune system recognizes foreign cells and destroys them, including cancer cells from another person.</p><p>• Answer URL: https://www.cancer.org/cancer/cancer-basics/is-cancer-contagious.html</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Additional Annotations: Foci, Question Types and Keywords</head><p>We provided additional annotations (Foci/Question Types/Keywords) after the challenge, for future efforts and evaluations. The examples below present some of the provided annotations: Foci are highlighted in blue, question types and their triggers in red and keywords in green: • Provided Annotations: Q-Focus fid="F1" fcategory="Problem": EDS Q-Focus fid="F2" fcategory="Problem": Osteogenesis Imperfecta Q-Type tid="T1" hasFocus="F1,F2": COMPARISON Q-Type tid="T2" hasFocus="F1": DIAGNOSIS Q-Type tid="T3" hasFocus="F1" hasKeyword="K1": PERSON ORGANIZATION Q-Type tid="T4" hasFocus="F2": DIAGNOSIS Q-Type tid="T5" hasFocus="F2" hasKeyword="K1": PERSON ORGANIZATION Q-Keyword kid="K1" kcategory="GeographicLocation": Southern California</p><p>Annotating the test questions also allowed us to provide more detailed statistics about the test set. Figures <ref type="figure" coords="5,180.91,665.99,17.34,9.57" target="#fig_2">2, 3</ref> and<ref type="figure" coords="5,222.62,665.99,5.45,9.57" target="#fig_3">4</ref> present the types of test questions, as well as the categories associated with the foci and keywords. These categories and types can help improving the scalability of question analysis methods and the coverage of answer resources used for the medical domain. Figure <ref type="figure" coords="6,199.09,136.96,5.45,9.57" target="#fig_4">5</ref> presents an example from the annotated test dataset.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Submissions and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Submissions</head><p>Five teams participated in the medical QA task with eight runs in total. Table <ref type="table" coords="8,465.74,155.21,5.45,9.57" target="#tab_1">1</ref> presents the participating teams and the submitted runs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>We use the same scoring scheme as the main TREC LiveQA challenge <ref type="bibr" coords="8,421.52,401.41,11.52,9.57" target="#b0">[1,</ref><ref type="bibr" coords="8,434.45,401.41,7.68,9.57" target="#b1">2]</ref>:</p><p>• avgScore [0-3 range]: the average score over all questions, transferring 1-4 level grades to 0-3 scores. This is the main score used to rank LiveQA runs.</p><p>• succ@i+: the number of questions with score i or above (i∈2,4) divided by the total number of questions.</p><p>• prec@i+: the number of questions with score i or above (i∈2,4) divided by number of questions answered by the system.</p><p>The results presented in this section use the number of questions which were answered by all systems (102) as the total number of questions, instead of the original number of test questions (104). Table <ref type="table" coords="8,195.21,559.22,5.45,9.57" target="#tab_2">2</ref> presents the Average Score and Success. CMU-OAQA achieved the best Average Score of 0.637. Table <ref type="table" coords="8,270.24,572.77,5.45,9.57" target="#tab_3">3</ref> presents the Precision results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>The LiveQA track has been running since 2015 at TREC. This year, the medical QA task was introduced focusing on consumer health questions. The proposed test questions cover a wide range of question types and have a slightly different distribution than the training questions to allow evaluating the performance and scalability of the proposed systems. The CMU-OAQA system <ref type="bibr" coords="9,225.73,434.91,16.97,9.57" target="#b13">[14]</ref> achieved the best performance of 0.637 on the medical task. They used an attentional encoder-decoder model for paraphrase identification and answer ranking. Quora question-similarity dataset was used for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participant</head><p>The PRNA system <ref type="bibr" coords="9,194.00,475.56,11.52,9.57" target="#b5">[6]</ref> achieved the second best performance in the medical task with 0.49 avgScore (prna-r1). They used Wikipedia as the first answer source and Yahoo and Google searches as secondary answer sources. Each medical question was decomposed into several subquestions. Then wikipages associated with the question focus were used as candidate answer sources. To extract the answer from the selected text passage, a bidirectional attention model trained on the SQUAD dataset was used.</p><p>Another technique was used by ECNU-ICA team <ref type="bibr" coords="9,341.73,556.85,11.52,9.57" target="#b2">[3]</ref> based on learning question similarity via two long short-term memory (LSTM) networks applied to obtain the semantic representations of the questions. To construct a collection of similar question pairs, they searched community question answering sites such as Yahoo! and Answers.com.</p><p>The CMU-LiveMedQA team <ref type="bibr" coords="9,238.28,611.05,16.97,9.57" target="#b15">[16]</ref> designed a specific system for the medical task. Using only the provided training datasets and the assumption that each question contains only one focus, the CMU-LiveMedQA system obtained an avgScore of 0.353. They used a convolutional neural network (CNN) model to classify a question into a restricted set of 10 question types and crawled "relevant" online web pages to find the answers.</p><p>Although the number of submitted runs ( <ref type="formula" coords="10,304.07,109.86,4.65,9.57">8</ref>) is limited, the tested methods and their results can give some insights and directions. For instance, Open-domain datasets (e.g. Quora, Wikipedia) were helpful for the medical domain. The best system on the medical task (CMU-OAQA) with 0.637 avgScore used only the Quora training dataset. It is also relevant to note that the same system and training data obtained a score of 1.139 on the LiveQA open-domain task <ref type="bibr" coords="10,211.17,177.61,10.91,9.57" target="#b0">[1]</ref>. These two results support the relevance of similar question matching for the end-to-end QA task. The gap in performance between the open domain and the medical domain can be explained in part by the discrepancies between the medical test questions and the open-domain questions used for training.</p><p>In contrast, the ECNU-ICA system achieved the best performance of 1.895 in the opendomain task but an average score of only 0.402 in the medical task (ECNU ICA 2). As the ECNU-ICA approach also relied on a neural network for question matching, this result shows that training attention-based decoder-encoder networks on the Quora dataset generalized a lot better to the medical domain than training LSTMs on similar questions from Yahoo! and Answers.com.</p><p>More generally, the current gap in performance between the open-domain task and the medical task supports the need for larger medical datasets to support deep learning approaches in dealing with the linguistic complexity of consumer health questions and the challenge of finding correct and complete answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We described the medical QA task organized at the TREC 2017 LiveQA Track. Two training datasets with pairs of NLM questions and relevant answers were provided. We have also provided reference answers as well as additional annotations of the test questions. All datasets were publicly released to support research efforts in medical question answering <ref type="foot" coords="10,505.88,451.17,4.23,6.99" target="#foot_0">3</ref> . The task attracted five teams that submitted eight runs. Different approaches relying on attention based decoders, LSTMs and CNNs were tested and compared. The obtained results highlight the difficulty of the medical QA task in comparison with open-domain QA, and provide pointers for future research and development.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,152.29,384.21,290.69,9.57;4,81.64,106.35,500.50,261.80"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Annotated example from the first training dataset.</figDesc><graphic coords="4,81.64,106.35,500.50,261.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,140.99,359.62,308.46,9.57;6,81.64,158.49,447.12,185.07"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Questions types covered by the medical test questions.</figDesc><graphic coords="6,81.64,158.49,447.12,185.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,116.74,594.23,356.94,9.57;6,81.64,393.10,447.12,185.07"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Categories associated with the foci of the medical test questions.</figDesc><graphic coords="6,81.64,393.10,447.12,185.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,103.37,307.48,383.70,9.57;7,81.64,106.35,447.12,185.07"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Categories associated with the keywords of the medical test questions.</figDesc><graphic coords="7,81.64,106.35,447.12,185.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,173.53,647.08,248.21,9.57;7,81.64,327.02,494.00,304.00"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Annotated example from the test dataset.</figDesc><graphic coords="7,81.64,327.02,494.00,304.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,88.28,192.69,398.43,156.53"><head>Table 1 :</head><label>1</label><figDesc>LiveQA 2017 Medical Task: Participating teams and submitted runs</figDesc><table coords="8,335.91,192.69,86.72,9.60"><row><cell>Country</cell><cell>Run</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,101.44,108.76,381.16,279.72"><head>Table 2 :</head><label>2</label><figDesc>Official Results of the Medical QA Task: Average Score and Success</figDesc><table coords="9,101.44,108.76,369.07,279.72"><row><cell></cell><cell cols="2">AvgScore [0-3]</cell><cell>S@2</cell><cell>S@3</cell><cell>S@4</cell></row><row><cell>CMU-OAQA</cell><cell>0.637</cell><cell></cell><cell>0.392</cell><cell>0.265</cell><cell>0.098</cell></row><row><cell>prna-r1</cell><cell>0.490</cell><cell></cell><cell>0.265</cell><cell>0.157</cell><cell>0.069</cell></row><row><cell>prna-run2</cell><cell>0.441</cell><cell></cell><cell>0.275</cell><cell>0.137</cell><cell>0.059</cell></row><row><cell>prna-run3</cell><cell>0.431</cell><cell></cell><cell>0.284</cell><cell>0.147</cell><cell>0.059</cell></row><row><cell>ECNU ICA 2</cell><cell>0.402</cell><cell></cell><cell>0.216</cell><cell>0.127</cell><cell>0.059</cell></row><row><cell>CMU-LiveMedQA</cell><cell>0.353</cell><cell></cell><cell>0.216</cell><cell>0.137</cell><cell>0</cell></row><row><cell>ECNU ICA 1</cell><cell>0.255</cell><cell></cell><cell>0.225</cell><cell>0.147</cell><cell>0.029</cell></row><row><cell>ECNU</cell><cell>0.137</cell><cell></cell><cell>0.216</cell><cell>0.088</cell><cell>0.01</cell></row><row><cell>Participant</cell><cell></cell><cell>P@2</cell><cell>P@3</cell><cell>P@4</cell></row><row><cell>CMU-OAQA</cell><cell></cell><cell>0.404</cell><cell>0.273</cell><cell>0.101</cell></row><row><cell>prna-r1</cell><cell></cell><cell>0.429</cell><cell>0.254</cell><cell>0.111</cell></row><row><cell>prna-run2</cell><cell></cell><cell>0.394</cell><cell>0.197</cell><cell>0.085</cell></row><row><cell>prna-run3</cell><cell></cell><cell>0.397</cell><cell>0.205</cell><cell>0.205</cell></row><row><cell>ECNU ICA 2</cell><cell></cell><cell>0.268</cell><cell>0.159</cell><cell>0.073</cell></row><row><cell cols="2">CMU-LiveMedQA</cell><cell>0.218</cell><cell>0.139</cell><cell>0</cell></row><row><cell>ECNU ICA 1</cell><cell></cell><cell>0.228</cell><cell>0.149</cell><cell>0.03</cell></row><row><cell>ECNU</cell><cell></cell><cell>0.216</cell><cell>0.088</cell><cell>0.01</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,154.93,401.91,285.43,9.57"><head>Table 3 :</head><label>3</label><figDesc>Official Results of the Medical QA Task: Precision</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="10,98.23,614.87,254.20,7.47"><p>https://github.com/abachaa/LiveQA_MedicalTask_TREC2017</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank <rs type="person">Sonya Shooshan</rs> for her help with the reference answers and <rs type="institution">NIST Assessors</rs> for judging participants' answers. We are also thankful to <rs type="person">Ellen Voorhees</rs> as well as all organizers and participants for their valuable efforts and support.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="11,104.06,136.06,409.57,8.33;11,104.06,147.77,409.58,9.57;11,104.06,161.32,353.17,9.57" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,104.06,147.77,199.01,9.57">Overview of the TREC 2017 liveqa track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Harman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Pinter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,329.63,147.77,184.02,9.57;11,104.06,161.32,100.04,9.57">Proceedings of The Twenty-Sixth Text REtrieval Conference</title>
		<meeting>The Twenty-Sixth Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,104.06,183.83,409.57,10.18;11,104.06,197.38,409.58,9.57;11,104.06,210.93,303.94,9.57" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,469.06,183.83,44.57,9.57;11,104.06,197.38,146.70,9.57">Overview of the TREC 2015 liveqa track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Pelleg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Pinter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,274.76,197.38,238.88,9.57;11,104.06,210.93,50.82,9.57">Proceedings of The Twenty-Fourth Text REtrieval Conference</title>
		<meeting>The Twenty-Fourth Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,104.06,235.29,409.58,8.33;11,104.06,246.99,409.58,10.18;11,104.06,260.54,409.58,9.57;11,104.06,274.09,303.94,9.57" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,170.31,246.99,343.33,9.57;11,104.06,260.54,158.90,9.57">Ecnu at 2017 liveqa track: Learning question similarity with adapted long short-term memory networks</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,285.44,260.54,228.20,9.57;11,104.06,274.09,50.82,9.57">Proceedings of The Twenty-Sixth Text REtrieval Conference</title>
		<meeting>The Twenty-Sixth Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,104.06,296.61,409.57,10.18;11,104.06,310.16,409.57,9.57;11,104.06,323.71,276.36,9.57" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,342.40,296.61,171.24,9.57;11,104.06,310.16,126.47,9.57">Recognizing question entailment for medical question answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,251.37,310.16,262.27,9.57;11,104.06,323.71,89.40,9.57">AMIA 2016, American Medical Informatics Association Annual Symposium</title>
		<meeting><address><addrLine>Chicago, IL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11">November (2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,104.06,346.22,409.57,10.18;11,104.06,359.77,409.57,9.57;11,104.06,373.32,267.94,9.57" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,323.63,346.22,190.01,9.57;11,104.06,359.77,316.12,9.57">MEANS: A medical question-answering system combining NLP techniques and semantic web technologies</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zweigenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,431.28,359.77,82.36,9.57;11,104.06,373.32,158.94,9.57">Information Processing and Management Journal</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="570" to="594" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,104.06,397.68,409.57,8.33;11,104.06,409.38,409.58,10.18;11,104.06,422.93,409.58,9.57;11,104.06,436.48,111.90,9.57" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,303.65,409.38,168.24,9.57">Prna at the TREC 2017 liveqa track</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Adduru</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,492.72,409.38,20.92,9.57;11,104.06,422.93,270.77,9.57">Proceedings of The Twenty-Sixth Text REtrieval Conference</title>
		<meeting>The Twenty-Sixth Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,104.06,459.00,409.57,10.18;11,104.06,472.55,409.57,9.57;11,104.06,486.10,33.33,9.57" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,314.20,459.00,199.44,9.57;11,104.06,472.55,246.23,9.57">Classifying Chinese Questions Related to Health Care Posted by Consumers Via the Internet</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,360.65,472.55,125.13,9.57">JMIR medical informatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,104.06,510.46,409.58,8.33;11,104.06,522.16,409.58,10.18;11,104.06,535.71,409.58,9.57;11,104.06,549.26,345.52,9.57" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,347.23,522.16,166.41,9.57;11,104.06,535.71,107.54,9.57">Annotating named entities in consumer health questions</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kilicoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Mrabet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Shooshan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,234.26,535.71,279.38,9.57;11,104.06,549.26,210.99,9.57">Proceedings of the Tenth International Conference on Language Resources and Evaluation LREC 2016</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation LREC 2016<address><addrLine>Portorož, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,104.06,573.62,409.58,8.33;11,104.06,585.33,409.57,10.18;11,104.06,598.87,305.49,9.57" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,354.48,585.33,159.16,9.57;11,104.06,598.87,75.71,9.57">Semantic annotation of consumer health questions</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kilicoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Mrabet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Shooshan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Masterton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,189.32,598.87,99.71,9.57">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,104.06,621.39,409.57,10.18;11,104.06,634.94,378.10,9.57" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,351.10,621.39,162.54,9.57;11,104.06,634.94,261.32,9.57">Question Popularity Analysis and Prediction in Community Question Answering Services</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,374.78,634.94,48.81,9.57">PLoS One</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,104.06,657.45,409.57,10.18;11,104.06,671.00,409.58,9.57;12,104.06,109.86,409.57,9.57;12,104.06,123.41,207.20,9.57" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,472.73,657.45,40.91,9.57;11,104.06,671.00,409.58,9.57;12,104.06,109.86,42.38,9.57">Combining open-domain and biomedical knowledge for topic recognition in consumer health questions</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Mrabet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kilicoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,169.75,109.86,343.89,9.57;12,104.06,123.41,20.74,9.57">AMIA 2016, American Medical Informatics Association Annual Symposium</title>
		<meeting><address><addrLine>Chicago, IL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11">November (2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,104.06,147.77,409.58,8.33;12,104.06,159.48,409.58,10.18;12,104.06,173.03,409.58,9.57;12,104.06,186.58,376.43,9.57" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,306.45,159.48,207.19,9.57;12,104.06,173.03,45.08,9.57">Semeval-2016 task 3: Community question answering</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Magdy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mubarak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Freihat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Randeree</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,170.68,173.03,342.97,9.57;12,104.06,186.58,138.17,9.57">Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2016</title>
		<meeting>the 10th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2016<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">June 16-17, 2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,104.06,209.09,409.58,10.18;12,104.06,222.64,409.58,9.57;12,104.06,236.19,409.57,9.57;12,104.06,249.74,144.83,9.57" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,467.27,209.09,46.36,9.57;12,104.06,222.64,293.15,9.57">Automatically classifying question types for consumer health questions</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kilicoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fiszman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,419.97,222.64,93.67,9.57;12,104.06,236.19,280.24,9.57">AMIA 2014, American Medical Informatics Association Annual Symposium</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">November 15-19, 2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,104.06,272.26,409.58,10.18;12,104.06,285.80,409.58,9.57;12,104.06,299.35,377.75,9.57" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,249.98,272.26,263.65,9.57;12,104.06,285.80,226.32,9.57">Cmu oaqa at trec 2017 liveqa: A neural dual entailment approach for question paraphrase identification</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,355.71,285.80,157.92,9.57;12,104.06,299.35,124.63,9.57">Proceedings of The Twenty-Sixth Text REtrieval Conference</title>
		<meeting>The Twenty-Sixth Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,104.06,321.87,409.58,10.18;12,104.06,335.42,409.57,9.57;12,104.06,348.97,383.63,9.57" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,471.93,321.87,41.71,9.57;12,104.06,335.42,409.57,9.57;12,104.06,348.97,190.62,9.57">A Semi-Supervised Learning Approach to Enhance Health Care Community-Based Question Answering: A Case Study in Alcoholism</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wongchaisuwat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klabjan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R</forename><surname>Jonnalagadda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,304.57,348.97,124.55,9.57">JMIR medical informatics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,104.06,371.48,409.57,10.18;12,104.06,385.03,409.58,9.57;12,104.06,398.58,409.58,9.57;12,104.06,412.13,33.33,9.57" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,400.03,371.48,113.61,9.57;12,104.06,385.03,289.24,9.57">Cmu livemedqa at trec 2017 liveqa: A consumer health question answering system</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,421.83,385.03,91.81,9.57;12,104.06,398.58,192.00,9.57">Proceedings of The Twenty-Sixth Text REtrieval Conference</title>
		<meeting>The Twenty-Sixth Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,104.06,434.65,409.57,10.18;12,104.06,448.20,409.58,9.57;12,104.06,461.74,107.38,9.57" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,424.89,434.65,88.75,9.57;12,104.06,448.20,404.72,9.57">A Topic Clustering Approach to Finding Similar Questions from Large Question and Answer Archives</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,104.06,461.74,48.81,9.57">PLoS One</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
