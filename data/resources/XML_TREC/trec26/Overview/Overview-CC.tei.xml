<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,153.88,112.60,304.25,14.93">TREC 2017 Common Core Track Overview</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,183.63,144.91,56.83,10.37"><forename type="first">James</forename><surname>Allan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Authors are</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,248.38,144.91,70.43,10.37"><forename type="first">Donna</forename><surname>Harman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Authors are</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,327.73,144.91,95.29,10.37"><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Authors are</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,197.53,158.86,31.98,10.37"><forename type="first">Dan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Authors are</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,237.04,158.86,102.77,10.37"><forename type="first">Christophe</forename><surname>Van Gysel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Authors are</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,347.95,158.86,66.52,10.37;1,414.47,156.86,1.41,6.99"><forename type="first">Ellen</forename><surname>Vorhees</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Authors are</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,153.88,112.60,304.25,14.93">TREC 2017 Common Core Track Overview</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">81AD4D6590A8624509E0F4328E4683A8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>in alphabetical order. 1 Participants have the opportunity to train their systems over the TREC 2004 Robust track (which, to some extend, alters the participants task to a routing task, i.e. fixed queries and updated corpus).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The primary goal of the TREC Common Core track is three-fold: (a) bring the information retrieval community back into a traditional ad-hoc search task; (b) attract a diverse set of participating runs and build a new test collections using more recently created documents; (c) establish a (new) test collection construction methodology that avoids the pitfalls of depth-k pooling. A number of side-goals are also set, including studying the shortcomings of test collections constructed in the past; experimenting with new ideas for constructing test collections; expand test collections by new participant tasks (ad-hoc/interactive), new relevance judgments (binary/multilevel), new pooling methods, new assessment resources (NIST / crowd-sourcing) and new retrieval systems contributing documents (manual/neural/strong baselines).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head><p>The participants task of the track is a traditional ad-hoc retrieval task. The organizers of the track provided participants with the title/description/narrative of TREC topics and allowed participating sites to run the experiment as they wish as long as they contribute a ranked list of documents as an output.</p><p>3 Test Collection</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Corpus</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Topics</head><p>The topics provided are an updated version of the TREC 2004 Robust track. Most of the topics remain the same as in the 2004 Robust track, but some were revised to reflect the time past (e.g. some descriptions and narratives were turned into historical topics, while others were brought up to date). 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">NIST Relevance Judgments</head><p>Two of the goals of the track were to 1) build a new ad hoc test collection by 2) exploring new collection construction methodologies. The original TREC ad hoc collections were built using depth-k pooling, which can create good collections but does so at a relatively high cost. The desiderata for the new methodology is that it build reusable collections (i.e., a collection that is fair even to systems that did not contribute to the building process) at minimal cost.</p><p>Multi-arm bandit methods have been demonstrated to be a cost-effective approach to building quality collections in simulations with existing TREC collections <ref type="bibr" coords="2,258.36,153.93,80.35,8.64" target="#b1">[Losada et al., 2016]</ref>. The general form of a bandit method for a single topic is as follows. A document selector process starts with an empty set of judgments and the ranked list of documents from each of a set of contributing runs. The selector picks the first document in the ranking of some run and receives a (binary) relevance judgment for that document. Based on the relevance judgments obtained so far and the ranks at which those documents appear in the runs, the selector chooses the next document to be judged. Bandit methods differ in how they choose this next document-whether to choose the subsequent document from the current run or to choose a different run. However, all of the methods are constrained to select the first (closest to rank 1) unjudged document from whichever run has been selected at each step. The idea behind the bandit methods is to focus assessing resources on those runs that continue to contribute relevant documents.</p><p>A particular bandit method, <ref type="bibr" coords="2,203.47,261.53,80.78,8.64" target="#b1">Losada et al. [2016]</ref>'s "MaxMean" or "MM" (stationary) method, was used in the Common Core track. In this method, each run is assigned a weight after each judgment is obtained, and the first unjudged document in the run with the highest weight is selected as the next document to be judged. The weight of a run is equal to 1+num-relevant-retrieved 2+num-judged . Whenever a document is judged, the number of relevant documents retrieved and the number of documents judged statistics are updated for all runs that retrieved that document, not just the run from which it was selected. For these purposes, a run has "retrieved" a document if the document appears in the top X ranks; we used X = 100. A run is randomly chosen from among those runs with the highest weight when there are ties.</p><p>Bandit methods are dynamic methods in that they require a relevance judgment for the current document to select another document to be judged. The logistical challenges in implementing dynamic methods for actual use (as opposed to simulation) is one reason why NIST had not used them to build collections in the past. But there are two other concerns as well: bias against systems that do not have good early precision and bias in assessor judgments since assessors see documents in rank order.</p><p>The assessor bias concern remains; we do not know of any way to control for assessor bias when using a dynamic method nor a way to gauge its effect (if any). Assessors were not told they would be seeing documents in any particular order, but did quickly figure out that they were seeing the best documents first. However, the results of a series of experiments on past collections detailed below suggest that the bandit methods are as fair to all runs participating in the judgment process as other collection-building methods. Further experiments demonstrated the way forward through the logistical challenges.</p><p>These experiments were carried out on two collections: the TREC-8 ad hoc collection and the collection built from the TREC 2005 Robust and HARD track submissions. The TREC-8 collection is among the most-studied IR test collections and is a very solid collections. It has more than 80,000 judgments across 50 topics and was built using depth-100 pooling over a strong set of runs. The TREC 2005 collection has just under 38,000 judgments, also across 50 topics, and was constructed using depth-55 pooling. This collection was the collection that first demonstrated the limits of pooling <ref type="bibr" coords="2,143.18,553.14,86.92,8.64" target="#b0">[Buckley et al., 2007]</ref>, in particular that a sufficient pooling depth depends on collection size and so must grow as collection size grows. Among the runs used to construct the collection is a run that contributed a large percentage of unique relevant documents. "Leave one out" tests show significant bias against this run when its uniquely retrieved relevant are omitted from the existing judgment sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Bias against "slow" runs</head><p>By design, bandit methods concentrate assessing resources on runs that have been demonstrated to deliver relevant documents. Since the bandit methods start from the first ranks of a set of runs and work their way down the ranked lists, the fear is that bandit methods could not find pockets of relevant documents in runs whose top documents were not relevant. .996</p><p>Table <ref type="table" coords="3,96.64,224.70,3.88,8.64">1</ref>: Kendall τ values between rankings produced using original qrels and rankings produced using test qrels for depth-k pooling (Pool), MaxMean bandit sampling (MM) and inferred measures sampling (Inferred). The size of the depth-k pool determines the budgets for the other two methods, and k is constrained by the original collection's pool depth.</p><p>To test whether the concern was well-founded, we ran a series of simulations of different collection-building techniques using the two existing collections and compared the quality of the resulting judgments sets (called qrels). The existing full-collection qrels ("original qrels") is taken as ground truth. The quality of qrels formed by other methods ("test qrels") was measured by how closely a ranking of runs produced by the test qrels matched the ranking produced by the original qrels. The similarity of two rankings was computed as the Kendall τ score between them.</p><p>The experiments compared qrels built using depth-k pooling, qrels built through sampling in support of inferred measures <ref type="bibr" coords="3,111.21,364.34,79.35,8.64" target="#b2">[Yilmaz et al., 2008]</ref>, and qrels built using the MaxMean bandit method for a range of values of k. To control for the total number of documents being judged, the depth-k pools were first constructed, and then the bandit method was given a budget equal to the size of the pool for that topic. The total number of documents in the pools over all 50 topics was given to the inferred-sampling method as its budget. This is somewhat unfair to the inferred-sampling method since the pool and bandit methods had variable budgets per topic while the inferred-sampling method had an equal budget per topic. The inferred-sampling method always used the same two-strata sampling strategy of judging all of the documents in the top 10 ranks (i.e., top-10 pools) plus a sample of documents in ranks 11-100 at the rate determined by maximizing the total number of judgments obtained while staying within the overall budget.</p><p>Table <ref type="table" coords="3,110.74,459.98,3.99,8.64">3</ref>.3.1 shows the τ values obtained using system rankings based on MAP (or infAP) scores for the different test qrels produced at different depths (k). The set of runs being ranked was the set of all runs (both judged and unjudged) that were submitted to the ad hoc task for TREC-8 and to the Robust track for TREC 2005. The τ values shown for the inferred-sampling method are means over 10 different trials where a given trial is based on an independent sample of documents to be judged. (The original qrels contain documents from runs other than those used in the simulations. So the depth-55 pools for the TREC 2005 collection in the simulations are not identical to the original qrels.)</p><p>The test qrels are all able to rank runs much the same as the original qrels. For the TREC-8 collection, the MaxMean method is somewhat better than the inferred-sampling method which is better than depth-k pooling, but even depth-10 pools rank systems very similarly as the original depth-100 pools. There is more of a difference for the TREC 2005 collection, and here the inferred-sampling method is somewhat better than the MaxMean method. But both methods still create qrels that rank systems very similarly as the original, especially for pool depths that are likely to be used in practice, demonstrating that fears of bias against unjudged runs for the MaxMean method are unfounded for these collections. This could be because the set of runs submitted to these tracks just did not exhibit this behavior, or because the behavior is truly rare, or because even though some runs do have this behavior other runs compensate for them, or since evaluation measures concentrate on top ranks system comparisons are unaffected even when the behavior is present, or other reasons and combinations of reasons. In any event, there is no discernible bias against judged runs by the MaxMean method in these tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Logistical challenges</head><p>Using the MaxMean method to actually build a new collection presents logistical challenges not encountered in simulations. For all dynamic methods, a judgment once obtained is assumed to be immutable and has instantaneous effect on which documents are subsequently selected. But human assessors need a "burn-in" period to learn the nuances of a topic. During a burn-in period, the assessor may make changes to past judgments to improve consistency with their current understanding of the topic. To accommodate a burn-in period, the track did not use a 'pure' MaxMean method, but instead first created depth-10 pools that assessors judged normally, and then used the judged pool as the initial set of judgments for the MaxMean bandit process. In addition to providing a burn-in period for the assessor, the top-10 pools allowed the track to guarantee that all teams would at least get judgments for the top ten documents in their judged runs. The top-10 pools also provided a means for setting per-topic assessment budgets, which was the second logistical challenge.</p><p>Simulations avoid the problem of deciding how to allocate an overall budget among individual topics by simply changing the order in which documents from some larger pool are encountered. But when building the collection from scratch, there needs to be some concrete decision as to when the dynamic method will stop. A simple technique is to give each topic an equal share of the overall budget, but this is known to be suboptimal. Different topics need more or fewer judgments depending on run overlap and total number of relevant documents, and a final qrels can be substantially improved if topics needing the larger number of judgments receive larger budgets. Another approach is to apply a bandit method to the question of which topic to judge next and then select the next document for that topic. In simulations, this dual-selection process does indeed generate very good qrels for a given overall budget, but it has a fatal flaw. A given topic needs to be judged by a single assessor and assessors should concentrate on one topic at a time because constant context switches both increase the time it takes to make a judgment and decreases the quality of those judgments. At NIST, multiple assessors work on their own set of topics at different times and at different rates, but the budget for one topic depends on the budget given to all the rest. So NIST decided to set the budgets for all topics at one time at the start of the dynamic process.</p><p>We used the information from the judged top-10 pools to create the individual topic budgets. For past collections, we have not only the judgments themselves, but also the ranks at which each relevant document was retrieved in a judged run and the documents in the top-10 pools. Using the ranks at which the relevant documents appear, we can determine the minimal number of documents to select from each run for each topic, subject to the constraint that all selections must start at the top of a run's document ranking, to get the maximum number of relevant documents from the original qrels that are obtainable given the overall budget. We call this an optimal qrels-optimal for any bandit method with the given budget, though likely inferior to the original qrels. There may be more than one optimal qrels for a budget, and any given optimal qrels may have multiple different selections from runs to obtain it. Using the minimal number of judgments per topic required to obtain an optimal qrels, we fit a model to predict how many judgments a topic should receive as a function of three statistics from top-10 pools: the overall pool size, the number of relevant documents in the pool, and the number of nonrelevant documents in the pool. This process demonstrated</p><formula xml:id="formula_0" coords="4,72.00,499.01,129.31,22.87">topfactor t = |pool t | |nonrel-in-pool t |</formula><p>to be a simple, effective factor for selecting topic t's budget. For the Common Core track's collection, once the top-10 pools were judged, we computed the topfactor for each topic and summed them to produce a normalizing factor, T . The residual budget remaining after pool judging was allocated to the individual topics by assigning ( topfactor t T × residual) judgments as topic t's budget. A third logistical challenge this year was introduced by having two separate run submission deadlines for the track separated by approximately one month. Two-thirds of the NIST assessing time was scheduled to occur during the month between deadlines, with the remaining third following the second deadline. Thus runs submitted at the earlier deadline could potentially receive many more judgments than runs submitted at the later deadline, even if the earlier runs were less effective. Yet another set of experiments was run on the TREC-8 collection to gauge the effect of unbalanced assessment time for different run sets on the quality of the final qrels.</p><p>In these experiments, an overall assessment budget B was picked and the TREC-8 judged runs were randomly assigned to either the "early" set or the "later" set. Top-10 pools were created from the runs in the early set and individual topic budgets were assigned as described above using a budget of 2/3 × B. The MaxMean method was run until those individual topic budgets were exhausted. The top-10 pools over the combined set of runs (early and later runs) was then produced and individual topic budgets produced using these pools and budget B. The MaxMean process was given the combined runs' top-10 pools and budgets, and began anew using the combined run set. When a document selected to be judged was already judged (because it was in the pool or in the early runs' bandit processing) the statistics for the runs were updated normally but the budget was not decreased. The MaxMean continued in this vein until the topic budgets were exhausted. The entire process was repeated 50 times using 50 different independent splits of runs into early and later runs.</p><p>A test qrels that resulted from this two-stage process was compared to the original qrels by computing the root mean square error (RMSE) of the number of relevant documents for each topic in each qrels. Given the variance in the number of relevant documents across topics, RMSE can be misleading since a qrels can have a relatively small RMSE if it contains many relevant documents for large topics even if it contains no relevant documents for small topics. Manual inspection of the results suggests that this did not happen, however. When using a budget of B = 36, 000, the RMSE values ranged from 9.98-19.63 indicating at most a modest impact of the the split of runs on final qrels quality. The larger RMSE's occurred when the early set contained few (less than 10) runs. This year's Common Core track received 41 runs by the early deadline and 34 runs at the later deadline, so the impact of the two-stage process should be negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Implementation</head><p>As noted, the track received a total of 75 runs from 14 participant teams and a pseudo-team called "BASELINE". Teams could submit up to 10 runs total and at most 3 runs by the early deadline. All 41 runs that were received at the first deadline contributed to the judging process in the first stage. At most 5 runs per team contributed to the judgment process in the second stage, resulting in a total of 55 judged runs. Several runs that were submitted at the early deadline were mistakenly omitted from the second round of judging when they were replaced by new runs from the same team. This means the qrels file released for the track contains about 50 relevant documents that were not retrieved by any of the runs that are officially designated as judged runs (where "retrieved" means in the top 100 ranks).</p><p>The server implementing the MaxMean method was developed by Aldo Lipani when he was visiting NIST. This version of the server accepts an arbitrary set of judgments as an initial qrels and adds to that qrels after receiving a judgment. Any document selected to be judged that is already in the qrels is not returned to the assessor for a second time. This version also enforces that top-k pools are judged for all runs that are participating in the process, and limits the depth that will be mined from a single run to no more than X. For the Common Core track, we used k = 10 and X = 100.</p><p>NIST estimated the total number of assessment that could be obtained in the time allocated for Common Core track judging to be between 30,000-35,000, so the budget set in the first stage was 22,000. However, assessing went more slowly than estimated, which was further complicated by the fact that the use of MaxMean meant that there was much less flexibility to move topics between assessors and that faster assessors had to wait for slower assessors to finish pool judgments so the bandit budgets could be set. The result was only about 15,000 judgments were made in the first stage, and while all 50 topics had at least some judgments, the number of judgments per topic reflected only whether an assessor had gotten to the topic. NIST was able to obtain more assessing time for the track for the second stage. Individual topic budgets were set based on estimates of the number of nonrelevant there would be in the combined top-10 pools using the first stage judgments to make the estimates, and then allocated as described above. With the additional time, the target number of 30,000 judgments was reached. Nonetheless, the total number of judgments for a topic in the final qrels has been skewed by the imbalance in the first stage, and so the final count does not accurately reflect the intended relative budgets for topics as predicted by topfactor.</p><p>In addition to the top-10 pools, the static judgment sets that were assessed at the beginning of the second stage also contained documents from an inferred-measure sample. For this sample, the two strata were again a top-10 pool (conveniently already judged) and a 20% sample of the documents retrieved in ranks 11-75. This sample was added so that a direct comparison can be made between the MaxMean and inferred measure approaches on this new collection (this analysis has not yet been done). This is another reason why the qrels released for the track do not reflect a pure MaxMean sampling approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Crowd-sourced Assessments</head><p>The Information and Language Processing Systems group at the University of Amsterdam, the Center for Intelligent Information Retrieval at the University of Massachusetts at Amherst, and the Web &amp; Media group at the VU University Amsterdam, funded a crowd-sourcing experiment with the goal of amending the NIST judgments by providing judgments both for the 50 NIST queries, and for the remaining 200 Robust track queries.</p><p>At the time of the writing of this paper a pilot crowd-sourcing experiment has been conducted to fine-tune the parameters of actual experiment to follow. The micro-task for the pilot experiment is simple: the title and the description of a topic is provided to a crowd-worker, along with a NYT article, with the crowd-worker asked to tag the document as either highly relevant, relevant or not relevant. For the pilot experiment a total of 300 topic-article pairs were selected. Articles were split into 5 bins on the basis of their length, and 60 articles were chosen from each bin; different payments were given to crowd-workers for the different bins with bin 1 and 2 paying 0.02$, bin 3 0.03$, and bin 4 and 5 0.04$. Quality metrics computed by the CrowdTruth framework (http://crowdtruth.org/) were positive, hence the full experiment is ready to start.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Evaluation Measures</head><p>Retrieval Effectiveness: The retrieval effectiveness was measured by traditional evaluation metrics produced by trec_eval. For the track three measures were selected as official measures, Average Precision, Precision@10, and nDCG. Further, the effectiveness of the systems biased the document selection method both in NIST and in Crowdsourced judgments.</p><p>Unique Documents Contribution: The number of unique documents contributed to the pool was not incorporated in any evaluation metric, but rather measured in a post-hoc analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Submitted Runs</head><p>As noted earlier the track received a total of 75 runs from 14 teams, out of which 55 runs contributed to the pool. Runs varied between automatic and manual, making use of the Robust 2004 track judgments or not.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis</head><p>In what follows we perform a preliminary analysis regarding the submitted runs, the 50 NIST topics, and the relevance judgments obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Uniquely identified relevant documents</head><p>Table <ref type="table" coords="8,97.44,436.95,4.98,8.64">3</ref> shows the number of uniquely retrieved relevant documents per team. Here, unique relevant document is defined using the following criteria: 1. consider only the top 100 ranks as the retrieved set; 2. a document retrieved by multiple runs of the same group is counted as unique as long as no other group retrieved it; 3. only judged runs are used in the computation; 4. total is over all topics and all judged runs of the group. Table <ref type="table" coords="8,111.07,484.77,24.61,8.64">reftab</ref>:unique-run shows the number of uniquely retrieved relevant documents per run. Here, unique relevant document is defined using the following criteria: 1. consider only the top 100 ranks as the retrieved set; 2. a document retrieved by multiple runs of the same group is counted as unique as long as no other group retrieved it; 3. only judged runs are used in the computation; 4. total is over all topics of each single run of the group. This really means that the table counts how many unique relevant documents a team would have contributed if they had just submitted the one run under consideration. Table reftab:unique-run also categorizes runs on the basis of manual vs. automatic and use of TREC Robust track labels or not.</p><p>Last, the numbers in Table <ref type="table" coords="8,202.10,568.46,4.98,8.64" target="#tab_2">5</ref> are computed in the same was as in Table <ref type="table" coords="8,390.37,568.46,3.74,8.64">4</ref>, however this time only automatic runs without any use of Robust track labels were considered, to illustrate a situation in which no prior judgments is available, while at the same time manual effort is completely avoided. sab17coreA and IlpsUvANvsm contribute most of the unique relevant documents in this situation, with the latter run using a latent neural algorithm. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Manual vs. Automatic with and without use of past labels</head><p>Figure <ref type="figure" coords="10,100.98,339.98,4.98,8.64" target="#fig_1">2</ref> demonstrates the mean performance of the contributing to the pool runs measured by MAP, colored on the basis of their categorization, while Figure <ref type="figure" coords="10,240.14,351.94,4.98,8.64" target="#fig_2">3</ref> summarizes the performance of the four categories of runs with a boxplot of MAP. As it can be observed in both figures, Manual runs and runs that make use of the Robust labels outperform on average the fully automatic runs, as expected. Interestingly enough, manual runs that make use of labeled data perform on average worse than manual runs that do not make use of labels, however no safe conclusions can be drawn here given that many other parameters and algorithms change across the different groups of runs.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Analysis of Topics</head><p>A total of 30030 query-document pairs were judged by NIST assessors, with 21028 of them found not relevant, 5549 found relevant, and 3453 highly relevant. On average 30% of the documents were found relevant or highly relevant.</p><p>Figure <ref type="figure" coords="12,100.50,355.67,4.98,8.64" target="#fig_3">4</ref> shows the percentage of relevant and highly relevant documents per topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Analysis of Judgments</head><p>Leave-one-out (LOO) experiments are one measure of collection quality. In a LOO experiment, the unique relevants for a target team are removed from the qrels and all runs are evaluated using this reduced qrels. The Kendall's τ correlation between the ranking of systems produced by the original qrels and the ranking of systems produced by the reduced qrels is then computed. Table <ref type="table" coords="12,230.19,447.42,4.98,8.64">7</ref> shows the results for the Common Core track when each participating team in turn is used as the target and MAP is the measure used to rank systems. The table also gives the number of unique relevant documents retrieved by the team. The final column in the table shows how individual runs were affected. For example, '-4 -3 -2 -2 -2 -1' means one run dropped four spots in the reduced qrels ranking, another dropped three spots, three runs dropped two spots, and a final run dropped one spot. Only the drops are listed (when one run drops at least one other must rise). The Kendall's τ scores are all well above 0.95 showing there is not much change in the rankings overall. Nonetheless, five runs dropped by seven or more spots, and one run by 18. For each of these five runs, the run was submitted by the team whose unique relevants were removed. Given are the number of unique relevant documents retrieved by a team over all 50 topics and the Kendall's τ correlation between system rankings formed when evaluating all submissions by MAP using the original qrels and the qrels formed by removing the team's unique relevants. The last column shows how many ranks some run dropped when evaluated using the reduced qrels. The largest drops were runs submitted by the team whose unique relevants were removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>The TREC 2017 Common Core track provided a traditional ad-hoc retrieval task to participants. The track attracted a diverse set of runs, including manual runs, but also state-of-the-art neural retrieval runs. A number of pooling methods were tested prior to the construction of the collection. Leave-one-out experiments yield a high value of Kendal's τ , nevertheless a small number of runs were penalized when they were left out of pooling. The manuscript will be updated with further analysis, and the results of the on-going crowdsourcing experiment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,189.06,323.49,233.87,8.64;8,72.00,72.00,468.00,236.06"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Box-plot of AP values over the 50 NIST queries.</figDesc><graphic coords="8,72.00,72.00,468.00,236.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="11,72.00,313.38,468.00,8.64;11,72.00,325.34,164.73,8.64;11,118.80,102.74,374.39,195.22"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Scatter plot of MAP values over the 50 NIST queries. Each circle mark represents a run. The figure includes only the runs that contributed to the pool.</figDesc><graphic coords="11,118.80,102.74,374.39,195.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="11,72.00,614.87,468.00,8.64;11,72.00,626.82,468.00,8.64;11,72.00,638.78,468.00,8.64;11,72.00,650.74,381.43,8.64;11,118.80,403.42,374.41,196.02"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Box-plot of MAP values over the 50 NIST queries. Each box-plot represents the MAP values of a set of runs under the same category: (a) automatic, without making use of past labels (Auto-NoFdbk), (b) automatic, making use of past labels (Auto-Fdbk), (c) manual, without making use of past labels (Manual-NoFdbk), and (d) manual, making use of past labels (Manual-Fdbk). The figure includes only the runs that contributed to the pool.</figDesc><graphic coords="11,118.80,403.42,374.41,196.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="12,159.70,281.03,292.61,8.64;12,118.80,72.00,374.40,193.60"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Percentage of relevant and highly relevant documents per topic.</figDesc><graphic coords="12,118.80,72.00,374.40,193.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,72.00,470.94,468.00,20.59"><head>Table 2 :</head><label>2</label><figDesc>Table 2 demonstrated the mean performance of the 75 submitted runs over the 50 NIST topics, while Figure5also shows the varying performance as measured by MAP, for each one of the submitted runs. Evaluation scores over the 50 NIST topics.</figDesc><table coords="7,86.09,139.48,421.42,463.33"><row><cell>Participating Run</cell><cell cols="3">MAP NDCG PC@10</cell><cell>Participating Run</cell><cell cols="3">MAP NDCG PC@10</cell></row><row><cell>ICT17ZCJL01</cell><cell>0.18</cell><cell>0.404</cell><cell>0.442</cell><cell>ims_wcs_p10</cell><cell>0.267</cell><cell>0.513</cell><cell>0.598</cell></row><row><cell>ICT17ZCJL02</cell><cell>0.188</cell><cell>0.413</cell><cell>0.448</cell><cell>ims_wcs_rbp</cell><cell>0.268</cell><cell>0.513</cell><cell>0.604</cell></row><row><cell>ICT17ZCJL03</cell><cell>0.148</cell><cell>0.371</cell><cell>0.4</cell><cell>ims_wcs_recall</cell><cell>0.259</cell><cell>0.509</cell><cell>0.586</cell></row><row><cell>ICT17ZCJL05</cell><cell>0.141</cell><cell>0.354</cell><cell>0.372</cell><cell>ims_wcs_rprec</cell><cell>0.265</cell><cell>0.512</cell><cell>0.588</cell></row><row><cell>ICT17ZCJL06</cell><cell>0.159</cell><cell>0.375</cell><cell>0.42</cell><cell>ims_wcs_twist</cell><cell>0.265</cell><cell>0.512</cell><cell>0.592</cell></row><row><cell>ICT17ZCJL07</cell><cell>0.178</cell><cell>0.402</cell><cell>0.458</cell><cell cols="2">mpiik10e105akDT 0.166</cell><cell>0.314</cell><cell>0.55</cell></row><row><cell>IlpsUvABoir</cell><cell>0.286</cell><cell>0.515</cell><cell>0.57</cell><cell cols="2">mpiik10e111akDT 0.156</cell><cell>0.307</cell><cell>0.518</cell></row><row><cell>IlpsUvANvsm</cell><cell>0.126</cell><cell>0.332</cell><cell>0.322</cell><cell>mpiik15e74akDT</cell><cell>0.164</cell><cell>0.312</cell><cell>0.544</cell></row><row><cell>IlpsUvAQlmNvsm</cell><cell>0.171</cell><cell>0.412</cell><cell>0.418</cell><cell>sab17coreA</cell><cell>0.272</cell><cell>0.53</cell><cell>0.614</cell></row><row><cell>MRGrandrel</cell><cell>0.319</cell><cell>0.578</cell><cell>0.566</cell><cell>sab17coreE1</cell><cell>0.276</cell><cell>0.515</cell><cell>0.646</cell></row><row><cell>MRGrankall</cell><cell>0.354</cell><cell>0.599</cell><cell>0.642</cell><cell>sab17coreO1</cell><cell>0.298</cell><cell>0.537</cell><cell>0.614</cell></row><row><cell>MRGrankrel</cell><cell>0.361</cell><cell>0.604</cell><cell>0.65</cell><cell>sabchmergeav45</cell><cell>0.405</cell><cell>0.673</cell><cell>0.748</cell></row><row><cell>RMITFDMQEA1</cell><cell>0.212</cell><cell>0.441</cell><cell>0.516</cell><cell>sabchoiceaqv45</cell><cell>0.343</cell><cell>0.607</cell><cell>0.718</cell></row><row><cell cols="2">RMITRBCUQVT5M1 0.341</cell><cell>0.6</cell><cell>0.712</cell><cell>sabchoicev45</cell><cell>0.395</cell><cell>0.661</cell><cell>0.74</cell></row><row><cell>RMITUQVBestM2</cell><cell>0.268</cell><cell>0.523</cell><cell>0.62</cell><cell>sabmerge50aqv45</cell><cell>0.416</cell><cell>0.676</cell><cell>0.762</cell></row><row><cell>UDelInfoEXPint</cell><cell>0.284</cell><cell>0.525</cell><cell>0.524</cell><cell>sabopt50av45</cell><cell>0.351</cell><cell>0.619</cell><cell>0.724</cell></row><row><cell>UDelInfoLOGext</cell><cell>0.252</cell><cell>0.492</cell><cell>0.53</cell><cell>sabopt50v45</cell><cell>0.376</cell><cell>0.645</cell><cell>0.696</cell></row><row><cell>UDelInfoLOGint</cell><cell>0.298</cell><cell>0.539</cell><cell>0.55</cell><cell>tgncorpBASE</cell><cell>0.255</cell><cell>0.51</cell><cell>0.526</cell></row><row><cell>UWatMDS_AFuse</cell><cell>0.434</cell><cell>0.685</cell><cell>0.706</cell><cell>tgncorpBOOST</cell><cell>0.276</cell><cell>0.535</cell><cell>0.594</cell></row><row><cell>UWatMDS_AUnion</cell><cell>0.397</cell><cell>0.653</cell><cell>0.654</cell><cell>udelIndri</cell><cell>0.232</cell><cell>0.483</cell><cell>0.526</cell></row><row><cell>UWatMDS_AWgtd</cell><cell>0.398</cell><cell>0.658</cell><cell>0.676</cell><cell>udelIndriB</cell><cell>0.232</cell><cell>0.483</cell><cell>0.526</cell></row><row><cell>UWatMDS_BFuse</cell><cell>0.441</cell><cell>0.694</cell><cell>0.73</cell><cell>umass_baselnrm</cell><cell>0.275</cell><cell>0.499</cell><cell>0.572</cell></row><row><cell>UWatMDS_BUnion</cell><cell>0.384</cell><cell>0.64</cell><cell>0.638</cell><cell>umass_baselnsdm</cell><cell>0.228</cell><cell>0.451</cell><cell>0.528</cell></row><row><cell>UWatMDS_BWgtd</cell><cell>0.416</cell><cell>0.663</cell><cell>0.738</cell><cell>umass_direlm</cell><cell>0.245</cell><cell>0.481</cell><cell>0.456</cell></row><row><cell>UWatMDS_HT10</cell><cell>0.408</cell><cell>0.666</cell><cell>0.462</cell><cell>umass_direlmnvs</cell><cell>0.23</cell><cell>0.473</cell><cell>0.466</cell></row><row><cell>UWatMDS_TARSv1</cell><cell>0.462</cell><cell>0.699</cell><cell>0.834</cell><cell>umass_diremart</cell><cell>0.251</cell><cell>0.498</cell><cell>0.462</cell></row><row><cell>UWatMDS_TARSv2</cell><cell>0.438</cell><cell>0.678</cell><cell>0.81</cell><cell>umass_emb1</cell><cell>0.235</cell><cell>0.476</cell><cell>0.53</cell></row><row><cell>UWatMDS_ustudy</cell><cell>0.32</cell><cell>0.566</cell><cell>0.654</cell><cell>umass_erm</cell><cell>0.289</cell><cell>0.517</cell><cell>0.57</cell></row><row><cell>WCrobust04</cell><cell>0.371</cell><cell>0.637</cell><cell>0.646</cell><cell>umass_letor_lm</cell><cell>0.259</cell><cell>0.493</cell><cell>0.538</cell></row><row><cell>WCrobust0405</cell><cell>0.428</cell><cell>0.696</cell><cell>0.75</cell><cell>umass_letor_lmn</cell><cell>0.244</cell><cell>0.485</cell><cell>0.492</cell></row><row><cell>WCrobust04W</cell><cell>0.366</cell><cell>0.63</cell><cell>0.658</cell><cell>umass_letor_m</cell><cell>0.274</cell><cell>0.514</cell><cell>0.512</cell></row><row><cell>ims_bm25_td</cell><cell>0.243</cell><cell>0.487</cell><cell>0.57</cell><cell cols="2">umass_maxpas150 0.213</cell><cell>0.438</cell><cell>0.484</cell></row><row><cell>ims_cmbsum</cell><cell>0.252</cell><cell>0.503</cell><cell>0.572</cell><cell>umass_maxpas50</cell><cell>0.19</cell><cell>0.411</cell><cell>0.426</cell></row><row><cell>ims_dfrinl2_td</cell><cell>0.243</cell><cell>0.485</cell><cell>0.574</cell><cell>webis_baseline</cell><cell>0.066</cell><cell>0.148</cell><cell>0.366</cell></row><row><cell>ims_wcmbsum_ap</cell><cell>0.268</cell><cell>0.515</cell><cell>0.59</cell><cell>webis_baseline2</cell><cell>0.066</cell><cell>0.148</cell><cell>0.366</cell></row><row><cell>ims_wcs_ap_uf</cell><cell>0.019</cell><cell>0.125</cell><cell>0.016</cell><cell>webis_reranked</cell><cell>0.053</cell><cell>0.135</cell><cell>0.28</cell></row><row><cell>ims_wcs_err</cell><cell>0.267</cell><cell>0.514</cell><cell>0.6</cell><cell>webis_reranked2</cell><cell>0.053</cell><cell>0.135</cell><cell>0.28</cell></row><row><cell>ims_wcs_ndcg</cell><cell>0.261</cell><cell>0.51</cell><cell>0.584</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,72.00,73.88,468.00,224.01"><head>Table 5 :</head><label>5</label><figDesc>Total number of unique relevant documents per automatic run (making no use of past labels) over 50 NISTjudged topics.</figDesc><table coords="10,114.08,73.88,367.60,190.16"><row><cell cols="2">Automatic run w/o labels unique</cell><cell cols="2">Automatic run w/o labels unique</cell></row><row><cell>umass_baselnrm</cell><cell>14</cell><cell>udelIndri</cell><cell>22</cell></row><row><cell>umass_baselnsdm</cell><cell>7</cell><cell>udelIndriB</cell><cell>22</cell></row><row><cell>ims_bm25_td</cell><cell>39</cell><cell>umass_direlm</cell><cell>25</cell></row><row><cell>ims_dfrinl2_td</cell><cell>45</cell><cell>umass_direlmnvs</cell><cell>88</cell></row><row><cell>ICT17ZCJL01</cell><cell>53</cell><cell>umass_diremart</cell><cell>84</cell></row><row><cell>ICT17ZCJL02</cell><cell>65</cell><cell>umass_maxpas150</cell><cell>44</cell></row><row><cell>ICT17ZCJL03</cell><cell>94</cell><cell>umass_maxpas50</cell><cell>96</cell></row><row><cell>IlpsUvANvsm</cell><cell>223</cell><cell>ICT17ZCJL06</cell><cell>79</cell></row><row><cell>IlpsUvAQlmNvsm</cell><cell>168</cell><cell>ICT17ZCJL07</cell><cell>74</cell></row><row><cell>ims_cmbsum</cell><cell>33</cell><cell>ims_wcs_ndcg</cell><cell>33</cell></row><row><cell>ims_wcmbsum_ap</cell><cell>34</cell><cell>ims_wcs_p10</cell><cell>37</cell></row><row><cell>ims_wcs_ap_uf</cell><cell>7</cell><cell>webis_baseline</cell><cell>43</cell></row><row><cell>RMITFDMQEA1</cell><cell>26</cell><cell>webis_baseline2</cell><cell>43</cell></row><row><cell>sab17coreA</cell><cell>240</cell><cell>webis_reranked</cell><cell>43</cell></row><row><cell></cell><cell></cell><cell>webis_reranked2</cell><cell>43</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="13,72.00,74.28,406.39,222.02"><head>Table 6 :</head><label>6</label><figDesc>LOO Results for Common Core Track Runs.</figDesc><table coords="13,133.61,74.28,344.77,200.32"><row><cell></cell><cell cols="2">Number Kendall's</cell><cell></cell></row><row><cell>Team</cell><cell>Unique</cell><cell>τ</cell><cell>Ranks Dropped</cell></row><row><cell>BASELINE</cell><cell>12</cell><cell>1.0</cell><cell></cell></row><row><cell>ICTNET</cell><cell>39</cell><cell>1.0</cell><cell></cell></row><row><cell>ims-core</cell><cell>8</cell><cell>0.999 -1</cell><cell></cell></row><row><cell>MPIID5</cell><cell>20</cell><cell>0.999 -1</cell><cell></cell></row><row><cell>MRG_UWaterloo</cell><cell>208</cell><cell cols="2">0.994 -2 -2 -1 -1 -1 -1 -1</cell></row><row><cell>RMIT</cell><cell>234</cell><cell>0.994 -8</cell><cell></cell></row><row><cell>Sabir</cell><cell>694</cell><cell cols="2">0.967 -18 -9 -4 -3 -2 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1</cell></row><row><cell>tgncorp</cell><cell>203</cell><cell>0.985 -11 -7 -2 -1</cell><cell></cell></row><row><cell>udel</cell><cell>1</cell><cell>1.0</cell><cell></cell></row><row><cell>udel_fang</cell><cell>36</cell><cell>0.999 -2</cell><cell></cell></row><row><cell>UMass</cell><cell>24</cell><cell>0.999 -1</cell><cell></cell></row><row><cell>UvA.ILPS</cell><cell>84</cell><cell>1.0</cell><cell></cell></row><row><cell>UWaterlooMDS</cell><cell>683</cell><cell cols="2">0.979 -3 -3 -3 -3 -3 -2 -2 -2 -2 -2 -2 -1 -1</cell></row><row><cell>WaterlooCormack</cell><cell>286</cell><cell cols="2">0.990 -4 -3 -2 -2 -2 -1</cell></row><row><cell>Webis</cell><cell>17</cell><cell>1.0</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0" coords="1,72.00,478.14,468.00,8.64;1,72.00,490.09,468.00,8.64;1,72.00,502.05,468.00,8.64;1,72.00,514.00,468.00,8.64;1,72.00,525.96,468.00,8.64;1,72.00,537.91,468.00,8.64;1,72.00,549.87,411.22,8.64"><p>The corpus used is the New York Times Annotated Corpus (https://catalog.ldc.upenn.edu/ldc2008t19). The corpus contains over 1.8 million articles written and published by the New York Times between January 1, 1987 and June 19, 2007 with article metadata provided by the New York Times Newsroom, the New York Times Indexing Service and the online production staff at nytimes.com. The text in this corpus is formatted in News Industry Text Format (NITF) developed by the International Press Telecommunications Council, an independent association of news agencies and publishers. NITF is an XML specification that provides a standardized representation for the content and structure of discrete news articles. NITF encompasses structural markup such as bylines, headlines and paragraphs.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="14,72.00,97.74,468.00,8.64;14,81.96,109.52,166.44,8.82" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,340.06,97.74,196.13,8.64">Bias and the limits of pooling for large collections</title>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Darrin</forename><surname>Dimmick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,81.96,109.52,85.07,8.59">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="491" to="508" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,72.00,129.62,468.00,8.64;14,81.96,141.40,331.35,8.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,294.60,129.62,245.40,8.64;14,81.96,141.58,108.72,8.64">Feeling lucky? Multi-armed bandits for ordering judgements in pooling-based evaluation</title>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Javier</forename><surname>Parapar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Barreiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,209.02,141.40,99.92,8.59">Proceedings of SAC 2016</title>
		<meeting>SAC 2016</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1027" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,72.00,161.50,468.00,8.64;14,81.96,173.28,458.03,8.82;14,81.96,185.24,300.47,8.82" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,316.19,161.50,223.81,8.64;14,81.96,173.46,57.10,8.64">A simple and efficient sampling method for estimating AP and NDCG</title>
		<author>
			<persName coords=""><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Javed</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,160.47,173.28,379.52,8.59;14,81.96,185.24,206.25,8.59">Proceedings of the Thirty-First Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2008)</title>
		<meeting>the Thirty-First Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2008)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="603" to="610" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
