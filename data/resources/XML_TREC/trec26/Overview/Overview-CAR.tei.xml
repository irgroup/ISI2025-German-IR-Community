<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,149.95,111.94,312.10,15.20">TREC Complex Answer Retrieval Overview</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,151.50,144.45,61.11,10.56;1,212.61,142.91,1.41,6.99"><forename type="first">Laura</forename><surname>Dietz</surname></persName>
							<email>dietz@cs.unh.edu</email>
						</author>
						<author>
							<persName coords="1,219.76,144.45,77.54,10.56"><forename type="first">Manisha</forename><surname>Verma</surname></persName>
						</author>
						<author>
							<persName coords="1,307.26,144.45,73.94,10.56"><forename type="first">Filip</forename><surname>Radlinski</surname></persName>
						</author>
						<author>
							<persName coords="1,390.22,144.45,70.28,10.56"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
						</author>
						<title level="a" type="main" coord="1,149.95,111.94,312.10,15.20">TREC Complex Answer Retrieval Overview</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C1DFEA79E9E11461AAB577A3A560E1C2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The SWIRL 2012 workshop on frontiers, challenges, and opportunities for information retrieval report <ref type="bibr" coords="1,529.49,260.85,10.51,8.80" target="#b0">[1]</ref> noted many important challenges. Among them, challenges such as conversational answer retrieval, subdocument retrieval, and answer aggregation share commonalities: We desire answers to complex needs, and wish to find them in a single and well-presented source. Advancing the state of the art in this area is the goal of this TREC track.</p><p>Consider a user investigating a new and unfamiliar topic. This user would often be best served with a single summary, rather than being required to synthesize his or her own summary from multiple sources. This is especially the case in mobile environments with restricted interaction capabilities. While these have led to extensive work on finding the best short answer, the target in this track is the retrieval of comprehensive answers that are composed of multiple text fragments from multiple sources. Retrieving high-quality longer answers is challenging as it is not sufficient to choose a lower rank-cutoff with the same techniques as for short answers. Instead, we need new approaches for finding relevant information in a complex answer space.</p><p>Many examples of manually created complex answers exist on the Web. Famous examples are articles from how-stuff-works.com, travel guides, or fanzines. These are collections of articles, that each constitutes a long answer to an information need represented by the title of the article.</p><p>The fundamental task of collecting references, facts, and opinions into a single coherent summary has traditionally been a manual process. We envision that automated information retrieval systems can relieve users from a large amount of manual work through sub-document retrieval, consolidation and organization. Ultimately, the goal is to retrieve synthesized information rather than documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head><p>While the long-term goal of this track is to retrieve complex answers without any more information than the given complex topic, in the first year we focus on a simpler task, where both the topic and an appropriate outline is provided as a query. An example outline is given in Figure <ref type="figure" coords="1,385.83,554.64,3.87,8.80" target="#fig_1">1</ref>. We run two tasks: passage and entity.</p><p>Passage Task: Given an outline for complex topic outline Q, retrieve for each of its sections H i , a ranking of relevant passages S.</p><p>Entity Task: Given outline for complex topic Q, retrieve for each of its sections H i , a ranking of relevant entities E and with support passages S. These support passage should motivate the why the entity is relevant for the query.</p><p>The passage S is taken from the provided passage corpus. The entity E refers to an entry in the provided knowledge base. We define a passage or entity as relevant if the passage content or entity is appropriate for the knowledge article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MUST be mentioned:</head><p>Sugar glass is made by dissolving sugar in water and heating it to at least the "hard crack" stage (approx. 150 C / 300 F) in the candy making process. Glucose or corn syrup is used to prevent the sugar from recrystallizing, by getting in the way of the sugar molecules forming crystals. Cream of tartar also helps by turning the sugar into glucose and fructose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CAN be mentioned:</head><p>Sinuklob is a very sweet candy that is also used in making Bukayo. It is the cheaper version of caramel in the Philippines. Sinuklob is made from melted brown sugar hardened into a chewy consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Roughly on TOPIC but non-relevant:</head><p>Most candies are made commercially. The industry relies significantly on trade secret protection, because candy recipes cannot be copyrighted or patented effectively, but are very difficult to duplicate exactly. Seemingly minor differences in the machinery, temperature, or timing of the candy-making process can cause noticeable differences in the final product.   The 2017 Complex Answer Retrieval track uses topics, outlines, and paragraphs that are extracted from English Wikipedia (XML dump from Dec 20th, 2016). Wikipedia articles are split into the outline of sections and the contained paragraphs. All paragraphs from all articles are gathered and deduplicated to form the paragraph corpus.</p><p>Each section outline is a description of a complex topic. By keeping the information which paragraph originates from which article and section, we have a means of providing a ground truth for the passage retrieval task. By preserving hyperlinks inside paragraphs that point to Wikipedia pages (also known as entities in the DBpedia knowledge graph), we have a means of providing a ground truth for the entity retrieval task.</p><p>Through filtering and processing procedures described in Section 3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Set Creation Pipeline</head><p>The TREC Complex Answer Retrieval benchmark (v1.5) is derived from Wikipedia so that complex topics are chosen from articles on open information needs, i.e., not people, not organizations, not events, etc. However, any paragraph or entity on Wikipedia is a legal paragraph/entity for the retrieval task even if a person entity or a paragraph from an article on an event. The data set creation process is as follows:</p><p>1. Mediawiki format of each article in the Wikipedia dump is parsed, preserving paragraph boundaries, intra-Wikipedia hyper links, and section hierarchy. 2. Templates, talk pages, portals, disambiguation, redirect, and category pages are discarded (redirect information is provided separately as entity-redirects). 3. Articles tagged with categories that indicate people, organizations, music, books, films, events, and lists are discarded. 4. Sections with headings that do not contain prose are discarded, for example external links, references, bibliography, notes, gallery etc. 5. Each article is separated into the outline of section headings on the one hand and paragraphs on the other hand. 6. The set of paragraphs across all of Wikipedia are collected, and uique paragraph IDs are derived through SHA256 hashes on the text content (ignoring links). 7. The paragraphs are further deduplicated with min hashing using word embedding vectors provided by GloVe. This is called the paragraphcorpus. Articles are rewritten to reference only the deduplicated set of paragraphs. 8. The set of articles is further filtered, to remove images, lead sections, sections with very long (&gt;100 character), and very short headings (&lt;3 letters). Articles with less than three remaining sections are discarded. 9. The set of articles is split into training and test data, training data is further split into five folds. To ensure uniform distribution and reproducibility, these decisions are made based on the SipHash of the article title. 10. The five folds of the training data, with separated outlines and paragraphs and extracted automatic qrels are made available as train. 11. For the benchmark, a total of 250 complex topics from English Wikipedia were manually selected, preferring topics that require a complex answer such as "Candy making", "Permaculture", "Soil Erosion", or "Air Ioniser". These were divided into train and test sets by training/test split defined previously in step 9. 12. Benchmark topics that are part of the training set are released with articles, outlines, and automatic ground truth as benchmarkY1train. 13. Benchmark topics that fall into the test set are only released as outlines as benchmarkY1test.public.</p><p>Official contributed runs to be submitted on these topics. Articles and automatic ground truth will be released as benchmarkY1test after the TREC workshop.</p><p>Pages from step 2, that would be in the training set (based on the SipHash of the page ID) are released as unprocessedtrain.</p><p>A dataset based on an earlier selection of 200 complex topics from training fold 0 is released with articles, outlines, and automatic ground truth as test200.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Automatic Ground Truth</head><p>Two kinds of ground truth signals are collected: automatic and manual. For each, we release true paragraphs and true entities. While the manual ground truth is assessed after participants submit runs, the automatic ground truth is derived along with the dataset from the Wikipedia dump. The automatic ground truth is released for all training sets and derived as follows.</p><p>• If a paragraph is contained in the page/section it is defined as relevant, and non-relevant otherwise. The ground truth signal is released for three granularities: paragraph contained in section (hierarchical), paragraph contained in section hierarchy below top level section (toplevel), paragraph contained anywhere in the page (article).</p><p>• After resolving redirects, if a hyperlink to an entity is contained in the page/section it is defined as relevant, and non-relevant otherwise. As with paragraphs, three granularities hierarchical, top level, and article are collected.</p><p>These six automatic ground truth files are released for train and benchmarkY1train before the submission, and for benchmarkY1test after the evaluation. Paragraphs that are relevant according to the automatic ground truth will be added to the pool for manual assessments (cf. Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Submission</head><p>For the passage ranking task, participants were asked to submit a ranking of paragraph IDs per heading in the outlines of benchmarkY1test. To participate in the entity task, participants submitted a ranking of entity IDs for each heading. To provide provenance and simplify the manual assessment, participants where asked to complement each entityId with a paragraph that explains why the entity is relevant for the corresponding section heading.</p><p>Participants were allowed to consider all headings in the outline at once, use external resources such as knowledge graphs, entity linking tools, pre-trained word embeddings, and any of the provided TREC CAR data sets. The participants were not allowed to directly use a dump of Wikipedia, as this would allow them to look up the paragraphs on the page-the information used in the automatic ground truth.</p><p>Each participating team was allowed to submit up to three runs to the passage task and three runs to the entity task. Seven teams participated in this first year of the track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Assessment of the Manual Ground Truth</head><p>The top elements of participant contributed runs were merged to build a pool of 50 paragraphs/entities per topic section. Additionally paragraphs relevant according to the ground truth were added to the pool for verification. For all 113 complex topics in benchmarkY1test, at least three sections (of the assessors choice) were annotated. A small set of topic sections were annotated by all assessors in order to measure interannotator agreement across the six NIST assessors (cf. Section 5). A screenshot of the assessment interface is given in Figure <ref type="figure" coords="4,151.07,431.69,3.87,8.80" target="#fig_3">3</ref>.</p><p>The assessor is presented with the complex topic (page title) and the topic sections as a heading hierarchy, followed by a list of paragraphs when assessing the passage task. In the case of the entity task, the list displayed the canonical entity names together with the provenance paragraph if given. In cases where participants did not submit provenance, the first paragraph from the entity's Wikipedia pages was displayed. In both cases, assessors were asked to judge the relevance of a paragraph or entity solely based on information displayed, and not resort to their world-knowledge. If more context would be needed to for the paragraph to become relevant, the assessors were instructed to assess it as non-relevant.</p><p>Assessors were asked to envision writing a Wikipedia article on the given complex topic. A graded assessment scale was used based on how much the paragraph/entity should be mentioned in this section of the article.</p><p>• MUST be mentioned • SHOULD be mentioned • CAN be mentioned • Non-relevant, but roughly on TOPIC of the page • NO, non-relevant • Trash Trash is assigned to paragraphs/entities that is of low quality therefore would not be relevant for any topic imaginable.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label distribution</head><p>Six assessors created 42,372 annotations on a total of 704 topic sections. For 702 topic sections passage assessments were created with 31,186 assessments in total. The grade histogram per annotator and the overall grade distribution is given in Table <ref type="table" coords="6,262.23,376.03,8.48,8.80" target="#tab_3">3a</ref>. We notice that only a quarter of all passages are graded as relevant, while an additional third were annotated as being on topic. 43% of passages in the assessment pool were marked as absolutely non-relevant. This demonstrates the feasibility of the task, while indicating that it is much easier to identify relevant for the general topic than for a particular aspect.</p><p>For 640 topics sections, entity assessments were created with 11,095 assessments in total. Histogram and distribution is given in Table <ref type="table" coords="6,203.92,435.80,8.85,8.80" target="#tab_3">3b</ref>. While the statistic seems to suggest that the entity retrieval problem is much harder, we believe that this is due to an issue with the submitted runs. One team indicated that a bug in the pipeline lead to random results. The remaining entity runs were derived from passage runs with some heuristics. An example of entities that are marked as relevant for the complex topic "Cocoa bean" is given in Figure <ref type="figure" coords="6,141.06,483.62,3.87,8.80" target="#fig_8">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inter-annotator agreement</head><p>A small number of topic sections are selected for annotation by all assessors to measure inter-annotator agreement. Two passage and two entity topic sections were annotated at the beginning of the assessment cycle, additionally three passage and one entity topic sections were annotated towards the end of the assessment period.</p><p>We measure inter-annotator agreement using Cohen's κ for pairwise comparison and Fleiss' κ across all annotators. We analyze agreement on the derived binarized assessment (Table <ref type="table" coords="6,449.40,589.67,9.22,8.80" target="#tab_4">4a</ref>) and the original graded assessment (Table <ref type="table" coords="6,184.81,601.63,8.58,8.80" target="#tab_4">4b</ref>). We are aware of the subtle difference between neighboring grades very subtle. Therefore, we additionally consider the case of graded assessment we call "off by one" in Table <ref type="table" coords="6,498.14,613.58,8.12,8.80" target="#tab_4">4c</ref>, where assessments that differ by no more than one grade step, e.g., grades "SHOULD" and "MUST", are also counted as agreements for both for p 0 and p e . Inspecting Cohen's κ, we find that on the whole the pair-wise agreement is relatively similar across all pairs of assessors. In other words, there is no "odd one out" which speaks to the quality of NIST's assessment procedures. As expected, the agreement for binarized judgments (Fleiss κ = 0.574) is higher than for graded judgments (Fleiss κ = 0.273). This may sound small, yet it is comparable to previous work <ref type="bibr" coords="6,481.33,685.31,9.96,8.80" target="#b1">[2]</ref>. However, once neighboring grades are counted as agreement ("off by one"), the inter-annotator agreement is even a bit higher agreement on binarized assessments. We conclude that, aside from subtle nuances in the grading scale, assessors agree on the whether the passage or entity should be included in the article on the complex topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Annotation Time</head><p>Over the course of two weeks, six assessors were hired for 40 hours each, yielding a total of 240 hours. Excluding breaks and training, the average annotation time per passage or entity judgments is 22 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Participant Submitted Runs</head><p>In total seven teams contributed runs. The majority of submissions were passage runs. Three teams submitted neural network ranking methods. Most methods are based on Lucene's BM25 ranking model as a candidate method. Some methods used entity linking, pre-trained word vectors, and other forms of query expansion. Table <ref type="table" coords="7,149.38,608.21,4.98,8.80">5</ref> gives an overview over the kinds of methods contributed by participating teams. Below, detailed descriptions of submitted runs:</p><p>• The MPIID5 submission includes three runs for passage retrieval, all of which are extensions of the PACRR <ref type="bibr" coords="7,135.22,673.96,10.51,8.80" target="#b4">[5]</ref>  </p><formula xml:id="formula_0" coords="8,162.91,165.43,286.18,105.24">X X Used neural network X X X Uses learning to rank X X X unsupervised X X Uses BM25 X X X X X X X Uses SDM X Language model X Passage Task X X X X X X X Entity Task X X</formula><p>Table <ref type="table" coords="8,249.97,283.79,3.87,8.80">5</ref>: Participant contributed runs.</p><p>heading position vector (pos), indicating if a given query term was from the title, intermediate heading, or bottommost heading. The hprec run includes a vector that models the prevalence of each heading in the training set. The tprob run includes a vector that estimates the likelihood that each query term is present in relevant paragraphs.</p><p>• The UTDHLTRI team developed the Complex Answer PAragraph Retrieval (CAPAR) system to perform complex answer retrieval consisting of the following five modules: (1) The Paragraph Indexing Module creates a searchable index of paragraphs from Wikipedia articles; (2) The Query Processing Module processes a Wikipedia article outline into a set of queries -one for each section of the outline;</p><p>(3) The Paragraph Search Module searches each query against the paragraph index, resulting in a list of relevant paragraphs for each section in the article outline; (4) The Feature Extraction Module is used to extract features from each paragraph; <ref type="bibr" coords="8,296.41,444.44,12.73,8.80" target="#b4">(5)</ref> The Paragraph Ranking Module produces a separate ranking of the retrieved paragraphs for each section. We use one of two Learning to Rank (L2R) systems to calculate relevance scores for each paragraph: (1) the Siamese Attention Network (SANet) for Pairwise Ranking and (2) AdaRank. A ranking is produced for each section of the article outline using the relevance scores from one of the two L2R systems. Their best run uses SANet which combines traditional IR features with a learned, attention-based semantic matching function.</p><p>• Team TREMA-UNH provided two passage runs and three entity runs. The passage runs are based on Lucene/Solr's BM25 method (k1 = 1.2, b = 0.75) using the page title, section heading and headings of parent sections as a query. The first method is just BM25 the second BM25 with uses query expansion. Expansion terms are selected through two sources: 1) Using the TagMe API <ref type="bibr" coords="8,505.43,559.89,10.51,8.80" target="#b3">[4]</ref> with the "include_abstract" option and only considering terms with the highest IDF score; 2) Based on ideas of Banerjee and Mitra <ref type="bibr" coords="8,224.04,583.80,9.96,8.80" target="#b2">[3]</ref>, in the provided collection of Wikipedia articles in training set ("un-processed_train"), content of sections with the same headings from other pages are extracted and identifying terms with the highest IDF terms. Up to 25 query expansion terms were selected, giving preference to terms selected through both sources. The entity runs are based on graph walks on the knowledge graph generated by co-mentions of entities.</p><p>• Team CUIS provided one passage and one entity run. The passage run uses Lucene's BM25 implementation to create a candidate set. This set is re-ranked using the sequential dependence model <ref type="bibr" coords="8,499.59,663.38,9.96,8.80" target="#b5">[6]</ref>. From this passage run, an entity run is derived by replacing the paragraph id with the containing article.</p><p>• Team NYUDL provides three passage runs using their neural retrieval network<ref type="foot" coords="8,430.98,693.59,3.97,6.16" target="#foot_0">1</ref>  <ref type="bibr" coords="8,437.76,695.15,9.96,8.80" target="#b7">[8]</ref>. The system consists of two stages. The first stage is query reformulation based on neural networks that rewrites a query to maximize the number of relevant documents returned. We train this neural network with reinforcement learning. The actions correspond to selecting terms to build a reformulated query, and the reward is the document recall. The second stage is a binary classifier based on a neural network that selects relevant documents. This classifier is trained with supervised learning. For all runs, BM25 is used to create a candidate set. The run "ds" is a simple document classifier using avg word embeddings in the documents as document vector and last hidden state of an LSTM as query vector. A 2-layer feed forward neural net is used to select which documents are relevant given a query. The run "qr" uses BM25 with query reformulation with deep reinforcement learning. The run "qrds" uses query reformulation using reinforcement learning + Lucene + Neural Net Classifier to select documents. All of these methods are trained with data provided in train-v1.5.</p><p>• Team ICTNET provides one passage run based on BM25.</p><p>• Team ECNU provides on passage run as follows: First, Lucene's BM25 is used to select candidate paragraphs, and then we use the BM25 score and word matching is used as features in a learning-torank framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>We evaluate participant-contributed runs on automatic and manual assessments with respect to four standard TREC evaluation measures, R-Precision (RPrec), Mean-average Precision (MAP), Reciprocal Rank (MRR), and Normalize Discounted Cumulative Gain (NDCG). Of these measures only NDCG includes a graded scale, for all other methods will use the positive/negative cutoff indicated in Table <ref type="table" coords="9,435.00,349.04,3.87,8.80" target="#tab_2">2</ref>.</p><p>Results are presented in Figure <ref type="figure" coords="9,226.68,360.99,4.98,8.80" target="#fig_4">4</ref> on the automatic binary scale, the manual graded scale, and a lenient variant of the manual graded scale. Standard error bars are given for reference. All analyses across all measures as well as automatic and manual assessments are painting the same picture. Acknowledging consistent patterns in the results, here the ranking of methods by across-the-board performance:</p><p>1. All variations of the neural network PACRR (MPIID5), which re-ranks a BM25 candidate set.</p><p>2. CUISPR (CUIS), which uses the sequential dependence model to rerank a BM25 candidate set.</p><p>3. BM25, BM25 with DBpedia expansion (UNH-TREMA), and the neural Siamese attention network ranking variants (UTDHLTRI), which are also DBpedia expansion and pre-trained embeddings. In comparison to plain BM25, DBpedia expansion is advantageous most of the time, while neural networkbased ranking seems to retrieve more paragraphs on that are roughly on topic.</p><p>4. DBpedia expansion features combined with AdaRank-based learning-to-rank (UTDHLTRIAR).</p><p>5. ICT's method, "qr" method (NYUDL), and "runOne" (ECNU), 6. Neural network methods "ds" and "qrds" (NYUDL)</p><p>To attempt an explanation, NYUDL's submissions only included a binary set of relevant (versus non-relevant) paragraphs. In the vast majority of cases these included only a single paragraph per topic section.</p><p>To study whether the differences are due to better performance on easy queries, difficult queries, or overall, we include divide the set of all annotated topic sections into percentiles ranging from easy to difficult according to the BM25 baseline. The results are presented in Figure <ref type="figure" coords="9,388.54,620.02,4.98,8.80" target="#fig_7">5</ref> and show, for instance, that all methods of MPIID5 are consistently the best on all but the easiest quartile of section topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this first year of the TREC Complex Answer Retrieval track we learned a lot about the structure of the problem.  0%-5% 5%-25% 25%-50% 50%-75% 75%-95% 95%-100% difficulty percentile according to UNH.bm25 0%-5% 5%-25% 25%-50% 50%-75% 75%-95% 95%-100% difficulty percentile according to UNH.bm25  • Human assessors can agree which passages and entities are relevant. We found that there are two predominant reasons why passages are not relevant: they are either completely missing the topic (NO) or they are roughly on topic, but non-relevant for the given section (TOPIC).</p><p>• Automatic and Manual ground truth agree which of the contributed systems works better than others. However, many retrieved passages that are marked as incorrect by the automatic ground truth, are actually correct when manually inspected.</p><p>• With much care, neural network methods can work significantly better than unsupervised methods. However, the distance to unsupervised methods such as, the sequential dependence model and BM25 is small (which corroborates earlier findings <ref type="bibr" coords="12,288.75,186.68,10.29,8.80" target="#b6">[7]</ref>). Using DBpedia as a source for expansion provides an advantage.</p><p>After the success of the first year, we are looking forward to exploring exciting directions in year two.</p><p>Title: Cocoa bean 1. Etymology</p><p>• Could: Criollo (cocoa bean)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">History</head><p>• Should: Cadbury </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Production</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,72.00,292.20,44.86,8.80;2,132.67,292.20,407.32,8.80;2,72.00,304.15,162.71,8.80"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2:Example passages and relevance for "Candy Making / Hard Candy" (Query ID "Candy%20making/Hard%20candy").</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,306.00,515.92,234.00,8.80;2,306.00,527.88,38.73,8.80"><head>3Figure 1 :</head><label>1</label><figDesc>Figure 1: Example outline for complex topic "Candy Making".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,276.26,354.21,62.81,7.04;5,260.42,474.07,94.47,7.04"><head></head><label></label><figDesc>(a) Passage task. (b) Entity task (excerpt).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,237.87,493.87,136.26,8.80;5,70.67,232.97,466.66,235.98"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Assessment interface.</figDesc><graphic coords="5,70.67,232.97,466.66,235.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="10,72.00,702.75,467.99,8.80;10,72.00,714.71,269.51,8.80"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results of contributed passage runs under automatic and manual ground truth. Lenient is based on the manual graded scale, but counting TOPIC as relevant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="11,100.93,688.38,410.14,8.80"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance according to manual truth on difficulty percentiles according to BM25.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="13,111.42,623.22,389.17,8.80"><head>•Figure 6 :</head><label>6</label><figDesc>Figure 6: Positively annotated entities for the outline of the complex topic "Cocoa bean".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,72.00,551.41,468.00,121.92"><head>Table 1 :</head><label>1</label><figDesc>1, several datasets are derived. The size of the datasets is given in Table1. The paragraph collection contains 29,678,367 unique paragraphs. Data set sizes in terms of articles, section, and automatic positive assessments.</figDesc><table coords="2,77.98,594.00,445.21,57.02"><row><cell></cell><cell cols="2">benchmarkY1train benchmarkY1test</cell><cell cols="2">train test200</cell></row><row><cell>number of articles (complex topics)</cell><cell>117</cell><cell>133</cell><cell>285,924</cell><cell>198</cell></row><row><cell>hierarchical sections (queries)</cell><cell>1,816</cell><cell>2,125</cell><cell>2,180,868</cell><cell>1,860</cell></row><row><cell>total positive paragraphs assessments</cell><cell>4,530</cell><cell>5,820</cell><cell>5,276,624</cell><cell>4,706</cell></row><row><cell>total positive entity assessments</cell><cell>13,031</cell><cell cols="2">15,085 12,310,616</cell><cell>11,396</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,72.00,589.04,468.00,96.08"><head>Table 2 :</head><label>2</label><figDesc>Assessment scale for manual assessments. Horizontal line: Cutoff for positive/negative assessments.</figDesc><table coords="5,122.43,603.08,367.14,82.04"><row><cell></cell><cell cols="3">binary scale manual scale manual lenient scale</cell></row><row><cell>MUST be mentioned</cell><cell>1</cell><cell>3</cell><cell>5</cell></row><row><cell>SHOULD be mentioned</cell><cell>1</cell><cell>2</cell><cell>4</cell></row><row><cell>CAN be mentioned</cell><cell>1</cell><cell>1</cell><cell>3</cell></row><row><cell cols="2">Non-relevant, but roughly on TOPIC 0</cell><cell>0</cell><cell>2</cell></row><row><cell>NO, non-relevant</cell><cell>0</cell><cell>-1</cell><cell>0</cell></row><row><cell>Trash</cell><cell>0</cell><cell>-2</cell><cell>-2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,81.30,73.50,440.62,225.20"><head>Table 3 :</head><label>3</label><figDesc>Grade histogram and distribution.</figDesc><table coords="6,81.30,95.05,440.62,203.65"><row><cell></cell><cell></cell><cell></cell><cell cols="2">(a) Passage judgments.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>annotator1</cell><cell>annotator2</cell><cell>annotator3</cell><cell>annotator4</cell><cell>annotator5</cell><cell cols="2">annotator6 Total %</cell></row><row><cell>Trash</cell><cell>25</cell><cell>8</cell><cell>4</cell><cell>6</cell><cell>2</cell><cell>0</cell><cell>0%</cell></row><row><cell>No</cell><cell>2869</cell><cell>1875</cell><cell>2910</cell><cell>1971</cell><cell>1381</cell><cell>2254</cell><cell>43%</cell></row><row><cell>Topic</cell><cell>536</cell><cell>1000</cell><cell>1349</cell><cell>2853</cell><cell>1665</cell><cell>2491</cell><cell>32%</cell></row><row><cell>Can</cell><cell>213</cell><cell>703</cell><cell>1186</cell><cell>263</cell><cell>114</cell><cell>827</cell><cell>11%</cell></row><row><cell>Should</cell><cell>380</cell><cell>12</cell><cell>827</cell><cell>120</cell><cell>226</cell><cell>563</cell><cell>7%</cell></row><row><cell>Must</cell><cell>340</cell><cell>771</cell><cell>131</cell><cell>874</cell><cell>229</cell><cell>256</cell><cell>8%</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">(b) Entity judgments.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>annotator1</cell><cell>annotator2</cell><cell>annotator3</cell><cell>annotator4</cell><cell>annotator5</cell><cell cols="2">annotator6 Total %</cell></row><row><cell>Trash</cell><cell>2</cell><cell>1</cell><cell>10</cell><cell>17</cell><cell>10</cell><cell>1</cell><cell>0%</cell></row><row><cell>No</cell><cell>1363</cell><cell>1060</cell><cell>2142</cell><cell>1979</cell><cell>989</cell><cell>1901</cell><cell>85%</cell></row><row><cell>Topic</cell><cell>33</cell><cell>223</cell><cell>129</cell><cell>259</cell><cell>224</cell><cell>283</cell><cell>10%</cell></row><row><cell>Can</cell><cell>10</cell><cell>114</cell><cell>80</cell><cell>16</cell><cell>16</cell><cell>52</cell><cell>3%</cell></row><row><cell>Should</cell><cell>18</cell><cell>4</cell><cell>15</cell><cell>12</cell><cell>26</cell><cell>26</cell><cell>1%</cell></row><row><cell>Must</cell><cell>5</cell><cell>76</cell><cell>4</cell><cell>18</cell><cell>11</cell><cell>9</cell><cell>1%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,81.30,73.50,444.61,339.06"><head>Table 4 :</head><label>4</label><figDesc>Inter annotator agreement according to Cohen's κ and Fleiss' κ.</figDesc><table coords="7,81.30,95.05,444.61,317.52"><row><cell></cell><cell cols="5">(a) Binary (TOPIC counting as negative). Fleiss κ = 0.574</cell></row><row><cell></cell><cell>annotator1</cell><cell>annotator2</cell><cell>annotator3</cell><cell>annotator4</cell><cell>annotator5</cell><cell>annotator6</cell></row><row><cell>annotator1</cell><cell></cell><cell>0.569</cell><cell>0.602</cell><cell>0.742</cell><cell>0.574</cell><cell>0.544</cell></row><row><cell>annotator2</cell><cell>0.569</cell><cell></cell><cell>0.574</cell><cell>0.658</cell><cell>0.458</cell><cell>0.417</cell></row><row><cell>annotator3</cell><cell>0.602</cell><cell>0.574</cell><cell></cell><cell>0.667</cell><cell>0.615</cell><cell>0.553</cell></row><row><cell>annotator4</cell><cell>0.742</cell><cell>0.658</cell><cell>0.667</cell><cell></cell><cell>0.638</cell><cell>0.575</cell></row><row><cell>annotator5</cell><cell>0.574</cell><cell>0.458</cell><cell>0.615</cell><cell>0.638</cell><cell></cell><cell>0.733</cell></row><row><cell>annotator6</cell><cell>0.544</cell><cell>0.417</cell><cell>0.553</cell><cell>0.575</cell><cell>0.733</cell></row><row><cell></cell><cell></cell><cell cols="3">(b) Graded. Fleiss κ = 0.273</cell><cell></cell></row><row><cell></cell><cell>annotator1</cell><cell>annotator2</cell><cell>annotator3</cell><cell>annotator4</cell><cell>annotator5</cell><cell>annotator6</cell></row><row><cell>annotator1</cell><cell></cell><cell>0.255</cell><cell>0.425</cell><cell>0.326</cell><cell>0.281</cell><cell>0.152</cell></row><row><cell>annotator2</cell><cell>0.255</cell><cell></cell><cell>0.214</cell><cell>0.379</cell><cell>0.181</cell><cell>0.336</cell></row><row><cell>annotator3</cell><cell>0.425</cell><cell>0.214</cell><cell></cell><cell>0.314</cell><cell>0.322</cell><cell>0.238</cell></row><row><cell>annotator4</cell><cell>0.326</cell><cell>0.379</cell><cell>0.314</cell><cell></cell><cell>0.334</cell><cell>0.380</cell></row><row><cell>annotator5</cell><cell>0.281</cell><cell>0.181</cell><cell>0.322</cell><cell>0.334</cell><cell></cell><cell>0.377</cell></row><row><cell>annotator6</cell><cell>0.152</cell><cell>0.336</cell><cell>0.238</cell><cell>0.380</cell><cell>0.377</cell></row><row><cell></cell><cell cols="5">(c) Graded, counting grades that are "off by one" as agreement.</cell></row><row><cell></cell><cell>annotator1</cell><cell>annotator2</cell><cell>annotator3</cell><cell>annotator4</cell><cell>annotator5</cell><cell>annotator6</cell></row><row><cell>annotator1</cell><cell></cell><cell>0.603</cell><cell>0.686</cell><cell>0.758</cell><cell>0.635</cell><cell>0.658</cell></row><row><cell>annotator2</cell><cell>0.603</cell><cell></cell><cell>0.581</cell><cell>0.739</cell><cell>0.485</cell><cell>0.520</cell></row><row><cell>annotator3</cell><cell>0.686</cell><cell>0.581</cell><cell></cell><cell>0.692</cell><cell>0.685</cell><cell>0.693</cell></row><row><cell>annotator4</cell><cell>0.758</cell><cell>0.739</cell><cell>0.692</cell><cell></cell><cell>0.623</cell><cell>0.625</cell></row><row><cell>annotator5</cell><cell>0.635</cell><cell>0.485</cell><cell>0.685</cell><cell>0.623</cell><cell></cell><cell>0.812</cell></row><row><cell>annotator6</cell><cell>0.658</cell><cell>0.520</cell><cell>0.693</cell><cell>0.625</cell><cell>0.812</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="8,87.24,715.13,182.52,6.64"><p>https://github.com/nyu-dl/QueryReformulator</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>We express our gratitude for many suggestions of several experts in the field, who helped to make this track successful. Special thanks to <rs type="person">Fernardo Diaz</rs> on whose ideas this evaluation is based on. We thank the <rs type="institution">University of New Hampshire</rs> for providing computational resources and web servers. We are grateful for <rs type="person">Ben Gamari</rs>'s invaluable support in developing the benchmark creation and assessment interface software. We are deeply thankful for Ellen Voorhees' experience, patience, and persistence in running the assessment process. Finally we thank all our participants.</p></div>
			</div>
			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>http://trec-car.cs.unh.edu Google group mailinglist: TREC-CAR</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="12,87.49,399.75,452.50,8.80;12,87.50,411.71,452.50,8.80;12,87.50,423.66,452.50,8.80;12,87.50,436.39,222.39,8.30" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,407.45,399.75,132.54,8.80;12,87.50,411.71,157.47,8.80">Frontiers, challenges, and opportunities for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>James Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alistair</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sanderson</surname></persName>
		</author>
		<idno type="DOI">10.1145/2215676.2215678</idno>
		<ptr target="http://doi.acm.org/10.1145/2215676.2215678" />
	</analytic>
	<monogr>
		<title level="m" coord="12,255.15,411.71,284.85,8.80;12,87.50,423.66,123.03,8.80">Report from SWIRL 2012 the second strategic workshop on information retrieval in Lorne</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="2" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,87.49,455.54,452.51,8.80;12,87.50,467.50,220.47,8.80" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,253.49,455.54,222.12,8.80">Using crowdsourcing for trec relevance assessment</title>
		<author>
			<persName coords=""><forename type="first">Omar</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefano</forename><surname>Mizzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,487.93,455.54,52.08,8.80;12,87.50,467.50,112.81,8.80">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1053" to="1066" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,87.49,487.42,452.51,8.80;12,87.50,499.38,134.22,8.80" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,279.82,487.42,239.69,8.80">Wikikreator: Improving wikipedia stubs automatically</title>
		<author>
			<persName coords=""><forename type="first">Siddhartha</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prasenjit</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,87.50,499.38,36.09,8.80">ACL (1)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="867" to="877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,87.49,519.30,452.51,8.80;12,87.50,531.26,452.50,8.80;12,87.50,543.21,172.34,8.80" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,242.06,519.30,297.94,8.80;12,87.50,531.26,34.16,8.80">Tagme: on-the-fly annotation of short text fragments (by wikipedia entities)</title>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Ferragina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ugo</forename><surname>Scaiella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,142.50,531.26,397.49,8.80;12,87.50,543.21,33.82,8.80">Proceedings of the 19th ACM international conference on Information and knowledge management</title>
		<meeting>the 19th ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1625" to="1628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,87.49,563.14,452.51,8.80;12,87.50,575.09,182.77,8.80" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,365.81,563.14,174.19,8.80;12,87.50,575.09,96.27,8.80">Pacrr: A position-aware neural ir model for relevance matching</title>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus</forename><surname>Berberich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gerard</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Melo</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,205.40,575.09,32.38,8.80">EMNLP</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,87.49,595.02,452.50,8.80;12,87.50,606.97,452.51,8.80;12,87.50,618.93,162.13,8.80" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,246.45,595.02,223.97,8.80">A markov random field model for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,490.35,595.02,49.64,8.80;12,87.50,606.97,452.51,8.80;12,87.50,618.93,34.56,8.80">Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 28th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,87.49,638.85,452.50,8.80;12,87.50,650.81,452.51,8.80;12,87.50,662.76,321.88,8.80" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,388.11,638.85,151.89,8.80;12,87.50,650.81,26.41,8.80">Benchmark for complex answer retrieval</title>
		<author>
			<persName coords=""><forename type="first">Federico</forename><surname>Nanni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matt</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laura</forename><surname>Dietz</surname></persName>
		</author>
		<idno type="DOI">10.1145/3121050.3121099</idno>
		<ptr target="https://doi.org/10.1145/3121050.3121099" />
	</analytic>
	<monogr>
		<title level="m" coord="12,132.97,650.81,407.03,8.80;12,87.50,662.76,37.32,8.80">ICTIR &apos;17 Proceedings of the ACM SIGIR International Conference on Theory of Information Retrieval</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="293" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,87.49,682.69,452.50,8.80;12,87.50,694.64,171.10,8.80" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="12,265.54,682.69,270.27,8.80">Task-oriented query reformulation with reinforcement learning</title>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04572</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
