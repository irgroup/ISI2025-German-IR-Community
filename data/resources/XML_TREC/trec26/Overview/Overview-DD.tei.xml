<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,165.75,76.23,279.44,12.29">TREC 2017 Dynamic Domain Track Overview</title>
				<funder ref="#_ceWfg4N">
					<orgName type="full">DARPA</orgName>
				</funder>
				<funder ref="#_Rx2GuNZ">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,124.50,111.88,65.50,8.78"><forename type="first">Grace</forename><forename type="middle">Hui</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName coords="1,282.75,111.88,53.57,8.78"><forename type="first">Zhiwen</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Ian Soboroff Georgetown University</orgName>
								<orgName type="institution">Georgetown University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">NIST</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,165.75,76.23,279.44,12.29">TREC 2017 Dynamic Domain Track Overview</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">834B05D773416CDB45E01811FA8CBB26</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The goal of dynamic domain track is promoting the research of dynamic, exploratory search within complex information domains, where the search process is usually interactive and user's information need is also complex. Dynamic Domain (DD) track has been held in the past three years. This track's name includes two parts. "Dynamic" means the search process may contain multiple runs of iteration, and the participating system is expected to adapt its search algorithm based on the relevance feedback. "Domain" means the search task focuses on special domains, where user's information need consists of multiple aspects, and the participating system is expected to help the user explore the domain through rich interaction. This task has received great attention and this track is inspired by interested groups in government, including DARPA MEMEX program.</p><p>The settings of DD track are motivated by professional search, such as prior art patent search or criminal network finding, where rich interaction is a great asset for improving the search results and users require stringent relevancy on the documents returned.</p><p>In order to simulate the interaction between the search engine and the user, as well as evaluate the whole search process, a simulated user (called Jig ) is developed. During each step in interaction, participating system sends a list 1 of documents to the simulated user, and the simulated user returns a real-time feedback to the participating system. Participating system learns the real intention behind the search topic, adapts its search algorithm and generates the next list of documents. This process is repeated until the participating system believes the user's information need has been satisfied. All the documents returned by the participating system are saved for the evaluation of the whole search session. DD track also uses fine-grained judgements. Different from open domain web search, all the relevance judgements are on the passage level, which expresses user's information need more accurately. Correspondingly, DD track also uses sophisticated metrics to evaluate the search results, which includes Cube Test <ref type="bibr" coords="1,431.99,517.63,10.62,8.78" target="#b0">[1]</ref>, session-DCG <ref type="bibr" coords="1,508.95,517.63,11.66,8.78" target="#b1">[2]</ref> and Expected Utility <ref type="bibr" coords="1,142.53,531.13,10.62,8.78" target="#b2">[3]</ref>. All these metrics evaluate the whole search process and each provides a distinct view on the effectiveness and efficiency of the participating systems. This year, DD track focuses on the exploration of New York Times archives <ref type="bibr" coords="1,389.08,571.63,10.62,8.78" target="#b3">[4]</ref>. 3 groups participated and 11 runs were submitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Task Description</p><p>The task of TREC DD track is based on a concept that search is driven by the feedback instead of queries. That is, a good search system need to learn user's real intents through the feedback given by the user regarding previous returned documents and the user does not need to reformulate queries to express or refine his/her information need.</p><p>The search system needs to adjust its search algorithm so as to help the user explore the complex domain.  In the beginning, the search system receives an initial query (the topic name) indicating user's intention. Then, the search system retrieves five documents from the index and sends them back to the user simulator Jig. Jig returns feedback about the returned five documents. The feedback sent back by Jig gives a detailed description about the relevance of returned documents on subtopic level. The search system needs to decide if it will continue returning documents or stop the current search. The search system may also consider how to rerank the documents so as to better satisfy user's information need.</p><p>This track is expecting participating systems achieve two basic points. First, the search system is expected to adapt its search algorithm based on the relevance feedback, for which rich information is provided in the feedback. Second, it is the search system's job, instead of the user's, to decide whether to continue search. Search results are evaluated using different sophisticated metrics, which measures the total amount of information the search system gains and the effort of user from various points of view. In 2017, DD track focuses on the exploring of a new domain, the archives of New York Times in 20 years <ref type="bibr" coords="3,507.40,493.63,10.62,8.78" target="#b3">[4]</ref>. The corpus contains all the articles published in New York Times (online and offline) from January </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic Development</head><p>Topics are developed by six NIST assessors in over six weeks during the summer of 2017. A topic, which is like a query, is the main search target for the whole search process. Every topic contains several subtopics, each addresses one aspect of the search topic. Each subtopic contains several number of passages which is discovered from the entire corpus. Each passage is graded based on the relevance between its content and the subtopic. An annotation tool is developed to help the assessors find the complete set of passages that are relevant to the query. Also, near-duplicate detection is utilized to help find possible relevant passages that may be missed. The graded passages are treated as the complete set of judgement.</p><p>The user interface of annotation tool is shown in Figure <ref type="figure" coords="4,304.33,210.13,5.00,8.78" target="#fig_4">5</ref> and Figure <ref type="figure" coords="4,359.60,210.13,3.75,8.78">6</ref>. Four algorithms are provided for search in topic level and two algorithms are used for search in subtopic level. Assessors first conduct search in topic level.</p><p>They can then go into the detailed page of every document where they will give rich feedback to the annotation tool.</p><p>They can decide if a document is irrelevant or duplicated. They can also drag and drop relevant passages to the subtopic box on the right side. Each passage is then graded based on the extent of relevance. The highest relevance score is 4 for key results and the lowest relevance score is 1 for marginally relevant.</p><p>In the topic level search, three mainstream open source search engine, Lemur , Solr and Terrier , and an active The active learning algorithm uses the feedback, i.e. the documents graded by assessors before, to refine its search results. For every query, assessors are required to search in every one of the four search buttons and go over the ranking lists so as to cover as many relevant documents as possible.</p><p>In the subtopic level search, two search algorithms are used to help assessors find more relevant passages. One of them uses the passages that have been tagged and the subtopic name to search for relevant documents, the other one only uses tagged passages to search. For every subtopic, assessors are also required to search in each one of them so as to find a complete set of relevant passages. After completing the human annotation, near-duplicate detection is used to find passages that are actually relevant but may be missed by assessors. In the ground truth data, passages that are tagged with "MANUAL" are those discovered by human assessors while passages that are tagged with "MATCHED" are those discovered using detection algorithm, which is based on its similarity to the human annotated passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation metrics</head><p>In order to provide a comprehensive evaluation of the whole search process, in 2017, DD track uses several evaluation metrics to measure the performance of search systems from different perspectives. The primary metric used in DD track is Cube Test <ref type="bibr" coords="5,205.46,478.63,10.62,8.78" target="#b0">[1]</ref>. DD track also uses session-DCG <ref type="bibr" coords="5,363.14,478.63,11.66,8.78" target="#b1">[2]</ref> and Expected Utility <ref type="bibr" coords="5,465.51,478.63,11.66,8.78" target="#b2">[3]</ref> for evaluation. These metrics reveal different aspects of the dynamic search process.</p><p>Cube Test (CT) <ref type="bibr" coords="5,143.23,519.13,11.66,8.78" target="#b0">[1]</ref> is a search effectiveness measurement evaluating the speed of gaining relevant information (could be documents or passages) in a dynamic search process. It measures the amount of relevant information a system could gather and the time needed in the entire search process. The higher the Cube Test score, the better the IR system. It is defined as:</p><formula xml:id="formula_0" coords="5,165.90,583.94,278.26,28.82">T C = L rel (i, j) γ I( el (m, n) + el (i, n)&lt;M axHeight) ∑ L i=1 ∑ |list | i j=1 ∑ c θ c * c * n(c, i, j-1) * ∑ i-1 m=1 ∑ |list | m n=1 r c ∑ j-1 n=1 r c</formula><p>where is the number of iterations so far, is the number of documents returned at the iteration. is a</p><formula xml:id="formula_1" coords="5,72.00,626.12,448.31,26.30">L list | | i i th c subtopic,</formula><p>is the importance factor of subtopic , is the relevance score of the document that is θ c c el (i, j) r c j th returned in iteration regarding subtopic , is the number of relevant documents found on subtopic i th c (c, i, j</p><p>) n -1 before the document in the iteration, is the discounting factor, is an indicator function and c j th i th γ ( ) I * is the cap for each subtopic. axHeight M Session-DCG (sDCG) <ref type="bibr" coords="6,166.63,75.13,11.66,8.78" target="#b1">[2]</ref> extends the classic DCG to a search session which consists of multiple iterations. The relevance scores of results that are ranked lower or returned in later iterations get more discounts. The discounted cumulative relevance score is the final result of this metric. It is defined as : Expected Utility (EU) <ref type="bibr" coords="6,165.01,200.38,11.66,8.78" target="#b2">[3]</ref> scores different runs by measuring the relevant information a system found and the length of documents. The relevance scores of documents are discounted based on ranking order and novelty. The document length is discounted only based on ranking position. The difference between the cumulative relevance score and the aggregated document length is the final score of each run. It is defined as:</p><formula xml:id="formula_2" coords="6,238.65,113.12,130.52,26.40">DCG s = ∑ L i=1 ∑ |list | i j=1 rel(i, j) (1+log j) (1+log i</formula><formula xml:id="formula_3" coords="6,181.65,259.37,247.75,20.71">U (ω) ( ( γ ) a en(i, j)) E = ∑ ω P ∑ (i,j)∈ω ∑ c∈d i,j θ c * n(c, i, j-1) -* l</formula><p>EU assumes that the user only reviews a subset ( ) of documents returned. is the probability of subset ω (ω) P ω being reviewed, is the length of document that is ranked in the iteration and is the coefficient. en(i, ) l j j th i th a</p><p>Apart from the raw scores of these metrics, DD track also uses normalized scores of these metrics following the methods proposed in <ref type="bibr" coords="6,160.03,354.13,11.66,8.78" target="#b4">[5]</ref> where the upper bound scores of every topic are used for normalization. DD track expects the normalized scores bringing more fairness to the dynamic search evaluation.</p><p>The parameters used for DD track evaluation are as follows: In CT, and . In sDCG, .5 γ = 0 axHeight M = 5 b = 2 and . In EU, and . All the subtopics within the same topic are assumed to be equally q 4 b = .5 γ = 0</p><p>.01 a = 0 important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Submission and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Submission</head><p>In 2017, 3 groups participated in the DD track and 11 runs are submitted in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group Country</head><p>University of Maryland (CLIP) USA Georgetown University (georgetown) USA</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chinese Academy of Science (ICTNET) China Table 2. Participating Groups</head><p>Here are the brief summary of submitted runs provided by the participating groups:</p><p>clip_addwords : Data was indexed/searched using Indri search engine. For the first run, the topic was used as the search term. For subsequent runs, words from the passage text (excluding stopwords) were added to the query.</p><p>clip_baseline : Baseline set of results using the topic description as the search terms (indexed/searched using Indri search engine). The top 25 results for each topic were submitted to the jig, 5 per topic for each run.</p><p>clip_filter : Data was indexed/searched using Indri search engine. For the first run, topics were used as search terms.</p><p>For subsequent runs, Indri filter operator was used to add terms from the relevant passages (provided via the jig).</p><p>The topic terms were used as "required" terms, and words from the passages were only valid if the other terms also appeared. ictnet_emulti : For ebola dataset, we use google suggested queries and jig feedback to ensure the diversification. For New York Times dataset, most of the queries is long and there is no suggested queries, we only use feedback information. We use xQuAD and query expansion algorithms ictnet_fom_itr1 : In this solution, we run xQuAD and query expansion algorithms which is sim with other solutions. But we change parameters.</p><p>ictnet_params1_s : Change params of other solutions. Use stop strategy.</p><p>ictnet_params2_ns : Change params of other solution. Not use stop strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results</head><p>The evaluation scores, including the raw scores and normalized scores, in the first ten iterations of all submitted runs are plotted from Figure <ref type="figure" coords="7,167.49,466.63,5.00,8.78" target="#fig_7">7</ref> to Figure <ref type="figure" coords="7,213.86,466.63,8.33,8.78" target="#fig_1">12</ref>. More detailed results can be found from Table <ref type="table" coords="7,417.05,466.63,5.00,8.78">3</ref> to Table <ref type="table" coords="7,460.07,466.63,8.33,8.78" target="#tab_1">12</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The dynamic domain track has been running in the past three years at TREC and this is the final year. DD track always focuses on interactive and exploratory search task with the vision that a good search system should be a guide to discover the domain of user's interest by adaptively learning user's intention. Participants have tried various methods marching towards this goal.</p><p>With the rising of Artificial Intelligence (AI) in recent years, it is exciting to see that this track shares a very similar goal with many AI tasks, that is, building an intelligent system that understands human's mind better. Although DD track ends this year, as organizers, we are still hoping researchers to keep interests in this task, especially the potential improvement brought by the latest AI techniques. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detailed Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CT</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,210.00,172.63,191.84,8.78;2,105.75,87.00,400.50,78.00"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Sample topic of TREC 2017 DD track</figDesc><graphic coords="2,105.75,87.00,400.50,78.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,176.25,460.63,259.28,8.78;2,87.75,252.00,437.25,201.00"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Sample Relevance Judgement of TREC 2017 DD track</figDesc><graphic coords="2,87.75,252.00,437.25,201.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,234.00,212.38,143.25,8.78;3,73.50,73.50,468.00,131.25"><head>Figure 3 .Figure 4 .</head><label>34</label><figDesc>Figure 3. DD track Task illustration</figDesc><graphic coords="3,73.50,73.50,468.00,131.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,397.37,303.53,3.00,5.27;4,423.53,303.53,3.00,5.27;4,476.71,303.53,3.00,5.27;4,72.00,318.13,467.66,8.78;4,72.00,331.63,149.19,8.78"><head></head><label></label><figDesc>are provided. DD track expects the combination of different search engines can reduce the inherent bias of each individual one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="4,213.00,661.63,185.14,8.78;4,90.00,438.00,431.25,216.00"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. User Interface(I) of Annotation Tool</figDesc><graphic coords="4,90.00,438.00,431.25,216.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,369.17,128.12,2.14,6.15;6,326.70,128.87,14.00,6.46;6,361.54,130.94,4.99,4.39;6,72.00,158.38,150.59,8.78;6,261.93,158.38,277.55,8.78;6,230.89,158.38,26.83,8.78;6,226.73,158.38,3.89,8.78;6,82.82,172.63,14.43,8.78;6,115.91,172.63,93.54,8.78;6,73.65,172.63,5.00,8.78;6,106.40,172.63,5.00,8.78;6,101.40,172.63,5.00,8.78"><head></head><label></label><figDesc>) b * bq sDCG does not consider subtopics so is the relevance score that is accumulated over all the subtopics. Both el(i, j) r and are discounting factors. b q b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="7,72.00,156.13,248.77,8.78;7,72.00,183.13,299.41,8.78;7,72.00,210.13,225.15,8.78;7,72.00,237.13,467.74,8.78;7,72.00,250.63,225.97,8.78"><head>dqn_5_actions:</head><label></label><figDesc>Use DQN to choose 5 possible search actions dqn_semantic_state : state is defined as query+feedback+iteration number galago_baseline : The first 50 results returned by galago ictnet_div_qe : We use xQuAD and query expansion algorithm to ensure both relevance and diversification. Use stop strategy. The first iteration we use the result of solr.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="8,217.50,324.88,176.84,8.78;8,102.75,73.50,405.75,243.75"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. CT scores in the first ten iterations</figDesc><graphic coords="8,102.75,73.50,405.75,243.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="5,84.75,100.50,442.50,222.00"><head></head><label></label><figDesc></figDesc><graphic coords="5,84.75,100.50,442.50,222.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="9,103.50,73.50,405.00,243.00"><head></head><label></label><figDesc></figDesc><graphic coords="9,103.50,73.50,405.00,243.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="9,103.50,376.50,405.00,243.00"><head></head><label></label><figDesc></figDesc><graphic coords="9,103.50,376.50,405.00,243.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="10,102.75,73.50,405.75,243.75"><head></head><label></label><figDesc></figDesc><graphic coords="10,102.75,73.50,405.75,243.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="10,102.75,350.25,405.75,243.75"><head></head><label></label><figDesc></figDesc><graphic coords="10,102.75,350.25,405.75,243.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,72.00,507.13,467.83,137.03"><head>1, 1987 to June 19, 2007 with metadata provided by the New York Times Newsroom, the New York Times Indexing Service and the</head><label></label><figDesc>online production staff at nytimes.com. Most articles are manually summarized and tagged by professional staffs. The original form of this dataset is in News Industry Text Format (NITF) . This corpus contains huge amount of information covering a wide range of topics and categories.</figDesc><table coords="3,77.25,594.13,428.17,50.03"><row><cell>3.2.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Compressed Size</cell><cell>Uncompressed Size</cell><cell>Number of Documents</cell><cell>Number of Queries</cell></row><row><cell>3.1 GB</cell><cell>16 GB</cell><cell>1855658</cell><cell>60</cell></row><row><cell></cell><cell cols="2">Table 1. Statistics of New York Time dataset</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="12,77.25,108.13,455.73,587.78"><head>Table 12 .</head><label>12</label><figDesc>Iteration 10   </figDesc><table coords="12,77.25,108.13,455.73,587.78"><row><cell></cell><cell></cell><cell>nCT</cell><cell>sDCG</cell><cell>nsDCG</cell><cell>EU</cell><cell>nEU</cell></row><row><cell>clip_addwords</cell><cell>0.269316</cell><cell>0.2729377</cell><cell>9.9550701</cell><cell>0.2779112</cell><cell>6.8649358</cell><cell>0.3182756</cell></row><row><cell>clip_baseline</cell><cell>0.4537569</cell><cell>0.4591389</cell><cell cols="2">14.5994951 0.4274217</cell><cell>10.950894</cell><cell>0.342133</cell></row><row><cell>clip_filter</cell><cell>0.269316</cell><cell>0.2729377</cell><cell>9.9550701</cell><cell>0.2779112</cell><cell>6.8649358</cell><cell>0.3182756</cell></row><row><cell>dqn_5_actions</cell><cell>0.4701114</cell><cell>0.4756687</cell><cell cols="2">15.4846276 0.4456217</cell><cell cols="2">11.6124865 0.3499484</cell></row><row><cell>dqn_semantic_state</cell><cell>0.4683404</cell><cell>0.4742194</cell><cell cols="2">14.8987069 0.4620971</cell><cell cols="2">11.1862801 0.3473296</cell></row><row><cell>galago_baseline</cell><cell>0.450791</cell><cell>0.4563555</cell><cell cols="2">13.7844676 0.4337153</cell><cell>10.248345</cell><cell>0.3431296</cell></row><row><cell>ictnet_div_qe</cell><cell>0.4618715</cell><cell>0.4677867</cell><cell cols="2">15.2283842 0.454452</cell><cell cols="2">10.8843135 0.3407438</cell></row><row><cell>ictnet_emulti</cell><cell>0.4618715</cell><cell>0.4677867</cell><cell cols="2">15.2283842 0.454452</cell><cell cols="2">10.8843135 0.3407438</cell></row><row><cell>ictnet_fom_itr1</cell><cell>0.4975938</cell><cell>0.5034492</cell><cell cols="2">14.8143864 0.4617947</cell><cell cols="2">11.0105412 0.3435733</cell></row><row><cell>ictnet_params1_s</cell><cell>0.4618715</cell><cell>0.4677867</cell><cell cols="2">15.2283842 0.454452</cell><cell cols="2">10.8843135 0.3407438</cell></row><row><cell>ictnet_params2_ns</cell><cell>0.4618715</cell><cell>0.4677867</cell><cell cols="2">15.2283842 0.454452</cell><cell cols="2">10.8843135 0.3407438</cell></row><row><cell></cell><cell></cell><cell cols="2">Table 3. Iteration 1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>CT</cell><cell>nCT</cell><cell>sDCG</cell><cell>nsDCG</cell><cell>EU</cell><cell>nEU</cell></row><row><cell>clip_addwords</cell><cell>0.1531085</cell><cell>0.3102113</cell><cell cols="2">13.3407969 0.2942657</cell><cell>9.4279455</cell><cell>0.5176928</cell></row><row><cell>clip_baseline</cell><cell>0.2666988</cell><cell>0.5395375</cell><cell cols="2">19.5664954 0.4394219</cell><cell cols="2">15.1047319 0.5388618</cell></row><row><cell>clip_filter</cell><cell>0.1564809</cell><cell>0.3175536</cell><cell cols="2">12.8289596 0.2739706</cell><cell>8.7072546</cell><cell>0.5152451</cell></row><row><cell>dqn_5_actions</cell><cell>0.2760114</cell><cell>0.5592316</cell><cell cols="2">21.6554912 0.4760824</cell><cell cols="2">17.4928539 0.5543615</cell></row><row><cell>dqn_semantic_state</cell><cell>0.2938375</cell><cell>0.5938134</cell><cell cols="2">20.9093702 0.4666493</cell><cell cols="2">16.8901066 0.5473597</cell></row><row><cell>galago_baseline</cell><cell>0.2722861</cell><cell>0.5507842</cell><cell cols="2">19.8558578 0.4589931</cell><cell cols="2">16.3226741 0.5476611</cell></row><row><cell>ictnet_div_qe</cell><cell>0.2916159</cell><cell>0.5902519</cell><cell cols="2">22.0556315 0.4948688</cell><cell cols="2">17.3078898 0.5449988</cell></row><row><cell>ictnet_emulti</cell><cell>0.2916159</cell><cell>0.5902519</cell><cell cols="2">22.0556315 0.4948688</cell><cell cols="2">17.3078898 0.5449988</cell></row><row><cell>ictnet_fom_itr1</cell><cell>0.2955616</cell><cell>0.5979995</cell><cell cols="2">22.1477807 0.4970647</cell><cell cols="2">18.8855788 0.5491104</cell></row><row><cell>ictnet_params1_s</cell><cell>0.2912779</cell><cell>0.5894399</cell><cell cols="2">21.9231463 0.4901891</cell><cell cols="2">17.3714137 0.5450564</cell></row><row><cell>ictnet_params2_ns</cell><cell>0.2912779</cell><cell>0.5894399</cell><cell cols="2">21.9231463 0.4901891</cell><cell cols="2">17.3714137 0.5450564</cell></row><row><cell></cell><cell></cell><cell cols="2">Table 4. Iteration 2</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,78.12,711.13,149.07,8.78"><p>https://github.com/trec-dd/trec-dd-jig</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,78.12,711.13,118.55,8.78"><p>https://iptc.org/standards/nitf/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,78.12,687.13,117.71,8.78"><p>http://www.lemurproject.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,78.12,699.13,116.58,8.78"><p>http://lucene.apache.org/solr/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="4,78.12,711.13,66.90,8.78"><p>http://terrier.org/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This research was supported by <rs type="funder">DARPA</rs> grant <rs type="grantNumber">FA8750-14-2-0226</rs> and <rs type="funder">NSF</rs> grant <rs type="grantNumber">IIS-145374</rs>. Any opinions, findings, conclusions, or recommendations expressed in this paper are of the authors, and do not necessarily reflect those of the sponsor.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ceWfg4N">
					<idno type="grant-number">FA8750-14-2-0226</idno>
				</org>
				<org type="funding" xml:id="_Rx2GuNZ">
					<idno type="grant-number">IIS-145374</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="11,89.15,358.63,450.64,8.78;11,72.00,372.13,467.84,8.78;11,72.00,385.63,269.85,8.78" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,358.06,358.63,181.73,8.78;11,72.00,372.13,212.51,8.78">The water filling model and the cube test: multi-dimensional evaluation for professional search</title>
		<author>
			<persName coords=""><forename type="first">Jiyun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Wing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marti</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,305.85,372.13,233.99,8.78;11,72.00,385.63,158.81,8.78">Proceedings of the 22nd ACM international conference on Information &amp; Knowledge Management</title>
		<meeting>the 22nd ACM international conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="709" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,88.40,412.63,451.56,8.78;11,72.00,426.13,365.52,8.78" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,401.27,412.63,138.69,8.78;11,72.00,426.13,160.84,8.78">Discounted cumulated gain based evaluation of multiple-query IR sessions</title>
		<author>
			<persName coords=""><forename type="first">Kalervo</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Susan</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lois</forename><surname>Delcambre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marianne</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,243.21,426.13,139.07,8.78">Advances in Information Retrieval</title>
		<imprint>
			<biblScope unit="page" from="4" to="15" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,87.65,453.13,452.17,8.78;11,72.00,466.63,404.45,8.78" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,249.27,453.13,271.93,8.78">Modeling expected utility of multi-session information distillation</title>
		<author>
			<persName coords=""><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abhimanyu</forename><surname>Lad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,72.00,466.63,203.43,8.78">Conference on the Theory of Information Retrieval</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="164" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,87.65,493.63,451.96,8.78;11,72.00,507.13,63.85,8.78" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,164.40,493.63,277.75,8.78">The new york times annotated corpus</title>
		<author>
			<persName coords=""><forename type="first">Evan</forename><surname>Sandhaus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,449.91,493.63,49.97,8.78">Philadelphia</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">26752</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>Linguistic Data Consortium</note>
</biblStruct>

<biblStruct coords="11,86.90,534.13,453.01,8.78;11,72.00,547.63,467.76,8.78;11,72.00,561.13,22.49,8.78" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,243.93,534.13,276.79,8.78">Investigating per Topic Upper Bound for Session Search Evaluation</title>
		<author>
			<persName coords=""><forename type="first">Zhiwen</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Grace</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,72.00,547.63,381.27,8.78">Proceedings of the ACM SIGIR International Conference on Theory of Information Retrieval</title>
		<meeting>the ACM SIGIR International Conference on Theory of Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="185" to="192" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
