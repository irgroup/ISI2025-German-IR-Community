<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,69.89,83.75,472.72,15.12">Overview of the TREC 2017 Real-Time Summarization Track</title>
				<funder>
					<orgName type="full">Natural Sciences and Engineering Research Council</orgName>
					<orgName type="abbreviated">NSERC</orgName>
				</funder>
				<funder>
					<orgName type="full">NIST</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,104.75,108.46,49.12,11.96"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,165.73,108.46,92.97,11.96"><forename type="first">Salman</forename><surname>Mohammed</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,272.66,108.46,70.47,11.96"><forename type="first">Royal</forename><surname>Sequiera</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,355.29,108.46,54.86,11.96"><forename type="first">Luchen</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,422.63,108.46,76.29,11.96"><forename type="first">Nimesh</forename><surname>Ghelani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,106.75,122.41,91.33,11.96"><forename type="first">Mustafa</forename><surname>Abualsaud</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,210.71,122.41,90.31,11.96"><forename type="first">Richard</forename><surname>Mccreadie</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<country>Scotland, the United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,313.73,122.41,82.22,11.96"><forename type="first">Dmitrijs</forename><surname>Milajevs</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">National Institute for Standards and Technology</orgName>
								<address>
									<settlement>Maryland</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,428.97,122.41,73.07,11.96"><forename type="first">Ellen</forename><surname>Voorhees</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">National Institute for Standards and Technology</orgName>
								<address>
									<settlement>Maryland</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,69.89,83.75,472.72,15.12">Overview of the TREC 2017 Real-Time Summarization Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9C390BFAF3A81149E9387F5523F76DE6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>e TREC 2017 Real-Time Summarization (RTS) Track is the second iteration of a community e ort to explore techniques, algorithms, and systems that automatically monitor streams of social media posts such as tweets on Twi er to address users' prospective information needs. ese needs are articulated as "interest pro les", akin to topics in ad hoc retrieval. In real-time summarization, the goal is for a system to deliver interesting and novel content to users in a timely fashion. We refer to these messages generically as "updates". For example, the user might be concerned about tensions on the Korean Peninsula and wishes to be noti ed whenever there are new developments.</p><p>Real-Time Summarization was introduced at TREC 2016 <ref type="bibr" coords="1,268.10,317.61,10.44,8.97" target="#b7">[8]</ref> and represented the merger of the Microblog (MB) Track, which ran from 2010 to 2015, and the Temporal Summarization (TS) Track, which ran from 2013 to 2015 <ref type="bibr" coords="1,158.08,350.48,9.27,8.97" target="#b1">[2]</ref>. e creation of RTS was designed to leverage synergies between the two tracks in exploring prospective information needs over document streams.</p><p>e evaluation design is largely based on the real-time ltering task in the TREC 2015 Microblog Track <ref type="bibr" coords="1,135.60,394.32,9.39,8.97" target="#b6">[7]</ref>.</p><p>Following the setup of the track in 2016, we originally considered two methods for disseminating updates, as outlined in the published track guidelines: 1   • Scenario A: "Push noti cations". As soon as the system identi es a relevant post, it is immediately sent to the user's mobile device as a push noti cation. At a high level, push noti cations should be relevant (on topic), novel (users should not be delivered multiple noti cations that say the same thing), and timely (updates should be provided as soon a er the actual event occurrence as possible). • Scenario B: Email digests. Alternatively, a user might wish to receive a daily email digest that summarizes "what happened" on that day with respect to the interest pro les. One might think of these emails as supplying "personalized headlines". ese results should be relevant and novel, but timeliness is not particularly important provided that the posts were all wri en on the day for which the digest was produced. For expository convenience and to adopt standard information retrieval parlance, we describe tweets that are desirable to the user as relevant, even though "relevant" in our context might be more accurately operationalized as a combination of interesting, novel, and timely.</p><p>As with the evaluation last year, we recruited a number of mobile assessors who evaluated output from scenario A systems in situ on their mobile devices during the evaluation period. Despite our initial intentions, there were last minute technical issues with 1 h p://trecrts.github.io/TREC2017-RTS-guidelines.html the implementation of the evaluation infrastructure: we were able to deploy a mobile web-based interface for the assessors, but it lacked push noti cation functionality. In other words, posts were "delivered" to the mobile devices of assessors, but without an accompanying noti cation signal-this setup is analogous to email inboxes into which relevant content is being continuously deposited, from which the assessors could "pull" new content as they desired. us, to be more accurate we refer to scenario A as "mobile delivery" in the remainder of this paper.</p><p>Overall, the evaluation design of the RTS Track in TREC 2017 remained unchanged from the 2016 iteration, with the exception of two substantive improvements:</p><p>• Participants in scenario A (mobile delivery) were able to obtain the mobile assessors' relevance judgments as they were being generated during the live evaluation period. is allowed systems to experiment, for the rst time, with relevance feedback and techniques based on active learning. • In addition to interest pro les developed by NIST assessors, the mobile assessors this year were also invited to contribute interest pro les of their own. is increased the realism of the task, since the mobile assessors were considering posts retrieved for their own information needs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">EVALUATION DESIGN 2.1 General Setup</head><p>e overall design of the TREC 2017 Real-Time Summarization Track followed the iteration of the track in TREC 2016 <ref type="bibr" coords="1,520.54,481.41,9.45,8.97" target="#b7">[8]</ref>, which was itself adapted from the real-time ltering task in the TREC 2015 Microblog Track <ref type="bibr" coords="1,398.60,503.33,9.30,8.97" target="#b6">[7]</ref>. Although we are interested in exploring ltering techniques over streams of social media posts in general, we restricted the content under consideration to tweets due to their widespread availability. In particular, Twi er provides a streaming API through which clients can obtain a sample (approximately 1%) of public tweets, colloquially known as the "spritzer". is level of access is available to anyone who signs up for an account.</p><p>During the o cial evaluation period, which began Saturday, July 29, 2017 00:00:00 UTC and lasted until Saturday, August 5, 2017 23:59:59 UTC, participants' systems "listened" to Twi er's live tweet sample stream to identify relevant posts with respect to users' interest pro les, under the mobile delivery (scenario A) or email digests (scenario B) setups. is design required participants to maintain running systems that continuously monitor the tweet sample stream during the evaluation period. e track organizers provided boilerplate code and reference implementations from previous years, but it was the responsibility of each individual team to run its own system(s), connect with the RTS evaluation broker to submit results (more details below), all the while coping with crashes, network glitches, power disruptions, etc. A number of recent tracks at TREC have required participants to deploy live systems, which demonstrates that the increased so ware engineering demands do not present an onerous barrier to entry for participating teams.</p><p>An important consequence of the evaluation design is that, unlike in most previous TREC evaluations, no collection or corpus was distributed ahead of time. Since each participant "listened" to tweets from Twi er's streaming API, the collection was generated in real time and delivered to each participant independently. In a previous pilot study <ref type="bibr" coords="2,150.08,195.52,9.52,8.97" target="#b8">[9]</ref>, we veri ed that multiple listeners to the public Twi er sample stream receive e ectively the same tweets. A more recent study by Sequiera and Lin <ref type="bibr" coords="2,238.84,217.43,14.85,8.97" target="#b14">[15]</ref> con rmed the same nding with respect to the Tweets2013 collection gathered for the TREC 2013 Microblog Track <ref type="bibr" coords="2,208.68,239.35,9.52,8.97" target="#b4">[5]</ref>. Di erences due to, for example, network glitches, do not appear to have a signi cant impact on evaluation results. Working directly on live data began in the TREC 2015 Microblog Track and continued through last year's iteration of RTS; thus, we consider this design fairly mature. Due to the transient nature of the collection, for archival purposes, both the University of Waterloo and NIST separately collected the live Twi er stream.</p><p>Despite super cial similarities, our task is very di erent from document ltering in the context of earlier TREC Filtering Tracks, which ran from 1995 <ref type="bibr" coords="2,130.35,348.94,10.44,8.97" target="#b3">[4]</ref> to 2002 <ref type="bibr" coords="2,171.09,348.94,13.23,8.97" target="#b10">[11]</ref>, and the general research program known as Topic Detection and Tracking (TDT) <ref type="bibr" coords="2,243.01,359.90,9.27,8.97" target="#b0">[1]</ref>. e TREC Filtering Tracks are best understood as binary classi cation on every document in the streaming collection with respect to standing queries, and TDT is similarly concerned with identifying all documents related to a particular event-with an intelligence analyst in mind. In contrast, we are focused on identifying a small set of the most relevant updates to deliver to users. Furthermore, in both TREC Filtering and TDT, systems must make online decisions as soon as documents arrive. In our case, for scenario A, systems can choose to deliver older posts (latency is one aspect of the evaluation), thus giving rise to the possibility of algorithms operating on bounded bu ers, trading o latency for quality. Finally, previous evaluations, including TDT, TREC Filtering, and Temporal Summarization, merely simulated the streaming nature of the document collection, whereas participants in our evaluation were actually required to process tweets posted in real time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Interest Pro les</head><p>Interest pro les for real-time summarization are di cult to develop because of their prospective nature-this was one of the key lessons learned from previous iterations of the evaluation. For retrospective ad hoc topics over a static collection, it is possible for topic developers to explore the document collection to get a sense of the amount of relevant material, range of topical facets, etc. for a particular information need. Typically, topic developers prefer information needs that have neither too many nor too few relevant documents. is is not possible for RTS interest pro les, since they essentially require pro le authors to "predict the future".</p><p>For this year's evaluation, a total of 188 new interest pro les were created: 148 interest pro les were developed by NIST assessors and 40 additional interest pro les were contributed by the mobile assessors (i.e., assessors who were recruited for the in situ evaluation; see Section 2.3 for more details). e la er set of pro les meant that mobile assessors interacted with tweets retrieved for their own information needs.</p><p>NIST contracted six assessors to develop interest pro les around topics that were likely to be discussed around the time of the evaluation. During the rst week of June 2017, they used a web interface 2 to search a collection of tweets from the public Twi er sample stream, the same source the participants used in the evaluation.</p><p>e collection consisted of tweets from August 2016, March 2017, and May 2017. Tweets from August 2016 were provided so that the assessors could get a sense of topics that are typically discussed in August. e tweets from March and May 2017 were provided so that the assessors could examine more recent content on Twitter. e time gap allowed the assessors to examine the temporal characteristics of topics being considered.</p><p>Using an interactive interface, the assessors were able to explore the collection by: (1) issuing a query, (2) clustering the search results, and (3) hiding or showing media associated with the tweets (images, videos, link previews, etc.). Once a query was issued to the system, the assessor could select the month from which results were shown. Apart from the top 100 ranked tweets, the total number of retrieved tweets was also displayed, which provided an indication of the size of the underlying topic. e assessors were asked to develop interest pro les that were not too big (less than 1000 tweets in a given month) but also not too small (at least 50 tweets in a given month). Assessors were additionally asked to provide relevance judgments for some of the tweets, to obtain a more reliable indication of the topic size. If the result of a query was too big, assessors could cluster the tweets. Clustering might surface additional search terms that retrieve topically similar tweets, and in this way the assessor could examine subtopics of the initial information need.</p><p>Following last year's RTS evaluation and the TREC 2015 Microblog Track before that, we adopted the "standard" TREC ad hoc topic format of "title", "description", and "narrative" for the interest pro les. e so-called title consists of a few keywords that provide the gist of the information need, akin to something a user might type into the query box of a search engine.</p><p>e description is a one-sentence statement of the information need, and the narrative is a paragraph-length chunk of prose that sets the context of the need and expands on what makes a tweet relevant. By necessity, these interest pro les are more generic than the needs expressed in typical retrospective topics because the assessor does not know what future events will occur. us, despite super cial similarities in format, we believe that interest pro les are qualitatively di erent from ad hoc topics.</p><p>e initial set of NIST-created interest pro les were publicly posted on the track website on July 13, 2017. e mobile assessors were asked to select the pro les they were interested in assessing, along with an option of supplying their own. ey were not given speci c instructions other than pointers to the NIST-developed interest pro les as reference. A er gathering interest pro les from  the mobile assessors, the organizers checked them for appropriateness and lightly edited the submi ed prose for forma ing and standardization, but without changing the original intents. For example, by convention, terms from the title should also appear in the description section of the interest pro le, and so we modi ed the pro les accordingly. e nal set of interest pro les (including the ones created by the mobile assessors) were publicly posted on July 21, 2017. <ref type="foot" coords="3,101.73,340.91,3.38,7.27" target="#foot_0">3</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Scenario A: Mobile Delivery</head><p>As in the RTS Track in TREC 2016, scenario A implemented a user evaluation whereby system outputs are delivered to the mobile devices of assessors in real time, who examine the tweets in situ. is general approach builds on growing interest in so-called "Living Labs" <ref type="bibr" coords="3,105.35,422.13,14.85,8.97" target="#b13">[14]</ref> and related Evaluation-as-a-Service (EaaS) <ref type="bibr" coords="3,283.37,422.13,10.68,8.97" target="#b2">[3]</ref> approaches that a empt to be er align evaluation methodologies with user task models and real-world constraints to increase the delity of research experiments. Our evaluation architecture is shown in Figure <ref type="figure" coords="3,235.81,465.96,3.05,8.97" target="#fig_1">1</ref>. All participating systems "listened" to the live Twi er sample stream during the evaluation period, and as the systems identi ed relevant tweets, they were submi ed to the RTS evaluation broker, which immediately recorded each tweet and delivered it to mobile assessors who provided judgments in situ-i.e., they were going about their daily lives and could choose to evaluate as many or as few tweets as they wished, whenever they wanted. We have, in e ect, built an A/B testing infrastructure for real-time summarization. is evaluation architecture was rst described in Roegiest et al. <ref type="bibr" coords="3,233.28,564.59,14.72,8.97" target="#b12">[13]</ref> and all code is available on GitHub. <ref type="foot" coords="3,137.10,573.57,3.38,7.27" target="#foot_1">4</ref>e entire evaluation was framed as a user study (with appropriate ethics approval). A few weeks prior to the beginning of the evaluation period, we recruited assessors from two sources: the undergraduate and graduate student population at the University of Waterloo (via posts on various email lists as well as personal contacts) and RTS participants on the track mailing list. We speci cally targeted the track participants so that system developers could gain a be er intuition for the types of output that RTS systems produced. All assessors were compensated $1 CAD per 20 judgments.</p><p>As part of the onboarding process, assessors selected (i.e., "subscribed to") interest pro les (from the NIST-developed set) they were interested in judging. To encourage diversity, we did not allow more than four assessors to select the same pro le (on a rst come, rst served basis). e assignment of interest pro les was interwoven with the solicitation of additional interest pro les from the mobile assessors themselves. However, before the evaluation period began, we arrived at a xed and static mapping between interest pro les and assessors, which determined which mobile assessor saw which tweets.</p><p>From an RTS participant's perspective, prior to the beginning of the evaluation period, each participant's system "registered" with the RTS evaluation broker via a REST API call to request a unique token, which was used in all subsequent interactions with the broker to associate all submi ed tweets with that system. <ref type="foot" coords="3,533.16,237.37,3.38,7.27" target="#foot_2">5</ref> Each system was allowed to submit at most ten tweets per interest pro le per day.</p><p>is tweet delivery limit represents a crude a empt to model user fatigue.</p><p>During the evaluation period, whenever a system identi ed a relevant tweet with respect to an interest pro le, the system submi ed the tweet id to the RTS evaluation broker via a REST API. e broker recorded the submission time, saved the tweet to a database, and immediately delivered the tweet to the LIFO (last in, rst out) assessment queues of all mobile assessors who had subscribed to the interest pro le. For convenience, we refer to each of these queues as the assessor's "inbox". Note that each tweet was delivered only once, even if it was submi ed by multiple systems at di erent times. is design operationalizes the temporal interleaving strategy proposed by Qian et al. <ref type="bibr" coords="3,466.94,392.78,13.42,8.97" target="#b9">[10]</ref>. Note that, critically, unlike last year, the delivery of a tweet was not accompanied by a push noti cation. at is, there was no explicit cue (noti cation message, chime, vibration, etc.) that a new tweet had been added to the assessor's inbox.</p><p>Mobile assessors provided judgments via a webapp that was speci cally designed for mobile devices; a screenshot is shown in Figure <ref type="figure" coords="3,343.10,469.49,3.02,8.97" target="#fig_2">2</ref>. e assessment interface was derived from so ware built by the University of Waterloo's team that participated in the TREC 2017 Core Track <ref type="bibr" coords="3,378.30,491.41,13.22,8.97" target="#b17">[18]</ref>. Substantial e ort was devoted to re ning the user experience and making the interface as responsive as possible.</p><p>e interface is divided into three sections: the interest pro le, the tweet, and the judgment bu ons. e top section shows the pro le title and description for which a system posted the tweet.</p><p>e widget in the middle shows the tweet, rendered using Twi er's API, which meant that the tweet appeared exactly as it would on Twi er's own clients (with proper preview of embedded content such as links and videos). e assessor can further interact with the embedded content, e.g., click on a link, watch a video, etc. Finally, there are three bu ons at the bo om of the screen for the assessor to render a judgment:</p><p>• relevant, if the tweet contains relevant and novel information;</p><p>• redundant (i.e., duplicate), if the tweet contains relevant information, but is substantively similar to another tweet that the assessor had already seen; • not relevant, if the tweet does not contain relevant information. Once the assessor taps one of the bu ons, the judgment is registered by the server. e page disappears and the next tweet in the queue is displayed. ere is no way to modify a judgment once it has been provided.</p><p>Previously mentioned but worth emphasizing: the mobile assessors provided judgments in situ, i.e., as they were going about their daily lives. In contrast to the push noti cation setup from last year, this year's design can be characterized as pull-based: that is, the assessors, on their own initiative, pulled relevant content to examine from their inboxes. We had no control over how frequently they visited the assessment interface, how many tweets they assessed, or any other aspect of assessment behavior.</p><p>Finally, to close the loop, the RTS broker provided an API for each participant's system to retrieve relevance judgments for tweets that it had posted (speci cally, a system did not have access to relevance judgments for tweets posted by another system). For rate limiting purposes, we asked participants not to call this API more than once per hour, but the constraint was not enforced.</p><p>is feature was a major addition to the RTS evaluation this year, and the design allowed participants to, for the rst time, experiment with relevance feedback and active learning techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Scenario B: Email Digests</head><p>e overall evaluation setup for scenario B is shown in Figure <ref type="figure" coords="4,289.16,678.68,3.13,8.97" target="#fig_3">3</ref>. As with scenario A, participants "listened" to the live Twi er sample stream to identify relevant tweets with respect to the interest</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stream of Tweets</head><p>Participating Systems</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Locally-Stored Runs</head><p>Twitter API bulk upload pro les. Each system was tasked with identifying up to 100 tweets per day per interest pro le, which are putatively delivered to the user daily. For simplicity, all tweets from 00:00:00 to 23:59:59 UTC are valid candidates for that particular day. It was expected that systems would compute the results in a relatively short amount of time a er the day ends (e.g., at most a few hours), but this constraint was not enforced. Each system recorded its own results (i.e., ranked lists) for each day, which were then uploaded to NIST servers in batch shortly a er the evaluation period ended. e per-day limit of 100 tweets was arbitrarily set, but at a value that is larger than what one might expect from a daily email digest, primarily to enrich the judgment pool (more details in Section 4). As with scenario A, we neglected to model real-world constraints in favor of simplicity, since de ning a "day" in terms of UTC does not take into account the reading habits of users in di erent time zones around the world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Submissions Types</head><p>For both scenarios, systems were asked to only consider tweets in English. Each team was allowed to submit up to three runs for scenario A and three runs for scenario B. Runs were categorized into three di erent types based on the level of human involvement:</p><p>• Automatic Runs: In this condition, system development (including all training, system tuning, etc.) must conclude prior to downloading the interest pro les from the track homepage (which were made available before the evaluation period). e system must operate without human input before and during the evaluation period. Note that it is acceptable for a system to perform processing on the pro les (for example, query expansion) before the evaluation period, but such processing cannot involve human input. • Manual Preparation: In this condition, the system must operate without human input during the evaluation period, but human involvement is acceptable before the evaluation period begins (i.e., a er downloading the interest pro le). Examples of manual preparation include human examination of the interest pro les to add query expansion terms or manual relevance assessment on a related collection to train a classi er. However, once the evaluation period begins, no further human involvement is permissible.</p><p>• Manual Intervention: In this condition, there are no limitations on human involvement before or during the evaluation period. Crowd-sourced judgments, human-in-the-loop search, etc. are all acceptable.</p><p>Note that judgments provided by the mobile assessors did not count as manual intervention for determining the run type category. For example, a run that exploited relevance feedback using mobile judgments could still be classi ed as an automatic run as long as there was no additional human input from the system's developers.</p><p>Participants were asked to designate the run type at submission time for the scenario B runs when they uploaded their results to NIST. For scenario A runs, we asked each team about the type of each of their runs over email a er the evaluation period.</p><p>All types of systems were welcomed; in particular, manual preparation and manual intervention runs are helpful for understanding human performance and for enriching the judgment pool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Runs Postprocessing</head><p>A detail worth discussing is postprocessing performed by the organizers to create the "o cial" scenario A runs. Due to the nature of a live evaluation, there might be minor di erences between records of posted tweets at the RTS broker and from the perspective of each individual system, for example, due to incomplete API requests. For the purposes of the evaluation, the record of activity at the RTS broker constituted the "ground truth".</p><p>e postprocessing had another speci c purpose: a er the evaluation began, we discovered a clock synchronization issue between the RTS broker and its backend database that allowed clients to submit more tweets than the ten-per-day limit. is bug was xed and the RTS broker was restarted on July 31, 2017 around 1pm EDT. To be er conform to the evaluation guidelines, we truncated runs that submi ed more than ten tweets on any day to the rst ten tweets submi ed on that day. e postprocessed "o cial" runs were provided back to the RTS participants, and these runs served as input to the evaluation scripts whose results we report in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">IN-SITU EVALUATION METRICS</head><p>In this section we describe how judgments from the mobile assessors in scenario A (see Section 2.3) are aggregated into evaluation metrics to quantify the e ectiveness of each run. At a high level, the RTS broker interleaved submi ed tweets from participating systems, delivered them to the mobile devices of assessors, and gathered a stream of judgments: whether a tweet is relevant, redundant, or not relevant with respect to an interest pro le. Because each tweet was delivered to all assessors who had subscribed to the pro le, the broker might have received more than one judgment per tweet.</p><p>Another implication of the interleaved evaluation setup is that an assessor may have encountered tweets from di erent systems, which makes proper interpretation of redundant judgments di cult. A tweet might only appear redundant because the same information was contained in a tweet delivered earlier by another system (and thus it was not the "fault" of the system that submi ed the tweet). In other words, the interleaving of outputs from di erent systems was responsible for introducing the redundancy. Furthermore, since assessors were always examining the most recent tweet rst, a more recent tweet might have caused an older tweet to appear redundant. ese are, unfortunately, unavoidable consequences of "messy" user evaluations, and systems must be designed with the ability to interpret noisy relevance signals.</p><p>To measure the e ectiveness of a run, we computed two aggregate metrics based on user judgments: Online Precision. A simple and intuitive metric is to compute precision, or the fraction of relevant judgments:</p><formula xml:id="formula_0" coords="5,372.42,195.84,185.78,21.19">relevant relevant + redundant + not relevant<label>(1)</label></formula><p>We term this "strict" precision because systems don't get credit for redundant judgments. Also, we can compute "lenient" precision, where systems do receive credit for redundant judgments:</p><formula xml:id="formula_1" coords="5,372.42,265.83,185.78,21.19">relevant + redundant relevant + redundant + not relevant (2)</formula><p>Two additional details are necessary for the proper interpretation of these metrics: First, tweets may be judged multiple times since each tweet was delivered to all users who had subscribed to the pro le. For simplicity, all judgments were included in our calculation. Second, our precision computations represent a micro-average (and not an average of per-pro le precision). is choice was made because di erent pro les received di erent numbers of judgments, and thus macro-averaging would magnify the e ects of interest pro les with few judgments.</p><p>Online Utility. As an alternative to online precision, we can take a utility-based perspective and measure the total gain received by the user. e simplest method would be to compute the following:</p><formula xml:id="formula_2" coords="5,372.42,433.30,185.78,8.97">relevant -redundant -not relevant<label>(3)</label></formula><p>which we refer to as the "strict" variant of online utility. Paralleling the precision variants above, we de ne a "lenient" version of the metric as follows:</p><formula xml:id="formula_3" coords="5,369.52,490.18,188.69,8.97">(relevant + redundant) -not relevant<label>(4)</label></formula><p>Of course, we could further generalize online utility with weights for each type of judgment. However, we lacked the empirical basis for se ing the weights and thus did not choose to do so.</p><p>To summarize: from user judgments, we computed two aggregate metrics-online precision and online utility. Note that there is no good way to compute a recall-oriented metric since we have no control over when and how frequently user judgments are provided. Finally, following last year's RTS evaluation, we made strict precision the primary metric for assessing scenario A runs using mobile assessors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">BATCH EVALUATION METRICS</head><p>In this section we describe the batch evaluation methodology and metrics used to evaluate both scenario A and scenario B runs. Note that scenario A runs were assessed using both the mobile assessor judgments (described in the previous section) as well as the batch methodology described here. Scenario B runs were evaluated with the batch methodology only.</p><p>At a high level, we adopted the Tweet Timeline Generation (TTG) evaluation methodology that was originally developed for the TREC 2014 Microblog Track <ref type="bibr" coords="6,169.85,107.85,10.43,8.97" target="#b5">[6]</ref> and was used both in the TREC 2015 Microblog Track <ref type="bibr" coords="6,139.37,118.80,10.68,8.97" target="#b6">[7]</ref> and the RTS Track last year <ref type="bibr" coords="6,262.80,118.80,9.52,8.97" target="#b7">[8]</ref>. e methodology is mature, in that it has been externally validated <ref type="bibr" coords="6,279.45,129.76,14.60,8.97" target="#b16">[17]</ref> and similar approaches have been deployed in evaluations dating back at least a decade. e assessment work ow proceeded in two stages: relevance assessment and semantic clustering. Both were performed by NIST assessors.</p><p>Relevance assessments were performed using pooling with a single pool across both scenario A and scenario B runs. e pools were built using all scenario A tweets (a er postprocessing, see Section 2.6) and a maximum of 90 tweets per pro le for each scenario B run, the same as last year. To select the nal set of interest pro les to assess, we removed from consideration pro les that had fewer than ten relevant judgments (i.e., were too sparse and/or too di cult) or had greater than 60% precision (i.e., were too easy) in the mobile judgments.</p><p>is still le too many pro les to judge, so NIST eliminated additional pro les by hand, discarding those whose pools were too large and culling pro les that were topically similar. Each pool was judged by the assessor who authored the pro le, although some assessors were given other interest pro les to judge as well. NIST sta also contributed some judgments, including pro les for which they were the author. In total, 96 interest pro les were judged. One additional interest pro le (RTS107) had only one tweet, marked as missing (see below).</p><p>e pools contained 94,307 tweets in total. e maximum number of tweets for an interest pro le was 1585, the minimum was one (RTS107), and on average there were 972 tweets per pro le.</p><p>ese pools were then judged by NIST assessors for relevance. To facilitate consistent judgments, tweets were rst clustered and presented in order of lexical similarity. Each tweet was independently assessed on a three-way scale of "not relevant", "relevant", and "highly relevant". Non-English tweets were marked as not relevant by at. If a tweet contained a mixture of English and non-English content, discretion was le to the assessor. As with previous TREC Microblog evaluations, assessors examined links embedded in tweets, but did not explore any additional external content. Retweets did not receive any special treatment and were assessed just like any other tweet.</p><p>e assessment interface rendered tweets using Twi er's o cial API, which meant that content appeared exactly as it would on the Twi er o cial site (for example, with previews of embedded content such as links and videos). Because of this, however, tweets submi ed by systems but were deleted prior to assessment could not be shown. ese tweet are speci cally marked as missing in the nal qrels.</p><p>In rendering judgments, the NIST assessors tried to maintain consistency with judgments made by the mobile assessors. In the assessment interface, next to each tweet, there are indicators of the number of mobile assessors who judged the tweet as relevant and not relevant. Since the tweets were presented in cluster order, the e ect of this interface design is that the temporal sequencing of mobile judgments was lost to the assessors-potentially, for example, re ecting evolving notions of relevance regarding an interest pro le. Speci cally, the assessors were provided the following guidance in writing: Because the systems might have adapted to the judgments they saw, we want you to judge tweets consistently with the existing judgments, to the extent possible. You are not required to always take the majority judgment, and you are not required to agree with a single judge if you are convinced that the [mobile] judgment is just plain wrong (because it might be). But if the tweet represents a gray area that you could legitimately assess in either direction, please opt to go with the [mobile assessor].</p><p>Anecdotally, most assessors independently re ected that the mobile assessments were poor in quality.</p><p>A er the relevance assessment process, the NIST assessors proceeded to perform semantic clustering on the relevant tweets using the Tweet Timeline Generation (TTG) protocol, originally developed for the TREC 2014 Microblog Track <ref type="bibr" coords="6,470.27,459.50,9.33,8.97" target="#b5">[6,</ref><ref type="bibr" coords="6,481.84,459.50,10.13,8.97" target="#b16">17]</ref>.</p><p>e TTG protocol was designed to reward novelty (or equivalently, to penalize redundancy) in system output. In both scenario A and scenario B, we assume that users would not want to see multiple tweets that "say the same thing", and thus the evaluation methodology should reward systems that eliminate redundant output. Following the TTG protocol, we operationalized redundancy as follows: for every pair of tweets, if the chronologically later tweet contains substantive information that is not present in the earlier tweet, the later tweet is considered novel; otherwise, the later tweet is redundant with respect to the earlier one. In our de nition, redundancy and novelty are antonyms, so we use them interchangeably but in opposite contexts.</p><p>Due to the temporal constraint, redundancy is not symmetric. If tweet A precedes tweet B and tweet B contains substantively similar information found in tweet A, then B is redundant with respect to A, but not the other way around. We also assume transitivity. Suppose A precedes B and B precedes C: if B is redundant with respect to A and C is redundant with respect to B, then by de nition C is redundant with respect to A.</p><p>For semantic clustering, the assessors were shown all the relevant tweets for an interest pro le (from the previous stage) in a custom assessment interface (see Figure <ref type="figure" coords="6,469.96,700.59,4.25,8.97" target="#fig_4">4</ref> for a screenshot). e tweets were shown in the le pane in the same order as during the relevance assessment process, such that lexically similar tweets were displayed next to each other, and the list of current clusters were shown in a pane on the right side. Tweets were also rendered with Twi er's o cial API, and so it was possible for assessors to encounter a deleted tweet during the clustering stage. ese tweet ids were also marked missing in the nal qrels. For each tweet in the le pane, the assessor could either use that tweet as the basis for a new cluster or add it to one of the existing clusters. In this way, clusters representing important pieces of information, comprised of semantically similar tweets, were constructed incrementally. To aid in the clustering process, assessors could enter a short textual description for each cluster. Assessors could also move tweets between clusters and mark a tweet as not relevant, in case they changed their mind. e instructions given to the assessors did not specify a particular target number of clusters to form. Instead, they were asked to use their best judgment, considering both the scope of the interest pro les and the actual tweets.</p><p>e nal output of the batch assessment process (for each interest pro le) is a list of clusters, where tweets in each cluster represent a particular "facet" of information that addresses the user's need.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Scenario A Metrics</head><p>For scenario A, we computed a number of metrics from the relevance judgments and clusters provided by NIST assessors, detailed below. At a high level, mobile delivery of content should be relevant (on topic), novel (users should not be shown multiple tweets that say the same thing), and timely (provide updates as soon a er the actual event occurrence as possible). Following the 2016 iteration of the track-instead of devising single-point metrics that a empt to incorporate relevance, novelty, and timeliness, the o cial metrics separately quantify output quality (relevance and novelty) and latency (timeliness).</p><p>We envision that systems might trade o latency with output quality: For example, a system might wait to accumulate evidence before submi ing tweets, thus producing high-quality output at the cost of high latency. Alternatively, a low-latency system might aggressively submit results that it might "regret" later. Computing metrics of output quality separately from latency allows us to understand the potential tradeo s. Additionally, we believe this approach is appropriate because we have no empirical evidence as to what the "human response curve" to latency looks like-that is, how much should we discount a quality metric based on tardiness? A empting to formulate a single-point metric collapses meaningful distinctions in what users may be looking for in systems.</p><p>Expected Gain (EG) for an interest pro le on a particular day is de ned as follows:</p><formula xml:id="formula_4" coords="7,154.88,621.41,139.17,21.05">1 N G(t )<label>(5)</label></formula><p>where N is the number of tweets returned and G(t ) is the gain of each tweet:</p><p>• Not relevant tweets receive a gain of 0.</p><p>• Relevant tweets receive a gain of 0.5.</p><p>• Highly-relevant tweets receive a gain of 1.0.</p><p>Once a tweet from a cluster is retrieved, all other tweets from the same cluster automatically become not relevant. is penalizes systems for returning redundant information.</p><p>Normalized Cumulative Gain (nCG) for an interest pro le on a particular day is de ned as follows:</p><formula xml:id="formula_5" coords="7,418.55,149.95,139.65,20.29">1 Z G(t )<label>(6)</label></formula><p>where Z is the maximum possible gain (given the ten tweet per day limit). e gain of each individual tweet is computed as above. Note that gain is not discounted (as in nDCG) because the notion of document ranks is not meaningful in this context. e score for a run is the mean of scores for each day over all the interest pro les. Since each pro le contains the same number of days, there is no distinction between micro-and macro-averages.</p><p>An interesting question, which has only recently been empirically "resolved", is how scores should be computed for days in which there are no relevant tweets: for rhetorical convenience, we call days in which there are no relevant tweets for a particular interest pro le (in the pool) "silent days", in contrast to "eventful days" (where there are relevant tweets). Tan et al. <ref type="bibr" coords="7,504.99,308.80,14.85,8.97" target="#b15">[16]</ref> examined this issue and proposed two metric variants, which were adopted in 2016 <ref type="bibr" coords="7,347.46,330.72,9.63,8.97" target="#b7">[8]</ref>: In the EG-1 and nCG-1 variants of the metrics, on a "silent day", a system receives a score of one (i.e., a perfect score) if it does not submit any tweets to the broker, or zero otherwise. In the EG-0 and nCG-0 variants of the metrics, for a silent day, all systems receive a gain of zero no ma er what they do. erefore, under EG-1 and nCG-1, systems are rewarded for recognizing that there are no relevant tweets for an interest pro le on a particular day and remaining silent (i.e., does not submit any tweets to the broker). e EG-0 and nCG-0 variants of the metrics do not reward recognizing silent days: that is, it never hurts to submit tweets. Recently, Roegiest et al. <ref type="bibr" coords="7,465.24,440.31,14.85,8.97" target="#b11">[12]</ref> concluded that EG-0 and nCG-0 are awed metrics precisely for this reason. ese metrics correlate with volume (number of tweets submi ed to the broker), and only weakly to sensible metrics of quality.</p><p>EG-1 and nCG-1, however, are both binary on a silent day (i.e., either zero or one), which makes optimization di cult because of a discontinuity. As a remedy, this year we introduced EG-p and nCG-p (p for proportional), where on a silent day, the score is one minus the fraction of the ten-tweet daily quota that is used. For example, if a system submits zero tweets, it receives a score of 1.0; if it submits one tweet, a score of 0.9; two tweets, 0.8; etc., such that if a system uses up its entire quota of ten tweets for a day, it receives a score of zero. EG-p and nCG-P still reward systems for recognizing silent days, but with a penalty that is proportional to how "quiet" the system is. EG-p was adopted as the primary metric (i.e., the sort key in the results table).</p><p>Gain Minus Pain (GMP) is de ned as follows:</p><formula xml:id="formula_6" coords="7,398.71,640.28,159.50,8.97">α • G -(1 -α ) • P<label>(7)</label></formula><p>e G (gain) is computed in the same manner as above. Pain P is the number of non-relevant tweets that the system submi ed, and α controls the balance between the two. We investigated three se ings: 0.33, 0.50, and 0.66. Note that this metric is the same as the linear utility metric used in the TREC Filtering Tracks <ref type="bibr" coords="7,534.00,700.59,9.39,8.97" target="#b3">[4,</ref><ref type="bibr" coords="7,545.62,700.59,10.17,8.97" target="#b10">11]</ref>, albeit with a di erent mathematical form. us, our metric builds squarely on previous work.</p><p>Latency. In addition to the quality metrics above, we report, for tweets that contribute to gain, the mean and median di erence between the time the tweet was delivered and the rst tweet in the semantic cluster that the tweet belongs to (based on the NIST assessors). For example, suppose tweets A, B, and C are in the same semantic cluster, and were authored at 09:00, 10:00, and 11:30, respectively. No ma er which of the three tweets was submi ed by a system, latency is computed with respect to the creation time of A (09:00). erefore, posting tweet C at 11:30 and posting tweet A at 11:30 yields the same latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Scenario B Metrics</head><p>Scenario B runs were evaluated in terms of nDCG as follows: for each interest pro le, the list of tweets returned per day is treated as a ranked list, and from this nDCG@10 is computed. Note that in this scenario, the evaluation metric does include gain discounting because email digests can be interpreted as ranked lists of tweets. Gain is computed in the same way as in scenario A with respect to the semantic clusters. Systems only receive credit for the rst relevant tweet they retrieve from a cluster. e score of an interest pro le is the mean of the nDCG scores across all days in the evaluation period, and the score of a run is the mean of scores for each pro le. Once again, the micro-vs. macro-average distinction is not applicable here. As with scenario A, we computed two variants of the metric: With nDCG-1, on a "silent day", the system receives a score of one (i.e., a perfect score) if it does not submit any tweets, or zero otherwise. With nDCG-p (p for proportional), the de nition is the same as in scenario A: on a silent day, the score is 1 -min(n, 10)/10 where n is the number of tweets submi ed for that day.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Scenario A</head><p>For scenario A, we received a total of 41 runs from 15 groups. ese runs submi ed a total of 78,556 tweets, or 50,124 unique tweets a er de-duplicating within each interest pro le (but not across interest pro les).</p><p>For the in situ mobile evaluation of scenario A systems, we recruited 42 assessors (six of whom were from participating teams). Over the entire evaluation period, we received 85,525 judgments, with a minimum of 16 and a maximum of 14,441 by an individual assessor. We found that 17,140 tweets received a single judgment, 18,306 tweets received two judgments, 8,671 tweet received three judgments, and 1,440 tweets received four judgments. All 188 interest pro les received at least one judgment; one pro le received 81 judgments; 129 received (100, 500] judgments; 50 received (500, 1000] judgments; seven received (1000, 2000] judgments; one received 2074 judgments. On average, the mobile assessors submitted 455 judgments per pro le.</p><p>e distribution of judgments by assessor is shown in Table <ref type="table" coords="8,289.16,667.72,3.13,8.97" target="#tab_0">1</ref>. e columns list: assessor id, the number of judgments provided, the number of pro les subscribed to, and the number of tweets delivered to that assessor. e nal column shows the response rate, computed as the ratio between the second and fourth columns. Note that these statistics include judgments of tweets that may have been subsequently removed in postprocessing, as described in Section 2.6. We see that there were quite a few highly-motivated assessors who judged nearly all the tweets that were delivered to them for the pro les they subscribed to; in one case, a particularly "diligent" assessor provided over 14k judgments.</p><p>Results of the in situ evaluation by the mobile assessors are shown in Table <ref type="table" coords="8,378.30,173.60,3.13,8.97" target="#tab_1">2</ref>. e rst two columns show the participating team and run. e next columns show the number of tweets that were judged relevant (R), redundant (D), and not relevant (N); the number of unjudged tweets (U); the length of each run (L), de ned as the total number of messages delivered by the system. e next column shows coverage (C), de ned as the fraction of unique tweets that were judged. Following that, the columns report the mean ( t) and median ( t) latency of submi ed tweets in seconds, measured with respect to the time the original tweet was posted.</p><p>e next sets of columns provide metrics of quality: strict and lenient precision, strict and lenient utility. e nal column shows the run type: 'A' denotes automatic and 'P' manual preparation; '?' indicates unknown (we did not receive a response from one team, despite repeated inquiries). e rows in the table are sorted by strict precision.</p><p>Results of the batch evaluation by NIST assessors are shown in Table <ref type="table" coords="8,339.32,348.94,3.01,8.97" target="#tab_2">3</ref>. e columns list the various metrics discussed in Section 4 and also the mean and median latency in seconds. Note that latency here is computed with respect to the rst tweet in each cluster (which is di erent from how latency is computed with respect to the mobile assessors' judgments), and thus a system may have a high latency even if it submits a tweet immediately a er it is posted. e second to last column shows the length of each run, de ned as the number of tweets posted for the interest pro les that were assessed. e nal column shows the run type: 'A' denotes automatic and 'P' manual preparation; '?' indicates unknown. e rows in the table are sorted by EG-p. For reference, an empty run (i.e., a system that does not submit any tweets) would receive a score of 0.1765 for EG-p/EG-1 and nCG-p/nCG-1 (with all other scores being zero).</p><p>We examined the correlations between strict precision (mobile metric) and expected gain variants (batch metric) in Figure <ref type="figure" coords="8,528.07,513.33,3.01,8.97" target="#fig_5">5</ref>, which shows sca erplots with EG-p (le ) and EG-1 (right). Each blue square represents an individual run. For ease of comparison, both plots have the same scales. We observe a higher correlation between EG-1 and strict precision than between EG-p and strict precision; this is con rmed by the R 2 values from applying linear regression. In fact, for systems with roughly the same strict precision-a vertical band with many runs between 0.35 and 0.40-there is quite a big range in EG-p scores.</p><p>In Figure <ref type="figure" coords="8,362.72,611.96,3.02,8.97">6</ref>, we plot mobile quality metrics against latency: each scenario A run is represented by a blue square. For ease of comparison, corresponding plots have the same scales. We note that most systems have very low latency-they appear to submit tweets almost immediately a er they are posted. However, there are a number of runs that exhibit much higher latency. ese runs do not appear to be able to achieve be er quality as measured by online precision or online utility. In other words, at least according to metrics derived from the mobile judgments, systems were not able to e ectively exploit the additional relevance signals that accumulate over time if tweets are not immediately submi ed.</p><p>In Figure <ref type="figure" coords="9,99.28,107.85,3.09,8.97">7</ref>, we plot batch quality metrics against latency: each scenario A run is represented by a blue square. In contrast to the mobile metrics, the runs that achieved the highest EG and nCG scores (but not GMP) are those with high latency. It seems that, from the perspective of the batch evaluation metrics (unlike metrics derived from the mobile assessors), systems were successful in exploiting signals that only become available if tweets are not submi ed immediately. at is, waiting to accumulate evidence before deciding to submit tweets a ords an opportunity to achieve higher quality-but of course, at the cost of incurring higher latency. Interestingly, this seems to be a new development in this year's evaluation. at is, systems this year were able to trade latency for higher quality (most pronounced in EG-p). From last year's evaluation, in contrast, the best high-latency system scored no higher in EG-1 than the best system that pushed tweets immediately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Scenario B</head><p>For scenario B, we received a total of 40 runs from 15 groups. Evaluation results based on NIST assessors are shown in Table <ref type="table" coords="9,267.12,309.08,3.04,8.97" target="#tab_3">4</ref>. Runs are sorted by nDCG-p. For reference, the empty run would have received nDCG-p and nDCG-1 scores of 0.1765.</p><p>e separation of quality metrics from latency allows us to unify the evaluation of scenario A and scenario B runs-we can simply convert scenario B runs into scenario A runs by pretending that up to ten tweets per day were submi ed at 23:59:59, and then running the evaluation scripts for scenario A exactly as Table <ref type="table" coords="9,265.92,385.79,4.09,8.97" target="#tab_4">5</ref> shows the results of such an evaluation setup by the mobile assessors, and Table <ref type="table" coords="9,76.80,407.71,4.25,8.97">6</ref> shows the results of such an evaluation based on NIST judgments. In Figures <ref type="figure" coords="9,139.90,418.67,6.26,8.97" target="#fig_5">5,</ref><ref type="figure" coords="9,148.94,418.67,3.13,8.97">6</ref>, and 7, all scenario B runs treated as scenario A runs are shown as empty black squares. In particular, such a treatment allows us to compare high-latency scenario A runs against scenario B runs. Interestingly, we nd that the best scenario B runs can achieve higher online precision than any "true" scenario A run. In terms of nCG, with the exception of an outlier, scenario B runs are quite e ective, which makes sense since delayed submission of tweets allows a system to be er accumulate evidence and achieve higher recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>e TREC 2017 RTS Track built on last year's evaluation to introduce additional novel elements. Continuing with live in situ evaluation using mobile assessors, we added a feedback mechanism that allowed systems to obtain judgments during the evaluation period and adapt their algorithms accordingly. is feature, coupled with information needs submi ed by the mobile assessors, further enhanced the realism of the evaluation setup. Healthy participation suggests continued interested in this problem, and our e orts will continue with another iteration of the track in TREC 2018.   e rst two columns show the participating team and run. e next columns show the number of tweets that were judged relevant (R), redundant (D), and not relevant (N); the number of unjudged tweets (U); the length of each run (L), de ned as the total number of messages delivered by the system. e next columns show coverage (C), de ned the fraction of unique tweets that were judged; the mean ( t) and median ( t) latency of submitted tweets in seconds, measured with respect to the time the original tweet was posted; strict and lenient precision; strict and lenient utility. e nal column shows the run type: 'A' denotes automatic, 'P' manual preparation, and '?' indicates unknown. Rows are sorted by strict precision. e columns marked "mean" and "median" show the mean and median latency with respect to the rst tweet in each cluster. e second to last column shows the length of each run, de ned as the number of tweets delivered for the interest pro les that were assessed. e nal column shows the run type: 'A' denotes automatic, 'P' manual preparation, and '?' indicates unknown. Rows are sorted by EG-p. e next columns show the number of tweets that were judged relevant (R), redundant (D), and not relevant (N); the number of unjudged tweets (U); the length of each run (L), de ned as the total number of messages delivered by the system. e next columns show coverage (C), de ned the fraction of unique tweets that were judged; the mean ( t) and median ( t) latency of submitted tweets in seconds, measured with respect to the time the original tweet was posted; strict and lenient precision; strict and lenient utility. e nal column shows the run type: 'A' denotes automatic, 'P' manual preparation, and 'I' manual intervention. Rows are sorted by strict precision.  <ref type="table" coords="17,77.67,538.72,3.45,7.94">6</ref>: Evaluation of scenario B runs as scenario A runs by NIST assessors. e columns marked "mean" and "median" show the mean and median latency with respect to the rst tweet in each cluster. e second to last column shows the length of each run, de ned as the number of tweets delivered for the interest pro les that were assessed. e nal column shows the run type: 'A' denotes automatic, 'P' manual preparation, and 'I' manual intervention. Rows are sorted by EG-p.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,53.80,192.91,240.25,7.94;3,53.80,203.87,241.85,7.94;3,53.80,214.83,241.85,7.94;3,53.80,225.79,240.24,7.94;3,53.45,236.74,183.38,7.94"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Evaluation setup for scenario A. Systems processed the Twitter sample stream in real time and submitted relevant tweets to the RTS evaluation broker, which immediately delivered the tweets to the mobile devices of assessors who had subscribed to those interest pro les.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,60.52,392.34,226.80,7.94"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Screenshot of the mobile assessment interface.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,317.96,192.91,240.25,7.94;4,317.96,203.87,241.85,7.94;4,317.96,214.83,241.85,7.94;4,317.96,225.79,213.22,7.94;4,90.70,82.68,166.45,295.91"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Evaluation setup for scenario B. Systems processed the Twitter sample stream in real time and stored their results locally during the evaluation period. A er the evaluation ended, the runs were uploaded to NIST in batch.</figDesc><graphic coords="4,90.70,82.68,166.45,295.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,342.23,242.69,191.70,7.94;6,317.96,82.70,247.12,146.24"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Screenshot of the clustering interface.</figDesc><graphic coords="6,317.96,82.70,247.12,146.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="10,53.80,261.22,504.40,7.94;10,53.80,272.18,505.94,7.94;10,63.98,283.14,464.47,7.94"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Scatterplots showing correlations between strict precision against EG-p (le ) and EG-1 (right). Each blue square represents a scenario A run and each empty square represents a truncated scenario B run treated as if it were a scenario A run. e horizontal red lines indicate the score of an empty run. Results of linear regression include scenario A runs only.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="10,53.80,644.57,504.40,7.94;10,53.47,655.53,504.74,7.94;10,53.80,666.49,253.10,7.94"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Scatterplots relating di erent mobile evaluation metrics to median latency. Each blue square represents a scenario A run and each empty square represents a truncated scenario B run treated as if it were a scenario A run. Top row: strict and lenient precision; Bottom row: strict and lenient online utility.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="12,53.50,85.95,504.70,502.36"><head>Table 1 :</head><label>1</label><figDesc>Assessor statistics. For each assessor, columns show the number of judgments provided, the number of interest pro les subscribed to, the number of tweets delivered to that assessor, and the response rate.</figDesc><table coords="12,202.36,85.95,207.28,471.63"><row><cell cols="5">Assessor Judgments Pro les Messages Response</cell></row><row><cell>1</cell><cell>7442</cell><cell>29</cell><cell>7565</cell><cell>98.37%</cell></row><row><cell>2</cell><cell>14441</cell><cell>68</cell><cell>14477</cell><cell>99.75%</cell></row><row><cell>3</cell><cell>1181</cell><cell>5</cell><cell>2100</cell><cell>56.24%</cell></row><row><cell>4</cell><cell>213</cell><cell>3</cell><cell>888</cell><cell>23.99%</cell></row><row><cell>5</cell><cell>575</cell><cell>3</cell><cell>624</cell><cell>92.15%</cell></row><row><cell>6</cell><cell>539</cell><cell>4</cell><cell>766</cell><cell>70.37%</cell></row><row><cell>7</cell><cell>1407</cell><cell>16</cell><cell>5469</cell><cell>25.73%</cell></row><row><cell>8</cell><cell>6120</cell><cell>59</cell><cell>13906</cell><cell>44.01%</cell></row><row><cell>9</cell><cell>2488</cell><cell>48</cell><cell>12987</cell><cell>19.16%</cell></row><row><cell>10</cell><cell>16</cell><cell>2</cell><cell>760</cell><cell>2.11%</cell></row><row><cell>11</cell><cell>281</cell><cell>16</cell><cell>5045</cell><cell>5.57%</cell></row><row><cell>12</cell><cell>2318</cell><cell>9</cell><cell>2490</cell><cell>93.09%</cell></row><row><cell>13</cell><cell>119</cell><cell>1</cell><cell>198</cell><cell>60.1%</cell></row><row><cell>14</cell><cell>193</cell><cell>4</cell><cell>1178</cell><cell>16.38%</cell></row><row><cell>15</cell><cell>2923</cell><cell>12</cell><cell>2929</cell><cell>99.8%</cell></row><row><cell>16</cell><cell>905</cell><cell>5</cell><cell>1068</cell><cell>84.74%</cell></row><row><cell>17</cell><cell>157</cell><cell>5</cell><cell>789</cell><cell>19.9%</cell></row><row><cell>18</cell><cell>3800</cell><cell>35</cell><cell>10063</cell><cell>37.76%</cell></row><row><cell>19</cell><cell>8407</cell><cell>71</cell><cell>19326</cell><cell>43.5%</cell></row><row><cell>20</cell><cell>49</cell><cell>5</cell><cell>1208</cell><cell>4.06%</cell></row><row><cell>21</cell><cell>8092</cell><cell>24</cell><cell>8487</cell><cell>95.35%</cell></row><row><cell>22</cell><cell>91</cell><cell>5</cell><cell>1401</cell><cell>6.5%</cell></row><row><cell>23</cell><cell>283</cell><cell>7</cell><cell>3463</cell><cell>8.17%</cell></row><row><cell>24</cell><cell>495</cell><cell>3</cell><cell>539</cell><cell>91.84%</cell></row><row><cell>25</cell><cell>83</cell><cell>3</cell><cell>1033</cell><cell>8.03%</cell></row><row><cell>26</cell><cell>43</cell><cell>4</cell><cell>1409</cell><cell>3.05%</cell></row><row><cell>27</cell><cell>194</cell><cell>7</cell><cell>1276</cell><cell>15.2%</cell></row><row><cell>28</cell><cell>530</cell><cell>3</cell><cell>1209</cell><cell>43.84%</cell></row><row><cell>29</cell><cell>706</cell><cell>6</cell><cell>1946</cell><cell>36.28%</cell></row><row><cell>30</cell><cell>675</cell><cell>16</cell><cell>3856</cell><cell>17.51%</cell></row><row><cell>31</cell><cell>152</cell><cell>3</cell><cell>1081</cell><cell>14.06%</cell></row><row><cell>32</cell><cell>899</cell><cell>25</cell><cell>8560</cell><cell>10.5%</cell></row><row><cell>33</cell><cell>1879</cell><cell>8</cell><cell>2225</cell><cell>84.45%</cell></row><row><cell>34</cell><cell>26</cell><cell>4</cell><cell>1277</cell><cell>2.04%</cell></row><row><cell>35</cell><cell>181</cell><cell>33</cell><cell>8317</cell><cell>2.18%</cell></row><row><cell>36</cell><cell>74</cell><cell>16</cell><cell>4835</cell><cell>1.53%</cell></row><row><cell>37</cell><cell>161</cell><cell>2</cell><cell>434</cell><cell>37.1%</cell></row><row><cell>38</cell><cell>220</cell><cell>3</cell><cell>857</cell><cell>25.67%</cell></row><row><cell>39</cell><cell>4404</cell><cell>14</cell><cell>4515</cell><cell>97.54%</cell></row><row><cell>40</cell><cell>9979</cell><cell>79</cell><cell>20640</cell><cell>48.35%</cell></row><row><cell>41</cell><cell>100</cell><cell>8</cell><cell>2549</cell><cell>3.92%</cell></row><row><cell>42</cell><cell>2684</cell><cell>15</cell><cell>2749</cell><cell>97.64%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="13,53.50,543.97,258.21,7.94"><head>Table 2 :</head><label>2</label><figDesc>Evaluation of scenario A runs by the mobile assessors.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="14,53.50,86.90,499.92,474.44"><head>Table 3 :</head><label>3</label><figDesc>Evaluation of scenario A runs by NIST assessors.</figDesc><table coords="14,60.75,86.90,492.67,454.72"><row><cell>team</cell><cell>run</cell><cell>EG-p</cell><cell cols="4">EG-1 nCG-p nCG-1 GMP .33 GMP .50 GMP .66</cell><cell cols="3">mean median length type</cell></row><row><cell>HLJIT</cell><cell>testRun2</cell><cell cols="6">0.3630 0.2088 0.2808 0.1266 -0.2720 -0.1566 -0.0479 119374</cell><cell>56744</cell><cell>621</cell><cell>A</cell></row><row><cell>HLJIT</cell><cell>testRun1</cell><cell cols="6">0.3318 0.1811 0.2610 0.1102 -0.3118 -0.1936 -0.0824 116649</cell><cell>49154</cell><cell>618</cell><cell>A</cell></row><row><cell>udel fang</cell><cell>UDInfoBL-run2</cell><cell cols="6">0.3226 0.2622 0.2489 0.1886 -0.1952 -0.1105 -0.0308 118653</cell><cell>55781</cell><cell>452</cell><cell>A</cell></row><row><cell>IRIT</cell><cell>IRIT-Run1</cell><cell cols="5">0.2918 0.2571 0.2321 0.1974 -0.1195 -0.0615 -0.0070</cell><cell>67555</cell><cell>1</cell><cell>320</cell><cell>A</cell></row><row><cell>udel fang</cell><cell>UDInfoSDWR-run1</cell><cell cols="6">0.2907 0.2571 0.2285 0.1949 -0.1190 -0.0622 -0.0087 126484</cell><cell>60685</cell><cell>308</cell><cell>A</cell></row><row><cell>PKUICST</cell><cell>PKUICSTRunA1</cell><cell cols="5">0.2869 0.2588 0.2864 0.2583 -0.7308 -0.4700 -0.2246</cell><cell>35761</cell><cell>36</cell><cell>1344</cell><cell>A</cell></row><row><cell>advanse</cell><cell>advanse lirmm-Run1</cell><cell cols="5">0.2686 0.2352 0.2835 0.2501 -0.7895 -0.5045 -0.2363</cell><cell>38389</cell><cell>1</cell><cell>1468</cell><cell>A</cell></row><row><cell>advanse</cell><cell>advanse lirmm-Run2</cell><cell cols="5">0.2653 0.2327 0.2728 0.2402 -0.7279 -0.4642 -0.2161</cell><cell>37570</cell><cell>1</cell><cell>1362</cell><cell>A</cell></row><row><cell>WUWien</cell><cell>WuWien-Run2</cell><cell cols="5">0.2640 0.2436 0.2737 0.2532 -0.4887 -0.3003 -0.1229</cell><cell>38666</cell><cell>1</cell><cell>996</cell><cell>A</cell></row><row><cell>advanse</cell><cell>advanse lirmm-Run3</cell><cell cols="5">0.2626 0.2298 0.2825 0.2498 -0.8527 -0.5532 -0.2712</cell><cell>38112</cell><cell>1</cell><cell>1532</cell><cell>A</cell></row><row><cell>QU</cell><cell>QUExpDyn-run3</cell><cell cols="5">0.2547 0.2068 0.2475 0.1996 -0.5182 -0.3457 -0.1833</cell><cell>77033</cell><cell>19</cell><cell>879</cell><cell>A</cell></row><row><cell>ICTNET</cell><cell>ICTNET-run2</cell><cell cols="5">0.2444 0.1976 0.2455 0.1987 -0.9911 -0.6891 -0.4049</cell><cell>47988</cell><cell>74</cell><cell>1473</cell><cell>?</cell></row><row><cell>HLJIT</cell><cell>testRun3</cell><cell cols="6">0.2426 0.1832 0.2420 0.1826 -0.4618 -0.3086 -0.1645 101708</cell><cell>32255</cell><cell>773</cell><cell>P</cell></row><row><cell>QU</cell><cell>QUBaseline-run1</cell><cell cols="5">0.2422 0.2146 0.2260 0.1984 -0.2326 -0.1459 -0.0644</cell><cell>64813</cell><cell>1</cell><cell>446</cell><cell>A</cell></row><row><cell>QU</cell><cell>QUExp-run2</cell><cell cols="5">0.2356 0.2185 0.2159 0.1987 -0.1498 -0.0909 -0.0354</cell><cell>63944</cell><cell>1</cell><cell>306</cell><cell>A</cell></row><row><cell>ICTNET</cell><cell>ICTNET-run3</cell><cell cols="5">0.2338 0.2005 0.2227 0.1893 -0.5869 -0.4040 -0.2318</cell><cell>66596</cell><cell>80</cell><cell>892</cell><cell>?</cell></row><row><cell>udel</cell><cell>udelRun081D-run2</cell><cell></cell><cell cols="4">0.2393 0.2002 -1.0364 -0.7252 -0.4323</cell><cell>61905</cell><cell>1</cell><cell>1521</cell><cell>A</cell></row><row><cell>udel</cell><cell>udelRun081HT-run1</cell><cell cols="5">0.2330 0.2023 0.2193 0.1886 -0.2165 -0.1401 -0.0683</cell><cell>31787</cell><cell>1</cell><cell>393</cell><cell>A</cell></row><row><cell>prna</cell><cell>PRNA-A3</cell><cell cols="5">0.2298 0.2016 0.2280 0.1998 -0.3278 -0.2052 -0.0899</cell><cell>39366</cell><cell>74</cell><cell>636</cell><cell>A</cell></row><row><cell>IRIT</cell><cell>IRIT-Run2</cell><cell cols="5">0.2212 0.2041 0.1996 0.1825 -0.0942 -0.0557 -0.0195</cell><cell>96894</cell><cell>14768</cell><cell>201</cell><cell>A</cell></row><row><cell>IRIT</cell><cell>IRIT-Run3</cell><cell cols="5">0.2194 0.1895 0.2015 0.1716 -0.1853 -0.1221 -0.0626</cell><cell>98865</cell><cell>15623</cell><cell>320</cell><cell>A</cell></row><row><cell>udel</cell><cell cols="6">udelRun081HTD-run3 0.2185 0.1979 0.2022 0.1816 -0.1891 -0.1279 -0.0703</cell><cell>37468</cell><cell>0</cell><cell>303</cell><cell>A</cell></row><row><cell>WUWien</cell><cell>WuWien-Run3</cell><cell cols="5">0.2146 0.2021 0.2095 0.1970 -0.1421 -0.0931 -0.0470</cell><cell>70499</cell><cell>2</cell><cell>245</cell><cell>A</cell></row><row><cell>prna</cell><cell>PRNA-A1</cell><cell cols="5">0.2090 0.1951 0.2052 0.1913 -0.1330 -0.0780 -0.0262</cell><cell>50613</cell><cell>69</cell><cell>295</cell><cell>A</cell></row><row><cell>prna</cell><cell>PRNA-A2</cell><cell cols="5">0.2066 0.1914 0.2058 0.1906 -0.2630 -0.1707 -0.0839</cell><cell>29994</cell><cell>78</cell><cell>470</cell><cell>A</cell></row><row><cell>irlab</cell><cell>irlab-Run1</cell><cell cols="5">0.2065 0.1774 0.1929 0.1638 -0.1156 -0.0696 -0.0263</cell><cell>72250</cell><cell>561</cell><cell>242</cell><cell>P</cell></row><row><cell>udel fang</cell><cell>UDInfoEXP-run3</cell><cell cols="5">0.2025 0.1988 0.3737 0.3700 -2.6753 -1.8402 -1.0542</cell><cell>66577</cell><cell>43980</cell><cell>4140</cell><cell>A</cell></row><row><cell>WUWien</cell><cell>WuWien-Run1</cell><cell cols="6">0.2018 0.1873 0.1912 0.1767 -0.0567 -0.0335 -0.0116 122571</cell><cell>19872</cell><cell>122</cell><cell>A</cell></row><row><cell>irlab</cell><cell>ldrp-Run2</cell><cell cols="5">0.1998 0.1617 0.1932 0.1551 -0.5084 -0.3634 -0.2269</cell><cell>71463</cell><cell>58</cell><cell>697</cell><cell>A</cell></row><row><cell>PKUICST</cell><cell>PKUICSTRunA3</cell><cell cols="6">0.1997 0.1892 0.1908 0.1804 -0.0657 -0.0438 -0.0232 106453</cell><cell>892</cell><cell>111</cell><cell>A</cell></row><row><cell>ICTNET</cell><cell>ICTNET-run1</cell><cell cols="5">0.1959 0.0667 0.1751 0.0458 -0.5525 -0.4037 -0.2636</cell><cell>58681</cell><cell>38</cell><cell>700</cell><cell>?</cell></row><row><cell>PKUICST</cell><cell>PKUICSTRunA2</cell><cell cols="6">0.1959 0.1866 0.1866 0.1774 -0.0705 -0.0477 -0.0262 135577</cell><cell>1957</cell><cell>114</cell><cell>A</cell></row><row><cell>umd-hcil</cell><cell>pertopicburst-run01</cell><cell cols="5">0.1947 0.1844 0.1850 0.1746 -0.1165 -0.0789 -0.0436</cell><cell>97337</cell><cell>1404</cell><cell>183</cell><cell>A</cell></row><row><cell>ST</cell><cell>SHNU run1</cell><cell cols="6">0.1914 0.1463 0.1781 0.1330 -0.2181 -0.1579 -0.1012 106013</cell><cell>33428</cell><cell>283</cell><cell>P</cell></row><row><cell>ST</cell><cell>SHNU run2</cell><cell cols="5">0.1857 0.1302 0.1785 0.1229 -0.1127 -0.0812 -0.0515</cell><cell>96782</cell><cell>47966</cell><cell>151</cell><cell>P</cell></row><row><cell>BJUT</cell><cell>BL2</cell><cell cols="5">0.1837 0.1625 0.1809 0.1598 -0.7058 -0.5184 -0.3420</cell><cell>88013</cell><cell>147</cell><cell>869</cell><cell>P</cell></row><row><cell>ST</cell><cell>SHNU run3</cell><cell cols="6">0.1820 0.1418 0.1775 0.1373 -0.2420 -0.1782 -0.1181 155030</cell><cell>53664</cell><cell>296</cell><cell>P</cell></row><row><cell>umd-hcil</cell><cell>retweet-run02</cell><cell cols="5">0.1785 0.1785 0.1776 0.1776 -0.0130 -0.0090 -0.0053</cell><cell>67657</cell><cell>28658</cell><cell>20</cell><cell>A</cell></row><row><cell>Empty run</cell><cell></cell><cell cols="2">0.1765 0.1765 0.1765 0.1765</cell><cell>0.0000</cell><cell>0.0000</cell><cell>0.0000</cell><cell>-</cell><cell>-</cell><cell>0</cell><cell>-</cell></row><row><cell>BJUT</cell><cell>BL1</cell><cell cols="5">0.1692 0.0774 0.1711 0.0793 -1.4431 -1.0493 -0.6787</cell><cell>74216</cell><cell>52</cell><cell>1859</cell><cell>P</cell></row><row><cell>BJUT</cell><cell>BL3</cell><cell cols="6">0.1602 0.1225 0.1636 0.1258 -0.8745 -0.6466 -0.4320 103187</cell><cell>2513</cell><cell>1058</cell><cell>P</cell></row><row><cell>SOIC</cell><cell>SOIC-Run1</cell><cell cols="5">0.0873 0.0057 0.0903 0.0088 -3.0285 -2.2526 -1.5223</cell><cell>95850</cell><cell>19535</cell><cell>3554</cell><cell>A</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="15,53.50,85.95,505.61,492.20"><head>Table 4 :</head><label>4</label><figDesc>Evaluation of scenario B runs by NIST assessors. e nal column shows the run type: 'A' denotes automatic, 'P' manual preparation, and 'I' manual intervention. Rows are sorted by nDCG-p.</figDesc><table coords="15,184.46,85.95,243.07,461.47"><row><cell>team</cell><cell>run</cell><cell cols="3">nDCG-p nDCG-1 type</cell></row><row><cell>HLJIT</cell><cell>qFB url</cell><cell>0.3656</cell><cell>0.2910</cell><cell>A</cell></row><row><cell>PKUICST</cell><cell>PKUICSTRunB1</cell><cell>0.3483</cell><cell>0.3003</cell><cell>A</cell></row><row><cell>HLJIT</cell><cell>HLJIT l2r</cell><cell>0.3274</cell><cell>0.2778</cell><cell>P</cell></row><row><cell>udel fang</cell><cell>UDInfoW2VPre</cell><cell>0.2933</cell><cell>0.2775</cell><cell>A</cell></row><row><cell>udel fang</cell><cell>UDInfoW2VTWT</cell><cell>0.2906</cell><cell>0.2759</cell><cell>A</cell></row><row><cell>udel fang</cell><cell>UDInfoJac</cell><cell>0.2886</cell><cell>0.2723</cell><cell>A</cell></row><row><cell>HLJIT</cell><cell>HLJIT rank svm</cell><cell>0.2865</cell><cell>0.2376</cell><cell>P</cell></row><row><cell>udel</cell><cell>udelRun081D-B</cell><cell>0.2808</cell><cell>0.2329</cell><cell>I</cell></row><row><cell>PRNA</cell><cell>PRNA-B2</cell><cell>0.2752</cell><cell>0.2400</cell><cell>A</cell></row><row><cell>NOVASearch</cell><cell>NOVASearchB3</cell><cell>0.2710</cell><cell>0.2587</cell><cell>A</cell></row><row><cell cols="2">advanse lirmm adv lirmm-Run1</cell><cell>0.2669</cell><cell>0.2289</cell><cell>A</cell></row><row><cell cols="2">advanse lirmm adv lirmm-Run3</cell><cell>0.2656</cell><cell>0.2285</cell><cell>A</cell></row><row><cell cols="2">advanse lirmm adv lirmm-Run2</cell><cell>0.2601</cell><cell>0.2227</cell><cell>A</cell></row><row><cell>udel</cell><cell>udelRun081HT-B</cell><cell>0.2552</cell><cell>0.2124</cell><cell>I</cell></row><row><cell>PKUICST</cell><cell>PKUICSTRunB3</cell><cell>0.2306</cell><cell>0.2024</cell><cell>A</cell></row><row><cell>udel</cell><cell>udelRun081HTD-B</cell><cell>0.2242</cell><cell>0.1933</cell><cell>I</cell></row><row><cell>ICTNET</cell><cell>ICTNET-Run3</cell><cell>0.2185</cell><cell>0.1527</cell><cell>A</cell></row><row><cell>PRNA</cell><cell>PRNA-B3</cell><cell>0.2143</cell><cell>0.1686</cell><cell>A</cell></row><row><cell>IRIT</cell><cell>IRIT-RunB2</cell><cell>0.2142</cell><cell>0.1833</cell><cell>A</cell></row><row><cell>IRIT</cell><cell>IRIT-RunB1</cell><cell>0.2130</cell><cell>0.1962</cell><cell>I</cell></row><row><cell>IRIT</cell><cell>IRIT-RunB3</cell><cell>0.2117</cell><cell>0.1961</cell><cell>I</cell></row><row><cell>PRNA</cell><cell>PRNA-B1</cell><cell>0.2071</cell><cell>0.1914</cell><cell>A</cell></row><row><cell>ICTNET</cell><cell>ICTNET-Run2</cell><cell>0.2047</cell><cell>0.1381</cell><cell>A</cell></row><row><cell>PKUICST</cell><cell>PKUICSTRunB2</cell><cell>0.1968</cell><cell>0.1809</cell><cell>A</cell></row><row><cell>NOVASearch</cell><cell>NOVASearchB1</cell><cell>0.1896</cell><cell>0.1896</cell><cell>A</cell></row><row><cell>umd-hcil</cell><cell>umc hcil ptbv1</cell><cell>0.1863</cell><cell>0.1747</cell><cell>A</cell></row><row><cell>BJUT</cell><cell>bjut tmg</cell><cell>0.1796</cell><cell>0.1456</cell><cell>A</cell></row><row><cell>umd-hcil</cell><cell>umc hcil rtv1</cell><cell>0.1778</cell><cell>0.1753</cell><cell>A</cell></row><row><cell>Empty run</cell><cell></cell><cell>0.1765</cell><cell>0.1765</cell><cell>-</cell></row><row><cell>ISIKol</cell><cell>lm-jm-lambda0.5</cell><cell>0.1725</cell><cell>0.1725</cell><cell>A</cell></row><row><cell>ST</cell><cell>SHNU run1</cell><cell>0.1551</cell><cell>0.0741</cell><cell>P</cell></row><row><cell>SOIC</cell><cell>IUB</cell><cell>0.1442</cell><cell>0.1442</cell><cell>A</cell></row><row><cell>NOVASearch</cell><cell>NOVASearchB2</cell><cell>0.1440</cell><cell>0.1333</cell><cell>A</cell></row><row><cell cols="2">IRLAB DAIICT IRLAB-DAIICT</cell><cell>0.1324</cell><cell>0.0697</cell><cell>A</cell></row><row><cell>ICTNET</cell><cell>ICTNET-Run1</cell><cell>0.1208</cell><cell>0.1143</cell><cell>A</cell></row><row><cell>BJUT</cell><cell>bjutg</cell><cell>0.1169</cell><cell>0.1169</cell><cell>A</cell></row><row><cell>ST</cell><cell>SHNU run3</cell><cell>0.1166</cell><cell>0.0689</cell><cell>P</cell></row><row><cell>ST</cell><cell>SHNU run2</cell><cell>0.1135</cell><cell>0.0729</cell><cell>P</cell></row><row><cell cols="2">IRLAB DAIICT IRLAB LDRP</cell><cell>0.1099</cell><cell>0.0773</cell><cell>A</cell></row><row><cell cols="2">IRLAB DAIICT IRLAB-LDRP2</cell><cell>0.0995</cell><cell>0.0619</cell><cell>A</cell></row><row><cell>BJUT</cell><cell>bjutgs</cell><cell>0.0746</cell><cell>0.0746</cell><cell>A</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="16,53.50,529.25,504.70,18.90"><head>Table 5 :</head><label>5</label><figDesc>Evaluation of scenario B runs as scenario A runs by the mobile assessors. e rst two columns show the participating team and run.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="17,53.50,86.89,499.96,459.77"><head>Table</head><label></label><figDesc></figDesc><table coords="17,60.69,86.89,492.78,440.07"><row><cell>team</cell><cell>run</cell><cell>EG-p</cell><cell cols="4">EG-1 nCG-p nCG-1 GMP .33 GMP .50 GMP .66</cell><cell cols="3">mean median length type</cell></row><row><cell>PKUICST</cell><cell>PKUICSTRunB1</cell><cell cols="5">0.2959 0.2541 0.3653 0.3236 -1.3363 -0.8676 -0.4265</cell><cell>73387</cell><cell>46551</cell><cell>2409</cell><cell>A</cell></row><row><cell cols="2">advanse lirmm adv lirmm-Run1</cell><cell cols="5">0.2676 0.2332 0.2836 0.2492 -0.7992 -0.5113 -0.2402</cell><cell>85809</cell><cell>51599</cell><cell>1483</cell><cell>A</cell></row><row><cell>PRNA</cell><cell>PRNA-B2</cell><cell cols="5">0.2674 0.2385 0.2622 0.2333 -0.7328 -0.4836 -0.2490</cell><cell>80632</cell><cell>46389</cell><cell>1272</cell><cell>A</cell></row><row><cell cols="2">advanse lirmm adv lirmm-Run2</cell><cell cols="5">0.2641 0.2316 0.2732 0.2407 -0.7400 -0.4726 -0.2210</cell><cell>84177</cell><cell>50264</cell><cell>1381</cell><cell>A</cell></row><row><cell cols="2">advanse lirmm adv lirmm-Run3</cell><cell cols="5">0.2620 0.2283 0.2826 0.2488 -0.8625 -0.5599 -0.2752</cell><cell>85436</cell><cell>52466</cell><cell>1547</cell><cell>A</cell></row><row><cell>udel</cell><cell>udelRun081HT-B</cell><cell cols="5">0.2515 0.2111 0.2427 0.2022 -0.3211 -0.2017 -0.0893</cell><cell>68787</cell><cell>51170</cell><cell>627</cell><cell>I</cell></row><row><cell>udel</cell><cell>udelRun081D-B</cell><cell cols="5">0.2460 0.2014 0.2925 0.2479 -1.3709 -0.9433 -0.5408</cell><cell>84751</cell><cell>44471</cell><cell>2121</cell><cell>I</cell></row><row><cell>PKUICST</cell><cell>PKUICSTRunB3</cell><cell cols="6">0.2403 0.2165 0.2225 0.1986 -0.2633 -0.1646 -0.0717 105965</cell><cell>64582</cell><cell>508</cell><cell>A</cell></row><row><cell>udel</cell><cell cols="6">udelRun081HTD-B 0.2332 0.2034 0.2155 0.1857 -0.2836 -0.1888 -0.0995</cell><cell>76252</cell><cell>59434</cell><cell>477</cell><cell>I</cell></row><row><cell>udel fang</cell><cell>UDInfoJac</cell><cell cols="5">0.2232 0.2054 0.2997 0.2819 -1.3598 -0.9198 -0.5057</cell><cell>81574</cell><cell>48242</cell><cell>2190</cell><cell>A</cell></row><row><cell>udel fang</cell><cell>UDInfoW2VPre</cell><cell cols="5">0.2229 0.2059 0.3055 0.2885 -1.4253 -0.9652 -0.5322</cell><cell>82742</cell><cell>49278</cell><cell>2289</cell><cell>A</cell></row><row><cell>udel fang</cell><cell>UDInfoW2VTWT</cell><cell cols="5">0.2220 0.2056 0.3036 0.2873 -1.4505 -0.9836 -0.5441</cell><cell>81348</cell><cell>46754</cell><cell>2320</cell><cell>A</cell></row><row><cell>IRIT</cell><cell>IRIT-RunB1</cell><cell cols="6">0.2196 0.2029 0.2053 0.1885 -0.1459 -0.0889 -0.0353 121242</cell><cell>66657</cell><cell>298</cell><cell>I</cell></row><row><cell>IRIT</cell><cell>IRIT-RunB3</cell><cell cols="6">0.2175 0.2019 0.2045 0.1889 -0.1405 -0.0860 -0.0348 126789</cell><cell></cell></row><row><cell>IRIT</cell><cell>IRIT-RunB2</cell><cell cols="6">0.2137 0.1828 0.2096 0.1787 -0.2648 -0.1762 -0.0928 124420</cell><cell>70250</cell><cell>449</cell><cell>A</cell></row><row><cell>HLJIT</cell><cell>qFB url</cell><cell cols="5">0.2133 0.1412 0.3942 0.3222 -2.3972 -1.6153 -0.8795</cell><cell>67876</cell><cell>42215</cell><cell>3946</cell><cell>A</cell></row><row><cell>ICTNET</cell><cell>ICTNET-Run3</cell><cell cols="5">0.2055 0.1456 0.2180 0.1581 -2.4390 -1.7697 -1.1397</cell><cell>84868</cell><cell>48076</cell><cell>3149</cell><cell>A</cell></row><row><cell>PRNA</cell><cell>PRNA-B1</cell><cell cols="5">0.2053 0.1896 0.2037 0.1880 -0.1729 -0.1073 -0.0455</cell><cell>97621</cell><cell>50383</cell><cell>344</cell><cell>A</cell></row><row><cell>PKUICST</cell><cell>PKUICSTRunB2</cell><cell cols="6">0.2039 0.1923 0.1932 0.1816 -0.1510 -0.1047 -0.0611 140193</cell><cell>63037</cell><cell>228</cell><cell>A</cell></row><row><cell>PRNA</cell><cell>PRNA-B3</cell><cell cols="6">0.2011 0.1619 0.2155 0.1763 -0.9414 -0.6569 -0.3892 104941</cell><cell>56998</cell><cell>1389</cell><cell>A</cell></row><row><cell>umd-hcil</cell><cell>umc hcil ptbv1</cell><cell cols="6">0.1927 0.1818 0.1836 0.1728 -0.1089 -0.0741 -0.0413 128880</cell><cell>55415</cell><cell>170</cell><cell>A</cell></row><row><cell>BJUT</cell><cell>bjut tmg</cell><cell cols="6">0.1843 0.1503 0.1779 0.1439 -0.3349 -0.2432 -0.1569 129274</cell><cell>60711</cell><cell>426</cell><cell>A</cell></row><row><cell>umd-hcil</cell><cell>umc hcil rtv1</cell><cell cols="5">0.1794 0.1768 0.1770 0.1745 -0.0264 -0.0180 -0.0102</cell><cell>69261</cell><cell>48350</cell><cell>41</cell><cell>A</cell></row><row><cell>HLJIT</cell><cell>HLJIT l2r</cell><cell cols="5">0.1775 0.1275 0.3714 0.3214 -2.8964 -1.9800 -1.1176</cell><cell>65680</cell><cell>41908</cell><cell>4574</cell><cell>P</cell></row><row><cell>Empty run</cell><cell></cell><cell cols="2">0.1765 0.1765 0.1765 0.1765</cell><cell>0.0000</cell><cell>0.0000</cell><cell>0.0000</cell><cell>-</cell><cell>-</cell><cell>0</cell><cell>-</cell></row><row><cell>ICTNET</cell><cell>ICTNET-Run2</cell><cell cols="5">0.1684 0.1086 0.2177 0.1579 -3.5487 -2.5760 -1.6606</cell><cell>87481</cell><cell>47392</cell><cell>4586</cell><cell>A</cell></row><row><cell>HLJIT</cell><cell>HLJIT rank svm</cell><cell cols="5">0.1596 0.1091 0.3192 0.2687 -3.1576 -2.2065 -1.3114</cell><cell>70265</cell><cell>43251</cell><cell>4660</cell><cell>P</cell></row><row><cell cols="2">IRLAB DAIICT IRLAB-DAIICT</cell><cell cols="5">0.1444 0.0852 0.1444 0.0852 -1.9129 -1.4272 -0.9701</cell><cell>31121</cell><cell>31121</cell><cell>2217</cell><cell>A</cell></row><row><cell>ST</cell><cell>SHNU run1</cell><cell cols="6">0.1355 0.0595 0.1574 0.0814 -2.6213 -1.9243 -1.2683 106742</cell><cell>62079</cell><cell>3251</cell><cell>P</cell></row><row><cell cols="2">IRLAB DAIICT IRLAB LDRP</cell><cell cols="5">0.1253 0.0889 0.1253 0.0889 -2.0825 -1.5541 -1.0568</cell><cell>0</cell><cell>0</cell><cell>2412</cell><cell>A</cell></row><row><cell cols="2">IRLAB DAIICT IRLAB-LDRP2</cell><cell cols="5">0.1222 0.0722 0.1222 0.0722 -2.2440 -1.6746 -1.1387</cell><cell>0</cell><cell>0</cell><cell>2599</cell><cell>A</cell></row><row><cell>ICTNET</cell><cell>ICTNET-Run1</cell><cell cols="6">0.1211 0.1112 0.1267 0.1168 -2.3595 -1.7481 -1.1726 131465</cell><cell>40543</cell><cell>2821</cell><cell>A</cell></row><row><cell>NOVASearch</cell><cell>NOVASearchB3</cell><cell cols="5">0.1203 0.1086 0.3340 0.3223 -3.8660 -2.7055 -1.6134</cell><cell>79407</cell><cell>49619</cell><cell>5684</cell><cell>A</cell></row><row><cell>ST</cell><cell>SHNU run3</cell><cell cols="6">0.1168 0.0657 0.1284 0.0773 -3.0533 -2.2635 -1.5202 157904</cell><cell>85379</cell><cell>3630</cell><cell>P</cell></row><row><cell>BJUT</cell><cell>bjutg</cell><cell cols="6">0.1160 0.1106 0.1259 0.1205 -2.1947 -1.6285 -1.0957 102827</cell><cell>50425</cell><cell>2610</cell><cell>A</cell></row><row><cell>ST</cell><cell>SHNU run2</cell><cell cols="6">0.1053 0.0695 0.1143 0.0785 -2.7875 -2.0599 -1.3752 130636</cell><cell>80036</cell><cell>3356</cell><cell>P</cell></row><row><cell>NOVASearch</cell><cell>NOVASearchB1</cell><cell cols="5">0.0881 0.0859 0.2290 0.2268 -4.0991 -2.9327 -1.8348</cell><cell>80135</cell><cell>40115</cell><cell>5580</cell><cell>A</cell></row><row><cell>NOVASearch</cell><cell>NOVASearchB2</cell><cell cols="5">0.0873 0.0760 0.1871 0.1759 -4.4537 -3.2204 -2.0595</cell><cell>94941</cell><cell>53018</cell><cell>5862</cell><cell>A</cell></row><row><cell>SOIC</cell><cell>IUB</cell><cell cols="5">0.0842 0.0820 0.1835 0.1813 -3.5923 -2.6002 -1.6665</cell><cell>85217</cell><cell>47307</cell><cell>4680</cell><cell>A</cell></row><row><cell>BJUT</cell><cell>bjutgs</cell><cell cols="6">0.0716 0.0662 0.0852 0.0798 -2.8204 -2.0905 -1.4036 105516</cell><cell>58134</cell><cell>3370</cell><cell>A</cell></row><row><cell>ISIKol</cell><cell>lm-jm-lambda0.5</cell><cell cols="5">0.0612 0.0598 0.2050 0.2036 -4.5514 -3.2680 -2.0602</cell><cell>86089</cell><cell>52416</cell><cell>6144</cell><cell>A</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="3,56.84,693.68,145.86,6.97"><p>h p://trecrts.github.io/TREC2017-RTS-topics1.json</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="3,56.72,702.09,110.39,6.97"><p>h ps://github.com/trecrts/trecrts-eval/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="3,321.00,702.09,202.82,6.97"><p>In this discussion, each participant run is considered a separate system.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">ACKNOWLEDGMENTS</head><p>is work was supported in part by the <rs type="funder">Natural Sciences and Engineering Research Council (NSERC) of Canada</rs>. We'd like to thank all the mobile assessors who participated in our user study and the <rs type="funder">NIST</rs> assessors.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,334.39,98.13,224.51,6.97;9,334.39,106.10,192.75,6.97" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="9,388.69,98.82,170.21,6.16;9,334.39,106.79,21.60,6.16">Topic Detection and Tracking: Event-Based Information Organization</title>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Dordrecht, e Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,334.39,114.07,224.58,6.97;9,334.16,122.04,224.05,6.97;9,334.18,130.01,224.02,6.97;9,333.96,137.98,107.29,6.97" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Javed</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ma</forename><surname>Hew Ekstrand-Abueg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Mccreadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Virgil</forename><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tetsuya</forename><surname>Sakai</surname></persName>
		</author>
		<title level="m" coord="9,449.42,122.04,108.78,6.97;9,334.18,130.01,224.02,6.97">TREC 2015 Temporal Summarization Track Overview. Proceedings of the Twenty-Fourth Text REtrieval Conference</title>
		<meeting><address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,334.39,145.95,224.99,6.97;9,334.39,153.92,224.58,6.97;9,334.39,161.89,224.99,6.97;9,334.39,169.86,221.83,6.97" xml:id="b2">
	<analytic>
		<author>
			<persName coords=""><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Torben</forename><surname>Brodt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ivan</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Frank</forename><surname>Hopfgartner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jayashree</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noriko</forename><surname>Kando</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anastasia</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Simon</forename><surname>Mercer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.07454</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,368.75,169.86,134.69,6.97">Evaluation-as-a-Service: Overview and Outlook</title>
		<title level="s" coord="9,513.27,161.89,46.11,6.97;9,334.39,169.86,13.00,6.97">and Martin Potthast</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,334.39,177.83,223.81,6.97;9,334.39,185.80,197.66,6.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,410.58,177.83,65.27,6.97">TREC-4 Filtering Track</title>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,488.24,178.52,69.96,6.16;9,334.39,186.49,96.44,6.16">Proceedings of the Fourth Text REtrieval Conference (TREC-4)</title>
		<meeting>the Fourth Text REtrieval Conference (TREC-4)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="165" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,334.39,193.77,224.89,6.97;9,334.39,201.74,224.99,6.97;9,334.39,209.71,45.70,6.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,430.51,193.77,125.82,6.97">Overview of the TREC-2013 Microblog Track</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Miles</forename><surname>Efron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,334.39,202.43,161.06,6.16">Proceedings of the Twenty-Second Text REtrieval Conference</title>
		<meeting>the Twenty-Second Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,334.39,217.68,223.81,6.97;9,334.39,225.65,223.81,6.97;9,334.39,233.62,139.40,6.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,522.42,217.68,35.78,6.97;9,334.39,225.65,90.42,6.97">Overview of the TREC-2014 Microblog Track</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Miles</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yulu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Garrick</forename><surname>Sherman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,429.53,226.34,128.67,6.16;9,334.39,234.31,30.36,6.16">Proceedings of the Twenty-ird Text REtrieval Conference</title>
		<meeting>the Twenty-ird Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,334.39,241.59,224.89,6.97;9,334.39,249.56,223.81,6.97;9,334.39,257.53,179.82,6.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,334.39,249.56,127.11,6.97">Overview of the TREC-2015 Microblog Track</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Miles</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yulu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Garrick</forename><surname>Sherman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,466.22,250.25,91.98,6.16;9,334.39,258.22,70.78,6.16">Proceedings of the Twenty-Fourth Text REtrieval Conference</title>
		<meeting>the Twenty-Fourth Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,334.39,265.50,224.58,6.97;9,334.39,273.47,223.81,6.97;9,334.18,281.44,225.10,6.97;9,334.39,289.41,69.94,6.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,406.72,273.47,151.48,6.97;9,334.18,281.44,15.21,6.97">Overview of the TREC 2016 Real-Time Summarization Track</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roegiest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luchen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Mccreadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,354.70,282.13,165.51,6.16">Proceedings of the Twenty-Fi h Text REtrieval Conference</title>
		<meeting>the Twenty-Fi h Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,334.39,297.38,223.95,6.97;9,334.39,305.35,223.82,6.97;9,334.39,313.32,213.75,6.97" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jiaul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Paik</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lin</surname></persName>
		</author>
		<title level="m" coord="9,434.91,297.38,123.42,6.97;9,334.39,305.35,223.82,6.97;9,334.39,314.01,164.84,6.16">Do Multiple Listeners to the Public Twi er Sample Stream Receive the Same Tweets? Proceedings of the SIGIR 2015 Workshop on Temporal, Social and Spatially-Aware Information Access</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,334.39,321.30,223.95,6.97;9,334.39,329.27,224.89,6.97;9,334.39,337.24,223.81,6.97;9,334.39,345.21,210.72,6.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,481.26,321.30,77.07,6.97;9,334.39,329.27,221.93,6.97">Interleaved Evaluation for Retrospective Summarization and Prospective Noti cation on Document Streams</title>
		<author>
			<persName coords=""><forename type="first">Xin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roegiest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,341.64,337.92,216.57,6.16;9,334.39,345.89,116.26,6.16">Proceedings of the 39th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 39th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="175" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,334.39,353.18,224.89,6.97;9,334.39,361.15,224.58,6.97;9,334.39,369.12,29.20,6.97" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboro</surname></persName>
		</author>
		<title level="m" coord="9,464.49,353.18,91.88,6.97;9,341.44,361.83,142.19,6.16">Proceedings of the Eleventh Text REtrieval Conference</title>
		<meeting>the Eleventh Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
	<note>TREC 2002 Filtering Track Report</note>
</biblStruct>

<biblStruct coords="9,334.39,377.09,223.81,6.97;9,334.39,385.06,223.81,6.97;9,334.39,393.71,223.81,6.16;9,334.39,401.00,128.72,6.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,482.56,377.09,75.64,6.97;9,334.39,385.06,138.03,6.97">Online In-Situ Interleaved Evaluation of Real-Time Push Noti cation Systems</title>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roegiest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luchen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,476.63,385.74,81.57,6.16;9,334.39,393.71,223.81,6.16;9,334.39,401.68,24.51,6.16">Proceedings of the 40th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="415" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,334.39,408.97,224.06,6.97;9,334.39,416.94,223.81,6.97;9,334.39,425.59,223.81,6.16;9,334.39,432.88,160.20,6.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,553.51,408.97,4.94,6.97;9,334.39,416.94,171.40,6.97">A Platform for Streaming Push Noti cations to Mobile Assessors</title>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roegiest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luchen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,510.11,417.62,48.09,6.16;9,334.39,425.59,223.81,6.16;9,334.39,433.56,59.26,6.16">Proceedings of the 39th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 39th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="1077" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,334.39,440.85,223.81,6.97;9,334.39,448.82,223.81,6.97;9,334.39,457.47,193.70,6.16" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,490.54,440.85,67.66,6.97;9,334.39,448.82,180.23,6.97">Overview of the Living Labs for Information Retrieval Evaluation (LL4IR) CLEF Lab 2015</title>
		<author>
			<persName coords=""><forename type="first">Anne</forename><surname>Schuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liadh</forename><surname>Kelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,526.63,449.50,31.57,6.16;9,334.39,457.47,190.83,6.16">Proceedings of the 6th International Conference of the CLEF Association (CLEF&apos;15)</title>
		<meeting>the 6th International Conference of the CLEF Association (CLEF&apos;15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,334.39,464.76,223.81,6.97;9,334.18,472.73,224.03,6.97;9,334.39,480.70,224.58,6.97;9,334.23,488.67,31.30,6.97" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,439.59,464.76,118.61,6.97;9,334.18,472.73,18.94,6.97">Finally, a Downloadable Test Collection of Tweets</title>
		<author>
			<persName coords=""><forename type="first">Royal</forename><surname>Sequiera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,358.01,473.41,200.20,6.16;9,334.39,481.38,145.15,6.16">Proceedings of the 40th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1225" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,334.39,496.64,223.81,6.97;9,334.39,504.61,223.81,6.97;9,334.39,513.26,223.82,6.16;9,334.39,520.55,161.00,6.97" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,549.40,496.64,8.80,6.97;9,334.39,504.61,180.19,6.97">An Exploration of Evaluation Metrics for Mobile Push Noti cations</title>
		<author>
			<persName coords=""><forename type="first">Luchen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roegiest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,519.04,505.29,39.16,6.16;9,334.39,513.26,223.82,6.16;9,334.39,521.23,66.54,6.16">Proceedings of the 39th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 39th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="741" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,334.39,528.52,223.95,6.97;9,334.39,536.49,223.81,6.97;9,334.39,545.14,223.82,6.16;9,334.39,552.43,176.54,6.97" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="9,532.97,528.52,25.37,6.97;9,334.39,536.49,179.77,6.97">Assessor Di erences and User Preferences in Tweet Timeline Generation</title>
		<author>
			<persName coords=""><forename type="first">Yulu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Garrick</forename><surname>Sherman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Miles</forename><surname>Efron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,518.94,537.17,39.26,6.16;9,334.39,545.14,223.82,6.16;9,334.39,553.11,66.54,6.16">Proceedings of the 38th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="615" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,334.39,560.40,224.58,6.97;9,334.39,568.37,224.99,6.97;9,334.39,576.34,223.81,6.97;9,334.39,584.31,179.82,6.97" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="9,535.43,568.37,23.95,6.97;9,334.39,576.34,137.94,6.97">UWater-looMDS at the TREC Core Track 2017 (Notebook)</title>
		<author>
			<persName coords=""><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mustafa</forename><surname>Abualsaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nimesh</forename><surname>Ghelani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Angshuman</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,477.08,577.02,81.13,6.16;9,334.39,584.99,70.78,6.16">Proceedings of the Twenty-Six Text REtrieval Conference</title>
		<meeting>the Twenty-Six Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
