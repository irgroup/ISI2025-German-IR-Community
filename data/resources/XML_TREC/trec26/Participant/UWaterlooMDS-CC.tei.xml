<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,80.70,104.51,357.41,12.58">UWaterlooMDS at the TREC 2017 Common Core Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,80.70,132.53,93.45,10.63"><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,183.54,132.53,119.60,10.63"><forename type="first">Mustafa</forename><surname>Abualsaud</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.53,132.53,95.31,10.63"><forename type="first">Nimesh</forename><surname>Ghelani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,417.23,132.53,117.50,10.63"><forename type="first">Angshuman</forename><surname>Ghosh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,80.70,146.48,103.69,10.63"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Management Sciences</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,194.58,146.48,123.08,10.63"><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,349.37,146.48,120.26,10.63"><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,80.70,104.51,357.41,12.58">UWaterlooMDS at the TREC 2017 Common Core Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">84916A920D6408301CFEB81170F77A11</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this report, we discuss the experiments we conducted for the TREC 2017 Common Core Track. We implemented a High-Recall retrieval system for assessors to e ciently nd relevant documents within a limited period of time. e system consists of an AutoTAR Continuous Active Learning (CAL) system based on paragraph/document level relevance feedback.</p><p>e system also includes a search engine where users can repeatedly enter their own queries to nd relevant documents. For the rst round of submissions, we used our system to judge 32,172 documents across 250 topics. e system's trained model of relevance was used to rank all unjudged documents. A er each judgment, our system can retrain the machine learning model and rank the whole collection while maintaining interactive performance without any user discernible lag. For the second round of submissions, we cra ed a user study to measure the impact of interaction mechanisms on technology assisted review and completed an initial version of the study with 10 users and 50 topics.</p><p>1 INTRODUCTION e task of TREC 2017 Common Core Track 1 is an ad-hoc retrieval of documents for a set of topics from the New York Times dataset 2 . Manually reviewing documents and labeling relevance always requires a lot of human e ort. e objective of our system is to assist assessors in nding as many relevant documents as possible within limited time. In other words, the target is to achieve high recall with reasonable e ort.</p><p>High-Recall retrieval addresses the problem of nding as many relevant documents as possible with reasonable e ort in terms of time or budget. One example task is building a test collection for IR evaluation. Interactive Searching and Judging (referred to as "ISJ") is one of the e ective methods to build test collections <ref type="bibr" coords="1,490.54,415.63,10.58,8.85" target="#b3">[4]</ref>. In ISJ, assessors repeatedly reformulate new queries and review top ranked results from search engines. Compared to traditional pooling method, ISJ can generate comparable gold standard with less e ort <ref type="bibr" coords="1,433.45,439.54,10.37,8.85" target="#b3">[4,</ref><ref type="bibr" coords="1,446.31,439.54,11.46,8.85" target="#b9">10,</ref><ref type="bibr" coords="1,460.26,439.54,11.25,8.85" target="#b14">15]</ref>.</p><p>AutoTAR -an implementation of Continuous Active Learning ("CAL") protocol <ref type="bibr" coords="1,433.99,451.50,10.49,8.85" target="#b3">[4,</ref><ref type="bibr" coords="1,447.79,451.50,8.25,8.85" target="#b4">5]</ref> applies relevance feedback and machine learning to nd the most-likely relevant documents for assessment. e TREC Total Recall Track 2015 and 2016 evaluated di erent retrieval methods with a simulated human-in-the-loop process to achieve high recall <ref type="bibr" coords="1,128.02,487.36,10.48,8.85" target="#b6">[7,</ref><ref type="bibr" coords="1,141.44,487.36,6.99,8.85" target="#b8">9]</ref>. A baseline model implementation ("BMI") based on AutoTAR was provided and served as the baseline model in Total Recall Track 2015 and 2016. None of the submi ed runs were able to consistently outperform BMI <ref type="bibr" coords="1,150.49,511.27,10.47,8.85" target="#b6">[7,</ref><ref type="bibr" coords="1,163.45,511.27,6.86,8.85" target="#b8">9,</ref><ref type="bibr" coords="1,172.80,511.27,7.55,8.85" target="#b18">19</ref>]. e BMI also showed its e ectiveness on other tasks such as systematic review for evidence based medicine [8] and nugget annotation <ref type="bibr" coords="1,292.96,523.23,10.44,8.85" target="#b0">[1]</ref>.</p><p>Recently, Zhang et al.</p><p>[17] used sentence-level feedback instead of document-level feedback in CAL to achieve high recall. eir results showed that by using sentence-level feedback only, achieved the same recall as using document-level feedback, at the same amount of e ort (number of judgments). If e ort was measured by the number of sentences reviewed, Zhang et al. <ref type="bibr" coords="1,262.94,571.05,16.36,8.85" target="#b16">[17]</ref> suggest using sentence-level feedback was far more e cient than document-level feedback.</p><p>However, sentence-level feedback on CAL as suggested by Zhang et al. [17]  was only evaluated inside a simulation study and not by human assessors. e system was running over a prede ned complete label set <ref type="bibr" coords="1,519.60,606.91,11.70,8.85" target="#b8">[9]</ref> and all aspects of human interactions were simulated. For the TREC Common Core Track, we designed a system</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>for the human assessors to e ciently nd and assess documents. According to the two deadlines made by TREC Common Core track, we designed a two step experiment. We rst implemented a prototype system and judged documents for 250 topics by ourselves. A er verifying the e ectiveness and e ciency of the prototype system, we improved the prototype system according to our own user experience and implemented an improved system. en 10 users were recruited to use the improved system to judge 50 NIST topics. We designed our system with the following objectives:</p><p>• E ectiveness: e assessors should be able to easily nd relevant documents using our system. Our system combines the advantages of ISJ and CAL. • E ciency: Our system applies the relevance feedback from assessor to retrain the machine learning model. erefore it requires that the machine learning model to be retrained and the entire collection to be re-ranked a er each judgment while maintaining interactive performance with no user discernible lag. Assessors should be able to quickly judge the document without the need to review its full content. e paragraph-length document summary and keyword highlighting features were provided to assist assessment. Our group (UWaterlooMDS) implemented an assessment system that included a search engine and a CAL-based component. Using our system, we manually judged documents for all the 250 topics. In addition to judging documents ourselves, we also cra ed a user study and recruited 10 users to use di erent variants of our improved assessment system to judge 50 NIST topics. In total, we submi ed 10 manual runs to TREC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ALGORITHM 1: e AutoTAR algorithm</head><p>Step 1. Find a relevant "seed" document using ad-hoc search, or construct a synthetic relevant document from the topic description;</p><p>Step 2. e initial training set consists of the seed document identi ed in step 1, labeled "relevant";</p><p>Step 3. Set the initial batch size B to 1;</p><p>Step 4. Temporarily augment the training set by adding 100 random documents from the collection, temporarily labeled "not relevant";</p><p>Step 5. Train an Logistic Regression classi er using the training set; Step 6. Remove the random documents added in step 4;</p><p>Step 7. Select the highest-scoring B documents for review;</p><p>Step 8. Review the documents, coding each as "relevant" or "not relevant";</p><p>Step 9. Add the documents to the training set;</p><p>Step 10. Increase B by B 10 ; Step 11. Repeat steps 4 through 10 until a su cient number of relevant documents have been reviewed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">ASSESSMENT SYSTEM</head><p>We implemented a document assessment system with an online front-end interface accessible through a web browser. e system was composed of two components: Search model and CAL model. Each model has its own interface that was used to assess documents. e search model was built to allow users to query and search for documents to judge. While in the CAL model, users were automatically presented with documents to assess.</p><p>e search model consists of a search engine based on Indri <ref type="bibr" coords="2,327.98,600.18,16.21,8.85" target="#b12">[13]</ref> with the same retrieval and snippet generation algorithms mentioned in Smucker et al. <ref type="bibr" coords="2,249.77,612.13,14.87,8.85" target="#b11">[12]</ref>. In the search interface, users can enter their own queries and the top ranked documents retrieved by the search engine were shown to the users. Each document included a title, a snippet, and three bu ons for judging. e three bu ons corresponded to three relevance categories: Highly-Rel, Relevant, Non-Relevant. Users can click on any of these bu ons to make a judgment or change an existing judgment. Users can also click on a document to view its full content, search its content and make a judgment.</p><p>e CAL model is a variation of BMI. It returns a batch of most likely relevant documents to the user for further review. Each time a user made a new judgment, the relevance judgment was used to retrain the machine learning model, and a new batch of documents was returned and the next most-likely relevant document was shown to the user for assessment. Judgments from both search interface and CAL interface were used to retrain the CAL model. In the CAL interface, users can undo and change their previous judgments. Modi ed judgments are also used to retrain the CAL model.</p><p>Users were not limited to a single interface and can switch to any of the two interfaces at any moment. In both interfaces, users were able to search and highlight desired keywords in the a document. Any word (or part of a word) entered in the highlighting search bar is highlighted if it exists in the document's body or title. In the CAL interface, the highlighting persists across successive documents. To make the process of judging documents faster and more e cient, we introduced 3 keyboard shortcuts that corresponded to the three relevance categories. Users can choose to click on any of the judging bu ons to make a judgment.</p><p>Two versions of the system were implemented for the two di erent submission deadlines of the TREC Common Core track. For the rst submission, a prototype system described in the next section was built. A er the rst deadline, we made several improvements based on our experience with using the prototype system. e improved system was then applied in a controlled user study. Judgments collected from the user study were used to compose runs for the second submission deadline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Prototype system for the first submission</head><p>Our UWaterlooMDS team used the prototype system to nd and judge documents for all the 250 topics. e judgments collected from the prototype system were used to compose three runs that were submi ed to the TREC Common Core track. Each component of the system is described in more details in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Search</head><p>Model. e search interface is shown in Figure <ref type="figure" coords="3,334.37,404.67,3.34,8.85" target="#fig_0">1</ref>. Users can enter their own queries in the search bar. In the prototype system, search engine only retrieved the top 20 results for a given query. Pagination in the search interface is not supported. Each result in the SERP (Search Engine Results Pages) is composed of a document title, id and a snippet. To help users re ne their search result, we allowed the following search operators:</p><p>• Double quotes (""): to search for an exact phrase or word. For example, "New York Times".</p><p>• Plus sign (+): to require the presence of a word in the search result. For example, +France.</p><p>• Combination of + and "": to require the presence of phrases in the search result. For example, +"New York Times". ree judgment bu ons: "Relevant", "On topic" and "Not relevant" are provided next to each result. In the prototype system, the relevance categories are:</p><p>• Relevant: If the document directly addresses the core information need of the topic and covers all the aspects of the topic. • On topic: If the document only mentions partial information of the topic and does not address the information need in the topic's description or its narrative. • Not relevant: If the document is not relevant to the topic.</p><p>Users can click on any of the judgment bu ons from the SERP to make a judgment. Clicking on any of the search result displayed a popup containing the full document. e full document popup is shown in Figure <ref type="figure" coords="3,511.15,624.09,3.36,8.85" target="#fig_1">2</ref>. As shown in the gure, the title and the content of the document are present, along with the same judgments bu ons for judging the document. e popup automatically closes once a judgment is made. Users can use the "Ctrl + F" shortcut or enter any keywords in search box to highlight keywords in the document. Multiple keywords separated by space can be entered to highlight each keyword simultaneously. e SERP may contain documents which have already been judged. ese documents are visually labeled with their current judgments as shown in Figure <ref type="figure" coords="4,259.63,465.20,3.41,8.85" target="#fig_0">1</ref>. Users may rejudge these documents.</p><p>2.1.2 CAL model. Unlike the search model where users have to enter their own queries and search for documents, the CAL model automatically selected the most-likely to be relevant document and presented it to the users for assessments. e CAL model is a Continuous Active Learning system, which is based on the AutoTAR baseline model implementation ("BMI") <ref type="foot" coords="4,241.87,519.12,3.38,6.46" target="#foot_2">3</ref> . e AutoTAR algorithm is described in Algorithm 1. e original BMI algorithm was wri en in BASH, which is suitable for simulations but ine cient for practical use. We rewrote the BMI algorithm in C++ and made several improvements and changes for our prototype system:</p><p>• Seed query: In BMI, the syntactic seed query for the initial classi er is the title of the topic. In our user study, the seed query was entered by the users themselves. Users could enter any terms they believed to be helpful and related to the topic. • Batch size: In BMI, the batch size increases exponentially. We propose a real-time re-ranking CAL system in our se ing. e system re-trains the model and re-ranks all the documents every time a new judgment is available from the user. • In memory processing: Re-ranking 1.8 million documents is computationally expensive, especially when it is done a er every judgment. A signi cant part of that computation is the score calculation which is the inner product of a document feature vector and the model feature vector. To enable fast re-ranking, we store all the document feature vectors in memory, and parallelize the computation across documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Documents Cache</head><p>eue: e process of re-ranking may cause noticeable delay for users. Instead of waiting for the ranking, we immediately show the user the next highest ranked document from the current ranking. We maintain a queue of 10 documents in the order of their rank. is queue is ushed and updated with a new ranking as soon as it is received from the back-end server.</p><p>• Logistic Regression: was done using So a-ML <ref type="foot" coords="5,307.88,524.11,3.38,6.46" target="#foot_3">4</ref> with the same hyperparameters as the BMI.</p><p>e judgment interface for the CAL model is shown in Figure <ref type="figure" coords="5,343.59,540.84,3.41,8.85" target="#fig_2">3</ref>. e interface is similar to the full document popup view in the search interface but has the following di erences:</p><p>(1) We provided a document summary above the full document content. Snippet generation was based on terms weighted by the CAL logistic regression model. Terms that are more relevant and informative to a particular task get higher weights. We identi ed the top 10 weighted terms. Snippets are generated using the paragraph ranking algorithm described in section 9.6 in <ref type="bibr" coords="5,498.43,603.52,10.58,8.85" target="#b1">[2]</ref>. e algorithm selects a portion or a cover [u, ] of a document such that it contains at least k query terms. Multiple such covers may exist and we nd all of them. We also collect covers that contain at least two of the query terms, by iterating over covers that contain 2 up to n terms where n is the number of terms in the given query.</p><p>A snippet s of length l produced by query q is scored as follows:</p><formula xml:id="formula_0" coords="6,287.07,451.06,244.23,24.23">p(t ) = l t l c<label>(1)</label></formula><formula xml:id="formula_1" coords="6,243.09,486.89,288.21,44.98">p(t, l ) = 1 -(1 -p(t ) l ) (2) score (s, q) = t ∈q p(t, len th(s))<label>(3)</label></formula><p>Here, p(t ) gives the prior probability for the appearance of a term t at any position in the corpus (l t is the frequency of term t, whereas l c is the total number of words in the corpus). us, p(t, l ) gives the probability of a snippet of length l containing all the terms in the query q. A su ciently large snippet would contain all words with a high probability. us, a shortest snippet s containing all the terms were ranked the lowest, and represents the case where it was least likely to have all the terms, but still does. is makes the snippet interesting to us.</p><p>To create a summary, we selected the paragraph that contained the lowest scoring snippets. In case snippet generated started at the end of a paragraph, and continued to the next, we selected both paragraphs. We continued selecting the lowest scoring snippet till the length of the summary became at least 500 characters long.</p><p>(2) e second di erence is that users were allowed to view and modify their previous 10 judgments made through the CAL interface. Users may want to modify their previous judgments in case they make mistakes or if their understanding of relevance during assessments change.</p><p>2.2 User study system for the second submission A er we judged all the 250 topics using the prototype system described in Section 2.1, we improved our system based on our own experiences and feedback on the prototype system. e improved system was then applied to a real user study of 10 participants. e user study covered only 50 NIST topics. e judgments collected in the user study were then submi ed as the second submission.</p><p>Changes made to the prototype system are described in the subsections below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Modified Relevance</head><p>Categories. We changed the relevance categories from {Relevant, On topic and Not Relevant} to {Highly Relevant, Relevant and Not Relevant}. We followed the same de nition of relevance de ned by Voorhees <ref type="bibr" coords="7,135.06,586.13,15.00,8.85" target="#b15">[16]</ref>: "Assume that you have the information need stated in the topic and that you are at home searching the web for appropriate material. " and the same relevance categories:</p><p>• Highly Relevant: If the document directly addresses the core information need of the topic.</p><p>• Relevant: If the document contains information that you would nd helpful in meeting your information need. • Not Relevant: If the document is not relevant to the topic. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Improved Search Options.</head><p>We found the search model to be very helpful to nd relevant documents during the beginning stage of assessment (when few relevant documents are available in the CAL model). Search was also very e ective when the topic was hard (CAL model found only non-relevant documents). Instead of restricting users to 20 search results, we added an option to set the number of returned results (10, 20, 50, 100) in the search interface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2.3</head><p>Improved CAL Model. For many topics in the prototype system, we realized the document summaries selected using the methods in Section 2.1.2 were inadequate for our information need (i.e. Figure <ref type="figure" coords="8,480.74,504.54,3.31,8.85" target="#fig_2">3</ref>). For some relevant documents, the displayed summary was not informative and lacked the necessary information to make a judgment. We therefore decided to replace the document summary generation algorithm with the summary selection method proposed in Zhang et al. <ref type="bibr" coords="8,254.04,540.40,16.36,8.85" target="#b16">[17]</ref> (i.e. Figure <ref type="figure" coords="8,318.57,540.40,3.26,8.85" target="#fig_4">5</ref>).</p><p>Instead of using a sentence as a summary of a document as proposed in Zhang et al. <ref type="bibr" coords="8,433.79,552.36,14.84,8.85" target="#b16">[17]</ref>, we use paragraphs. A single paragraph usually contains more context than a sentence, while still being signi cantly shorter than the full document. e sentence feedback method proposed in Zhang et al. <ref type="bibr" coords="8,369.60,576.27,16.36,8.85" target="#b16">[17]</ref> was simulated on datasets from the TREC Total Recall track. Providing a single sentence to judge a document can be limiting, as it is o en short in length and does not describe all aspects of the topic. However, paragraphs can provide more context and information than a single sentence.</p><p>We made several changes to our CAL system to adapt to paragraph relevance feedback. We rst extracted and indexed all paragraphs from all the documents, and calculated the tf-idf features for each paragraph using the same document frequency (df ) corresponding full documents (instead of counting each paragraph as a separate document). e seed query, for which the initial classi er for each topic was built, was composed of the topic's title and description. e classi er is initially trained with 100 randomly chosen documents that we label as non-relevant. e classi er then ranks all documents paragraphs and select the highest ranking paragraph and its corresponding document. e highest ranking paragraph is then shown to the user for assessment.</p><p>We also provided the ability to view the full document content by clicking on the "View full document" bu on (Figure <ref type="figure" coords="9,111.74,166.83,3.26,8.85" target="#fig_3">4</ref>). Clicking the bu on would display the full document content below the paragraph.</p><p>Upon making a judgment, the relevance of the paragraph and its corresponding document were sent to the CAL model to retrain the classi er. en, the next most likely paragraph with its corresponding document was selected from the remaining unassessed documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TWO STAGE ASSESSMENTS</head><p>As described in the Section 2, we designed two di erent systems for the two TREC Common Core deadlines. For the rst deadline, six graduate students and one faculty member from the UWaterlooMDS team used the prototype system described in Section 2.1 to judge documents for all the 250 topics. For our own assessments, there was no time limit for single topic. e assessors could judge as many documents as they want and were free to use/switch between the search interface and the judging interface during the task.</p><p>We judged 31,661 unique documents for all the 250 topics and 6,854 unique documents for the 50 NIST topics. We also calculated the time taken for judging any document. is was done by calculating the di erence between the timestamps at which the server received two successive judgments. In cases where judgments lasted more than 5 minutes, the judgment time for those documents were set to 5 minutes. We set the time spent to judge the rst document of every topic to the average judgment time for that task. e average and median time for a single judgment over all 250 topics were 13 and 4.1 seconds respectively. e total time to judge all the 250 topics was 115.9 hours. For the 50 NIST topics, we spent 25.4 hours in total. An average of 27.6 minutes was spent on each topic.</p><p>For the second submission deadline, the improved user study system described in Section 2.2 was used to collect users' judgments in a controlled user study. We recruited 10 participants for the user study. We divided the 50 NIST topics into 10 sets, with each set having 5 di erent topics. Each participant judged 1 such set.</p><p>In order to compare the e ectiveness and e ciency of di erent components of our interface, we designed 5 di erent treatments. ese treatments are described in Table <ref type="table" coords="9,329.82,443.42,3.41,8.85" target="#tab_0">1</ref>. e 5 treatments are described below:</p><p>• • Reference: We randomly sampled 60 documents for each topic. e probability of each document being sampled is based on the relevance of that document. e interface shows the full documents one by one in a randomized order. We wrapped our online user study system into a JavaScript executable application using Electron <ref type="foot" coords="9,505.15,602.48,3.38,6.46" target="#foot_4">5</ref> . e purpose was to restrict participants from opening other applications/tabs while judging, and to allow us to be er measure the time spent judging each document. Upon opening the application, a full-screen view of our system Reference is shown to the user. Users can exit the application by pressing the "ESC" keyboard bu on. Prior to judging, the participants completed a 1 hour in-lab tutorial on how to assess documents using the interface, relevance de nition, user study procedure and so on. Participants were asked to install the application on their laptops and were instructed to complete the study at any time of the day but within 5 to 7 days. Each user had to nish 5 tasks, with each task having 1 topic and 1 treatment. We made sure each user was given 5 di erent topics and 5 di erent treatments. Moreover, we ensured that over 10 users, we covered all 50 topics. For all tasks under the Para, Search/Para, Para&amp;Doc and Search/Para&amp;Doc treatments, users were allocated 1 hour of active time. Active time is the amount of time a user spends interacting with the system. Any mouse or keyboard action made within a 2 minute window contributes to the user's active time. As for the Reference treatment, users had to judge 60 documents without any time limit.</p><p>A er analyzing the user study data, we found that the 10 users made a total of 8,093 judgments for all 50 topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">COMPOSE SUBMITTED MANUAL RUNS</head><p>In total, we submi ed 10 manual runs to TREC Common Core for the two submissions deadlines. e rst 3 runs were composed using our own judgments from the rst assessment stage (prototype system) and were submi ed for the rst deadline (June 18, 2017). For the remaining seven runs, they were based on a mix of our own judgments and the judgments collected from the user study in the second assessment stage. e seven runs were then submi ed for the second deadline (July 31, 2017). Table <ref type="table" coords="10,351.03,444.89,4.63,8.85">2</ref> shows a brief description of each run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">First Submission</head><p>4.1.1 UWatMDS TARSv1. e classi er was trained on positive examples ("Relevant" and "On Topic" documents) and negative examples ("Not Relevant" documents). e classi er then computed relevance score for each document. e unjudged documents were regarded as "Not Relevant". erefore, the "Not Relevant" documents labeled by users were merged with unjudged documents.</p><p>For this run, every document was ranked using the tuple &lt; rele ance, score &gt;. e documents were rst ranked by "relevance" (where "Relevant" &gt; "On-Topic" &gt; "Not Relevant"), and then by the relevance scores. e top 10,000 ranked documents were selected to compose this run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">UWatMDS TARSv2</head><p>. is run is similar to UWatMDS TARSv1 except for the sample weights used for training examples. A single training iteration in so a-ml comprises of randomly sampling a positive and a negative example denoted by vectors a and b. e loss is computed over the di erence of a and b <ref type="bibr" coords="10,477.23,598.32,14.97,8.85" target="#b10">[11]</ref>:</p><formula xml:id="formula_2" coords="10,254.04,620.54,277.26,23.13">L(w, a, b) = 1 1 + e &lt;w,a-b&gt;<label>(4)</label></formula><p>When using the sample weights s a and s b , we modi ed the loss function to</p><p>• "G": is the relevance set generated from Grossman and Cormack <ref type="bibr" coords="12,375.64,107.06,10.42,8.85" target="#b5">[6]</ref>. Grossman and Cormack <ref type="bibr" coords="12,491.63,107.06,11.73,8.85" target="#b5">[6]</ref> used a variation of CAL and manually judged documents for 250 topics. ey used binary relevance -relevant and non relevant to label each judged document. • "M": is the relevance set generated from UWaterlooMDS team in the rst assessment stage.</p><p>• "U": is built out of data collected using the ve interface treatments from the user study experiment over the 50 NIST topics and 10 users. UWatMDS ustudy run was composed based on the judgments set "U" which was derived from real users' judgments. e six other runs investigated ways to use multiple judgments to produce a ranking. ree runs used the "G" + "M" relevance judgment sets (250 topics). e remaining runs used "G" + "M" + "U" (50 NIST). Judgment sets were merged using one of the three strategies:</p><p>• Union: A document is considered relevant if at least one relevance set marked it as relevant.</p><p>• Weighted: e judgment labels for documents are converted to integer labels: 0, 1, 2 for "M" and "U", and 0, 1 for "G". For any document, the new label is the sum of individual integer labels. e sample weights for each nal integer label was determined using the same approach used in UWatMDS TARSv2. • Fusion: A classi er is trained on each judgment set (e.g. classi er for "G", classi er for "M" and classi er for "U") and each classi er is used to produce a rank list r . Every document d in the document set D has a rank r (d ) in rank list r . ese rank lists R for di erent classi ers are fused into a nal ranking using Reciprocal Rank Fusion <ref type="bibr" coords="12,210.21,314.73,10.38,8.85" target="#b2">[3]</ref>. e documents in the fused rank list are ordered based on their RRF scor e as shown in Formula 6. In our experiment, k is set to 60.</p><formula xml:id="formula_3" coords="12,241.69,338.60,289.61,26.43">RRF scor e (d ∈ D) = r ∈R 1 k + r (d )<label>(6)</label></formula><p>4.2.1 UWatMDS ustudy. is run was built the same way as UWatMDS TARSv1 but using the judgments "U" from user study. e classi er was trained on positive examples ("Highly Relevant" and "Relevant" documents) and negative examples ("Not Relevant"). e classi er computed classi cation score for each document. e unjudged documents were regarded as "Not Relevant".</p><p>For this run, every document was ranked using the tuple &lt; rele ance, score &gt; as the key (where "Highly Relevant" &gt; "Relevant" &gt; "Not Relevant"). e ranking strategy is the same as the run UWatMDS TARSv1. e top 10,000 ranked documents were selected to compose this run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">UWatMDS</head><p>AFuse. Two di erent classi ers were built from the judgment "G" and "M" on 250 topics separately. Two ranked lists of documents were generated from these two classi ers and merged using reciprocal rank fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">UWatMDS AUnion.</head><p>Judgment sets "G" and "M" were merged using the Union strategy. For each topic, a classi er was trained using the merged judgment set and the resulting rank lists were used to compose the run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">UWatMDS AWgtd.</head><p>Judgment sets "G" and "M" were merged and classi ers were trained using the Weighted strategy. 4.2.5 UWatMDS BFuse. ree di erent classi ers were built from the judgments set: "G" and "M" and "U" on 50 NIST topics separately. ree rank lists of documents were generated from these three classi ers separately. We fused these three rank lists using reciprocal rank fusion to compose this run. 4.2.6 UWatMDS BUnion. We merge the three judgments sets "G", "M" and "U" using the union strategy. We built a classi er using the merged judgment set and the resulting rank list was used to compose the run. 4.2.7 UWatMDS BWgtd. Judgment sets "G", "M" and "U" (50 NIST topics) were merged and classi ers were trained using the Weighted strategy.</p><p>Table <ref type="table" coords="13,101.62,105.43,3.01,7.97">3</ref>. MAP, NDCG, P@10 and Recall@1000 for the 10 submi ed runs evaluated on the NIST 50 topics only. The AP of each topic from every run is compared with the median AP from Manual Routing runs, Automatic Routing runs and Automatic Non-Routing runs, separately. The right side of the table shows the number of topics on each run with an AP greater than or equal to the median AP of the 3 di erent routing categories. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS AND DISCUSSION</head><p>We summarize our results in Table <ref type="table" coords="13,225.70,540.40,3.46,8.85">3</ref>. Out of our 10 runs, only half of them contributed to the judgment pools. Among all the manual-non-routing runs, our runs had the highest average precision for 45 of the 50 topics. Among our runs, UWatMDS TARSv1 achieved the highest MAP, NDCG and P@10 scores. It is worth mentioning that UWatMDS TARSv1 was built only using the judgment set M. UWatMDS BFuse achieves the highest recall@1000 among all our runs. As described in Section 4.2, UWatMDS BFuse used the RRF fusion of rank lists obtained from three classi ers (each trained on judgment set "G", "M" and "U", respectively). Compared to UWatMDS BUnion and UWatMDS BWgtd which also used the same judgment sets, RRF fusion was able to build a run with higher MAP, NDCG, P@10 and recall. We also observe that UWatMDS AFuse performs be er than UWatMDS AUnion and UWatMDS AWgtd runs. Runs submi ed to TREC Common Core for evaluation have three di erent categories depending on whether the runs used judgments from past tracks: Manual Ad hoc runs, Automatic Routing runs and Automatic Ad hoc. Our runs fall under Manual Ad hoc since we did not use any judgments from past tracks. For each topic, we compare AP of our runs with the median AP of all the submi ed runs. We observe that UWatMDS BFuse and UWatMDS TARSv1 runs perform relatively be er than our other runs based on the number of topics with AP greater than or equal to the median. We also show the results of di erent treatments from user study in Table <ref type="table" coords="14,386.38,154.88,3.37,8.85" target="#tab_1">4</ref>. For each treatment, we calculated the number of judged documents across each relevance category. We observe that Para can help users nd the more documents which are marked as Highly Relevant and Relevant by users within limited time. We evaluate the user study data using the gold standard -NIST qrels and measure the number of True Positives (documents considered relevant by the users and the NIST assessors). Tasks under the Para treatment retrieved the most number of relevant documents. In addition to TP, we also measure the True Positive Rate (TPR) and False Positive Rate (FPR). Tasks under the Para treatment have the lowest FPR.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,80.70,405.78,450.60,7.97;4,80.70,416.74,62.54,7.97;4,84.09,106.99,450.61,281.63"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Search Interface layout in prototype system. The first two results shown in the SERP are "Relevant", and third result is "Not relevant".</figDesc><graphic coords="4,84.09,106.99,450.61,281.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,122.43,405.78,367.14,7.97;5,84.09,106.99,450.61,281.63"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Clicking at any result from Search Interface will show the full document judgment interface.</figDesc><graphic coords="5,84.09,106.99,450.61,281.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,221.22,397.20,169.56,7.97;6,84.09,107.00,450.60,273.04"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. CAL interface in the prototype system.</figDesc><graphic coords="6,84.09,107.00,450.60,273.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,123.62,475.80,364.77,7.97;7,84.09,177.01,450.61,281.63"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. CAL interface which shows paragraph with toggled full document in the user study system.</figDesc><graphic coords="7,84.09,177.01,450.61,281.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,165.76,405.78,280.48,7.97;8,84.09,106.99,450.61,281.63"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. CAL interface which only shows paragraph in the user study system.</figDesc><graphic coords="8,84.09,106.99,450.61,281.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="9,112.84,458.12,418.47,8.86;9,112.84,470.08,202.63,8.85;9,102.90,481.73,428.40,9.16;9,112.84,493.99,105.54,8.85;9,102.90,505.64,428.40,9.16;9,112.84,517.90,418.72,8.85;9,112.84,529.86,270.80,8.85;9,102.90,541.50,428.65,9.16;9,112.84,553.77,380.65,8.85"><head></head><label></label><figDesc>Para: Only the CAL model is active in this treatment. e CAL interface shows a single paragraph from the document for judgment, as shown in Figure 5. • Search/Para: Both the search model and the CAL model are active in this treatment. e CAL interface shows a single paragraph. • Para&amp;Doc: Only the CAL model is active in this treatment. On the CAL interface, a single paragraph from the document is shown by default. However, users are able to view the full document content by clicking on the "View full document" bu on, as shown in Figrue 4. • Search/Para&amp;Doc: Both the search model and the CAL model are active for this treatment. e CAL interface shows a single paragraph but users can view the full document content if they wish.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="10,115.66,105.43,378.18,107.91"><head>Table 1 .</head><label>1</label><figDesc>The 5 treatments used in the user study and their corresponding features.</figDesc><table coords="10,115.66,128.78,378.18,84.56"><row><cell>Treatments</cell><cell>Search Model</cell><cell>CAL model showing Paragraph</cell><cell>CAL model showing toggle full document</cell><cell>Reference Model showing full document</cell></row><row><cell>Para</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Search/Para</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Para&amp;Doc</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Search/Para&amp;Doc</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="13,80.46,160.52,451.01,351.47"><head>Table 4 .</head><label>4</label><figDesc>Results of the di erent treatments from the user study over the NIST 50 topics. Every treatment covers 10 di erent topics, with no overlapping topics among treatments. The total number of judged documents, highly relevant, relevant and non-relevant documents judged by users using each treatment are shown. We also calculated the total number of True Positive (TP), averaged True Positive Rate (TPR) and averaged False Positive Rate (FPR) for each treatment based on the gold standard -NIST qrels. For all treatments except for the Reference, we only count judgments made from the start of a topic's task to the end of the first hour of active judging (inactivity time was not counted). Few users have spent more than 1 hour on some tasks but their judgments a er the first hour were not counted. Tasks under the Reference treatment were not time-limited (users only needed to judge 60 sampled documents for the task's topic).</figDesc><table coords="13,85.01,160.52,439.83,351.47"><row><cell></cell><cell cols="5">MAP NDCG P@10 Recall@1000</cell><cell cols="2">Contributed to the pools</cell><cell cols="2"># Topics with &gt;= Median AP Manual Ad Hoc Auto Routing Auto Ad hoc</cell></row><row><cell cols="2">UWatMDS TARSv1 0.46</cell><cell>0.70</cell><cell>0.83</cell><cell cols="2">0.77</cell><cell></cell><cell></cell><cell></cell><cell>39</cell><cell>40</cell><cell>45</cell></row><row><cell cols="2">UWatMDS TARSv2 0.44</cell><cell>0.68</cell><cell>0.81</cell><cell cols="2">0.75</cell><cell></cell><cell></cell><cell></cell><cell>32</cell><cell>38</cell><cell>43</cell></row><row><cell>UWatMDS HT10</cell><cell>0.41</cell><cell>0.67</cell><cell>0.46</cell><cell cols="2">0.77</cell><cell></cell><cell></cell><cell></cell><cell>28</cell><cell>36</cell><cell>39</cell></row><row><cell>UWatMDS ustudy</cell><cell>0.32</cell><cell>0.57</cell><cell>0.65</cell><cell cols="2">0.68</cell><cell></cell><cell></cell><cell></cell><cell>17</cell><cell>20</cell><cell>37</cell></row><row><cell>UWatMDS AFuse</cell><cell>0.43</cell><cell>0.68</cell><cell>0.71</cell><cell cols="2">0.81</cell><cell></cell><cell></cell><cell></cell><cell>42</cell><cell>37</cell><cell>46</cell></row><row><cell cols="2">UWatMDS AUnion 0.40</cell><cell>0.65</cell><cell>0.65</cell><cell cols="2">0.77</cell><cell></cell><cell></cell><cell></cell><cell>30</cell><cell>29</cell><cell>42</cell></row><row><cell>UWatMDS AWgtd</cell><cell>0.40</cell><cell>0.66</cell><cell>0.68</cell><cell cols="2">0.79</cell><cell></cell><cell></cell><cell></cell><cell>33</cell><cell>27</cell><cell>40</cell></row><row><cell>UWatMDS BFuse</cell><cell>0.44</cell><cell>0.69</cell><cell>0.73</cell><cell cols="2">0.82</cell><cell></cell><cell></cell><cell></cell><cell>45</cell><cell>38</cell><cell>46</cell></row><row><cell cols="2">UWatMDS BUnion 0.38</cell><cell>0.64</cell><cell>0.64</cell><cell cols="2">0.77</cell><cell></cell><cell></cell><cell></cell><cell>28</cell><cell>27</cell><cell>40</cell></row><row><cell>UWatMDS BWgtd</cell><cell>0.42</cell><cell>0.66</cell><cell>0.74</cell><cell cols="2">0.78</cell><cell></cell><cell></cell><cell></cell><cell>36</cell><cell>32</cell><cell>44</cell></row><row><cell>Treatments</cell><cell cols="2">#Highly-Rel by users</cell><cell cols="2">#Rel by users</cell><cell cols="2">#Non-Rel by users</cell><cell cols="2">#Highly-Rel &amp; Rel by users</cell><cell>#Total judged by users</cell><cell>TPR vs. NIST</cell><cell>FPR vs. NIST</cell><cell>TP vs. NIST</cell></row><row><cell>Para</cell><cell></cell><cell>324</cell><cell>528</cell><cell></cell><cell cols="2">1410</cell><cell cols="2">852</cell><cell>2262</cell><cell>0.56 0.43 453</cell></row><row><cell>Search/Para</cell><cell></cell><cell>468</cell><cell>333</cell><cell></cell><cell>553</cell><cell></cell><cell cols="2">801</cell><cell>1354</cell><cell>0.72</cell><cell>0.62</cell><cell>403</cell></row><row><cell>Para/Doc</cell><cell></cell><cell>179</cell><cell>349</cell><cell></cell><cell cols="2">2168</cell><cell cols="2">528</cell><cell>2696</cell><cell>0.85</cell><cell>0.49</cell><cell>263</cell></row><row><cell cols="2">Search/Para+Doc</cell><cell>219</cell><cell>236</cell><cell></cell><cell>333</cell><cell></cell><cell cols="2">455</cell><cell>788</cell><cell>0.70</cell><cell>0.53</cell><cell>200</cell></row><row><cell>Reference</cell><cell></cell><cell>72</cell><cell>91</cell><cell></cell><cell>437</cell><cell></cell><cell cols="2">163</cell><cell>600</cell><cell>0.86 0.51</cell><cell>83</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,83.93,639.34,108.90,7.08"><p>h ps://github.com/trec-core/2017</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,84.07,649.30,130.97,7.08"><p>h ps://catalog.ldc.upenn.edu/ldc2008t19</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,84.07,649.30,141.28,7.08"><p>h ps://plg.uwaterloo.ca/ gvcormac/trecvm/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,83.93,649.30,141.96,7.08"><p>h ps://code.google.com/archive/p/so a-ml/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="9,84.07,649.30,78.64,7.08"><p>h ps://electron.atom.io/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="14,96.68,252.39,104.15,9.70;14,80.70,267.94,450.61,8.85;14,80.70,279.90,25.51,8.85"><p>ACKNOWLEDGMENTSpecial thanks to Royal Sequeira and Salman Mohammed for their help with judgments during the rst submission phase.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(5)</p><p>During classi er training for UWatMDS TARSv1, "Relevant" and "On-Topic" documents were treated as a positive example (wei ht = 1), and "Non Relevant" documents were treated as a negative example (wei ht = -1). For this run, we experimented with various weights corresponding to each relevance label. Each parameter choice was evaluated using Mean Average Precision. We performed k-fold cross validation (k = 5) to select the best weight combination for each topic, which was used to train the nal classi er for that topic. e run generation was performed in the same way as UWatMDS TARSv1.</p><p>4.1.3 UWatMDS HT10. is run used the same classi er used in UWatMDS TARSv1. Documents were ranked by its relevance score. For this run, 10 documents of each topic were randomly sampled according to the probability estimated by the Horvitz-ompson estimator <ref type="bibr" coords="11,340.75,262.16,15.12,8.85" target="#b13">[14,</ref><ref type="bibr" coords="11,358.45,262.16,11.34,8.85" target="#b17">18]</ref>. In our se ing, the probability of each document being relevant was set to the reciprocal rank based on its classi cation score. e remaining 9990 documents in this run were selected based on their classi cation scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Second Submission</head><p>e second submission contained 7 runs. e judgments used for the second submission are described below: Table <ref type="table" coords="11,101.89,346.18,3.02,7.97">2</ref>. Brief description of the 10 submi ed runs to TREC. Three sources of judgments were used to compose the runs. "G" judgments set were generated from Grossman and Cormack <ref type="bibr" coords="11,298.94,357.14,9.43,7.97" target="#b5">[6]</ref>. "M" judgments set were generated from the UWaterlooMDS team during the first assessment phase. "U" judgments set were generated from 10 participants of the user study during the second assessment phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runs</head><p>Judgments </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="14,98.77,320.54,432.54,7.08;14,98.49,330.50,434.04,7.08;14,98.49,340.46,56.62,7.08" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,467.71,320.54,63.60,7.08;14,98.49,330.50,106.61,7.08">Optimizing Nugget Annotations with Active Learning</title>
		<author>
			<persName coords=""><forename type="first">Gaurav</forename><surname>Baruah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rakesh</forename><surname>Gu Ikonda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olga</forename><surname>Vechtomova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,218.22,330.51,310.47,7.06">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 25th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2359" to="2364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,98.77,350.42,433.76,7.08;14,98.77,360.39,32.10,7.08" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="14,318.45,350.43,210.90,7.06">Information retrieval: Implementing and evaluating search engines</title>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Bü</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cher</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles La</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Mit Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,98.77,370.35,432.74,7.08;14,98.77,380.31,433.76,7.08;14,98.49,390.27,49.21,7.08" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,318.12,370.35,213.39,7.08;14,98.77,380.31,55.20,7.08">Reciprocal rank fusion outperforms condorcet and individual rank learning methods</title>
		<author>
			<persName coords=""><surname>Gordon V Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefan Bue</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,168.41,380.32,361.30,7.06">Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 32nd international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="758" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,98.77,400.24,432.53,7.08;14,98.77,410.20,433.41,7.08;14,98.58,420.16,28.36,7.08" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,259.00,400.24,272.30,7.08;14,98.77,410.20,29.94,7.08">Evaluation of machine-learning protocols for technology-assisted review in electronic discovery</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,143.62,410.21,364.07,7.06">Proceedings of the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</title>
		<meeting>the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,98.77,430.12,432.53,7.08;14,98.77,440.09,222.18,7.08" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="14,265.70,430.12,265.60,7.08;14,98.77,440.09,21.47,7.08">Autonomy and Reliability of Continuous Active Learning for Technology-Assisted Review</title>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<idno>arxiv.org/abs/1504.06868</idno>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,98.77,450.05,433.48,7.08;14,98.77,460.01,82.83,7.08" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="14,253.36,450.05,278.89,7.08;14,98.77,460.01,46.04,7.08">MRG UWaterloo and WaterlooCormack Participation in the TREC 2017 Core Track: Notebook Dra</title>
		<author>
			<persName coords=""><forename type="first">Maura</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,161.98,460.02,15.69,7.06">TREC</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,98.77,469.98,432.53,7.08;14,98.77,479.95,333.16,7.06" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="14,326.73,469.98,130.85,7.08">TREC 2016 Total Recall Track Overview</title>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roegiest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,472.32,469.98,58.98,7.06;14,98.77,479.95,121.90,7.06">Proceedings of e Twenty-Fi h Text REtrieval Conference</title>
		<meeting>e Twenty-Fi h Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-15">2016. 2016. November 15-18, 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,98.77,489.90,432.54,7.08;14,98.77,499.86,154.61,7.08" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="14,322.51,489.90,208.79,7.08;14,98.77,499.86,27.88,7.08">Clef 2017 technologically assisted reviews in empirical medicine overview</title>
		<author>
			<persName coords=""><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Leif</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rene</forename><surname>Spijker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="14,133.01,499.87,73.35,7.06">Working Notes of CLEF</title>
		<imprint>
			<biblScope unit="page" from="11" to="14" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,98.77,509.83,432.54,7.08;14,98.53,519.79,21.33,7.08" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roegiest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles</forename><surname>Clarke</surname></persName>
		</author>
		<title level="m" coord="14,352.04,509.83,179.27,7.08">TREC 2015 Total Recall track overview. Proc. TREC-2015</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,98.77,529.75,432.53,7.08;14,98.77,539.71,287.54,7.08" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="14,226.04,529.75,153.40,7.08">Forming test collections with no system pooling</title>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hideo</forename><surname>Joho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,393.30,529.76,138.00,7.06;14,98.77,539.72,240.95,7.06">Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 27th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,98.77,549.68,432.53,7.08;14,98.77,559.64,137.00,7.08" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="14,165.96,549.68,108.53,7.08">Combined regression and ranking</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Sculley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,288.53,549.68,242.77,7.06;14,98.77,559.65,82.31,7.06">Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 16th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="979" to="988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,98.77,569.60,432.53,7.08;14,98.77,579.56,328.13,7.08" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="14,295.57,569.60,235.73,7.08;14,98.77,579.56,50.27,7.08">Human question answering performance using an interactive document retrieval system</title>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Mark D Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Blagovest</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Dachev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,163.20,579.57,216.05,7.06">Proceedings of the 4th Information Interaction in Context Symposium</title>
		<meeting>the 4th Information Interaction in Context Symposium</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,98.77,589.53,432.73,7.08;14,98.77,599.49,350.68,7.08" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="14,345.66,589.53,185.84,7.08;14,98.77,599.49,22.18,7.08">Indri: A language model-based search engine for complex queries</title>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Howard</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cro</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,134.79,599.50,207.65,7.06">Proceedings of the International Conference on Intelligent Analysis</title>
		<meeting>the International Conference on Intelligent Analysis<address><addrLine>Amherst, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,98.77,609.45,241.23,7.08" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="14,153.44,609.46,64.41,7.06">Sampling algorithms</title>
		<author>
			<persName coords=""><forename type="first">Yves</forename><surname>Tillé</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,98.77,619.42,432.81,7.08;14,98.77,629.38,112.11,7.08" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="14,179.96,619.42,265.21,7.08">Variations in relevance judgments and the measurement of retrieval e ectiveness</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,451.24,619.42,80.34,7.06;14,98.77,629.39,41.13,7.06">Information processing &amp; management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="697" to="716" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,98.77,639.34,432.54,7.08;14,98.77,649.30,214.51,7.08" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="14,178.05,639.34,129.84,7.08">Evaluation by highly relevant documents</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,322.10,639.35,209.21,7.06;14,98.77,649.31,167.92,7.06">Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 24th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="74" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,98.77,108.36,432.74,7.08;15,98.77,118.32,171.28,7.08" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="15,378.07,108.36,153.44,7.08;15,98.77,118.32,120.79,7.08">Evaluating Sentence-Level Relevance Feedback for High-Recall Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Submi ing</note>
</biblStruct>

<biblStruct coords="15,98.77,128.29,432.54,7.08;15,98.77,138.25,433.41,7.08;15,98.77,148.21,28.36,7.08" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="15,356.25,128.29,175.05,7.08;15,98.77,138.25,33.64,7.08">Sampling Strategies and Active Learning for Volume Estimation</title>
		<author>
			<persName coords=""><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,146.44,138.26,361.88,7.06">Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="981" to="984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,98.77,158.17,432.54,7.08;15,98.53,168.14,429.94,7.08" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="15,396.81,158.17,134.49,7.08;15,98.53,168.14,17.04,7.08">WaterlooClarke: TREC 2015 Total Recall Track</title>
		<author>
			<persName coords=""><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yipeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Smucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,129.65,168.15,187.55,7.06">Proceedings of e Twenty-Fourth Text REtrieval Conference</title>
		<meeting>e Twenty-Fourth Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-11-17">2015. 2015. November 17-20, 2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
