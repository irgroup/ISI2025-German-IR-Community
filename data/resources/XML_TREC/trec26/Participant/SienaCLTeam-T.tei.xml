<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,201.31,76.79,213.34,16.00">Siena&apos;s Tasks Track System</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,161.62,117.04,68.01,8.75"><forename type="first">Emily</forename><surname>Barranca</surname></persName>
							<email>ebarran1@swarthmore.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Swathmore College Siena College</orgName>
								<address>
									<addrLine>500 College Avenue 515 Loudon Road Swathmore</addrLine>
									<postCode>19081, 12211</postCode>
									<settlement>Loudonville</settlement>
									<region>PA, NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,363.98,117.04,88.72,8.75"><forename type="first">Sharon</forename><surname>Gower Small</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Swathmore College Siena College</orgName>
								<address>
									<addrLine>500 College Avenue 515 Loudon Road Swathmore</addrLine>
									<postCode>19081, 12211</postCode>
									<settlement>Loudonville</settlement>
									<region>PA, NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,201.31,76.79,213.34,16.00">Siena&apos;s Tasks Track System</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">27A9E91CAE43904A7AC64312D3DB741E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper discusses our development of Siena's Tasks Track System (STTS) and its participation in the Tasks Track of the 2017 Text Retrieval Conference (TREC). The purpose of this track is to try to understand the underlying task a user might be trying to complete when searching a specific query. The Task Understanding portion of this task aimed to find a list of up to 100 keywords and phrases that might be related to a user's task given the example query. In the first year of our work in this track we submitted two fairly simple runs. In the first run, we used the jsoup library to run these queries through Google and processed the first five documents resulting from the search. We automatically extracted keywords and phrases that appeared commonly across the documents. In the second run we utilized Wordnet to generate new words and phrases to augment the query. These two methods, the Wordnet approach and the Google approach were submitted as two separate runs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The Tasks Track <ref type="bibr" coords="1,159.27,416.80,107.29,10.67">(Verma et. al., 2016)</ref> is a track in the Text Retrieval Conference <ref type="bibr" coords="1,484.56,416.80,58.12,10.67;1,72.00,430.72,30.22,10.67" target="#b7">(Voorhees, 2007)</ref>, hosted by the National Institute of Standards and Technology. The Tasks Track is one track in the TREC conference, which aims to understand the larger infromation task a user might be attempting to accomplish when entering a single query into a search engine. In the completion of the Task Understanding piece of this track, participants were to produce a list of up to 100 ranked keywords/phrases for each of 50 queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">TREC 2017 Literature Review</head><p>While designing the experimental procedure for this year's task track, our team reviewed literature from the 2015 and 2016 Task Track submissions. The Microsoft Research team produced a list of ranked keywords using anchor text to grab links where the destination is an external site <ref type="bibr" coords="1,162.47,593.92,134.16,10.67">(Bennett &amp; White, 2015)</ref>. Similarly in 2016 at Bauhaus University in Weimar, Germany, their team used anchor text to generate keyword queries under "the assumption that the texts in HTML anchors (with href attribute) are short descriptions of the documents they link to" <ref type="bibr" coords="1,232.94,636.16,109.70,10.67">(Hagen et. al, 2016)</ref>. The Bauhaus University team also submitted a run consisting of related search suggestions from the common search engines Bing and Google, as they had done for the previous year's submission.</p><p>In 2016, the University of Stavanger team <ref type="bibr" coords="1,316.40,684.40,137.60,10.67" target="#b2">(Garigliotti, Balog, 2016)</ref> used keywords extracted from relevant documents and search engine suggestions, taken from Google and Bing. In their procedure, the top ten documents were collected from Google and Bing, and then they applied the RAKE keyword extraction system, which first isolates all possible words and phrases and then formulaically and using machine learning, ranks the likelihood a word is a keyword. Their team then formulaically considered the success of adding the keywords found to the beginning of a given query, the end, or whether the keyword stood best on its own</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Main System Components</head><p>To complete the Task Understanding problem of the 2017 Tasks Track, our team submitted two runs, the first of which was focused on extracting keywords and phrases from the top N documents (N=5) returned by Google for each given query. The second run submitted by our team focused on the use of Wordnet <ref type="bibr" coords="2,301.20,252.40,76.68,10.67" target="#b4">(Miller, 1995)</ref> software to find keywords or phrases that can be used to augment the given queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Run 1: Topic Detection 3.1.1 Document Processing</head><p>In order to retrieve the top documents returned by Google, our team used the open source jsoup<ref type="foot" coords="2,100.08,350.64,4.39,7.04" target="#foot_0">1</ref> Java library, a parser for working with real world HTML. Using a test set of previous years' results we ran experiments to determine the ideal number of documents to process and the ideal window of text to process within those documents.</p><p>After experimentation of document ranges between 2 and 15 we found that the first 5 documents of the Google search gave the best results when processed. We also found, for our test set, that the first 1000 characters from each document is the ideal set of text to process. From this text we removed common stopwords. We also generated a custom list of stopwords that were common in webpages, not filtered by the jsoup html to text method, and not useful for the purposes of this task, i.e. site map, site content, english version, etc. This list certainly could have been longer and more encompassing, but in the time frame we had we were able to generate 192 stopwords and phrases by manual analysis of results from the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Common Phrases</head><p>Our next step was to find the most common overlapping noun phrases between the top 5 documents in their first 1000 characters. Based on previous research <ref type="bibr" coords="2,490.72,568.96,51.93,10.67;2,72.00,583.12,108.21,10.67" target="#b5">(Small &amp; Strzalkowski, 2004)</ref> in topic generation we hypothesized that these overlapping noun phrases would generate a good list of topics related to the queries. We used the Stanford part of speech tagger <ref type="bibr" coords="2,189.92,611.20,162.84,10.67" target="#b6">(Toutanova &amp; Manning, 2000)</ref> to tag the text. We then used this tagged text to generate a list of the noun phrases that existed in each block of text and then looked for the most commonly occurring phrases in this set, with longer phrases taking precedence over shorter phrases. For this task we returned the top 20 phrases found, ordered by frequency as the primary metric and number of words in a phrase as a secondary metric. That is words/phrases are ordered based on frequency first and within these they are ordered based on number of words in each phrase. For each of the topics we submitted our top 20 keywords and phrases to be evaluated. One of our better performing topics, topic 8 is shown below. We scored a .97 nDCG@20 for this topic. TREC Topic: For our second run, we hypothesized that useful phrases could be generated by finding synonyms or other related words to replace particular words in the given queries. Again, for our second run, we tested our procedure with sample queries from previous years to optimize our output. This process however did not always return a variation of 20 new phrases. When our Run 2 did not have 20 new phrases generated we supplemented its list with the phrases from Run 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Wordnet MIT Java Wordnet Interface</head><p>To access the Wordnet software in our design, we used the MIT Java Wordnet Interface Library <ref type="bibr" coords="4,114.04,131.44,134.11,10.67" target="#b1">(Finlayson &amp; Alan, 2014)</ref>. Our team found synonyms for each word in the query and found synsets for each verb in each query. Synsets are defined by Wordnet to be "words that denote the same concept and are interchangeable in many contexts".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Related Verbs</head><p>After analyzing the sample queries we worked with, we found that the nouns in the query were typically most vital to the user's task and desired information. The verbs, however, when replaced by synsets and related words, did not result in loss of meaning in the user's task. To distinguish which words from the queries were verbs and which words were nouns, we ran each query through the Stanford Part of Speech tagger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Generating Key Words</head><p>Our new keywords/phrases were generated by switching out the related words returned to us by Wordnet with each verb in each query. Additionally, we replaced nouns and verbs with their synonyms, only changing one word at a time from the original query, as to not lose the user's original task. Our highest ranked output words were the phrases generated by replacing verbs with related words, then with synonyms. Lastly, came our results generated by replacing nouns from the original query with synonyms. Shown below are the generated key words for topic 23. For this run we did not have to augment with any phrases from Run 1. For Run 2, these words received a .83 nDCG@20 score. TREC Topic: &lt;task id = "23"&gt; &lt;query&gt;put a newborn baby to sleep&lt;/query&gt; &lt;freebase entity = "1"&gt; &lt;id&gt;/m/0jnvp&lt;/id&gt; &lt;text&gt;newborn baby&lt;/text&gt; &lt;/freebase&gt; &lt;freebase entity = "2"&gt; &lt;id&gt;/m/06xzh&lt;/id&gt; </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results Run 1</head><p>Our STREC@20 score for Run One performed very well with a recall of 81.96%. Our alpha--nDCG@20 was 50.22%. We have just received our results and are still performing our error analysis. Further discusion will be included in our final paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 2</head><p>Our STREC@20 score for one run performed very well with a recall of 76.97%. Our alpha--nDCG@20 was 62.43%. We have just received our results and are still performing our error analysis. Further discusion will be included in our final paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Previous Year</head><p>The metrics reported last year used nERR--IA@20 and alpha-nDCG@20. We do not have scores for this year yet so we will compare to 2016. The best scores reported for 2016 for nERR--IA@20 were 0.57 and ours was 0.59. The best scores reported for 2016 for alpha-nDCG@20 were 0.70 and ours was 0.62.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In reviewing output for Run 2, our team found many of our generated results were either identical to the original or with little to no change in meaning. In understanding the overall task, there is more to be done in terms of finding synsets for query words and phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Future Work</head><p>Our intial thoughts for future work is to develop a synthesized solution utilizing ideas from both Run 1 and Run 2. Potentially, we could run our keywords and phrases generated in Run 2 through our Google search algorithm from Run 1.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,72.00,187.36,252.71,444.83"><head>Run 2: Wordnet on Query Phrases</head><label></label><figDesc></figDesc><table coords="3,72.00,187.36,252.71,444.83"><row><cell cols="2">&lt;task id = "8"&gt;</cell></row><row><cell cols="2">&lt;query&gt;time travel&lt;/query&gt;</cell></row><row><cell cols="2">&lt;freebase entity = "1"&gt;</cell></row><row><cell cols="2">&lt;id&gt;/m/07s2s&lt;/id&gt;</cell></row><row><cell cols="2">&lt;text&gt;time travel&lt;/text&gt;</cell></row><row><cell cols="2">&lt;/freebase&gt;</cell></row><row><cell>&lt;/task&gt;</cell><cell></cell></row><row><cell cols="2">STTS Output in Ranked Order:</cell></row><row><cell>1</cell><cell>other uses time travel</cell></row><row><cell>2</cell><cell>time machine travel</cell></row><row><cell>3</cell><cell>certain points time travel</cell></row><row><cell>4</cell><cell>quantum mechanics time travel</cell></row><row><cell>5</cell><cell>different points time travel</cell></row><row><cell>6</cell><cell>time travel</cell></row><row><cell>7</cell><cell>hypothetical device time travel</cell></row><row><cell>8</cell><cell>time dilation travel</cell></row><row><cell>9</cell><cell>distant points time travel</cell></row><row><cell>10</cell><cell>arbitrary point time travel</cell></row><row><cell>11</cell><cell>limited support time travel</cell></row><row><cell>12</cell><cell>theoretical physics time travel</cell></row><row><cell>13</cell><cell>Einstein--Rosen bridges time travel</cell></row><row><cell>14</cell><cell>narrow sense time travel</cell></row><row><cell>15</cell><cell>well--understood phenomenon time travel</cell></row><row><cell>16</cell><cell>special relativity time travel</cell></row><row><cell>17</cell><cell>general relativity time travel</cell></row><row><cell>18</cell><cell>large amount time travel</cell></row><row><cell>19</cell><cell>current technology time travel</cell></row><row><cell>20</cell><cell>possible Russian Cosmonaut time travel</cell></row><row><cell>3.2</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,79.07,709.12,91.88,10.67"><p>https://jsoup.org</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,72.00,102.69,467.98,8.64;6,72.00,114.21,131.04,8.64" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="6,251.04,102.69,288.94,8.64;6,72.00,114.21,126.74,8.64">Mining Tasks from the Web Anchor Text Graph: MSR Notebook Paper for the TREC 2015 Tasks Track</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ryen</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.00,137.25,467.96,8.64;6,72.00,148.77,467.95,8.64;6,72.00,160.29,77.43,8.64" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,190.14,137.25,322.57,8.64">Java Libraries for Accessing the Princeton Wordnet: Comparison and Evaluation</title>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Alan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,237.17,148.77,263.17,8.64">Proceedings of the 7th International Global WordNet Conference</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Orav</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Vossen</surname></persName>
		</editor>
		<meeting>the 7th International Global WordNet Conference<address><addrLine>Tartu, Estonia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="78" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.00,183.33,452.30,8.64" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="6,224.41,183.33,240.13,8.64">The University of Stavanger at the TREC 2016 Tasks Track</title>
		<author>
			<persName coords=""><forename type="first">Dario</forename><surname>Garigliotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.00,206.13,467.83,8.64;6,72.00,217.65,208.21,8.64" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,474.43,206.13,65.40,8.64;6,72.00,217.65,101.20,8.64">Webis at TREC 2016: Tasks, Total Recall</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Adineh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Masoud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fatehifar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bahrami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Fichtl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>and Open Search Tracks</note>
</biblStruct>

<biblStruct coords="6,72.00,240.69,452.94,8.64" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,136.67,240.69,166.80,8.64">WordNet: A Lexical Database for English</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,312.77,240.69,116.20,8.64">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.00,263.73,467.85,8.64;6,72.00,275.25,378.81,8.64" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,262.76,263.73,277.10,8.64;6,72.00,275.25,40.70,8.64">HITIQA: A Data Driven Approach to Interactive Analytical Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Sharon</forename><surname>Small</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tomek</forename><surname>Strzalkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,119.78,275.25,230.52,8.64">Proceedings of Human Language Technology Conference</title>
		<meeting>Human Language Technology Conference<address><addrLine>Boston, Massachusetts</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.00,298.29,467.85,8.64;6,72.00,309.81,468.05,8.64;6,72.00,321.33,189.63,8.64" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,309.06,298.29,230.79,8.64;6,72.00,309.81,124.89,8.64">Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger</title>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,204.47,309.81,335.57,8.64;6,72.00,321.33,184.39,8.64">Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-2000)</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.00,344.13,467.89,8.64;6,72.00,355.65,57.17,8.64" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,182.82,344.13,102.56,8.64">Overview of TREC 2007</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,305.76,344.13,234.13,8.64">Proceedings of The Sixteenth Text Retrieval Conference</title>
		<meeting>The Sixteenth Text Retrieval Conference</meeting>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
