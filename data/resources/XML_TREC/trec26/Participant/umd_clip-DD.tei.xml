<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,136.68,76.48,339.21,12.60;1,224.04,93.52,164.46,12.60">UMD_CLIP: Using Relevance Feedback to Find Diverse Documents for TREC Dynamic Domain 2017</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,216.48,133.80,72.95,10.77"><forename type="first">Kristine</forename><surname>Rogers</surname></persName>
							<email>krogers@umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">College of Information Studies</orgName>
								<orgName type="institution">UMIACS University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,313.10,133.80,82.18,10.77"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
							<email>oard@umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">College of Information Studies</orgName>
								<orgName type="institution">UMIACS University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,136.68,76.48,339.21,12.60;1,224.04,93.52,164.46,12.60">UMD_CLIP: Using Relevance Feedback to Find Diverse Documents for TREC Dynamic Domain 2017</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9B4C6154A81D666266633418ECAF816E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The University of Maryland's participation in TREC's 2017 Dynamic Domain track focused on two types of experiments: adding new terms from passages judged as being relevant, and exclusion of terms from documents that the track's jig feedback system indicated were not relevant to the topic. The best results for iterative multi-step retrieval were obtained by restricting retrieval to documents that contained all topic terms, and then ranking those documents using terms extracted from known relevant passages.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The participation of the Computational Linguistics and Information Processing Lab at the University of Maryland (UMD_CLIP) in TREC 2017 Dynamic Domain track (TREC-DD) focused on characterizing user intent from jig feedback, and on improving over a simple no-feedback baseline that performed a single query using TREC-DD topic terms and then sequentially returned the documents in rank order, going deeper in that list with every iteration. Our experimental conditions can be categorized in two ways: adding relevant terms, and excluding irrelevant terms. We obtained our best results from an approach that required the inclusion of the original topic terms and then ranked that subset based on the presence of terms from relevant passages, as provided by the TREC-DD jig-the system through which relevance feedback was provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General Approach</head><p>We implemented all of our methods using the Indri information retrieval system from the University of Massachusetts using Indri's default ranking function with stemming disabled. All of our results are for the automatic condition, with no human intervention. Each TREC-DD run<ref type="foot" coords="1,529.20,579.34,4.06,7.21" target="#foot_0">1</ref> involves repeatedly submitting iterations of 5 document results at a time, until a total of 25 documents (5 iterations of 5 results) have been submitted. The TREC-DD track coordinators allowed up to 10 iterations per run; however, we stopped after 5 iterations as a static estimate of the state where the user's information need has been met.</p><p>In every run, our first iteration (i.e. set of 5 document results) was obtained by using the TREC topic terms as the query, performing ranked retrieval, and submitting the top 5 results to the jig-the system that provided simulated user feedback. We then formulated the query for each subsequent set of 5 submitted document results separately, based solely on the topic terms and the 5 results reported as relevant or as not relevant by the jig for the immediate prior query. Stopwords were consistently removed from the TREC-DD topic and from documents in the result sets before queries were formed. To avoid submission of duplicates, we maintained a running list of submitted documents for each run, and any documents on that list were removed from the result set before selecting the 5 document results to submit to the jig.</p><p>Our principal focus is on Normalized Cube Test (nCT) results, the normalized version of the Cube Test (an evaluation measure used in all three TREC-DD tracks). For completeness, we also report Cube Test (CT), Average Cube Test (ACT), nCT, Session Discounted Cumulative Gain (sDCG), normalized sDCG (nsDCG), Expected Utility (EU), and normalized EU (nEU) results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Official Runs</head><p>Official runs are those runs that are submitted on time and scored by NIST. We submitted three official runs to TREC-DD 2017: clip_baseline: Our baseline run performed a single search using the topic terms as the query and then iteratively returned the first 25 documents, five at a time, ignoring any relevance feedback from the jig. clip_addwords: Our addwords run used positive feedback to perform query expansion. As always, the first query was formed only from topic terms. Subsequent queries were formed from topic terms plus terms from documents reported by the jig to be relevant. Each term (from the topic or from the positive feedback expansion set) was given equal weight.</p><p>clip_filter: Our filter run used positive feedback to perform reranking. Indri's filter operator performs a Boolean conjunction (i.e., an AND operator) over the "required" term set and then reranks the remaining documents using an "optional" term set. We used the topic terms as the required term set and the terms found in the relevant document as the optional term set.</p><p>The expansion terms in our addwords run and the optional terms in our filter run were identical. Our unconstrained approach to positive feedback (removing only stopwords) resulted in term sets ranging from small numbers-less than ten-to several hundred new terms.</p><p>For the baseline run we searched the full New York Times (NYT) collection . Because of a configuration error, the other two official runs searched only the first half of the collection <ref type="bibr" coords="2,122.04,646.92,26.11,10.77">(1987)</ref><ref type="bibr" coords="2,148.15,646.92,5.22,10.77">(1988)</ref><ref type="bibr" coords="2,148.15,646.92,5.22,10.77">(1989)</ref><ref type="bibr" coords="2,148.15,646.92,5.22,10.77">(1990)</ref><ref type="bibr" coords="2,148.15,646.92,5.22,10.77">(1991)</ref><ref type="bibr" coords="2,148.15,646.92,5.22,10.77">(1992)</ref><ref type="bibr" coords="2,148.15,646.92,5.22,10.77">(1993)</ref><ref type="bibr" coords="2,148.15,646.92,5.22,10.77">(1994)</ref><ref type="bibr" coords="2,148.15,646.92,5.22,10.77">(1995)</ref><ref type="bibr" coords="2,148.15,646.92,5.22,10.77">(1996)</ref><ref type="bibr" coords="2,153.38,646.92,26.11,10.77">(1997)</ref>. We therefore focus on our baseline run results here, and we report unofficial results below for our addwords and filter runs with the entire collection. Figure <ref type="figure" coords="3,105.09,152.88,6.06,10.77" target="#fig_1">3</ref> is a chart drawn from the TREC-DD results at the time of the conference. The clip_baseline run, the only valid official run we submitted, is shown in orange (it is the best of our 3 official runs). We note with interest that, although no new results were submitted after the fifth iteration (the Cube Test score remains static), the nCT score appears to increase; the measure rewards result sets that provide relevant documents in a smaller number of iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unofficial Runs</head><p>After the submission deadline we repeated our addwords and filter runs on the entire collection, and we report those results here as locally scored unofficial runs. We also ran three additional contrastive conditions as unofficial runs. For this larger set of runs, we found it convenient to adopt a richer and more compact nomenclature: T=topic terms, N=new (positive feedback) terms, S=spam (negative feedback) terms, f=filter, w=weighted, r=reject.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Positive Feedback</head><p>T+N: This is the same as our official addwords run, but searching the entire collection.</p><p>fT+N: This is the same as our official filter run, but searching the entire collection.</p><p>wT+N: This is a variant of T+N in which we give five times as much weight to a topic term as to an expansion term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Negative Feedback</head><p>We tried some negative feedback approaches that were motivated generally by an approach described in the TREC 2015 Dynamic Domain track overview <ref type="bibr" coords="3,367.45,489.36,116.77,10.77">(Yang &amp; Soboroff, 2015)</ref> that was attributed there to Beijing University of Posts and Telecommunications (BUPT). We refer to the non-stopword terms in documents reported by the jig not to be relevant as "spam" terms. We tried two negative feedback variants:</p><p>rS+T: Indri includes a variant of the filter operator can accept list of "reject" terms. If any reject term is present in a document, that document will be removed from the result set. For our rS+T run we specified the spam terms as reject; the remaining documents were then reranked based on the topic terms.</p><p>T!S: This run (read "T not S") used Indri's fuzzy negation operator to downweight documents that contained spam terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unofficial Results</head><p>As shown in Table <ref type="table" coords="3,162.37,688.56,4.53,10.77" target="#tab_1">2</ref>, our fT+N run did much better than all of the others by the nCT measure, with statistically significant improvements over the baseline (two-tailed paired t-test, p&lt;0.05).</p><p>The rS+T approach also yielded a statistically significant improvement (two-tailed paired t-test, p&lt;0.05), though it had a smaller improvement in nCT scores than observed with fT+N. Here the baseline result is denoted with the letter T.  <ref type="figure" coords="4,202.92,364.68,6.06,10.77" target="#fig_1">3</ref> the line for the baseline values is obscured by the lines for T+N and wT+N, which received similar nCT scores.  Our analysis revealed that approaches that made use of weighting produced CT that were not statistically significantly different from the baseline scores. This included assigning high weights for new terms from the jig, low weights for spam terms, and combining the two options. In Figure <ref type="figure" coords="6,117.23,75.84,6.06,10.77" target="#fig_1">3</ref> we show where our highest scoring run-fT+N-would have fallen relative to the TREC-DD official submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Future Work</head><p>We should note that aside from removing stopwords, we did not perform any cleanup of added words or spam words. Looking back at the generated query sets, there are cases where identical or variant spellings of terms were contained in the original keywords, added keywords, or spam keywords. We thus should also consider variants that make selective use of stemming.</p><p>We also should ensure a careful separation between topic and feedback terms, since with our current approach, a topic term could also appear in the positive feedback.</p><p>Future extensions of our approach could also include combining the negative and positive approaches-resulting a case where we add in relevant terms learned from the jig, while also removing documents with "spam" terms.</p><p>Other approaches for accomplishing this task could include clustering the document results for a given subtopic, then submitting the highest scoring documents from five separate clusters for each run. This could further improve the diversity of the document result sets. Some of our runs were quite slow. For our positive feedback approaches, it took Indri 10-60 minutes to run the full set of queries. For negative feedback, Indri took 1-4 hours for each set of queries. Clearly, once we settle on an effective set of techniques, we would then want to work on efficient implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Our participation in the 2017 TREC-DD track included experiments with Indri operators for adding and removing search terms as a method for producing a more diverse document result set. A statistically significant improvement was obtained over a simple ranked retrieval baseline using Indri's filter operator with positive feedback. Smaller but potentially promising positive effects -also statistically significant -were seen from the use of a variant of the filter operator with negative feedback.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,72.00,649.39,366.25,8.09"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Comparing positive feedback approaches, sorted in descending order by baseline nCT score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,72.00,691.39,450.40,8.09;5,72.00,702.43,79.79,8.09;5,72.00,412.44,468.00,267.00"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. TREC-DD 2017 official runs, with an added point showing our highest scoring unofficial run (fT+N) after 5 iterations (best viewed in color).</figDesc><graphic coords="5,72.00,412.44,468.00,267.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,72.00,75.49,396.47,49.28"><head>Table 1 :</head><label>1</label><figDesc>Baseline Results</figDesc><table coords="3,84.72,98.88,383.75,25.89"><row><cell>CT</cell><cell>ACT</cell><cell>nCT</cell><cell>sDCG</cell><cell>nsDCG</cell><cell>EU</cell><cell>nEU</cell></row><row><cell>0.1228</cell><cell>0.2136</cell><cell>0.6215</cell><cell>25.9919</cell><cell>0.4492</cell><cell>30.3262</cell><cell>0.4487</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,72.00,130.93,458.54,244.52"><head>Table 2 :</head><label>2</label><figDesc>Results from baseline, positive, and negative feedback retrieval approaches for 5 iterations.</figDesc><table coords="4,77.64,155.41,452.90,149.47"><row><cell></cell><cell>CT</cell><cell>ACT</cell><cell>nCT</cell><cell>sDCG</cell><cell>nsDCG</cell><cell>EU</cell><cell>nEU</cell></row><row><cell>Baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>T</cell><cell>0.1228</cell><cell>0.2136</cell><cell cols="5">0.6215 25.9919 0.4492 30.3262 0.4487</cell></row><row><cell>Positive Feedback</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>T+N</cell><cell>0.1278</cell><cell>0.2195</cell><cell cols="5">0.6468 30.7061 0.5404 37.6496 0.5064</cell></row><row><cell>fT+N</cell><cell>0.2667</cell><cell>0.2872</cell><cell cols="5">1.3506 22.2979 0.3731 24.1459 0.4079</cell></row><row><cell>wT+N</cell><cell>0.1236</cell><cell>0.2167</cell><cell cols="5">0.6257 27.2080 0.4690 31.7507 0.4576</cell></row><row><cell>Negative Feedback</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>rS+T</cell><cell>0.1410</cell><cell>0.2265</cell><cell cols="5">0.7133 23.2507 0.3995 26.1024 0.4223</cell></row><row><cell>T!S</cell><cell>0.1172</cell><cell>0.2115</cell><cell cols="5">0.5927 23.4830 0.4102 26.8278 0.4312</cell></row></table><note coords="4,72.00,333.12,426.49,10.77;4,72.00,348.84,444.77,10.77;4,72.00,364.68,128.21,10.77"><p>Figures 1 and 2 present comparisons of our positive and negative feedback approaches, respectively. Both graphs are sorted in descending order based on the original baseline nCT scores. Note that in Figure</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,87.67,450.78,443.34,176.83"><head></head><label></label><figDesc>Figure 2. Comparing negative feedback approaches, sorted in descending order by baseline nCT score.</figDesc><table coords="4,87.67,450.78,443.34,176.83"><row><cell>Normalized CT Score</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">13 dd17-29 dd17-40 dd17-42 dd17-15 dd17-04 dd17-35 dd17-10 dd17-59 dd17-57 dd17-41 dd17-33 dd17-06 dd17-28 dd17-18 dd17-21 dd17-11 dd17-22 dd17-24 dd17-46 dd17-20 dd17-08 dd17-39 dd17-58 dd17-43 dd17-47 dd17-45 dd17-25 dd17-44 dd17-14 dd17-31 dd17-01 dd17-48 dd17-26 dd17-60 dd17-09 dd17-54 dd17-17 dd17-19 dd17-52 dd17-37 dd17-50 dd17-53 dd17-27 dd17-30 dd17-12 dd17-36 dd17-05 dd17-23 dd17-49 dd17-38 dd17-32 dd17-02 dd17-34 dd17-07 dd17-56 dd17-55 dd17-51 dd17-03 dd17-16</cell></row><row><cell></cell><cell cols="2">TREC-DD Topic Number</cell><cell></cell></row><row><cell>T</cell><cell>T+N</cell><cell>fT+N</cell><cell>wT+N</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,77.49,686.73,446.14,8.94;1,72.00,698.85,445.89,8.94;1,72.00,711.09,37.51,8.94"><p>Here we use "run" in the usual TREC sense to mean the complete set of results on which an aggregate score is computed. Each run in our submissions contained five iterations, containing no more than five documents per iteration.</p></note>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1.6 1.8 dd17-13 dd17-29 dd17-40 dd17-42 dd17-15 dd17-04 dd17-35 dd17-10 dd17-59 dd17-57 dd17-41 dd17-33 dd17-06 dd17-28 dd17-18 dd17-21 dd17-11 dd17-22 dd17-24 dd17-46 dd17-20 dd17-08 dd17-39 dd17-58 dd17-43 dd17-47 dd17-45 dd17-25 dd17-44 dd17-14 dd17-31 dd17-01 dd17-48 dd17-26 dd17-60 dd17-09 dd17-54 dd17-17 dd17-19 dd17-52 dd17-37 dd17-50 dd17-53 dd17-27 dd17-30 dd17-12 dd17-36 dd17-05 dd17-23 dd17-49 dd17-38 dd17-32 dd17-02 dd17-34 dd17-07 dd17-56 dd17-55 dd17-51 dd17-03 dd17-16</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="6,72.00,578.52,424.33,10.77" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="6,235.64,578.52,212.75,10.77">TREC 2016 Dynamic Domain Track Overview</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In TREC</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
