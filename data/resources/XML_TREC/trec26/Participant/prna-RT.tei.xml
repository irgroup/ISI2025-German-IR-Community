<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,78.24,81.09,455.52,14.69;1,248.09,99.02,115.83,14.69">Recognizing Tweet Relevance with Profile-specific and Profile-independent Supervised Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,184.39,138.47,50.88,12.24"><forename type="first">Kathy</forename><surname>Lee</surname></persName>
							<email>kathy.lee1@philips.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Philips Research North America</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,243.65,138.47,71.88,12.24"><forename type="first">Ashequl</forename><surname>Qadir</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Philips Research North America</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,323.93,138.47,51.32,12.24"><forename type="first">Yuan</forename><surname>Ling</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Philips Research North America</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,383.62,138.47,41.75,12.24"><forename type="first">Joey</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Philips Research North America</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,143.76,152.42,76.46,12.24"><forename type="first">Sadid</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Philips Research North America</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,229.13,152.42,57.93,12.24"><forename type="first">Vivek</forename><surname>Datla</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Philips Research North America</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,295.19,152.42,83.32,12.24"><forename type="first">Aaditya</forename><surname>Prakash</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Philips Research North America</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,387.19,152.42,81.06,12.24"><forename type="first">Oladimeji</forename><surname>Farri</surname></persName>
							<email>dimeji.farri@philips.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Philips Research North America</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,78.24,81.09,455.52,14.69;1,248.09,99.02,115.83,14.69">Recognizing Tweet Relevance with Profile-specific and Profile-independent Supervised Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">213B59C5C6E3FE358EA8A209D690CBA2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the 2017 TREC (Text Retrieval Conference) Real-Time Summarization (RTS) track, we explored supervised methods for identifying relevant tweets based on a user's interest profile. We primarily focused on two approaches: profile-specific and profile-independent. For profile-specific, we trained a model for each interest profile with features specific to the target profile. In case of profileindependent, a single model was trained with features that were general across all profiles. For training the supervised models, we used labeled data from the previous year's challenge. We additionally introduced a novel method for automatically labeling tweets with relevance scores. The method treated keywords from titles as an essential information and penalized the relevance score for a tweet when the keywords were absent; while treating keywords from description as supporting information, and rewarding the relevance score when these keywords were present. In scenario A (real-time push notification), our best run yielded 9.95% EG-p and 11.11% nDCG-p improvements over the median in batch evaluation. In scenario B (daily digest), our best run achieved 25.43% nDCGp improvement over the median.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For TREC Real-Time Summarization (RTS) track,<ref type="foot" coords="1,294.32,663.11,3.99,7.57" target="#foot_0">1</ref> the challenge of training supervised models arises due to 1) a large collection of interest profiles, 2) diversity of topics, and 3) limitations for generating labeled data for new interest profiles. For a given set of interest profiles, human experts can manually label some training data and train one supervised model per profile. This solution is difficult to scale if new interest profiles are introduced since a trained model would only be tied to a specific interest profile that the framework has been exposed to. An alternative approach can be to create a single profileindependent supervised model to determine profile relevance of a tweet, which would then be applicable for any interest profile. In this case, feature modeling needs to be carefully designed so that the extracted features are not tied to any specific profile.</p><p>For the TREC-2017 RTS challenge, we explored the use of both profile-specific and profileindependent supervised models for recognizing tweet relevance. We used the labeled data from the past year's challenge <ref type="bibr" coords="1,408.55,481.96,75.49,10.36" target="#b4">(Lin et al., 2016)</ref> as the training data to build a single profile-independent supervised regression model. In this method, we created training instances by pairing up tweets with interest profiles and the trained regression model predicts a real-valued relevance score. As features, different overlap statistics between a tweet and a profile were used (e.g., number of overlapping words, phrases, parts-of-speech tokens, etc.). We additionally explored an attention-based deep learning model that learns semantic characteristics of the words in a profile that may be important for recognizing relevance. For making push decisions we used a threshold relevance score.</p><p>We also introduced a novel method for automatically labeling new data to train a dedicated supervised model for each profile <ref type="bibr" coords="1,446.93,699.15,71.47,10.36">(profile-specific)</ref>. In this method, we collected tweets using keywords ex-tracted from the profiles, and assigned a relevance score based on a method that penalized for any missing title keywords, but gave a reward for additional keywords from profile descriptions.</p><p>We submitted a total of six automatic runs, three for real-time push notification (scenario A) and three for daily digest (scenario B). For scenario A, our three runs corresponded to 1) our best method in scenario A from the 2016 challenge, 2) a profileindependent supervised regression model trained on labeled data from the past challenges which exploits features based on different overlap statistics between a tweet and a profile, and 3) a combination of ( <ref type="formula" coords="2,122.35,252.60,4.24,10.36">1</ref>) and (2). For scenario B, our three runs corresponded to 1) our best method in scenario B from the RTS 2016 challenge, 2) a profiledependent supervised regression model trained on data that were automatically labeled with our novel method of reward/penalty-based relevance scoring, and 3) a profile-independent attention-based Convolutional Neural Network model that automatically determines which profile words may need more focus when searching for relevant tweets, trained on the data from the past challenges. In scenario A, our best run yielded 9.95% EG-p (+0.0104) and 11.11% nDCG-p (+.0185) improvements over the median in batch evaluation. In scenario B, our best run achieved 25.43% nDCGp (+0.0558) improvement over the median.</p><p>2 System Description for Scenario A 2.1 Real-Time Push Notification for Run 1</p><p>For our run 1 in scenario A, we used our best method from the 2016 TREC RTS challenge (scenario A) <ref type="bibr" coords="2,72.00,551.46,78.11,10.36" target="#b3">(Lee et al., 2016)</ref>. This method uses a set of assorted textual features extracted from the interest profiles and determines relevance of a tweet based on a weighted relevance score. The seven categories of textual features that we used are:</p><p>• Title words: we extract all unigrams (individual words) from the profile title after excluding stopwords and punctuations.</p><p>• Title phrases: we extract all noun phrases and verb phrases that only appear in the title of an interest profile.</p><p>• Noun phrases: we identify all noun phrases from the title, description and narrative fields of the interest profiles.</p><p>• Phrases within quotations: we extract phrases from title, description, and narrative that appear within quotation marks. Intuitively, phrases within quotation carry special importance, and tweets that mention these phrases exactly, could be highly relevant to the profiles.</p><p>• Named Entity Phrases: we extract phrases that contain a named entity. For extracting named entities, we use the NLTK toolkit.</p><p>• Location Named Entity Phrases: We extract all named entity phrases that mention locations.</p><p>• TF-IDF phrases from narrative: We calculate TF-IDF scores for words in profile narratives, considering each narrative of an interest profile as a document. We take the top 10 words with the highest TF-IDF scores (excluding stopwords), and extract noun phrases and verb phrases that contain one of these high scoring TF-IDF words.</p><p>The textual feature categories are further expanded to include paraphrases of the extracted phrases so that phrases that are synonymous can also contribute towards measuring relevance. We use the PPDB Paraphrase Database <ref type="bibr" coords="2,453.28,497.67,86.72,10.36;2,313.20,511.22,25.45,10.36" target="#b1">(Ganitkevitch et al., 2013)</ref> (L-size) for the paraphrase-based feature expansion. We do not expand the named entity phrases and phrases within quotations. For the other textual features, we create four new categories with only the paraphrase terms. After feature expansion, we have a total of 11 categories of textual features.</p><p>To identify relevant messages from the Twitter feed, messages are first filtered based on language, minimal number of title words, presence of named entity or quoted phrase, etc. Finally, relevance with respect to feature categories and profiles are determined using the equations:</p><formula xml:id="formula_0" coords="2,340.81,695.28,199.19,32.70">relevance(x, C i ) = X c2Ci l 2 c n c ⇥ max n (C i ) (1) prof ile relevance(x) = C X i=1 w i ⇥ relevance(x, C i ) (2)</formula><p>Here, x is an input tweet, C i is the set of textual features for the i th textual feature category, l c is the maximum number of rightmost words from phrase c that appears in the tweet consecutively and in the same order, n c is the total number of words in c, and max n (C i ) is the maximum phrase length (in terms of words) among all of the phrases in C i . Weights w i are feature category weights estimated from the TREC RTS 2015 data, maximizing the Expected Gain (EG). The tweet that is assigned a high relevance score is then checked for novelty using a semantic similarity model <ref type="bibr" coords="3,190.76,289.71,88.67,10.36" target="#b2">(Hasan et al., 2015)</ref> and pushed to user if above a threshold of 0.75. More details on the method can be found in <ref type="bibr" coords="3,227.15,316.81,71.65,10.36" target="#b3">Lee et al. (2016)</ref> 2.2 Real-Time Push Notification for Run 2</p><p>For our run 2 in scenario A, we design a profileindependent supervised regression model. One of the important considerations in scenario A is when a tweet can be pushed to a user. So instead of a categorical relevant vs. not relevant decision, we use a regression model that allows us to directly predict a real-valued relevance score. As our regression algorithm, we use L2-regularized L2-loss support vector regression from the LIBLINEAR library <ref type="bibr" coords="3,252.08,468.23,46.72,10.36;3,72.00,481.78,23.48,10.36" target="#b0">(Fan et al., 2008)</ref>. To create training instances, we use tweets from the 2016 TREC RTS challenge where each tweet is assigned a relevance score using batch evaluation. As our training instances, we pair a tweet with its respective interest profile and use the relevance score as the data label.</p><p>For this profile-independent regression model, the textual features described in Section 2.1 could not be directly used because they are lexical features closely tied to the respective interest profiles. For example, in profile RTS2, a user is interested in information related to Zica virus in Ecuador. One of our assorted feature types would identify Ecuador as a key named entity feature (among others) for finding tweets relevant to RTS2. But such lexical features are ideally useful for training a model specifically for RTS2, which limits the ability to train a new model for a profile if labeled training data is not readily available.</p><p>To create a profile-independent model, we instead determine how much these features overlap between an interest profile and a tweet in their respective feature categories, and derive statistics with respect to the categories instead of the features themselves so that they can be generalized across all profiles. For the above example, instead of relying on specific features such as "Ecuador", our new general features would now try to assess the importance of having an overlap between a tweet and a profile for different feature categories (e.g. named entity) to determine the tweet-topic relevance. To achieve this, for each feature type described in Section 2.1, we create five binary overlap features to use in our regression model. These features are:</p><p>• Full overlap: If all of the phrases from a feature type are present in a target tweet, we set this feature to 1 (or 0 otherwise).</p><p>• Much overlap: If more than half of the phrases from the feature type are present in a target tweet but not all, we set this feature to 1 (or 0 otherwise). We require that a minimum of three phrases are in the respective feature type.</p><p>• Moderate overlap: If half of the phrases from a feature type are present in a target tweet, we set this feature to 1 (or 0 otherwise).</p><p>• Some overlap: If less than half of the phrases from the feature type are present in a target tweet but more than one, we set this feature to 1 (or 0 otherwise). We require that a minimum of three phrases are in the respective feature type.</p><p>• Bare-minimal overlap: If one of the phrases from a feature type are present in a target tweet, we set this feature to 1 (or 0 otherwise). We require that a minimum of three phrases are in the respective feature type.</p><p>In addition to the feature types described in Section 2.1, we additionally use person, organization, nouns, verbs, adjectives and unigrams, bigrams, and trigrams when deriving the overlap features. From the profile, we only use title. We also experimented with descriptions and narratives but found the title part of the profile to be more effective with this modeling.</p><p>Once the support vector regression model is trained on the paired tweets and profiles, as we monitor the twitter stream, for a candidate tweet, we pair the tweet with each interest profile and apply the trained model. If the predicted relevance score for a profile-tweet pair is above a threshold value 0.5, we push the tweet to the respective user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Real-Time Push Notification for Run 3</head><p>For run 3, we combined outputs of our run 1 and run 2 models.</p><p>3 System Description for Scenario B</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Daily Digest for Run 1</head><p>For run 1 in scenario B, we use our last year's best run for scenario B (Run1 from 2016), the details of which can be found in <ref type="bibr" coords="4,171.68,318.43,70.28,10.36" target="#b3">Lee et al. (2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Daily Digest for Run 2</head><p>For the run 2 in scenario B, we build profiledependent supervised models trained for each profile separately. The major obstacle for building such models is the availability of training data with respect to all interest profiles. Past challenge data are insufficient for this purpose since they do not have relevance judgment for the new profiles. To this end, we devise a method to automatically generate labeled training data allowing us to train supervised models for a given profile.</p><p>The main assumption of our automatic data labeling method is that the information present in the title of an interest profile is the most important part of the profile, while the information from the description provide additional supporting information. Under this assumption, we design a reward-penalty driven method for assigning a relevance score to a tweet for a given profile. First, we create an initial set of candidate relevant tweets (streamed from Twitter during the week before the challenge) for a given profile where each tweet contains at least one title word (limited to nouns, verbs and adjectives) from the profile. We assign a zero score to each tweet in the candidate tweets set as the initial relevance score.</p><p>We then make an assumption that if all words from this target profile's title are present in a tweet, then the tweet is a relevant tweet for that profile. For these tweets, we do not update the relevance score. But for the tweets that do not have all of the words from the profile's title, we want to penalize the assigned relevance score to reflect that the tweet may be missing some key information for establishing relevance. As the penalty, we remove 1 point from the relevance score for each missing title word. For example, if a profile title is "Zica virus in Ecuador", and the only common words in the tweet are "Zica" and "virus", but "Ecuador" is missing, then the score is updated to -1 from 0 to reflect the missing title word "Ecuador" (we only judge overlap or missing words with respect to nouns, verbs and adjectives).</p><p>Finally, whenever a tweet has additional words from the description of an interest profile, we want to reward these cases. This is because, although the title generally specifies the main topic of an interest profile, a user may be interested in more specific information on the topic which can be reflected in the description. For example, for the profile title "heating pad recommendations", it becomes clear that the user is interested in knowing about which heating pads other consumers prefer and suggest. However, the description "What heating pads are recommended for treating low back pain?" further reveals that the user is mainly interested in heating pads in the context of back pain. Although an experience with a general heating pad may also apply in this case, when a tweet mentions a heating pad recommendation in the context of back pain, the tweet should be judged as more relevant.</p><p>To achieve this, when all of the title words are already present in a tweet, but the tweet also has additional words from the description of the interest profile, for each additional word from the description, we reward 1 point to the relevance score. For the above example profile, if a tweet mentions all of the title words "heating", "pad", "recommendation", and has the additional words from description such as "back" and "pain", the assigned relevance score is updated from 0 to +2 to reflect the presence of the supporting context "back" and "pain".</p><p>Once a set of tweets are automatically labeled with a relevance score using the method above, we train a L2-regularized L2-loss support vector regression from the LIBLINEAR library <ref type="bibr" coords="4,463.90,685.55,71.41,10.36" target="#b0">(Fan et al., 2008)</ref>. As features for regression, we use bag-of-words.</p><p>To create a daily digest for a given interest pro-file, we first use an initial filtering of all the streamed tweets to create a set of candidate tweets to rank. As the filtering method, we check for the following two conditions: 1) total number of unique title and description keyword overlaps are more than total number of unique title keywords from the profile, and 2) total number of unique title keywords overlaps are more than a half of total number of unique title keywords from the profile. After creating the candidate tweets set, the trained model is run on the new tweets to predict a relevance score. The tweets are sorted based on the predicted score and the top 100 ranked tweets above a threshold of -1.5 are selected to put in the batch/digest to send to the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Daily Digest for Run 3</head><p>For run 3, we employed an Attention-Based Convolutional Neural Network (ABCNN) model <ref type="bibr" coords="5,279.40,306.32,19.40,10.36;5,72.00,319.87,58.13,10.36" target="#b5">(Yin et al., 2015)</ref> which was designed to model sentence pairs by taking into account the interdependence between the two sentences, and has shown to achieve state-of-the-art performance in tasks such as paraphrase identification, answer selection and textual entailment. This model is profile-independent since it does not require profile-specific labeled data for training and can predict profile-tweet relevance score for previously unseen profiles. Evaluation data from past two years (RTS2016, Microblog2015) was used to train the model by setting a tweet and a user profile (topic title plus description) as a sentence pair. We used ABCNN-3 model with 2 convolution layers which computes attention weights on both the input representation and the output of convolution. For hyperparameters, we used 0.08 as the learning rate, 0.0004 for L2 regularization, batch size of 64, 20 epochs, and support vector machine (SVM) as classifier. If the final relevance score by SVM is above a threshold value, the model classifies the tweet-profile pair as relevant (i.e., the tweet is relevant to the user profile).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Post Hoc Batch Evaluation in Scenario A</head><p>For scenario A, tweets pushed are evaluated with the following evaluation metrics: 2 2 http://trecrts.github.io/ TREC2017-RTS-guidelines.html</p><p>• Expected gain (EG) (for an interest profile on a particular day) is defined as follows:</p><formula xml:id="formula_1" coords="5,395.24,116.42,144.76,28.06">EG = 1 N X G(t)<label>(3)</label></formula><p>where N is the number of tweets submitted by a system and G(t) is the gain of each tweet.</p><p>• Normalized Cumulative Gain (nCG) (for an interest profile on a particular day) is defined as follows:</p><formula xml:id="formula_2" coords="5,392.88,252.45,147.12,28.06">nCG = 1 Z X G(t)<label>(4)</label></formula><p>where Z is the maximum possible gain (given the ten tweets per day limit).</p><p>In determining gain G(t), not relevant, relevant, and highly-relevant tweets receive a gain of 0, 0.5, and 1.0, respectively. EG and nCG metrics have two variations each. For the EG-1 and nCG-1 variation of the metrics, on a silent day when there are no relevant tweets for a particular interest profile, a system receives a score of 1 (i.e., perfect score) if it does not push any tweet. That is, if the system pushes 0 tweets, it receives a score of 1. However, under the EG-p and nCG-p metrics, there is a penalty proportional to how "quiet" the system is. The score is one minus the fraction of the ten-tweet daily quota that is used. If it pushes 1 tweet, it gets a score of 0.9, if it pushes 2 tweets, it gets a score of 0.8, and so on, such that if a system uses up its quota of ten tweets for a silent day, it receives a score of zero. EG-p is the primary metric for judging systems in scenario A.</p><p>• Gain Minus Pain (GMP), defined as follows:</p><formula xml:id="formula_3" coords="5,366.60,629.03,173.40,17.98">GM P = ↵ ⇥ G (1 ↵) ⇥ P<label>(5)</label></formula><p>Here G (gain) is computed in the same manner as above; and P (pain) is the number of non-relevant tweets that are pushed, and controls the balance between the two. Evaluations are done at three ↵ settings: 0.33, 0.5, and 0.66. The EG, nCG and Table 1: Scenario A batch evaluation results. EG = Expected Gain (1 = with silent day reward, p = proportional silent day reward), nCG1 = Normalized Cumulative Gain (1 = with silent day reward, p = proportional reward on silent day), GMP = Gain Minus Pain (at ↵ = 0.33, 0.5 and 0.66). GMP metrics are used in a post hoc batch evaluation where relevant tweets are semantically clustered into groups containing tweets that share substantively similar information and judged as notrelevant, relevant, or highly relevant by the pools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation</head><p>Table <ref type="table" coords="6,110.90,304.06,5.45,10.36">1</ref> presents the results of our three runs in scenario A and compares them with the median results among the participating teams. For both variations of the EG and nCG metrics, our run 3, which combines our last year's best run with this year's profile-independent supervised support vector regression model, performed the best among the three runs. Run 3 also outperformed the median results on these metrics. On the official EG-p metric, our run 3 improved the results by +0.0104 (9.95% of the median). Individually, run 1 and 2 could only perform better than the median results for the nCG-1 metric. However, on the GMP.5 metric, run 1 still achieved the best result among our three runs, which is +0.0927 higher than the median result (54.31% of the median).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Live User-in-the-loop Assessments for</head><p>Scenario A</p><p>In a live user-in-the-loop assessments, tweets submitted by the participating systems are immediately routed to the mobile phone of a human assessor to be judged as relevant, not-relevant or redundant. Precision is calculated as the ratio of the relevant tweets to the judged tweets. In strict precision, relevant but redundant tweets are excluded. In lenient precision, relevant but redundant tweets are still included.</p><p>Table <ref type="table" coords="6,110.48,672.05,5.45,10.36" target="#tab_1">2</ref> presents the evaluation by mobile assessors for scenario A. Run 1 had the best results among the three runs. Our run 1 and run 3 both performed better than the median results where run 1 achieved +0.0737 and +0.0609 better strict and lenient precision respectively, compared to the median results (21.66% and 14.59% of the median). For scenario A, a total of 41 runs were submitted from 15 participating teams. Table <ref type="table" coords="6,465.34,534.28,5.45,10.36">3</ref> shows the top 5 runs and strict precision based on evaluation by the mobile assessors. Our best model (PRNA Run 1) ranked third among the 41 runs. Table <ref type="table" coords="6,351.65,699.15,5.45,10.36" target="#tab_2">4</ref> shows the time latency of the runs. The average latency is very high but median latency is low. The main reason for the high average latency is the semantic similarity checking method we used to remove duplicate contents, which increases overall latency as the runs progress and the queue of the streamed tweets increases, resulting in very high latency in the later days of the challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Post Hoc Batch Evaluation in Scenario B</head><p>For scenario B runs, Normalized Discounted Cumulative Gain nDCG@10 is used as the evaluation metrics with two variants as the scenario A. nDCG@10-1 rewards a system for not pushing tweets on a silent day when there are no relevant tweets, and nDCG@10-p proportionally rewards depending on how quite a system is on a silent day.  <ref type="table" coords="7,112.00,439.21,5.45,10.36" target="#tab_3">5</ref> compares the results of our runs with median results. Among our three runs, run 2, the profile-dependent support vector regression model trained with automatically labeled data, achieved the best results. This run yielded +0.0558 nDCG-p and +0.0535 nDCG-1 results over the median (25.43% and 28.69% of the median).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation of Scenario B Runs as Scenario</head><p>A Runs For scenario B, a total of 40 runs were submitted from 15 participating teams. Table <ref type="table" coords="7,224.14,712.70,5.45,10.36" target="#tab_4">6</ref> shows the top 5 scenario B runs and strict precision when evaluated as scenario A runs by the mobile assessors. Our best run (PRNA Run 1) ranked first among the 40 runs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we explored the use of both profilespecific and profile-independent supervised models for recognizing tweet relevance. In scenario A, our run 3, which combines our last year's best run with a new supervised regression model, yielded +0.0104 EG-p and +.0185 nCG-p improvements over the median. In scenario B, our run 2, a profile-dependent support vector regression-based model trained with automatically labeled training data, obtained nDCGp of 0.2752, achieving +0.0558 nDCG-p improvement over the median. In future work, we will explore the use of automatic data labeling methods to label training data in large quality for training deep learning-based models to retrieve relevant tweets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,313.20,292.99,226.80,211.52"><head>Table 2 :</head><label>2</label><figDesc>Evaluation by mobile assessors in scenario A.</figDesc><table coords="6,313.20,304.88,226.80,199.63"><row><cell></cell><cell>Strict</cell><cell>Strict</cell><cell>Lenient</cell><cell>Lenient</cell></row><row><cell></cell><cell>Precision</cell><cell>Utility</cell><cell>Precision</cell><cell>Utility</cell></row><row><cell>Median</cell><cell>0.3403</cell><cell>-805</cell><cell>0.4174</cell><cell>-456</cell></row><row><cell></cell><cell cols="2">PRNA Systems</cell><cell></cell><cell></cell></row><row><cell>PRNA Run 1</cell><cell>0.4140</cell><cell>-262</cell><cell>0.4783</cell><cell>-66</cell></row><row><cell>PRNA Run 2</cell><cell>0.3346</cell><cell>-678</cell><cell>0.3912</cell><cell>-446</cell></row><row><cell>PRNA Run 3</cell><cell>0.3625</cell><cell>-852</cell><cell>0.4264</cell><cell>-456</cell></row><row><cell cols="5">Table 3: Top 5 scenario A runs evaluated by mobile as-</cell></row><row><cell>sessors.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Team (Run)</cell><cell cols="2">Strict Precision</cell><cell></cell></row><row><cell cols="3">WUWien (Run 1)</cell><cell>0.4337</cell><cell></cell></row><row><cell cols="2">IRIT (Run 1)</cell><cell></cell><cell>0.4200</cell><cell></cell></row><row><cell cols="2">PRNA (Run 1)</cell><cell></cell><cell>0.4140</cell><cell></cell></row><row><cell cols="3">udel fang (Run 1)</cell><cell>0.4096</cell><cell></cell></row><row><cell cols="3">udel fang (Run 2)</cell><cell>0.3980</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,342.92,605.39,167.35,77.76"><head>Table 4 :</head><label>4</label><figDesc>Time Latency in Scenario A.</figDesc><table coords="6,342.92,617.79,167.35,65.36"><row><cell>latency (seconds)</cell><cell>mean</cell><cell>median</cell></row><row><cell cols="2">PRNA Systems</cell><cell></cell></row><row><cell>PRNA Run 1</cell><cell>50612.7</cell><cell>69.0</cell></row><row><cell>PRNA Run 2</cell><cell>29994.0</cell><cell>78.0</cell></row><row><cell>PRNA Run 3</cell><cell>39366.4</cell><cell>74.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,72.00,294.19,226.80,155.38"><head>Table 5 :</head><label>5</label><figDesc>Scenario B batch evaluation results. nDCG = Normalized Discounted Cumulative Gain (1 = with silent day reward, p = proportional reward on silent day).</figDesc><table coords="7,82.91,330.51,177.29,119.07"><row><cell>Evaluation Metrics</cell><cell>nDCGp nDCG1</cell></row><row><cell>Median</cell><cell>0.2194 0.1865</cell></row><row><cell cols="2">PRNA Systems</cell></row><row><cell cols="2">PRNA Run 1 0.2071 0.1914</cell></row><row><cell cols="2">PRNA Run 2 0.2752 0.2400</cell></row><row><cell cols="2">PRNA Run 3 0.2143 0.1686</cell></row><row><cell>Table</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,72.00,587.79,226.80,93.51"><head>Table 6 :</head><label>6</label><figDesc>Top 5 scenario B runs evaluated as scenario A runs by the mobile assessors.</figDesc><table coords="7,111.48,611.67,147.83,69.64"><row><cell>Team (Run)</cell><cell>Strict Precision</cell></row><row><cell>PRNA (Run 1)</cell><cell>0.4811</cell></row><row><cell>IRIT (Run 1)</cell><cell>0.4666</cell></row><row><cell>PKUICST (Run 3)</cell><cell>0.4625</cell></row><row><cell>IRIT (Run 3)</cell><cell>0.4560</cell></row><row><cell>IRIT (Run 2)</cell><cell>0.4497</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,313.20,133.60,226.80,160.69"><head>Table 7 :</head><label>7</label><figDesc>Top 5 scenario B runs evaluated as scenario A runs by NIST assessors.Table 7 presents the top 5 scenario B runs and EG-p when evaluated as scenario A runs by NIST assessors. Our best run (PRNA Run 2) ranked third among the 40 runs.</figDesc><table coords="7,369.16,157.48,114.89,69.64"><row><cell>Team (Run)</cell><cell>EG-p</cell></row><row><cell cols="2">PKUICST (Run 1) 0.2959</cell></row><row><cell cols="2">adv lirmm (Run 1) 0.2676</cell></row><row><cell>PRNA (Run 2)</cell><cell>0.2674</cell></row><row><cell cols="2">adv lirmm (Run 2) 0.2641</cell></row><row><cell cols="2">adv lirmm (Run 3) 0.2620</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,88.14,713.54,134.50,8.58"><p>http://trecrts.github.io/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,313.20,557.29,226.80,9.46;7,324.11,569.25,215.89,9.46;7,324.11,581.20,215.89,9.46;7,324.11,593.16,177.99,9.46" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,487.24,569.25,52.76,9.46;7,324.11,581.20,145.03,9.46">Liblinear: A library for large linear classification</title>
		<author>
			<persName coords=""><forename type="first">Rong-En</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiang-Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,479.24,584.62,60.76,5.94;7,324.11,596.57,92.59,5.94">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008-08">2008. Aug</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,313.20,605.43,226.80,9.46;7,324.11,617.38,63.37,9.46;7,403.11,617.38,22.42,9.46;7,441.16,617.38,98.84,9.46;7,324.11,629.34,215.89,9.46;7,324.11,641.29,215.89,9.46;7,324.11,653.25,79.15,9.46" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,441.16,617.38,98.84,9.46;7,324.11,629.34,32.70,9.46">PPDB: The paraphrase database</title>
		<author>
			<persName coords=""><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,375.65,632.75,111.15,5.94">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-06">2013. June</date>
			<biblScope unit="page" from="758" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,313.20,665.51,226.80,9.46;7,324.11,677.47,215.89,9.46;7,324.11,689.43,215.89,9.46;7,324.11,704.80,215.89,5.94;7,324.11,716.75,139.91,5.94" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,507.34,665.51,32.66,9.46;7,324.11,677.47,215.89,9.46;7,324.11,689.43,14.39,9.46">Exploiting Neural Embeddings for Social Media Data Analysis</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,362.08,692.84,177.92,5.94;7,324.11,704.80,72.38,5.94">Proceedings of the Twenty-Fourth Text REtrieval Conference</title>
		<meeting>the Twenty-Fourth Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-11-17">2015. 2015. November 17-20, 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,76.29,226.80,9.46;8,82.91,88.25,215.89,9.46;8,82.91,100.20,215.89,9.46;8,82.91,112.16,215.89,9.46;8,82.91,127.53,215.89,5.94;8,82.91,139.49,215.89,5.94;8,82.91,151.44,93.98,5.94" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,141.60,100.20,157.20,9.46;8,82.91,112.16,193.22,9.46">Assorted textual features and dynamic push strategies for real-time tweet notification</title>
		<author>
			<persName coords=""><forename type="first">Kathy</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ashequl</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vivek</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sadid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joey</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaditya</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oladimeji</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Farri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,82.91,127.53,215.89,5.94;8,82.91,139.49,27.42,5.94">Proceedings of The Twenty-Fifth Text REtrieval Conference</title>
		<meeting>The Twenty-Fifth Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-15">2016. 2016. November 15-18, 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,160.73,226.80,9.46;8,82.91,172.68,215.89,9.46;8,82.91,184.64,215.89,9.46;8,82.91,196.59,165.57,9.46" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="8,82.91,184.64,215.89,9.46;8,82.91,196.59,18.67,9.46">Overview of the TREC 2016 real-time summarization track</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roegiest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luchen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Mc-Creadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>DTIC Document</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="8,72.00,209.30,226.80,9.46;8,82.91,221.25,215.89,9.46;8,82.91,233.21,215.89,9.46;8,82.91,245.16,65.86,9.46" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="8,139.91,221.25,158.89,9.46;8,82.91,233.21,178.19,9.46">ABCNN: attention-based convolutional neural network for modeling sentence pairs</title>
		<author>
			<persName coords=""><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno>CoRR, abs/1512.05193</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
