<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,160.32,100.55,274.54,12.22">ICTNET at TREC 2017 Common Core Track</title>
				<funder ref="#_a63y7Nd">
					<orgName type="full">Taishan Scholars Program of Shandong Province， China</orgName>
				</funder>
				<funder ref="#_mYhdUM8 #_XEXjQEz">
					<orgName type="full">NSF Foundation of China</orgName>
				</funder>
				<funder ref="#_jBYvx3B">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
				<funder>
					<orgName type="full">NIST</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,126.96,141.49,34.10,8.85"><forename type="first">Xu</forename><surname>Chang</surname></persName>
							<email>changxu@software.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Key Laboratory of Web Data Science and Technology</orgName>
								<address>
									<country>CAS</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,171.13,141.49,35.33,8.85"><forename type="first">Liying</forename><surname>Jiao</surname></persName>
							<email>jiaoliying@software.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Key Laboratory of Web Data Science and Technology</orgName>
								<address>
									<country>CAS</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,216.24,141.49,35.81,8.85"><forename type="first">Jinlong</forename><surname>Liu</surname></persName>
							<email>liujinlong@software.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Key Laboratory of Web Data Science and Technology</orgName>
								<address>
									<country>CAS</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,261.84,141.49,39.65,8.85"><forename type="first">Weijian</forename><surname>Zhu</surname></persName>
							<email>zhuweijian@software.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Key Laboratory of Web Data Science and Technology</orgName>
								<address>
									<country>CAS</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.77,141.49,41.95,8.85"><forename type="first">Yuanhai</forename><surname>Xue</surname></persName>
							<email>xueyuanhai@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Key Laboratory of Web Data Science and Technology</orgName>
								<address>
									<country>CAS</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,362.22,141.49,20.28,8.85"><forename type="first">Li</forename><surname>Zha</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Key Laboratory of Web Data Science and Technology</orgName>
								<address>
									<country>CAS</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,389.53,141.49,25.71,8.85"><forename type="first">Yue</forename><surname>Liu</surname></persName>
							<email>liuyue@ict.ac.cn</email>
						</author>
						<author>
							<persName coords="1,423.84,141.49,43.23,8.85"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Key Laboratory of Web Data Science and Technology</orgName>
								<address>
									<country>CAS</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Institute of Network Technology，ICT(YANTAI)，CAS</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,160.32,100.55,274.54,12.22">ICTNET at TREC 2017 Common Core Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4F2B8533D3A773A99CA93C0C34675416</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The common core track is a new track for TREC 2017. The track will serve as a common task for a wide spectrum of IR researchers, thus attracting a diverse run set that can be used to investigate new methodologies for test collection construction. This track provides the structured NYTimes corpus. Its primary goal is to get the relative documents from the corpus with the given topics, and there are 1855660 news across from 1987 to 2007 on NYTimes in this common core track. Two sets of topics are used for searching relative News, one of which is to be judged by NIST and crowd workers and the other is to be judged by crowd workers only. Participants need choose the first or the both topics according to the requirements. There are three common text information retrieval model: Vector Space Model, Probabilistic Model and Inference Network Model. BM25 is a probabilistic function to rank the list of matched documents according to a given query <ref type="bibr" coords="1,284.40,423.23,7.67,5.69">[4]</ref> . Solr uses the BM25 rank function when doing the search work. It is an open source enterprise search platform and can do the full-text search work. Solr runs in the servlet container. User can get the results from the web interface. In our work, we choose Solr as the tool for indexing documents and searching topics.</p><p>We have an exploration on the topics, which are distributed by the NIST. Most of the topics contain less than three words and some include even one word. Therefore, we need to expand the query words to query more information. We select the apache Solr <ref type="bibr" coords="1,310.08,516.83,7.67,5.69">[5]</ref> as our retrieve frame in order to improve the effectiveness on the automatic query expansion. Recently, neural network is widely used in NLP. We use the word-embedding model for automatic query expansion and the TextRank algorithm with the description of the topic to extract the keywords as the expansion words. Next, we build index for the corpus and get the query results with Solr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">QUERY PREPROCESS</head><p>In this section, we mainly preprocess the query words. First, we train a model to convert words into vectors by the approach in Section 2.1. Second, we extend the query words following an incremental procedure based on word embedding in Section 2.2. Finally, using the TextRank algorithm in Section 2.3, we obtain the keywords from the description of the topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">CBOW [6]</head><p>Word2vec is a kind of algorithm used to generate the distributed description of words. In our experiment, we use the CBOW(Continuous Bag of Words Model)model to produce embedded word. We will introduce the CBOW model in detail.</p><p>In the CBOW architecture, the model predicts the current word from a window of surrounding context words. The order of context words does not influence prediction (bag-of-words assumption) <ref type="bibr" coords="2,464.40,122.27,7.44,5.69">[7]</ref> . For the sentence, "the cat jumps over the lazy dog", we can set {"the", "cat", "jumps", "the", "lazy", "dog"} as the context to predict the current word "over". The model can catch multiple different degrees of similarity between words such as semantic and syntactic similarity.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Query expansion based on word embedding</head><p>In this section, we implement two approaches to expand the query words according to the paper roy2016using <ref type="bibr" coords="2,145.20,538.19,7.67,5.69">[9]</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Pre-retrieval kNN based approach</head><p>We assume that the given query word set Q is {q1…qm}. We define the set of candidate expansion</p><formula xml:id="formula_0" coords="2,90.00,612.37,415.19,25.50">words is C as = ⋃ ( ) ∈<label>(1)</label></formula><p>In Equation (1), NN (q) are the nearest k neighbors of q in extended query word set. We compute the mean cosine similarity between each candidate word and all the query words in C as shown in the follow formula.</p><p>(</p><formula xml:id="formula_1" coords="2,269.04,687.44,75.45,28.43">, ) = 1 | | • ∈</formula><p>The candidate word set is in descending order of the value of ( , ). The top K words are chosen as the final expansion words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Pre-retrieval incremental kNN based approach</head><p>The second method is a simple extension of the Pre-retrieval kNN based approach. Compared with the method in Section 2.2.1, it follows an incremental procedure rather than the procedure that the words are selected according to the similarity to each query word in a single step. The second method is based on the assumption that the most similar words have lower drift than the words occurring later in the list.</p><p>Because the most similar words are the most appropriate candidate expansion words, we assume that the expansion words should be similar to each other. According to the assumption above, we use an iterative process to prune words of NN (q).</p><p>First, we arrange the nearest neighbors of in descending order of similarity to . The candidate word list is , , … , . Then, we prune the K least similar neighbors to obtain word list , , … , . Next, we add the first word to the expansion word set and reorder the list , … , in descending order of similarity to . We repeat the process, prune the K least similar neighbors in the new list to obtain list ′ , … , ′ and add the first word ′ to the expansion word set, for times. At each step, the nearest neighbors list is reordered and pruned based on the list obtained in the previous step. Finally, we get the candidate expansion word set of as ( ). In this set, the words are similar to each other and the query word . With regards to the parameter , high value may cause query drift while low value will perform similarly to the first method. In our model, we choose = 5. Finally, we construct the expansion words set as in Section 2.2.1, except that use ( ) in place of ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">TextRank</head><p>TextRank <ref type="bibr" coords="3,128.88,426.35,10.79,5.69">[10]</ref> algorithm is based on PageRank <ref type="bibr" coords="3,272.40,426.35,10.56,5.69">[11]</ref> . It is usually used to generate keywords and summaries from the text. We use TextRank with the description of the topics to generate keywords for extending the query words.</p><p>First, we split the text into words. Every word is considered as a node in the network. We set parameter k as the window size. For example, in the sentence, "the cat jumps over the lazy dog", we could set k to four. Then we obtain the windows, which consist of "the cat jumps over", "cat jumps over the", "jumps over the lazy" and "over the lazy dog" .if two words are included in one window, there will be an unbounded edge to connect them. At last, we call the PageRank algorithm to generate keywords from text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EVALUATION</head><p>In our work, we make six submissions about 50-topic set judged by NIST assessors. In the submission of ICT17ZCJL02, query word are manually expanded, while in other submissions the expansion is automatic  <ref type="table" coords="4,136.32,76.69,5.04,8.85" target="#tab_0">1</ref> shows the results of different submitted runs and the average of best and median score of this common core track. We find that weight of query word causes many retrieve differences according to ICT17ZCJL06 and ICT17ZCJL07. Additionally, there is a decrease on the ICT17ZCJL03 and ICT17ZCJL05 compared with ICT17ZCJL01, which shows the phenomenon called "query drift" as we add new query words. This problem causes the semantics of new query terms drift from the originals, but this effect is weakened when we add to the keywords of description in ICT17ZCJL06. We raise the weight of the original query words on ICT17ZCJL07. This method improves the hit rates on top 10 performance and keep the overall performance stable at the mean time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION&amp; Acknowledgements</head><p>In this paper, we study how to extend the query word automatically and compare the effects of several approaches. The evaluation shows that query expansion using the word-embedding model and TextRank to extract the key words from the description below the topic perform badly because of the query drift. We solve the problem of query drift by trimming the weight of query word and the expansion words. Word-embedding based on the given NYTimes can enhance the Top@10 hit rates, but it costs much more time for build the word embedding model and its scalability is poor when adding new document.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,111.12,186.08,166.63,8.96;2,282.48,184.56,14.20,6.19;2,297.12,186.08,45.91,8.96;2,342.96,184.56,14.09,6.19;2,367.44,186.13,127.27,8.85;2,90.00,201.44,415.35,8.96;2,90.00,217.09,415.29,8.85;2,90.00,232.64,13.30,8.96;2,122.40,232.64,2.07,8.96;2,154.08,232.64,15.03,8.96;2,189.84,232.64,7.56,8.96;2,197.52,231.47,7.67,5.69;2,205.20,232.69,87.35,8.85;2,312.72,232.64,2.07,8.96;2,344.40,232.64,14.79,8.96;2,380.88,232.69,124.44,8.85;2,90.00,248.29,28.88,8.85;2,144.96,248.24,2.07,8.96;2,176.88,248.24,14.79,8.96;2,219.36,248.29,147.78,8.85;2,386.88,248.29,118.47,8.85;2,90.00,263.89,34.05,8.85;2,139.68,263.89,62.35,8.85;2,219.12,263.84,286.28,8.96;2,90.00,279.49,415.26,8.85;2,90.00,295.28,191.35,8.96;2,286.08,293.76,13.96,6.19;2,300.72,295.28,43.03,8.96;2,343.68,293.76,14.33,6.19;2,363.12,295.33,80.18,8.85;2,459.60,295.33,46.08,8.85;2,90.00,310.69,70.21,8.85"><head></head><label></label><figDesc>There are two parameter matrices ∈ ℝ ×| | and ∈ ℝ | |× in CBOW model. In addition, is the size of embedding space, | | is the vocabulary size of training set. The specific calculation process is shown in Figure-1. First, it generates the one-hot word vectors for the input context of size k: ( , , … , ). [8] Second, we multiply , , … , with the matrix , obtain the vectors , , … , , and calculate the average vector of these vectors. Third, we multiply with the matrix to produce score vector z. Finally, we convert the score vector z into probabilities by using softmax function. The Loss function of model is cross entropy, using the gradient descent to update the parameter matrices ∈ ℝ ×| | and ∈ ℝ | |× . The rows of matrix are actually our word vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,289.92,466.93,36.48,8.85"><head>Figure- 1</head><label>1</label><figDesc>Figure-1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,90.00,665.16,382.49,97.67"><head>Table 1 . Performance of different Run Tag, compared to the best and the median results of the track Table</head><label>1</label><figDesc></figDesc><table coords="3,95.76,665.16,376.73,86.39"><row><cell>Run Tag</cell><cell>MAP</cell><cell>NDCG</cell><cell>P@10T</cell><cell>Description</cell></row><row><cell>ICT17ZCJL01</cell><cell>0.1837</cell><cell>0.4117</cell><cell>0.4508</cell><cell>Title only Solr Retrieve Framework</cell></row><row><cell>ICT17ZCJL03</cell><cell>0.1513</cell><cell>0.3785</cell><cell>0.4080</cell><cell>Query expansion with google news corpus</cell></row><row><cell>ICT17ZCJL05</cell><cell>0.1434</cell><cell>0.3607</cell><cell>0.3794</cell><cell>Query expansion with NYTimes corpus</cell></row><row><cell>ICT17ZCJL06</cell><cell>0.1620</cell><cell>0.3821</cell><cell>0.4284</cell><cell>Use the description below the topic</cell></row><row><cell>ICT17ZCJL07</cell><cell>0.1816</cell><cell>0.4103</cell><cell>0.4672</cell><cell>Trim the weight for the query term</cell></row><row><cell>Best</cell><cell>0.5377</cell><cell>0.7699</cell><cell>0.9160</cell><cell>Best trec results</cell></row><row><cell>Median</cell><cell>0.2280</cell><cell>0.6787</cell><cell>0.5480</cell><cell>Median trec results</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>We would like to thank all organizers and assessors of <rs type="institution">TREC</rs> and <rs type="funder">NIST</rs>. This work is sponsored by <rs type="funder">Taishan Scholars Program of Shandong Province， China</rs> (No.<rs type="grantNumber">ts201511082</rs>) , This work is also supported by <rs type="funder">National Key Research and Development Program of China</rs> under grant <rs type="grantNumber">2016YFB1000902</rs>, and <rs type="funder">NSF Foundation of China</rs> under grants <rs type="grantNumber">61572473</rs> and <rs type="grantNumber">61772501</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_a63y7Nd">
					<idno type="grant-number">ts201511082</idno>
				</org>
				<org type="funding" xml:id="_jBYvx3B">
					<idno type="grant-number">2016YFB1000902</idno>
				</org>
				<org type="funding" xml:id="_mYhdUM8">
					<idno type="grant-number">61572473</idno>
				</org>
				<org type="funding" xml:id="_XEXjQEz">
					<idno type="grant-number">61772501</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="4,111.12,518.76,394.19,6.95;4,111.12,534.36,140.45,6.95" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,200.37,518.76,192.10,6.95">The probabilistic relevance framework: BM25 and beyond</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,409.32,518.76,95.99,6.95;4,111.12,534.36,68.44,6.95">Foundations and Trends® in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="333" to="389" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,111.12,565.56,394.36,6.95;4,111.12,581.16,73.24,6.95" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,310.05,565.56,192.06,6.95">Efficient estimation of word representations in vector space</title>
		<author>
			<persName coords=""><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,111.12,581.16,49.35,6.95">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,111.12,627.96,394.38,6.95;4,111.12,643.56,18.29,6.95" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="4,208.38,627.96,174.12,6.95">Using word embeddings for automatic query expansion</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.07608</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="4,111.12,659.16,320.92,6.95" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,206.64,659.16,116.92,6.95">TextRank: Bringing Order into Texts</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,337.42,659.16,90.19,6.95">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,111.12,674.76,394.36,6.95;4,111.12,690.36,32.70,6.95" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="4,139.18,674.76,190.93,6.95">The PageRank citation ranking: Bringing order to the web</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
	<note type="report_type">Stanford Digital Libraries Working Paper</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
