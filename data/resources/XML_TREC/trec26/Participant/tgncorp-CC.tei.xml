<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,206.23,76.64,199.51,12.54;1,256.43,93.22,99.15,10.81">Tarragon Consulting at TREC 2017: Common Core Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,271.08,112.72,69.76,8.75"><forename type="first">Richard</forename><forename type="middle">M</forename><surname>Tong</surname></persName>
							<email>rtong@tgncorp.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Tarragon Consulting Corporation Berkeley</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,206.23,76.64,199.51,12.54;1,256.43,93.22,99.15,10.81">Tarragon Consulting at TREC 2017: Common Core Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6CA89A6A1C48F2AD6AF8EB13F28F6159</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Tarragon Consulting Corporation (henceforth Tarragon) contributed two runs to the new Common Core track. Both were manual runs using the NIST judged topics. Both used Solr as the base search engine with the queries semi-automatically constructed from the Topic descriptions and augmented with information from Wordnet and Wikipedia.</p><p>Results are generally below the published median scores but for several topics our results are very competitive. And we did contribute a significant number of "unique relevant" documents to the judged pool.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Our primary reason for participating in TREC this year was to respond to the track organizers desire to "â€¦ bring together the community in a common track that could lead to a diverse set of participating runs".</p><p>We have a long-standing interest in exploring whether query-augmented commercial search engines can be used effectively on TREC-style ad hoc retrieval tasks, so our TREC 2017 submissions are in the spirit of those previous efforts.</p><p>Our experience is that such approaches can achieve reasonable precision and often retrieve significant numbers of unique documents, although they are not usually competitive, overall, with more traditional IR methods. Given the goal of the track, this seemed like an opportunity to reengage with TREC and contribute a potentially distinctive pair of runs.</p><p>The rest of paper is organized conventionally. We briefly describe the approach we took, highlight a few of the results, and then offer general observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Basic Approach</head><p>Solr is a widely deployed open-source platform and is the search engine we use internally, so was the natural choice for this effort. It has an expressive query language that supports various forms of proximity and wildcarding, and the engine itself is very fast so enabling time-effective experimentation. We used version 6.4.1 of Solr for TREC 2017. <ref type="bibr" coords="1,373.92,300.96,3.59,5.76">1</ref> The track data were indexed into four fields: DocId, DocTitle, DocDate, and DocBody. Text in the DocTitle and DocBody fields was lower-cased but not stemmed. Stopwords were not removed.</p><p>The topics were processed using the Stanford CoreNLP toolkit to extract named entities and part-of-speech information. <ref type="bibr" coords="1,441.84,395.04,3.59,5.76">2</ref> We then used a three-step process to create our two submissions-we call these the FLOOR, BASE and BOOST steps.</p><p>In the FLOOR step we automatically created simple disjunctive queries from the original topics and ran these to get a preliminary pool of 10,000 documents per topic.</p><p>In the BASE step we manually created more focused queries that capture the necessary elements of the topic. The results of running these queries were merged with the corresponding FLOOR results. The BASE results were ranked ahead of the FLOOR results and the merged set trimmed to 10,000 documents. These are the tgncorpBASE submissions.</p><p>In the BOOST step the BASE queries were further manually augmented to include auxiliary information from the topic. The results of running these queries were merged with the tgncorpBASE submissions. The BOOST results were ranked ahead of the tgncorpBASE results and the merged 1 https://lucene.apache.org/solr/ 2 https://stanfordnlp.github.io/CoreNLP/ set trimmed to 10,000 documents. These are the tgncorpBOOST submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Analysis</head><p>The following table shows the overall results. The scores for the two runs are the averages, and these are contrasted with the average medians for all the manual submissions on the NIST judged topics. The BOOST vs. BASE strategy was generally successful, producing decent improvements on all three metrics. These are still well below the medians, however.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Nevertheless, there were a few Topics on which the approach performed very well. For example, Topic 416 (Three Gorges Project). The BASE Solr query is:</p><p>{!complexphrase inOrder=false} ( DocBody:("Three Gorges") || DocTitle:("Three Gorges")^2)</p><p>This is a disjunction of the phrase "Three Gorges" in the body of the document, and the same phrase in the title, but with a title getting more weight.</p><p>The BOOST query then adds a conjunctive clause that looks for project related words in the document body. Thus:</p><p>{!complexphrase inOrder=false} ( DocBody:("Three Gorges") || DocTitle:("Three Gorges")^2) &amp;&amp; DocBody:(complet* cost* output electric*)</p><p>The following table shows our performance on Topic 416 relative to the scores for all the manual submissions on this topic. Given our approach, certain topics are not easily converted into augmented queries. Topics that require a determination of counts of things (e.g., Topic 626), or the identification of named entities (e.g., Topic 389), or are very broad (e.g., Topic 344), or underspecified (e.g., Topic 442), are inherently hard to express.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Our a priori assessment was that 20 of the 50 topics were "hard". Partitioning the results by hard vs. non-hard produces the average scores shown in the following table for the tgncorpBOOST submission. These are compared with corresponding average median scores for all manual submissions similarly partitioned. It is not an especially pronounced effect, but nonhard queries did perform better than the hard queries-both in absolute terms and with respect to their corresponding average medians.</p><p>Since our main goal was to provide potentially distinct runs, more interesting for us was to look at the "unique relevant" documents we returned.</p><p>We found at least one unique document in 34 of the 50 topics, and found 203 overall.</p><p>The following table shows our top ten topics based on the proportion of the total number of unique documents found by at least one of our submissions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comments</head><p>Overall results are consistent with our expectations. Our focus on structured queries inevitably emphasizes precision over recall, but at the same time can find documents that more conventional techniques do not.</p><p>Given our goals, we regard this as a successful participation in TREC 2017. And it was, we hope, a positive contribution to the community effort.</p></div>		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
