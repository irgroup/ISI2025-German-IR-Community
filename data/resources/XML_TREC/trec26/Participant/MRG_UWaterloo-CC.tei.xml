<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,56.69,110.81,475.05,15.18;1,56.69,132.73,310.17,15.18">MRG_UWaterloo and WaterlooCormack Participation in the TREC 2017 Common Core Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,56.69,167.64,114.12,12.61"><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,200.60,167.64,113.22,12.61"><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,56.69,217.20,156.00,10.62"><forename type="first">Mrg_Uwaterloo</forename><surname>Overview</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,56.69,110.81,475.05,15.18;1,56.69,132.73,310.17,15.18">MRG_UWaterloo and WaterlooCormack Participation in the TREC 2017 Common Core Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">ED8D95D6A8CBB13501E75E07CFDF6189</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The MRG_UWaterloo group from the University of Waterloo used a Continuous Active Learning ("CAL") approach <ref type="bibr" coords="1,56.69,252.41,10.51,8.80" target="#b0">[1]</ref> to identify and manually review a substantial fraction of the relevant documents for each of the 250 Common Core topics. Our primary goal was to create, with less effort, a set of relevance assessments ("qrels") comparable to the official Common Core Track qrels (cf. [12, 2]). To this end, we adapted for live, human-in-the-loop use, the AutoTAR CAL implementation, 1 which had demonstrated superior effectiveness as a baseline for the TREC 2015 and 2016 Total Recall Tracks [9, 5]. In total, for 250 topics, the authors spent 64.1 hours assessing 42,587 documents (on average, 15.4 mins/topic; 5.4 secs/doc), judging 30,124 of them to be relevant (70.7%).</p><p>While the principal outcome of the MRG_UWaterloo effort was a set of relevant documents for each topic, it was necessary to submit ranked lists of 10,000 documents for each topic, to be evaluated using the standard rankbased measures calculated by "trec_eval." 2 In theory, according to the probability ranking principle, the optimal strategy to maximize these measures is to construct a ranked list of the 10,000 most-likely relevant documents, with the documents ordered by their likelihood of relevance. In practice, the official qrels used for TREC evaluation are influenced by the submitted runs, confounding the theoretical optimal strategy. Participants were asked to prioritize their runs, and each participant was assured only that the ten highest-ranked documents from their highest-priority submission would be assessed for relevance, and included in the qrels. An unspecified number of additional highlyranked documents were also to be included, depending on the results of assessing the higher-ranked documents, relative to the results of other participants' runs (cf. [6]).</p><p>Overall, the qrels for each topic represent a non-statistical sample of the document population, biased heavily toward documents that one or more runs deemed to have a high likelihood of relevance. To estimate the precision of our alternate qrels according to the TREC assessors, we applied a random permutation to the documents we assessed as relevant. These documents, in the order determined by the random permutation, were afforded the highest ranks in our highest-priority run ("MRGrandrel"), thus assuring that a random sample (i.e., the first ten) would be assessed by TREC. The remaining documents were scored using the final AutoTAR model, and ranked from highest to lowest score.</p><p>Our secondary and tertiary runs ("MRGrankel" and "MRGrankall") were ordered slightly differently. The ranked lists in MRGrankrel consisted of all documents that we assessed as relevant, ordered by score; followed by the top-scoring documents that we did not assess or assessed as non-relevant, ordered by score. The ranked lists in MRGrankall consisted of the top-scoring documents, ordered by score, regardless of whether or not we had assessed them as relevant.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WaterlooCormack Overview</head><p>The WaterlooCormack submission consisted of "automatic routing" runs, as defined in TRECs 1 through 8 <ref type="bibr" coords="1,542.03,625.97,9.96,8.80" target="#b6">[7]</ref>. Document rankings were derived using logistic regression on prior relevance assessments for the same topics (but with respect to different corpora), without manual intervention. Feature engineering, learning software, and parameter settings were identical to those used in the TREC 2015 and 2016 Total Recall Tracks <ref type="bibr" coords="1,432.93,661.84,10.51,8.80" target="#b8">[9,</ref><ref type="bibr" coords="1,446.83,661.84,7.01,8.80" target="#b4">5]</ref>, and identical to those used by the MRG_UWaterloo group.</p><p>Our highest-priority submission, "WCrobust04," used the TREC 2004 Robust test collection <ref type="bibr" coords="1,485.94,685.75,14.61,8.80" target="#b12">[13]</ref>, which used the same 250 topics (with slightly revised narratives), for training. We formed the union of the TREC 2004 and Common Core 2017 corpora, from which tf-idf word-based features were derived. Sofia-ML<ref type="foot" coords="2,453.17,356.58,3.97,6.16" target="#foot_2">3</ref> was used to construct a logistic regression model from the TREC 2004 qrels, and the model was used to score the documents in the Common Core corpus. For each topic, the 10,000 highest-scoring documents were submitted, in decreasing order by score.</p><p>Our second-priority submission, "WCrobust0405," used the same TREC 2004 Robust Track assessments for training, augmented by assessments from the TREC 2005 Robust Track <ref type="bibr" coords="2,374.46,417.92,14.61,8.80" target="#b13">[14]</ref>, which used 50 of the 250 topics, and yet another corpus. For these 50 topics, we formed the union of the three corpora from TREC 2004, TREC 2005, and Common Core 2017. We trained the model using the TREC 2004 and TREC 2005 assessments, and used the model to score the documents in the Common Core corpus. For these 50 topics, the WCrobust0405 submission consisted of the 10,000 highest-scoring documents, in decreasing order by score. For the remaining 200 Common Core topics that were not used in the TREC 2005 Robust track, the WCrobust0405 submission was identical to WCrobust04.</p><p>Our third-priority submission, "WCrobust04W," used the TREC 2004 Robust Track assessments for training, augmented by alternate assessments created for the TREC 6 Ad Hoc task <ref type="bibr" coords="2,377.49,513.56,14.61,8.80" target="#b10">[11]</ref>, by the University of Waterloo, using Interactive Search and Judging (ISJ) <ref type="bibr" coords="2,220.87,525.51,9.96,8.80" target="#b1">[2]</ref>. TREC 6 used a subset of 50 of the 250 Common Core topics (a different subset from the 2005 Robust Track). For each of the 50 topics, we formed the union of the official TREC qrels and the Waterloo qrels; in the union, we labeled a document "relevant" if it was labeled relevant in either the TREC or Waterloo qrels, and otherwise "non-relevant." For these 50 topics, the WCrobust04W submission consisted of the 10,000 highest-scoring documents, in decreasing order by score. For the remaining 200 Common Core topics that were not used in the TREC 6 Ad Hoc task, the WCrobust04W submission was identical to WCrobust04.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ranked-Retrieval Measures Using NIST Assessments</head><p>At the time of this writing, official relevance assessments were available only for 50 of the 250 Common Core topics (i.e., the NIST subset). A dynamic "bandit" strategy <ref type="bibr" coords="2,296.59,648.02,10.51,8.80" target="#b5">[6]</ref> was employed by NIST to select documents for manual assessment by contract reviewers, starting with the ten highest-ranked documents from the highest-priority runs identified by Track participants. A total of 30,030 documents were assessed, of which 3,453 were judged "highly relevant," 5,549 were judged "relevant," and 21,028 were judged "not relevant." In the following analysis, we consider binary relevance in which the 9,002 documents judged "highly relevant" or "relevant" are considered relevant, and documents judged "not relevant," as well as unjudged documents, are considered non-relevant.</p><p>In accordance with the methodology employed by the Common Core Track organizers, we consider only the highest-ranked 1,000 documents for each topic. Table <ref type="table" coords="3,99.17,187.44,4.98,8.80">1</ref> reports the average over the 50 NIST-assessed topics of average precision (MAP), precision at rank 10 (P@10), and number of relevant documents retrieved returned at rank 1,000 (Relret@1000). Table <ref type="table" coords="3,495.42,199.40,4.98,8.80">1</ref> also reports the same measures, averaged over the 33 NIST topics that were also used in the TREC 2005 Robust Track. The WaterlooCormack automatic routing runs scored higher than the MRG_UWaterloo manual ad hoc runs according to the measures reported here, and substantially all the measures reported by trec_eval. Of the WaterlooCormack runs, WCrobust0405 achieved the highest score according to all measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ranked-Retrieval Measures Using Waterloo Relevance Assessments</head><p>As noted in the overview above, the primary goal of the MRG_UWaterloo team was to assess substantially all of the relevant documents for each topic. To this end, the team assessed a total of 40,400 documents for relevance to the 250 Common Core topics, marking 30,122 relevant. Of the 40,400 assessments, 11,825 pertained to the 50 NIST topics, of which 8,986 were judged relevant by NIST. 9,285 of the 11,825 assessments also pertained to the Robust 2005 topics, of which 7,247 were judged relevant by NIST.</p><p>Table <ref type="table" coords="3,100.17,357.77,4.98,8.80">2</ref> shows the same measures as Table <ref type="table" coords="3,269.57,357.77,3.87,8.80">1</ref>, using the MRG_UWaterloo relevance assessments. The first three rows are included for completeness, but should be discounted, as they amount to a circular assessment of the MRG_UWaterloo runs. The last three rows, on the other hand, represent the WaterlooCormack runs, which were not influenced by and did not influence the MRG_UWaterloo assessments. Although the numerical results are substantially lower, the MRG_UWaterloo-assessed scores yield the same relative result as the NIST scores: WCrobust0405 is superior by all measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Set-Based Measures</head><p>The aim of the MRG_UWaterloo effort, as well as the aim of the NIST assessment effort, was to identify, as effectively as possible, a complete set of documents relevant to each topic. If ground-truth relevance with respect to every topic and every document were known, we could evaluate the effectiveness of these two efforts using standard set-based measures, the most prominent of which are recall and precision. In general, however, ground truth can never be known with certainty, because the determination of relevance is based on human judgment, which is not entirely reliable. Even if the relevance determination of a competent assessor were deemed to be "reliable enough," it is feasible to render such a determination for only a small subset of the documents in the collection.</p><p>For the 50 NIST topics, the 11,825 Waterloo assessments and the 30,030 NIST assessments represent 0.01% and 0.03%, respectively, of the 92.7M assessments that would be required to render a human determination of relevance for each document in the test collection with respect to each topic. Of the 8,986 documents that the Waterloo assessors deemed relevant and the 9,002 documents that the NIST assessors deemed relevant, only 3,715 were deemed relevant by both efforts. In other words, the overlap (i.e., Jaccard index) between the Waterloo and NIST assessments was 0.26.</p><p>If we deem the NIST assessments to be ground truth, Waterloo achieved (micro-averaged) recall of 0.4127 (3,715/9,002), and precision of 0.4134 <ref type="bibr" coords="3,221.82,635.69,56.51,8.80">(3,175/8,986)</ref>. Conversely, if we deem the Waterloo assessments to be ground truth, NIST achieved (micro-averaged) recall of 0.4134 and precision of 0.4127. In either case, F 1 = 0.4131.</p><p>Macro-averaged recall, precision, and F 1 of Waterloo according to NIST (i.e., precision, recall, and F 1 of NIST according to Waterloo) are 0.4193, 0.5091, and 0.3952, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Incomplete Assessments Versus Assessor Disagreement</head><p>Table <ref type="table" coords="3,84.31,722.33,4.98,8.80">3</ref> gives a 3 × 3 confusion matrix showing the agreement between the Waterloo and NIST assessments. It is important to note that the document selection and assessment methods are not independent. For the Waterloo assessments, the user's relevance determinations were used to train a learning method that chose further documents to present to the user. The selection of documents for assessment by NIST was, at first, determined by the submitted runs (including those derived from the Waterloo assessments), and subsequently, by the relevance determinations of the NIST assessors. While it would be misleading to draw statistical inferences from the raw numbers, it is apparent that there are several sources of discord between the Waterloo and NIST assessment efforts.</p><p>The columns headed Rel and N onRel give statistical estimates of the number of documents that Waterloo judged relevant that would have been judged relevant or non-relevant by the NIST assessor, had they all been assessed. The estimate is based on the uniform sample of ten documents placed at the top of the MRGrandrel run, which was fully assessed. The difference between the Rel and Rel is substantial, and significant (P ≈ 0.000).</p><p>Table <ref type="table" coords="4,99.33,185.35,4.98,8.80">4</ref> shows the Rel and Rel statistics for every topic, ordered by their difference. As can be seen from the first several rows of the table, most of the total difference between Rel and Rel is accounted for by a few topics for which the number of judged documents is substantially smaller than the number of documents deemed relevant by Waterloo. Large differences are generally associated with small P-values, indicating that the large negative differences at the top of the table are unlikely attributable to chance, and reflect incompleteness in the NIST assessments.</p><p>With two notable exceptions -Topics 307 and 427 at the bottom of the table -we find that most or all of the documents we deemed relevant were judged by NIST, and that Rel is consistent with Rel. For both of these exceptional topics, Rel is substantially and significantly greater than Rel. This result is sufficiently improbable (P ≈ 0.002 in both cases) that there must be some correlation between the sample and the NIST assessments. The sampling method (i.e., drawing the first ten elements returned by the Linux utility "sort -R") is uniform and (pseudo) random, and therefore unlikely to be a causal factor.</p><p>We do know that documents in the sample, being at the top of the rankings from our highest-priority run, were assessed at the outset by the NIST assessors. Some of the other documents might also have been judged at the outset, while others were probably judged later. That is, documents in the sample were more likely to be judged at the outset than the remaining documents. For Topics 307 and 427, our results suggest that the assessors were less likely to judge the documents relevant at the outset. A possible explanation for this is that the pool of initially judged documents contained a higher proportion of relevant documents, resulting in stricter relevance assessments, as has been observed in prior work <ref type="bibr" coords="4,211.63,403.78,10.51,8.80" target="#b7">[8,</ref><ref type="bibr" coords="4,225.46,403.78,11.62,8.80" target="#b9">10]</ref>.</p><p>Topic 307 concerned new hydroelectric projects. Three of the ten documents in our sample were coded relevant. Of the remaining seven that were marked non-relevant, five mentioned the Three Gorges hydroelectric project, though sometimes only by the name "Three Gorges Dam." We conjecture that, at the outset, the assessor judged documents mentioning "Three Gorges" to be non-relevant, but later in the process marked similar documents relevant.</p><p>Topic 427 concerned eye damage from ultraviolet exposure. Three of the ten documents in our sample were coded relevant. Of the remaining seven, five mentioned cataracts and ozone-layer depletion. We conjecture that, at the outset, the assessor coded documents about the ozone layer and cataracts as non-relevant, but later in the process marked similar documents relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>We were surprised that the WaterlooCormack runs outperformed the MRG_UWaterloo runs according to rankedretrieval measures, as we had expected that the use of corpus-specific feedback would be more effective than transfer learning from a different corpus. In accordance with our goal of achieving high recall, we spent a disproportionate amount of time assessing documents for the larger topics, which yielded no benefit -and may even have been counterproductive in terms of maximizing trec_eval scores -given the apparent incompleteness of the NIST assessments for those topics. Also, in accordance with our goal, we aimed to construe relevance broadly, but this does not fully explain the level of disagreement between the Waterloo and NIST assessments, which bears further investigation.</p><p>The problem of "concept drift" in using dynamic document-selection methods for assessment is worthy of investigation. If the order of assessment affects those assessments, how should one try to avoid any bias that might arise? The literature suggests that bias may disfavor systems that find novel relevant documents early, or that rank relevant, but perhaps not obviously relevant, documents first <ref type="bibr" coords="4,325.40,677.20,10.51,8.80" target="#b2">[3,</ref><ref type="bibr" coords="4,339.23,677.20,7.01,8.80" target="#b3">4]</ref>.</p><p>It remains to be seen whether the disagreement between the WaterlooCormack and NIST assessments matters, as far as their use as ground truth for evaluating other IR systems. Waterloo's alternate assessments for TREC 6 [2, Fig. <ref type="figure" coords="4,92.34,713.07,3.87,8.80">1</ref>] had an overlap of 0.25 with the official NIST assessments (compared to 0.26 for the current effort), but showed comparable effectiveness for the purpose of evaluating the other TREC 6 submissions <ref type="bibr" coords="4,467.27,725.02,10.51,8.80" target="#b1">[2,</ref><ref type="bibr" coords="4,481.10,725.02,11.62,8.80" target="#b11">12]</ref>.</p><p>A third group from Waterloo (UWaterlooMDS <ref type="bibr" coords="4,277.21,736.98,14.76,8.80" target="#b14">[15]</ref>), using a similar technique to MRG_UWaterloo, derived an alternate set qrels that had considerably lower recall, and slightly higher precision, than the MRG_UWaterloo set, but scored much higher according to the official NIST ranked-retrieval measures, including Relret@1000.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,56.69,72.37,498.62,264.68"><head></head><label></label><figDesc>Ranked-retrieval measures based on official NIST assessments for 50 topics, and for the 33 NIST topics that were also used in the TREC 2005 Robust Track. Ranked-retrieval measures based on MRG_UWaterloo assessments for 50 topics, and for the 33 NIST topics that were also used in the TREC 2005 Robust Track.</figDesc><table coords="2,56.69,72.37,435.75,252.72"><row><cell>Topics:</cell><cell></cell><cell cols="2">50 NIST</cell><cell cols="3">33 NIST &amp; Robust '05</cell></row><row><cell>Measure:</cell><cell>MAP</cell><cell>P@10</cell><cell>Relret@1000</cell><cell>MAP</cell><cell>P@10</cell><cell>Relret@1000</cell></row><row><cell cols="2">MRGrandrel 0.3190</cell><cell>0.5660</cell><cell>6001</cell><cell>0.2752</cell><cell>0.5545</cell><cell>4425</cell></row><row><cell cols="2">MRGrankall 0.3538</cell><cell>0.6420</cell><cell>6010</cell><cell>0.3126</cell><cell>0.6394</cell><cell>4437</cell></row><row><cell cols="2">MRGrankrel 0.3609</cell><cell>0.6500</cell><cell>6029</cell><cell>0.3177</cell><cell>0.6455</cell><cell>4453</cell></row><row><cell cols="2">WCrobust04 0.3711</cell><cell>0.6460</cell><cell>6396</cell><cell>0.3462</cell><cell>0.6212</cell><cell>4779</cell></row><row><cell cols="3">WCrobust0405 0.4278 0.7500</cell><cell>6785</cell><cell cols="2">0.4307 0.7788</cell><cell>5161</cell></row><row><cell cols="2">WCrobust04W 0.3656</cell><cell>0.6580</cell><cell>6295</cell><cell>0.3405</cell><cell>0.6424</cell><cell>4687</cell></row><row><cell>Tab. 1: Topics:</cell><cell></cell><cell cols="2">50 NIST</cell><cell cols="3">33 NIST &amp; Robust '05</cell></row><row><cell>Measure:</cell><cell>MAP</cell><cell>P@10</cell><cell>Relret@1000</cell><cell>MAP</cell><cell>P@10</cell><cell>Relret@1000</cell></row><row><cell cols="2">MRGrandrel 0.9927</cell><cell>0.9660</cell><cell>8537</cell><cell>0.9890</cell><cell>0.9667</cell><cell>6798</cell></row><row><cell cols="2">MRGrankall 0.9118</cell><cell>0.9500</cell><cell>8418</cell><cell>0.9092</cell><cell>0.9515</cell><cell>6693</cell></row><row><cell cols="2">MRGrankrel 0.9927</cell><cell>0.9660</cell><cell>8537</cell><cell>0.9890</cell><cell>0.9667</cell><cell>6798</cell></row><row><cell cols="2">WCrobust04 0.2322</cell><cell>0.4400</cell><cell>4890</cell><cell>0.1914</cell><cell>0.3545</cell><cell>3570</cell></row><row><cell cols="3">WCrobust0405 0.2570 0.5040</cell><cell>5258</cell><cell cols="2">0.2291 0.4485</cell><cell>3934</cell></row><row><cell cols="2">WCrobust04W 0.2319</cell><cell>0.4400</cell><cell>4822</cell><cell>0.1923</cell><cell>0.3606</cell><cell>3505</cell></row><row><cell>Tab. 2:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,56.69,83.76,498.61,654.34"><head></head><label></label><figDesc>Difference between the actual number of NIST-assessed relevant documents and the estimate from the statistical sample, ordered by difference.</figDesc><table coords="5,56.69,83.76,403.81,642.38"><row><cell></cell><cell>UW Assessments</cell><cell></cell><cell cols="3">NIST Assessments</cell><cell></cell></row><row><cell>Topic</cell><cell>Rel</cell><cell cols="5">Judged Rel Rel Rel -Rel P-value</cell></row><row><cell>325</cell><cell>1281</cell><cell>441</cell><cell cols="2">298 897</cell><cell>-599</cell><cell>0.0043</cell></row><row><cell>436</cell><cell>797</cell><cell>303</cell><cell cols="2">277 797</cell><cell>-520</cell><cell>0.0000</cell></row><row><cell>354</cell><cell>901</cell><cell>401</cell><cell cols="2">297 541</cell><cell>-244</cell><cell>0.1434</cell></row><row><cell>372</cell><cell>1168</cell><cell>371</cell><cell cols="2">293 467</cell><cell>-174</cell><cell>0.4508</cell></row><row><cell>422</cell><cell>518</cell><cell>362</cell><cell cols="2">246 414</cell><cell>-168</cell><cell>0.0754</cell></row><row><cell>439</cell><cell>254</cell><cell>128</cell><cell>60</cell><cell>178</cell><cell>-118</cell><cell>0.0040</cell></row><row><cell>408</cell><cell>255</cell><cell>218</cell><cell cols="2">111 153</cell><cell>-42</cell><cell>0.4537</cell></row><row><cell>626</cell><cell>143</cell><cell>138</cell><cell>95</cell><cell>129</cell><cell>-34</cell><cell>0.1862</cell></row><row><cell>350</cell><cell>113</cell><cell>89</cell><cell>53</cell><cell>79</cell><cell>-26</cell><cell>0.2296</cell></row><row><cell>426</cell><cell>318</cell><cell>260</cell><cell cols="2">231 254</cell><cell>-23</cell><cell>0.9080</cell></row><row><cell>443</cell><cell>69</cell><cell>29</cell><cell>18</cell><cell>41</cell><cell>-23</cell><cell>0.0315</cell></row><row><cell>399</cell><cell>92</cell><cell>80</cell><cell>71</cell><cell>92</cell><cell>-21</cell><cell>0.1281</cell></row><row><cell>375</cell><cell>330</cell><cell>217</cell><cell cols="2">112 132</cell><cell>-20</cell><cell>0.9150</cell></row><row><cell>330</cell><cell>59</cell><cell>59</cell><cell>26</cell><cell>41</cell><cell>-15</cell><cell>0.1440</cell></row><row><cell>353</cell><cell>81</cell><cell>81</cell><cell>38</cell><cell>49</cell><cell>-11</cell><cell>0.5836</cell></row><row><cell>416</cell><cell>141</cell><cell>136</cell><cell>74</cell><cell>85</cell><cell>-11</cell><cell>0.8734</cell></row><row><cell>336</cell><cell>43</cell><cell>43</cell><cell>25</cell><cell>34</cell><cell>-9</cell><cell>0.2144</cell></row><row><cell>394</cell><cell>130</cell><cell>128</cell><cell>97</cell><cell>104</cell><cell>-7</cell><cell>1.0000</cell></row><row><cell>362</cell><cell>114</cell><cell>114</cell><cell>85</cell><cell>91</cell><cell>-6</cell><cell>1.0000</cell></row><row><cell>400</cell><cell>31</cell><cell>31</cell><cell>20</cell><cell>25</cell><cell>-5</cell><cell>0.4041</cell></row><row><cell>419</cell><cell>29</cell><cell>29</cell><cell>12</cell><cell>17</cell><cell>-5</cell><cell>0.2805</cell></row><row><cell>423</cell><cell>131</cell><cell>130</cell><cell cols="2">126 131</cell><cell>-5</cell><cell>1.0000</cell></row><row><cell>321</cell><cell>67</cell><cell>67</cell><cell>56</cell><cell>60</cell><cell>-4</cell><cell>0.9592</cell></row><row><cell>367</cell><cell>85</cell><cell>85</cell><cell>31</cell><cell>34</cell><cell>-3</cell><cell>1.0000</cell></row><row><cell>389</cell><cell>36</cell><cell>36</cell><cell>29</cell><cell>32</cell><cell>-3</cell><cell>0.7092</cell></row><row><cell>435</cell><cell>51</cell><cell>39</cell><cell>2</cell><cell>5</cell><cell>-3</cell><cell>0.7137</cell></row><row><cell>442</cell><cell>83</cell><cell>83</cell><cell>55</cell><cell>58</cell><cell>-3</cell><cell>1.0000</cell></row><row><cell>620</cell><cell>133</cell><cell>129</cell><cell>50</cell><cell>53</cell><cell>-3</cell><cell>1.0000</cell></row><row><cell>646</cell><cell>76</cell><cell>76</cell><cell>59</cell><cell>61</cell><cell>-2</cell><cell>1.0000</cell></row><row><cell>677</cell><cell>27</cell><cell>27</cell><cell>14</cell><cell>16</cell><cell>-2</cell><cell>0.8037</cell></row><row><cell>347</cell><cell>16</cell><cell>16</cell><cell>4</cell><cell>5</cell><cell>-1</cell><cell>1.0000</cell></row><row><cell>355</cell><cell>56</cell><cell>54</cell><cell>10</cell><cell>11</cell><cell>-1</cell><cell>1.0000</cell></row><row><cell>404</cell><cell>22</cell><cell>21</cell><cell>3</cell><cell>4</cell><cell>-1</cell><cell>0.8571</cell></row><row><cell>614</cell><cell>47</cell><cell>47</cell><cell>46</cell><cell>47</cell><cell>-1</cell><cell>1.0000</cell></row><row><cell>341</cell><cell>54</cell><cell>54</cell><cell>16</cell><cell>16</cell><cell>-0</cell><cell>1.0000</cell></row><row><cell>344</cell><cell>5</cell><cell>5</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>1.0000</cell></row><row><cell>356</cell><cell>4</cell><cell>4</cell><cell>2</cell><cell>2</cell><cell>0</cell><cell>1.0000</cell></row><row><cell>393</cell><cell>131</cell><cell>131</cell><cell>79</cell><cell>79</cell><cell>0</cell><cell>1.0000</cell></row><row><cell>433</cell><cell>4</cell><cell>4</cell><cell>3</cell><cell>3</cell><cell>0</cell><cell>1.0000</cell></row><row><cell>445</cell><cell>160</cell><cell>110</cell><cell>32</cell><cell>32</cell><cell>0</cell><cell>1.0000</cell></row><row><cell>414</cell><cell>41</cell><cell>41</cell><cell>23</cell><cell>20</cell><cell>2</cell><cell>0.9299</cell></row><row><cell>345</cell><cell>38</cell><cell>38</cell><cell>30</cell><cell>27</cell><cell>3</cell><cell>0.6953</cell></row><row><cell>397</cell><cell>118</cell><cell>118</cell><cell>62</cell><cell>59</cell><cell>3</cell><cell>1.0000</cell></row><row><cell>378</cell><cell>103</cell><cell>90</cell><cell>66</cell><cell>62</cell><cell>4</cell><cell>1.0000</cell></row><row><cell>690</cell><cell>36</cell><cell>36</cell><cell>14</cell><cell>7</cell><cell>7</cell><cell>0.2888</cell></row><row><cell>310</cell><cell>164</cell><cell>153</cell><cell>60</cell><cell>49</cell><cell>11</cell><cell>0.9372</cell></row><row><cell>379</cell><cell>73</cell><cell>73</cell><cell>21</cell><cell>7</cell><cell>14</cell><cell>0.2996</cell></row><row><cell>363</cell><cell>77</cell><cell>77</cell><cell>62</cell><cell>46</cell><cell>16</cell><cell>0.1940</cell></row><row><cell>427</cell><cell>104</cell><cell>104</cell><cell>80</cell><cell>31</cell><cell>49</cell><cell>0.0024</cell></row><row><cell>307</cell><cell>177</cell><cell>175</cell><cell>140</cell><cell>53</cell><cell>87</cell><cell>0.0016</cell></row><row><cell>Tab. 4:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,71.93,717.18,157.96,7.04"><p>See http://cormack.uwaterloo.ca/trecvm/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,71.93,726.67,133.31,7.04"><p>See http://trec.nist.gov/trec_eval/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,71.93,738.67,158.98,7.04"><p>See https://github.com/glycerine/sofia-ml.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,77.17,136.70,478.14,8.80;6,77.17,148.65,226.67,8.80" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="6,291.72,136.70,263.59,8.80;6,77.17,148.65,112.84,8.80">Autonomy and reliability of continuous active learning for technology-assisted review</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Grossman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06868</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,77.17,168.58,478.14,8.80;6,77.17,180.53,119.45,8.80" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,404.14,168.58,151.17,8.80;6,77.17,180.53,43.67,8.80">Efficient construction of large test collections</title>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">R</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,141.68,180.53,28.25,8.80">SIGIR</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,77.17,200.46,478.14,8.80;6,77.17,212.41,478.14,8.80;6,77.17,224.37,256.56,8.80" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,410.27,200.46,145.04,8.80;6,77.17,212.41,162.79,8.80">The effect of document order and topic difficulty on assessor agreement</title>
		<author>
			<persName coords=""><forename type="first">Falk</forename><surname>Tadele T Damessie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kalvero</forename><surname>Scholer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J Shane</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Culpepper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,264.19,212.41,291.12,8.80;6,77.17,224.37,138.65,8.80">Proceedings of the 2016 ACM on International Conference on the Theory of Information Retrieval</title>
		<meeting>the 2016 ACM on International Conference on the Theory of Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="73" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,77.17,244.29,478.14,8.80;6,77.17,256.25,478.14,8.80;6,77.17,268.20,22.69,8.80" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,240.25,244.29,315.06,8.80;6,77.17,256.25,169.95,8.80">Order effects: a study of the possible influence of presentation order on user judgements of document relevance</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Eisenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carol</forename><surname>Barry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,256.38,256.25,247.84,8.80">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">293</biblScope>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,77.17,288.13,478.14,8.80;6,77.17,300.08,54.97,8.80" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,356.80,288.13,179.32,8.80">TREC 2016 Total Recall Track Overview</title>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Maura R Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Roegiest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,77.17,300.08,28.28,8.80">TREC</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,77.17,320.01,478.14,8.80;6,77.17,331.96,478.14,8.80;6,77.17,343.92,183.80,8.80" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,333.28,320.01,222.03,8.80;6,77.17,331.96,175.26,8.80">Feeling lucky?: Multi-armed bandits for ordering judgements in pooling-based evaluation</title>
		<author>
			<persName coords=""><forename type="first">Javier</forename><surname>David E Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Parapar</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Barreiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,280.06,331.96,275.24,8.80;6,77.17,343.92,45.10,8.80">Proceedings of the 31st Annual ACM Symposium on Applied Computing</title>
		<meeting>the 31st Annual ACM Symposium on Applied Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1027" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,77.17,363.84,478.14,8.80;6,77.17,375.80,478.14,8.80;6,77.17,387.75,144.12,8.80" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,251.09,363.84,91.71,8.80">Routing and filtering</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,112.94,375.80,322.12,8.80">TREC -Experiment and Evaluation in Information Retrieval, chapter 5</title>
		<editor>
			<persName><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Donna</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, Massachusetts</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="99" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,77.17,407.68,478.14,8.80;6,77.17,419.63,106.67,8.80" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,262.56,407.68,292.76,8.80;6,77.17,419.63,30.99,8.80">Impact of review-set selection on human assessment for text classification</title>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roegiest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,128.90,419.63,28.25,8.80">SIGIR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,77.17,439.56,478.13,8.80;6,77.17,451.51,143.27,8.80" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="6,446.81,439.56,108.49,8.80;6,77.17,451.51,66.60,8.80">TREC 2015 Total Recall Track Overview</title>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roegiest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles L A</forename><surname>Clarke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,77.17,471.44,478.14,8.80;6,77.17,483.40,478.14,8.80;6,77.17,495.35,165.19,8.80" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,301.69,471.44,233.04,8.80">Human performance and retrieval precision revisited</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chandra</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jethani</forename><surname>Prakash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,77.17,483.40,478.14,8.80;6,77.17,495.35,37.32,8.80">Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="595" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,77.17,515.28,478.14,8.80;6,77.17,527.23,207.79,8.80" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,238.24,515.28,259.40,8.80">Overview of the Sixth Text REtrieval Conference (TREC-6)</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,519.48,515.28,35.83,8.80;6,77.17,527.23,91.08,8.80">th Text REtrieval Conference</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,77.17,547.16,478.14,8.80;6,77.17,559.11,206.74,8.80" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="6,166.47,547.16,353.77,8.80">Variations in relevance judgments and the measurement of retrieval effectiveness</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,529.96,547.16,25.35,8.80;6,77.17,559.11,146.66,8.80">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,77.17,579.04,478.14,8.80;6,77.17,590.99,188.18,8.80" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="6,168.65,579.04,190.22,8.80">Overview of the TREC 2004 Robust Track</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,383.82,579.04,171.49,8.80;6,77.17,590.99,46.13,8.80">Proceedings of the 13th Text REtrieval Conference</title>
		<meeting>the 13th Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,77.17,610.92,434.48,8.80" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="6,161.48,610.92,132.89,8.80">The TREC 2005 Robust Track</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,315.80,610.92,84.66,8.80">ACM SIGIR Forum</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">40</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,77.17,630.84,478.14,8.80;6,77.17,642.80,428.50,8.80" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="6,189.58,642.80,239.47,8.80">UWaterlooMDS at TREC Core Track 2017 (Notebook)</title>
		<author>
			<persName coords=""><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mustafa</forename><surname>Abualsaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nimesh</forename><surname>Ghelani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Angshuman</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In TREC</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
