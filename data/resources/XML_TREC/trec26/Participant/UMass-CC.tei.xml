<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,152.99,116.95,309.39,12.62">UMass at TREC 2017 Common Core Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,166.75,155.63,47.96,8.74"><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Intelligent Information Retrieval College of Information and Computer Sciences</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,222.38,155.63,64.40,8.74"><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
							<email>zamani@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Intelligent Information Retrieval College of Information and Computer Sciences</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,295.15,155.63,71.40,8.74"><forename type="first">Stephen</forename><surname>Harding</surname></persName>
							<email>harding@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Intelligent Information Retrieval College of Information and Computer Sciences</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,374.61,155.63,69.68,8.74"><forename type="first">Shahrzad</forename><surname>Naseri</surname></persName>
							<email>shnaseri@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Intelligent Information Retrieval College of Information and Computer Sciences</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,235.53,167.59,53.60,8.74"><forename type="first">James</forename><surname>Allan</surname></persName>
							<email>allan@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Intelligent Information Retrieval College of Information and Computer Sciences</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.83,167.59,68.00,8.74"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
							<email>croft@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Intelligent Information Retrieval College of Information and Computer Sciences</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,152.99,116.95,309.39,12.62">UMass at TREC 2017 Common Core Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">84CEF8B15A19B37C9B6DF75CA5C345C6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This is an overview of University of Massachusetts efforts in providing document retrieval run submissions for the TREC Common Core Track with the goal of using newly developed techniques in retrieval and ranking to provide many new documents for relevance judgments. It is hoped these new techniques will reveal new documents not seen via traditional techniques, that will increase the numbers of relevant judged documents for the research collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Tools</head><p>We used the Galago search engine, freely available through the Lemur Project (http://www.lemurproject.org) with source code available through Source-Forge (https://sourceforge.net/p/lemur/galago/). This application provides a framework for search engine research and has a very efficient, distributed indexing capability especially useful for large text collections.</p><p>The entire LDC2008T19 collection consisting of documents from New York Times articles from 1987 through 2007 was indexed with many different text fields defined to enable very focused query definitions if needed.</p><p>The Galago search engine provides the query operators used in defining the search models. These operators include query likelihood, RM3 (relevance model), SDM (sequential dependence model), WSDM (weighted sequential dependence model), Okapi BM25, and PL2 weighting models. Only terms from topic titles were used. Some of our non-baseline submissions made use of MART and LambdaMART learning to rank models implemented in RankLib (https://sourceforge. net/p/lemur/RankLib-2.8/), another Lemur Project library providing several learning to rank models. Additionally, we used Tensorflow (https://www. tensorflow.org/) to implement our DIRE model (see <ref type="bibr" coords="1,377.54,632.19,42.62,8.74">Section 5)</ref>.</p><p>For the models that use pre-trained word embedding vectors, we used the vectors learned by GloVe (https://nlp.stanford.edu/projects/glove/) <ref type="bibr" coords="1,467.31,657.11,9.96,8.74" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Baseline Submissions</head><p>Two search models were used in producing the baseline results: Sequential Dependence Model <ref type="bibr" coords="2,208.65,155.45,10.52,8.74" target="#b6">[7]</ref> and Relevance Model <ref type="bibr" coords="2,318.66,155.45,10.52,8.74" target="#b4">[5]</ref> which are briefly described below.</p><p>1. umass baselnsdm: A sequential dependence model (SDM) using the Galago #sdm query operator to transform raw query text into a query form reflecting the Sequential Dependence Model. This model assumes dependencies between adjacent query terms. The resulting query form contains three components consisting of unigram, ordered distance and unordered distance operations, each with differing weights that may be specified by the user. Default weights were used for the baseline SDM queries consisting of 0.8 for unigrams, 0.15 for ordered distance and 0.05 for unordered window of query terms. No additional resources were used in producing this baseline and the process was entirely automatic. This model was described in more detail by Metzler and Croft <ref type="bibr" coords="2,233.67,296.31,9.96,8.74" target="#b6">[7]</ref>. 2. umass baselnrm: A relevance model (RM3) using Galagos #rm operator.</p><p>This is an implementation of a pseudo-relevance feedback process based on language models producing a specified number of query expansion terms from a specified number of top ranked documents returned by an initial query. A second pass search is performed using the original query with a specified weight combined with the derived new expansion terms to obtain the final ranked document returns. Our baseline runs used a default original query weight of 0.25 using the top 10 terms from each of the top 10 originally returned documents. No additional resources were used in producing this baseline and the process was entirely automatic. This model was described in more detail by Lavrenko and Croft <ref type="bibr" coords="2,318.80,427.71,10.52,8.74" target="#b4">[5]</ref> as well as Abdul-Jaleel et al. <ref type="bibr" coords="2,461.01,427.71,9.96,8.74" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Determining Run Submission Priorities</head><p>We focused on retrieved document "uniqueness" in deciding what priority each submission would have with the objective of submitting runs with the most unseen or unique document IDs for later judgment. This involved the following, iterative process:</p><p>-Combine top 10 ranked documents from our two baseline runs as the initial pool of document IDs to compare our submission runs against. -Determine a "uniqueness score" for each submission against this initial pool of document IDs. This score was simply the number of top 10 document IDs in a submission that do not appear in the comparison pool. -Assign the next available priority level to the current best unique score document. -Add the top 10 ranked documents from this submission to the comparison pool. -Continue determining a uniqueness score using this new comparison pool of document IDs to the remaining, not yet prioritized submissions.</p><p>-Continue until all submissions have been assigned a priority.</p><p>-Submit the runs to NIST in priority order.</p><p>Uniqueness determinations were applied only using the 50 NIST topics and not the full 250 topic set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">UMass Submissions</head><p>The following is a list of UMass submissions (other than the baselines provided as a service to the track) to the Common Core Track. They totalled 10 submissions with three for the phase 1 deadline and seven more for the phase 2 deadline. The runs are ordered by the submission priority (see Section 4). In the following, we briefly describe the method produced each submission. were same as the umass direlm submission except we used results from MART trained on Robust04 as the input to the DIRE model. 4. umass maxpas150: A passage-based retrieval model based on the language modeling approach <ref type="bibr" coords="3,237.36,454.11,9.96,8.74" target="#b5">[6]</ref>. The model uses a sliding window with length N and step size N/2 to extract passages from a document. We used N=150 to extract passages and computed the language modeling scores between the query and each of the passages. The documents were ranked by the highest score of its passages. Our language modeling approach is the same as the one introduced by Huston and Croft <ref type="bibr" coords="3,313.98,513.89,10.52,8.74" target="#b3">[4]</ref> (settings for Robust04). 5. umass maxpas50: A passage-based retrieval model based on the language modeling approach <ref type="bibr" coords="3,236.82,537.68,9.96,8.74" target="#b5">[6]</ref>. All settings were same with as the umass maxpas150 submission except we used N=50. 6. umass letor lm: A learning-to-rank model based on LamdaMART <ref type="bibr" coords="3,449.77,561.47,9.96,8.74" target="#b1">[2]</ref>. We used the 250 topic titles with annotations from Robust04 as our training data. Our ranking features include query features (the sum, max, arithmetic/harmonic/geometric means and the standard deviation of the inverse document/corpus frequency and the clarity score <ref type="bibr" coords="3,370.66,609.29,15.50,8.74" target="#b9">[10]</ref> for each query term) and model features (the scores from BM25, PL2, LM, SDM, WSDM, RM3, MaxPassage50 and MaxPassage150; the setting for model features is similar to the one used by Liu and Croft <ref type="bibr" coords="3,320.14,645.16,10.52,8.74" target="#b5">[6]</ref> as well as Huston and Croft <ref type="bibr" coords="3,463.43,645.16,10.30,8.74" target="#b3">[4]</ref>). All features were normalized before the training process. We tuned the tree number from 1000 to 3000, leaf number from 10 to 30, learning rate from 0.1 to 0.3, threshold candidates from 256 to 768 and randomly selected 10% of the data to validate the performance of different models. We reported the results of the best model on the validation set. 7. umass letor m: A learning-to-rank model based on MART <ref type="bibr" coords="4,413.79,167.22,9.96,8.74" target="#b2">[3]</ref>. All settings were same as with the umass letor lm submission except we used MART as the learning algorithm. 8. umass-eqe1: Embedding-based Query Expansion (EQE), a query expansion model using word embeddings. The underlying assumption of this model is "conditional independence of query terms". We used the word embedding vectors learned by GloVe <ref type="bibr" coords="4,266.26,238.36,10.52,8.74" target="#b7">[8]</ref> on the Wikipedia dump 2014 plus Gigawords 5. The embedding dimensionality was set to 300.We linearly interpolated the original query with the expansion terms (the number of expansion terms was set to 10). The interpolation coefficient was set to 0.5. This model was described in more detail by Zamani and Croft <ref type="bibr" coords="4,355.52,286.18,10.52,8.74" target="#b8">[9]</ref> (see Section 3.2). 9. umass erm: A pseudo-relevance feedback approach based on word embedding vectors, called Embedding-based Relevance Model (ERM). This model uses both embedding similarities and the information from the top-retrieved documents for each query. We use the same pre-trained embedding vectors as those used in umass-eqe1. We used top 10 retrieved documents and expanded the original query using 10 terms. The parameters α and β are both set to 0.5. This model was described in more detail by Zamani and Croft <ref type="bibr" coords="4,470.07,369.27,10.52,8.74" target="#b8">[9]</ref> (see Section 3.3). 10. umass letor lmn: A learning-to-rank model based on LamdaMART <ref type="bibr" coords="4,451.15,392.59,9.96,8.74" target="#b2">[3]</ref>. All settings were same as with the umass letor lm submission except we did not sample the training data to form a validation set. Instead, we used the performance on the training data to select the best model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis</head><p>Fig. <ref type="figure" coords="4,155.39,486.51,4.98,8.74" target="#fig_0">1</ref> plots the number of relevant, non-relevant, and unjudged documents for the top 10 (Fig. <ref type="figure" coords="4,208.54,498.47,9.22,8.74" target="#fig_0">1a</ref>) and 50 (Fig. <ref type="figure" coords="4,283.96,498.47,9.59,8.74" target="#fig_0">1b</ref>) retrieved documents per submission run.</p><p>As depicted in the figures, umass maxpas50 retrieved the highest number of unjudged documents, hence is more likely to retrieve documents that cannot be retrieved by conventional retrieval models. Interestingly, there are few unjudged documents among the top 10 documents retrieved by umass direlmnvs, while it substantially increases when it comes to the top 50 documents. Furthermore, umass erm retrieves the highest number of relevant documents in both cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>This work was supported in part by the Center for Intelligent Information Retrieval and in part by NSF IIS-1160894. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,134.77,313.42,345.82,8.74;5,134.77,325.38,90.69,8.74"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Number of relevant, non-relevant, and unjudged retrieved documents for each submission run.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,138.97,287.10,341.62,128.33"><head></head><label></label><figDesc>1. umass direlmnvs: A Deep Interaction Reranking (DIRE) model. It takes the results from the ranked list produced by a learning-to-rank algorithm and reranks them according to the local feature distribution extracted from the list. Here we used the results from LambdaMART trained on Robust04 without validations (umass letor lm) as the input for the DIRE model and reranked the top 60 documents.</figDesc><table /><note coords="3,138.97,358.71,341.62,9.02;3,151.70,370.67,328.89,9.02;3,151.70,382.62,328.89,9.02;3,151.70,394.58,90.11,8.74;3,138.97,406.41,341.62,9.02"><p>2. umass direlm: A Deep Interaction Reranking (DIRE) model. All settings were same as the umass direlmnvs submission except we used results from the LambdaMART with a validation set (umass letor lmn) as the inputs for the DIRE model. 3. umass diremart: A Deep Interaction Reranking (DIRE) model. All settings</p></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,142.96,378.67,337.64,7.86;5,151.52,389.63,329.07,7.86;5,151.52,400.59,314.07,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,349.41,389.63,131.19,7.86;5,151.52,400.59,16.81,7.86">Umass at trec 2004: Novelty and hard</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Abdul-Jaleel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Larkey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,188.14,400.59,249.81,7.86">Proceedings of the 2004 Text REtrieval Conference, TREC &apos;04</title>
		<meeting>the 2004 Text REtrieval Conference, TREC &apos;04</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,142.96,410.82,337.63,7.86;5,151.52,421.78,117.23,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="5,205.78,410.82,229.86,7.86">From ranknet to lambdarank to lambdamart: An overview</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010-06">June 2010</date>
			<publisher>Microsoft</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="5,142.96,432.01,337.63,7.86;5,151.52,442.97,194.44,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,223.72,432.01,252.42,7.86">Greedy function approximation: A gradient boosting machine</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,151.52,442.97,96.18,7.86">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,142.96,453.20,337.64,7.86;5,151.52,464.16,329.07,7.86;5,151.52,475.12,61.25,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="5,275.05,453.20,205.54,7.86;5,151.52,464.16,130.77,7.86">Parameters learned in the comparison of retrieval models using term dependencies</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Huston</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts Amherst</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="5,142.96,485.35,337.64,7.86;5,151.52,496.31,329.07,7.86;5,151.52,507.27,329.07,7.86;5,151.52,518.23,48.38,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,279.59,485.35,133.99,7.86">Relevance based language models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,434.71,485.35,45.88,7.86;5,151.52,496.31,329.07,7.86;5,151.52,507.27,174.83,7.86">Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;01</title>
		<meeting>the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;01<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,142.96,528.46,337.64,7.86;5,151.52,539.42,329.07,7.86;5,151.52,550.38,304.02,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,261.13,528.46,178.12,7.86">Passage retrieval based on language models</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,462.94,528.46,17.66,7.86;5,151.52,539.42,329.07,7.86;5,151.52,550.38,96.35,7.86">Proceedings of the Eleventh International Conference on Information and Knowledge Management, CIKM &apos;02</title>
		<meeting>the Eleventh International Conference on Information and Knowledge Management, CIKM &apos;02<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="375" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,142.96,560.62,337.63,7.86;5,151.52,571.57,329.07,7.86;5,151.52,582.53,329.07,7.86;5,151.52,593.49,117.25,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,268.20,560.62,208.15,7.86">A markov random field model for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,163.53,571.57,317.07,7.86;5,151.52,582.53,242.49,7.86">Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;05</title>
		<meeting>the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;05<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,142.96,603.73,337.64,7.86;5,151.52,614.68,329.07,7.86;5,151.52,625.64,235.46,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="5,325.25,603.73,155.35,7.86;5,151.52,614.68,35.30,7.86">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,205.47,614.68,275.12,7.86;5,151.52,625.64,135.58,7.86">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;14</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;14</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,142.96,635.88,337.64,7.86;5,151.52,646.84,329.07,7.86;5,151.52,657.79,289.64,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="5,274.67,635.88,166.20,7.86">Embedding-based query language models</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,462.94,635.88,17.66,7.86;5,151.52,646.84,329.07,7.86;5,151.52,657.79,81.97,7.86">Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval, ICTIR &apos;16</title>
		<meeting>the 2016 ACM International Conference on the Theory of Information Retrieval, ICTIR &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="147" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.62,120.67,337.97,7.86;6,151.52,131.63,329.07,7.86;6,151.52,142.59,329.07,7.86;6,151.52,153.55,196.59,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,297.85,120.67,182.74,7.86;6,151.52,131.63,186.83,7.86">Effective pre-retrieval query performance prediction using similarity and variability evidence</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Scholer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tsegay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,356.29,131.63,124.30,7.86;6,151.52,142.59,299.21,7.86">Proceedings of the IR Research, 30th European Conference on Advances in Information Retrieval, ECIR&apos;08</title>
		<meeting>the IR Research, 30th European Conference on Advances in Information Retrieval, ECIR&apos;08<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="52" to="64" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
