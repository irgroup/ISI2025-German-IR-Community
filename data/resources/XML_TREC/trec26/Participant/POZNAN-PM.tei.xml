<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,178.73,74.27,238.40,12.54">POZNAN Contribution to TREC PM 2017</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,169.58,110.93,74.92,10.04"><forename type="first">Artur</forename><surname>Cieślewicz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Clinical Pharmacology</orgName>
								<orgName type="institution">Poznan University of Medical Sciences</orgName>
								<address>
									<settlement>Poznan</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,252.41,111.04,79.27,9.94"><forename type="first">Jakub</forename><surname>Dutkiewicz</surname></persName>
							<email>jakub.dutkiewicz@put.poznan.pl</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">IARiII</orgName>
								<orgName type="institution" key="instit2">Poznan University of Technology</orgName>
								<address>
									<settlement>Poznan</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,340.63,110.93,81.54,10.04"><forename type="first">Czesław</forename><surname>Jędrzejek</surname></persName>
							<email>czeslaw.jedrzejek@put.poznan.pl</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">IARiII</orgName>
								<orgName type="institution" key="instit2">Poznan University of Technology</orgName>
								<address>
									<settlement>Poznan</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,178.73,74.27,238.40,12.54">POZNAN Contribution to TREC PM 2017</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EC82563544B20A55A160951FF63AC453</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Disease</term>
					<term>Variant</term>
					<term>Demographic</term>
					<term>and Other. For instance: Disease: Acute lymphoblastic leukemia Variant: ABL1</term>
					<term>PTPN11 Demographic: 12-year-old male Other:</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work describes the medical information retrieval systems designed by the Poznan Consortium for TREC PM, personalized medicine track, which was submitted to the TREC 2017. The baseline is the Terrier DPH Bo1 which recently has been shown to be the most effective Terrier option. We also used Mesh query expansion, word2vec query expansion, and the combination of these two options. In all measures our results are approximately 0,02 above the median.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="841.92" lry="595.32"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="841.92" lry="595.32"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="841.92" lry="595.32"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="841.92" lry="595.32"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The TREC PM 2017 is following three previous Clinical Decision Support Track contests. Its aim was the retrieval of biomedical articles relevant for answering generic clinical questions about medical records. This year's track was dedicated to the personalization aspect of retrieved information, namely, " if an abstract provides information relevant to the treatment of the patient's cancer?" or "is a patient eligible for this clinical trial?".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Source and target documents and rules</head><p>There are two target document collections for the Precision Medicine track: scientific abstracts (a January 2017 snapshot of PubMed abstracts plus from AACR and ASCO proceedings targeted toward cancer therapy) and clinical trials (an April 2017 snapshot of ClinicalTrials.gov). Although we submitted contributions in both area our main concentration was clinical trials. Topics' description for clinical trials was structured information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Experience</head><p>The Poznan Consortium team for TREC PM consists of contributors of two institutions: Department of Clinical Pharmacology, Poznan University of Medical Sciences, and Information Systems and Technologies Division, IARiII, Poznan University of Technology. We participated in TREC CDS 2016 track and in bioCADDIE 2016 <ref type="bibr" coords="2,70.82,157.74,74.46,8.96">[Cieslewicz, 2017]</ref>. <ref type="bibr" coords="2,152.90,157.74,116.98,8.96">Dutkiewicz, Jedrzejek, 2017]</ref>. In our research we use the word embedding <ref type="bibr" coords="2,460.66,157.64,63.78,9.05;2,70.82,174.92,26.00,9.05">[Mikolov et al., 2013a]</ref>, with semantic (relation) knowledge <ref type="bibr" coords="2,258.05,175.02,84.82,8.96">[Faruqui et.al., 2015]</ref>, <ref type="bibr" coords="2,350.43,175.02,120.48,8.96">[Dutkiewicz, Jedrzejek, 2018]</ref>. We did not preprocess the target Open Access Subset of PubMed Central (PMC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Baseline System</head><p>Our experience shows that for biomedical systems Terrier 4.1 is among the best for baseline systems. The best performing were 1) BB2 (Bernoulli-Einstein model with Bernoulli after-effect and normalization, also denoted as "DPH + Bo1 2), LGD (a log-logistic model for information retrieval) <ref type="bibr" coords="2,371.95,289.41,74.82,8.96">[Cieslewicz, 2017]</ref>. Another valuable feature implemented in Terrier is pseudo relevance feedback query expansion (PRF) -a mechanism allowing for extraction of n most informative terms from m top ranked documents (ranking created in the first search run) which are then added to the original query in the second retrieval rank. Terrier provides both parameter free (Bose-Einstein 1, Bose-Einstein 2, Kullback-Leibler) and parameterized (Rocchio) models for query expansion (24). Rocchio feedback approach was developed using the Vector Space Model. The modified vectors are moved in a direction closer or farther away, from the original query depending on whether documents are related or non-related.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Query Expansion</head><p>Expanding queries by adding potentially relevant terms is a common practice in improving relevance in IR systems. There are many methods of query expansion. Relevance feedback takes the documents on top of a ranking list and adds terms appearing in these document to a new query. In this work we use the idea to add synonyms and other similar terms to query terms before the pseudo-relevance feedback. This type of expansion can be divided into two categories. The first category involves the use of ontologies or lexicons (relational knowledge). In biomedical area UMLS, MeSH (20), SNOMED-CT, <ref type="bibr" coords="2,462.30,535.31,39.73,8.96">WordNet,</ref><ref type="bibr" coords="2,508.88,535.31,15.04,8.96;2,70.82,552.59,107.81,8.96">and Wikipedia are used (27)</ref>. Generally, the result of lexicon type of expansion is positive. The second category is word embedding (WE). This belongs to a class of distributional semantics, feature learning techniques in natural language processing. One can draw experience on effect of using lexicons from other semantic task areas.</p><p>For natural language queries requiring an answer using multiple choice, relational learning using dictionaries encompassing the whole corpus gives always better results than pure word embedding (word2vec). However, having synonym dictionaries (flat structure) can significantly improve the word2vec results.</p><p>In this contribution we do not use word embedding for query expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Retrieval methodology setup</head><p>We are using a dedicated retrieval methodology. We process and index metadata provided by Clinical Trials articles. We use manually constructed gene knowledge base to expand the query. We index all articles from Clinical Trials and Scientific Medline Abstracts into two separate data entities. Algorithms are described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Methodology developed for Medline Abstracts</head><p>The Medline documents were converted into a dedicated XML format as follows:</p><p>1. Value of &lt;DOCNO&gt; : the value of &lt;PMID&gt; tag, 2. Value of &lt;TEXT&gt; : concatenation of vales of tags &lt;KEYWORDS&gt; &lt;ARTICLETITLE&gt; &lt;ABSTRACT&gt; and &lt;MESHHEADING&gt;.</p><p>Queries were constructed with two different setups:</p><p>1. simple: find documents with any term from &lt;disease&gt;, &lt;gene&gt; or &lt;other&gt; tag present in &lt;TEXT&gt; field of indexed content, 2. strict: find documents with all terms from &lt;disease&gt; and &lt;gene&gt; tag and any term from &lt;other&gt; tag present in &lt;TEXT&gt; field of indexed content.</p><p>"strict" queries were expanded manually in the same way as described in the method developed for Clinical Trials.</p><p>Medline documents were indexed with Terrier and 3 retrieval runs were carried out: a. POZabsBB2sn: simple query was used as input; ranking function was BB2 b. POZabsBB2GRn: a set of "strict" queries (generated with manual expansion) was used as input; ranking function was BB2 c. POZabsLGDGRn: a set of "strict" queries (generated with manual expansion) was used as input; ranking function was LGD For runs b and c, generated results contained duplicates, which were removed by taking only one document with highest score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Methodology developed for Clinical Trials data</head><p>The Medline documents were converted into a dedicated XML format as follows:</p><p>1. tag &lt;DOCNO&gt;: the value of &lt;nct_id&gt; tag 2. tag &lt;TEXT&gt;: the values of tags <ref type="bibr" coords="3,235.70,648.62,200.63,8.96;3,106.82,665.90,392.30,8.96;3,106.82,683.18,241.02,8.96">&lt;brief_title&gt;, &lt;official_title&gt;, &lt;brief_summary&gt;, &lt;detailed_description&gt;, &lt;primary_outcome, secondary_outcome&gt;, &lt;condition&gt;, &lt;arm_group&gt;, &lt;condition_browse&gt;, &lt;intervention_browse&gt;, &lt;keyword&gt;;</ref> additionally, inclusion criteria were extracted from criteria tag 3. tag &lt;NEGATIVE&gt;: exclusion criteria extracted from &lt;criteria&gt; tag Three types of queries were constructed, based on the topic structure: a. simple: find documents with any term from &lt;disease&gt; or &lt;gene&gt; tag present in &lt;TEXT&gt; field of indexed content and no term from &lt;other&gt; tag present in &lt;NEGATIVE&gt; field of indexed content, b. strict: find documents with all terms from &lt;disease&gt; and &lt;gene&gt; tag present in &lt;TEXT&gt; field of indexed content and no term from &lt;other&gt; tag present in &lt;NEGATIVE&gt; field of indexed content, c. optional gene: find documents with all terms from &lt;disease&gt; tag present in &lt;TEXT&gt; field of indexed content, any term from &lt;gene&gt; tag present in &lt;TEXT&gt; field of indexed content, and no term from &lt;other&gt; tag present in &lt;NEGATIVE&gt; field of indexed content. b) and c) queries were manually expanded in a following manner: b. for each gene name a list of synonymous gene names was prepared (based on the data from NCBI Gene database: ftp://ftp.ncbi.nih.gov/gene/DATA/GENE_INFO/Mammalia/Homo_sapiens.gene_info.gz, c. for each disease term a list of synonymous names was prepared (based on entry terms from NCBI</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MeSH database)</head><p>For each query a set of queries was produced based on prepared synonymous terms.</p><p>Five Terrier runs were carried out, using LGD as ranking function: a. LGDraw: "simple" query was put as input for terrier; the query was expanded with terrier pseudo relevance feedback (PRF) b. LGDStrict: a set of "strict" queries (generated with manual expansion) was used as input c. LDGprfStrict: as LGDStrict but with terrier PRF d. LGDnoprfGOpt: a set of "optional gene" queries (generated with manual expansion) was used as input e.</p><p>LGDprfGOpt: as LGDnoprfGOpt but with terrier PRF f. For runs b-e, generated results contained duplicates, which were removed by taking only one document with highest score.</p><p>All generated results were then checked according to the value of &lt;demographic&gt; tag of each topic. Resulting documents that were describing trials recruiting patients with inappropriate age or gender were removed from the result set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>In this section we go through the outcome of every retrieval setup implemented by our group and applied to the competition data sets. We compare our results to median and best of the PM submissions. Finally, we discuss the best application for each setup. For the evaluation we show measures that were used by TREC PM evaluators for abstracts and clinical trials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results for Medline abstracts</head><p>In this category we submitted 3 runs. Summary for the runs we provided is presented in Table <ref type="table" coords="5,454.33,181.74,3.74,8.96" target="#tab_1">4</ref>.1. BB2 with no query expansion is little better than TREC PM median and much better than BB2 with query expansion and</p><p>LGD with query expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results for Clinical Trials</head><p>In this category we submitted 5 runs, all using LGD as a baseline. Summary for the runs we provided is presented in Table <ref type="table" coords="5,153.91,269.73,3.76,8.96" target="#tab_1">4</ref>.2. Similarly to the Medline Abstracts results, baseline LGD performs better that any expansion option and is much better than TREC PM median. The gain over TREC PM median is 27% for P5 measure, 47% for P10 measure, and 49% for P15 measure.</p><p>The results generated for Clinical trials data have shown that using Pseudo Relevance Feedback (PRF) to expand the query had negative impact on almost all measures (see the Table <ref type="table" coords="5,348.57,361.17,3.66,8.96" target="#tab_1">4</ref>.3). Terrier PRF was configured to expand queries with 10 terms from top 2 documents. Observed worsening of the results could be explained with the fact that PRF was carried out before documents describing trials recruiting patients with inappropriate age or gender were removed from the result set. Another aspect worth considering is that Clinical trials information retrieval required finding the documents describing clinical trials for which the patient described in the query is eligible for the recruitment. PRF, by adding additional terms to the query, could therefore improve the score of not relevant documents. Table <ref type="table" coords="5,96.13,515.27,4.17,8.96" target="#tab_1">4</ref>.3 The effect of PRF on P5, P10 and P15 measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measure</head><p>LGD strict</p><p>LGD strict + PRF Difference(%)</p><p>LGD opt</p><p>LGD  best TREC 0,61 0,88 0,51 0,83 0,30 0,57 0,70 0,56 0,85 0,39 0,79 0,00 0,32 0,67 0,51 0,69 0,53 0,50 0,47 0,48 0,60 0,64 0,91 0,56 0,53 0,63 0,31 0,48 0,46 0,48 0,56 median TREC 0,46 0,60 0,28 0,41 0,16 0,42 0,34 0,27 0,64 0,17 0,21 0,00 0,06 0,03 0,13 0,41 0,26 0,31 0,19 0,17 0,38 0,18 0,51 0,25 0,26 0,09 0,10 0,12 0,21 0,19 0,26 absLGDGRn 0,40 0,50 0,10 0,50 0,12 0,22 0,13 0,23 0,55 0,14 0,31 0,00 0,00 0,11 0,25 0,32 0,30 0,35 0,21 0,18 0,36 0,30 0,17 0,05 0,03 0,00 0,09 0,11 0,18 0,05 0,21 absBB2sn 0,41 0,55 0,28 0,78 0,20 0,38 0,22 0,32 0,66 0,20 0,31 0,00 0,00 0,01 0,36 0,40 0,27 0,46 0,23 0,17 0,49 0,36 0,74 0,08 0,00 0,12 0,09 0,04 0,11 0,29 0,28 absBB2GRn 0,40 0,47 0,13 0,58 0,15 0,15 0,16 0,17 0,41 0,17 0,26 0,00 0,00 0,12 0,24 0,45 0,26 0,34 0,22 0,18 0,42 0,38 0,45 0,02 0,04 0,00 0,05 0,13 0,18 0,05 0,22</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P10</head><p>best TREC 1,00 1,00 0,90 1,00 0,50 0,90 1,00 0,90 1,00 0,60 0,90 0,00 0,50 0,80 0,30 1,00 1,00 1,00 0,90 0,70 0,90 0,90 1,00 1,00 0,70 0,80 0,80 0,90 1,00 0,90 0,83 median TREC 0,60 0,90 0,30 0,70 0,20 0,60 0,50 0,30 0,80 0,20 0,20 0,00 0,10 0,00 0,10 0,60 0,30 0,50 0,20 0,20 0,50 0,40 0,60 0,40 0,30 0,10 0,10 0,20 0,30 0,20 0,35 absLGDGRn 0,50 1,00 0,40 0,60 0,20 0,10 0,10 0,50 0,70 0,30 0,70 0,00 0,00 0,20 0,20 0,40 0,50 0,70 0,20 0,30 0,80 0,10 0,00 0,20 0,00 0,00 0,10 0,10 0,30 0,10 0,31 absBB2sn 0,50 0,80 0,30 0,70 0,30 0,60 0,10 0,40 0,90 0,30 0,90 0,00 0,00 0,00 0,10 1,00 0,10 0,80 0,30 0,10 0,60 0,50 0,90 0,10 0,00 0,10 0,20 0,10 0,20 0,50 0,38 absBB2GRn 0,40 1,00 0,40 0,70 0,00 0,00 0,20 0,40 0,50 0,30 0,60 0,00 0,00 0,20 0,10 0,70 0,40 0,80 0,30 0,10 0,50 0,70 0,60 0,00 0,00 0,00 0,10 0,10 0,30 0,10 0,32</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R-prec</head><p>best TREC 0,50 0,44 0,41 0,42 0,21 0,42 0,38 0,34 0,46 0,27 0,60 0,00 0,24 0,55 0,30 0,46 0,37 0,30 0,34 0,35 0,39 0,42 0,57 0,44 0,37 0,50 0,21 0,40 0,40 0,33 0,38 median TREC 0,34 0,27 0,20 0,21 0,12 0,30 0,17 0,23 0,35 0,13 0,14 0,00 0,04 0,00 0,10 0,26 0,23 0,15 0,09 0,12 0,30 0,11 0,31 0,17 0,17 0,05 0,09 0,07 0,14 0,15 0,17 absLGDGRn 0,39 0,17 0,08 0,16 0,09 0,21 0,07 0,14 0,19 0,10 0,29 0,00 0,00 0,06 0,20 0,26 0,20 0,10 0,14 0,20 0,24 0,25 0,19 0,03 0,02 0,00 0,05 0,07 0,07 0,03 0,13 absBB2sn 0,35 0,22 0,20 0,26 0,18 0,26 0,12 0,23 0,31 0,16 0,26 0,00 0,00 0,00 0,10 0,23 0,28 0,19 0,11 0,18 0,27 0,16 0,38 0,03 0,00 0,10 0,07 0,02 0,05 0,19 0,16 absBB2GRn 0,37 0,16 0,08 0,12 0,10 0,20 0,09 0,12 0,16 0,13 0,21 0,00 0,00 0,06 0,10 0,25 0,12 0,10 0,14 0,18 0,25 0,20 0,17 0,03 0,02 0,00 0,04 0,09 0,07 0,03 0,12 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TREC BEST RESULTS</head><p>P5 1,00 1,00 1,00 1,00 0,80 0,80 1,00 0,80 1,00 0,00 0,80 0,00 0,60 1,00 0,40 0,40 0,60 0,60 0,80 0,40 1,00 1,00 0,80 1,00 1,00 0,20 0,80 0,00 0,80 1,00 0,77 P10 0,90 0,90 1,00 0,90 0,50 0,80 1,00 0,70 1,00 0,00 0,70 0,00 0,60 0,60 0,20 0,30 0,60 0,40 0,50 0,30 1,00 1,00 0,70 1,00 1,00 0,30 0,60 0,10 0,60 0,70 0,68 P15 0,67 0,93 0,87 0,80 0,53 0,67 1,00 0,67 1,00 0,00 0,67 0,00 0,67 0,40 0,13 0,27 0,53 0,33 0,40 0,27 0,93 0,93 0,53 0,87 0,80 0,20 0,47 0,07 0,40 0,53 0,59</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TREC MEDIAN RESULTS</head><p>P5 0,80 0,60 0,60 0,40 0,20 0,60 0,60 0,20 0,60 0,00 0,40 0,00 0,00 0,40 0,00 0,20 0,20 0,00 0,20 0,00 0,20 0,40 0,20 0,60 0,40 0,00 0,00 0,00 0,20 0,20 0,29 P10 0,40 0,60 0,60 0,40 0,20 0,50 0,60 0,20 0,70 0,00 0,20 0,00 0,00 0,30 0,00 0,10 0,20 0,10 0,10 0,00 0,30 0,30 0,10 0,50 0,30 0,00 0,10 0,00 0,10 0,20 0,25 P15 0,27 0,60 0,47 0,33 0,20 0,40 0,67 0,27 0,67 0,00 0,20 0,00 0,00 0,20 0,00 0,07 0,20 0,07 0,13 0,07 0,27 0,20 0,13 0,33 0,27 0,00 0,13 0,00 0,07 0,13 0,23</p><p>LGDprf Strict P5 0,80 0,20 0,40 0,40 0,20 0,60 0,40 0,20 1,00 0,00 0,20 0,00 0,00 0,60 0,00 0,00 0,00 0,40 0,00 0,20 0,20 0,40 0,00 1,00 0,60 0,00 0,00 0,00 0,20 0,20 0,29 P10 0,50 0,40 0,20 0,40 0,20 0,50 0,30 0,30 1,00 0,00 0,10 0,00 0,00 0,60 0,00 0,10 0,00 0,20 0,20 0,10 0,20 0,70 0,00 0,60 0,40 0,00 0,00 0,00 0,10 0,10 0,26 P15 0,33 0,47 0,13 0,33 0,13 0,53 0,27 0,27 0,93 0,00 0,20 0,00 0,00 0,40 0,00 0,20 0,00 0,13 0,13 0,07 0,13 0,60 0,00 0,40 0,27 0,00 0,13 0,00 0,07 0,07 0,22</p><p>LGDnoprf Strict P5 0,80 0,80 0,40 0,80 0,40 0,60 0,20 0,00 1,00 0,00 0,80 0,00 0,00 0,80 0,00 0,40 0,20 0,40 0,40 0,20 0,20 0,60 0,00 0,80 0,60 0,00 0,00 0,00 0,20 0,20 0,39 P10 0,50 0,90 0,20 0,60 0,20 0,50 0,10 0,10 0,80 0,00 0,40 0,00 0,00 0,60 0,00 0,20 0,10 0,20 0,20 0,10 0,10 0,70 0,10 0,60 0,30 0,10 0,10 0,00 0,10 0,10 0,28 P15 0,33 0,80 0,13 0,60 0,13 0,53 0,20 0,33 0,73 0,00 0,27 0,00 0,00 0,40 0,00 0,20 0,20 0,13 0,13 0,07 0,20 0,67 0,20 0,47 0,33 0,07 0,20 0,00 0,07 0,07 0,27</p><p>LGD P5 0,60 0,80 0,80 0,40 0,20 0,60 0,40 0,00 0,60 0,00 0,60 0,00 0,00 0,80 0,20 0,20 0,20 0,40 0,40 0,20 0,20 0,60 0,00 1,00 0,40 0,00 0,20 0,00 0,20 0,40 0,37 P10 0,60 0,70 0,90 0,50 0,20 0,50 0,30 0,20 0,70 0,00 0,40 0,00 0,00 0,50 0,20 0,20 0,40 0,30 0,40 0,10 0,20 0,60 0,10 1,00 0,70 0,00 0,40 0,00 0,10 0,20 0,37 P15 0,40 0,80 0,80 0,53 0,20 0,53 0,40 0,33 0,80 0,00 0,40 0,00 0,00 0,33 0,13 0,20 0,40 0,20 0,33 0,07 0,20 0,60 0,07 0,67 0,53 0,00 0,33 0,00 0,07 0,13 0,34</p><p>LGDprf Opt P5 0,60 1,00 0,80 0,20 0,40 0,60 0,40 0,20 1,00 0,00 0,20 0,00 0,00 0,60 0,20 0,20 0,00 0,40 0,40 0,20 0,40 0,60 0,00 1,00 0,60 0,00 0,20 0,00 0,20 0,60 0,39 P10 0,60 0,70 0,80 0,20 0,30 0,50 0,30 0,20 1,00 0,00 0,30 0,00 0,00 0,50 0,10 0,20 0,00 0,20 0,20 0,10 0,20 0,70 0,00 0,60 0,40 0,00 0,10 0,00 0,10 0,40 0,31 P15 0,40 0,67 0,73 0,27 0,27 0,53 0,27 0,13 0,93 0,00 0,20 0,00 0,00 0,40 0,07 0,20 0,00 0,13 0,13 0,07 0,13 0,53 0,07 0,40 0,27 0,00 0,13 0,00 0,07 0,33 0,26</p><p>LGDnoprf Opt P5 0,60 0,80 0,80 0,20 0,20 0,60 0,20 0,00 1,00 0,00 0,60 0,00 0,00 0,80 0,00 0,40 0,20 0,40 0,40 0,20 0,20 0,60 0,00 0,80 0,60 0,00 0,00 0,00 0,20 0,80 0,38 P10 0,60 0,90 0,90 0,40 0,20 0,50 0,20 0,10 0,90 0,00 0,40 0,00 0,00 0,60 0,00 0,20 0,10 0,20 0,20 0,10 0,10 0,70 0,10 0,60 0,30 0,10 0,20 0,00 0,10 0,70 0,34 P15 0,40 0,80 0,73 0,47 0,27 0,53 0,13 0,27 0,73 0,00 0,27 0,00 0,00 0,40 0,00 0,20 0,20 0,13 0,13 0,07 0,13 0,67 0,20 0,47 0,33 0,07 0,13 0,00 0,07 0,47 0,30 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LGDprfStric</head><p>LGDnoprfStrict LGD</p><p>LGDprfOpt LGDnoprfOpt</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation</head><p>In the evaluation, we focus mainly on the results achieved for the Clinical Trials test set, as it was a main focus of our contribution. We attach the evaluation of results generated with the Medical Abstracts test set to the figures. The main purpose of the evaluation is the research on the poor performance of strict queries (used in</p><p>LGDStrict and LGDprfStrict runs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation framework</head><p>We use a precision and recall framework to evaluate the method. As we are operating on a partially annotated corpus, we provide dedicated definitions of recall and precision. We believe that our definitions follow the spirit of an original, commonly used precision and recall framework.</p><p>A traditional TREC evaluation methodology suffers from serious distortion of results when a number of relevant documents is smaller than @k. Beyond the number of relevant documents the results are padded with zeroes.</p><p>This does not change a ranking, but caused the loss of normalization between questions, and in our opinion distorts averages. We propose a method that rectifies this shortcoming.</p><p>The annotated corpus provides a set of relevant and irrelevant documents with the annotation of relevancy for each document with regard to a given query. The result sets consist of retrieved documents with a score related to each document and a specific query. Let us define ' a number of true positives at k retrieved documents'</p><p>(tp@k) as a number of retrieved documents, which are annotated as relevant. Analogically, 'a number of false positives at k retrieved documents' (fp@k) is defined as a number of retrieved documents which are annotated as irrelevant. 'A number of true negatives at k retrieved documents' (tn@k) is a number of documents, which are annotated as relevant and are not retrieved. We define precision(prec) as a number of relevant and retrieved documents divided by a number of retrieved and annotated (either as relevant or irrelevant) documents.</p><p>Similarly to the precision measure, we define recall as a number of retrieved and relevant documents divided by a number of all relevant documents.</p><p>It should be noted that, with this definition of recall, the ideal score might not be equal to one, as the number of relevant documents might be higher than k. In that case the ideal number of retrieved and relevant documents should be equal to the number of retrieved documents. We can apply this amendment to the formula by applying a rectifier to the number of retrieved and annotated documents.</p><p>While absolute values produced by ( <ref type="formula" coords="10,221.81,743.68,3.90,8.96">2</ref>) and (3) are different, rankings of methods obtained with both formulas are equal. As we believe there is little to no difference between both methods of evaluation, we are using equation ( <ref type="formula" coords="11,111.79,73.02,3.55,8.96">2</ref>). The same thinking can be applied to the precision formula. Given the definitions of precision and recall we define the evaluation metric F1. We use a classical definition given by (4).</p><p>,</p><p>although strictly speaking we should use modified precision and recall in Eq. ( <ref type="formula" coords="11,387.76,147.78,3.55,8.96" target="#formula_0">4</ref>). We do not do this in this work since F1 is not a primary measure of evaluation in TREC.</p><p>We expect the F1 measure within a wide range of a parameter k to highlight the details of invalid behavior of restricting queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Setup</head><p>We export annotated sets of documents and queries, as well as the result sets to an SQL database. Architecture of the database is illustrated in the Figure <ref type="figure" coords="11,236.32,277.05,3.76,8.96" target="#fig_2">5</ref>.1. Within the database, we join the Annotations and Results tables.</p><p>Based on the joined table, we construct a set of SQL queries, which allow us to retrieve tp@k, tn@k and fp@k measures. Finally, we define a procedure, which generates a view of retrieved values for k of 10, 100 and 1000 for each query. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation Results</head><p>We present evaluation results for all three of the evaluation measuresrecall, precision and F1 measure at 10, 100 and 1000 retrieved documents. Figure <ref type="figure" coords="11,244.23,664.34,4.17,8.96" target="#fig_2">5</ref>.2 shows, that our extension performs well within a large number of retrieved documents due to the relatively large increase in precision. For 1000 retrieved documents, we observe a drop of recall. That behavior is intuitive, with strict queries, we expect to retrieve a smaller number of relevant documents, at the same time, the results to be precise. The semi-strict queries (with the optional appearance of the gene name within the document) are between the baseline queries and the strict queries. The performance worsens as the number of retrieved documents gets smaller. We hoped to achieve a similar result regardless of a parameter k. Figure <ref type="figure" coords="12,153.70,73.02,4.17,8.96" target="#fig_2">5</ref>.3 illustrates the behavior of restricting queries at 100 retrieved documents. Strict queries are still applicable at that point, however we already observe a drop in the F1 measure. With a very small number of retrieved documents, strict queries perform worse than simple queries, that is presented in Figure <ref type="figure" coords="12,509.32,107.58,3.82,8.96" target="#fig_2">5</ref>.4.</p><p>Strict queries improve the quality of documents within the tail of the distribution. Simple, non-restricted queries retrieve equally precise documents in the head of the distribution, while preserving the higher value of recall measure.  Better results were generated for Clinical Trials task (P5, P10 and P15 better than median by averagely 0.07, 0.06 and 0.05) than for Scientific Abstracts (infNDCG and P10 were better than median by 0.03 only in one BB2 run). The difference may be due to the fact, that in the Scientific Abstracts task we did not check if the documents were describing patients with correct age and gender, as this data was hard to be extracted from unstructured text. Surprisingly, more strict queries (requiring the presence of all terms in relevant document or all disease terms and any of gene terms) provided worse results than a baseline query (all query terms were treated as optional). We provide an extended evaluation of that problem. It turns out, that restricted queries enhance the tail, while worsening the head of retrieval distribution. With small numbers of 5, 10 and 15 retrieved documents strict queries perform worse.</p><p>A traditional TREC evaluation methodology suffers from serious distortion of results when a number of relevant documents is smaller than @k. Beyond the number of relevant documents the results are padded with zeroes.</p><p>This does not change a ranking, but caused the loss of normalization between questions., and in our opinion distorts averages. We propose a method that rectifies this shortcoming, and facilitates comparison of methods when analyzing averages. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="11,82.22,596.68,173.21,9.05;11,167.90,351.82,259.10,227.97"><head>Figure 5 . 1</head><label>51</label><figDesc>Figure 5.1 Evaluation database architecture</figDesc><graphic coords="11,167.90,351.82,259.10,227.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="12,154.46,386.75,297.42,9.05"><head>Figure</head><label></label><figDesc>Figure 5.2 F1, precision and recall measures for 1000 retrieved documents</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="13,159.38,275.63,287.30,9.05"><head>Figure 5</head><label>5</label><figDesc>Figure 5.4 F1, precision and recall measures for 10 retrieved documents</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="13,106.22,662.28,410.39,9.94;13,70.82,681.24,426.74,9.94;13,70.82,700.32,435.99,9.94;13,70.82,719.16,222.07,9.94;13,89.78,205.65,4.58,9.02;13,78.41,180.19,15.94,9.00;13,82.97,154.70,11.38,9.00;13,78.41,129.21,15.94,9.00;13,82.97,103.75,11.38,9.00;13,124.97,81.15,40.38,14.04;13,232.25,193.46,4.56,9.00;13,225.46,175.51,11.41,9.02;13,225.46,157.58,11.38,9.00;13,225.46,139.63,11.38,9.00;13,225.46,121.67,11.38,9.00;13,225.46,103.75,11.38,9.00;13,296.78,205.17,16.16,9.00;13,271.46,81.15,51.65,14.04;13,394.34,192.26,4.56,9.00;13,382.97,174.55,15.98,9.02;13,382.97,156.86,15.94,9.00;13,382.97,139.15,15.94,9.00;13,382.97,121.43,15.94,9.00;13,387.53,103.75,11.38,9.00;13,449.64,203.97,19.84,9.00;13,421.01,81.15,57.42,14.04"><head></head><label></label><figDesc>Amati,G.,van Rijsbergen,C. J., (2002)  Probabilistic models of information retrieval based on measuring the divergence from randomness, ACM Trans. Inf. Syst. 20(4): 357-389 bioCADDIE: BIOCADDIE 2016 DATASET RETRIEVAL CHALLENGE https://biocaddie.org/biocaddie2016dataset-retrieval-challenge, access: 21 march 2017</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,17.40,72.92,807.31,141.21"><head>Table 4 .</head><label>4</label><figDesc>1 Summary for the provided Medline abstracts runs.</figDesc><table coords="6,17.40,106.93,807.31,107.20"><row><cell>TOPIC</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell><cell>14</cell><cell>15</cell><cell>16</cell><cell>17</cell><cell>18</cell><cell>19</cell><cell>20</cell><cell>21</cell><cell>22</cell><cell>23</cell><cell>24</cell><cell>25</cell><cell>26</cell><cell>27</cell><cell>28</cell><cell>29</cell><cell>30</cell><cell>ALL TOPICS</cell></row><row><cell>MEASURE METHOD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>infNDCG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,18.24,72.92,796.27,78.05"><head>Table 4 .</head><label>4</label><figDesc>2 Summary for the provided Clinical Trials runs.</figDesc><table coords="7,18.24,123.13,796.27,27.84"><row><cell>TOPIC</cell><cell>MEASURE</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell><cell>14</cell><cell>15</cell><cell>16</cell><cell>17</cell><cell>18</cell><cell>19</cell><cell>20</cell><cell>21</cell><cell>22</cell><cell>23</cell><cell>24</cell><cell>25</cell><cell>26</cell><cell>27</cell><cell>28</cell><cell>29</cell><cell>30</cell><cell>ALL TOPICS</cell></row><row><cell>METHOD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="14,106.22,73.24,410.95,9.94;14,70.82,92.32,414.76,9.94;14,70.82,111.28,439.20,9.94;14,70.82,130.12,41.95,9.94" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gururaj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Pournejati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ohnomachado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<title level="m" coord="14,234.63,92.32,250.96,9.94;14,70.82,111.28,439.20,9.94">A Publicly Available Benchmark for Biomedical Dataset Retrieval: The Reference Standard for the 2016 bioCADDIE Dataset Retrieval Challenge. Database</title>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,106.22,157.13,390.92,10.04;14,70.82,176.20,439.81,9.94;14,70.82,195.04,195.63,9.94;14,106.22,222.05,383.53,10.05;14,70.82,241.04,417.74,10.05;14,70.82,260.11,302.13,9.94;14,70.82,278.95,250.30,9.94;14,106.22,305.96,411.14,10.05;14,70.82,325.03,433.47,9.94;14,70.82,343.99,428.85,9.94;14,70.82,362.95,445.12,9.94;14,70.82,381.79,22.08,9.94;14,106.22,408.93,402.64,9.94;14,70.82,427.89,441.34,9.94;14,70.82,446.85,451.54,9.94;14,70.82,465.81,30.36,9.94" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,336.98,157.13,160.16,10.04;14,70.82,176.20,369.08,9.94;14,138.36,222.05,174.88,10.05;14,295.21,241.15,145.88,9.94;14,266.93,305.96,250.43,10.04;14,70.82,325.03,206.82,9.94">Baseline and extensions approach to information retrieval of complex medical data: Poznan Contribution to bioCADDIE</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cieślewicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dutkiewicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Jędrzejek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dutkiewicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Jędrzejek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Frąckowiak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Werda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dutkiewicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Jędrzejek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">K</forename><surname>Jauhar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dean</forename></persName>
		</author>
		<idno>CoRR abs/1301.3781</idno>
		<ptr target="https://www.nlm.nih.gov/mesh/download_mesh.html" />
	</analytic>
	<monogr>
		<title level="m" coord="14,332.39,222.05,41.13,10.04;14,471.31,241.15,17.26,9.94;14,70.82,260.11,297.43,9.94;14,217.29,343.99,282.38,9.94;14,70.82,362.95,118.27,9.94;14,470.30,408.93,38.56,9.94;14,70.82,427.89,231.43,9.94;14,165.33,446.85,247.72,9.94">Retrofitting Word Vectors to Semantic Lexicons. HLT-NAACL 1606-1615 MeSH database</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Won</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Wilbur</forename><forename type="middle">W J</forename></persName>
		</editor>
		<meeting><address><addrLine>Access</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2017. 2016. 2010. 2016. 2016. 2018. 2017. 2015. 21 March 2017 Mikolov,. 2013. 2009</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="69" to="80" />
		</imprint>
	</monogr>
	<note>Evaluation of Query Expansion Using MeSH in PubMed</note>
</biblStruct>

<biblStruct coords="14,103.94,465.81,415.83,9.94;14,70.82,484.77,446.27,9.94;14,70.82,503.61,191.91,9.94" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,481.30,465.81,38.48,9.94;14,70.82,484.77,423.93,9.94">State-ofthe-art in biomedical literature retrieval for clinical cases: a survey of the TREC 2014 CDS track</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hersh</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,501.63,484.77,15.47,9.94;14,70.82,503.61,21.81,9.94">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="148" />
			<date type="published" when="2016-04">2016. April 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,106.22,530.73,411.76,9.94;14,70.82,549.69,427.28,9.94;14,70.82,568.56,272.12,9.94" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gururaj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Pournejati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ohno-Machado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<title level="m" coord="14,246.03,549.69,252.08,9.94;14,70.82,568.56,227.50,9.94">Information Retrieval for Biomedical Datasets: The 2016 bioCADDIE Dataset Retrieval Challenge. Database</title>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,106.22,595.68,386.89,9.94;14,70.82,614.64,447.42,9.94;14,70.82,633.60,391.29,9.94;14,70.82,652.68,409.98,9.94;14,70.82,671.52,167.73,9.94" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="14,193.04,595.68,192.26,9.94;14,475.97,595.68,17.13,9.94;14,70.82,614.64,330.69,9.94">The SMART retrieval system-Experiments in automatic document processing</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rocchio</surname></persName>
		</author>
		<ptr target="http://trec-cds.appspot.com/2017.html" />
	</analytic>
	<monogr>
		<title level="m" coord="14,187.19,633.60,268.69,9.94">Terrier IR Platform www.terrier.org 26 Oct 2016 TREC-CDS</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</editor>
		<meeting><address><addrLine>New York City, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1971">1971. 2017</date>
			<biblScope unit="page" from="313" to="323" />
		</imprint>
	</monogr>
	<note>Relevance feedback in information retrieval</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
