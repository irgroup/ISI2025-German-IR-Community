<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,99.49,56.73,421.08,20.73;1,90.17,84.63,439.73,20.73;1,233.23,112.52,153.60,20.73">Retrieving documents based on gene name variations: MedIER at TREC 2017 Precision Medicine Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,99.48,172.76,41.93,9.50"><forename type="first">Tong</forename><surname>Yin</surname></persName>
							<email>tongyin@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,235.64,172.76,73.70,9.50"><forename type="first">Danny</forename><forename type="middle">T Y</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Biomedical Informatics</orgName>
								<orgName type="institution">University of Cincinnati</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,406.59,172.76,110.98,9.50"><forename type="first">V</forename><forename type="middle">G Vinod</forename><surname>Vydiswaran</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Learning Health Sciences</orgName>
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,99.49,56.73,421.08,20.73;1,90.17,84.63,439.73,20.73;1,233.23,112.52,153.60,20.73">Retrieving documents based on gene name variations: MedIER at TREC 2017 Precision Medicine Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9F75874BC5B8A90BDDA549B1EED72A61</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>gene name variation, query modification</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The TREC 2017 Precision Medicine Track focused on finding relevant medical documents -scientific abstracts and clinical trials -for cancer patient cases based on specific genetic variation and demographic information. We focused on the genetic variations mentioned in the query and explored ways to modify the search query and the retrieval ranking using this information. Further, we explored filtering retrieved results based on demographic information matching for clinical trials. The results show little improvements of the approaches over baseline runs, and suggest need for additional exploration.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In its previous incarnation, the Precision Medicine ran as the Clinical Decision Support track from 2014 to 2016. The Clinical Decision Support track focused on retrieving biomedical articles relevant to answering generic clinical questions about diagnosis, treatment, or test procedure for a given query topic. This year's task was primarily different in two areas: first, the query topics were focused on cancer diagnoses with specific genetic variations, and second, demographic information was available to further filter most relevant documents for the given query. The track included two tasks corresponding to retrieving (a) scientific abstracts and (b) clinical trials relevant to specific cancer cases.</p><p>Our participation in TREC 2017 Precision Medicine track was a collaborative endeavor between the University of Michigan and the University of Cincinnati. We participated in both tasks, and our approach focused on two research directions: (a) enhancing search queries with genetic variation information, and (b) using demographic information to select relevant clinical trials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Since the Precision Medicine track grew out of the Clinical Decision Support track, the previous teams that participated in those track tasks were of immediate interest to us. In previous tracks, systems were expected to retrieve biomedical articles that were relevant to answering questions about the diagnoses, treatment plans, or test procedures related to clinical case reports. The case reports included information about a patient reported complaints, test results, and observations from the first few hours of patients' visit to the hospital.</p><p>We conducted a survey of participating systems for TREC 2014 <ref type="bibr" coords="1,342.46,255.06,10.58,8.64" target="#b0">[1]</ref>, TREC 2015 <ref type="bibr" coords="1,412.57,255.06,10.58,8.64" target="#b1">[2]</ref>, and TREC 2016 Clinical Decision Support tracks <ref type="bibr" coords="1,386.52,266.02,10.58,8.64" target="#b2">[3]</ref>. Our analysis of the challenge reports showed that most participating teams used pseudo-relevance feedback to improve the ranking results. Team MERCK-KGAA <ref type="bibr" coords="1,351.86,298.90,11.62,8.64" target="#b3">[4]</ref> used pseudo-relevance feedback to expand initial query by adding words of the titles of the top k biomedical articles. MayoNLP <ref type="bibr" coords="1,400.44,320.82,11.62,8.64" target="#b4">[5]</ref> team used pseudo-relevance feedback model to utilize co-occurring MeSH heading terms to expand the query topics. in addition, the queries were expanded using (a) document's keyword meta-information field, (b) highranking TF-IDF terms from the title, abstract, and the full article when available, and (c) MeSH or UMLS concepts from the title and abstract.</p><p>Other teams use negation-aware ranking, but it was not universally beneficial. The ETH Zurich team used a BM25 variant that could detect natural language negations by converting "no diabetes" to "[nx]diabetes." <ref type="bibr" coords="1,490.23,437.94,11.62,8.64" target="#b5">[6]</ref> They combined both the original terms and the negation terms, and learned the weight function during training. The team performance was above the baseline performance. Team SCIAICL also considered use of negated concepts and achieved close to baseline performance. <ref type="bibr" coords="1,411.45,492.73,11.62,8.64" target="#b6">[7]</ref> Another key feature deployed by many teams was the use of word embedding models. Well-trained models were found to generate useful features by the top performing teams, while other teams suffered from setting less suitable parameters and reported poorer results. Team MERCKKGAA used word embedding to calculate document similarity between document centroids of topics and articles <ref type="bibr" coords="1,450.79,576.97,10.58,8.64" target="#b3">[4]</ref>. They found that such an approach contributed to a significant improvement in overall ranking. Team CBUN constructed semantic word vectors using the medical terms on word embedding. <ref type="bibr" coords="1,486.95,609.85,10.58,8.64" target="#b7">[8]</ref>. ETH Zurich <ref type="bibr" coords="1,559.48,609.85,11.62,8.64" target="#b5">[6]</ref> used the modified version of word2vec to expand to k neighbors and maximizing the cosine similarity with the given query. Although this was an interesting direction, we could not use this approach for TREC 2017 tasks because the disease name and gene variants need to match exactly.</p><p>Learning to rank was also a popular choice among the TREC 2016 clinical decision support track participants. Team SCIAICL used it to determine the weight for symptom queries <ref type="bibr" coords="1,351.39,716.01,10.58,8.64" target="#b6">[7]</ref>, Team MERCKKGAA used learning to rank with gradient boosting to maximize NDCG. <ref type="bibr" coords="1,484.83,726.97,11.62,8.64" target="#b3">[4]</ref> Both teams were able to improve their results above the baseline performance. However, because there was no specific training dataset for this year's tasks, we could not use this approach.</p><p>Finally, some teams made use of Wikipedia or results from Google search directly. Team CBUN found the corresponding symptoms for each diseases using Wikipedia and created the clinical causal relationships <ref type="bibr" coords="2,190.21,437.24,10.58,8.64" target="#b7">[8]</ref>. Team PRNACL used the Wikipedia clinical medicine category pages to build a directed knowledge graph <ref type="bibr" coords="2,156.03,459.15,10.58,8.64" target="#b8">[9]</ref>, with symptoms included as leaf nodes. Team CSIRO <ref type="bibr" coords="2,139.74,470.11,16.60,8.64" target="#b9">[10]</ref> used Wikipedia to expand names of diseases. Team iRiS built a Wikipedia index to predict the patient diagnosis <ref type="bibr" coords="2,137.12,492.03,15.27,8.64" target="#b10">[11]</ref>. Some of these approaches seemed relevant in this year's task, especially with respect to handling disease names.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. APPROACH</head><p>Our approach focused on two research directions: (a) enhancing search queries with genetic variation information, and (b) using demographic information to select relevant clinical trials. For this track, all participating teams were provided two datasets -a collection of scientific abstracts of peerreviewed biomedical research abstracts from PubMed, and a collection of clinical trials from ClinicalTrials.gov. The goal of the track was to identify the most relevant scientific abstracts and clinical trials pertaining to a given query, where a query is specified as a type of cancer, a specific gene variation, and some patient demographic information. The demographic information is especially important in filtering out clinical trials for which the patient may not be eligible.</p><p>A schematic diagram of our system architecture is shown in Fig. <ref type="figure" coords="2,67.10,716.01,3.74,8.64" target="#fig_0">1</ref>. Our system consisted of three components that are fairly common in any retrieval system, viz. (a) pre-processing and indexing the corpora, (b) query modification, and (c) retrieval and ranking. These components are explained in detail in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pre-processing and building index</head><p>The scientific abstracts corpus mainly comes from the January 2017 snapshot of PubMed abstracts, along with the abstracts obtained from AACR and ASCO proceedings. The overall corpus consists of 27.8 billion articles. In addition to the title and abstracts of each scientific paper in plain text form, the dataset also included some metadata about the journal publication dates, history, author, and MeSH identifiers of medical terms that appeared in that document. We parsed all available data in the XML version of the corpus to build two separate indexes: one index over the free text of article titles and abstracts, which another on the MeSH identifiers of medical terms noted for each article.</p><p>The clinical trials corpus consists of 176,000 trials and includes title, summary, and a detailed description. In addition, they also specify the eligibility criteria on age and gender for cohorts included in the trial. To specifically match and filter trials based on age and gender, we built an SQL database to store the age and gender specific eligibility criteria for every trial. The SQL database was later used in the retrieval and ranking phase to filter out non-eligible trials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Query modification</head><p>A set of 30 query topics were released as part of the TREC task, each with a set of four relevant factors: type of cancer, relevant genetic variation, demographic criteria, and other pertinent information. Since the scientific abstracts have no specific demographic information, we use the demographic information only for the clinical trial task.</p><p>1) Identifying candidate synonyms for gene names:: One of our primarily goals in participating for this year's challenge was to understand how gene variations are expressed in relevant documents and to enrich the keyword queries with information about the genetic variants. In preparing for the task, we collected information about gene names and synonyms from the National Center for Biotechnology Information (NCBI). A dataset of gene names and PubMed articles was selected from the NCBI gene resource 1 . Every gene is assigned a unique identifier, and are then listed as metadata in scientific articles on PubMed if the article mentions the specific gene. Hence, this resource provides a many-to-many relationship between genes and peer-reviewed research articles. For example, the PubMed ID 9873079 has four corresponding gene ID 1246502 "leuA", 1246503 "leuB", 1246504 "leuC", and 1246505 "leuD". The specific geneID 1246502 also has another related PubMed article with ID 9812361. The gene name and synonym resource generated our candidate list of synonyms.</p><p>2) Pruning ambiguous synonyms:: In addition to gene names, the NCBI dataset also includes the most relevant PubMed articles corresponding to the genes. We used these documents in a relevance feedback setup to prune out candidate synonyms that either do not contribute to finding relevant articles or are ambiguous and lead to retrieving many nonrelevant articles. We begin with all gene synonyms as a query and retrieve relevant articles from the free-text scientific abstract index. Based on the NCBI dataset, we calculate the baseline precision of the retrieved results. Then, we remove the gene synonyms one-by-one and compute the precision of the retrieved results at every step. If the precision increases above the baseline precision, the synonym will be pruned. For example, gene "cdkn2a" has geneID of 1029 in the NCBI dataset. The dataset has 2,031 PubMed articles related to geneID 1029. According to the gene name database, there are sixteen synonyms (aliases) of "cdkn2a", including "arf", "mlm", "p14", "p16", etc. Using all sixteen synonyms, we identified 80 relevant documents. It should be noted that the index is built over just the title and abstract text, while the NCBI dataset has access to the full paper to search for gene name variants. This contributes for the relatively low precision (80 instead of 2,031). If we remove "p16" from the synonym query, the number of relevant document retrieved increases to 189. This indicates that "p16" was too ambiguous, since including it in the query results in many non-relevant articles in the retrieved results. In contrast, if we only remove "p14", the number of relevant documents in the retrieved set drops to 58, implying that "p14" is necessary to retrieve at least 22 additional relevant documents. So, we keep "p14" in the gene name expansion set. This procedure is followed for every gene synonym over all the gene names in the query. Although timeconsuming, this one-time pre-processing helps in selecting the most representative synonyms for gene names.</p><p>3) Expanding free-text queries:: Finally, we combined the gene names and filtered synonyms with the cancer type keywords. We expanded the names of the cancer diagnoses 1 Can be downloaded from ftp://ftp.ncbi.nlm.nih.gov/gene/DATA/ with common synonyms. For example, "skin cancer" and "melanoma"; "stomach cancer" and "gastric cancer", etc. These alternate names were gathered from the MeSH disease tree. 4) Query using MeSH identifiers: Our past experience in previous years' TREC tasks demonstrated that adding MeSH identifiers could significantly improve the performance of retrieval result <ref type="bibr" coords="3,381.58,141.18,15.27,8.64" target="#b11">[12]</ref>. Hence, we create a new query based on MeSH terms and used it to query the MeSH identifier index (the second free-text index) and retrieve additional documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Retrieval Process</head><p>As the third and final component of our system, we use the Galago toolkit<ref type="foot" coords="3,393.95,213.06,3.49,6.05" target="#foot_0">2</ref> for our retrieval step. Galago deploys an inference network based retrieval model. We build two sets of queries for gene names and diseases. One is MeSH identifiersbased query that is run against the MeSH identifier index; while the second is a text query that runs against the free-text scientific abstracts index. The two retrieved results are merged to get a final ranked list. The documents which appear in both MeSH identifiers-based retrieval and free-text based retrieval are ranked at the top of the re-ranked result set in the order in which they appeared in the free-text query result. These documents are followed by the documents that only appear in MeSH identifiers result; followed by documents that appear only in the free-text retrieval results. The ranking order is based on the original normalized ranking scores in the corresponding ranked list.</p><p>For the clinical trials dataset, a similar approach was followed. However, since there was no MeSH identifier information available for the clinical trials, we ran the queries only against the free text index. On the other hand, the demographic information such as age and gender of eligible patients was provided in the clinical trail description. These demographic factors are matched against the characteristics of the given patient case (query topic). If a retrieved clinical trial specifically articulates gender or age criteria, and these criteria do not match with the given query, then the clinical trial is removed from the retrieved results. Trials with no eligibility criteria or the ones that match the given query topic are returned in the original retrieved order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SUBMITTED RUNS AND RESULTS</head><p>We submitted four runs for both the scientific abstract and clinical trial tasks. Table I summarizes all the runs submitted by our team. The runs consist of two baseline runs and two runs varying the fusion algorithms deployed to combine and re-rank the retrieved documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Scientific Abstracts</head><p>For the scientific abstracts task, the baseline free-text query and baseline MeSH identifier query were submitted as MedIER sa2 and MedIER sa4, respectively. The re-ranked list obtained by running pseudo-relevance feedback on free text query was submitted as the MedIER sa3 run. The final run, MedIER sa1, was generated by merging the ranking results from the free-text query and the MeSH identifier   The official results of our scientific abstract runs along with benchmarking runs from other participants is shown in Table <ref type="table" coords="4,75.76,373.37,6.08,8.64" target="#tab_2">II</ref>. The results show three measures: inferred NDCG (infNDCG), precision at 10 (P@10), and precision at recall of 100% (R-prec).</p><p>The run generated using MeSH identifiers for gene and disease (MedIER sa2) was the best one on infNDCG and Rprec measures. This is consistent with our prior experiments on the importance of MeSH terms in finding relevant results. The combination of MeSH terms and free-text query (Me-dIER sa1) performs the best on the P@10 measure. This is also consistent with our expectation that relevant documents that appear in both MeSH-based queries and free-text queries are truly relevant. The results indicate that additional information could lead to even better performance among the top candidate documents, while introducing some noise. When we compare the runs that enabled pseudo-relevance feedback against the baseline free-text query run, we notice that the pseudo-relevance feedback based run performed better on the infNDCG and R-prec measures, but the original free-text query performed better on the P@10 measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Clinical Trials</head><p>We submitted four runs for the clinical trials task. The first run, MedIER ct1, is the generated using free-text queries, followed by the filtering step on demographics characteristics of age and gender. The second run, MedIER ct2, is generated using similar approach followed by pseudo-relevance feedback to augment the results. Since the first two runs yielded fewer results, we expanded the number of retrieval documents to 3000 using pseudo-relevance feedback, followed by the demographic criteria filter to generate the other two runs, MedIER ct3 and MedIER ct4.</p><p>The official results of our clinical trials runs along with benchmarking runs from other participants is shown in Table III. The results show precision measures at three levels: at 5, 10, and 15 retrieved documents.  Among our models, the pseudo-relevance feedback on 1000 results (MedIER ct2) and the expanded set with 3000 results (MedIER ct4) performs the best on average. However, the overall precision values are low. Our initial analysis showed that there were considerable number of topics in which no relevant documents were extracted by our runs. Additional error analysis is needed to check the root cause of this anomaly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE WORK</head><p>Our participation in this year's Precision Medicine track focused on the genetic variations mentioned in the query and explored ways to modify the search query and the retrieval ranking using this information. We also explored filtering retrieved clinical trial results based on demographic information. To achieve these, we built two indices for the scientific abstracts corpus -one on the free-text and the other on the MeSH identifiers mentioned in the metadata of the research articles. For the clinical trials corpus, we created one free-text index and an SQL database to store demographic eligibility criteria on age and gender extracted from the clinical trial descriptions. Corresponding to these indices, we created queries focusing on cancer types and gene names, augmented with a list of carefully selected synonyms; and a set of alternate queries on MeSH identifiers. The retrieval results from these queries were merged to get the final ranking. The results show that the merged results did better in pulling more relevant documents to the top of the ranked list (higher precision at 10), and that pseudo-relevance feedback improves the results even further. However, it is possible that using more flexible merging algorithms may boost the performance further. With additional training data, we would experiment with learning to rank algorithms and tuning parameters to boost the overall performance.</p><p>In the future, we plan to continue exploring improvements based on gene variant names for cancer-related document retrieval. Additional error analysis is needed to understand the key limitations in the experiments over the clinical trials corpus. In particular, we would analyze the accuracy of the demographic feature extraction component and its impact on the exclusion of valid clinical trials from our submitted runs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,57.81,324.09,504.44,8.64;2,101.17,54.00,417.70,261.78"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: System architecture. Blue color represents components using MeSH identifiers, while red color represents free text.</figDesc><graphic coords="2,101.17,54.00,417.70,261.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,105.15,143.60,286.75,92.72"><head>TABLE I :</head><label>I</label><figDesc>Summary of submission runs</figDesc><table coords="4,105.15,181.66,139.72,54.66"><row><cell>Runs</cell><cell>infNDCG</cell><cell>P@10</cell><cell>R-prec</cell></row><row><cell>MedIER sa1</cell><cell>0.2036</cell><cell>0.3300</cell><cell>0.1177</cell></row><row><cell>MedIER sa2</cell><cell>0.2103</cell><cell>0.2967</cell><cell>0.1326</cell></row><row><cell>MedIER sa3</cell><cell>0.1986</cell><cell>0.2733</cell><cell>0.1143</cell></row><row><cell>MedIER sa4</cell><cell>0.1774</cell><cell>0.2800</cell><cell>0.1061</cell></row><row><cell>Best</cell><cell>0.5856</cell><cell>0.8600</cell><cell>0.3950</cell></row><row><cell>Median</cell><cell>0.2766</cell><cell>0.3733</cell><cell>0.1761</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,48.96,245.92,252.10,95.27"><head>TABLE II :</head><label>II</label><figDesc>Scientific Abstracts Results. All four submitted runs are compared against the best and averaged median performance on three measures: inferred NDCG, precision at 10, and precision at recall of 100%.</figDesc><table coords="4,48.96,321.49,252.10,19.70"><row><cell>query. The ranking function used for merging was: (3000 -</cell></row><row><cell>Ranking sa2) + (3000 -Ranking sa4).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,319.00,303.94,252.10,30.55"><head>TABLE III :</head><label>III</label><figDesc>Clinical Trials Results. All four submitted runs are compared against the best and averaged median performance on three measures: precision at 5, 10, and 15 documents.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="3,330.45,728.26,131.46,6.91"><p>https://www.lemurproject.org/galago.php</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,70.21,170.11,230.85,6.91;5,70.21,178.93,230.85,7.05;5,70.21,187.90,164.33,7.05" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,269.95,170.11,31.11,6.91;5,70.21,179.07,166.16,6.91">Overview of the TREC 2014 Clinical Decision Support track</title>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><forename type="middle">S</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,252.56,178.93,48.50,6.87;5,70.21,187.90,139.79,6.87">Proceedings of the 23rd Text Retrieval Conference (TREC)</title>
		<meeting>the 23rd Text Retrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,70.21,199.50,230.85,6.91;5,70.21,208.46,230.85,6.91;5,70.21,217.29,224.44,7.05" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,95.85,208.46,202.23,6.91">Overview of the TREC 2015 Clinical Decision Support track</title>
		<author>
			<persName coords=""><forename type="first">Kirk</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><forename type="middle">S</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,79.64,217.29,190.46,6.87">Proceedings of the 24th Text Retrieval Conference (TREC)</title>
		<meeting>the 24th Text Retrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,70.21,228.89,230.85,6.91;5,70.21,237.85,230.85,6.91;5,70.21,246.68,224.44,7.05" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,95.85,237.85,202.23,6.91">Overview of the TREC 2016 Clinical Decision Support track</title>
		<author>
			<persName coords=""><forename type="first">Kirk</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,79.64,246.68,190.46,6.87">Proceedings of the 25th Text Retrieval Conference (TREC)</title>
		<meeting>the 25th Text Retrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,70.21,258.28,230.85,6.91;5,70.21,267.24,230.85,6.91;5,70.21,276.07,230.86,7.05;5,70.21,285.03,86.75,7.05" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,140.75,267.24,160.31,6.91;5,70.21,276.21,80.19,6.91">Semi-supervised information retrieval system for clinical decision support</title>
		<author>
			<persName coords=""><forename type="first">Harsha</forename><surname>Gurulingappa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Toldo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Claudia</forename><surname>Schepers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gerard</forename><surname>Megaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,170.78,276.07,130.29,6.87;5,70.21,285.03,62.20,6.87">Proceedings of the 25th Text Retrieval Conference (TREC)</title>
		<meeting>the 25th Text Retrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,70.21,296.63,230.85,6.91;5,70.21,305.60,230.85,6.91;5,70.21,314.56,230.85,6.91;5,70.21,323.39,230.86,7.05;5,70.21,332.50,17.93,6.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,194.36,305.60,106.70,6.91;5,70.21,314.56,230.85,6.91;5,70.21,323.53,22.66,6.91">An ensemble model of clinical information extraction and information retrieval for clinical decision support</title>
		<author>
			<persName coords=""><forename type="first">Yanshan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Majid</forename><surname>Rastegar-Mojarad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ravikumar</forename><surname>Komandur Elayavilli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hongfang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,108.55,323.39,188.69,6.87">Proceedings of the 25th Text Retrieval Conference (TREC)</title>
		<meeting>the 25th Text Retrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,70.21,343.95,230.85,6.91;5,70.21,352.92,230.85,6.91;5,70.21,361.74,230.86,7.05;5,70.21,370.71,118.63,7.05" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,230.42,352.92,70.64,6.91;5,70.21,361.89,106.63,6.91">ETH Zurich at TREC Clinical Decision Support 2016</title>
		<author>
			<persName coords=""><forename type="first">Simon</forename><surname>Greuter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Philip</forename><surname>Junker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lorenz</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felix</forename><surname>Mance</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Virgile</forename><surname>Mermet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Angela</forename><surname>Rellstab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carsten</forename><surname>Eickhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,200.67,361.74,100.39,6.87;5,70.21,370.71,94.08,6.87">Proceedings of the 25th Text Retrieval Conference (TREC)</title>
		<meeting>the 25th Text Retrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,70.21,382.31,230.85,6.91;5,70.21,391.28,230.85,6.91;5,70.21,400.10,230.86,7.05;5,70.21,409.07,47.51,7.05" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,178.23,391.28,122.84,6.91;5,70.21,400.24,54.27,6.91">Siena&apos;s clinical decision assistant with machine learning</title>
		<author>
			<persName coords=""><forename type="first">Brendan</forename><surname>Kish</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Katherine</forename><surname>Small</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Gassert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kylie</forename><surname>Small</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sharon</forename><forename type="middle">Gower</forename><surname>Small</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,139.39,400.10,161.67,6.87;5,70.21,409.07,22.96,6.87">Proceedings of the 25th Text Retrieval Conference (TREC)</title>
		<meeting>the 25th Text Retrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,70.21,420.67,230.85,6.91;5,70.21,429.49,230.86,7.05;5,70.21,438.46,86.75,7.05" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="5,201.07,420.67,99.99,6.91;5,70.21,429.63,78.37,6.91">CBNU at TREC 2016 Clinical Decision Support Track</title>
		<author>
			<persName coords=""><forename type="first">Seung-Hyeon</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyung-Soon</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,169.86,429.49,131.20,6.87;5,70.21,438.46,62.20,6.87">Proceedings of the 25th Text Retrieval Conference (TREC)</title>
		<meeting>the 25th Text Retrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,70.21,450.06,230.85,6.91;5,70.21,459.02,230.85,6.91;5,70.21,467.99,230.85,6.91;5,70.21,476.81,215.01,7.05" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="5,246.51,459.02,54.55,6.91;5,70.21,467.99,217.59,6.91">Clinical question answering using key-value memory networks and knowledge graph</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sadid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Siyuan</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vivek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joey</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kathy</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ashequl</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaditya</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oladimeji</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Farri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,70.21,476.81,190.46,6.87">Proceedings of the 25th Text Retrieval Conference (TREC)</title>
		<meeting>the 25th Text Retrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,70.21,488.41,230.85,6.91;5,70.21,497.24,230.86,7.05;5,70.21,506.20,118.63,7.05" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="5,247.01,488.41,54.05,6.91;5,70.21,497.38,108.57,6.91">CSIRO at TREC Clinical Decision Support Track</title>
		<author>
			<persName coords=""><forename type="first">Sarvnaz</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sara</forename><surname>Falamaki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,201.52,497.24,99.55,6.87;5,70.21,506.20,94.08,6.87">Proceedings of the 25th Text Retrieval Conference (TREC)</title>
		<meeting>the 25th Text Retrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,70.21,517.80,230.85,6.91;5,70.21,526.77,230.85,6.91;5,70.21,535.59,230.85,7.05;5,70.21,544.56,47.51,7.05" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="5,281.15,517.80,19.92,6.91;5,70.21,526.77,230.85,6.91;5,70.21,535.74,36.69,6.91">Query expansion with automatically predicted diagnosis: iRiS at TREC CDS Track 2016</title>
		<author>
			<persName coords=""><forename type="first">Danchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,128.84,535.59,172.22,6.87;5,70.21,544.56,22.96,6.87">Proceedings of the 25th Text Retrieval Conference (TREC)</title>
		<meeting>the 25th Text Retrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,70.21,556.16,230.85,6.91;5,70.21,565.13,230.85,6.91;5,70.21,573.95,230.85,7.05;5,70.21,582.91,152.55,7.05" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="5,94.73,565.13,206.33,6.91;5,70.21,574.09,153.87,6.91">Learning from medical summaries: The University of Michigan at TREC 2015 Clinical Decision Support Track</title>
		<author>
			<persName coords=""><forename type="first">Fengmin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Danny</forename><forename type="middle">T Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">G</forename><surname>Vinod Vydiswaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,240.28,573.95,60.78,6.87;5,70.21,582.91,128.00,6.87">Proceedings of the 24th Text REtrieval Conference (TREC)</title>
		<meeting>the 24th Text REtrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
