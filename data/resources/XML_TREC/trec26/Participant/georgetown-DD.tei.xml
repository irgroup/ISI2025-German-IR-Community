<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,132.00,76.23,347.42,12.29;1,158.25,94.72,295.24,9.66">A Reinforcement Learning Approach for Dynamic Search Georgetown University at TREC 2017 Dynamic Domain Track</title>
				<funder ref="#_JBYfpj7">
					<orgName type="full">DARPA</orgName>
				</funder>
				<funder ref="#_75pwbae">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,185.25,126.88,53.57,8.78"><forename type="first">Zhiwen</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgetown University Georgetown University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,357.75,126.88,65.50,8.78"><forename type="first">Grace</forename><forename type="middle">Hui</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgetown University Georgetown University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,132.00,76.23,347.42,12.29;1,158.25,94.72,295.24,9.66">A Reinforcement Learning Approach for Dynamic Search Georgetown University at TREC 2017 Dynamic Domain Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">350DB5619C781F156C46F6B5D72E7614</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>TREC Dynamic Domain (DD) track is intended to support the research in dynamic, exploratory search within complex domains. It simulates an interactive search process where the search system is expected to improve its efficiency and effectiveness based on its interaction with the user. We propose to model the dynamic search as a reinforcement learning problem and use neural network to find the best policy during a search process. We show a great potential of deep reinforcement learning on DD track.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>TREC 2017 Dynamic Domain (DD) track simulates a professional search scenario where the search system needs to satisfy user's hierarchical information need in multiple rounds of search. This track target at search in within specific complex domains. User's information need are usually informational and composed of multiple aspects. Participant systems are expected to learn user's intent from previous interaction and find relevant documents with high efficiency.</p><p>In DD, a program called "Jig" simulates user's response. At the beginning of each search topic, the participant 1 systems receive the topic name (query) , a high-level description of user's information need. Each topic consists of several subtopics, but the name and the number of subtopics are not known to the participant systems. Then the participant systems need to find related documents in the next few rounds of retrieval. In each round, a participant system can return up to 5 documents to the simulated user (Jig). Jig responds with the relevance information regarding documents it receives. The feedback from Jig provides detailed relevance information, including graded relevance score regarding each subtopic for passages in the returned documents. Participant systems can then adjust its search algorithm based on the feedback, or choose to stop if they believe no relevant documents can be found in later rounds.</p><p>In 2017, DD track provides 60 search topics (queries) in New York Times Annotated Corpus <ref type="bibr" coords="1,450.47,546.13,10.62,8.78" target="#b0">[1]</ref>. Three metrics, i.e. Cube Test (CT) <ref type="bibr" coords="1,143.98,559.63,10.62,8.78" target="#b1">[2]</ref>, Session-DCG (sDCG) <ref type="bibr" coords="1,259.01,559.63,11.66,8.78" target="#b2">[3]</ref> and Expected Utility (EU) <ref type="bibr" coords="1,392.10,559.63,10.62,8.78" target="#b3">[4]</ref>, including their raw scores and normalized scores <ref type="bibr" coords="1,149.94,573.13,10.62,8.78" target="#b4">[5]</ref>, are used for evaluation. These metrics models the outcome of search and the effort of user from different perspectives.</p><p>In our work, we model the dynamic search as a reinforcement learning problem. We employ a Deep Q-learning Network <ref type="bibr" coords="1,110.97,627.13,10.62,8.78" target="#b8">[9]</ref>, which is successful in many reinforcement learning tasks, to optimize the search process. We show that Deep Reinforcement Learning has a great potential in dynamic search and it has hugely improves the performance of dynamic search system especially in terms of efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reinforcement Learning Framework</head><p>The setting of DD shares many similarities with Reinforcement learning <ref type="bibr" coords="2,363.45,88.63,10.62,8.78" target="#b5">[6]</ref>. In DD track, the search system interacts with the simulated user and makes a series of decisions, such as whether to stop the search and how to rerank the documents. The search system is expected to make optimal decisions to maximize the evaluation scores. In reinforcement learning, an agent interacts with the environment and during every interaction, the agent need to take an action. The environment responds the action with a reward and the agent also need to update its internal state. The goal of reinforcement learning is to find the optimal policy that guides the agent to maximize the accumulated reward in the long term. These similarities inspire us to model the dynamic search as a reinforcement learning problem. In this section, we will first review the basic concepts in reinforcement learning, then introduce one the most successful deep reinforcement learning framework, Deep Q-learning Network. Our approaches will be described in the following sections of this paper. is a scale value. The agent's ultimate goal is to maximize the (s, ) R a accumulated reward in the long term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Markov Decision</head><p>While the reward function represents the immediate effects of an action, a value function describe the expected cumulative reward for the current state or action. The action value function (Q function) evaluates the expected long term reward in terms of an action in the given state. That is (s , )</p><formula xml:id="formula_0" coords="2,207.15,645.88,196.93,11.13">E[r γ max Q(T (s , a ), a )] Q t a t = t+1 + * a′ t t ′</formula><p>Q learning is the approach that directly approximate the Q function, from which the optimal policy can be derived. It is proposed by Watkins and Dayan <ref type="bibr" coords="2,214.14,687.13,15.32,8.78" target="#b13">[14]</ref>. The Q function is updated in a step-by-step style. The updated rule is:</p><formula xml:id="formula_1" coords="2,173.40,699.95,264.28,11.82">(s , ) Q(s , ) α Q t a t ← t a t + r γ max Q(s , a) (s , a ) [ t+1 + a t+1 -Q t t ]</formula><p>Where is the discounting factor and is the learning rate. γ α</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Deep Q learning Network (DQN)</head><p>As the rise of Deep Learning, Mnih et al. propose DQN, where the (s,a) is estimated using a deep neural network Q <ref type="bibr" coords="3,72.00,171.13,10.62,8.78" target="#b8">[9]</ref>. DQN also introduces two import mechanism to stabilize the training and improve the performance.</p><p>The first mechanism is double Q learning. DQN contains two neural networks. The first one is the Q network while the second one is the target Q network. During training, The parameters of Q network are updated with Stochastic Gradient Descent (SGD). But the parameters of the target Q network will be kept frozen for a while and every after a given interval, the parameters of target Q network will be synchronized with those of Q network.</p><p>The second mechanism is experience. During each step, the DQN agent choose the action with -greedy algorithm. ε The newly chosen action will change the state and brings in the reward. All these transitions will stored in a experience buffer. During training, each time a batch of transitions are sampled from the experience buffer and used for training the Q network.</p><p>The training of Q network is based on the loss function shown below:</p><formula xml:id="formula_2" coords="3,176.40,351.62,258.63,20.32">(θ) (r γ max Q(s , | θ ) Q(s, a | θ) ) L = ∑ (s, a, s , r) ∈batch ′ + a ′ ′ a ′ -- 2</formula><p>Where are the parameters of the Q network and are the parameters for the target Q network. The expected θ θ output for given state action pair</p><formula xml:id="formula_3" coords="3,206.33,389.35,204.46,12.66">is its Q value, i.e. . s, ) ( a γ max Q(T (s, a), ) r + a′ a ′</formula><p>DQN has made huge success in many reinforcement learning tasks, especially in Game Playing <ref type="bibr" coords="3,456.46,419.38,10.62,8.78" target="#b8">[9]</ref>. Inspired by the huge success of DQN and the similarity between Dynamic Search and Reinforcement Learning, we propose our RL-based solutions, which are detailed in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DQN for Dynamic Search</head><p>We propose two frameworks that using DQN for dynamic search. Two frameworks have same definition of reward, similar actions and different compositions of states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Reward for Dynamic Search</head><p>Finding the key results for each topic is crucial for maintaining high efficiency in dynamic search, for which we "highlight" the passages with the highest relevance scores. For every passage, its relevance score (rating) is redefined as: iff. else where is the original rating for the passage. The incremental of r′ = r r = 4 .1 r′ = 0 * r r the gain of Cube Test <ref type="bibr" coords="3,163.05,609.13,11.66,8.78" target="#b1">[2]</ref> based on the redefined relevance score is the gain for the current step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Framework 1</head><p>In this framework, we use state-of-the-art deep learning techniques to embed words and sentences. The embedding results make up the state representation of the DQN agent. The actions for the agent are reformulating queries, including adding term, removing term, reweighting terms and stop search, all of those are detailed in <ref type="bibr" coords="3,500.96,690.13,15.32,8.78" target="#b10">[11]</ref>. The whole framework is shown in Figure <ref type="figure" coords="3,221.92,703.63,3.75,8.78" target="#fig_1">2</ref>.</p><p>The state is defined as a tuple,</p><p>. All the words are first &lt; uery, Relevant P assages, Iteration number S = Q &gt; embedded with word2vec <ref type="bibr" coords="4,177.76,89.38,15.32,8.78" target="#b11">[12]</ref>. Since the length of query and relevance passages may vary, we use Long Short Term Memory (LSTM) <ref type="bibr" coords="4,253.60,102.88,16.65,8.78" target="#b12">[13]</ref> to encode the query and passages respectively. The LSTM takes in as input the sequence of word vectors, which are obtained in the previous step. Then the final output of LSTMs and the iteration number are concatenated to form the state.</p><p>The state representation goes through the hidden layer and the output layer to produce the estimated long term reward for each action. Each unit in the output layer corresponds to a possible action. The agent will make decisions based on the output estimated cumulative reward. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Framework 2</head><p>In this framework, we handpick some features of the dynamic search process that we believe better represents the search status. Apart from all the actions defined in Framework 1, we also add another action, i.e. re-retrieval with the top 10 words with highest tf*idf values.</p><p>The state is defined as a tuple . The &lt; ubtopic f ound f lag, subtopic hit count, teration number, miss count S = s i &gt; is a boolean vector with a fixed length, where each entry indicates if the subtopic has been ubtopic f ound f lag s found or not. If an entry is 0, it means the subtopic has not been found so far or the subtopic may not even exist in current topic. The is in the same length of , and each entry is the number of ubtopic hit count s ubtopic f ound f lag s documents have been found on the corresponding subtopic.</p><p>is the same as in Framework 1. teration number i is the number of iterations where no relevant documents are retrieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>iss count m</head><p>Different items in this tuple may go through different hidden layers. The final output layer is similar to that in Framework 1 where each unit corresponds with an action. The whole Framework is shown in Figure <ref type="figure" coords="4,476.47,656.38,3.75,8.78" target="#fig_2">3</ref>. DQN Parameters : The size of experience buffer is set to 10000, the learning rate of the neural network is set to 0.001, the discounting factor for MDP is set to 0.5. The target Q network and the Q network are synchronized γ every after 20 steps.</p><p>Other : We use galago as our backend search engine and use the default structured query operator. We use gensim <ref type="bibr" coords="5,163.89,455.78,3.00,5.27;5,536.78,455.78,3.00,5.27">2 3</ref> to train the word2vec model and Keras to build the deep neural network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>We submitted 3 runs, dqn_semantic_state , dqn_5_actions , and galago_baseline . dqn_semantic_state uses Framework 1 and dqn_5_actions uses Framework 2. galago_baseline is the top 50 results of galago with no feedback information being used, which serves as the baseline for comparison.</p><p>The performance of three runs regarding Cube Test, session-DCG and Expected Utility and their normalized scores are shown in Figure <ref type="figure" coords="5,154.18,591.88,5.00,8.78" target="#fig_3">4</ref> to Figure <ref type="figure" coords="5,200.54,591.88,3.75,8.78">9</ref>, more detailed results can be found in Table <ref type="table" coords="5,385.96,591.88,8.33,8.78" target="#tab_1">10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Every metric reveals some different characteristics of these runs, which brings in very interesting discussions about our methods.</p><p>In terms of Cube Test, both frameworks surpass the baseline. Especially dqn_semantic_state , which uses Framework 1. It doubles the baseline in the end. It means that both frameworks improve the efficiency of the dynamic search system. The improvement on efficiency might come from more gaining of relevant information in given time. It may also come from early stopping on the topics where search system may not perform so well. Session DCG gives more insight about the gaining of relevant information. It can be found that the session DCG score of dqn_semantic_state is below the baseline while dqn_5_actions is still better than the baseline. It can be inferred that the high performance of dqn_semantic_state in terms of Cube Test does not come from retrieving more relevant documents.</p><p>Expected Utility evaluates how well the search system balance the gaining of information and the effort of user. It is seen that both frameworks makes improvement over the baseline and dqn_5_actions is the best this time. It confirms again dqn_semantic_state achieve high performance in CT by early stopping while dqn_5_actions does find more relevant documents.</p><p>One of the major discoveries in our runs this year is the power of early stopping. A good stopping strategy can greatly improve the efficiency of search system and satisfies the user better. We also show the great potential of deep reinforcement learning in dynamic search. It found a stopping criterion that improve the efficiency this time. And it will be a much more interesting question that how to use it to find a good retrieval algorithm. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,207.75,316.63,195.97,8.78;2,195.00,181.50,222.00,127.50"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Framework of Reinforcement Learning</figDesc><graphic coords="2,195.00,181.50,222.00,127.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,259.50,396.13,92.17,8.78;4,156.00,209.25,300.00,179.25"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Framework 1</figDesc><graphic coords="4,156.00,209.25,300.00,179.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,259.50,294.13,92.17,8.78;5,164.25,73.50,282.75,213.00"><head>Figure 3</head><label>3</label><figDesc>Figure 3. Framework 2</figDesc><graphic coords="5,164.25,73.50,282.75,213.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,227.44,469.28,3.00,5.27"><head>4</head><label>4</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,266.25,405.88,78.85,8.78;6,102.75,154.50,405.75,243.75"><head>Figure 4</head><label>4</label><figDesc>Figure 4. CT scores</figDesc><graphic coords="6,102.75,154.50,405.75,243.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="7,103.50,100.50,405.00,243.00"><head></head><label></label><figDesc></figDesc><graphic coords="7,103.50,100.50,405.00,243.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="7,103.50,417.00,405.00,243.00"><head></head><label></label><figDesc></figDesc><graphic coords="7,103.50,417.00,405.00,243.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="8,102.75,73.50,405.75,243.75"><head></head><label></label><figDesc></figDesc><graphic coords="8,102.75,73.50,405.75,243.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="8,102.75,390.75,405.75,243.75"><head></head><label></label><figDesc></figDesc><graphic coords="8,102.75,390.75,405.75,243.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,72.00,414.20,467.68,158.71"><head>Process and Q learning</head><label></label><figDesc>: the immediate reward function. The environment response the agent's actions with immediate rewards. Reward R describes how "well" the action is.</figDesc><table coords="2,72.00,438.13,467.68,108.63"><row><cell cols="8">Markov Decision Process (MDP) is usually used for model a reinforcement learning problem. An MDP can be</cell></row><row><cell cols="7">represented as a tuple</cell><cell>&lt; S</cell><cell>, A, T , R</cell><cell>&gt;</cell><cell>. The meaning of each symbol is described as follows:</cell></row><row><cell>S</cell><cell cols="5">: the set of states</cell><cell>s) (</cell><cell>. State is agent's belief about the its status in the environment.</cell></row><row><cell cols="2">A :</cell><cell cols="5">the set of actions</cell><cell>a) (</cell><cell>. Action is the agent's behavior under a given state. An action will result in the state</cell></row><row><cell cols="8">transitions and bring in reward.</cell></row><row><cell>T</cell><cell cols="7">: the transition function of the state. Transition function defines the state transition brought by the action of agent.</cell></row><row><cell cols="3">That is,</cell><cell>T :</cell><cell>S</cell><cell cols="2">A × →</cell><cell>S</cell><cell>,</cell><cell>(s , a ) s t+1 = T t t</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="11,67.50,93.13,488.73,617.78"><head>Table 1 .</head><label>1</label><figDesc>evaluation results in first 10 iterations</figDesc><table coords="11,67.50,93.13,488.73,601.28"><row><cell>Iteration</cell><cell>Run</cell><cell>CT</cell><cell>nCT</cell><cell>sDCG</cell><cell>nsDCG</cell><cell>EU</cell><cell>nEU</cell></row><row><cell></cell><cell>dqn_5_actions</cell><cell>0.4701114</cell><cell>0.4756687</cell><cell cols="2">15.4846276 0.4456217</cell><cell cols="2">11.6124865 0.3499484</cell></row><row><cell>1</cell><cell>dqn_semantic_state</cell><cell>0.4683404</cell><cell>0.4742194</cell><cell cols="2">14.8987069 0.4620971</cell><cell cols="2">11.1862801 0.3473296</cell></row><row><cell></cell><cell>galago_baseline</cell><cell>0.450791</cell><cell>0.4563555</cell><cell cols="2">13.7844676 0.4337153</cell><cell>10.248345</cell><cell>0.3431296</cell></row><row><cell></cell><cell>dqn_5_actions</cell><cell>0.2760114</cell><cell>0.5592316</cell><cell cols="2">21.6554912 0.4760824</cell><cell cols="2">17.4928539 0.5543615</cell></row><row><cell>2</cell><cell>dqn_semantic_state</cell><cell>0.2938375</cell><cell>0.5938134</cell><cell cols="2">20.9093702 0.4666493</cell><cell cols="2">16.8901066 0.5473597</cell></row><row><cell></cell><cell>galago_baseline</cell><cell>0.2722861</cell><cell>0.5507842</cell><cell cols="2">19.8558578 0.4589931</cell><cell cols="2">16.3226741 0.5476611</cell></row><row><cell></cell><cell>dqn_5_actions</cell><cell>0.1930831</cell><cell>0.5867918</cell><cell cols="2">24.8716411 0.4804548</cell><cell cols="2">20.0101208 0.6468404</cell></row><row><cell>3</cell><cell>dqn_semantic_state</cell><cell>0.2217546</cell><cell>0.6720723</cell><cell cols="2">24.1273948 0.4687206</cell><cell cols="2">20.1680049 0.6419633</cell></row><row><cell></cell><cell>galago_baseline</cell><cell>0.193695</cell><cell>0.5879702</cell><cell cols="2">22.6569676 0.4594243</cell><cell cols="2">18.6602872 0.639583</cell></row><row><cell></cell><cell>dqn_5_actions</cell><cell>0.1501568</cell><cell>0.6087354</cell><cell cols="2">26.8278014 0.4818966</cell><cell cols="2">21.1361679 0.7025435</cell></row><row><cell>4</cell><cell>dqn_semantic_state</cell><cell>0.1846151</cell><cell>0.7457376</cell><cell cols="2">25.3275609 0.4564798</cell><cell cols="2">20.7058817 0.6966453</cell></row><row><cell></cell><cell>galago_baseline</cell><cell>0.1484205</cell><cell>0.6008163</cell><cell cols="2">24.1543098 0.452551</cell><cell cols="2">18.9989242 0.6942974</cell></row><row><cell></cell><cell>dqn_5_actions</cell><cell>0.1242476</cell><cell>0.6294138</cell><cell cols="2">28.1966415 0.4836972</cell><cell cols="2">21.0581117 0.7395657</cell></row><row><cell>5</cell><cell>dqn_semantic_state</cell><cell>0.169166</cell><cell>0.8536985</cell><cell cols="2">25.8106292 0.4440257</cell><cell cols="2">20.3719695 0.7338354</cell></row><row><cell></cell><cell>galago_baseline</cell><cell>0.1211844</cell><cell>0.6132961</cell><cell cols="2">25.3420453 0.450617</cell><cell cols="2">19.1343022 0.7323573</cell></row><row><cell></cell><cell>dqn_5_actions</cell><cell>0.1114354</cell><cell>0.6770347</cell><cell cols="2">29.2426583 0.4840687</cell><cell cols="2">21.3944172 0.7682166</cell></row><row><cell>6</cell><cell>dqn_semantic_state</cell><cell>0.1589661</cell><cell>0.9625451</cell><cell cols="2">26.1356308 0.4332171</cell><cell cols="2">20.1245082 0.7620878</cell></row><row><cell></cell><cell>galago_baseline</cell><cell>0.102758</cell><cell>0.6240457</cell><cell cols="2">26.5015414 0.451918</cell><cell cols="2">19.1694004 0.7608299</cell></row><row><cell></cell><cell>dqn_5_actions</cell><cell>0.1014448</cell><cell>0.7187933</cell><cell cols="2">29.9681515 0.4836611</cell><cell cols="2">21.2873622 0.7899146</cell></row><row><cell>7</cell><cell>dqn_semantic_state</cell><cell>0.1532954</cell><cell>1.0830598</cell><cell cols="2">26.4518386 0.425639</cell><cell cols="2">19.9831539 0.784234</cell></row><row><cell></cell><cell>galago_baseline</cell><cell>0.0892321</cell><cell>0.6321475</cell><cell cols="2">27.6992079 0.4548045</cell><cell cols="2">19.6283697 0.7840226</cell></row><row><cell></cell><cell>dqn_5_actions</cell><cell>0.094049</cell><cell>0.7613281</cell><cell cols="2">30.6252363 0.4857005</cell><cell cols="2">21.1754035 0.8075685</cell></row><row><cell>8</cell><cell>dqn_semantic_state</cell><cell>0.1496234</cell><cell>1.208232</cell><cell cols="2">26.6684806 0.4182104</cell><cell cols="2">19.6949104 0.8018887</cell></row><row><cell></cell><cell>galago_baseline</cell><cell>0.0792522</cell><cell>0.6417305</cell><cell cols="2">28.4367084 0.4564056</cell><cell cols="2">18.9915583 0.8011377</cell></row><row><cell></cell><cell>dqn_5_actions</cell><cell>0.0877909</cell><cell>0.7992842</cell><cell>31.197493</cell><cell>0.4852854</cell><cell cols="2">20.9258899 0.8220898</cell></row><row><cell>9</cell><cell>dqn_semantic_state</cell><cell>0.1475708</cell><cell>1.3405783</cell><cell cols="2">26.9146451 0.4124845</cell><cell cols="2">19.7538581 0.8171259</cell></row><row><cell></cell><cell>galago_baseline</cell><cell>0.0710383</cell><cell>0.6471881</cell><cell cols="2">29.2236247 0.4592193</cell><cell cols="2">18.7172524 0.8159354</cell></row><row><cell></cell><cell>dqn_5_actions</cell><cell>0.082483</cell><cell>0.834239</cell><cell cols="2">31.7663899 0.4850046</cell><cell cols="2">20.9282911 0.8347376</cell></row><row><cell>10</cell><cell>dqn_semantic_state</cell><cell>0.1464314</cell><cell>1.4784141</cell><cell cols="2">27.1194546 0.4107508</cell><cell cols="2">19.8255847 0.8302033</cell></row><row><cell></cell><cell>galago_baseline</cell><cell>0.0643294</cell><cell>0.6512856</cell><cell cols="2">29.6418512 0.4581143</cell><cell cols="2">17.9727267 0.8280541</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,78.12,711.13,149.07,8.78"><p>https://github.com/trec-dd/trec-dd-jig</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,78.12,687.13,165.74,8.78"><p>https://www.lemurproject.org/galago.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,78.12,699.13,135.75,8.78"><p>https://radimrehurek.com/gensim/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,78.12,711.13,61.91,8.78"><p>https://keras.io/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This research was supported by <rs type="funder">DARPA</rs> grant <rs type="grantNumber">FA8750-14-2-0226</rs> and <rs type="funder">NSF</rs> grant <rs type="grantNumber">IIS-145374</rs>. Any opinions, ndings, conclusions, or recommendations expressed in this paper are of the authors, and do not necessarily reflect those of the sponsor.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_JBYfpj7">
					<idno type="grant-number">FA8750-14-2-0226</idno>
				</org>
				<org type="funding" xml:id="_75pwbae">
					<idno type="grant-number">IIS-145374</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,86.15,102.13,436.25,8.78;10,72.00,115.63,63.85,8.78" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,159.91,102.13,268.02,8.78">The new york times annotated corpus</title>
		<author>
			<persName coords=""><forename type="first">Evan</forename><surname>Sandhaus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,434.95,102.13,49.97,8.78">Philadelphia</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">26752</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>Linguistic Data Consortium</note>
</biblStruct>

<biblStruct coords="10,86.15,129.13,412.44,8.78;10,72.00,142.63,464.88,8.78;10,72.00,156.13,269.85,8.78" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,332.59,129.13,166.00,8.78;10,72.00,142.63,209.52,8.78">The water filling model and the cube test: multi-dimensional evaluation for professional search</title>
		<author>
			<persName coords=""><forename type="first">Jiyun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Wing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marti</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,302.87,142.63,234.01,8.78;10,72.00,156.13,158.81,8.78">Proceedings of the 22nd ACM international conference on Information &amp; Knowledge Management</title>
		<meeting>the 22nd ACM international conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="709" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,86.15,169.63,426.84,8.78;10,72.00,183.13,365.52,8.78" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,378.79,169.63,134.20,8.78;10,72.00,183.13,160.84,8.78">Discounted cumulated gain based evaluation of multiple-query IR sessions</title>
		<author>
			<persName coords=""><forename type="first">Kalervo</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Susan</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lois</forename><surname>Delcambre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marianne</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,243.21,183.13,139.07,8.78">Advances in Information Retrieval</title>
		<imprint>
			<biblScope unit="page" from="4" to="15" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,86.15,196.63,435.71,8.78;10,72.00,210.13,404.45,8.78" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,240.29,196.63,263.70,8.78">Modeling expected utility of multi-session information distillation</title>
		<author>
			<persName coords=""><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abhimanyu</forename><surname>Lad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,72.00,210.13,203.43,8.78">Conference on the Theory of Information Retrieval</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="164" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,86.15,223.63,443.31,8.78;10,72.00,237.13,458.05,8.78;10,72.00,250.63,19.99,8.78" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,238.70,223.63,271.56,8.78">Investigating per Topic Upper Bound for Session Search Evaluation</title>
		<author>
			<persName coords=""><forename type="first">Zhiwen</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Grace</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,72.00,237.13,373.05,8.78">Proceedings of the ACM SIGIR International Conference on Theory of Information Retrieval</title>
		<meeting>the ACM SIGIR International Conference on Theory of Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="185" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,86.15,264.13,440.03,8.78;10,72.00,277.63,68.85,8.78" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="10,255.51,264.13,162.05,8.78">Reinforcement learning: An introduction</title>
		<author>
			<persName coords=""><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT press</publisher>
			<biblScope unit="volume">1</biblScope>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,86.15,291.13,388.18,8.78" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="10,130.06,291.13,168.95,8.78">Deep reinforcement learning: An overview</title>
		<author>
			<persName coords=""><forename type="first">Yuxi</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07274</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,86.15,304.63,435.58,8.78;10,72.00,318.13,434.29,8.78" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,128.64,318.13,211.07,8.78">Mastering the game of Go without human knowledge</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,351.38,318.13,27.20,8.78">Nature</title>
		<imprint>
			<biblScope unit="volume">550</biblScope>
			<biblScope unit="issue">7676</biblScope>
			<biblScope unit="page" from="354" to="359" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,86.15,331.63,441.69,8.78;10,72.00,345.13,452.57,8.78" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,129.42,345.13,229.87,8.78">Human-level control through deep reinforcement learning</title>
		<author>
			<persName coords=""><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Koray</forename><surname>Volodymyr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joel</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,369.66,345.13,27.20,8.78">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,91.15,358.63,280.94,8.78" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,146.40,358.63,106.68,8.78">Markov decision processes</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><forename type="middle">R</forename><surname>Thie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,259.67,358.63,26.15,8.78">Comap</title>
		<imprint>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
	<note>Incorporated</note>
</biblStruct>

<biblStruct coords="10,91.15,372.13,436.08,8.78;10,72.00,385.63,418.36,8.78" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,244.93,372.13,200.77,8.78">A Contextual Bandit Approach to Dynamic Search</title>
		<author>
			<persName coords=""><forename type="first">Angela</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Grace</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,467.55,372.13,59.68,8.78;10,72.00,385.63,310.87,8.78">Proceedings of the ACM SIGIR International Conference on Theory of Information Retrieval</title>
		<meeting>the ACM SIGIR International Conference on Theory of Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="301" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,91.15,399.13,431.32,8.78;10,72.00,412.63,424.94,8.78;10,72.00,426.13,70.80,8.78" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,403.67,399.13,118.80,8.78;10,72.00,412.63,179.66,8.78">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName coords=""><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,273.19,412.63,204.49,8.78">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,91.15,439.63,435.40,8.78;10,72.00,453.13,45.81,8.78" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,273.30,439.63,96.70,8.78">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,382.47,439.63,79.67,8.78">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,91.15,466.63,419.57,8.78" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,277.14,466.63,41.51,8.78">Q-learning</title>
		<author>
			<persName coords=""><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jch</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,329.45,466.63,70.22,8.78">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="279" to="292" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
