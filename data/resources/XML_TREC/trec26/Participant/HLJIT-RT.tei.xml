<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,150.86,83.07,293.76,12.64">HLJIT at TREC 2017 Real-Time Summarization</title>
				<funder ref="#_kJTJn4c">
					<orgName type="full">Social Science Foundation of Heilongjiang Province</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,133.82,107.33,71.14,10.04"><forename type="first">Zhongyuan</forename><surname>Han</surname></persName>
							<email>hanzhongyuan@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Heilongjiang Institute of Technology Harbin</orgName>
								<address>
									<postCode>150050</postCode>
									<settlement>Heilongjiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,219.17,107.33,35.11,10.04"><forename type="first">Song</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Heilongjiang Institute of Technology Harbin</orgName>
								<address>
									<postCode>150050</postCode>
									<settlement>Heilongjiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Engineering University Harbin</orgName>
								<address>
									<postCode>150001</postCode>
									<settlement>Heilongjiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,268.73,107.33,52.90,10.04"><forename type="first">Leilei</forename><surname>Kong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Heilongjiang Institute of Technology Harbin</orgName>
								<address>
									<postCode>150050</postCode>
									<settlement>Heilongjiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,330.55,107.33,59.14,10.04"><forename type="first">Liuyang</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Heilongjiang Institute of Technology Harbin</orgName>
								<address>
									<postCode>150050</postCode>
									<settlement>Heilongjiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Engineering University Harbin</orgName>
								<address>
									<postCode>150001</postCode>
									<settlement>Heilongjiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,403.99,107.33,53.98,10.04"><forename type="first">Haoliang</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Heilongjiang Institute of Technology Harbin</orgName>
								<address>
									<postCode>150050</postCode>
									<settlement>Heilongjiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,150.86,83.07,293.76,12.64">HLJIT at TREC 2017 Real-Time Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">446F7B7AD946F33C0CF6DF019E40B8F1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the approaches used at the TREC 2017 Real-Time Summarization. This task contains two scenarios: push notifications and email digest. For the scenario of push notifications, three filtering models, which are based on the hyperlink-extended retrieval model, the Learning to Rank and the hybrid filtering model, are proposed to filter the relevant tweets for a given topic. A novelty verification method is given for further filter the tweets for push notification. For the scenario of email digest, three ranking models, the hyperlink-extended retrieval model, the retrieval model based on learning to rank, and the personal retrieval model, are presented to rank the relevant tweets. Similarly, a novelty verification is proposed for filtering the redundant tweets. The evaluation results of TREC 2017 Real-Time Summarization show that the performance of our models is competitive.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The evaluation of TREC 2017 Real-Time Summarization contains two scenarios: push notifications (Scenario A) and email digest (Scenario B). Scenario A requires pushing the relevant tweets that concentrating on the different aspects of one thing in real-time, while Scenario B identifies a batch of up to 100 ranked tweets per day for per interest profile. It is expected that the systems have the abilities of computing the results in a relatively short time after the day ends on the condition of not using the future evidences.</p><p>Focused on the problem of Real-Time Summarization, three filtering models based on retrieval models or online classification models are developed to decide the relevant tweets in Scenario A, while the Scenario B is viewed as a retrieval task and three different retrieval models were exploited for email digest.</p><p>This paper is organized as follows: Section 2 introduces our methods for Scenario A. Section 3 depicts the detailed methods applied in Scenario B. Section 4 reports experimental setting and results. And Section 5 gives the conclusion. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Scenario A: Push Notifications</head><p>For the task of Push Notifications, we present three filtering models: the filtering model based on the hyperlink-extended retrieval model (denoted as HLJIT_testRun1_06), the filtering model based on Learning to Rank (denoted as HLJIT_testRun2_07) and the hybrid filtering model incorporating the retrieval model and the classification model (denoted as HLJIT_testRun3_08). The first two models exploit the idea of retrieval models and the third one adopts the idea of online classification model. The tweets within a certain period is firstly ranked by the proposed filtering model, then the sorted tweets are further filtered using a novelty verification method. Additionally, the assessment by the mobile assessors are only considered by the proposed hybrid filtering model. In this section, we first describe the filtering models, then introduce the method of novelty verification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Filtering model based on the hyperlink-extended retrieval model</head><p>The first filtering model we adopt is the hyperlink-extended retrieval model. In microblog retrieval, the content linked by the URLs provided more important information for a microblog. The proposed hyperlink-extended model combines the content of microblogs and the embedded hyperlinks webpages using a ranking function based on language model <ref type="bibr" coords="2,392.59,332.79,8.08,6.33" target="#b0">[1]</ref> .</p><p>Given a tweet D and a topic T, the relevant score is calculated as follow:</p><formula xml:id="formula_0" coords="2,144.93,369.40,360.54,10.52">) ( ) ( ) ( ) ( ) ( URL T URL text T text ,Θ Θ QL |T T P ,Θ Θ QL |T T P T,D LM  <label>(1)</label></formula><p>where P(T text |T) denotes the probability that the user's information needs might be satisfied by the microblog's text, while P(T URL |T) denotes the probability that the user's information needs are more likely to meet by the hyperlink documents. Then the similarity of the language model of topic and the language model of document is estimated by using the following equation:</p><p>(</p><p>where T  and D  are language model of topic and document respectively, qi is the word in topic.</p><p>The language model of topic T  is estimated by integrating the title field and the description field in each topic as follows:</p><p>(3) Ttitle and Tdescription are the text in title field and description field respectively. P(w|Ttitle) and P(w|Tdescription) are the probabilities that the term w occurs within the title or the description by using the Maximum Likelihood Estimation.</p><p>The document model D  is estimated using Maximum Likelihood Estimation and considers unseen words through probability smoothing by using Dirichlet smoothing method <ref type="bibr" coords="2,451.30,619.28,8.08,6.33" target="#b1">[2]</ref> , shown in Eq. ( <ref type="formula" coords="2,110.85,632.26,3.79,9.60" target="#formula_2">4</ref>):</p><formula xml:id="formula_2" coords="2,196.69,647.17,308.77,21.20">( ; ) ( | ) = ( | )= || D c w D P w C P w D D     <label>(4)</label></formula><p>where c(w,D) is the term frequency of w in document D, P(w|C) is the probability of w in corpus C, and |D| is the total number of words in the document D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Filtering model based on Learning to Rank</head><p>The second filtering model is performed based on learning to rank algorithm. The algorithm of ListNet <ref type="bibr" coords="2,134.18,746.59,8.08,6.33" target="#b2">[3]</ref> is adopted for ranking the tweets. We rely on the features of text similarity and the</p><formula xml:id="formula_3" coords="3,184.37,464.66,148.76,28.44">(1 ) ( ) ( ( , ) ) (1 ) ( ) ( ( , ) ) () 1 wx b LM T D T wx b LM T D e hD e                  </formula><p>scores of language models for ListNet model. All features are presented in Table <ref type="table" coords="3,430.51,76.32,3.96,9.60" target="#tab_0">1</ref>.  Then the evaluation metric MAP is selected to optimize on training data, and the Gradient Descent is used to update the parameters of model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">The hybrid filtering model</head><p>The third filtering model is a hybrid model adopted to estimate the relevance between the topic T and the Tweet D. The hybrid model incorporates the retrieval model (language model) and the model (logistic regression), and uses the retrieval model as prior knowledge to revise the hyper plane of classification. Specifically, we built a relevance estimation model hT(D) for each topic: (5)   where δ is a controlling parameter, x is the term vector of tweet D, w is the weight vector, b is the bias, LM(T,D) is the similarity score and γ is a threshold computed by Eq.( <ref type="formula" coords="3,401.57,513.19,4.23,9.60" target="#formula_4">6</ref>):</p><formula xml:id="formula_4" coords="3,210.64,528.14,294.81,11.57">( ) _ ( ( )) t find kth NRM t  <label>(6)</label></formula><p>where NRM(t) is the number of the relevant tweets at the time t and find_kth(•) is a function returning the k-th max similarity score. We set the k=100 in this task.</p><p>The tweet D will be judged as the relevant tweet if hT(D)&gt;0.5, and the relevant tweets accumulated within half a day are ranked according to the score of hT(D). The online filtering model is updated according to assessment by the assessors. The updating details were described in REF. [4].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Novelty Verification for Push Notifications</head><p>For guaranteeing the pushed tweets not talking about the same thing, we perform a novelty Verification.</p><p>Novelty Verification uses Cosine similarity to check the novelty. Specifically, the Cosine similarity is used to compare candidate tweets from the filtering models mentioned above sequentially with those in push pool.</p><p>For HLJIT_testRun1_06 and HLJIT_testRun2_07, only the first tweet having a similarity score lower than a certain threshold (0.7 is used in our method) is viewed as the valuable one. The novelty tweet at the top of the list will be pushed. If there is no any satisfied tweet, then the top1-ranked tweet will be pushed.</p><p>For HLJIT_testRun3_08, the push number K is set as half of N (the number of relevant tweets in the current list) if N&lt;10. Otherwise, the K is set by the zoom logistic function which maps N into [1,10]. Then the Tweet will be pushed if it has passed the Novelty Verification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Scenario B: Email Digest</head><p>We regard the task of Email Digest as a problem of relevant tweet retrieval and propose three models, the hyperlink-extended retrieval model (denoted as qFB_url), the model based on learning to rank (denoted as HLJIT_l2r), and the personal retrieval model (denoted as HLJIT_rank_svm), to rank the relevant tweets. Similarly, a novelty verification is operated on the ranking list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The hyperlink-extended retrieval model (qFB_url)</head><p>A retrieval model based on hyperlink-extended model described in 2.1 is exploited to rank the relevant tweet. The difference is that we use the Relevance Model <ref type="bibr" coords="4,409.03,293.79,8.08,6.33" target="#b4">[5]</ref> for query language modeling. The 50 top-ranked feedback documents searched by Google search engine are used to query expansion. We select top 10 tweets posted in a day as the relevant ones and send them to the Novelty Verification in batch after the day ends. The model for a given topic is estimated by Eq. ( <ref type="formula" coords="4,94.12,357.17,4.09,9.60" target="#formula_5">7</ref>)</p><formula xml:id="formula_5" coords="4,169.50,379.10,335.96,12.64">title ( | ) (1 ) ( | ) ( | ) R TR P w T P w T P w T      <label>(7)</label></formula><p>where P(w|TR) is estimated by Relevance Model described in Ref. [5].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The retrieval model based on Learning to Rank (HLJIT_l2r)</head><p>The learning to rank method based on ListNet is also used in the task of Email Digest. On the basis of the ranking model described in 2.2, two new features, Additionally, all the tweets that has been judged as relevant ones by the mobile RTS evaluation broker are sent to the novelty verification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The personal retrieval model (HLJIT_rank_svm)</head><p>In this scheme, the assessment by the mobile assessors are regarded as a person's feedback, which represent the user's interest, and RankSVM <ref type="bibr" coords="4,304.61,569.69,8.08,6.33" target="#b5">[6]</ref> is used to learn the personal retrieval model. For each topic, we exploit the assessment by the mobile assessors to train a ranking model. The pairwise-based RankSVM is adopted as the learning algorithm, while the terms in tweets are selected as the features.</p><p>Additionally, all the tweets that has been judged as relevant ones by the mobile RTS evaluation broker are sent to the Novelty Verification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Novelty Verification for Email Digest</head><p>The similarity estimation in Novelty Verification for Email Digest also adopts the Cosine similarity.</p><p>In qFB_url, tweets in the list ranked by Relevant Tweet Retrieval are checked by a novelty verification sequentially, all the tweets that go through the novelty verification are pushed to the RTS evaluation broker.</p><p>In HLJIT_l2r and HLJIT_rank_svm, we select K novel tweets sequentially and push them to the RTS evaluation broker. K is determined according to the assessment by the mobile assessors from the RTS evaluation broker on that day. If there are any relevant tweets in the assessment, K=10, otherwise, K is set a value in [3,5] according the number of the irrelevant tweets in the assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Set</head><p>We download 38,199,201 tweets by using the official API to listen the tweets stream. In these tweets, we get 29,255,621 effective tweets which contains 8,962,062 tweets written in English. Then, total 3,596,304 tweets are remained after the following processing. Firstly, the trash tweets are abandoned according to the following rules proposed in [7,8].</p><p>1) The number of ASIIC characters (0-128) is less than 80%;</p><p>2) The length of text is less than 20 (characters);</p><p>3) The number of HashTags is more than 4; 4) Non-English characters are more than 35%. Secondly, further preprocessing operation are performed according to the following rules. 1) Only the tweets which contain at least one word in the topic title field is selected; 2) RT tag, user_mentions and stop words are removed from tweet text; 3) Porter stemming are used.</p><p>4) The webpage of the URL is downloaded;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Parameters setting</head><p>All the parameters used in the proposed models are showed in Table <ref type="table" coords="5,400.15,442.99,3.90,9.60" target="#tab_1">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Results</head><p>Table <ref type="table" coords="5,137.90,704.38,5.28,9.60" target="#tab_2">3</ref> shows the experimental results of scenario A runs by the mobile assessors, Table <ref type="table" coords="5,500.26,704.38,5.28,9.60" target="#tab_3">4</ref> shows the experimental results of scenario A runs by NIST assessors, Table <ref type="table" coords="5,413.23,719.98,5.28,9.60" target="#tab_4">5</ref> shows the results of scenario B runs by NIST assessors. In the table 5, the language model with original query and tweet(LM), the language model with query expansion and original tweet(qFB), and the hyperlink-extended model with original query (URL) are reported to show the effect of query expansion and document expansion with hyperlink-extended. From table 3, it can be seen that the HLJIT-testRun3-08 found the most relevant microblog, but the number of irrelevant microblogs is much more than HLJIT-testRun2-07. Relatively, HLJIT-testRun2-07 is better. The result of table 4 also cites this. The one-side feedback makes the low number of the relevant documents returned by mobile assessors. This may be the reason that the HLJIT-testRun3-08 does not fully play its role.</p><p>As can be seen from table <ref type="table" coords="6,228.65,532.63,3.90,9.60" target="#tab_4">5</ref>, the qFB_url has the highest score, which indicates that query expansion and URL information alleviate the problem of short text matching and achieve better performance. The HLJIT_rank_svm score is the lowest, which may be due to the fact that the number of relevant tweets in the training set(returned by the mobile assessors) is too small to learn an efficient ranking model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have introduced the key aspects of the proposed models for TREC 2017 Real-Time Summarization task.</p><p>Three filtering models have been proposed for the scenarios of push notifications. The content linked by the URLs is attempted to estimate the similarity of the topic and the document. The model based on learning to rank is also considered in the proposed models. Combining with the novelty verification strategies, the model based on the ListNet achieved 0.363 on the main measure metrics EG-p (the highest EG-p score of the proposed three models). The online filtering model is also used for the scenarios of push notifications and the assessment by the mobile assessors are used to update the filtering model.</p><p>For the scenarios of email digest, we deem it as a retrieval task and three ranking-based methods are attempted. The model based on hyperlink-extended retrieval model achieved the highest nDCG@10-p for the richer extended content.</p><p>From the experimental results, it is obvious that vocabulary mismatch is the main problem for the short query and short document. Query expansion and hyperlink-extended model have the obvious effect on improving the performance and achieved the best results. Although the hybrid filtering model and the personal retrieval model did not get good results in the evaluation, we still believe that they have a certain potential. The future work will be further explored how to use the feedback in these two aspects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,161.53,142.55,3.21,8.34;3,215.83,142.55,2.41,8.34;3,234.49,142.55,3.21,8.34;3,131.68,148.88,37.87,1.00;3,187.32,148.88,5.36,1.00;3,219.23,148.88,6.96,1.00;3,179.31,142.46,7.40,8.42;3,290.57,144.14,9.06,8.18;3,323.68,143.52,22.18,8.38;3,309.64,149.87,33.30,1.00;3,352.39,143.53,89.94,8.19;3,112.46,160.22,4.50,8.18;3,168.82,163.70,67.47,4.85;3,161.54,158.65,3.19,8.34;3,215.87,158.65,2.40,8.34;3,237.03,158.65,3.19,8.34;3,131.70,164.98,37.86,1.00;3,187.37,164.98,5.33,1.00;3,219.30,164.98,6.92,1.00;3,179.34,158.56,7.37,8.42;3,290.57,160.22,9.06,8.18;3,323.68,159.62,22.18,8.38;3,309.64,165.97,33.30,1.00;3,352.39,159.61,89.94,8.19;3,112.46,177.62,4.50,8.18;3,168.82,181.10,41.76,4.85;3,161.66,176.05,3.18,8.34;3,192.96,176.05,2.39,8.34;3,211.68,176.05,3.18,8.34;3,131.69,182.38,37.95,1.00;3,196.38,182.38,6.90,1.00;3,290.57,177.62,9.06,8.18;3,323.68,177.02,22.18,8.38;3,309.64,183.37,33.30,1.00;3,352.39,177.01,96.66,8.19;3,112.46,196.22,4.50,8.18;3,168.53,199.70,23.12,4.85;3,161.41,194.65,3.19,8.34;3,192.48,194.65,2.39,8.34;3,212.43,194.65,3.19,8.34;3,201.65,203.38,9.99,1.00;3,131.69,200.98,37.75,1.00;3,195.94,200.98,6.91,1.00;3,290.57,196.22,9.06,8.18;3,323.68,195.62,22.18,8.38;3,309.64,201.97,33.30,1.00;3,352.39,195.61,96.66,8.19;3,112.46,213.62,4.50,8.18;3,152.60,217.40,26.53,4.60;3,131.15,212.61,32.64,7.94;3,180.28,212.61,3.06,7.94;3,147.98,218.64,23.64,1.00;3,290.57,213.62,9.06,8.18;3,323.68,213.02,22.18,8.38;3,309.64,219.37,33.30,1.00;3,352.39,213.01,89.94,8.19;3,112.46,229.70,4.50,8.18;3,152.29,233.50,8.01,4.60;3,131.13,228.71,32.25,7.94;3,180.96,228.71,3.06,7.94;3,170.19,236.99,9.82,1.00;3,147.75,234.74,23.40,1.00;3,290.57,229.70,9.06,8.18;3,323.68,229.12,22.18,8.38;3,309.64,235.47,33.30,1.00;3,352.39,229.09,89.94,8.19;3,112.46,244.76,47.22,9.20;3,180.99,244.76,11.40,7.94;3,218.01,244.76,2.33,7.94;3,236.24,244.76,3.10,7.94;3,163.51,253.04,8.16,1.00;3,196.39,253.04,38.65,1.00;3,159.13,250.79,5.17,1.00;3,191.84,250.79,5.17,1.00;3,221.44,250.79,6.72,1.00;3,174.25,244.67,5.11,8.02;3,290.57,245.78,188.06,8.18;3,112.46,262.31,47.22,9.20;3,181.00,262.31,11.41,7.94;3,218.02,262.31,2.33,7.94;3,238.04,262.31,3.10,7.94;3,163.51,270.59,8.16,1.00;3,196.40,270.59,40.71,1.00;3,159.13,268.34,5.17,1.00;3,191.85,268.34,5.17,1.00;3,221.46,268.34,6.72,1.00;3,174.23,262.22,5.11,8.02;3,90.02,295.96,415.63,9.61;3,90.02,311.56,214.59,9.61"><head></head><label></label><figDesc>Jaccard (T, D) is the Jaccard coefficient of T and D, Cos(T, D) is the Cosine similarity of T and D, and LM(T, D) is the score defined in Eq.(1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,382.52,457.81,18.72,8.36;4,417.31,457.81,3.18,8.36;4,394.88,467.90,2.43,1.00;4,392.20,466.56,23.98,1.00;4,370.90,464.16,11.75,1.00;4,385.61,457.72,23.36,8.45;4,427.39,456.79,15.23,9.60;4,460.96,457.80,18.72,8.34;4,497.53,457.80,3.17,8.34;4,473.32,467.87,2.42,1.00;4,470.64,466.53,26.07,1.00;4,449.36,464.13,11.73,1.00;4,464.06,457.71,23.36,8.42;4,502.66,456.79,2.64,9.60;4,90.02,474.42,415.40,9.61;4,90.02,488.23,215.34,9.60"><head></head><label></label><figDesc>the proposed ranking model, where QL() is the score of language model which taking the query expansion into account (described in 3.1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,112.46,92.42,280.87,60.03"><head>Table 1 .</head><label>1</label><figDesc>Features used for learning to rank algorithm</figDesc><table coords="3,112.46,108.61,266.31,43.84"><row><cell>1</cell><cell cols="2">title ( , Jaccard T D text</cell><cell cols="2">)</cell><cell>11</cell><cell>description TD text Cos( ,</cell><cell>)</cell></row><row><cell>2</cell><cell cols="3">title ( , Jaccard T D URL</cell><cell>)</cell><cell>12</cell><cell>description TD URL Cos( ,</cell><cell>)</cell></row><row><cell>3</cell><cell>title</cell><cell cols="3">description</cell><cell>text</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,102.50,459.07,391.47,198.89"><head>Table 2 .</head><label>2</label><figDesc>Parameters setting</figDesc><table coords="5,102.50,475.30,391.47,182.67"><row><cell>Run Id</cell><cell>Model</cell><cell>Parameters</cell></row><row><cell>Scenario A testRun1</cell><cell>LM</cell><cell>μ=100, P(TURL|T)=0.1, α=0.5, period=1 day</cell></row><row><cell>testRun2</cell><cell>LISTNET</cell><cell>Learning rate=0.01, the number of epochs to train=100,</cell></row><row><cell></cell><cell></cell><cell>the number of hidden layers=1, the number of hidden</cell></row><row><cell></cell><cell></cell><cell>nodes per layer=10, Metric to optimize on the training</cell></row><row><cell></cell><cell></cell><cell>data=MAP, period=1 day</cell></row><row><cell>testRun3</cell><cell>LR</cell><cell>γ=0.5</cell></row><row><cell></cell><cell></cell><cell>The number of epochs to train=50</cell></row><row><cell></cell><cell></cell><cell>Learning rate= 0.005, period=half day</cell></row><row><cell>Scenario B qFB_url</cell><cell>LM-FB</cell><cell>μ=100, P(TURL|T)=0.2, β=0.7, fbTermNum=20</cell></row><row><cell>HLJIT_l2r</cell><cell>LISTNET</cell><cell>Same as testRun2</cell></row><row><cell>HLJIT_rank_svm</cell><cell>RANK SVM</cell><cell>c=2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,162.50,109.46,277.92,77.78"><head>Table 3 .</head><label>3</label><figDesc>Evaluation of scenario A runs by the mobile assessors.</figDesc><table coords="6,162.50,129.17,277.92,58.08"><row><cell>run</cell><cell cols="4">relevant redundant Not relevant unjudged</cell></row><row><cell>HLJIT-testRun1-06</cell><cell>847</cell><cell>173</cell><cell>1479</cell><cell>153</cell></row><row><cell>HLJIT-testRun2-07</cell><cell>1018</cell><cell>178</cell><cell>1494</cell><cell>106</cell></row><row><cell>HLJIT-testRun3-08</cell><cell>1027</cell><cell>196</cell><cell>1694</cell><cell>168</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,160.94,211.46,275.12,78.65"><head>Table 4 .</head><label>4</label><figDesc>Evaluation of scenario A runs by NIST assessors</figDesc><table coords="6,160.94,229.25,275.12,60.87"><row><cell>runtag</cell><cell>EGp</cell><cell>EG1</cell><cell>nCGp</cell><cell>nCG1</cell></row><row><cell>HLJIT-testRun1-06</cell><cell>0.3318</cell><cell>0.1811</cell><cell>0.261</cell><cell>0.1102</cell></row><row><cell>HLJIT-testRun2-07</cell><cell>0.363</cell><cell>0.2088</cell><cell>0.2808</cell><cell>0.1266</cell></row><row><cell>HLJIT-testRun3-08</cell><cell>0.2426</cell><cell>0.1832</cell><cell>0.242</cell><cell>0.1826</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,193.61,315.05,207.97,131.42"><head>Table 5 .</head><label>5</label><figDesc>Evaluation of scenario B runs by NIST assessors</figDesc><table coords="6,203.33,332.84,183.92,113.64"><row><cell>runtag</cell><cell>nDCGp</cell><cell>nDCG1</cell></row><row><cell>LM</cell><cell>0.2906</cell><cell>0.2289</cell></row><row><cell>qFB</cell><cell>0.3267</cell><cell>0.2651</cell></row><row><cell>URL</cell><cell>0.3283</cell><cell>0.2725</cell></row><row><cell>qFB_url</cell><cell>0.3501</cell><cell>0.291</cell></row><row><cell>HLJIT_rank_svm</cell><cell>0.2697</cell><cell>0.2376</cell></row><row><cell>HLJIT_l2r</cell><cell>0.3107</cell><cell>0.2778</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This work is supported by the <rs type="funder">Social Science Foundation of Heilongjiang Province</rs> (No.<rs type="grantNumber">16XWB02</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_kJTJn4c">
					<idno type="grant-number">16XWB02</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,111.02,380.57,394.40,9.60;7,111.26,396.17,377.60,9.60" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,278.57,380.57,226.85,9.60;7,111.26,396.17,44.98,9.60">A hyperlink-extended language model for microblog retrieval[J]</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,162.62,396.17,240.24,9.60">International Journal of Database Theory and Application</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="89" to="100" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,111.02,411.77,394.54,9.60;7,111.26,427.37,141.75,9.60" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,241.01,411.77,182.90,9.60">A hierarchical Dirichlet language model[J]</title>
		<author>
			<persName coords=""><forename type="first">D J C</forename><surname>Mackay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L C B</forename><surname>Peto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,432.07,411.77,73.49,9.60;7,111.26,427.37,47.96,9.60">Natural language engineering</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="289" to="308" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,111.02,442.99,394.52,9.60;7,111.26,458.59,150.39,9.60" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="7,305.09,442.99,200.45,9.60;7,111.26,458.59,76.35,9.60">Learning to Rank: From Pairwise Approach to Listwise Approach</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,111.02,474.19,394.50,9.60;7,111.26,489.79,223.61,9.60" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,274.01,474.19,227.67,9.60">A Hybrid Model for Microblog Real-time Filtering[J]</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,111.26,489.79,127.17,9.60">Chinese Journal of Electronics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="432" to="440" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,111.02,505.39,394.52,9.60;7,111.26,520.99,254.93,9.60" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,244.61,505.39,145.10,9.60">Relevance-based language models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,398.23,505.39,107.31,9.60;7,111.26,520.99,186.66,9.60">Proceedings of the 24 th annual international ACM SIGIR conference</title>
		<meeting>the 24 th annual international ACM SIGIR conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,111.02,536.59,254.43,9.60" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="7,173.42,536.59,174.40,9.60">Training linear SVMs in linear time</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,373.03,536.59,132.46,9.60;7,111.26,552.19,327.65,9.60" xml:id="b6">
	<monogr>
		<title level="m" coord="7,373.03,536.59,132.46,9.60;7,111.26,552.19,230.07,9.60">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,111.02,567.79,394.40,9.60;7,111.26,583.39,394.32,9.60;7,111.26,598.99,105.51,9.60" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,343.87,567.79,161.55,9.60;7,111.26,583.39,75.23,9.60">University of Waterloo at TREC 2015 Microblog Track</title>
		<author>
			<persName coords=""><forename type="first">Luchen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roegiest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,216.65,583.39,288.93,9.60;7,111.26,598.99,49.63,9.60">the Twenty-Fourth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>TREC 2015) Proceedings</note>
</biblStruct>

<biblStruct coords="7,111.02,614.59,394.29,9.60;7,111.26,630.22,280.61,9.60" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,208.01,614.59,204.42,9.60">PolyU at TREC 2016 Real-time Summarization</title>
		<author>
			<persName coords=""><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,433.39,614.59,71.92,9.60;7,111.26,630.22,251.83,9.60">the Twenty-Fifth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>TREC 2016) Proceedings. NIST</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
