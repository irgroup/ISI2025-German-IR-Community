<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,111.18,88.06,383.94,12.62;1,182.06,105.99,235.95,12.62">Some thoughts from IRIT about the scenario A of the TREC RTS 2016 and 2017 tracks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,140.18,143.66,56.76,8.74"><forename type="first">Gilles</forename><surname>Hubert</surname></persName>
							<email>gilles.hubert@irit.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">Université de Toulouse UPS-IRIT</orgName>
								<address>
									<addrLine>118 route de Narbonne</addrLine>
									<postCode>F-31062</postCode>
									<settlement>Toulouse cedex 9</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,205.08,143.66,66.31,8.74"><forename type="first">Jose</forename><forename type="middle">G</forename><surname>Moreno</surname></persName>
							<email>jose.moreno@irit.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">Université de Toulouse UPS-IRIT</orgName>
								<address>
									<addrLine>118 route de Narbonne</addrLine>
									<postCode>F-31062</postCode>
									<settlement>Toulouse cedex 9</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,279.81,143.66,100.63,8.74"><forename type="first">Karen</forename><surname>Pinel-Sauvagnat</surname></persName>
							<email>karen.sauvagnat@irit.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">Université de Toulouse UPS-IRIT</orgName>
								<address>
									<addrLine>118 route de Narbonne</addrLine>
									<postCode>F-31062</postCode>
									<settlement>Toulouse cedex 9</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,403.14,143.66,62.99,8.74"><forename type="first">Yoann</forename><surname>Pitarch</surname></persName>
							<email>yoann.pitarch@irit.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">Université de Toulouse UPS-IRIT</orgName>
								<address>
									<addrLine>118 route de Narbonne</addrLine>
									<postCode>F-31062</postCode>
									<settlement>Toulouse cedex 9</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,111.18,88.06,383.94,12.62;1,182.06,105.99,235.95,12.62">Some thoughts from IRIT about the scenario A of the TREC RTS 2016 and 2017 tracks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2FB2AB1D59E4CA935193B12FB9BB11CF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The TREC Real-Time Summarization (RTS) track provides a framework for evaluating systems monitoring the Twitter stream and pushing tweets to users according to given profiles. It includes metrics, files, settings and hypothesis provided by the organizers. In this work, we perform a thorough analysis of each component of the framework used for batch evaluation of scenario A in 2016 and 2017. We found some weaknesses of the metrics and took advantage of these limitations to submit our official run in the 2017 edition. The good evaluation results validate our findings. This paper also gives clear recommendations to fairly reuse the collection. This is an extended version of our paper "Everything You Always Wanted to Know About TREC RTS* (*But Were Afraid to Ask)" [3].</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A common usage of Twitter is to watch other users' tweets and never post anything. This usage scenario considers Twitter as a real time information source by scanning incessantly the tweet stream. The users adopting this usage aim to catch new (information they did not hear about before), fresh (information that appeared very recently) and precise (information that concerns them) information. There is growing interest in systems that could address these issues by providing information that satisfy this type of users with respect to their information needs.</p><p>The TREC campaign took an interest in the evaluation of such systems through various tracks and notably the ongoing Real-Time Summarization (RTS) track. As usual in information retrieval and evaluation campaigns, the researchers who tackle the issues on which focuses a track test their approaches using the framework provided for the track during the campaign period. Many researchers also test their solutions using the framework after the campaign period.</p><p>Our participations to these successive tracks have motivated a thorough analysis of the provided evaluation frameworks. This paper presents our main findings about the batch evaluation of the scenario A of the 2016 and 2017 benchmarks. On the one hand, it highlights some limitations of the provided evaluation framework with respect to the organizers' settings. On the other hand, it identifies precautions to take when reusing the evaluation framework after the campaign period. Leaving aside these precautions would lead to erroneous evaluation results and invalidate conclusions on system performance.</p><p>The remainder of this paper is organized as follows. Section 2 presents an overview of the TREC RTS track while Section 3 describes the metrics defined for the evaluations of systems corresponding to the track scenario A. The limitations highlighted on the evaluation framework are discussed in Sections 5 and 6. Section 8 introduces the precautions to take to obtain valid results when reusing the evaluation framework after the TREC campaign. Finally, Section 9 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview of the TREC RTS Scenario A</head><p>Introduced in 2016 and continued in 2017, the RTS track merges some previous TREC tracks: the Microblog (MB) track ran from 2010 to 2015 and the Temporal Summarization (TS) track run from 2013 to 2015. It intends to promote the development of systems that automatically monitor a document stream to keep the user up-to-date on topics of interest, by proposing a framework to evaluate such systems. The track considers two scenarios: scenario A -Push notifications -and scenario B -Email digest. The scenario A corresponds to the systems intended to send immediately the posts identified as relevant. The scenario B corresponds to systems intended to send once a day a summarization of the relevant posts of the day. This paper sheds some light on the evaluation framework defined for the scenario A. Each participant to the task must process a publicly accessible sample provided by Twitter which corresponds to the 1 % of the total available tweets. The evaluation period is partitioned in days, making 8 or 10 days long the evaluation window. To identify relevant tweets, a set of profiles is provided. Each profile (called topic in the TREC jargon) is composed of a title, a description and a narrative of the interest profiles. Table <ref type="table" coords="2,363.62,376.17,4.98,8.74" target="#tab_0">1</ref> provides some statistics about the 2016 and 2017 tasks. Each system must push at most 10 tweets per profile per day to a central system called the broker. Note that silence of a system is a desired effect when there are no relevant tweets during a day.</p><p>Two ways of evaluation were performed: online judgments and batch judgments. The earlier was performed during the evaluation period and the latter was performed once the challenge was over. Some works studied these two ways of evaluation and showed they are correlated <ref type="bibr" coords="2,149.00,459.86,10.52,8.74" target="#b7">[8,</ref><ref type="bibr" coords="2,161.18,459.86,11.62,8.74" target="#b9">10]</ref>. This work is interested only in the latter due to the reusability problems already identified in the earlier one <ref type="bibr" coords="2,265.53,471.81,9.96,8.74" target="#b8">[9]</ref>. In order to perform the batch judgments, a pool of tweets was built using all the pushed tweets in both scenarios A and B. The combined set of tweets was annotated following a two-step methodology. Given a tweet, assessors first assigned a relevance score. In all editions of this task, three levels of relevance (not relevant, relevant and very relevant) were considered. However without loss of generality, we consider only two levels of relevance to simplify our study, i.e., relevant tweets are considered as very relevant. Then, a unique cluster 1 was assigned to each relevant tweet. A tweet is considered relevant if its content is related to one profile. The clusters were found following the Tweet Timeline Generation (TTG) approach <ref type="bibr" coords="2,271.90,567.46,15.50,8.74" target="#b10">[11]</ref> which takes into account the creation timestamp to sort relevant tweets. Tweets are examined one per one traveling from past to future. A new cluster is created if the current tweet content is substantially dissimilar to all the previous tweets seen. All clusters are then considered equally important in the evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Metrics</head><p>The RTS track in its guidelines asks for effectiveness (tweet quality) and efficiency (no latency). As participant systems might favour effectiveness or efficiency depending on their 1 A cluster can be considered as a group of tweets sharing the same semantic information.</p><p>approaches, the organizers decided from 2016 to compute metrics for quality and latency separately <ref type="bibr" coords="3,149.29,103.05,9.96,8.74" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notations and Preliminary Definition</head><p>The notations used in this paper are summarized in Table <ref type="table" coords="3,359.01,151.10,3.87,8.74" target="#tab_1">2</ref>. The creation date of the tweet t Πi(t)</p><p>The date at which the tweet t has been pushed to the user by the system Si W = {w1, . . . , wT } The set of temporal windows, i.e., the set of days considered during the evaluation campaign N</p><p>The maximum number of tweets to push per window Ti(wj)</p><p>The list of tweets published during the window wj and pushed by the system Si ordered by their Θ(t) Ri</p><p>The set of relevant tweets pushed by the system Si</p><p>A key concept for all the metrics is how relevance is defined. A tweet is considered as relevant for a system S i if it satisfies two criteria: it is contained in a relevant cluster and it is the first tweet returned by S i for this cluster. Once a tweet from a cluster has been retrieved, all the other tweets from the same cluster are redundant and automatically become not relevant <ref type="bibr" coords="3,157.26,437.79,9.96,8.74" target="#b3">[4]</ref>. This implies that the relevance of a tweet is system-dependent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Gain-Oriented Metrics</head><p>Gain. The three metrics proposed to evaluate quality are based on the concept of gain, i.e., the usefulness of a tweet in the list of the tweets pushed by the system. The way the gain is evaluated is thus decisive. Given a time window w j and T i (w j ), i.e., the tweets returned by the system S i published during w j , the gain G(w j , S i ) is evaluated as follows:</p><formula xml:id="formula_0" coords="3,251.09,544.87,253.32,20.53">G(S i , w j ) = t∈Ti(wj ) g(t)<label>(1)</label></formula><p>where g(t) is the gain of the tweet t: g(t) = 1 if the tweet is relevant, g(t) = 0 otherwise, i.e., t is non relevant or redundant. It should be noted that this definition has been clarified from <ref type="bibr" coords="3,101.89,601.44,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="3,114.07,601.44,7.75,8.74" target="#b5">6]</ref> by specifying that the tweets considered during w j are picked using Θ(t) rather than Π i (t). We now detail the official metrics that rely on the gain.</p><p>Expected gain. The expected gain metric, denoted by EG, is adapted from <ref type="bibr" coords="3,452.39,642.39,9.96,8.74" target="#b1">[2]</ref>. Given a time window w j , it is evaluated as:</p><formula xml:id="formula_1" coords="3,230.32,673.47,274.09,23.23">EG(w j , S i ) = 1 |T i (w j )| • G(S i , w j )<label>(2)</label></formula><p>where |T i (w j )| is the number of tweets returned by S i and published during w j .</p><p>An important question about this metric is how to score systems during the so-called silent days, i.e., the days where no relevant tweets are published. Some variants of the EG metric have been introduced differing on how the silent days are considered:</p><p>-EG-0 in which systems receive a gain of 0 during the silent days no matter the tweets they returned. -EG-1 in which systems receive a gain of 1 during the silent days when they do not return any tweet published during the day, 0 otherwise. It should be noted that this definition has been slightly extended from <ref type="bibr" coords="4,261.25,197.81,10.52,8.74" target="#b5">[6]</ref> to perfectly fit with the evaluation tool. This will be further discussed in Section 5. -EG-p in which the proportion of tweets returned during a silent day is considered: a system receives a score of N -|t| N , where |t| is the number of non-relevant tweets published during the day and returned by the system. For instance, if a system pushes one tweet published during the day but not relevant (instead of 0), it gets a score of 0.9; two nonrelevant tweets imply a score of 0.8, etc. Similarly to EG-1, this definition has been slightly extended.</p><p>The way the silent days are considered is crucial, since a huge impact of silent vs. eventful days is observed in the evaluation <ref type="bibr" coords="4,252.35,317.24,14.61,8.74" target="#b9">[10]</ref>.</p><p>Normalized Cumulative Gain. Given a time window w j , the nCG metric is evaluated as follows:</p><formula xml:id="formula_2" coords="4,240.21,370.63,264.20,22.31">nCG(w j , S i ) = 1 Z • G(S i , w j )<label>(3)</label></formula><p>Z is the maximum possible gain (given the N tweets per day limit). As for EG, three variants are considered regarding how the silent days are taken into account: nCG-1, nCG-0, and nCG-p.</p><p>Gain Minus Pain. The GMP metric evaluates the utility of the run:</p><formula xml:id="formula_3" coords="4,189.69,481.49,314.72,9.65">GMP (S i , w j ) = α G(S i , w j ) -(1 -α) • P (S i , w j )<label>(4)</label></formula><p>The gain Gain G(S i , w j ) is computed in the same manner as above, the pain P (S i , w j ) is the number of non-relevant tweets published during w j and returned by the system S i , and α controls the balance between the two. Three α settings were considered: 0.33, 0.50, and 0.66.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Latency-Oriented Metric</head><p>The latency metric is defined as:</p><formula xml:id="formula_4" coords="4,220.78,608.68,279.38,27.70">Latency(S k ) = t (•) i ∈R k Π k (t (•) i ) -Θ(t 1 i ) (<label>5</label></formula><formula xml:id="formula_5" coords="4,500.16,611.81,4.24,8.74">)</formula><p>where t</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(•)</head><p>i is the oldest tweet pushed by the system S k for the cluster C i . In other terms, latency is evaluated only for tweets contributing to the gain as the difference between the time a tweet was pushed and the first tweet in the semantic cluster that the tweet belongs to. Fig. <ref type="figure" coords="5,119.16,206.26,3.58,7.86">1</ref>: Examples of runs retrieved by S1 and S2 as well as the associated ground truth (GS). In this example, a time window wi lasts 50 seconds.</p><p>Table <ref type="table" coords="5,185.66,241.61,3.58,7.86">3</ref>: Behaviours of the studied metrics with respect to the metrics.</p><p>Metrics Systems </p><formula xml:id="formula_6" coords="5,106.47,268.93,328.44,72.50">S 1 S 2 EG-0 ( 1 3 * 1 + 0 + 0 + 1 3 * 1 + 1 1 * 1)/5 = 0.33 (0 + 1 1 * 1 + 0 + 1 1 * 1 + 1 1 * 1)/5 = 0.6 EG-1 ( 1 3 * 1 + 0 + 0 + 1 3 * 1 + 1 1 * 1)/5 = 0.33 (0 + 1 1 * 1 + 1 + 1 1 * 1 + 1 1 * 1)/5 = 0.8 EG-p ( 1 3 * 1 + 9 10 + 9 10 + 1 3 * 1 + 1 1 * 1)/5 = 0.69 (0 + 1 1 * 1 + 1 + 1 1 * 1 + 1 1 * 1)/5 = 0.8 nCG-0 ( 1 1 * 1 + 0 + 0 + 1 1 * 1 + 1 2 * 1)/5 = 0.5 (0 + 1 1 * 1 + 0 + 1 1 * 1 + 1 2 * 1)/5 = 0.5 nCG-1 ( 1 1 * 1 + 0 + 0 + 1 1 * 1 + 1 2 * 1)/5 = 0.5 (0 + 1 1 * 1 + 1 + 1 1 * 1 + 1 2 * 1)/5 = 0.7 nCG-p (( 1 1 * 1 + 9 10 + 9 10 + 1 1 * 1 + 1 2 * 1)/5 = 0.86 (0 + 1 1 * 1 + 1 + 1 1 * 1 + 1 2<label>*</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Metrics Exemplifications</head><p>Fig. <ref type="figure" coords="5,122.82,423.24,4.98,8.74">1</ref> and Table <ref type="table" coords="5,179.68,423.24,4.98,8.74">3</ref> run through some examples of systems and the way the official metrics are calculated. The results presented in Table <ref type="table" coords="5,304.57,435.20,4.98,8.74">3</ref> are decomposed and were checked using the 2016 and 2017 official evaluation tools of the track<ref type="foot" coords="5,321.98,445.58,3.97,6.12" target="#foot_0">2</ref> .</p><p>In the EG family of metrics, the gain in a time window is divided by the number of tweets returned by the system and published during the time window. For instance and considering the system S 1 , the gain in the time window w 1 is divided by 3 while it is divided by 1 in w 2 . In the nCG family of metrics, the gain in a time window is divided by the optimal gain. For instance, in the time window w 5 the optimal gain is 2 (2 new clusters C 3 and C 4 ), but neither S 1 nor S 2 reach this optimal gain.</p><p>If we now consider the silent days, EG-1 and nCG-1 reward the systems for returning no tweets, and strongly penalize them otherwise. For instance, S 1 breaks the silence during w 3 and thus obtains a score of 0 for this window. The silent days can be different from one system to another: S 1 breaks the silence during w 2 since t 3  1 is not relevant in this case (the C 1 cluster has already been retrieved), whereas this is not the case for S 2 for which C 1 was not retrieved at this time. Conversely, S 1 and S 2 receive a score of 0 for w 3 considering the EG-0 and nCG-0 metrics, whereas S 2 has a perfect behavior during this window. Whatever the systems return, the silent days are associated with a score of 0, and it never hurts to push tweets. The "silent days effect" is lowered for the evaluation of EG-p et nCG-p: S 1 receives a score of 9/10 on w 2 and w 3 (whereas it receives a score of 0 for EG-0, nCG-0, EG-1, and nCG-1 metrics).</p><p>For all the gain-oriented metrics, a tweet participates to the gain of the time window on which it was published (and not on which it was pushed by the systems). As a consequence, if we consider the system S 2 and the window w 5 , the t 1 2 tweet participates to the gain of w 4 (which is the window in which it was published). S 2 has thus a non-zero score for w 4 whereas it did not return any tweet. Another point to discuss relates to redundant tweets. As expected, t 2 1 returned by S 1 during w 1 is considered as not relevant, only t 1 1 participates to the gain.</p><p>At last, the Latency metric is evaluated independently of the time windows as the difference between the first tweet found in the cluster by the system and the publication date of the first tweet in the cluster in the gold standard. A side effect of this metric is that a perfect latency can be obtained without returning any relevant tweets. Table <ref type="table" coords="6,144.81,433.72,4.98,8.74" target="#tab_3">4</ref> provides additional information about the metrics. The EG metrics attempt to capture precision while the nCG ones are recall-oriented. The GMP metrics aim to fill the gap between these two contradictory objectives and thus represent a trade-off between precision and recall. As stated in <ref type="bibr" coords="6,205.03,469.59,9.96,8.74" target="#b7">[8]</ref>, EG-0 and nCG-0 metrics are poorly formulated metrics and were thus abandoned in 2017. The gain-oriented metrics are computed for each interest profile and each window w j . The score for a competitor is the mean of the scores for each day over all the profiles. Since each profile contains the same number of days, there is no distinction between micro-vs. macro-averages. The EG-1 and EG-p metrics were respectively considered as the official metrics in 2016 and 2017.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Metric Integration in the Evaluation Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Hypotheses and Settings of the Evaluation Framework</head><p>The metric evaluation is based on two hypotheses assumed by the organizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H1 -Redundant information is non relevant. As mentioned in [6]:</head><p>Once a tweet from a cluster is retrieved, all other tweets from the same cluster automatically become not relevant. This penalizes systems for returning redundant information.</p><p>H2 -A perfect daily score is obtained when silence is respected. As mentioned in <ref type="bibr" coords="6,101.89,684.92,9.96,8.74" target="#b5">[6]</ref>:</p><p>In the EG-1 and nCG-1 variants of the metrics, on a "silent day", the system receives a score of one (i.e., perfect score) if it does not push any tweets, or zero otherwise.</p><p>Note that since relevance is system-dependent, it implies that the silent days are systemdependent as well. Moreover, we would like to shed some light on two settings of the framework.</p><p>S1 -N = 10. This consists in forcing the systems to push a maximum of only 10 tweets per day and per profile. There is a twofold explanation for the value chosen for this parameter: first, to impose to the a realistic limit to the number of daily tweets that could be desired by a user and second, to impose a reasonable limit for the annotation phase.</p><p>S2 -Evaluation window. For the gain-oriented metrics, whatever the Π i (t) value for a tweet, only Θ(t) is considered for the evaluation of G(•, •) (see equation 1). In other terms, each returned tweet is sent back to its emission window. This can affect the systems that use buffering-based strategies as suggested by the guidelines. This setting is implicitly defined by the organizers since the latency metric is calculated separately from the main metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Metrics Adequacy under RTS Hypotheses</head><p>We now refute the aforementioned hypotheses through 2 counterexamples. H1. Considering the example of Fig. <ref type="figure" coords="7,264.90,400.40,4.98,8.74">2</ref> and Table <ref type="table" coords="7,319.50,400.40,3.87,8.74">5</ref>, the system S 2 has higher scores than S 1 on the EG metrics, whereas both return results supposed as equivalent (the first tweet of the cluster C 1 during w 1 and respectively a redundant and non relevant tweet during w 2 ). S 1 is more penalized for returning a redundant tweet than a non relevant one. This thus violates H1. </p><formula xml:id="formula_7" coords="7,119.99,492.12,76.62,83.22">P !# (t) t 1 2 t 1 1 t 1 1 t 1 1 t 2 1 t 2 1</formula><p>Fig. <ref type="figure" coords="7,119.16,587.32,3.58,7.86">2</ref>: Examples of runs retrieved by S1 and S2 as well as the associated ground truth (GS) with respect to H1. In this example, a time window wi lasts 50 seconds.</p><p>Metrics Systems Table <ref type="table" coords="7,280.91,620.20,3.58,7.86">5</ref>: Behaviours of the studied metrics with respect to H1.</p><formula xml:id="formula_8" coords="7,257.24,508.46,180.96,72.50">S 1 S 2 EG-0 ( 2 * 1 + 0)/2 = 0.25 ( 1 1 * 1 + 0)/2 = 0.5 EG-1 ( 1 2 * 1 + 0)/2 = 0.25 ( 1 1 * 1 + 0)/2 = 0.5 EG-p ( 1 2 * 1 + 0)/2 = 0.25 ( 1 1 * 1 + 0)/2 = 0.5 nCG-0 ( 1 1 * 1 + 1 1 * 0)/2 = 0.5 ( 1 1 * 1 + 0)/2 = 0.5 nCG-1 ( 1 1 * 1 + 0)/2 = 0.5 ( 1 1 * 1 + 0)/2 = 0.5 nCG-p ( 1 1 * 1 + 0/2 = 0.5 ( 1 1 * 1 + 0)/2 =</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H2.</head><p>Considering the example of Fig. <ref type="figure" coords="7,267.47,661.01,4.98,8.74">3</ref> and Table <ref type="table" coords="7,323.38,661.01,3.87,8.74">6</ref>, w 2 is a silent day for both systems. S 2 breaks the silence with t 2 1 and however obtains a perfect score on this day, as S 1 which did not push any tweet. This thus violates H2.</p><p>the systems are uniformly distributed over the time window. Last but not least, pushing only one tweet adopting a very basic strategy, i.e., either the Random or the First one, without any guarantees that this tweet is relevant, provides similar or better results than pushing 3 or more tweets using a sophisticated strategy such as the Gold one, i.e., in which the number of retrieved clusters is maximized. This very interesting result reinforces our claim about the regrettable non-consideration of the coverage in the official metric. This point will be further discussed in Section 9. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2.</head><p>While the side effect of S2 have been assessed in Section 3, we now focus on its practical consequences during the TREC RTS 2016 track. For this purpose, we simulated an alternative evaluation framework in which the tweets are not sent back to their publication window. We observe from these statistics that:</p><p>very few tweets have been pushed in another window than their creation window. Specifically, this concerns only 0.12 % (53/45751) of the pushed tweets all the systems taken together. these 53 tweets have been pushed by 8 different systems over the 41 runs. Notably, one of these systems have pushed 42 tweets among the 53 tweets while the other 7 systems have pushed only 1 or 2 tweets outside their creation window. due to the rarity of these push window gaps, there is no differences in the rankings whatever the date taken into account for the evaluation.</p><p>Note that S2 could theoretically impact the performances of the systems, but this situation is not observed in the 2016 runs. Moreover, S2 may send back tweets to a window without any restrictions and making it greater than N , calling into question S1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experimental validation of the metrics limitations</head><p>We submitted to the 2017 official track one baseline run which exploited the above-discussed conclusions about the evaluation metrics. Its principle is as follows: after a standard preprocessing step, i.e., stopwords removal and stemming, the first tweet of the day containing all the query terms is returned. We thus return at most one tweet per day and per profile.</p><p>The results of our run for mobile and batch evaluation are respectively presented in Tables <ref type="table" coords="9,101.89,625.14,4.98,8.74" target="#tab_4">7</ref> and<ref type="table" coords="9,130.49,625.14,3.87,8.74" target="#tab_5">8</ref>. Our run, although being very simple, can be considered as a very strong baseline. We are ranked 2nd and 4th for the mobile and batch evaluation, while maintaining latency as low as possible.</p><p>These a posteriori results clearly confirm the need for a deeper reflection and discussion about the official batch metrics and their possible biases. Since mobile evaluation also seems to be concerned by the bias, we advocate for its inclusion in future discussions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Reusability of the Test Collection</head><p>In order to enable comparison of new solutions against the TREC RTS 2016 and 2017 results, the organizers publicly provide an evaluation script as well as 3 ground truth files: (i) the qrels file that contains the relevance level of each tweet from the pool, (ii) the cluster file that gives cluster for each profile and (iii) the epoch file that contains the publication date of tweets from the pool. We conducted a standard "leaved-one out" analysis to evaluate the reusability of the 2016 collection. To do so, we simulated a rerun setup for all the 41 runs submitted during 2016 and evaluated them using the official metric, EG-1. The ground truth files, i.e., cluster, qrels, and epoch files, were created for each run i as if it has not taken part in the track by removing its unique tweets. For each of these new 41 evaluation files, an alternative ranking i was obtained using the EG-1 metric. The official ranking of each run i was then compared to this new ranking i in order to determine how effective would have been this run i in a rerun setup. The position of each run i in the ranking i showed either improvement or no variability with respect to its position in the original ranking, resulting in an average gain of 2.1 positions. This very surprising result has motivated a deep analysis of the evaluation tool. We observed a very odd behavior on how the unassessed tweets, i.e., the tweets that are not referenced in the ground truth files, are considered. Indeed, such tweets are simply ignored instead of being considered as irrelevant as traditionally done in classical evaluation setups. This point is even more problematic since the way the runs deal with the silent days is crucial for the calculation of gain-oriented metrics. By decreasing the number of tweets per profile/day and increasing chances to respect the silent days, the performances of new runs in the rerun setup are artificially increased. This situation is only attenuated, but still not solved, thanks to the introduction of the EG-p and nCG-p metrics in 2017. However, ignoring the tweets that must be considered as not relevant will still increase the score obtained by those metrics during the silent days. This bias in the (re)evaluation can be solved by including all the tweets of the Twitter stream during the evaluation period (11.5 M tweets) in the epoch file. In this case, our results showed a different ranking behaviour. None of the runs improved its position in its respective ranking, dealing with an average lost of 0.6 positions when compared to their original position.</p><p>Regarding the settings, we would like to draw attention to the fact that S1 is under the responsibility of each user of this collection. This setting was automatically handled by the organizers through the broker during the task. Not respecting this limit during the rerunning leads to underestimated performance since the gain is calculated only over the first 10 tweets but it is normalized by the total number of tweets, which could be greater than 10. Contrary to S1, S2 is always applied without user intervention.</p><p>Finally, users of the collection must consider analysis and remarks presented in Sections 5 and 6 because they are also valid under the rerun setup. We confirm the reusability of the 2016 and 2017 collections only under the aforementioned conditions, in particular, use of a complete epoch file and strict application of S1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Recommendation and Conclusion</head><p>To conclude, we would like to summarize our main findings in this paper:</p><p>we clarified some definitions and assumptions of the track guidelines. We highlight here two of them, which are not clearly stated in the guidelines and overviews of the track although crucial for a good understanding of the evaluation framework. Only a deep analysis of the evaluation tool lead us to these conclusions, causing us to believe that some participants may not be conscious of these findings:</p><p>• the evaluation window used in EG and nCG metrics is not the window corresponding to the tweet push-timestamp. Each returned tweet is sent back to its emission window, which significantly impacts the way metrics are evaluated. • silent days are system-dependent. This is thus non-sense to elaborate approaches that try to detect silent days independently of already returned tweets. we shed the light on the fact that coverage is not really evaluated by the official metrics.</p><p>The systems would better return few tweets that are very likely relevant to optimize the metrics. Trying to maximize the coverage and thus returning many tweets will probably lead to a result degradation. As a consequence, when developing a system for the track, all the improvements against the metrics should be compared to a very simple run returning at most one tweet per time window. This behavior of the results has already been noticed by the track organizers <ref type="bibr" coords="11,221.05,529.33,9.96,8.74" target="#b6">[7]</ref>, but this was credited to misconfigurations of the systems that returned very few tweets. On the contrary, we do think that, given the metrics and the way the silent days are considered, systems should return few tweets to be top-ranked. This unusual behavior of the metrics is not observed on the other traditionally-used precisionoriented metrics such as P@K and MAP. Our official results on the 2017 track confirm these findings. We submitted a baseline run returning the first tweet of the day containing all the query terms (i.e., at most one tweet per profile and per day was returned). This very simple baseline allowed us to be ranked 2 nd on the mobile evaluation and 4 th (out of 41 participants) on the batch one of Scenario A <ref type="bibr" coords="11,341.16,624.97,9.96,8.74" target="#b4">[5]</ref>. -Concerning the reusability of the collection, we found a problem on the epoch file used in evaluation. In case of rerun, researchers should add all their tweets to the official epoch file, which is not mentioned in the evaluation tool documentation. Otherwise, the results are largely over-evaluated since the evaluation does not consider the non-relevant tweets that are absent from the epoch file. As this problem has never been mentioned before by track organizers or participants, it is very likely that some already-published research papers using the TREC RTS collection as evaluation framework report over-evaluated results.</p><p>In future and concerning the metrics, since the track will be pursued in 2018, we suggest to focus on the relative importance of clusters. For instance, let us consider the 2017 profile RTS60 entitled "Beyonce's babies". The very famous photo posted on Instagram in which Beyonce officially announced the names of the twins with their first image is a crucial information for this profile. Other information such as the name of the nurse is also relevant but less crucial.</p><p>With equal numbers of retrieved clusters, the systems that find the cluster about this first announcement should thus be more rewarded than the systems that do not find it.</p><p>Separating latency and effectiveness should also be (re)considered. In 2015 the TREC microblog track included a very first version of the task (named Scenario A -Push notification) where a latency penalty was applied to the EG metric <ref type="bibr" coords="12,337.14,266.44,9.96,8.74" target="#b3">[4]</ref>. The metric has been given up since 2016 to understand the potential tradeoffs between quality and latency. However, we think that separating latency and gain metrics may lead to some side effects that could be avoided with a single-point metric.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,403.51,334.43,31.40,4.35;5,106.47,347.93,206.35,4.37;5,131.70,353.91,58.19,4.37;5,315.62,347.93,177.51,4.37;5,106.47,364.61,74.90,4.35;5,315.62,364.61,56.46,4.35"><head></head><label></label><figDesc>1)/5 = 0.7 GMP.50 ((0.5 * 1 -0.5 * 2) + (0) + (-0.5 * 1) + (0.5 * 1 -0.5 * 2) + (0.5 * 1))/5 = -0.1 ((-0.5 * 1) + (0.5 * 1) + (0) + (0.5 * 1) + (0.5 * 1))/5 = 0.2 Latency 2 + 10 + 20 = 32 65 + 40 + 10 = 115</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,429.14,573.96,9.06,4.35;7,257.24,587.46,210.75,4.37;7,257.24,600.99,30.00,4.35;7,379.62,600.99,3.39,4.35"><head>0. 5 GMP 1 )</head><label>51</label><figDesc>.50 ((0.5 * 1 -0.5 * 1) + (0))/2 = 0 ((0.5 * 1) + (-0.5 *</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,222.41,302.14,161.47,7.86"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Impact of S1 on the EG-1 metric</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,101.89,241.44,402.52,56.59"><head>Table 1 :</head><label>1</label><figDesc>Statistics of scenario A in 2016 and 2017. Times are provided in UTC in order to have fixed time intervals regardless the participant location.</figDesc><table coords="2,139.84,270.97,323.53,27.06"><row><cell>Year Evaluation period</cell><cell cols="2"># judged topics # competitors</cell></row><row><cell>2016 From 02/08/2016 00:00:00 to 11/08/2016 23:59:59</cell><cell>56</cell><cell>41</cell></row><row><cell>2017 From 29/07/2017 00:00:00 to 05/08/2017 23:59:59</cell><cell>97</cell><cell>41</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,118.62,184.62,277.41,86.24"><head>Table 2 :</head><label>2</label><figDesc>Notations used throughout the paper</figDesc><table coords="3,118.62,202.65,250.68,68.20"><row><cell>Notation</cell><cell>Definition</cell></row><row><cell cols="2">C = {C1, . . . , C k } The set of clusters</cell></row><row><cell>t j i</cell><cell>The j th tweet belonging to the cluster Ci</cell></row><row><cell>t</cell><cell>A non relevant tweet</cell></row><row><cell>Si</cell><cell>A system</cell></row><row><cell>Θ(t)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,101.89,281.56,402.52,134.57"><head>Table 4 :</head><label>4</label><figDesc>Official metrics for the 2016 and 2017 tracks. The primary metric for each year is denoted with *.</figDesc><table coords="6,146.10,311.09,314.09,105.05"><row><cell>Metrics</cell><cell cols="2">Variants Years</cell><cell>Recall Precision Utility Latency Averaged over</cell></row><row><cell></cell><cell>EG-1</cell><cell>2016*, 2017</cell><cell>profiles</cell></row><row><cell>EG</cell><cell>EG-0</cell><cell>2016</cell><cell>and</cell></row><row><cell></cell><cell>EG-p</cell><cell>2017*</cell><cell>days</cell></row><row><cell>nCG</cell><cell>nCG-1 nCG-0</cell><cell>2016, 2017 2016</cell><cell>profiles and</cell></row><row><cell></cell><cell>nCG-p</cell><cell>2017</cell><cell>days</cell></row><row><cell></cell><cell cols="2">GMP.33 2016, 2017</cell><cell>profiles</cell></row><row><cell>GMP</cell><cell cols="2">GMP.50 2016, 2017</cell><cell>and</cell></row><row><cell></cell><cell cols="2">GMP.66 2016, 2017</cell><cell>days</cell></row><row><cell>Latency</cell><cell></cell><cell>2016, 2017</cell><cell>profiles</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,101.89,93.07,402.52,221.66"><head>Table 7 :</head><label>7</label><figDesc>Official results of our system (run IRIT-Run1-14) and median for Scenario A (mobile evaluation). The mean (τ ) and median ( τ ) latency of submitted tweets in seconds. Strict (Ps) and lenient (P l ) precision. Average metrics were calculated using the 41 participant scores.</figDesc><table coords="10,121.41,93.07,363.49,221.66"><row><cell>Run</cell><cell cols="3">Online utility (strict) Online utility (lenient) τ</cell><cell>τ</cell><cell>Ps</cell><cell>P l</cell></row><row><cell>1st participant run</cell><cell>-93</cell><cell>-25</cell><cell>1</cell><cell cols="2">1 0.4337 0.4822</cell></row><row><cell>IRIT-Run1-14</cell><cell>-198</cell><cell>-46</cell><cell>1</cell><cell cols="2">1 0.4200 0.4814</cell></row><row><cell>3rd participant run</cell><cell>-262</cell><cell>-66</cell><cell cols="3">296 31 0.4140 0.4783</cell></row><row><cell>Average</cell><cell>-1507</cell><cell>-1071</cell><cell cols="3">6742 5580 0.3043 0.3699</cell></row><row><cell>Median</cell><cell>-805</cell><cell>-456</cell><cell cols="3">102 35 0.3403 0.4174</cell></row><row><cell>Run</cell><cell cols="4">EG-p EG-1 nCG-p nCG-1 GMP.33 GMP.50 GMP.66</cell><cell>τ</cell><cell>τ</cell></row><row><cell cols="6">1st participant run 0.3630 0.2088 0.2808 0.1266 -0.2720 -0.1566 -0.0479 119374 56744</cell></row><row><cell cols="6">2nd participant run 0.3318 0.1811 0.2610 0.1102 -0.3118 -0.1936 -0.0824 116649 49154</cell></row><row><cell cols="6">3rd participant run 0.3226 0.2622 0.2489 0.1886 -0.1952 -0.1105 -0.0308 118653 55781</cell></row><row><cell>IRIT-Run1-14</cell><cell cols="5">0.2918 0.2571 0.2321 0.1974 -0.1195 -0.0615 -0.0070 67555</cell><cell>1</cell></row><row><cell cols="6">5th participant run 0.2907 0.2571 0.2285 0.1949 -0.1190 -0.0622 -0.0087 126484 60685</cell></row><row><cell>Average</cell><cell cols="5">0.2273 0.18740 0.2187 0.1787 -0.5106 -0.3501 -0.1990 77911 13175</cell></row><row><cell>Median</cell><cell cols="5">0.2194 0.1951 0.2095 0.1826 -0.2630 -0.1707 -0.0839 71463</cell><cell>80</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,101.89,320.34,402.52,29.78"><head>Table 8 :</head><label>8</label><figDesc>Official results of our system (run IRIT-Run1-14) and median for Scenario A (batch evaluation). The mean (τ ) and median ( τ ) latency of submitted tweets in seconds. Average metrics were calculated using the 41 participant scores.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="5,111.85,674.64,392.56,8.12;5,111.85,685.60,357.36,8.12"><p>Official evaluation tools are available at http://trec.nist.gov/data/rts2016.html (2016) and http://trec.nist.gov/act_part/act_part.html (2017), last checked: October 6, 2017.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. <ref type="figure" coords="8,119.16,191.33,3.58,7.86">3</ref>: Examples of runs retrieved by S1 and S2 as well as the associated ground truth (GS) with respect to H2. In this example, a time window wi lasts 50 seconds.</p><p>Metrics Systems</p><p>GMP.50 ((0.5 * 1) + (0))/2 = 0.25 ((0.5 * 1 -0.5 * 1) + (0))/2 = 0 Latency 2 2</p><p>Table <ref type="table" coords="8,280.91,224.20,3.58,7.86">6</ref>: Behaviours of the studied metrics with respect to H2.</p><p>These two counterexamples are a side effect of S2.</p><p>6 Discussion of the RTS Settings S1. Allowing up to 10 tweets to be pushed per profile per day is an arbitrary limit of the task. In this section, we wonder how much the 2016 official metric would have been impacted by a modification of this value. With this aim in mind, we adopt the following methodology.</p><p>Given N ∈ {1..10}, we apply three distinct strategies to restrict the 2016 official runs to push only N tweets per profile per day and then calculate the average value of the EG-1 metric 3 . The three strategies are as follows:</p><p>-In the First strategy, the first N tweets according to their pushing date are considered. This strategy intuitively simulates a change in the setting but no self-adaptation of the systems to this tighter constraint. -In the Gold strategy, N tweets are chosen to maximize the number of clusters and thus the official metric. Given a window w j , a profile p, and a system S i , if N is greater than the number of clusters retrieved by S i during w j for p, non-relevant tweets, i.e., either redundant or irrelevant tweets, are pushed to fulfill our requirement. Contrary to the First strategy, this strategy simulates a self-adaptation of the systems under this tighter constraint. -In the Random strategy, N tweets are randomly chosen. To overcome any bias in the sampling, 100 random draws were performed and the EG-1 metric values for all these 100 runs were then averaged. This strategy represents a fair compromise between the naive First and the optimal Gold strategies.</p><p>It should be noted that if less than N tweets have been pushed by a system S i for a profile p during a window w j , the set returned by any strategy is the same as the original set of pushed tweets. Finally, to fairly evaluate the impact of varying N , we compare the results obtained by these strategies to the average value of EG-1 in the official runs. The impact of the window size on the aforementioned strategies is shown in Fig. <ref type="figure" coords="8,382.20,618.57,3.87,8.74">4</ref>. Several strong conclusions can be drawn from these results. First, whatever the strategy, it is always beneficial to return very few tweets reinforcing the idea that EG-1 is essentially a precision-oriented metric. This conclusion is obviously even more true for the Gold strategy. Second, the performances of the First and Random strategies are very close suggesting that relevant tweets retrieved by</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="12,110.08,357.78,394.33,7.86" xml:id="b0">
	<monogr>
		<ptr target="http://trecrts.github.io/TREC2017-RTS-guidelines.html" />
		<title level="m" coord="12,118.64,357.78,123.63,7.86">Trec 2017 evaluation guidelines</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.08,368.74,394.33,7.86;12,118.64,379.70,195.21,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="12,427.74,368.74,76.68,7.86;12,118.64,379.70,119.35,7.86">Trec 2015 temporal summarization track overview</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ekstrand-Abueg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mccreadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct coords="12,110.08,390.66,394.33,7.86;12,118.64,401.62,352.77,7.86" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Pinel-Sauvagnat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Pitarch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04671</idno>
		<title level="m" coord="12,368.03,390.66,136.39,7.86;12,118.64,401.62,186.80,7.86">Everything you always wanted to know about trec rts*(* but were afraid to ask)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,110.08,412.58,394.33,7.86;12,118.64,423.53,49.17,7.86" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sherman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">V</forename><surname>Voorhees</surname></persName>
		</author>
		<title level="m" coord="12,358.86,412.58,145.54,7.86;12,118.64,423.53,20.50,7.86">Overview of the trec-2015 microblog track</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.08,434.49,394.33,7.86;12,118.64,445.45,385.76,7.86;12,118.64,456.41,326.56,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,230.45,445.45,230.03,7.86">Overview of the trec 2017 real-time summarization track</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sequiera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ghelani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Abualsaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mccreadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Milajevs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,161.67,456.41,221.17,7.86">Pre-Proceedings of the 26th Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>notebook draft</note>
</biblStruct>

<biblStruct coords="12,110.08,467.37,394.33,7.86;12,118.64,478.33,385.76,7.86;12,118.64,489.29,55.30,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,400.09,467.37,104.32,7.86;12,118.64,478.33,121.36,7.86">Overview of the trec 2016 real-time summarization track</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roegiest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mccreadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,261.67,478.33,206.14,7.86">Proceedings of the 25th Text REtrieval Conference</title>
		<meeting>the 25th Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.08,500.25,394.32,7.86;12,118.64,511.21,385.76,7.86;12,118.64,522.16,201.87,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,257.16,500.25,247.24,7.86;12,118.64,511.21,184.25,7.86">Interleaved evaluation for retrospective summarization and prospective notification on document streams</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roegiest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,325.54,511.21,178.87,7.86;12,118.64,522.16,71.47,7.86">Proceedings of the 39th International ACM SIGIR Conference</title>
		<meeting>the 39th International ACM SIGIR Conference</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.08,533.12,394.33,7.86;12,118.64,544.08,385.76,7.86;12,118.64,555.04,40.45,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,239.52,533.12,264.90,7.86;12,118.64,544.08,29.48,7.86">Online in-situ interleaved evaluation of real-time push notification systems</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roegiest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,169.14,544.08,248.90,7.86">Proceedings of the 40th International ACM SIGIR Conference</title>
		<meeting>the 40th International ACM SIGIR Conference</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.08,566.00,394.33,7.86;12,118.64,576.96,385.76,7.86;12,118.64,587.92,107.19,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,236.50,566.00,267.91,7.86;12,118.64,576.96,96.29,7.86">On the reusability of &quot;living labs&quot; test collections: A case study of real-time summarization</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Baruah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,236.04,576.96,248.47,7.86">Proceedings of the 40th International ACM SIGIR Conference</title>
		<meeting>the 40th International ACM SIGIR Conference</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,109.74,598.88,394.67,7.86;12,118.64,609.84,385.77,7.86;12,118.64,620.79,69.30,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,293.21,598.88,211.20,7.86;12,118.64,609.84,48.06,7.86">An exploration of evaluation metrics for mobile push notifications</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roegiest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,189.14,609.84,255.67,7.86">Proceedings of the 39th International ACM SIGIR Conference</title>
		<meeting>the 39th International ACM SIGIR Conference</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,109.74,631.75,394.66,7.86;12,118.64,642.71,385.77,7.86;12,118.64,653.67,88.76,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,301.23,631.75,203.17,7.86;12,118.64,642.71,75.60,7.86">Assessor differences and user preferences in tweet timeline generation</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sherman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Efron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,214.94,642.71,248.34,7.86">Proceedings of the 38th International ACM SIGIR Conference</title>
		<meeting>the 38th International ACM SIGIR Conference</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
