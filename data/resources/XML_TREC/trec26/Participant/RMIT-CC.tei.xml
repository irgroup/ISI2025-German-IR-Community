<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,165.24,83.75,282.02,15.12">RMIT at the 2017 TREC CORE Track</title>
				<funder ref="#_Vt6gMQe">
					<orgName type="full">Australian</orgName>
				</funder>
				<funder>
					<orgName type="full">Mozilla Foundation</orgName>
				</funder>
				<funder ref="#_3gAmyXr">
					<orgName type="full">Australian Research Council&apos;s Discovery Projects Scheme</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,103.29,109.62,78.13,10.60"><forename type="first">Rodger</forename><surname>Benham</surname></persName>
						</author>
						<author>
							<persName coords="1,269.24,109.62,74.74,10.60"><forename type="first">Luke</forename><surname>Gallagher</surname></persName>
						</author>
						<author>
							<persName coords="1,433.72,109.62,73.66,10.60"><forename type="first">Joel</forename><surname>Mackenzie</surname></persName>
						</author>
						<author>
							<persName coords="1,94.95,155.20,93.44,10.60"><forename type="first">Tadele</forename><forename type="middle">T</forename><surname>Damessie</surname></persName>
						</author>
						<author>
							<persName coords="1,261.13,155.20,89.75,10.60"><forename type="first">Ruey-Cheng</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName coords="1,440.34,155.20,59.85,10.60"><forename type="first">Falk</forename><surname>Scholer</surname></persName>
						</author>
						<author>
							<persName coords="1,187.95,200.78,55.34,10.60"><forename type="first">Alistair</forename><surname>Mo</surname></persName>
						</author>
						<author>
							<persName coords="1,341.53,200.78,93.12,10.60"><forename type="first">J</forename><forename type="middle">Shane</forename><surname>Culpepper</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">RMIT University Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">RMIT University Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">RMIT University Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">RMIT University Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">RMIT University Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">RMIT University Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">University of Melbourne Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">RMIT University Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,165.24,83.75,282.02,15.12">RMIT at the 2017 TREC CORE Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FE8BAA23C6ECB03302C7359C2107A71C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>e TREC 2017 CORE Track 1 is a re-run of the classic TREC ad hoc search evaluation campaign, with the vision of establishing new methodologies for creating IR test collections. e previous TREC newswire ad hoc task was the 2004 Robust Track, where the emphasis was on improving the e ectiveness of poorly performing topics in previous tracks <ref type="bibr" coords="1,141.13,348.96,13.22,7.95" target="#b15">[16]</ref>. e TREC CORE 2017 track reuses the Robust 2004 topic set, for the development of relevance judgments over a new New York Times corpus, composed of newswire articles published between 1987 and 2007.</p><p>In this track, our interest is driven by two related lines of research: e cient multi-stage retrieval <ref type="bibr" coords="1,161.78,403.76,39.11,7.95">[3-7, 9, 14]</ref>, where it is believed that improving recall in early stage retrieval can improve end-to-ende ectiveness; and more reliable deep evaluation when using shallow judgments <ref type="bibr" coords="1,96.08,436.63,9.44,7.95" target="#b7">[8,</ref><ref type="bibr" coords="1,108.52,436.63,7.70,7.95" target="#b9">[10]</ref><ref type="bibr" coords="1,116.22,436.63,3.85,7.95" target="#b10">[11]</ref><ref type="bibr" coords="1,116.22,436.63,3.85,7.95" target="#b11">[12]</ref><ref type="bibr" coords="1,120.08,436.63,11.55,7.95" target="#b12">[13]</ref>. By participating in CORE, we a empted to develop a recall-oriented approach that exploits user query variations and rank fusion. We venture that, in an evaluation campaign such as the TREC CORE Track, which typically a racts runs of a high e ectiveness caliber from research groups worldwide, the ability to retrieve a large number of relevant documents that other systems fail to nd is indicative of a high-recall system.</p><p>A useful consequence of this approach is the ability to compare query variation phenomena across corpora. e UQV100 test collection contains one hundred single-faceted topics with over ve thousand unique query variations <ref type="bibr" coords="1,201.76,546.22,9.52,7.95" target="#b0">[1]</ref>, but to date has only judgments against the ClueWeb12B collection. e variationrich collection produced for participation in the CORE track, while smaller in scope than the UQV100 collection, now enables comparisons of query variations to be made across di erent document representations and editorial quality. Observing the relative e ectiveness across ClueWeb12B and Robust04, one consisting of websites, and the other composed of journalistic content, is of considerable interest, as is the question of the bene t of rank fusion mechanisms based on those variations.</p><p>Research Goals. We focus on these research questions:  • RQ1: Can tuned and parameterized query fusion be used to improve the number of unique relevant documents found relative to other participants? • RQ2: Do query variations that are good in one collection also perform well in another collection? • RQ3: Are the score ranges caused by user query variations consistent across di erent test collections? In the next section, we describe the process used to collect query variations. In Section 3 we discuss our submi ed runs in more detail, and place our query variations into context with the existing UQV100 test collection; and then in Section 4 we provide the results of each of our submi ed runs using the y queries that were assessed by NIST. As of writing, crowd-sourced relevance assessments for two hundred additional topics remain unreleased, reducing the scope of our exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">GENERATING QUERY VARIATIONS</head><p>In recent work Mo at et al. <ref type="bibr" coords="1,429.57,570.77,14.72,7.95" target="#b14">[15]</ref> demonstrate that the way in which queries are posed can have a substantial e ect on retrieval outcomes.</p><p>e UQV100 collection was created out of that same work, a set of qrels for the ClueWeb12B documents, built to provide coverage for more than 5,000 unique queries over a set of 100 topics <ref type="bibr" coords="1,317.96,625.57,9.51,7.95" target="#b0">[1]</ref>. e UQV100 collection then provided a framework in which further work was possible, including the introduction of retrieval consistency as an a ribute of a search system, and measurement of the bene ts achieved by combining the runs generated as a result of query variations into a single "smoothed" run <ref type="bibr" coords="1,497.12,669.40,9.39,7.95" target="#b1">[2]</ref>.</p><p>We sought to explore and build on those themes in our 2017 TREC CORE submission. To collect query variations that could be used in a range of experimental se ings, such as query fusion, and query rewriting over three distinct document collections, a tool to collect variations was developed, and over a ten-day period each of the authors (eight participants in total) contributed a set of up to ten query variations per 2017 topic (of which there are 250) using that tool. e information-need descriptions used as the basis for query solicitation were taken from the modernized Robust04 topic narratives and descriptions supplied by the track organizers, with the title of the topic not shown at all, to avoid biasing the new queries. Figure <ref type="figure" coords="2,110.41,381.02,4.17,7.95" target="#fig_0">1</ref> shows a screenshot taken while lling out query variations for one of the 250 topics.</p><p>No query transformations were applied on the queries collected using the tool, and the onus was on each user to supply one or more queries that they thought would be accurate and useful representations of the corresponding information need. ery normalization is an additional step that might be applied in future experiments, although we note that many browsers already o er spell checking functionality which may have been used during the creation of query variants.</p><p>Although the text-boxes were presented to the user in an ordinal numbered list (Figure <ref type="figure" coords="2,133.51,501.56,2.89,7.95" target="#fig_0">1</ref>), this was not used to indicate a preference for that user's "best" query. Another opportunity for future analysis will be to determine whether there was any consistent pa ern of "ge ing be er" or "ge ing worse" evident in the sets of queries authored by each of the participants. About half-way through the solicitation period, it was clear that there was a heavy focus on the early topics, including the set of 50 that were to be NIST-assessed.</p><p>is was useful, to allow a focus on the human judgment task; but therea er we also sought to balance the collection process, by circulating lists of topics that had the smallest numbers of variations so far.</p><p>By the end of the collection phase, a minimum of eight query variations were captured for each of the 250 topics (not all participants covered all of the topics). e author of each query was recorded as they submi ed their suggestions, and Table <ref type="table" coords="2,264.03,654.99,4.25,7.95">1</ref> shows the relative contributions made, with the "participant" value an arbitrary identi er. Generating a more representative sample of query variations across users remains an option to be explored in future analysis.</p><p>While the authors are familiar with many of these 2017 topics and their nominal "title queries" as a result of other TREC-based activities (including, for two of them, working on the UQV100 queries), we nevertheless engaged fully with the spirit of the new task, and made best e orts to provide queries based on the description and narratives that were presented via the solicitation interface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head><p>We used Indri 5.11 <ref type="foot" coords="2,389.50,185.27,3.38,6.45" target="#foot_0">2</ref> to index the collection and form our runs, converting the NYT corpus from XML to SGML by parsing the elds using Nokogiri -a libxml2 wrapper. Table <ref type="table" coords="2,508.34,209.34,4.25,7.95" target="#tab_2">2</ref> displays an abridged description of each run; details are provided later in this section. An automatic run is formed by an IR system where there is no human involvement when retrieving ranked documents, outside of issuing the topics supplied by NIST to the system. Manual runs are all other types of runs; as two of our runs were built on multi-human involvement in the query construction stage, they are marked manual as to allow them to be treated appropriately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Oracle-Based Manual</head><p>ery Rewriting. To illustrate the power of using a be er query to satisfy an information need, we observe the e ectiveness of individual queries on a per-topic basis using AP on the Robust04 collection (computed using trec eval<ref type="foot" coords="2,550.57,333.22,3.38,6.45" target="#foot_1">3</ref> ), and NDCG@10 (computed using gdeval<ref type="foot" coords="2,476.57,344.18,3.38,6.45" target="#foot_2">4</ref> ) using the UQV100 judgments. e New York Times collection could not be utilized, as no judgments for this collection existed at the time of run submission. Instead, we use the Robust04 collection as there are relevance assessments available for the same topics, and the documents were formed with similar editorial quality to those in the NYT corpus, allowing inferences to be made about the spread of retrieval e ectiveness using di erent queries.</p><p>To analyze the volatility in query e ectiveness, we used the newly created query variations and contrast with the existing UQV100 set on the separate ClueWeb12B collection. On the ClueWeb12B collection, no such editorial quality control exists, as is the nature of Web data. is gives us an additional reference point, to explore whether volatility holds constant across collections, to answer research question RQ3. As another point of comparison, we show how the spread of retrieval e ectiveness varies with respect to the title queries published with the Robust04 set, and the most frequently submi ed query variation in the UQV100 set.</p><p>Figure <ref type="figure" coords="2,354.75,543.58,4.25,7.95">2</ref> shows the variance of the Okapi BM25 per-topic AP e ectiveness, with topics sorted by decreasing 75 th percentile e ectiveness, juxtaposed with the best and worst performing query variant in our new query variation collection.</p><p>e spread of e ectiveness for the query variations is stark. e UQV100 test collection had a similar spread of per-topic e ectiveness, per query variation submi ed. Figure <ref type="figure" coords="2,426.76,609.34,4.25,7.95" target="#fig_1">3</ref> shows a contrasting view of the per-topic e ectiveness (using NDCG@10 to respect the shallow pooling process used in these judgments). At the tail end of both distributions, outliers that outperform the interquartile range are surprisingly common. From this analysis, we answer RQ3 in the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type Description</head><p>RMITUQVBestM2 Manual e best query variation per topic (FDM+QE), as determined by outcomes on the Robust04 collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RMITRBCUQVT5M1 Manual</head><p>Combines the top ve runs per topic based on Robust04. Okapi and SDM+QE was executed over all variations, and the Robust04 collection used to determine the ve best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RMITFDMQEA1</head><p>Automatic FDM plus RM3 query expansion using only the title queries supplied by the track organizers.  Title Only. To compare the e ectiveness of using the best query variations per topic on the Robust04 collection using FDM+QE, as a baseline we submi ed an automatic run that only used the supplied  TREC titles. Figure <ref type="figure" coords="3,388.84,442.53,4.13,7.95" target="#fig_2">4</ref> compares the per-topic AP scores of the titleonly automatic run, sorted by decreasing score, and against the best score for that topic achieved by any of the query variations that were generated. In most cases, the best query variation is more e ective than the TREC title query counterpart, an e ect also noted by Mo at et al. <ref type="bibr" coords="3,421.38,497.32,13.39,7.95" target="#b14">[15]</ref>. However there is a fraction of cases where the TREC title query is more e ective than any of the variations. As we already planned to submit as a baseline an automatic run using the TREC title queries (and probably other participants did likewise), we took the best Robust04-identi ed query variation, despite knowledge that some are performing be er than the best query variation. We did this in an a empt to nd more unique-relevant documents. It is of interest how the performance pro le exhibited in Figure <ref type="figure" coords="3,416.17,584.99,4.22,7.95" target="#fig_2">4</ref> on the Robust04 collection compares with the editorially similar NYT corpus, when the pro les of the corresponding NYT runs RMITUQVBestM2 and RMITFDMQEA1 are able to be inspected with the new judgment set.</p><p>ery Fusion. Bailey et al. <ref type="bibr" coords="3,421.78,634.31,10.55,7.95" target="#b1">[2]</ref> show that fusing query variations improves retrieval e ectiveness. By introducing more diversity into the nal coalesced run, our hypothesis is that it also increases the chances of nding more uniquely relevant documents. Experimentation on Robust04 found that fusing the top ve AP-scored query variation runs for each topic, using the RBC fusion method of Bailey et al., yielded a very high mean (across topics) AP score of 0.430. For each query fusion performed, the RBC persistence parameter ϕ was swept to nd the optimal value using the Robust04 relevance assessments. is process was conducted over two distinct retrieval systems, BM25 and SDM+QE. Finally, the best per-topic fused run was selected with only BM25 runs or SDM+QE runs. In other words, a top-ve fused run at the topic level consisted of a single retrieval system only. e parameters and retrieval system used were logged, and then applied to the New York Times document collection to form the run RMITRBCUQVT5M1.</p><p>Figure <ref type="figure" coords="4,88.78,426.44,4.09,7.95" target="#fig_3">5</ref> shows the retrieval e ectiveness of all three experimental con gurations on the previous Robust04 collection. e query variation approaches are statistically sign cantly more e ective than the title query run (paired two-tailed t-test, p &lt; 0.001), and we anticipate that the same also holds when the same query and weighting arrangements are applied on the New York Times corpus. Of course, these scores must be interpreted with caution, as they are based on tuning derived from knowledge of the corresponding relevance assessments. Even so, the approach shows strong gains on the Robust04 collection; and hence our conjecture was that at least some of those gains would be preserved in the new CORE collection, for which the judgment-based feedback was not so directly applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS</head><p>e CORE track organizers published the relevance assessments formed by NIST for their y topics. We restrict our analysis of our results to the NIST sample as the crowd-sourcing relevance judgments are not yet released.</p><p>All of our runs met the e ectiveness requirements of the track organizers to contribute to the pool of relevance assessments. Figure <ref type="figure" coords="4,79.29,658.57,4.16,7.95">6</ref> shows a box-plot to observe how our knowledge transfer approach worked -note that the distribution of scores on Robust04 is di erent to Figure <ref type="figure" coords="4,137.79,680.49,4.25,7.95" target="#fig_3">5</ref> as only the NIST topics are compared here. is plot helps answer RQ2, where we nd that the "best" UQV found for each topic over AP on the Robust04 collection outperforms the TREC title run. However, when the same approach of selecting the best query variation was employed over the new judgment set, only 12 of the 50 topics had the same "best" query as Robust04. Where the best query variations were selected over the judgment set for failure analysis, an AP score of 0.346 is achieved, outperforming our fusion run. is seems to suggest that there is no "best" query, and that the best query is coupled to the collection. Although we do not get the same performance as fusion, both manual runs exhibit scores with acceptable e ectiveness and contributed 229 uniquely relevant documents that no other system found. Out of all een submissions, our approach ranked fourth, over the 50 topic NIST sample. e most uniquely relevant document count came from the Sabir submission, with a count of 694 from automatic runs. Our submission was the only contribution to the pool that retrieved 3 uniquely relevant documents across both our automatic run and manual runs. e number of uniquely contributed documents for each submi ed run is RMITFDMQEA1: 5, RMITRBCUQVT5M1: 126 and RMITUQVBestM2: 144. Surprisingly, the RMITUQVBestM2 run extracts more unique relevant documents than our fusion run. e number of uniques for RMITUQVBestM2 and RMITRBCUQVT5M1 sum to 270, indicating that 41 uniquely relevant documents overlapped between both retrieval approaches. is is unsurprising, as the top 5 submission contained the top queries used in RMITUQVBestM2. Figure <ref type="figure" coords="5,265.17,196.39,4.09,7.95">7</ref> shows the per-topic breakdown of uniquely relevant documents returned for each of our runs. We observe a relatively even distribution of uniquely contributed documents between the RMITUQVBestM2 and RMITRBCUQVT5M1 approaches, where both runs appear to be complementary. We therefore positively answer RQ1 as query fusion can yield competitive results compared to other participants, however they were not the best over the top 100 -even in the case of our single query run.</p><p>Alongside knowledge of the number of uniquely relevant documents retrieved for each research group, a per-topic breakdown of the best, median and worst scores were supplied to each group for the evaluation metrics AP, NDCG and P@10. ese gures were provided for relative comparison between automatic submissions and manual submissions. Table <ref type="table" coords="5,174.08,349.81,4.25,7.95">3</ref> shows how our automatic run performed relative to others in the same category at a per-topic level. We nd that no topics in our automatic run achieved the best or worst AP or NDCG scores relative to others. We achieve the best P@10 value on four topics (336, 394, 416 and 614), but also achieve the worst scores on four topics (325, 356, 367 and 445). Note that other groups may have tied e ectiveness scores. Less than half of our topics surpassed the median scores on AP and NDCG, indicating that our automatic run was relatively weak compared to other submissions.</p><p>Table <ref type="table" coords="5,85.74,459.40,4.16,7.95">3</ref> also shows the per-topic breakdown of our two manual runs relative to other manual runs. Our query fusion oracle run achieved the best results out of all three runs, where 6 topics performed the best out of all manual submissions for AP: 372, 379, 404, 419, 422 and 443. We also achieved the best NDCG score for ten topics: 341, 353, 379, 404, 416, 419, 443, 614, 620 and 677. Conversely, we achieve the worst performance on topic 690 for AP, and for 626, 646 and 690 for NDCG. As an example, topic 404 occurs in both of these lists, with the title query "Ireland, peace talks" -where the goal is to extract documents that discuss "How o en were the peace talks in Ireland delayed or disrupted as a result of acts of violence?". e most e ective query fusion con guration found on the Robust04 set, were the query variations (in order): "ireland peace talk delay violence bombing", "ireland peace talks disruption roken o violence a ack threat ghter IRA republican army british ulster" [sic], "peace talks delayed Ireland violence", "Ireland peace talk delay disrupt violence" and "North Ireland peace process delayed violence". All of these query variations were fused using RBC ϕ = 0.99, where it was found that BM25 gave more e ective results than SDM+QE for this query on the Robust04 collection. Our best query variation run RMITUQVBestM2 achieved the best AP score for topic 435 "curbing population growth", with the query "population growth control". Finally, in Table <ref type="table" coords="5,389.96,86.80,3.13,7.95">3</ref>, we merge the best and worst scores across automatic and manual submissions to see how we compared globally against all participants. RMITRBCUQVT5M1 achieves the best AP scores on a per-topic level for topics 372; "Native American casino" with AP score 0.691 and 379; "mainstreaming" with score 0.374. e median scores for both of these queries were 0.423 and 0.199 respectively. For NDCG, 379 reappears with an NDCG score of 0.781 where the median is 0.549, and 416; " ree Gorges Project" appears with the NDCG score 0.912 where the median is 0.855.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>All three of our runs met the track organizers' quality criteria for inclusion into the judgment pool. RMITRBCUQVT5M1 was our most e ective run in terms of AP, achieving a score of 0.341, however RMITUQVBestM2 identi ed more unique and relevant documents than the former to cuto 100. We were met with some erce competition in the track, where we were outperformed in uniquely contributed documents by the Sabir run produced by Chris Buckley, and two submissions from the University of Waterloo, placing us fourth out of een participants in this respect. Our automatic run did not appear to perform well compared to other automatic submissions, however we did not anticipate it to do so as it was used as a baseline for comparison with our manual query fusion approaches.</p><p>ery fusion was able to produce a highly e ective result list with a sub-par retrieval model in comparison to other participants. is con rms previous observations the query fusion is an e ective technique for maximizing recall, and in future work we plan to explore this approach further using stronger systems. We look forward to reading about the approaches other participants utilized, and to participating in future ad hoc retrieval tracks.</p><p>Table <ref type="table" coords="6,77.85,84.49,3.45,7.94">3</ref>: e per-topic e ectiveness of our runs when placed into context with other submissions over di erent categories. e Median column represents how many topics we achieve be er or equal scores than the median value across all participants. As we merge the best scores across manual and automatic runs for Auto Manual, the median value across this set is unknown as other participant's runs are not publicly available. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AP</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,343.29,352.35,189.58,7.97;1,317.96,241.49,240.24,97.10"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: e query variation submission interface.</figDesc><graphic coords="1,317.96,241.49,240.24,97.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,53.80,593.34,240.24,7.97;3,53.80,604.31,240.25,7.95;3,53.80,615.27,225.91,7.95"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: NDCG@10 score distributions for all 100 topics in the UQV100 judgment set and the ClueWeb12B corpus. Lines represent IQR, topics are sorted from greatest 75 th percentile to lowest.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,317.96,381.44,240.45,7.97;3,317.96,392.42,241.76,7.95;3,317.96,403.38,240.25,7.95;3,317.96,414.33,73.49,7.95"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison of FDM+QE runs using AP on the Robust04 title-only queries from most e ective to least, and the best pertopic AP query of the TREC CORE query variants, according to the Robust04 collection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,53.80,288.64,240.25,7.97;4,53.80,299.62,137.56,7.95"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: A comparison of e ectiveness by AP, for all runs to be submi ed on the Robust04 collection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="4,317.96,288.64,240.25,7.97;4,317.96,299.62,240.25,7.95;4,317.96,310.58,240.25,7.95;4,317.96,321.54,239.57,7.95"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: A comparison of e ectiveness by AP, for all runs on the 50 NIST assessed topics. Our Robust04 oracle run is listed on the le -restricted to the 50 topics assessed by NIST assessors, for a like-for-like comparison with the NIST assessed plot on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,53.80,187.48,337.04,384.57"><head>Table 2 :</head><label>2</label><figDesc>A brief description of the RMIT runs.</figDesc><table coords="3,53.80,225.66,241.63,346.39"><row><cell></cell><cell>1.00</cell><cell></cell><cell></cell></row><row><cell>Average Precision</cell><cell>0.25 0.50 0.75</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.00</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">TREC CORE 2017 Topics</cell></row><row><cell></cell><cell>Query Types</cell><cell>Best Query Variant</cell><cell>Outlier</cell><cell>Worst Query Variant</cell></row><row><cell cols="5">Figure 2: Average precision score distributions for all 249 topics in</cell></row><row><cell cols="5">the author-generated CORE query variations on Robust04 corpus.</cell></row><row><cell cols="5">Lines represent IQR, topics are sorted from greatest 75th percentile</cell></row><row><cell cols="2">to lowest.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.75</cell><cell></cell><cell></cell></row><row><cell>NDCG@10</cell><cell>0.50</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.25</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.00</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">UQV100 Topics</cell></row><row><cell></cell><cell>Query Types</cell><cell>Best Query Variant</cell><cell>Outlier</cell><cell>Worst Query Variant</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,321.00,669.60,103.49,6.19"><p>h ps://www.lemurproject.org/indri/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="2,321.00,678.00,82.25,6.19"><p>h p://trec.nist.gov/trec eval/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="2,320.88,686.40,117.66,6.19"><p>h p://trec.nist.gov/data/web/10/gdeval.pl</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. is work was supported by the <rs type="funder">Australian Research Council's Discovery Projects Scheme</rs> (<rs type="grantNumber">DP170102231</rs>), by an <rs type="funder">Australian</rs> <rs type="programName">Government Research Training Program Scholarship</rs>, and by a grant from the <rs type="funder">Mozilla Foundation</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_3gAmyXr">
					<idno type="grant-number">DP170102231</idno>
				</org>
				<org type="funding" xml:id="_Vt6gMQe">
					<orgName type="program" subtype="full">Government Research Training Program Scholarship</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,334.39,489.10,223.82,6.19;5,334.13,497.07,131.53,6.19" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,484.20,489.10,74.01,6.19;5,334.13,497.07,60.94,6.19">UQV100: A test collection with query variability</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mo At</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Scholer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Omas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,406.93,497.07,29.32,6.16">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="725" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,334.39,505.04,223.81,6.19;5,334.39,513.01,150.55,6.19" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,481.09,505.04,77.11,6.19;5,334.39,513.01,79.76,6.19">Retrieval consistency in the presence of query variations</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mo At</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Scholer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Omas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,426.21,513.01,29.32,6.16">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="395" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,334.39,520.98,224.58,6.19;5,334.39,528.95,212.25,6.19" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,394.43,528.95,107.65,6.19">RMIT at the TREC 2015 LiveQA Track</title>
		<author>
			<persName coords=""><forename type="first">R.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Culpepper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">T</forename><surname>Damessie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mourad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Scholer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yulanti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,514.42,528.95,28.79,6.16">Proc. TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,334.39,536.92,223.81,6.19;5,334.39,544.89,182.36,6.19" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,503.28,536.92,54.92,6.19;5,334.39,544.89,111.76,6.19">E cient cost-aware cascade ranking in multi-stage retrieval</title>
		<author>
			<persName coords=""><forename type="first">R.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Culpepper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,458.03,544.89,29.32,6.16">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="445" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,334.39,552.86,223.81,6.19;5,334.39,560.83,213.88,6.19" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,490.21,552.86,67.99,6.19;5,334.39,560.83,122.88,6.19">E ciency-e ectiveness tradeo s in two-stage retrieval mechanisms</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Culpepper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,463.12,560.83,23.44,6.16">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="351" to="377" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,334.39,568.80,224.06,6.19;5,334.39,576.77,224.57,6.19;5,334.39,584.74,47.85,6.19" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,553.51,568.80,4.94,6.19;5,334.39,576.77,197.19,6.19">A comparison of document-at-a-Time and score-at-a-Time evaluation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Crane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Culpepper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mackenzie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Trotman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,545.38,576.77,13.59,6.16;5,334.39,584.74,17.04,6.16">Proc. WSDM</title>
		<meeting>WSDM</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="201" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,334.39,592.71,223.81,6.19;5,334.39,600.68,194.41,6.19" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,474.21,592.71,83.99,6.19;5,334.39,600.68,80.71,6.19">Dynamic cuto prediction in multi-stage retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Culpepper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,427.45,600.68,81.27,6.16">Proc. Aust. Doc. Comp. Symp</title>
		<meeting>Aust. Doc. Comp. Symp</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,334.39,608.65,223.81,6.19;5,334.39,616.62,138.59,6.19" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="5,521.03,608.65,37.17,6.19;5,334.39,616.62,61.07,6.19">TREC: Topic engineeRing ExerCise</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Culpepper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mizzaro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Scholer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,407.77,616.62,29.32,6.16">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1147" to="1150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,334.39,624.59,223.81,6.19;5,334.39,632.56,171.24,6.19" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="5,474.07,624.59,84.12,6.19;5,334.39,632.56,56.54,6.19">Language independent ranked retrieval with NeWT</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Culpepper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yasukawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Scholer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,404.29,632.56,77.87,6.16">Proc. Aust. Doc. Comp. Symp</title>
		<meeting>Aust. Doc. Comp. Symp</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="18" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,334.39,640.53,223.82,6.19;5,334.39,648.50,222.26,6.19" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="5,528.89,640.53,29.32,6.19;5,334.39,648.50,118.47,6.19">Improving test collection pools with machine learning</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">K</forename><surname>Jayasinghe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Webber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Culpepper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,464.46,648.50,79.03,6.16">Proc. Aust. Doc. Comp. Symp</title>
		<meeting>Aust. Doc. Comp. Symp</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,334.39,656.47,223.81,6.19;5,334.39,664.44,145.12,6.19" xml:id="b10">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Culpepper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,361.39,664.44,56.41,6.19">IR metrics. Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="416" to="445" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,334.39,672.41,223.81,6.19;5,334.39,680.38,180.29,6.19" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="5,460.32,672.41,97.88,6.19;5,334.39,680.38,115.92,6.19">Can deep e ectiveness metrics be evaluated using shallow judgment pools?</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Culpepper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,462.44,680.39,29.32,6.16">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,334.39,688.35,223.81,6.19;5,334.39,696.32,96.72,6.19" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="5,457.08,688.35,101.12,6.19;5,334.39,696.32,37.38,6.19">Modeling relevance as a function of retrieval rank</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Culpepper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,384.04,696.33,27.20,6.16">Proc. AIRS</title>
		<meeting>AIRS</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,70.23,316.27,223.81,6.19;6,70.23,324.24,82.98,6.19" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="6,224.12,316.27,69.92,6.19;6,70.23,324.24,38.41,6.19">RMIT at the TREC 2016 LiveQA Track</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mackenzie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Culpepper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,120.99,324.24,28.79,6.16">Proc. TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,70.23,332.21,223.95,6.19;6,70.23,340.18,223.81,6.19;6,70.23,348.15,144.04,6.19" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="6,238.80,332.21,55.38,6.19;6,70.23,340.18,204.13,6.19">Incorporating user expectations and behavior into the measurement of search e ectiveness</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mo At</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Scholer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Omas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,279.66,340.18,14.39,6.16;6,70.23,348.15,75.62,6.16">ACM Trans. Information Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">38</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,334.39,316.27,223.81,6.19;6,334.39,324.24,32.22,6.16" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="6,399.35,316.27,147.97,6.19">Overview of the TREC 2004 Robust Retrieval Track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,334.39,324.24,28.79,6.16">Proc. TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
