<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,106.46,71.55,384.64,12.90">UTD HLTRI at TREC 2017: Complex Answer Retrieval Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,145.71,100.96,95.49,10.75"><forename type="first">Ramon</forename><surname>Maldonado</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science Human Language Technology Research Institute</orgName>
								<orgName type="institution">University of Texas at Dallas</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,250.33,100.96,68.97,10.75"><forename type="first">Stuart</forename><surname>Taylor</surname></persName>
							<email>stuart@hlt.utdallas.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science Human Language Technology Research Institute</orgName>
								<orgName type="institution">University of Texas at Dallas</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,344.55,100.96,107.29,10.75"><forename type="first">Sanda</forename><forename type="middle">M</forename><surname>Harabagiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science Human Language Technology Research Institute</orgName>
								<orgName type="institution">University of Texas at Dallas</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,106.46,71.55,384.64,12.90">UTD HLTRI at TREC 2017: Complex Answer Retrieval Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">663FC7B2F6CB221F71B1A3C83C9F2BA0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Complex Question</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents our Complex Answer PAragraph Retrieval (CAPAR) system designed for our participation in the TREC Complex Answer Retrieval (CAR) track. Because we were provided with a massive training set consisting of complex questions as well as the paragraphs that answered each aspect of the complex question, we cast the paragraph ranking as a learning to rank (L2R) problem, such that we can produce optimal results at testing time. We considered two alternative Learning to Rank (L2R) approaches for obtaining the relevance scores of each paragraph: (1) the Siamese Attention Network (SANet) for Pairwise Ranking and (2) AdaRank. The evaluation results obtained for CAPAR revealed that the Siamese Attention Network (SANet) for Pairwise Ranking outperformed AdaRank as the L2R approach for CAPAR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While the problem of textual Question Answering (QA) is not new, most previous research has considered it as either (a) an extension of information retrieval, in which an exact answer is returned to a question instead of a ranked list of relevant documents; or (b) a reading comprehension task, in which the answer to a question is provided by processing only one document. Although several decades of research have been dedicated to both forms of QA, most systems tackled only simple forms of questions, known as factoid questions, e.g. "What is Population of New York City?" or"What does a sea turtle eat?". However, not all questions of interest are simple. The need to process complex questions, e.g. "What are the key activities in the research and development phase of creating new drugs?" was considered in the pioneering work reported in <ref type="bibr" coords="1,418.53,221.95,102.32,9.46" target="#b4">(Harabagiu et al., 2006)</ref>, where complex questions were decomposed into a series of simple questions, capable of being accurately processed by state-of-the-art QA systems of that time. Nevertheless, research in QA involving complex questions is still needed, as many issues remain unresolved. In order to encourage research in the retrieval of answers to more complex information needs (e.g. "Explain the features and issues of the new iPhone."), in 2017 the Text REtrieval Conference Complex Answer Retrieval (TREC CAR) track was designed to evaluate systems performing open-domain, complex answer retrieval. One of the most interesting characteristics of this task is its reliance on Wikipedia.</p><p>Wikipedia is a vast online encyclopedia containing over 5 million articles. The TREC CAR track is based on the assumption that a Wikipedia article represents a complex topic, presenting information about different aspects of the topic contained in the paragraphs of each section and subsection. If the complex topic is viewed as a complex question, than its complete answer should consider all the aspects of the topic. Therefore, given a complex topic articulated as the title of a Wikipedia article, the table of contents of the article should also be considered as an expression of a complex question. For example, consider the complex question (CQ) regarding the "Sea Turtle", the CQ is represented by (1) the title of a Wikipedia article along with (2) all its characterizing aspects, expressed in the table of contents of the article, organized in sections and sub-sections, as illustrated in Figure 1. The article title ("Sea Turtle") specifies a broad, complex topic and each section in the table of contents represents a subtopic or aspect of the topic represented by the article title. Nested sections represent more specific aspects of a topic (e.g. the "Diet" section of the Sea Turtle article is a subtopic of its "Ecology" section which is a Title:</p><p>Sea Turtle  (2) its table of contents.</p><p>subtopic of the sea turtle in general). Moreover, in a Wikipedia article, each section contains text paragraphs that describe the aspect of the topic associated with that section. Moreover, within the text of the paragraphs, mentions of entities are observed, where an entity mention corresponds to a link to another Wikipedia article.</p><p>The TREC CAR track provided two tasks: (1) paragraph retrieval and (2) entity retrieval. For the paragraph retrieval task a participating system should, given a Wikipedia article outline, for each section in the outline, retrieve and rank paragraphs from the collection of Wikipedia paragraphs provided by the organizers. A separate ranking of paragraphs should be provided for each section of the article; i.e. for the "Sea Turtle" article, we would produce 16 separate paragraph rankings for each of the 16 sections. For the entity retrieval task a participating system should instead retrieve and rank entities for each section in a given Wikipedia article outline. In our participation in the 2017 TREC CAR track, we focused only on the paragraph retrieval task.</p><p>The remainder of the paper is organized as fol-lows: Section 2 presents the architecture of our system, Section 4 provides results, which are analyzed in Section 5 while Section 6 summarizes the conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Approach</head><p>The paragraph retrieval task of TREC CAR requires teams to retrieve a ranked list of paragraphs for each section of a Wikipedia article representing aspects of a complex topic. To address this task, we developed the Complex Answer PAragraph Retrieval (CAPAR) system having the architecture illustrated in Figure <ref type="figure" coords="2,447.69,281.64,4.09,9.46" target="#fig_2">2</ref>. Given a complex question (CQ), corresponding to a Wikipedia article title, either from the training or the testing set, the article outline is provided to the Query Processing Module in order to produce for each section of the outline a set of queries, used for searching relevant paragraphs at testing time. The search is made possible by the Paragraph Indexing Module, which creates a searchable index of paragraphs from Wikipedia articles. The role of the Paragraph Search Module is to search each query against the paragraph index, resulting in a list of relevant paragraphs for each section in the Wikipedia article outline. Because we were provided with a massive training set consisting of complex questions (i.e. Wikipedia articles along with their titles and tables of contents) as well as the paragraphs that answered each aspect of the complex question (i.e. the paragraphs from each section of the article), we cast the paragraph ranking as a learning to rank (L2R) problem, such that we can produce optimal results at testing time. The L2R paradigm relies on features extracted from training data. Thus, the Feature Extraction Module was used to extract features from each paragraph and to inform the Paragraph Ranking Module, which produces a separate ranking of the retrieved paragraphs for each section. We considered two alternative Learning to Rank (L2R) approaches for obtaining the relevance scores of each paragraph: (1) the Siamese Attention Network (SANet) for Pairwise Ranking and (2) AdaRank. Finally, a ranking was produced for each section of the article outline using the relevance scores produced by one of the two L2R approaches.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Query Processing</head><p>Searching for relevant paragraphs for each aspect of a complex question entails formulating a query for each section/ subsection of a Wikipedia article outline. In order to facilitate searching for paragraphs pertaining to each section of a Wikipedia article, TREC CAR participants were provided a set of Article Outlines. An Article Outline contains the Article Title and the table of contents for a Wikipedia article 1 as shown in Figure <ref type="figure" coords="3,248.84,410.14,4.09,9.46" target="#fig_0">1</ref>. A simple and obvious method of formulating a query for each section consists of considering each content word from the Section Heading (e.g. the Section Heading for section 6.1 in Figure <ref type="figure" coords="3,237.70,464.33,5.45,9.46" target="#fig_0">1</ref> is "Diet"). However, many Wikipedia articles share the same heading for sections in their outlines, thus the Section Heading alone is not sufficient to specify the information need for the aspect of a complex topic (e.g. "Diet" for section 6.1). Nevertheless, a Section Heading is not isolated in the article outline, but it forms a section heading path connecting it to the title of the article. For example, the section heading path for section 6.1 in Figure <ref type="figure" coords="3,248.63,586.28,5.45,9.46" target="#fig_0">1</ref> is "Sea Turtle → Ecology → Diet". The section heading paths are unique and thus can express the aspects of the complex topic and provide means for formulating a query. Our query processing module has formulated the query for each section using three stages: STAGE 1: consider each content word from the section heading path to generate the query.</p><p>1 Some entries in the table of contents are removed by the task organizers. These include "References", "See also", "External links", and "Further reading". A full list can be found here: http://trec-car.cs.unh.edu/ process/dataselection.html</p><p>In addition, many Wikipedia article titles have associated acronyms and these acronyms are often used in place of the article title in the text of the article. For example, in the "Cancelled-to-order" article, the acronym CTO appears throughout the article instead of "Cancelled-to-order". Additionally, we noticed that if an Article Title has an acronym it usually appears in the article's lead paragraphs. Lead paragraphs, as defined by the task organizers, are the paragraphs in a Wikipedia article appearing before the first section listed in the table of contents. Since Article Title acronyms can be expanded by extracting information from lead paragraphs, we made use of the DBpedia Long Abstracts corpus<ref type="foot" coords="3,405.05,470.85,3.99,6.91" target="#foot_0">2</ref> which provides a mapping from Article Titles to their lead paragraphs. This allowed us to design a simple rule-based extraction of the acronym for the article, if one exists. STAGE 2: consider each content word from the expansion of the acronym of the Wikipedia article title and add it to the query along with the acronym itself.</p><p>In ad-hoc retrieval Raviv et al. ( <ref type="formula" coords="3,475.45,583.27,19.39,9.46">2016</ref>) have shown that incorporating information from automatically extracted entity mentions can improve retrieval. In addition to containing acronyms for the article title, the lead paragraphs supplied by DBPedia function as a high-level description of the article and can contain entity mentions. The entities mentioned in the article lead paragraphs are likely to be related to the article since they are mentioned in its description. To incorporate entity information in our queries, we extract entity men-tions from the article's lead paragraphs using the DBpedia Spotlight entity extractor. STAGE 3: consider each content word from the entity mentions and add it to the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Paragraph Indexing</head><p>To facilitate the retrieval of Wikipedia paragraphs, participants in TREC CAR were provided with a corpus (Paragraph Corpus) containing automatically extracted paragraphs from Wikipedia. Moreover, the Paragraph Corpus contains a set of automatically extracted entities mentioned in each paragraph, where an entity is represented by a Wikipedia article title. The automatically extracted entities correspond to links to other Wikipedia articles. However, the automatically extracted entities in the Paragraph Corpus are often incomplete since each entity mention is not always accompanied by a link. We use the DBpedia Spotlight entity extractor <ref type="bibr" coords="4,186.65,325.57,91.40,9.46" target="#b3">(Daiber et al., 2013)</ref> to extract additional entity mentions from each paragraph. To make the paragraphs and their entity mentions readily available to our system, we index them using Lucene<ref type="foot" coords="4,173.33,377.72,3.99,6.91" target="#foot_1">3</ref> version 5.2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Paragraph Search</head><p>The Paragraph Search module is designed to retrieve a subset of paragraphs for each query from the Paragraph Index (containing over 25 million paragraphs) which can be ranked using our paragraph ranking module in a reasonable amount of time. For each query, we search our paragraph index using the BM25 relevance model <ref type="bibr" coords="4,241.79,503.18,48.48,9.46;4,72.00,516.73,51.69,9.46" target="#b7">(Robertson et al., 1995)</ref> and take the top z results (where z is a hyper-parameter). Since we use a separate module to produce the final paragraph rankings, we strive for a high-recall retrieval from the paragraph search. To increase recall, we combine the sets of results of the queries for the sections in the same article into a single set, P . The Paragraph Search module returns this same set of paragraphs, P , for each section of the article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Feature Extraction</head><p>The role of the Feature Extraction module is to extract dynamic features between a section s m in an article outline and each paragraph p ∈ P returned by the Paragraph Search module. In Learningto-Rank, dynamic features are features that depend on both the query and the document. Re-  <ref type="formula" coords="4,399.79,224.08,19.39,9.46">2017</ref>) have shown that when both word and entity information is available for queries and documents, four types of useful dynamic features can be extracted that model interactions between the words and entities associated with the query and the words and entities associated with the document. In our work, queries are derived from Wikipedia article sections representing subtopics of the complex topic expressed by the article title. We extract 4 types of features: (1) features between words associated with the section subtopic and paragraph words (Sw-Pw), ( <ref type="formula" coords="4,359.24,386.67,4.24,9.46">2</ref>) between section subtopic entities and paragraph words (Se-Pw), (3) between section subtopic words and paragraph entities (Sw-Pe), and (4) between section subtopic entities and paragraph entities (Se-Pe).</p><p>Section Subtopic Word-Paragraph Word (Sw-Pw) Features. The matching and comparison of words to words has been widely studied in information retrieval. Standard retrieval models like BM25 and Language Models (LMs) measure the relevance of a document to a query by aggregating scores between terms from the query and terms from the document. The words representing the section subtopic are taken from the section heading path (the article title and the section heading from each section in the path from the article title in the outline). For each section subtopic word, we extract 9 such features, Φ Sw-Dw , listed in Table 1. A different set of features is extracted for a word from the section heading (Φ h Sw-Pw ), the article title (Φ t Sw-Pw ), and the intermediate section headings (Φ m Sw-Pw ), respectively. Each feature vector Φ Sw-Dw is comprised of three relevance model features (BM25, TF-IDF, and a Dirichlet Smoothed LM) and two wordrelatedness measures (Pointwise Mutual Information and Normalized Google Distance). Normalized Google Distance (NGD) <ref type="bibr" coords="4,463.46,755.86,62.08,9.46">(Cilibrasi and</ref>  </p><formula xml:id="formula_0" coords="5,78.55,247.83,211.72,29.30">N GD(u, v) = max {log f (u), log f (v)} -log f (u, v) log N -min {log f (u), log f (v)} (1)</formula><p>where f (u) returns the number of Google hits for the term u and N is the total number of web pages indexed by Google. We adapt NGD to our task by constraining u to be a word from an article title or heading and v to be a word from a paragraph. In this work, f (u) returns the number of articles with the word u in its title or a contained heading, f (v) returns the number of articles with the word v in a paragraph, and f (u, v) is the number of articles with the word u in the title or a heading and the word v in a paragraph. N is the total number of articles. To compute f (u), f (v), f (u, v), and Pointwise Mutual Information (PMI), we use the training corpus automatically generated from roughly half of Wikipedia. For the NGD and PMI features, we take the average, maximum, and sum of the NGD and PMI of the query word and each word in the paragraph. Section Subtopic Entity-Paragraph Word (Se-Pw) Features. The article containing a section s m represents an entity and the lead paragraph of that article represents a description of that entity, as described in Section 2.1. We will refer to the article containing the section s m as the article entity and the lead paragraph of an entity's article as the entity description. The article entity description contains mentions of other entities likely to be related to the article since they are mentioned in its description. We refer to this collection of related entities and the article entity as the section subtopic entities. For section s m , we extract Se-Pw features between the text of a paragraph and (1) the article entity description and (2) each entity mentioned in the article entity description. For each word in the article entity description, we extract 3 IR retrieval model features (Φ 1 Se-Pw ) listed in Ta- Qe-Pw , each retrieval model is associated with 6 features because we extract a separate feature for the average, maximum, and sum each model's score using both the entity name and entity description. Section Subtopic Words-Paragraph Entity (Sw-Pe) Features. Entities mentioned in a paragraph can be thought of as providing additional information characterizing the paragraph and can help a reader better understand its meaning. For example, someone reading a Wikipedia article might refer to the description of a mentioned entity to gain useful background information necessary to understand the article. Table <ref type="table" coords="5,467.51,511.98,5.45,9.46" target="#tab_3">3</ref> enumerates the features extracted between section subtopic words and paragraph entities. As noted in <ref type="bibr" coords="5,498.27,539.08,27.27,9.46;5,307.28,552.63,50.98,9.46" target="#b8">Xiong et al. (2017)</ref>, not every entity mentioned in a paragraph will be relevant to an arbitrary query so we only consider the maximum score for each retrieval model among all paragraph entities. Section Subtopic Entity-Paragraph Entity (Se-Pe) Features. To model the interaction between section subtopic entities and paragraph entities, we use features similar to those used for comparing section words to paragraph entities. As described above, the article containing a section s m represents an entity and the lead paragraph of that article represents a description of that entity. We extract features between the article entity description and each entity mentioned in the paragraph. Table <ref type="table" coords="5,335.49,742.32,5.45,9.46" target="#tab_4">4</ref> lists the features extracted between section subtopic entities and paragraph entities. As with features between section subtopic words and paragraph entities, we only consider the maximum score for each retrieval model among all paragraph entities. We found that extracting features between the related entities mentioned in the article entity description (used for Se-Pw features) and entities mentioned in a paragraph were too noisy and decreased performance, so we do not extract features considering these entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Paragraph Ranking</head><p>To create our final paragraph rankings, we trained two Learning-to-Rank (L2R) approaches: (1) Siamese Attention Network for Pairwise Ranking and (2) AdaRank. Given a section s m and a set of paragraphs P , each L2R approach produces a relevance score for each paragraph p ∈ P and the final rankings are produced by sorting P by relevance score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1">Siamese Attention Network for Pairwise Ranking</head><p>The Siamese Attention Network (SANet) for Pairwise Ranking is a deep neural network that learns to rank pairs of paragraphs according to their relevance to a Wikipedia article section. <ref type="bibr" coords="6,255.00,400.51,35.27,9.46;6,72.00,414.06,44.61,9.46" target="#b10">Yang et al. (2016)</ref> have shown that deep neural models utilizing an attention mechanism can be used to learn semantic text matching functions for short answer ranking. Moreover, the Word-Entity Duet model <ref type="bibr" coords="6,102.48,468.26,87.28,9.46" target="#b8">(Xiong et al., 2017)</ref> incorporates entity information using IR features to perform document ranking. SANet combines a semantic matching attention mechanism with Word-Entity Duet features to learn scoring function that can be used to determine the relevance of a given paragraph to a section.</p><p>Given the text and features of two paragraphs along with the text of a section heading path, SANet determines which of the two paragraphs is more relevant to the section using the following four main components illustrated in Figure <ref type="figure" coords="6,261.69,617.30,8.49,9.46" target="#fig_3">3:</ref> 1. The Section Path Encoder encodes the sequence of words in a section heading path into a sequence of vector encodings that can be used to perform semantic soft-matching against the words of the two paragraphs;</p><p>2. The Attention Module determines which pairs of paragraph and section words indicate a relevance match and produces a vector representing this soft-matching;</p><p>3. The Feature Encoder encodes the raw features for each section-paragraph pair into a single vector encoding;</p><p>4. The Scorer combines the feature encoding and the attention vector into a single real valued score φ(s, p) representing the relevance of paragraph p to section s.</p><p>SANet is a Siamese network that shares weights among the Feature Encoders, Attention Modules, and Scorers for both input paragraphs. We train the network by optimizing the pairwise hinge loss between the scores from the two halves of the Siamese network:</p><formula xml:id="formula_1" coords="6,307.28,267.75,225.73,10.63">l(s, p 1 , p 2 ) = [1+τ (p 1 , p 2 ) (φ (s, p 1 ) -φ (s, p 2 ))] +</formula><p>(2) where [•] + is the hinge loss and</p><formula xml:id="formula_2" coords="6,308.56,319.36,214.01,26.07">τ (p 1 , p 2 ) = 1 if p 1 is more relevant than p 2 -1 otherwise.</formula><p>(3)</p><p>The Section Path Encoder. In order to facilitate the semantic soft-matching between words from a section heading path and words from a paragraph by the attention module, the Section Path Encoder produces a sequence of vectors, o 1 , . . . , o N representing the sequence of words in a section heading path, w s 1 , . . . w s N . Each word w s i in a section heading path has an associated n-dimensional word embedding, e(w s i ) ∈ R n , learned by GloVe (Pennington et al., 2014) on the full text of Wikipedia that captures fine-grained semantic and syntactic information about the word w s i . However, it is not sufficient to use the GloVe embedding as a representation for the words in a section heading path since the same word may have a completely different meaning depending on its context (e.g. the word "turtle" in the Wikipedia articles "Sea Turtle" and "Turtle Ship"). Moreover, if a word is from the article title of a section heading path, it plays a different role when determining relevance than if it were from a section heading. To capture this sort of context information in our section path encoding, we use a Recurrent Neural Network (RNN) with Gated Recurrent Units (GRU) <ref type="bibr" coords="6,398.12,701.67,92.89,9.46" target="#b1">(Chung et al., 2014)</ref> to produce an encoding of each section heading word that takes context information into account. We insert a special delimiter token between each heading of the section heading path to allow the GRU  to differentiate between words from different section headings. Formally, the encoding for a word w s i from a section heading path is given by</p><formula xml:id="formula_3" coords="8,127.28,113.93,162.99,14.19">o i = GRU (e(w s i ), o i-1 ) (4)</formula><p>where GRU (•) is the Gated Recurrent Unit activation, e(w s i ) is the embedding for word w s i (or the zero-vector if w s i is the heading delimiter token), and o i-1 is the hidden state of the previous GRU cell. The Attention Module. The role of the Attention Module, illustrated in Figure <ref type="figure" coords="8,205.23,221.08,4.55,9.46" target="#fig_3">3</ref>.b is to perform a semantic soft-matching between each word from the paragraph text and each word from the section heading path using a learned a set of weights, called alignments. These alignment weights determine which pairs of words help indicate relevance between a section heading path and a paragraph, even if the words are not the same (i.e. a hard match).</p><p>Formally, for each paragraph word, w p i , there are N alignment weights α i,1 , . . . , α i,N , one for each word in the section path.</p><formula xml:id="formula_4" coords="8,108.31,390.78,181.95,28.94">α ij = exp(e ij ) N k=1 exp(e ik )<label>(5)</label></formula><formula xml:id="formula_5" coords="8,110.21,422.16,180.06,14.19">e ij = v T a tanh (W a d i-1 + U a o j ) (6)</formula><p>where W a , U a ∈ R n×n are weight matrices and v a ∈ R n is a weight vector. Inspired by <ref type="bibr" coords="8,265.43,461.56,19.87,9.46;8,72.00,475.11,77.30,9.46" target="#b0">(Bahdanau et al., 2014)</ref>, we compose the alignments for a paragraph word, w p i , into a single context vector, c i as follows:</p><formula xml:id="formula_6" coords="8,149.07,522.50,141.20,33.71">c i = N j=1 α ij o j (7)</formula><p>We use a Recurrent Neural Network (RNN) to update a hidden state for each paragraph word, given the word's GloVe embedding and its alignments with each section path word. The hidden state d i is updated given a paragraph word, w p i , and its context vector, c i , using the Gated Hidden Unit <ref type="bibr" coords="8,94.58,649.40,103.58,9.46" target="#b0">(Bahdanau et al., 2014)</ref> described in the following equations:</p><formula xml:id="formula_7" coords="8,83.46,682.74,206.81,44.45">d i = (1 -g i ) d i-1 + g i di (8) di = tanh (W e(w p i ) + U [r i d i-1 ] + Cc i )<label>(9)</label></formula><formula xml:id="formula_8" coords="8,83.93,731.61,206.33,32.09">g i = σ(W g e(w p i ) + U g d i-1 + C g c i ) (10) r i = σ(W r e(w p i ) + U r d i-1 + C r c i ) (11)</formula><p>where W, W g , W r , U, U g , U r , C, C g , C r ∈ R n×n are weight matrices and σ(•) is the sigmoid function.</p><p>The final attention vector representing the semantic soft-matching between a paragraph p and the section heading path of a section s is given by a(s, p) = d M where M is the length of paragraph p. The Feature Encoder. To incorporate features extracted between a section s and a paragraph p, SANet encodes the full set of features into a single vector, f (s, p), called the feature encoding. Recall from section 2.4 that we extract (1) two feature vectors for each word in the section heading path (Φ Sw-P w , Φ Sw-P e ), ( <ref type="formula" coords="8,444.01,257.86,4.24,9.46">2</ref>) two feature vectors for each word in the article lead paragraph (Φ 1</p><p>Se-P w , Φ Se-P e ), and (3) one feature vector for each entity mentioned in the article lead paragraph (Φ 2</p><p>Se-P w ). Depicted in Figure <ref type="figure" coords="8,450.01,312.06,3.94,9.46" target="#fig_3">3</ref>.c, we compose each feature vector into a single feature encoding, f (s, p), using a series of 1-dimensional convolution operations with 1 filter and a linear activation <ref type="bibr" coords="8,307.28,366.25,89.33,9.46" target="#b8">(Xiong et al., 2017)</ref>. Together, the elements of f (s, p) correspond to (1) each word from the section heading, (2) each word from the article title, (3) words from an intermediate heading, (4) each word from the article lead paragraph, and (5) each entity from the article lead paragraph. Formally, the i th element of the feature encoding f (s, p) i is given by the equation:</p><formula xml:id="formula_9" coords="8,370.41,487.28,155.13,14.27">f (s, p) i = v T f Φ + b f (12)</formula><p>where v f is a learned weight vector, b f is a learned bias weight, and Φ is either [Φ h Sw-P w , Φ Sw-P e ], [Φ t Sw-P w , Φ Sw-P e ], [Φ a Sw-P w , Φ Sw-P e ], [Φ Se-P w , Φ Sw-P e ], or Φ Se-P e depending on which of the five fields the features are extracted from. We learn different parameters, v f and b f for each of the five fields, as well. The Scorer. The Scorer learns a function that combines the feature encoding with the attention vector to produce a real valued score representing how relevant a paragraph is to the section. Formally, the score φ(s, p) for a section s and a paragraph p is given by the following equation:</p><formula xml:id="formula_10" coords="8,334.16,707.91,191.38,15.81">φ(s, p) = v T s [a(s, p), f (s, p)] +<label>(13)</label></formula><p>where v T s ∈ R |a(s,p)|+|f (s,p)| is a learned weight vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2">AdaRank</head><p>To test the effectiveness of our features and SANet we used the learning to rank algorithms from the publicly available RankLib library<ref type="foot" coords="9,222.19,108.26,3.99,6.91" target="#foot_2">4</ref> . The best performing RankLib algorithm on the validation set was AdaRank <ref type="bibr" coords="9,138.70,137.41,81.98,9.46" target="#b9">(Xu and Li, 2007)</ref>. To make the extracted features usable by AdaRank (which requires a fixed-sized feature vector), we take the average, sum, and maximum value of each feature type's values. We use the -zscore option in RankLib which normalizes each feature by its mean/standard deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.3">Training Details</head><p>In this section we describe the training data given to participants by the task organizers and how this data is used to train the Learning to Rank approaches.</p><p>To enable machine learning approaches that require large amounts of data, like SANet, each participating team is provided silver-standard training data (called the "train-v1.5 set") where only the paragraphs from a section of a Wikipedia article are labeled as relevant to that section. The train-v1.5 set is comprised of half of all Wikipedia articles that meet the selection criteria<ref type="foot" coords="9,245.17,404.36,3.99,6.91" target="#foot_3">5</ref> , and the train-v1.5 set is split into 5 folds. Moreover, two smaller subsets of the train-v1.5 set, comprising 316 articles, are manually selected to provide data with similar characteristics, types, and structure as the articles in the test topics. The first of the two smaller sets is the "test200 set" set comprising 199 articles from the first fold of the train-v1.5 set, and second of the two smaller sets is the "bench-markY1train set" comprising 117 articles selected from all folds of the train-v1.5 set.</p><p>To train SANet we use all folds from the train-v1.5 set (with the test200 set removed from the first fold) to create our training data, and use the test200 set as a development set. We trained AdaRank using the test200 set as training data, and the benchmarkY1train set as a development set. However, for our AdaRank submission we trained AdaRank using both the benchmarkY1train set and test200 set as training data. For both SANet and AdaRank we used the benchmarkY1train set to determine our choice of z (retrieval set size in the Paragraph Retrieval module).</p><p>In order to train pairwise learning-to-rank (L2R) approaches, training data consisting of pairs of paragraphs with different levels of relevancy is necessary. Since the given training data only indicates which paragraphs are relevant for each section, we needed to automatically locate irrelevant paragraphs to train our rankers. Additionally, pairwise Learning-to-Rank can make use of different degrees of relevancy. To take advantage of this property, we use the following 4 degrees of relevancy (from most relevant to least relevant):</p><p>1 for paragraphs from the correct section; 0 for paragraphs from the correct article but incorrect section;</p><p>-1 for paragraphs from the incorrect article but having some information in common;</p><p>-2 for random paragraphs.</p><p>The pairwise L2R approaches are trained by providing pairs of training examples with different relevance labels from the set of four labels listed above. We extracted paragraphs matching each relevance degree using the following assumptions:</p><p>To reduce the amount of noise in our training data, we ignore any paragraph with fewer than 4 words or greater than 200 words. Paragraphs from the correct section. We consider paragraphs that are from the correct article and section as the most relevant in our training data.</p><p>Paragraphs from the correct article but incorrect section. The first set of irrelevant paragraphs for each section are paragraphs that appear in the same article, but in a different section of the article. These paragraphs are chosen because, unlike random examples which are likely to be completely off topic, they discuss the same topic while still being irrelevant due to discussing the wrong aspect of the topic. For example, in the "Sea Turtle" article both the "Description" and "Ecology" sections are discussing sea turtles, but the "Description" section is discussing an aspect that is different from the "Ecology" section. However, we also noticed that some paragraphs from the correct article but incorrect section can be more relevant than others. For example, paragraphs from section "6.2 Relationship with humans" of the "Sea Turtle" article are more relevant to section "6.4 Conservation status and threats" than they are to section "1 Description", since sections 6.2 and 6.4 are both subtopics of the ecology of the Sea Turtle. To allow the L2R approaches to model this phenomenon, we introduce pairwise training examples where both paragraphs are from the correct article but incorrect section, however one paragraph shares more sections in common with the correct section's section path than the other.</p><p>Paragraphs from the incorrect article but having some information in common. For this degree of relevancy we have two categories which are discussed below. The first category is paragraphs that appear in a different article, but have the same Section Heading. These paragraphs are chosen because they are off topic, but are likely to use language similar to relevant paragraphs. For example, in the "Description" section of the "Sea Turtle" and "Dire Wolf" articles the weight, length, height, and other physical properties of the animals are discussed. The second category is paragraphs that appear in a different article, and have some entity mentions in common. We include these paragraphs to avoid biasing our learners toward entity mentions. Random paragraphs. We include randomly sampled paragraphs from the entire training corpus as irrelevant examples to ensure our rankers can distinguish between completely off topic paragraphs, and paragraphs that are at least partially relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Overview of Runs</head><p>We submitted three runs to TREC CAR 2017. All three runs use the CAPAR architecture, but use different Learning to Rank relevance models (SANet and AdaRank) and different values of z (the initial number of documents retrieved for each section in the Paragraph Search module). An overview of the runs is presented below:</p><p>1. UTDHLTRINN50: This run used SANet for relevance scoring with a retrieval size of 50 in the Paragraph Search Module (z=50).</p><p>2. UTDHLTRINN20: This run used SANet for relevance scoring with a retrieval size of 20 in the Paragraph Search Module (z=20).</p><p>3. UTDHLTRIAR: This run used AdaRank for relevance scoring with a retrieval size of 10 in the Paragraph Search Module (z=10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>The results for CAPAR in the 2017 TREC CAR evaluation are summarized in Table <ref type="table" coords="10,464.63,254.54,4.09,9.46" target="#tab_6">5</ref>. Three evaluation protocols are used to evaluate the paragraph retrieval task: (1) automatic evaluation, (2) Manual Evaluation, and (3) Lenient Evaluation.</p><p>The automatic evaluation uses silver-standard relevance judgments that are extracted the same way as in the train-v1.5 set (i.e. relevant if it appeared in that section of the article, assumed irrelevant otherwise). For the lenient and manual evaluations, the top 100 results for participant systems were pooled, and judgments were created by National Institute for Standards and Technology (NIST) judges assigning each paragraph a score of 3 (MUST BE MENTIONED), 2 (SHOULD BE MENTIONED), 1 (COULD BE MENTIONED), 0 (ROUGHLY ON TOPIC), -1 (NON-RELEVANT), or -2 (TRASH). In the lenient evaluation a paragraph is considered relevant if it is determined to be at least ROUGHLY ON TOPIC. However, in the manual evaluation paragraphs are only relevant if their relevance is at least COULD BE MENTIONED. For both the manual and lenient evaluations a paragraph is considered irrelevant if it has not been manually assessed for relevance.</p><p>Clearly, the runs 1 and 2 using the SANet relevance model perform best on the automatic evaluation, with UTDHLTRINN50 slightly outperforming UTDHLTRINN20 on each evaluation metric. This is not surprising considering that the two neural models were trained on substantially more training examples with the same automatically extracted judgments used by the evaluation. However, the two SANet runs also outperform AdaRank in the manual and lenient evaluations, as well in 5 of the 6 metrics. Interestingly, while UTDHLTRINN50 outperforms UTDHLTRINN20 in each metric for the automatic evaluation, UT-DHLTRINN20 outperforms UTDHLTRINN50 in both R-Precision and Mean Reciprocal Rank for the lenient and manual evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Complex answer retrieval is a challenging task and it is clear from these results that further research is necessary to address its complexities. The evaluations suggest that the semantic matching performed by SANet's attention module allows SANet to outperform AdaRank. Interestingly, providing the Paragraph Ranking module using SANet with a larger set of paragraphs to rank (UTDHLTRINN50 vs UTDHLTRINN20) increases performance on the automatic evaluations but decreases performance on the manual and lenient evaluations. Moreover, the recall of the Paragraph Search module's top 50 results is 0.4906, hindering the theoretically achievable performance of the final rankings. This suggests that, improved Query Processing and Paragraph Search are necessary for complex answer retrieval.</p><p>While the SANet Learning to Rank approach outperforms AdaRank according to the evaluations, SANet struggles to correctly score some paragraphs for several reasons. For example, consider the "Wine" section of the "Aftertaste" article with the section heading path: "Aftertaste → Foods with distinct aftertastes → Wine". The 4th top result for this section produced by CA-PAR using the SANet relevance model is the paragraph: "The aroma of Morbier is strong, but the flavor is rich and creamy, with a slightly bitter aftertaste." While this paragraph contains a description of the flavor and aftertaste of "Morbier", the system fails to recognize that "Morbier" is a cheese, not a wine, causing the paragraph to be irrelevant. The inclusion of knowledge resources could help to address this deficiency. We experimented with an embedded knowledge graph created from Wikidata 6 with graph edges corresponding to Wikipedia links removed (since resources utilizing Wikipedia links were forbidden for this task). However, without links from Wikipedia itself, the Wikidata knowledge graph did not improve performance during validation.</p><p>Another typical cause of errors made by CA-PAR is that it can rely on word matching for rare words. For example, for the section path "Disturbance (ecology)" → "Criteria", relevant paragraphs describe the criteria of an ecological dis-6 www.wikidata.org turbance without saying the actual word "criteria". The fact that these paragraphs describe criteria is implied, though not explicitly stated. However, the top ranked results for CAPAR contain explicit mentions of the word "criteria". Future work may benefit from modeling this sort of implied relationship between a section heading and a relevant paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>The CAPAR system performed at its best when the paragraph ranking benefits from the Siamese Attention Network (SANet) for Pairwise Ranking. The evaluations suggest that the semantic matching performed by SANet's attention module allows SANet to outperform AdaRank. Interestingly, providing the Paragraph Ranking module using SANet with a larger set of paragraphs to rank increases performance on the automatic evaluations but decreases performance on the manual and lenient evaluations. While the learning to rank based on SANet outperforms AdaRank, it still produces errors. This indicated that additional research is required for paragraph retrieval in response to complex questions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,72.00,366.81,218.27,9.46;2,72.00,380.36,218.27,9.46;2,72.00,393.91,218.27,9.46;2,72.00,407.46,103.32,9.46"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Example of complex question as the topic "Sea Turtle". Each complex question is represented by (1) the title of a Wikipedia article and (2) its table of contents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,189.26,249.97,219.02,9.46"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The Architecture of the CAPAR system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,72.00,711.18,453.55,9.46;7,72.00,724.73,350.59,9.46"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The Siamese Attention Network for Pairwise Ranking. The full network is illustrated in (a), the Attention module is depicted in (b), and the Feature Encoder is shown in (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,307.28,73.11,218.27,160.43"><head>Table 1 :</head><label>1</label><figDesc>Ranking features from section words to paragraph words (Φ Sw-Pw ).</figDesc><table coords="4,307.28,99.90,197.93,133.64"><row><cell>Feature Description</cell><cell>Dimension</cell></row><row><cell>BM-25</cell><cell>1</cell></row><row><cell>TF-IDF</cell><cell>1</cell></row><row><cell>Dirichlet Smoothed LM</cell><cell>1</cell></row><row><cell>Normalized Google Distance</cell><cell>3</cell></row><row><cell>Pointwise Mutual Information</cell><cell>3</cell></row><row><cell>Total</cell><cell>9</cell></row><row><cell>cently, Xiong et al. (</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,72.00,73.11,218.27,165.20"><head>Table 2 :</head><label>2</label><figDesc>Ranking features from section subtopic entities to paragraph words. Φ 1Se-Pw refers to query article lead paragraph features and Φ 2</figDesc><table coords="5,268.52,103.54,21.25,7.17"><row><cell>Se-Pw</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,307.28,73.11,218.27,91.24"><head>Table 3 :</head><label>3</label><figDesc>Ranking features from section subtopic words to paragraph entities (Φ Sw-Pe ).</figDesc><table coords="5,334.07,99.90,164.68,64.45"><row><cell>Feature Description</cell><cell>Dimension</cell></row><row><cell>BM-25</cell><cell>1</cell></row><row><cell>TF-IDF</cell><cell>1</cell></row><row><cell>Dirichlet Smoothed LM</cell><cell>1</cell></row><row><cell>Total</cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,307.28,190.16,218.27,155.14"><head>Table 4 :</head><label>4</label><figDesc>Ranking features from section subtopic entities to paragraph entities (Φ Se-Pe ).</figDesc><table coords="5,307.28,216.95,218.27,128.35"><row><cell>Feature Description</cell><cell cols="2">Dimension</cell></row><row><cell>BM-25</cell><cell>1</cell></row><row><cell>TF-IDF</cell><cell>1</cell></row><row><cell>Dirichlet Smoothed LM</cell><cell>1</cell></row><row><cell>Total</cell><cell>3</cell></row><row><cell cols="3">ble 2. For each entity mentioned in the article en-</cell></row><row><cell cols="2">tity description, we extract 18 features (Φ 2</cell><cell>Qe-Pw ),</cell></row><row><cell>also listed in Table 2. For Φ 2</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="10,72.00,64.34,453.55,84.18"><head>Table 5 :</head><label>5</label><figDesc>Results for submitted runs on all evaluations using Mean Average Precision (MAP), R-Precision (R-Prec), and Mean Reciprocal Rank (MRR). Highest scores in bold.</figDesc><table coords="10,86.59,64.34,424.36,48.42"><row><cell></cell><cell></cell><cell>Automatic</cell><cell></cell><cell></cell><cell>Lenient</cell><cell></cell><cell></cell><cell>Manual</cell><cell></cell></row><row><cell>Run ID</cell><cell>MAP</cell><cell>R-Prec</cell><cell>MRR</cell><cell>MAP</cell><cell>R-Prec</cell><cell>MRR</cell><cell>MAP</cell><cell>R-Prec</cell><cell>MRR</cell></row><row><cell>UTDHLTRINN50 (z=50)</cell><cell cols="9">0.1092 0.0832 0.1672 0.1920 0.2724 0.4437 0.1475 0.1648 0.3426</cell></row><row><cell>UTDHLTRINN20 (z=20)</cell><cell cols="9">0.1049 0.0830 0.1608 0.1893 0.3048 0.4723 0.1373 0.1713 0.3453</cell></row><row><cell>UTDHLTRIAR(z=10)</cell><cell cols="9">0.0893 0.0751 0.1351 0.1406 0.2564 0.4844 0.1076 0.1463 0.3283</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="3,323.42,747.17,202.13,7.77;3,307.28,757.97,157.01,6.31"><p>Data set can be found here: http://wiki. dbpedia.org/downloads-2016-10</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="4,88.14,757.97,134.50,6.31"><p>http://lucene.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="9,88.14,727.17,199.05,6.31;9,72.00,737.13,43.04,6.31"><p>https://sourceforge.net/p/lemur/wiki/ RankLib/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="9,88.14,748.01,188.80,6.31;9,72.00,757.97,96.84,6.31"><p>http://trec-car.cs.unh.edu/process/ dataselection.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,307.28,457.93,218.27,8.64;11,318.19,468.89,207.36,8.64;11,318.19,479.68,207.35,8.81;11,318.19,490.63,70.29,8.58" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="11,371.42,468.89,154.13,8.64;11,318.19,479.84,129.94,8.64">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName coords=""><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,307.28,513.25,218.27,8.64;11,318.19,524.21,207.36,8.64;11,318.19,535.17,207.36,8.64;11,318.19,545.96,148.50,8.81" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m" coord="11,431.15,524.21,94.40,8.64;11,318.19,535.17,207.36,8.64;11,318.19,546.13,11.42,8.64">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,307.28,568.58,218.27,8.64;11,318.19,579.37,202.28,8.81" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="11,510.05,568.58,15.49,8.64;11,318.19,579.54,101.53,8.64">The google similarity distance</title>
		<author>
			<persName coords=""><forename type="first">Rudi</forename><surname>Cilibrasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">B</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Vitányi</surname></persName>
		</author>
		<idno>CoRR, abs/cs/0412098</idno>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,307.28,601.99,218.27,8.64;11,318.19,612.95,207.36,8.64;11,318.19,623.74,207.36,8.81;11,318.19,634.70,207.36,8.58;11,318.19,645.66,119.80,8.58" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,424.16,612.95,101.39,8.64;11,318.19,623.91,166.92,8.64">Improving efficiency and accuracy in multilingual entity extraction</title>
		<author>
			<persName coords=""><forename type="first">Joachim</forename><surname>Daiber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Max</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Hokamp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pablo</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,507.73,623.74,17.81,8.58;11,318.19,634.70,207.36,8.58;11,318.19,645.66,115.79,8.58">Proceedings of the 9th International Conference on Semantic Systems (I-Semantics)</title>
		<meeting>the 9th International Conference on Semantic Systems (I-Semantics)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,307.28,668.28,218.27,8.64;11,318.19,679.24,207.36,8.64;11,318.19,690.03,207.36,8.81;11,318.19,700.98,207.36,8.58;11,318.19,711.94,207.35,8.81;11,318.19,723.07,44.71,8.64" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,347.62,679.24,177.92,8.64;11,318.19,690.19,48.73,8.64">Answering complex questions with random walk models</title>
		<author>
			<persName coords=""><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Finley</forename><surname>Lacatusu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Hickl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,386.49,690.03,139.06,8.58;11,318.19,700.98,207.36,8.58;11,318.19,711.94,151.42,8.58">Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 29th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="220" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,307.28,745.52,218.27,8.64;11,318.19,756.48,207.36,8.64;12,82.91,67.11,207.36,8.81;12,82.91,78.07,207.36,8.58;12,82.91,89.03,148.59,8.81" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,394.21,756.48,131.34,8.64;12,82.91,67.28,55.00,8.64">Glove: Global vectors for word representation</title>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,159.45,67.11,130.82,8.58;12,82.91,78.07,207.36,8.58;12,82.91,89.03,68.38,8.58">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.00,109.12,218.27,8.64;12,82.91,120.08,207.36,8.64;12,82.91,130.87,207.36,8.81;12,82.91,141.83,207.36,8.58;12,82.91,152.79,203.39,8.81" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,82.91,120.08,207.36,8.64;12,82.91,131.04,26.81,8.64">Document retrieval using entity-based language models</title>
		<author>
			<persName coords=""><forename type="first">Hadas</forename><surname>Raviv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oren</forename><surname>Kurland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Carmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,134.79,130.87,155.48,8.58;12,82.91,141.83,207.36,8.58;12,82.91,152.79,117.17,8.58">Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.00,172.89,218.27,8.64;12,82.91,183.84,207.36,8.64;12,82.91,194.63,207.36,8.81;12,82.91,205.59,115.68,8.81" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="12,140.63,194.63,149.64,8.81;12,82.91,205.59,35.70,8.58">Okapi at trec-3. Nist Special Publication Sp</title>
		<author>
			<persName coords=""><forename type="first">Steve</forename><surname>Stephen E Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Susan</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Micheline</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gatford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="500" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.00,225.69,218.27,8.64;12,82.91,236.65,207.36,8.64;12,82.91,247.44,207.36,8.81;12,82.91,258.40,207.36,8.58;12,82.91,269.35,207.36,8.81;12,82.91,280.48,113.18,8.64" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,82.91,236.65,207.36,8.64;12,82.91,247.61,11.42,8.64">Word-entity duet representations for document ranking</title>
		<author>
			<persName coords=""><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,114.79,247.44,175.48,8.58;12,82.91,258.40,207.36,8.58;12,82.91,269.35,135.68,8.81">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;17</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="763" to="772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.00,300.41,218.27,8.64;12,82.91,311.20,207.36,8.81;12,82.91,322.16,207.36,8.58;12,82.91,333.12,207.36,8.58;12,82.91,344.07,207.36,8.81;12,82.91,355.20,67.77,8.64" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,190.81,300.41,99.46,8.64;12,82.91,311.37,129.54,8.64">Adarank: A boosting algorithm for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,230.61,311.20,59.66,8.58;12,82.91,322.16,207.36,8.58;12,82.91,333.12,207.36,8.58;12,82.91,344.07,85.45,8.81">Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;07</title>
		<meeting>the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="391" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.00,375.13,218.27,8.64;12,82.91,386.09,207.36,8.64;12,82.91,396.88,207.36,8.81;12,82.91,407.84,207.36,8.58;12,82.91,418.79,207.36,8.58;12,82.91,429.92,89.81,8.64" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,144.69,386.09,145.58,8.64;12,82.91,397.05,171.07,8.64">anmm: Ranking short answer texts with attention-based neural matching model</title>
		<author>
			<persName coords=""><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,167.28,407.84,122.99,8.58;12,82.91,418.79,202.42,8.58">ACM International on Conference on Information and Knowledge Management</title>
		<imprint>
			<biblScope unit="page" from="287" to="296" />
			<date type="published" when="2016">2016</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
	<note>In Proceedings of the 25th</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
