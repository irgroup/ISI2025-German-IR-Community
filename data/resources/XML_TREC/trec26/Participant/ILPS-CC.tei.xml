<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,161.41,112.60,289.18,14.93">ILPS at TREC 2017 Common Core Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,181.97,144.91,102.77,10.37"><forename type="first">Christophe</forename><surname>Van Gysel</surname></persName>
							<email>cvangysel@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,292.87,144.91,31.98,10.37"><forename type="first">Dan</forename><surname>Li</surname></persName>
							<email>d.li@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,332.39,144.91,97.65,10.37;1,430.03,142.92,1.41,6.99"><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
							<email>e.kanoulas@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,161.41,112.60,289.18,14.93">ILPS at TREC 2017 Common Core Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">55717FFA8663190A102327DF66E81DE4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The TREC 2017 Common Core Track aimed at gathering a diverse set of participating runs and building a new test collection using advanced pooling methods. In this paper, we describe the participation of the IlpsUvA team at the TREC 2017 Common Core Track. We submitted runs created using two methods to the track: (1) BOIR uses Bayesian optimization to automatically optimize retrieval model hyperparameters. (2) NVSM is a latent vector space model where representations of documents and query terms are learned from scratch in an unsupervised manner.</p><p>We find that BOIR is able to optimize hyperparameters as to find a system that performs competitively amongst track participants. NVSM provides rankings that are diverse, as it was amongst the top automated unsupervised runs that provided the most unique relevant documents.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>TREC 2017 Core Track aims to bring the information retrieval community back into a traditional ad-hoc search task. The primary goal of itself is to build new test collections using recently created documents using new test collection construction methodology based on a diverse set of participating runs. In this work, we applied two methods on the ad-hoc task: a Bayesian Optimization intensified lexical method (BOIR), and a latent vector space method, named neural vector space model (NVSM).</p><p>BOIR uses Bayesian Optimization method to automatically optimize configurations of Retrieval system. In this work, we take Indri as our experiment platform. Indri is a search engine pipeline consisting of many components such as indexing module, retrieval module, and pseudo-relevance feedback module. It has a big configuration (or hyperparameter) space. We use Bayesian Optimization, a sequential decision making method which suggests the next most promising configuration to be tested on the basis of the retrieval effectiveness of hyperparameters that have been examined so far, to jointly search and optimize over the hyperparameter space. As the retrieval models in Indri are TF-IDF, BM25, Language model, we submitted BOIR run -IlpsUvABoir -as a lexical run.</p><p>NVSM, on the other hand, is a method that learns representations of documents in an unsupervised manner for news article retrieval. In the NVSM paradigm, we learn low-dimensional representations of words and documents from scratch using gradient descent and rank documents according to their similarity with query representations that are composed from word representations. Contrast to BOIR, NVSM overcomes the vocabulary mismatch between query and documents and provide semantic matching. We therefore also submitted two runs generated by NVSM: IlpsUvANvsm and IlpsUvAQlmNvsm.</p><p>This paper is organized as follows: Section 2 describes our approach to this task. Section 3 discusses the results obtained from applying this approach to the test datasets. Finally, Section 4 presents the conclusions and potential future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural Models for IR</head><p>Neural Vectors Spaces Models for Unsupervised Retrieval. We constructed the vector space models from <ref type="bibr" coords="2,499.31,118.42,40.70,8.64;2,72.00,130.37,49.11,8.64">Van Gysel et al. (2017a</ref><ref type="bibr" coords="2,121.11,130.37,30.74,8.64">, 2016)</ref> on the New York Times corpus. The rankings of the IlpsUvANvsm run were obtained using the methodology as outlined in the Unsupervised Deployment section of <ref type="bibr" coords="2,368.35,142.33,97.34,8.64">Van Gysel et al. (2017a)</ref>. To construct the rankings of the IlpsUvAQlmNvsm run, we took the scores of the IlpsUvANvsm rankings and combined them with the log-probability retrieval scores of the Query-Likelihood Model <ref type="bibr" coords="2,337.49,166.24,100.69,8.64" target="#b8">(Zhai and Lafferty, 2004)</ref> with Dirichlet smoothing (µ = 1000, i.e., Indri's default value). The combination was obtained by summing the per-query standarized scores of the QLM and the ensemble of NVSM. An open-source implementation of NVSM is available at https://github. com/cvangysel/cuNVSM. Hyperparameter Optimization Process. The Bayesian Optimization framework provides a mechanism to sequentially search for the global optimum x of an objective function f (x) : X → R. There are two key components in Bayesian Optimization <ref type="bibr" coords="2,169.08,558.44,88.63,8.64" target="#b3">(Shahriari et al., 2016)</ref>. The first is a probabilistic surrogate model used to predict the objective function value y given a point x. For every x, there is a random variable y, whose distribution p(y|x) is given by the surrogate model. The predictive distribution of y can then be used to construct an acquisition function. An acquisition function is a policy for selecting the sequence of points {x 1 , x 2 , ..., x i , ...}, i.e. a mechanism to select the next configuration x n+1 to test given D 1:n := {(x 1 , y 1 ), (x 2 , y 2 ), ..., (x n , y n )}. As the acquisition function x is usually a closed-form expression of hyperparameters it is easier to be optimized than the original objective function. The second component is the objective function itself, a function of the target model requiring hyperparameter optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Bayesian Optimization for IR</head><p>Following the Bayesian Optimization framework for optimizing retrieval systems, proposed by (Li and Kanoulas, 2018), we have two major modules in our algorithmic pipeline, the IR module and the BO module (see Figure <ref type="figure" coords="2,510.67,654.08,3.60,8.64" target="#fig_0">1</ref>). The IR module tackles the conditional hyperparameters, and computes the objective function value y n , given a hyperparameter configuration x n . The BO module adds (x n , y n ) into the sample set, updates the posterior distribution of the surrogate models, selects the next hyperparameter configuration x n+1 , and passes it back to IR module. The process stops when the stop condition is satisfied, such as the computation budget is run out.</p><p>Implementation of Retrieval System and Bayesian Optimization. We use Pyndri <ref type="bibr" coords="3,428.94,99.39,99.27,8.64">(Van Gysel et al., 2017b)</ref>, a Python Interface to the Indri Search Engine <ref type="bibr" coords="3,246.47,111.34,90.58,8.64" target="#b4">(Strohman et al., 2005)</ref> <ref type="foot" coords="3,339.01,109.67,3.49,6.05" target="#foot_0">1</ref> , as the IR module in our pipeline, which is mainly decomposed to indexing, retrieval, and pseudo-relevance feedback modules. All the three modules are considered in our optimization experiments. The objective function can be any retrieval effectiveness measure. In this work we optimize for the Mean Average Precision (MAP) and use trec_eval<ref type="foot" coords="3,345.17,145.54,3.49,6.05" target="#foot_1">2</ref> for the computations. We use Pybo <ref type="bibr" coords="3,500.97,147.21,39.03,8.64;3,72.00,159.16,81.57,8.64" target="#b1">(Hoffman and Shahriari, 2014)</ref> in our experiments, a Python package for Bayesian Optimization . We set Gaussian process with Squared Exponential covariance function as the surrogate model, and Expected Improvement <ref type="bibr" coords="3,441.26,171.12,98.74,8.64" target="#b0">(Dixon and Szegö, 1978)</ref> as the acquisition function. The hyperparameter optimization precess was conducted on the Rubost04 topics and the corresponding document collection of Volume 4 &amp; 5 (minus Congress Record).</p><p>Search Space of Hyperparameters. When configuring Indri, there are two choices to make when indexing documents: the stopword list and the stemmer, and three retrieval models to choose: TF-IDF with BM25 term weighting, Okapi BM25, and Language Models. Indri also supports pseudo-relevance feedback models. Each of these choices contains subsequent parameters to be decided. In total, we have a conditional hyperparameter space of 18 dimensions (see Table <ref type="table" coords="3,413.33,535.07,3.60,8.64" target="#tab_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>The IlpsUvA team submitted 3 automatic runs to TREC Core Track. IlpsUvABoir is an automated run that made use of the existing Robust04 judgments. The two automated remaining runs, IlpsUvANvsm and IlpsUvAQlmNvsm, were constructed without any prior knowledge of the topics.</p><p>TREC Core Track provides two types of relevance assessments: judgments provided by NIST assessors and judgments obtained through crowd sourcing. In this paper, we report the result evaluated on the 50 topics judged by the NIST assessors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Test Dataset</head><p>TREC Core track uses The New York Times corpus<ref type="foot" coords="4,281.78,92.49,3.49,6.05" target="#foot_2">3</ref> as the document collection, and the TREC Robust Track topics as its topics. Most of the topics have remained the same but some have been revised to reflect the time past. There are two sets of topics: topics judged by NIST and topics to be judged by crowd sourcing. Submission should either be either on the 50 topics to be judged by NIST or on all 250 topics to be judged by crowd workers. The 3 runs submitted by IlpsUvA include all the 250 topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">IlpsUvA runs</head><p>Table 2 compares the overall performance of ILPS within the TREC Core Track. The entries in the table listed as best, median and worst correspond to the average of the respective aggregate across all topics (i.e., a virtual system). That is, the best automated run is the average (across topics) of the maximum AP (across participating runs). It is important to note that the aggregate measures of the automated runs provided by the organizers were computed from runs that used existing relevance judgments. IlpsUvABoir performs worse than the best automated run. To little surprise, IlpsUvABoir performs better than the worst automated run. IlpsUvABoir also performs better than the median automated run in terms of AP and NDCG. When optimizing for the Robust04 topics and relevance judgments, the Bayesian optimization algorithm chose the following hyperparameters: the QLM <ref type="bibr" coords="4,416.60,272.86,102.79,8.64" target="#b8">(Zhai and Lafferty, 2004)</ref> with Dirichlet smoothing (µ = 721) and pseudo-relevance feedback for query expansion. The result indicates that the classical lexical models are quite strong baselines.</p><p>The IlpsUvANvsm and IlpsUvAQlmNvsm perform worse than the virtual automated median system. This is not surprising, as the median retrieval effectiveness was computed from automated runs that additionally used existing relevance judgments. However, it is important to note that the IlpsUvANvsm and IlpsUvAQlmNvsm were amongst the top-3 automated systems (using no existing relevance judgments) that contributed the most unique relevant documents to the test collection. This is due to the fact that NVSM provides semantic matching and, consequently, retrieves relevant documents that were previously not retrieved by methods that perform retrieval using exact term matching.</p><p>We also plot the result on each of the 50 topics (see Figure <ref type="figure" coords="4,328.53,392.41,3.60,8.64" target="#fig_1">2</ref>). By each topic, we calculate the difference of AP scores between IlpsUvA runs and the best run in TREC Core in order to discover how the lexical method and the semantic method differentiate with each other. The result shows that they are complementary to some extent. For example, on topic 325 (Cult Lifestyles), 336 (Black Bear Attacks), and 345 (Overseas Tobacco Sales). This reminds us that the future work may lie in combing the two kinds of philosophy and take their respective advantages.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we described the participation of the IlpsUvA team at TREC 2017 Core Track. We applied two methods on the ad-hoc task: a Bayesian Optimization intensified lexical method (BOILM), and a latent vector space method, NVSM. Overall, IlpsUvA contributed 3 runs to the track: IlpsUvABoir, IlpsUvANvsm and IlpsUvAQlmNvsm.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,201.53,509.46,208.94,8.64"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Hyperparameter optimization architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,122.91,349.89,366.18,8.64"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance difference between IlpsUvA and the Auto-Best run in TREC Core.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,148.67,265.33,312.17,250.53"><head>Table 1 :</head><label>1</label><figDesc>Hyperparameters and their search ranges in Indri</figDesc><table coords="3,148.67,290.83,312.17,225.03"><row><cell>Hyperparamter</cell><cell>Type</cell><cell>Values</cell></row><row><cell>Stopper</cell><cell>Boolean</cell><cell>{True, False}</cell></row><row><cell>Stemmer</cell><cell>Boolean</cell><cell>{True, False}</cell></row><row><cell>rm</cell><cell>Integer</cell><cell>{TF-IDF, BM25, LM-JM, LM-DIR, LM-TS}</cell></row><row><cell>k1</cell><cell cols="2">Real value [1,2]</cell></row><row><cell>b</cell><cell cols="2">Real value [0,1]</cell></row><row><cell>k1</cell><cell cols="2">Real value [1,10]</cell></row><row><cell>k3</cell><cell cols="2">Real value [1,10]</cell></row><row><cell>b</cell><cell cols="2">Real value [0,1]</cell></row><row><cell>λ doc</cell><cell cols="2">Real value [0,1]</cell></row><row><cell>λ col</cell><cell cols="2">Real value [0,1]</cell></row><row><cell>µ dir</cell><cell cols="2">Real value [0,3000]</cell></row><row><cell>µ ts</cell><cell cols="2">Real value [0,3000]</cell></row><row><cell>λ ts</cell><cell cols="2">Real value [0,1]</cell></row><row><cell>prf</cell><cell>Boolean</cell><cell>{True, False}</cell></row><row><cell>f bDocs</cell><cell>Integer</cell><cell>[1,50]</cell></row><row><cell>f bT erms</cell><cell>Integer</cell><cell>[1,50]</cell></row><row><cell>f bM u</cell><cell cols="2">Real value [0,3000]</cell></row><row><cell cols="3">f bOrigW eight Real value [0,1]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,72.00,462.71,468.00,178.80"><head>Table 2 :</head><label>2</label><figDesc>The overall performance of IlpsUvA in TREC Core Track. IlpsUvABoir is an automated run that used existing relevance judgments, whereas IlpsUvANvsm and IlpsUvAQlmNvsm are automated runs that used no prior information. The Auto-* and Manu-* runs were provided by the track's organizers and contain runs that made use of existing judgments.</figDesc><table coords="4,207.32,524.08,194.87,117.43"><row><cell>Run</cell><cell>AP</cell><cell cols="2">NDCG P@10</cell></row><row><cell>IlpsUvABoir</cell><cell cols="2">0.286 0.515</cell><cell>0.570</cell></row><row><cell>IlpsUvANvsm</cell><cell cols="2">0.126 0.333</cell><cell>0.322</cell></row><row><cell cols="3">IlpsUvAQlmNvsm 0.172 0.412</cell><cell>0.418</cell></row><row><cell>Auto-Best</cell><cell cols="2">0.538 0.770</cell><cell>0.916</cell></row><row><cell>Auto-Median</cell><cell cols="2">0.228 0.479</cell><cell>0.548</cell></row><row><cell>Auto-Worst</cell><cell cols="2">0.006 0.048</cell><cell>0.002</cell></row><row><cell>Manu-Best</cell><cell cols="2">0.543 0.770</cell><cell>0.922</cell></row><row><cell>Manu-Median</cell><cell cols="2">0.379 0.638</cell><cell>0.672</cell></row><row><cell>Manu-Worst</cell><cell cols="2">0.165 0.399</cell><cell>0.282</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,86.35,667.97,125.31,6.91"><p>https://www.lemurproject.org/indri.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,86.35,678.42,148.24,5.61"><p>http://trec.nist.gov/trec_eval/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,86.35,668.06,129.95,6.91"><p>https://catalog.ldc.upenn.edu/ldc2008t19</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,83.07,495.74,383.68,8.82" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="5,204.30,495.74,120.09,8.59">Towards global optimisation 2</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">W</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">P</forename><surname>Szegö</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978">1978</date>
			<pubPlace>North-Holland Amsterdam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.00,515.07,468.00,8.82;5,81.96,527.03,116.66,8.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,212.70,515.25,190.79,8.64">Modular mechanisms for bayesian optimization</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Shahriari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,424.04,515.07,115.96,8.59;5,81.96,527.03,50.33,8.59">NIPS Workshop on Bayesian Optimization</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.00,546.36,468.00,8.82;5,81.96,558.31,427.94,8.82" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,168.19,546.53,217.33,8.64">Bayesian optimization for optimizing retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,404.75,546.36,135.25,8.59;5,81.96,558.31,283.42,8.82">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, WSDM &apos;18</title>
		<meeting>the Eleventh ACM International Conference on Web Search and Data Mining, WSDM &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.00,577.82,468.00,8.64;5,81.96,589.59,288.70,8.82" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,350.19,577.82,189.82,8.64;5,81.96,589.77,86.14,8.64">Taking the human out of the loop: A review of bayesian optimization</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Shahriari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,175.76,589.59,96.01,8.59">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="148" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.00,609.10,468.00,8.64;5,81.96,621.06,364.81,8.64" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,279.88,609.10,256.22,8.64">Indri: a language-model based search engine for complex queries</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,150.16,621.06,267.56,8.64">Proceedings of the International Conference on Intelligent Analysis</title>
		<meeting>the International Conference on Intelligent Analysis</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="report_type">Techn. report</note>
</biblStruct>

<biblStruct coords="5,72.00,640.21,468.00,8.82;5,81.96,652.16,457.72,8.82" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,255.59,640.39,190.77,8.64">Learning latent vector spaces for product search</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Van Gysel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,465.41,640.21,74.59,8.59;5,81.96,652.16,335.57,8.59">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 25th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.00,671.49,468.00,8.82;5,81.96,683.45,139.38,8.82" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Van Gysel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02702</idno>
		<title level="m" coord="5,265.65,671.67,241.83,8.64">Neural vector spaces for unsupervised information retrieval</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,72.00,75.30,468.00,8.82;6,81.96,87.25,283.27,8.82" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,263.31,75.48,215.86,8.64">Pyndri: A python interface to the indri search engine</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Van Gysel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,501.16,75.30,38.84,8.59;6,81.96,87.25,145.57,8.59">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="744" to="748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.00,107.18,468.00,8.82;6,81.96,119.13,269.49,8.82" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,171.89,107.36,338.52,8.64">A study of smoothing methods for language models applied to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,519.27,107.18,20.73,8.59;6,81.96,119.13,176.10,8.59">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="214" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
