<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,175.23,81.40,261.51,12.63">TREC 2001 Cross-lingual Retrieval at BBN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,183.51,99.72,38.40,10.80"><forename type="first">Jinxi</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">BBN</orgName>
								<address>
									<addrLine>Technologies 10 Moulton Street Cambridge</addrLine>
									<postCode>02138</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,230.78,99.72,82.71,10.80"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">BBN</orgName>
								<address>
									<addrLine>Technologies 10 Moulton Street Cambridge</addrLine>
									<postCode>02138</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,340.82,99.72,87.57,10.80"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">BBN</orgName>
								<address>
									<addrLine>Technologies 10 Moulton Street Cambridge</addrLine>
									<postCode>02138</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,175.23,81.40,261.51,12.63">TREC 2001 Cross-lingual Retrieval at BBN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">88AB2046E60149E53F767D597B9CECE1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>BBN only participated in the cross-lingual track in TREC 2001. Arabic, the language of the TREC 2001 corpus, presents a number of challenges to both monolingual and crosslingual IR. First, many inflected Arabic words can correspond to multiple uninflected words, requiring context to disambiguate them. Second, orthographic variations are prevalent; certain glyphs are sometimes written as different, but similar looking glyphs. Third, broken plurals, analogous to irregular nouns in English, are very common. Such nouns cannot be easily reduced to their singular forms using a rule-based approach. Fourth, Arabic words are highly ambiguous due to the tri-literal root system and the omission of short vowels in written Arabic. The focus of this report is to explore the impact of these issues on Arabic monolingual and cross-lingual retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">ISSUES IN ARABIC RETRIEVAL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Stemming</head><p>We used a modified version of Buckwalter's stemmer <ref type="bibr" coords="1,351.63,398.85,90.03,10.80" target="#b1">(Buckwalter 2001)</ref> for stemming Arabic words. It is table-driven, employing a number of tables that define all valid prefixes, stems, suffixes, and their valid combinations. Given an Arabic word w, the stemmer tries every segmentation of w into three sub-strings, w=x+y+z. If x is a valid prefix, y a valid stem and z a valid suffix, and if the combination is valid, then y is considered a stem. We re-implemented the stemmer to make it faster and compatible with UTF8 encoding. Also, we modified it so that if no valid combination of prefixstem-suffix is found, the word itself is returned as the stem.</p><p>Ambiguities arise when a word has several stems. We used two techniques to deal with this problem. With the sure-stem technique, we only stem a word if it has exactly one stem. Otherwise, the word is left alone. With the all-stems technique, we probabilistically map a word to all possible stems. Since our retrieval system is based on a probabilistic generative model, such ambiguities can be easily accommodated. In the absence of training data, we assume that all possible stems are equally probable. That is, if a word has n possible stems, each stem gets 1/n probability. The advantage of surestem is that it does not introduce additional ambiguity, while the advantage of all-stems is that it always finds a stem for a word when one exists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Orthographic Variation</head><p>Arabic orthography is highly variable. For instance, changing the letter YEH ( ) to the letter ALEF MAKSURA (¡ ) at the end of a word is very common. (Not surprisingly, the shapes of the two letters are very similar.) Since variations of this kind usually result in an "invalid" word that is un-stemmable by the Buckwalter stemmer, our solution is to detect such "errors" using the stemmer and restore the correct word ending.</p><p>A much trickier type of orthographic variation is when certain diacritical ALEFs (e.g. ¢ , £ and ¤ ) are written as the plain ALEF ( ¥ ). Often, both the intended word and what is actually written are valid words. This is much like confusing "résumé" with "resume" in English. We explored two techniques to address the problem. With the normalization technique, we replace all occurrences of the diacritical ALEFs by the plain ALEF. With the mapping technique, we map a word with the plain ALEF to a set of words that can potentially be written as that word by changing diacritical ALEFs to the plain ALEF. In this absence of training data, we will assume that all the words in the set are equally probable. Both techniques have pros and cons. The normalization technique is simple, but it increases ambiguity. The mapping technique, on the other hand, does not introduce additional ambiguity, but it is more complex. Another problem is that the uniform probability assignment may deviate from the true probability distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Broken Plurals</head><p>Broken plurals, analogous to irregular nouns in English (e.g. "woman/women"), are very common in Arabic. It is hard if not impossible to write a rule-based algorithm to reduce them to singulars. As such, broken plurals are not dealt with by the Buckwalter stemmer.</p><p>The problem is primarily a concern for monolingual retrieval. For CLIR, it is not a major problem because plurals and singulars can be translated separately. For monolingual IR, we use a statistical thesaurus, derived from the UN parallel corpus, to address the problem of broken plurals. The basic idea is that the singular and the plural forms of the same Arabic word should have the same stemmed translations in English. The problem can be formalized as the problem of estimating the probability that a user uses one Arabic word b to describe another Arabic word a. That is achieved by translating a to an English word x and then translating x to b. Translation probabilities from a to x and x to b are estimated by applying a statistical machine translation tool-kit, GIZA++ (to be described later), on the UN parallel corpus. It is easy to verify that where p diag (b|a)=1 if a=b and 0 otherwise. The mixture weights were chosen based on experiments using the TREC-8 English monolingual test queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Tri-literal root system and omission of vowels</head><p>Most Arabic words can be derived from a small number (e.g. a few thousands) of roots. Most roots consist of only three consonants. Making things worse, short vowels are normally omitted in written Arabic. As a result, Arabic words tend to have a high level of ambiguity. If not addressed, this problem will hurt cross-lingual retrieval, because an Arabic word would have many translations. Instead of explicit disambiguation, which weeds translations out based on context, we use a probabilistic solution that differentiates likely and unlikely translations. Although an Arabic word may have many translations, certain translations are more likely than others; hence, probability estimates limit the impact of ambiguity. In our CLIR experiments, we estimate translation probabilities from a large parallel corpus (the UN corpus) in addition to a manual bilingual lexicon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BILINGUAL RESOURCES</head><p>We used a manual lexicon and a parallel corpus for estimating term translation probabilities. The manual lexicon consists of word pairs from three sources:</p><p>• A bilingual term list from Buckwalter <ref type="bibr" coords="3,292.47,327.45,90.84,10.81" target="#b1">(Buckwalter, 2001)</ref>, with 86,000 word pairs.</p><p>• 20,000 word pairs, derived by applying the Sakhr machine translation system (http://www.sakhr.com/) on a list of frequent English words</p><p>• 10,000 word pairs gleaned from NMSU's named entity lexicon (http://crl.nmsu.edu/~ahmed/downloads.html).</p><p>Uniform translation probabilities are assumed for the English translations in the lexicon.</p><p>That is, if an Arabic word has n English translations, each translation gets probability 1/n.</p><p>The parallel corpus was obtained from the United Nations (UN). The United Nations web site (http://www.un.org) publishes all UN official documents under a document repository, which is accessible by paying a monthly fee. A special purpose crawler was used to extract documents that have versions in English and Arabic. After a series of clean-ups, we obtained 38,000 document pairs with over 50 million English words. For sentence alignment, a simple BBN alignment algorithm was used. Translation probabilities were obtained by applying a statistical machine translation toolkit, GIZA++ <ref type="bibr" coords="3,90.13,558.07,100.71,10.81" target="#b4">(Och and Ney, 2000)</ref> on the UN corpus. GIZA++ is based on the statistical translation work pioneered by <ref type="bibr" coords="3,182.54,571.87,91.56,10.81" target="#b0">(Brown et al, 1993)</ref>. Model 1 in Brown's work was used in this work for its efficiency.</p><p>The translation probabilities for the two sources were linearly combined to produce a single probability estimate for each word pair: where e is an English word, a is an Arabic word, p un and p lexicon are probabilities from the UN corpus and the manual lexicon respectively. We gave a higher weight to the UN corpus because it appears to be of higher quality.</p><p>Our retrieval system was documented in <ref type="bibr" coords="4,285.98,97.43,128.13,10.81" target="#b5">(Xu and Weischedel 2000;</ref><ref type="bibr" coords="4,417.14,97.43,69.72,10.81" target="#b6">Xu et al, 2001)</ref>. Our system ranks documents based on the probability that a query is generated from a document:</p><formula xml:id="formula_0" coords="4,111.39,143.01,280.82,29.70">∏ - + = Q in t D in t d q d q q d t t p D t p GL t P D Q p )) | ( ) | ( ) 1 ( ) | ( ( ) | ( α α</formula><p>Where Q is a query, D is a document, t q 's are query terms, t d 's terms in the document. GL is a background corpus of the query language. The mixture weight α is fixed to 0.3. p(t q |t d ) is the translation probability from t d to t q . We estimate p(t q |GL) and p(t d |D) as:</p><formula xml:id="formula_1" coords="4,110.19,239.37,168.55,64.93">D of size D in t of frequency D t P GL of size GL in t of frequency GL t p d d q q = = ) | ( ) | (</formula><p>In our cross-lingual experiments, the general English corpus (i.e. GL in the formulas) consists of newspaper articles in the TREC English disks 1-5 and more recent articles from FBIS. Translation probabilities were estimated as described in the previous section.</p><p>Because monolingual retrieval is a special case of cross-lingual IR, where document terms and query terms happen to be of the same language, the same system was used for both cross-lingual and monolingual IR. For simple monolingual IR, the translation matrix is an identity matrix (a diagonal matrix with 1's on the diagonal). In that case, the retrieval model is the same as the one proposed by <ref type="bibr" coords="4,334.65,424.52,163.31,10.81" target="#b3">(Miller, Leek and Schwartz, 1999)</ref>. For thesaurus-based retrieval, the translation matrix is the thesaurus.</p><p>Our system can easily accommodate the all-stems technique for stemming and the mapping technique for orthographic resolution, since both are simple probabilistic translations. In CLIR, these translations are applied before the translations to English terms. In other words, the translation from a document term to a query term consists of a number of intermediate translations. It is easy to verify that the translation matrix from document terms to query terms is the product of the intermediate translation matrixes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">OFFICIAL RESULTS</head><p>In all submitted runs, the document terms are unstemmed Arabic words. Words with apparently incorrect endings such as substitution of ALEF MAKSURA (¡ ) for YEH ( ) were handled automatically as described in Section 2.2. We submitted one official monolingual run and four official cross-lingual runs as follows:</p><p>• BBN10MON. Our monolingual run. Only the title and description fields were used for query formulation. Queries consist of Arabic stems. In query processing, each Arabic word is replaced by its stem(s). The statistical thesaurus described before was used for translations between Arabic stems.</p><p>Stop words were removed. Our stop word list was obtained from Yaser Al-Onaizan at ISI (http://www.isi.usc.edu). That list was augmented with a few manually selected high frequency words from the AFP corpus.</p><p>The mapping technique was used for orthographic resolution. The all-stems technique was used for stemming. Both were applied before the thesaurus translations of Arabic stems.</p><p>Automatic query expansion was used to add additional terms to the queries. An initial retrieval was performed on an Arabic corpus consisting of AFP (i.e. the TREC 2001 corpus) and additional articles from newspaper sources Al-Hayat and An-Nahar.</p><p>For each query, 50 terms were selected from 10 top retrieved documents based on their total TF⋅IDF in the top documents. The expansion terms and the original query terms were re-weighted:</p><formula xml:id="formula_2" coords="5,126.13,268.89,225.04,11.51">weight(t) = old_weight(t)+0.4* TFIDF(t, D i )</formula><p>where D i 's are the top retrieved documents.</p><p>• BBN10XLC. Cross-lingual run without query expansion. Only the title and description fields of the English topics were used for query formulation. Term translation used both the manual bilingual dictionary and the statistical bilingual dictionary described in the previous section.</p><p>• BBN10XLB. Cross-lingual run with Arabic expansion. In addition to BBN10XLC, Arabic query expansion terms were used. The same query expansion procedure in BBN10MON was used here.</p><p>• BBN10XLA. Cross-lingual run with Arabic and English expansions. In addition to BBN10XLB, English expansion terms were used. English documents were retrieved from a newspaper corpus with 1.2 million articles from sources AP, Reuters and FBIS.</p><p>• BBN10XLD. Cross-lingual run with long queries. Same as BBN10XLA, except the narrative field was also used in query formulation. Arabic and English expansions were used.</p><p>The mapping technique was used for orthographic resolution and the all-stems technique was used for stemming in BBN10MONO. In contrast, in the cross-lingual runs, normalization and sure-stem were used in deriving term translations from the UN corpus.</p><p>Table <ref type="table" coords="5,120.43,583.51,6.01,10.81" target="#tab_0">1</ref> shows the TREC average precision for each run. In addition, it shows the number of queries in each run that achieved the best monolingual or cross-lingual performance among all submitted runs and the number of queries above the median. Overall, all our runs achieved very good performance. The TREC 2001 topics are very long. Excluding stop words, the full topics have 26 English words per topic. Without the narrative field, the average query length is 12 words per query, still too long for typical ad hoc retrieval. The title field, which has an average of 6.6 words per topic, is more realistic. Table <ref type="table" coords="6,314.34,273.57,6.01,10.81" target="#tab_1">2</ref> shows the scores our official runs would have achieved h`ad we used only the title field in query formulation. The degradation due to the shortened queries is modest, except for BBN10XLA, for which the degradation is very large. The goal of the following experiments is to demonstrate the impact of a number of issues on monolingual retrieval. In all experiments, query formulation used the title and description fields of the topics.</p><p>a. No text processing except for the removal of stop words in query and indexing. The translation matrix is an identity matrix.</p><p>b. All-stems stemming was used in addition to the removal of stop words. Elements in the translation matrix are "translation" probabilities from unstemmed words to stems.</p><p>c. The difference from b is that sure-stem stemming was used.</p><p>d. In addition to b, the mapping technique was used for orthographic resolution.</p><p>e. The only difference from d is that normalization instead of mapping was used for orthographic resolution.</p><p>f. Same as d, except that the statistical thesaurus was used for term translation g. In addition to f, query expansion was used, based on AFP, Al-Hayat, and An-Nahar. This is our official monolingual run, BBN10MON.</p><p>h. Same as g, except that query expansion used only the AFP corpus. Retrieval scores in Table <ref type="table" coords="7,212.42,215.62,6.01,10.81">3</ref> show that:</p><p>• Stemming is very useful for Arabic retrieval (a-&gt;b). The absolute change in performance is 0.05. The value of stemming seems to be even greater for Arabic than for English monolingual retrieval. This is not surprising given the fact that Arabic has more complex morphologies.</p><p>• There is a small difference between all-stems and sure-stem (b-&gt;c), the latter being slightly better. The difference is not statistically significant.</p><p>• Orthographic resolution is very important (b-&gt;d). The change in performance is 0.075. This suggests that word spellings in the documents are very different from those in the queries.</p><p>• There is little difference between the mapping and the normalization techniques for orthographic resolution (d-&gt;e). More research is needed to determine whether a better probability estimation procedure will improve the mapping technique.</p><p>• The automatically derived thesaurus is very useful (d-&gt;f). The performance change is 0.05. We believe that most of the improvement is due to the broken plurals successfully resolved by the thesaurus. The rest of the improvement is probably due to general synonyms captured by the thesaurus.</p><p>• Query expansion is very useful for TREC 2001 queries (f-&gt;g). The performance change is 0.085. This is not very surprising given the success of query expansion techniques in earlier TRECs.</p><p>• Query expansion using only AFP is as effective as using the combined corpus of AFP, Al-Hayat and An-Nahar (g-&gt;h). The advantage of using a larger corpus for query expansion suggested by earlier studies (e.g. <ref type="bibr" coords="7,318.56,587.94,111.88,10.81" target="#b2">Kwok and Chan, 1998)</ref> is not observed. The probable reason is that the AFP corpus already has enough relevant documents for the queries (165 relevant documents per query on average). The additional relevant documents in Al-Hayat and An-Nahar did not improve the worthiness of the top documents for the purpose of query expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CROSS-LINGUAL EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Impact of Orthographic Variations</head><p>We compared BBN10XLC with an unofficial run where orthographic variations were not handled. Other conditions are the same for both runs. We found that there is little difference between the two runs (0.3604 vs 0.3584). This is very different from monolingual retrieval, where orthographic resolution is critical. However, the result is not surprising given the fact that different variants of the same word can be translated individually. Indeed, a casual inspection of the Buckwalter lexicon indicates that it often has separate entries for different spellings of the same word. It appears that the UN corpus also contains such spelling variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Effect of Arabic Stemming in Inducing a Bilingual Lexicon from a Parallel Corpus</head><p>We have compared three modes of learning term translations from the UN corpus. The first did not use stemming. The second used sure-stem. The third used all-stems. All three have pros and cons. The first keeps the maximum amount of word distinction, but requires more training data. The third requires less training data due to the reduced dimensionality, but increases word ambiguity, and the probability estimates are affected due to the one-to-many mapping from words to stems. The second is a compromise.</p><p>The retrieval scores in Table <ref type="table" coords="8,230.10,361.76,6.01,10.81" target="#tab_3">4</ref> show that no-stem is slightly better than sure-stem, which is slightly better than all-stems. While the differences are too small to make firm conclusions, they suggest that Arabic stemming is not an important issue in CLIR. No-stem Sure-stem All-stems 0.3106 0.2994 0.2895</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Impact of Resource Combination</head><p>Table <ref type="table" coords="8,120.27,489.79,6.01,10.81" target="#tab_4">5</ref> shows the retrieval scores when:</p><p>• The Buckwalter lexicon was used for term translation.</p><p>• The augmented Buckwalter lexicon (with additional word pairs from Sakhr and NMSU) was used. • The UN corpus was used.</p><p>• All resources were combined. The scores indicate that the additional translation pairs from Sakhr and NMSU are not helpful. The combination of the UN and the manual lexicon significantly outperforms either resource alone, suggesting that the word ambiguity problem in Arabic is satisfactorily handled by complementing a manual lexicon with a parallel corpus. The result is consistent with our TREC9 Chinese CLIR work <ref type="bibr" coords="8,363.90,708.78,126.60,10.81" target="#b5">(Xu and Weischedel 2000)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Query Expansion</head><p>Table <ref type="table" coords="9,120.27,97.43,6.01,10.81" target="#tab_5">6</ref> shows that both English and Arabic expansion terms improve retrieval scores. The Arabic expansion terms are more effective than English expansion terms. This is expected because we know that the particular English corpus we used for query expansion is not a very good match for the Arabic test corpus. It is disappointing that using both sources of expansion terms does not improve retrieval further. In fact, it is worse than using Arabic expansion alone. One possible reason is that the weights for English expansion terms are larger than they should be. That suggests that reducing the weight on English expansion terms may result in better retrieval. Concerning monolingual Arabic retrieval, the following proved true empirically:</p><p>• As in other languages, stemming is very important.</p><p>• Proper handling of orthographic variations is critical; the probabilistic model handled this type of ambiguity. • A statistically derived thesaurus from a parallel corpus can effectively cope with the broken plural problem. • Automatic query expansion by unsupervised relevance feedback proved very helpful, just as it has in other languages.</p><p>Concerning cross-lingual IR, the following was demonstrated empirically:</p><p>• Combining manual lexicons and parallel corpora in a probabilistic model gave much better performance than either alone. • Stemming and handling of orthographic variations proved less critical for CLIR than for monolingual IR. • Query expansion significantly improved retrieval performance, though query expansion in Arabic alone proved most effective. • Cross-lingual retrieval outperformed monolingual retrieval, as it had in our Chinese experiments in TREC-9.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,90.03,74.88,411.14,145.55"><head>Table 1 Retrieval results for official runs</head><label>1</label><figDesc></figDesc><table coords="6,90.03,94.91,411.14,125.52"><row><cell></cell><cell cols="2">Average Precision =best(out of 25)</cell><cell>&gt;median(out of 25)</cell></row><row><cell>BBN10MON</cell><cell>0.4537</cell><cell>14</cell><cell>21</cell></row><row><cell>BBN10XLA</cell><cell>0.4382</cell><cell>6</cell><cell>23</cell></row><row><cell>BBN10XLB</cell><cell>0.4639</cell><cell>8</cell><cell>24</cell></row><row><cell>BBN10XLC</cell><cell>0.3604</cell><cell>0</cell><cell>22</cell></row><row><cell>BBN10XLD</cell><cell>0.4453</cell><cell>3</cell><cell>22</cell></row><row><cell cols="2">6 EXPERIMENTS USING SHORT QUERIES</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,90.03,360.70,399.06,143.63"><head>Table 2 Title and description vs title-only for query formulation</head><label>2</label><figDesc></figDesc><table coords="6,90.03,386.72,399.06,117.60"><row><cell></cell><cell cols="3">BBN10MON BBN10XLA BBN10XLB</cell><cell>BBN10XLC</cell></row><row><cell>Title+Desc words (official runs)</cell><cell>0.4537</cell><cell>0.4382</cell><cell>0.4639</cell><cell>0.3604</cell></row><row><cell>Title words</cell><cell>0.4222</cell><cell>0.3699</cell><cell>0.4475</cell><cell>0.3441</cell></row><row><cell cols="3">7 MONOLINGUAL EXPERIMENTS</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,113.26,412.41,385.76,10.80"><head>Table 4 Three modes of GIZA++ training: no-stem, sure-stem and all-stems</head><label>4</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,125.35,585.20,354.02,58.92"><head>Table 5 Impact of resource combination</head><label>5</label><figDesc></figDesc><table coords="8,125.35,605.22,354.02,38.89"><row><cell>Buckwater only</cell><cell>Augmented Buckwalter</cell><cell>UN only</cell><cell>ALL (BBN10XLA)</cell></row><row><cell>0.2695</cell><cell>0.2697</cell><cell>0.2994</cell><cell>0.3604</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,90.03,216.95,408.39,116.75"><head>Table 6 Effect of query expansion on CLIR retrieval</head><label>6</label><figDesc></figDesc><table coords="9,90.03,237.21,408.39,96.48"><row><cell>No expansion (BBN10XLC)</cell><cell>English expansion</cell><cell>Arabic expansion (BBN10XLB)</cell><cell>English &amp; Arabic expansions (BBN10XLA)</cell></row><row><cell>0.3604</cell><cell>0.4060</cell><cell>0.4639</cell><cell>0.4382</cell></row><row><cell>9 CONCLUSIONS</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,95.79,707.66,384.55,8.96"><p>Alexander Fraser is currently with Information Sciences Institute, University of South California</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement:</head><p>We would like to thank <rs type="person">Ghada Osman</rs>, <rs type="person">Mohamed Noamany</rs> and <rs type="person">John Makhoul</rs> for their invaluable help.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="9,90.16,668.94,388.35,10.81;9,90.17,682.74,427.42,10.81;9,90.18,696.42,117.71,10.81" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,460.57,668.94,17.95,10.81;9,90.17,682.74,335.88,10.81">The Mathematics of Statistical Machine Translation: Parameter Estimation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,454.83,682.74,62.77,10.81;9,90.18,696.42,51.15,10.81">Computation Linguistics</title>
		<imprint>
			<date type="published" when="1993">1993. 1993</date>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.03,74.63,233.75,10.81" xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Buckwalter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Personal Communications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.04,97.43,403.64,10.81;10,90.05,111.22,195.71,10.81" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,253.88,97.43,239.80,10.81;10,90.05,111.22,35.41,10.81">Improving Two-Stage Ad-Hoc Retrieval for Short Queries</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">L</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,151.25,111.22,104.49,10.81">proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.05,134.02,395.99,10.81;10,90.07,147.82,271.55,10.81" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,305.58,134.02,180.46,10.81;10,90.07,147.82,79.45,10.81">A Hidden Markov Model Information Retrieval System</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,196.02,147.82,135.57,10.81">Proceedings of ACM SIGIR</title>
		<meeting>ACM SIGIR</meeting>
		<imprint>
			<date type="published" when="1999">1999. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.07,170.62,415.94,10.81;10,90.08,184.30,52.08,10.81" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,221.80,170.62,186.75,10.81">Improved Statistical Alignment Models</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,435.29,170.62,70.72,10.81;10,90.08,184.30,46.68,10.81">proceedings of ACL 2000</title>
		<meeting>ACL 2000</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.09,207.10,392.98,10.81;10,90.09,220.90,62.88,10.81" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,251.76,207.10,179.81,10.81">TREC9 Crosslingual Retrieval at BBN</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,447.66,207.10,35.41,10.81;10,90.09,220.90,57.64,10.81">TREC9 Proceedings</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.09,243.69,429.86,10.81;10,90.11,257.49,332.99,10.81" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,310.45,243.69,209.51,10.81;10,90.11,257.49,78.99,10.81">Evaluating a Probabilistic Model for Crosslingual Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,194.26,257.49,134.85,10.81">proceedings of ACM SIGIR</title>
		<meeting>ACM SIGIR</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="105" to="110" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
