<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,199.50,59.44,223.95,12.60;1,162.50,77.44,297.92,12.60">Report on the TREC-10 Experiment: Distributed Collections and Entrypage Searching</title>
				<funder ref="#_gyYnzMe">
					<orgName type="full">SNSF (Swiss National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,238.50,96.80,67.25,10.80"><forename type="first">Jacques</forename><surname>Savoy</surname></persName>
							<email>jacques.savoy@unine.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Institut interfacultaire d&apos;informatique</orgName>
								<orgName type="institution">Université de Neuchâtel</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,314.12,96.80,70.43,10.80"><forename type="first">Yves</forename><surname>Rasolofo</surname></persName>
							<email>yves.rasolofo@unine.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Institut interfacultaire d&apos;informatique</orgName>
								<orgName type="institution">Université de Neuchâtel</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,199.50,59.44,223.95,12.60;1,162.50,77.44,297.92,12.60">Report on the TREC-10 Experiment: Distributed Collections and Entrypage Searching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3A645EC6860073547F4FBB80A01F092F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For our participation in TREC-10, we will focus on the searching distributed collections and also on designing and implementing a new search strategy to find homepages.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The Web of today represents a new paradigm, one that generates new challenges for the IR community. Included among these are: managing huge amounts of documents via distributed IR models, crawling through the Web in order to find appropriate Web sites to index, accessing documents written in various languages, measuring the quality or authority of available information, providing answers to very short user requests often expressed in ambiguous terms, satisfying a large range of search types (ad hoc, question-answering, location of online services, and interactive searches for specific document types or Web pages in order to satisfy a particular geographical or time constraint).</p><p>For our participation in TREC-10, we are focusing on two problems. One involves the presentation of a new merging strategy (collection fusion problem, Chapter 1) for the Web ad hoc track, and the other developing a search strategy intended to resolve homepage search problems (Chapter 2).</p><p>In order to evaluate our hypothesis when implementing the Okapi probabilistic model <ref type="bibr" coords="1,219.87,567.17,77.13,9.00;1,71.50,579.17,24.86,9.00" target="#b5">(Robertson et al., 2000)</ref> we will use the SMART system as a test bed. This year our experiments are fully automated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Distributed collections</head><p>In order to evaluate the retrieval effectiveness of various merging strategies, we formed four separate collections from the WT10g test collection <ref type="bibr" coords="1,254.50,657.17,42.78,9.00;1,71.50,669.17,67.53,9.00">(Savoy &amp; Rasolofo, 2001)</ref>. The same indexing scheme and retrieval procedure is used for each collection involved in this study. This type of distributed context more closely reflects digital libraries or search engines available on the Internet than do meta search engines, where different search engines may collaborate in response to a given user request <ref type="bibr" coords="1,484.50,175.17,66.78,9.00" target="#b9">(Selberg, 1999;</ref><ref type="bibr" coords="1,325.50,187.17,102.55,9.00" target="#b2">Le Calvé &amp; Savoy, 2000)</ref>. This chapter is organized as follows: Section 1.1 explains our indexing and search model. Section 1.2 describes related work on database merging strategies, while Section 1.3 presents our merging procedure. Finally, in Section 1.4 we evaluate our search model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Indexing and retrieval scheme</head><p>From the original Web pages, we retained only the following logical sections: &lt;TITLE&gt;, &lt;H1&gt;, &lt;CENTER&gt;, &lt;BIG&gt;, with the most common tags &lt;P&gt; (together with &lt;/P&gt;) being removed. Texts delimited by &lt;DOCHDR&gt;, &lt;/DOCHDR&gt; tags were also removed. For longer requests, various insignificant keywords were removed (such as "Pertinent documents should include …"). Moreover, search keywords appearing in topic title sections were assigned a term frequency of 3 (a feature that should have no impact on short requests).</p><p>For the ad hoc Web track, we conducted different experiments using the Okapi probabilistic model, in which the weight wij was assigned to a given term tj in a document di and was computed according to the following formula:</p><formula xml:id="formula_0" coords="1,355.50,480.90,191.66,63.03">w ij = (k 1 + 1) ⋅ tf ij K + tf ij (1) with K = k 1 ⋅ (1-b) + b ⋅ l i avdl       (2)</formula><p>where tfij indicates the within-document term frequency, and b, k1 are parameters. K represents the ratio between the length of di measured by li (sum of tfij) and the document mean length is denoted by advl.</p><p>To index each search keyword tj included in the request, the following formula was used:</p><formula xml:id="formula_1" coords="1,355.50,630.90,191.66,34.03">w qj = tf qj k 3 + tf qj ⋅ ln N -df j df j        <label>(3)</label></formula><p>where tfqj indicates the search term frequency, dfj the collection-wide term frequency, N the number of documents in the collection, and k3 is a parameter.</p><p>To adjust the underlying Okapi search model parameters, we used the values suggested by <ref type="bibr" coords="2,243.13,76.17,54.04,9.00;2,71.50,88.17,26.24,9.00" target="#b11">Walker et al. (1998)</ref>: advl = 900, b = 0.75, k1 = 1.2, and k3 = 1000. We did however believe that it might be more effective to assign a lower value to the parameter b, and in order to verify this assumption. We also evaluated the Okapi model using b = 0.7 or b = 0.5, values, resulting in interesting retrieval performances for TREC-9 topics.</p><p>Finally, for the request q containing m search terms the retrieval status value (denoted RSVi) of a Web page di was estimated as:</p><formula xml:id="formula_2" coords="2,101.50,206.85,191.66,29.10">RSV(d i ,q) = RSV i = w ij ⋅ w qj j=1 m ∑<label>(4)</label></formula><p>In order to obtain a broader picture of our evaluations, we considered two different query formulations:</p><p>(1) using only the Title section (T) or (2) all three logical sections (Title, Descriptive and Narrative, noted T-D-N). Finally, we should mention that these queries were "real topics" in the sense that they were taken from a MSNSearch log.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Previous work on merging strategies</head><p>Various solutions have been suggested for merging separate result lists obtained from distributed collections. As a first approach, and taking only the rank of the retrieved items into account, we might interleave results in a round-robin fashion. According to previous studies <ref type="bibr" coords="2,102.48,420.17,100.59,9.00" target="#b10">(Voorhees et al., 1995;</ref><ref type="bibr" coords="2,209.50,420.17,87.78,9.00" target="#b0">Callan et al., 1995;</ref><ref type="bibr" coords="2,71.50,432.17,105.15,9.00">Savoy &amp; Rasolofo, 2001;</ref><ref type="bibr" coords="2,179.46,432.17,91.15,9.00" target="#b4">Rasolofo et al., 2001)</ref>, such interleaving schemes have a retrieval effectiveness of around 20% to 40% below that achieved from single retrieval schemes, working with a single huge collection representing an entire set of documents.</p><p>In order to account for document scores computed for each retrieved item (or its retrieval status value), we might formulate the hypothesis that each collection is searched by the same or very similar search engines and that RSV values are therefore directly comparable <ref type="bibr" coords="2,270.58,544.17,22.48,9.00;2,71.50,556.17,69.36,9.00" target="#b10">(Voorhees et al., 1995;</ref><ref type="bibr" coords="2,143.55,556.17,103.78,9.00">Savoy &amp; Rasolofo, 2001;</ref><ref type="bibr" coords="2,250.02,556.17,47.70,9.00;2,71.50,568.17,38.59,9.00">Rasolofo et al., 2001)</ref>. Such a strategy, called raw-score merging, produces a final list sorted by the document score computed by each collection. However, as indicated by <ref type="bibr" coords="2,71.50,604.17,63.99,9.00" target="#b1">Dumais (1994)</ref>, collection-dependent statistics contained in document or query weights may vary widely among collections, and therefore this phenomenon may invalidate the raw-score merging hypothesis.</p><p>To deal with this fact, we could normalize document scores within each collection through dividing them by the maximum score (i.e., the document score of the retrieved record found in the first position). <ref type="bibr" coords="2,338.50,64.17,83.17,9.00" target="#b0">Callan et al. (1995)</ref> and <ref type="bibr" coords="2,443.27,64.17,86.37,9.00" target="#b12">Xu &amp; Callan (1998)</ref> suggested a merging strategy called CORI, one that incorporates scores achieved by both collection and document. The collection score corresponds to the probability that the related collection would respond appropriately to the current request. In this scheme, each collection is viewed as a huge document and we might therefore use an IR scheme to rank the various collections according to the submitted request, since IR systems rank these documents according to their retrieval status values. In a second step, we simply multiply the document scores by the corresponding collection scores and then sort the result lists according to this value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Our merging strategy</head><p>Our new merging strategy, denoted LMS for "using result Length to calculate Merging Score", and as does the CORI model, begins by estimating a score for each collection. The underlying idea is to use these weights to increase document scores from those collections having scores greater than the average score, and to decrease those for any collections having scores less than the average score. Our approach has the advantage of being simple, since it only uses document scores and result lengths as input. Also, since collection statistics are not required, systems using our approach do not need to store collection information. By contrast, when collections statistics are required within a dynamic environment such as the Web, they need to be updated frequently, and this is not possible without establishing some sort of cooperation between the main system and collection servers.</p><p>Thus, our approach is more practical.</p><p>Our merging strategy consists of calculating a collection score according to the proportion of documents retrieved (result length) by each collection. This score is based on our intuition that a collection would contain more relevant documents for a given query if its collection server were to find more documents. The score for the kth collection is determined by:</p><formula xml:id="formula_3" coords="2,354.31,556.49,115.03,65.55">s k = ln 1 + l k ⋅K l j j=1 |C| ∑          </formula><p>where -K is a constant (set to 600 in our evaluations), -lk is the number of documents retrieved by the kth collection, and -|C| is the number of collections.</p><p>Our model uses a constant K in order to normalize the collection score as well as the natural logarithm, an order-preserving transformation used in similar contexts <ref type="bibr" coords="3,71.50,72.17,111.68,9.00" target="#b2">(Le Calvé &amp; Savoy, 2000)</ref>. Based on this collection score, our merging algorithm calculates the collection weight denoted wk for the kth collection as follows:</p><formula xml:id="formula_4" coords="3,101.50,104.80,117.33,26.06">w k = 1 + s k -s m ( )/sm [ ]</formula><p>where -sk is the kth collection score, and -sm is the mean collection score.</p><p>As in the CORI approach, the final document score is the product of wk and the document score RSVi computed by the server for this document. The value of this product is used as the key to sort the retrieved items in the final single result list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.">Evaluation</head><p>To evaluate our propositions, we first used the TREC-9 topics (50 queries, 2,617 relevant documents) taken from the WT10g test collection. Average precision comparisons (computed by the TREC-EVAL system based on 1,000 retrieved items) achieved by the raw-score merging approach are depicted in the second column of Table <ref type="table" coords="3,142.07,340.17,8.12,9.00" target="#tab_1">1a</ref>. On the other hand, average precision decreases with the use of round-robin merging strategy (third column of Table <ref type="table" coords="3,208.68,364.17,8.09,9.00" target="#tab_1">1a</ref>). As one can see, considering result lengths in the merging process may marginally improve average precision over this baseline (last column of Table <ref type="table" coords="3,413.82,72.17,7.64,9.00" target="#tab_1">1a</ref>).</p><p>After having considered different values for the parameter b, a smaller value (e.g., b = 0.5) seems to improve average precision (from 20.04 to 20.64, meaning an enhancement of +3% using raw-score merging or +2.6% when using our result length merging scheme (20.24 vs. 20.76)).</p><p>From studying the retrieval performance using TREC-10 topics (Table <ref type="table" coords="3,423.46,176.17,8.02,9.00" target="#tab_2">1b</ref>), our previous findings were confirmed: the round-robin strategy results in lower average precision. From an analysis of parameter b, one can see that when setting b = 0.5, there is an improvement of +3.4% (from 16.59 to 17.16 in average precision). This fact is not however confirmed by longer requests, where the best value for the parameter b seems to be 0.7.</p><p>The data in Table <ref type="table" coords="3,417.06,276.17,10.32,9.00" target="#tab_2">1b</ref> also indicates that by taking more search terms into account we can increase retrieval effectiveness substantially (around +30%). Finally, Table <ref type="table" coords="3,351.32,312.17,5.00,9.00" target="#tab_3">3</ref> provides a detailed analysis of our official runs applied to the Web ad hoc track, and Table <ref type="table" coords="3,517.79,324.17,10.42,9.00" target="#tab_2">1b</ref> lists their retrieval performance in bold characters.</p><p>In order to analyze our merging strategy, we listed the number and the percentage of relevant items provided by each collection in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Homepage searching</head><p>In the previous chapter, users sending a request to our search engine would obtain a ranked list of Web pages containing pertinent information about their information need. In this chapter, our objective is to design and implement a search strategy that would retrieve, at the limit, only one pertinent Web page that corresponds to the entrypage or to the online service location being sought by the user. For example, when users submit a request for "Quantas", they will retrieve the Quantas Airlines homepage, not several Web pages about this airline company.</p><p>To achieve this objective, we will first search URL addresses (Section 2.1). As a second search strategy, we will implement a combined retrieval model (Section 2.2) that searches the Web pages (Section 2.3) and then reranks the retrieved list by URL address length (Section 2.4). We will then examine any similarity between the query and the corresponding URL addresses (Section 2.5), and finally combine these three approaches (Section 2.6). An evaluation of our official runs is given in Section 2.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Searching the URL address only</head><p>For each of the 1,692,096 Web pages included in the WT10g test collection, we know the corresponding URL address. Thus as a first approach, we will build a text collection from these URL s and then obtain a mean number of distinct indexing terms (5.58 per URL address, max = 28, min = 1). From the available requests, we might then search this text database using the various IR models described using SMART notations <ref type="bibr" coords="4,347.98,187.17,95.07,9.00" target="#b6">(Savoy &amp; Picard, 2001)</ref>.</p><p>In this first attempt, we considered using a classical retrieval scheme to find the correct URL address (e.g., "www.cdsnet.net:80/vidiot/") when responding to the query "Vidiot". From examining usability studies, it was recommended that URL addresses contain information about the owner's name (usually the company name) and/or about content that might help users find their way around the Web or within the site <ref type="bibr" coords="4,512.90,287.17,38.39,9.00;4,325.50,299.17,52.70,9.00">(Nielsen, 2000, p. 246)</ref> In this first experiment, we evaluated eleven IR models, and as a retrieval performance measure, we calculated the mean reciprocal rank (MRR) over 145 topics (see Table <ref type="table" coords="4,374.03,583.17,3.96,9.00" target="#tab_6">5</ref>, Column 2). As a second measure, we noted the number of topics for which a correct entrypage was found in the top 10 (see Table <ref type="table" coords="4,487.93,607.17,3.85,9.00" target="#tab_6">5</ref>, Column 3). Overall, this search strategy does not work well for the problem of finding homepages. Moreover, although the Okapi probabilistic model provides the best search engine performance <ref type="bibr" coords="4,399.70,655.17,94.98,9.00" target="#b6">(Savoy &amp; Picard, 2001)</ref>, it is not best in terms of retrieval performance. For this particular retrieval task it seems that the vector-space model "doc=ltc, query=ltc" represents the best IR scheme, being able to retrieve 40 correct entrypages in the top 10 (over a total of 145, or 27.6% of the cases). This rather limited performance thus reflects the fact that words found in an URL address are not necessarily those one would place in a query. For example, URL s often contain abbreviations (e.g., "www.iti.gov.sg" is the URL for "Information Technology Institute"). Moreover, if a given query contains very frequently occurring words (e.g., "of" "at", "in"), we add a second form of acronym that ignores these terms (e.g., from the request "Point of View Cafe", we form a first acronym as "povc" and a second as "pvc").</p><p>Concatenation represents another form of URL construction, with two (or more) words being joined (e.g., "www.dogzone.com" and "Dog Zone's dog clubs") or only the first (of the first two) letter(s) of a word are concatenated with the second word (e.g., "Digital Realms" expressed as "www.drealms.co.uk"). In order to deal with these various word formations, we designed our system such that it considers various URL construction possibilities. For example, for a simple request such as "Worldnet Africa", our system would construct the following expanded query: "Worldnet Africa wa worldnetafrica worldneta aworldnet woafrica worldnetaf".</p><p>Evaluating these extended request forms does not however result in appreciable performance enhancements, as can be seen in the last two columns in Table <ref type="table" coords="5,99.02,408.17,3.94,9.00" target="#tab_6">5</ref>. Although the number of correct entrypages found in the top 10 seems to increase, the MRR measure indicates degradation. Thus, using only URL texts does not seem to be an adequate strategy, since establishing a link between query words and a URL addresses is a difficult task (e.g., based on the query "PiperINFO" which finds the URL "www.hamline.edu/" or "DaMOO" that finds the URL "lrc.csun.edu").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Guidelines for our combined search model</head><p>In order to develop a better search strategy, we decided to construct a two-stage retrieval strategy. In the first stage we used the Okapi probabilistic model to search Web page content for relevant homepages, although we did not employ the exact same Okapi search model used in the Web ad hoc track (see Section 1.1). Rather, we adapted the search model described in Section 2.3, and from the list of retrieved Web pages we were able to generate a corresponding list of URL addresses.</p><p>In the second stage of our retrieval strategy we inspected the corresponding URL addresses in order to verify whether or not they could be considered as appropriate URL candidates. To do so, we considered URL length, attributing more importance to short addresses (Section 2.4). Thus, in order to appear within the first positions in our final result list, a Web page would contain the search's keywords within its first 50 terms and its URL address would have to be short. As an alternative, we believe that URL address content should bear some similarity to the submitted request (Section 2.5), meaning we would again rank our retrieved list according to this similarity.</p><p>So far we have considered three types of retrieval expertise. The first retrieves and ranks Web sites according to page content, the second reranks these results according to URL address length and the last reranks the results according to URL address and submitted request similarity. In order to account for the results of these three approaches, we suggested reranking the retrieved items according to these three expert opinions (Section 2.6). Thus, with this search strategy, an entrypage will be found within the first positions if its Web page shares common words with the request, if its URL address is short and if this address contains some of the search keywords (or an abbreviation, a concatenation of two search keywords, or some URL construction as shown in Section 2.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Okapi model adaptation</head><p>In the first and most important stages of our combined retrieval strategy, we employed the Okapi probabilistic model to search Web page content for relevant homepages, although we did not employ the exact same Okapi search model as used in the Web ad hoc track (see Section 1.1). In fact, we added a precision device that considered only the first 50 words of each item retrieved and ranked only those documents having at least one query term within the first 50 words. This precision device was based on our intuition that document titles (or document headings) should provide an adequate indication as to whether or not corresponding Web pages are relevant to a given homepage search. This feature works as follows. First, we form the set of all search keywords pairs. For example, from the query q = (ti, tj, tk), we may deduce the following six terms pairs (ti, tj), (tj, ti), (ti, tk), (tk, ti), (tj, tk) and (tk, tj). We then inspect the document to see if the corresponding couple of search terms appears within a maximum distance of 5 (or with a maximum of four terms between the pair of search words). For example, supposing we are looking for the pair of search terms (tj, tk), then if we find a word sequence <ref type="bibr" coords="5,488.17,651.17,62.83,9.00;5,325.50,663.17,8.98,9.00">(tj, ta, tb, tc, td, tk)</ref>, we search it for an occurrence of our pair of search terms within a maximum distance of 5. The weight assigned to the occurrence of this search keyword pair (tj, tk) in the document di is denoted as δwjk and computed as follows:</p><formula xml:id="formula_5" coords="6,100.50,65.90,129.33,23.05">δw jk = 0.5 + 5 position(t k )</formula><p>where position(tk) indicates the position (word number) of the term tk from the beginning of the Web page. Thus, if the term tk appears in the second position (in this case, the first position occupied by tj), the function position(tk) returns 1 and δwjk is 5.5. On the other hand, if the distance is greater than 5, we ignore this occurrence.</p><p>It is possible however that a pair of search keywords (tj, tk) would appear more than once (within a maximum distance of 5) in the document di. To account for all occurrences of the search term pairs (tj, tk), we compute the following expression:</p><formula xml:id="formula_6" coords="6,73.50,247.88,214.33,52.05">w i(j,k) = (k 1 + 1) ⋅ δw jk occ(j,k) ∑ K + δw jk occ(j,k) ∑           ⋅ min(w qj ;w qk )</formula><p>where wi (j,k) represents the weight attached to the search term phrase (tj, tk) in the document di, the parameter k1 and K are evaluated as described in Equation <ref type="formula" coords="6,254.23,334.17,5.00,9.00">2</ref>and wqj and wqk represent the weights of the search keyword tj and tk in the request (as shown in Formula 3).</p><p>In order to consider all occurrences of all search keywords pairs, we compute an additional weight ph(d i ,q) attached to the presence of these multiple search keywords pair occurrences in document di as follows: ph (d i ,q) = w i(j,k) all (j,k)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>∑</head><p>The presence of search keyword pairs within the beginning of a given Web page di would change the value of its RSVi, and this would be done simply by adding the phrase weight ph(d i ,q) to the previously computed RSVi (see Equation <ref type="formula" coords="6,150.21,507.17,3.61,9.00" target="#formula_2">4</ref>), as follows:</p><formula xml:id="formula_7" coords="6,101.50,521.90,191.66,15.84">RS ′ V i = RSV i + ph (d i ,q)<label>(5)</label></formula><p>However, we suggest a variation that assigns a lower phrase weight ph(d i ,q) if the document di does not appear in the top ranked retrieved documents. Thus an alternative retrieval status value is assigned using the following formula:</p><formula xml:id="formula_8" coords="6,79.91,603.83,213.25,55.12">RS ′ V i = RSV i + 1 -α ( ) ⋅ ph (d i ,q) (6) with α = RSV max -RSV i RSV max -RSV min</formula><p>where RSVmax and RSVmin are the RSV values assigned by the first and the last retrieved items respectively (computed according to Equation <ref type="formula" coords="6,207.59,690.17,3.61,9.00" target="#formula_2">4</ref>). Eq.6, nostem 0.367 86 (59.3%) 24 (16.6%) Table <ref type="table" coords="6,362.61,178.17,3.85,9.00">6</ref>. Evaluation of results using various Okapi probabilistic models (TREC-10)</p><p>Table <ref type="table" coords="6,364.70,210.17,5.00,9.00">6</ref> lists the various results of our search models, in which we reported the mean reciprocal rank (MRR), the number of queries for which the correct homepage was found within the first ten retrieved items, and the number of queries for which the relevant entrypage could not be found in our result list. Row two of this table contains an evaluation of the classical Okapi probabilistic model, as described in Section 1.1. As a variation in this particular search problem, we decided to analyze the impact of the stemming procedure, and as seen in row three, we were able improve retrieval effectiveness compared to the classical Okapi model, when the stemming procedure was discarded.</p><p>In the fourth and fifth rows are listed our adapted Okapi model's retrieval performance, based on the retrieval status values computed according to Equation <ref type="formula" coords="6,543.88,394.17,3.79,9.00" target="#formula_7">5</ref>. Finally, the last two rows show the performance achieved by using Equation 6 with our adaptation of the Okapi model.</p><p>From analyzing this data we concluded that the stemming procedure was not really appropriate for this type of search. Moreover, our modified Okapi model provides better results than does the classical Okapi model, and computing retrieval status value using Equation 6 exhibits the best retrieval performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Reranking based on URL length</head><p>Upon examining the result lists obtained using our Okapi homepage search model, we can find corresponding URL addresses using a look-up procedure and pass this list to one of our reranking schemes. In this second step, we first consider the URL address length, its length being defined by the number of "/"s contained within it. If however a URL address ends with a "/", then this final slash is ignored. Also, for any URL addresses ending with "index.html" or "index.htm", these terms are removed before we compute the length. Thus, the URL "www.ibm.com" has the same length as "www.ibm.com/index.html" or "www.ibm.com/". Based on these findings, we reranked the retrieved URL addresses according to the inverse of their length, with ties being broken by using retrieval status values (RSV'i) computed according to our Okapi model (Section 2.3).</p><p>Table <ref type="table" coords="7,110.67,627.17,5.00,9.00">8</ref> shows the first five URL s retrieved after the first query, according to our adaptation of the Okapi model (top part) or according to our reranking scheme based on URL length (second part). As one can see, the Okapi model retrieved various Web pages from the same Web site ("africa.cis.co.za"). The retrieval status value computed according to this search model, as depicted in Column 4, does not vary greatly. When we reranked this result list according to the inverse of URL length, the relevant item ("africa.cis.co.za:81") appears in the first position. However, the first five URL s have the same length (1 in this case), and the second key used is always the retrieval status value computed by the Okapi system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Reranking based on URL similarity</head><p>URL address length does not however account for similarity between request and URL terms. Based on the data depicted in Table <ref type="table" coords="7,448.93,187.17,3.85,9.00">8</ref>, for the response to "Worldnet Africa" we can see that the URL address "www.kvvp.com" response is listed in second place. Thus, it seems a good idea to rerank the result list provided by our Okapi model, based on a similarity between the URL address and the request. This type of scheme is advantageous because we can account for any similarity between requests and Web pages, as well as requests and URL addresses. To compute this similarity between requests and URL addresses, we must however take various phenomena into consideration, including acronyms and concatenations of two search keywords found in URL addresses, etc. (see Section 2.1).</p><p>The basic principals underlying our similarity measure are described in Table <ref type="table" coords="7,454.79,375.17,3.79,9.00">9</ref>, where we distinguish mainly between three cases. First, when a request is one word only, our similarity function determines whether this word appears in the URL head (defined as the server name) or in the URL's last component (defined as the tail of the URL ). If it does and the corresponding URL is short (with a length of 1 or 2), we return the maximum value of 1.0. One the other hand, if this search term appears within the URL , the similarity is defined as the inverse of the URL 's length. Finally, we determine whether there might be a fuzzy match between the search keyword and the URL . In this "fuzzySimilarity()" function, we counted the maximum length of the ordered sequence of letters between the search word and each term appearing in the URL . For example, for the search word "market" and the word "markCie", the maximum ordered sequence is "mark" which has a length of 4. This length is then divided by the maximum length of the two corresponding terms (7 in our case, giving a final fuzzy similarity of 4/7).</p><p>As a second case, we computed the similarity between the request and a URL with a length of one. Here we tried to establish a similarity between the request and some variations of this short URL (its acronym, the concatenation of adjacent word pairs, the concatenation of a word with the two letters of the next term).</p><p>Table <ref type="table" coords="9,153.61,474.17,3.85,9.00">9</ref>. Outline of algorithm used to determine similarity between query and URL address However, from considering only the top 15 items for each of our three search models, a maximum of 45 retrieved items per query could be obtained. In order to increase these results to 100 (and to help generate relevance assessments), we expanded this list by adding URL addresses found by our Okapi model. The evaluation of our combined search model is depicted in the last row of Table <ref type="table" coords="9,200.64,702.17,13.17,9.00" target="#tab_10">10a</ref>. The retrieval per-formance seems to have increased when considering MRR values or the number of queries for which the relevant item appeared in the top ten records. Moreover, the combination of our experts seems to have a clear impact on the number of queries for which the retrieval system cannot find corresponding relevant items (last column of Table <ref type="table" coords="9,395.41,566.17,12.36,9.00" target="#tab_10">10a</ref>). If our combined retrieval model seems to perform well, it also has a drawback in that it retrieves various items corresponding to the same Web site, as shown in the last part of Table <ref type="table" coords="9,489.09,602.17,3.87,9.00">8</ref>. Thus incorporating a pruning process in our fusion scheme may hopefully enhance retrieval performance.</p><p>When we created our official results for the homepage search problem, we selected the wrong Okapi results list before considering our two reranking schemes and our combined approach. The evaluations based on this incorrect result list are shown in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7.">Description of our official runs</head><p>Table <ref type="table" coords="10,110.89,231.17,10.19,9.00" target="#tab_12">11</ref> shows our official homepage search runs. The "UniNEep1" run corresponds to the search model merges described in Section 2.6. To produce the "UniNEep2" run in positions 45 to 50 we added the top five URL addresses found by our simple search model, as described in Section 2.1 (doc=nnn, query=ntn), if these URL s were not found previously. As depicted in The "UniNEep4" run represents a variation of our search model, based on the normalized merging of the URL address searches (more precisely the "doc=nnn, query=ntn" model using our both our extended queries (see Table <ref type="table" coords="10,118.24,495.17,4.06,9.00" target="#tab_6">5</ref>)) and our adaptation of the Okapi model (search Web page content, Section 2.3). The last run labeled "UniNEep3" represents the combined search model based on the "UniNEep4" run (with reranking based on URL length and URL similarity).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>The various experiments carried out within the Web track demonstrate that:</p><p>-our new merging strategy based on results list length may improve average precision slightly; -using a lower value for the parameter b when dealing with short requests may improve retrieval performance; -our adaptation of the Okapi model for the homepage search problem performs relatively well; -reranking the URL addresses based on a combination of URL length and URL similarity with the re-quest improves retrieval performance for our Okapi model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,122.50,376.17,415.99,102.00"><head>Table 4 .</head><label>4</label><figDesc>As one can see,</figDesc><table coords="3,122.50,395.17,376.90,83.00"><row><cell></cell><cell></cell><cell cols="2">Average precision (% change)</cell></row><row><cell></cell><cell>TREC-9</cell><cell>TREC-9</cell><cell>TREC-9</cell></row><row><cell>Query (Title only)</cell><cell>50 queries</cell><cell>50 queries</cell><cell>50 queries</cell></row><row><cell>Model / merging strategy</cell><cell>Raw-score</cell><cell>Round-robin</cell><cell>Result lengths</cell></row><row><cell>Okapi (b=0.75)</cell><cell>20.04</cell><cell>17.66 (-11.9%)</cell><cell>20.24 (+0.9%)</cell></row><row><cell>Okapi (b=0.7)</cell><cell>20.34</cell><cell>17.96 (-11.7%)</cell><cell>20.60 (+1.3%)</cell></row><row><cell>Okapi (b=0.5)</cell><cell>20.64</cell><cell>17.66 (-14.4%)</cell><cell>20.76 (+0.6%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,88.50,488.17,433.13,110.00"><head>Table 1a .</head><label>1a</label><figDesc>Average precision of various retrieval schemes based on TREC-9 topics</figDesc><table coords="3,88.50,512.17,433.13,86.00"><row><cell></cell><cell></cell><cell cols="3">Average precision (% change)</cell><cell></cell></row><row><cell></cell><cell>TREC-10</cell><cell>TREC-10</cell><cell>TREC-10</cell><cell>TREC-10</cell><cell>TREC-10</cell></row><row><cell>Query</cell><cell>Title only</cell><cell>Title only</cell><cell>Title only</cell><cell cols="2">Title-Desc-Narr Title-Desc-Narr</cell></row><row><cell>Model / merging</cell><cell>Raw-score</cell><cell>Round-robin</cell><cell>Result lengths</cell><cell>Raw-score</cell><cell>Round-robin</cell></row><row><cell>Okapi (b=0.75)</cell><cell>16.59</cell><cell cols="4">16.43 (-1.0%) 17.15 (+3.4%) 22.12 (+33.3%) 20.39 (+22.9%)</cell></row><row><cell>Okapi (b=0.7)</cell><cell>16.73</cell><cell cols="4">16.24 (-2.9%) 16.99 (+1.6%) 22.42 (+34.0%) 20.76 (+24.1%)</cell></row><row><cell>Okapi (b=0.5)</cell><cell>17.16</cell><cell cols="4">16.03 (-6.6%) 17.50 (+2.0%) 21.68 (+26.3%) 19.87 (+15.8%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,141.50,609.17,340.55,87.00"><head>Table 1b .</head><label>1b</label><figDesc>Average precision of various retrieval schemes based on TREC-10 topics</figDesc><table coords="3,156.50,633.17,291.38,63.00"><row><cell>Run name</cell><cell cols="3">Aver. pr. Query Parameter</cell><cell>Merging</cell></row><row><cell>UniNEtd</cell><cell>16.59</cell><cell>T</cell><cell>b = 0.75</cell><cell>raw-score merging</cell></row><row><cell>UniNEtdL</cell><cell>17.15</cell><cell>T</cell><cell>b = 0.75</cell><cell>result-length merging</cell></row><row><cell>UniNEt7dL</cell><cell>16.99</cell><cell>T</cell><cell>b = 0.7</cell><cell>result-length merging</cell></row><row><cell>UniNEn7d</cell><cell>22.42</cell><cell cols="2">T-D-N b = 0.7</cell><cell>raw-score merging</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="3,215.50,707.17,191.07,9.00"><head>Table 3 .</head><label>3</label><figDesc>Description of official Web ad hoc run the first collection (WT10g.1) contains a larger number of pertinent Web pages and the third collection (WT10g.3) a smaller number. From looking at the top 10, the top 100 and the first 1,000 retrieved pages, we can see how the percentage of pages extracted from each collection varies. More precisely, these numbers increase for the first and fourth collection and decrease for the other two.</figDesc><table coords="4,85.50,168.17,193.86,81.00"><row><cell>Number of queries</cell><cell>50</cell></row><row><cell>Number of relevant doc.</cell><cell>3,363</cell></row><row><cell>Mean rel. doc. / request</cell><cell>67.26</cell></row><row><cell>Standard error</cell><cell>11.81</cell></row><row><cell>Median</cell><cell>39</cell></row><row><cell>Maximum</cell><cell>372 (q#: 541)</cell></row><row><cell>Minimum</cell><cell>2 (q#: 506, 538, 548)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,71.50,260.17,223.72,128.00"><head>Table 2 .</head><label>2</label><figDesc>Relevance judgment statistics (TREC-10)</figDesc><table coords="4,71.50,284.17,223.72,104.00"><row><cell></cell><cell></cell><cell cols="3">Percentage of retrieved items</cell></row><row><cell></cell><cell cols="4">WT10g.1 WT10g.2 WT10g.3 WT10g.4</cell></row><row><cell># rel. items</cell><cell>1007</cell><cell>800</cell><cell>640</cell><cell>916</cell></row><row><cell>% of rel.</cell><cell cols="4">29.94% 23.79% 19.03% 27.24%</cell></row><row><cell cols="2">Round-robin 25%</cell><cell>25%</cell><cell>25%</cell><cell>25%</cell></row><row><cell>Top 10</cell><cell>13.2%</cell><cell>28.8%</cell><cell>39.6%</cell><cell>18.4%</cell></row><row><cell>Top 100</cell><cell cols="4">19.54% 27.02% 30.62% 22.82%</cell></row><row><cell>Top 100</cell><cell cols="4">23.53% 25.16% 27.50% 23.82%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="4,102.50,399.17,164.41,21.00"><head>Table 4 .</head><label>4</label><figDesc>Distribution of retrieved items (UniNEtd, raw-score merging, 50 topics)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="4,325.50,299.17,225.92,237.00"><head>Table 5 .</head><label>5</label><figDesc>. If this principle is applied, our approach may work well. Evaluation of URL searches (TREC-10, 145 topics)</figDesc><table coords="4,325.50,331.17,220.85,173.00"><row><cell></cell><cell cols="2">Simple queries</cell><cell cols="2">Extended queries</cell></row><row><cell>IR model</cell><cell>MRR</cell><cell># top 10</cell><cell>MRR</cell><cell># top 10</cell></row><row><cell>Okapi</cell><cell>0.161</cell><cell>29</cell><cell>0.141</cell><cell>27</cell></row><row><cell>Lnu-ltc</cell><cell>0.077</cell><cell>15</cell><cell>0.091</cell><cell>17</cell></row><row><cell>atn-ntc</cell><cell>0.013</cell><cell>3</cell><cell>0.016</cell><cell>6</cell></row><row><cell>dtu-dtn</cell><cell>0.108</cell><cell>20</cell><cell>0.108</cell><cell>21</cell></row><row><cell>ltn-ntc</cell><cell>0.010</cell><cell>2</cell><cell>0.014</cell><cell>5</cell></row><row><cell>ntc-ntc</cell><cell>0.215</cell><cell>37</cell><cell>0.187</cell><cell>41</cell></row><row><cell>ltc-ltc</cell><cell>0.217</cell><cell>40</cell><cell>0.192</cell><cell>44</cell></row><row><cell>lnc-ltc</cell><cell>0.203</cell><cell>37</cell><cell>0.197</cell><cell>47</cell></row><row><cell>bnn-bnn</cell><cell>0.009</cell><cell>1</cell><cell>0.009</cell><cell>1</cell></row><row><cell>nnn-ntn</cell><cell>0.009</cell><cell>1</cell><cell>0.012</cell><cell>3</cell></row><row><cell>nnn-nnn</cell><cell>0.004</cell><cell>1</cell><cell>0.005</cell><cell>1</cell></row><row><cell>Mean</cell><cell>0.093</cell><cell>16.91</cell><cell>0.088</cell><cell>19.36</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="7,71.50,68.17,226.55,452.00"><head>Table 7a .</head><label>7a</label><figDesc>URL length distributionTable7ashows the URL length distribution across our test collection, while Table7bdepicts the same distribution based on our two relevance sets (entrypage 2000 and entrypage 2001). From the training data (denoted "2000"), we found that correct answers usually correspond to short URL addresses, those with lengths of 1 (77 cases in Table7b) or 2 (15 observations). Data from the Entrypage 2001 test collection provided by relevance assessments displays a similar pattern. Thus it seems reasonable to assign more importance to short URL addresses than to longer ones.</figDesc><table coords="7,71.50,68.17,224.04,452.00"><row><cell>URL length</cell><cell cols="4">Number of observations</cell></row><row><cell></cell><cell>Number</cell><cell cols="2">Percentage</cell><cell>Cumul.</cell></row><row><cell>= 1</cell><cell>11,622</cell><cell></cell><cell>0.69%</cell><cell>11,622</cell></row><row><cell>= 2</cell><cell>253,250</cell><cell cols="2">14.97%</cell><cell>264,872</cell></row><row><cell>= 3</cell><cell>458,356</cell><cell cols="2">27.09%</cell><cell>723,228</cell></row><row><cell>= 4</cell><cell>451,255</cell><cell cols="2">26.67%</cell><cell>1,174,483</cell></row><row><cell>= 5</cell><cell>297,339</cell><cell cols="2">17.57%</cell><cell>1,471,822</cell></row><row><cell>= 6</cell><cell>119,035</cell><cell></cell><cell>7.03%</cell><cell>1,590,857</cell></row><row><cell>= 7</cell><cell>69,091</cell><cell></cell><cell>4.08%</cell><cell>1,659,948</cell></row><row><cell>= 8</cell><cell>19,612</cell><cell></cell><cell>1.16%</cell><cell>1,679,560</cell></row><row><cell>= 9</cell><cell>6,399</cell><cell></cell><cell>0.38%</cell><cell>1,685,959</cell></row><row><cell>= 10 ≥ 11</cell><cell>1,235 4,902</cell><cell></cell><cell>0.07% 0.29%</cell><cell>1,687,194 1,692,096</cell></row><row><cell></cell><cell cols="4">All relevant items One rel. item / query</cell></row><row><cell>URL length</cell><cell>2000</cell><cell>2001</cell><cell>2000</cell><cell>2001</cell></row><row><cell>= 1</cell><cell>79</cell><cell>138</cell><cell>77</cell><cell>93</cell></row><row><cell>= 2</cell><cell>19</cell><cell>56</cell><cell>15</cell><cell>32</cell></row><row><cell>= 3</cell><cell>8</cell><cell>33</cell><cell>7</cell><cell>9</cell></row><row><cell>= 4</cell><cell>2</cell><cell>16</cell><cell>1</cell><cell>6</cell></row><row><cell>= 5</cell><cell>0</cell><cell>7</cell><cell>0</cell><cell>4</cell></row><row><cell>= 6</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>= 7</cell><cell>0</cell><cell>2</cell><cell>0</cell><cell>1</cell></row><row><cell>Total</cell><cell>108</cell><cell>252</cell><cell>100</cell><cell>145</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="7,89.50,531.17,188.66,21.00"><head>Table 7b .</head><label>7b</label><figDesc>URL length distribution for a set of relevant items</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="9,71.50,578.17,222.85,101.00"><head>Table 10a .</head><label>10a</label><figDesc>Evaluation of our three search models and their combinations (corrected results)</figDesc><table coords="9,71.50,578.17,216.49,69.00"><row><cell>Run name</cell><cell>MRR</cell><cell># in top 10</cell><cell># not found</cell></row><row><cell>Okapi only</cell><cell>0.367</cell><cell>86 (59.3%)</cell><cell>24 (16.6%)</cell></row><row><cell>URL length</cell><cell cols="3">0.653 112 (77.2%) 21 (14.5%)</cell></row><row><cell>URL simil.</cell><cell>0.470</cell><cell>95 (65.5%)</cell><cell>18 (12.4%)</cell></row><row><cell>Fusion</cell><cell cols="2">0.693 115 (79.3%)</cell><cell>10 (6.9%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="9,487.06,690.17,64.10,9.00"><head>Table 10b .</head><label>10b</label><figDesc>Table 10b, and they reveal the same conclusions as do our unofficial but corrected search schemes (Table10a). Evaluation of our three search models and their combinations (official results)</figDesc><table coords="10,71.50,92.17,216.49,69.00"><row><cell>Run name</cell><cell>MRR</cell><cell># in top 10</cell><cell># not found</cell></row><row><cell>Okapi only</cell><cell>0.295</cell><cell>64 (44.1%)</cell><cell>38 (26.2%)</cell></row><row><cell>URL length</cell><cell>0.598</cell><cell>96 (66.2%)</cell><cell>21 (14.5%)</cell></row><row><cell>URL simil.</cell><cell>0.431</cell><cell>81 (55.9%)</cell><cell>31 (21.4%)</cell></row><row><cell>Fusion</cell><cell cols="2">0.637 100 (69.0%)</cell><cell>12 (8.3%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="10,71.50,315.17,226.12,101.00"><head>Table 11 ,</head><label>11</label><figDesc>this feature does not have any significant impact on retrieval performance.</figDesc><table coords="10,71.50,347.17,216.49,69.00"><row><cell>Run name</cell><cell>MRR</cell><cell># in top 10</cell><cell># not found</cell></row><row><cell>UniNEep1</cell><cell cols="2">0.637 100 (69.0%)</cell><cell>12 (8.3%)</cell></row><row><cell>UniNEep2</cell><cell cols="2">0.637 100 (69.0%)</cell><cell>11 (7.6%)</cell></row><row><cell>UniNEep3</cell><cell>0.529</cell><cell>99 (68.3%)</cell><cell>10 (6.9%)</cell></row><row><cell>UniNEep4</cell><cell>0.477</cell><cell>99 (68.3%)</cell><cell>16 (11.0%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="10,116.50,427.17,135.10,9.00"><head>Table 11 .</head><label>11</label><figDesc>Official run evaluation</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors would like to thank <rs type="person">C. Buckley</rs> from <rs type="affiliation">SabIR</rs> for allowing us the opportunity to use the SMART system. This research was supported by the <rs type="funder">SNSF (Swiss National Science Foundation</rs>) under grant <rs type="grantNumber">21-58'813.99</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_gyYnzMe">
					<idno type="grant-number">21-58&apos;813.99</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For example as depicted in Table <ref type="table" coords="8,233.93,374.17,3.85,9.00">9</ref>, the request "Worldnet Africa" and the URL "africa.cis.co.za:81" represents a similarity evaluated as 0.9999, and if no match was found, we applied our fuzzy match function.</p><p>In the latter case, we computed the similarity between each search keyword and a given URL (function inFuzzy()). To define the similarity measure, we took the number of matches, the length of the URL , the value of the match between the URL head and the URL tail into account, as shown in the last lines of Table <ref type="table" coords="8,267.88,486.17,3.79,9.00">9</ref>.</p><p>In order to evaluate this reranking scheme, we ranked the URL address result list according to request their similarity. An example of the results of this reranking is shown in Table <ref type="table" coords="8,197.49,538.17,5.00,9.00">8</ref> (third part). For this query one can see the relevant item "africa.cis.co.za:81/" appears in the first position. The following four URL addresses have the same similarity value (depicted in fifth column of Table <ref type="table" coords="8,165.30,586.17,4.29,9.00">8</ref>) and are ranked according to their retrieval status values, computed from our adaptation of the Okapi model (shown in the fourth column).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Evaluation and data mergers</head><p>So far, we have described two reranking schemes that might hopefully improve the ranking obtained from our adaptation of the Okapi model (for which the corresponding evaluation is shown in Table <ref type="table" coords="8,242.37,689.17,3.72,9.00">6</ref>). Table <ref type="table" coords="8,282.58,689.17,14.76,9.00">10a</ref> lists an evaluation of these two reranking schemes under the label "URL length" and "URL simil." The results depicted in this table indicate that we can enhance the mean reciprocal rank (MRR) and the number of queries for which the correct homepage can be found within the first ten retrieved items. Moreover, we may also decrease the number of queries for which the relevant entrypage cannot be found in our result list (having a size of 100). Based on these results, the best scheme seems to be that of reranking, based on URL length.</p><p>As shown in Table <ref type="table" coords="8,424.14,482.17,5.00,9.00">8</ref> however, each of our three search systems seems to retrieve different URL addresses. Thus, we have decided to combine these three expert techniques. To do so, we selected the first fifteen retrieved items from each of three result lists and separately added the corresponding similarities achieved by our three experts. This additive process was chosen because in other data merging contexts, it also provided the best results <ref type="bibr" coords="8,390.45,578.17,84.25,9.00" target="#b8">(Savoy et al., 1997)</ref>. The result lists are then sorted by URL length, with ties being broken by using the sum of retrieval status values computed by our Okapi model (Section 2.3). For example, the last part of Table <ref type="table" coords="8,385.70,626.17,5.00,9.00">8</ref> shows the first five retrieved items, listed according to this fusion scheme. For the first item, RSV'i = 9935.86 because it was found by both the URL length expert (RSV'i = 4967.93) and the URL similarity expert <ref type="bibr" coords="8,396.33,674.17,71.94,9.00">(RSV'i = 4967.93)</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,326.50,202.17,216.88,9.00;10,340.50,214.17,200.03,9.00;10,340.50,226.17,171.78,9.00" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,502.22,202.17,41.16,9.00;10,340.50,214.17,184.93,9.00">Searching distributed collections with inference networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,340.50,226.17,138.33,9.00">Proceedings of the ACM-SIGIR&apos;95</title>
		<meeting>the ACM-SIGIR&apos;95</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="21" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,326.50,238.17,215.95,9.00;10,340.50,250.17,205.93,9.00" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,415.02,238.17,127.44,9.00;10,340.50,250.17,48.92,9.00">Latent semantic indexing (LSI) and TREC-2</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,408.45,250.17,83.71,9.00">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="105" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,326.50,262.17,212.94,9.00;10,340.50,274.17,198.78,9.00;10,340.50,286.17,178.91,9.00" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,465.88,262.17,73.56,9.00;10,340.50,274.17,144.49,9.00">Database merging strategy based on logistic regression</title>
		<author>
			<persName coords=""><forename type="first">Le</forename><surname>Calvé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Savoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,491.50,274.17,47.78,9.00;10,340.50,286.17,107.47,9.00">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="341" to="359" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,326.50,298.17,221.32,9.00;10,340.50,310.17,177.96,9.00" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="10,404.50,298.17,143.32,9.00;10,340.50,310.17,65.66,9.00">Designing Web usability: The practice of simplicity</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Nielsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>New Riders</publisher>
			<pubPlace>Indianapolis</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,326.50,322.17,201.87,9.00;10,340.50,334.17,207.19,9.00;10,340.50,346.17,205.06,9.00;10,340.50,358.17,115.95,9.00" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,512.60,322.17,15.77,9.00;10,340.50,334.17,207.19,9.00;10,340.50,346.17,139.92,9.00">Approaches to collection selection and results merging for distributed information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Rasolofo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Abbaci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,497.09,346.17,48.47,9.00;10,340.50,358.17,44.10,9.00">Proceedings ACM-CIKM</title>
		<meeting>ACM-CIKM</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,326.50,370.17,213.85,9.00;10,340.50,382.17,200.25,9.00;10,340.50,394.17,206.91,9.00;10,340.50,406.17,17.50,9.00" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,340.50,382.17,194.64,9.00">Experimentation as a way of life: Okapi at TREC</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beaulieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,340.50,394.17,158.49,9.00">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="95" to="108" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,326.50,418.17,211.63,9.00;10,340.50,430.17,197.96,9.00;10,340.50,442.17,87.91,9.00" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,446.77,418.17,91.36,9.00;10,340.50,430.17,44.73,9.00">Retrieval effectiveness on the Web</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,393.50,430.17,144.96,9.00;10,340.50,442.17,17.20,9.00">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="543" to="569" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,326.50,454.17,192.92,9.00;10,340.50,466.17,201.91,9.00;10,340.50,478.17,195.96,9.00;10,340.50,490.17,31.19,9.00" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,463.83,454.17,55.59,9.00;10,340.50,466.17,201.91,9.00;10,340.50,478.17,76.98,9.00">Report on the TREC-9 experiment: Link-based retrieval and distributed collections</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Rasolofo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,435.05,478.17,82.48,9.00">Proceedings TREC&apos;9</title>
		<meeting>TREC&apos;9</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="10,326.50,502.17,210.90,9.00;10,340.50,514.17,196.98,9.00;10,340.50,526.17,191.96,9.00;10,340.50,538.17,17.50,9.00" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,522.73,502.17,14.67,9.00;10,340.50,514.17,196.98,9.00;10,340.50,526.17,66.27,9.00">Report on the TREC-5 experiment: Data fusion and collection fusion</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Le Calvé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Vrajitoru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,424.30,526.17,82.07,9.00">Proceedings TREC&apos;5</title>
		<meeting>TREC&apos;5</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="489" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,326.50,550.17,208.97,9.00;10,340.50,562.17,193.96,9.00;10,340.50,574.17,25.82,9.00" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">W</forename><surname>Selberg</surname></persName>
		</author>
		<title level="m" coord="10,417.49,550.17,117.98,9.00;10,340.50,562.17,24.39,9.00">Towards comprehensive web search</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>University of Washington (WA)</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct coords="10,326.50,586.17,212.87,9.00;10,340.50,598.17,190.80,9.00;10,340.50,610.17,181.78,9.00" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,372.16,598.17,144.65,9.00">Learning collection fusion strategies</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Johnson-Laird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,340.50,610.17,138.33,9.00">Proceedings of the ACM-SIGIR&apos;95</title>
		<meeting>the ACM-SIGIR&apos;95</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="172" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,326.50,622.17,217.84,9.00;10,340.50,634.17,176.91,9.00;10,340.50,646.17,196.93,9.00;10,340.50,658.17,198.94,9.00;10,340.50,670.17,17.50,9.00" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,482.49,634.17,34.92,9.00;10,340.50,646.17,196.93,9.00;10,340.50,658.17,59.72,9.00">Okapi at TREC-6: Automatic ad hoc, VLC, routing, filtering and QSDR</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Boughamen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sparck Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,419.77,658.17,93.46,9.00">Proceedings of TREC&apos;6</title>
		<meeting>TREC&apos;6</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="125" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,326.50,682.17,213.95,9.00;10,340.50,694.17,207.48,9.00;10,340.50,706.17,79.93,9.00" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,446.60,682.17,93.85,9.00;10,340.50,694.17,87.29,9.00">Effective retrieval with distributed collections</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,444.94,694.17,103.04,9.00;10,340.50,706.17,36.42,9.00">Proceedings of the ACM-SIGIR&apos;98</title>
		<meeting>the ACM-SIGIR&apos;98</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="112" to="120" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
