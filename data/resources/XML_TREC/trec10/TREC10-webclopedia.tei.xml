<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,167.76,75.20,276.55,12.53">The Use of External Knowledge in Factoid QA</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,196.56,108.67,61.39,10.80"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<email>hovy@isi.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute University of Southern California</orgName>
								<address>
									<addrLine>4676 Admiralty Way Marina del Rey</addrLine>
									<postCode>90292-6695</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,266.89,108.67,70.29,10.80"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute University of Southern California</orgName>
								<address>
									<addrLine>4676 Admiralty Way Marina del Rey</addrLine>
									<postCode>90292-6695</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,345.87,108.67,69.66,10.80"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute University of Southern California</orgName>
								<address>
									<addrLine>4676 Admiralty Way Marina del Rey</addrLine>
									<postCode>90292-6695</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,167.76,75.20,276.55,12.53">The Use of External Knowledge in Factoid QA</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E107BF7167A165A3C9CF3991C700FEB5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes recent development in the Webclopedia QA system, focusing on the use of knowledge resources such as WordNet and a QA typology to improve the basic operations of candidate answer retrieval, ranking, and answer matching.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The Webclopedia factoid QA system increasingly makes use of syntactic and semantic (world) knowledge to improve the accuracy of its results. Previous TREC QA evaluations made clear the need for using such external knowledge to improve answers. For example, for definition-type questions such as Q: what is bandwidth? the system uses WordNet to extract words used in the term definitions before searching for definitions in the answer corpus, and boosts candidate answer scores appropriately. Such definitional WordNet glosses have helped definition answers (10% for definition questions, which translates to about 2% overall score in the TREC-10 QA evaluation, given that as many as a little over 100 out of 500 TREC-10 questions were definition questions).</p><p>This knowledge is of one of two principal types: generic knowledge about language, and knowledge about the world. After outlining the general system architecture, this paper describes the use of knowledge to improve the purity of phase 1 of the process (retrieval, segmenting, and ranking candidate segments), and to improve the results of phase 2 (parsing, matching, and ranking answers).</p><p>Webclopedia adopts the by now more or less standard QA system architecture, namely question analysis, document / passage retrieval, passage analysis for matching against the question, and ranking of results. Its architecture (Figure <ref type="figure" coords="1,172.64,514.06,4.13,8.86">1</ref>) contains the following modules, which are described in more detail in <ref type="bibr" coords="1,471.27,514.06,50.99,8.86;1,90.00,525.58,22.42,8.86" target="#b10">(Hovy et al., 2001;</ref><ref type="bibr" coords="1,114.96,525.58,69.78,8.86" target="#b9">Hovy et al., 2000)</ref>:</p><p>• Question parsing: Using BBN's IdentiFinder <ref type="bibr" coords="1,302.42,543.10,77.82,8.86" target="#b4">(Bikel et al., 1999)</ref>, the CONTEX parser produces a syntactic-semantic analysis of the question and determines the QA type.</p><p>• Query formation: Single-and multi-word units (content words) are extracted from the analysis, and WordNet synsets are used for query expansion. A series of Boolean queries is formed.</p><p>• IR: The IR engine MG <ref type="bibr" coords="1,201.31,601.42,79.53,8.86" target="#b11">(Witten et al., 1994)</ref> returns the top-ranked N documents.</p><p>• Selecting and ranking sentences: For each document, the most promising K&lt;&lt;N sentences are located and scored using a formula that rewards word and phrase overlap with the question and its expanded query words. Results are ranked.</p><p>• Parsing segments: CONTEX parses the top-ranked 300 sentences.</p><p>• Pinpointing: Each candidate answer sentence parse tree is matched against the parse of the question; sometimes also the preceding sentence. As a fallback the window method is used.</p><p>• Ranking of answers: The candidate answers' scores are compared and the winner(s) are output.</p><p>Webclopedia classifies desired answers by their semantic type, using the approx. 140 classes developed in earlier work on the project <ref type="bibr" coords="2,205.46,85.90,78.06,8.86" target="#b9">(Hovy et al., 2000)</ref>. These types include common semantic classes such as PROPER-PERSON, EMAIL-ADDRESS, LOCATION, and PROPER-ORGANIZATION, but also classes particular to QA such as WHY-FAMOUS, YES:NO, and ABBREVIATION-EXPANSION. They have been taxonomized as the Webclopedia QA Typology, of which an older version can be found at http://www.isi.edu/natural-language/projects/webclopedia/Taxonomy/taxonomy_toplevel.html.</p><p>Figure <ref type="figure" coords="2,263.59,456.22,3.69,8.86">1</ref>. Webclopedia architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Parsing</head><p>CONTEX is a deterministic machine-learning based grammar learner/parser that was originally built for MT <ref type="bibr" coords="2,108.41,511.18,76.04,8.86" target="#b6">(Hermjakob, 1997)</ref>. For English, parses of unseen sentences measured 87.6% labeled precision and 88.4% labeled recall, trained on 2048 sentences from the Penn Treebank. Over the past few years it has been extended to Japanese and Korean <ref type="bibr" coords="2,244.67,534.22,73.65,8.86" target="#b7">(Hermjakob, 2000)</ref>.</p><p>For Webclopedia, CONTEX required two extensions. First, its grammar had to be extended to include question forms. The grammar learner portion of CONTEX was trained on approx. 1150 questions and achieved accuracies of approx. 89% labeled precision and labeled recall <ref type="bibr" coords="2,388.21,574.78,75.45,8.86" target="#b8">(Hermjakob, 2001)</ref>. Second, the grammar had to be augmented to recognize the semantic type of the desired answer (which we call the qtarget ). Its semantic type ontology was extended to include currently abourt 140 qtarget types, plus some combined types <ref type="bibr" coords="2,156.86,610.54,75.41,8.86" target="#b8">(Hermjakob, 2001)</ref>. Beside the qtargets that refer to semantic concepts, qtargets can also refer to part of speech labels (e.g., S-PROPER-NAME) and to constituent roles or slots of parse trees (e.g., [ROLE REASON]). For questions with the Qtargets Q-WHY-FAMOUS, Q-WHY-FAMOUS-PERSON, Q-SYNONYM, and others, the parser also provides qargs-information helpful for matching: Who was Betsy Ross? QTARGET: Q-WHY-FAMOUS-PERSON QARGS: ("Betsy Ross") How is "Pacific Bell" abbreviated? QTARGET: Q-ABBREVIATION QARGS: ("Pacific Bell") What are geckos? QTARGET: Q-DEFINITION QARGS: (("geckos" "gecko") ("animal")) These qtargets are determined during parsing using approx. 300 hand-written rules. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Document Retrieval and Sentence Ranking Analyzing the Question to Create a Query</head><p>We parse input questions using CONTEX (Section 2) to obtain a semantic representation of the questions. For example, we determine that the question "How far is it from Denver to Aspen?" is asking for a distance quantity. The question analysis module identifies noun phrases, nouns, verb phrases, verbs, adjective phrases, and adjectives embedded in the question. These phrases/words are assigned significance scores according to the frequency of their type in our question corpus (a collection of 27,000+ questions and answers), secondarily by their length, and finally by their significance scores, derived from word frequencies in the question corpus.</p><p>We remain indebted to BBN for the use of IdentiFinder <ref type="bibr" coords="3,316.48,202.78,74.09,8.86" target="#b4">(Bikel et al., 1999)</ref>, which isolates proper names in a text and classifies them as person, organization, or location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expanding Queries</head><p>Query expansion comes from two sources and used in different stages. In the document retrieval stage, the highly relevant question terms (identified by CONTEX) are expanded in order to boost recall, for example going from "Russian" to "Soviet" or from "capital of the United States" to "Washington". In the sentence ranking stage, we use WordNet 1.6 <ref type="bibr" coords="3,242.32,288.22,71.33,8.86" target="#b5">(Fellbaum, 1998)</ref> to match expanded query terms. Although these expanded terms contribute to the final score, their contribution is discounted. This application of expansion strategy aims to achieve high precision and moderate recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Retrieving Documents</head><p>We use MG <ref type="bibr" coords="3,144.70,350.62,84.56,8.86" target="#b11">(Witten et al., 1994)</ref> as our search engine. Although MG is capable of performing ranked query, we only use its Boolean query capability. For the entire TREC-10 test corpus, the size of the inverse index file is about 200 MB and the size of the compressed text database is about 884 MB. The stemming option is turned on. Queries are sent to the MG database, and the retrieved documents are ranked according to their ranking from query analysis. We order queries most specific first, then gradually relax them to more general, until we have retrieved a sufficient number of documents. For example, (Denver&amp;Aspen) is sent to the database first. If the number of documents returned is less than a prespecified threshold, for example, 500, then we retain this set of documents as the basis for further processing, while also submitting the separate queries (Denver) and (Aspen).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ranking Sentences</head><p>If the total numbers of sentences contained in the documents returned by MG is N for a given Boolean query, we would like to rank the sentences in the documents to maximize answer recall and precision in the topmost K &lt;&lt; N, in order to minimize the parsing and subsequent processing. In this stage we set K=300. We assign goodness score to a sentence according to the following criteria:</p><p>1. Exact match of proper names such as "Denver" and "Aspen" get 100% bonus score.</p><p>2. Upper case term match of length greater than 1 get 60% bonus, otherwise get 30%. For example, match of "United States" is better than just of "United".</p><p>3. Lower case matches get the original score.</p><p>4. Lower case term match with WordNet expansion stems get 10% discount. If the original term is capital case then it gets 50% discount. For example, when Cag(e) matches cag(e), the former may be the last name of some person while the latter is an object; therefore, the case mismatch signals less reliable information.</p><p>5. Lower case term matches after Porter stemming get 30% discount. If the original term is capital case then 70% discount. The Porter stemmed match is considered less reliable than a WordNet stem match.</p><p>6. Porter stemmer matches of both question word and sentence word get 60% discount. If the original term is capital case then get 80% discount.</p><p>7. If CONTEX indicates a term as being QSUBUMED then it gets 90% discount. For example, "Which country manufactures weapons of mass destruction?" where "country" will be marked as qsubsumed.</p><p>Normally common words are ignored unless they are part of a phrase in question word order. Based on these scores, the total score for a sentence is:</p><p>Sentence score = sum of word scores</p><p>At the end of the ranking we apply qtarget filtering to promote promising answer sentences. For example, since the question "How far is it from Denver to Aspen?" is asking for a distance quantity, any sentence that contains only "Denver" or "Aspen" but not any distance quantities are thrown out. Only the top 300 remaining sentences are passed to the answer pinpointing module.</p><p>The bonus and discount rates given here are heuristics. We are in the process of developing mechanisms to learn these parameters automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Answer Matching using Qtarget-Specific Knowledge</head><p>Once the candidate answer passages have been identified, their sentences are parsed by CONTEX. The Matcher module then compares their parse trees to the parse tree of the original question. The Matcher performs two independent matches <ref type="bibr" coords="4,230.46,280.30,74.06,8.86" target="#b10">(Hovy et al., 2001;</ref><ref type="bibr" coords="4,306.97,280.30,69.77,8.86" target="#b9">Hovy et al., 2000)</ref>:</p><p>• match qtargets and qargs/qwords in the parse trees,</p><p>• match over the answer text using a word window.</p><p>Obviously, qtargets and their accompanying qargs play an important role; they enable the matcher to pinpoint within the answer passage the exact, syntactically delimited, answer segment. (In contrast, word window matching techniques, that have no recourse to parse structures, have no accurate way to delimit the exact answer boundaries.)</p><p>Unfortunately, there are many questions, for which the qtarget (which can be as generic as NP), syntactic clues and word overlap are insufficient to select a good answer. Over the past year we therefore focused on strategies for dealing with this, and developed the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expected Answer Range</head><p>For quantity-targeting questions, humans often have a good sense of reasonable answer ranges and would find it easy to identify the correct answer in the following scenario:</p><p>Q: What is the population of New York?</p><p>S1. The mayor is held in high regards by the 8 million New Yorkers.</p><p>S2. The mayor is held in high regards by the two New Yorkers.</p><p>Even without any knowledge about the population of specific cities and countries, a population of 8,000,000 makes more sense than a population of 2. We mirror this 'common sense' knowledge by biasing quantity questions like the one above towards normal value ranges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and Results</head><p>We entered the TREC-10 QA track, and received an overall Mean Reciprocal Rank (MRR) score of 0.435, which puts Webclopedia among the top performers. The average MRR score for the main task is about 0.234. The answer rank distribution is shown in Figure <ref type="figure" coords="8,320.26,315.10,3.72,8.86">2</ref>. It indicates that we cannot find answers in the top 5 in about 43% of the cases. Once we find answers we usually rank them at the first place.</p><p>Analysis of the answers returned by the TREC assessors revealed several problems, ranging from outright errors to judgments open to interpretation. One example of an error is a ruling that the answer to "what is cryogenics?" is not "engineering at low temperature" (as defined for example in Webster's Ninth New Collegiate Dictionary, and as appears in the TREC collection), but rather the more colloquial "freezing human being for later resustication" (which also appears in the collection). Although Webclopedia returned both, the correct answer (which it preferred) was marked wrong. While we recognize that it imposes a great administrative burden on the TREC QA administrators and assessors to re-evaluate such judgments, it is also clearly not good R&amp;D methodology to train systems to produce answers that are incorrect but colloquially accepted. (Checking whether their knowledge is correct is precisely one of the reasons people need QA systems!) We therefore propose an appeals procedure by which the appellant must provide to the administrator the question, the correct answer, and proof, drawn from a standard reference work, of correctness. The administrator can provide a list of acceptable reference works beforehand, which should include dictionaries, lists of common knowledge facts (the seven wonder of the world, historical events, etc.), abbreviation lists, etc., but which would presumably not include local telephone books, etc. (thereby ruling out local restaurants as answer to "what is the Taj Mahal?"). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,403.92,465.32,89.91,8.62;8,403.92,476.84,95.65,8.62;8,403.92,488.36,50.84,8.62"><head></head><label></label><figDesc>Figure 2. Webclopedia answer rank distribution in TREC-10.</figDesc></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abbreviation Knowledge</head><p>Multi-word expressions are not abbreviated arbitrarily: Q: What does NAFTA stand for? S1. This range of topics also includes the North American Free Trade Agreement, NAFTA, and the world trade agreement GATT. <ref type="bibr" coords="4,103.68,640.78,8.60,8.86">S2</ref>. The interview now changed to the subject of trade and pending economic issues, such as the issue of opening the rice market, NAFTA, and the issue of Russia repaying economic cooperation funds.</p><p>After Webclopedia identifies the qtarget of the question as I-EN-ABBREVIATION-EXPANSION, the system extracts possible answer candidates, including "North American Free Trade Agreement" from S1 and "the rice market" from S2. Based on the perfect match of the initial letters of the first candidate with the acronym NAFTA, an acronym evaluator easily prefers the former over the latter candidate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Mark-Up in Parse Trees</head><p>Phone numbers, zip codes, email addresses, URLs, and different types of quantities follow patterns that can be exploited to mark them up, even without any explicit mentioning of key words like "phone number". For a question/sentence candidate pair like Q: What is the zip code for Fremont, CA? S1. From Everex Systems Inc., 48431 Milmont Drive, Fremont, CA 94538.</p><p>Webclopedia identifies the qtarget as C-ZIP-CODE. To match such qtargets, the CONTEX parser marks up (likely) zip codes, based on both structure (e.g., 5 digits) and context (e.g., preceding state code). Two more question/answer pairs that are matched this way:</p><p>Q: What's Dianne Feinstein's email address? Qtarget: C-EMAIL-ADDRESS S1. Comments on this issue should be directed to Dianne Feinstein at senator@feinstein.senate.gov Q: How hot is the core of the earth? Qtarget: I-EN-TEMPERATURE-QUANTITY S1. The temperature of Earth's inner core may be as high as 9,000 degrees Fahrenheit (5,000 degrees Celsius).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Using External Glosses for Definition Questions</head><p>We have found a 10% increase in accuracy in answering definition questions by using external glosses.</p><p>Q: What is the Milky Way? Candidate 1: outer regions Candidate 2: the galaxy that contains the Earth For the above question, Webclopedia identified two leading answer candidates. Comparing these answer candidates with the gloss that the system finds in Wordnet:</p><p>Wordnet: Milky Way-the galaxy containing the solar system</p><p>Webclopedia biases the answer to the candidate with the greater overlap, in this case clearly "the galaxy that contains the Earth".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Finding Support For a Known Answer</head><p>It seems against all intuition that a question like Q1: What is the capital of the United States? initially poses great difficulties for a question answering system. While a question like Q2: What is the capital of Kosovo? can easily be answered from text such as S2. ... said Mr Panic in Pristina, the capital of Kosovo, after talks with Mr Ibrahim Rugova ... many American readers would find a newspaper sentence such as S1. Later in the day, the president returned to Washington, the capital of the United States. almost insulting. The fact that Washington is the capital of the United States is too basic to be made explicit. In this unexpectedly difficult case, we can fall back on sources like Wordnet: Wordnet: Washington-the capital of the United States which, as luck would have it, directly answers our question. Based on this knowledge, Webclopedia produces the answer, represented as a lexical target (LEX "Washington"), which the IR module then uses to focus its search on passages containing "Washington", "capital" and "United States". The matcher then limits the search space to "Washington". The purpose of this exercise is not as ridiculous as it might first appear: even though the system already knows the answer before consulting the document collection, it makes a contribution by identifying documents that support "Washington" as the correct answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Relation Matching in Webclopedia</head><p>In question answering, matching words and groups of words is often insufficient to accurately score an answer. As the following examples demonstrate, scoring can benefit from the correct matching of semantic relations in addition: Note: Answer candidates are bold ("red"), while constituents with corresponding words in the question are underlined ("blue") (http://www.isi.edu/natural-language/projects/webclopedia/sem-rel-examples.html).</p><p>Both answer candidates S1 and S2 receive credit for matching "Lee Harvey Oswald" and "kill", as well as for finding an answer (underlined) of the proper type (I-EN-PROPER-PERSON), as determined by the qtarget. However, is the answer "Jack Ruby" or "President John F. Kennedy"? The only way to determine this is to consider the semantic relationship between these candidates and the verb "kill", for which Webclopedia uses the following question and answer parse trees (simplified here): For S1, based on these parse trees, the matcher awards additional credit to node [2] (Jack Ruby) for being the logical subject of the killing (using anaphora resolution) as well as to node [20] (Lee Harvey Oswald) for being the head of the logical object of the killing. Note that-superficially-John F. Kennedy appears to be closer to "killed", but the parse tree correctly records that node [13] is actually not the object of the killing. The candidate in S2 receives no extra credit for semantic relation matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness</head><p>It is important to note that the Webclopedia matcher awards extra credit for each matching semantic relationship between two constituents, not only when everything matches. This results in robustness that comes in handy in cases such as: In S1, the matcher gives points to Caesar for being the object of the killing, but (at least as of now) still fails to establish the chain of links that would establish Brutus as his assassin. The predicate-object credit however is enough to make the first answer score higher than in S2, which, while having all agents right next to each other at the surface level, receives no extra credit for semantic relation matching. In the S1, [world] receives credit for modifying snake, even though it is the (semantic) head of a postmodifying prepositional phrase in the question and the head of a pre-modifying determiner phrase in the answer sentence. While the system still of course prefers "in the world" over "the world's" on the constituent matching level, its proper relationship to snake (and the proper relationship between "largest" and "snakes", as well as "pythons" and "snakes") by far outweigh the more literal match of "in the world".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Good Generalization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Using a Little Additional Knowledge</head><p>Additionally, Webclopedia uses its knowledge of the semantic relationships between concepts like "to invent", "invention" and "inventor", so that in example 209, "Johan Vaaler" gets extra credit for being a likely logical subject of "invention", while "David" actually loses points for being outside of the clausal scope of the inventing process in the second case. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,112.33,74.38,409.83,8.86;8,103.68,85.90,186.96,8.86;8,103.68,103.42,233.12,8.86;8,103.68,114.94,101.98,8.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="8,124.48,74.38,393.31,8.86">Like the guy who invented the safety pin, or the guy who invented the paper clip</title>
		<idno>Score: 236.47534; 07/20/89; LA072089-0033</idno>
		<imprint/>
	</monogr>
	<note>David added. Question 3: What does the Peugeot company manufacture? Qtargets: S-NP, S-NOUN</note>
</biblStruct>

<biblStruct coords="8,103.68,161.74,418.30,8.86;8,103.68,173.26,418.51,8.86;8,103.68,184.78,418.39,8.86;8,103.68,196.30,165.64,8.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="8,126.87,161.74,395.11,8.86;8,103.68,173.26,418.51,8.86;8,103.68,184.78,414.02,8.86">These include Coca Cola and Pepsico, the US soft drinks giants, Peugeot the French car manufacturer, finance companies GE Capital and Morgan Stanley, Nippon Denro, the Japanese steel manufacturer, and the Scotch whisky maker Seagram and United Distillers, the spirits arm of Guinness</title>
		<author>
			<persName coords=""><surname>S2</surname></persName>
		</author>
		<idno>Score: 323.76758; 93/06/25; FT932-902</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,213.82,431.97,8.86;8,90.00,225.34,432.20,8.86;8,90.00,237.10,432.15,8.86;8,90.00,248.62,364.55,8.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="8,145.47,213.82,376.50,8.86;8,90.00,225.34,432.20,8.86;8,90.00,237.10,432.15,8.86;8,90.00,248.62,360.22,8.86">gets credit as a likely logical object of the manufacturing process, and &quot;Peugeot&quot;, being recognized as a &quot;manufacturer&quot;, is boosted for playing the proper logical subject role. This example shows that particularly when the qtarget doesn&apos;t help much in narrowing down the answer candidate space, semantic relation matching can often make the crucial difference in finding the right answer</title>
		<author>
			<persName coords=""><forename type="first">S2</forename><surname>In</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,94.50,74.59,69.47,10.80" xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,94.30,432.11,8.86;9,104.40,105.82,198.74,8.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,298.92,94.30,177.47,8.86">An Algorithm that Learns What&apos;s in a Name</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,487.68,94.30,34.43,8.86;9,104.40,105.82,160.01,8.86">Machine Learning-Special Issue on NL Learning</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,123.34,368.54,8.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="9,193.86,123.34,165.11,8.86">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName coords=""><forename type="first">Ch</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,140.86,432.09,8.86;9,104.40,152.38,417.51,8.86;9,104.40,163.90,83.91,8.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="9,178.35,140.86,310.27,8.86">Learning Parse and Translation Decisions from Examples with Rich Context</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Hermjakob</surname></persName>
		</author>
		<ptr target="file://ftp.cs.utexas.edu/pub/mooney/papers/hermjakob-dissertation-97.ps.gz" />
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
		<respStmt>
			<orgName>University of Texas at Austin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct coords="9,90.00,181.66,432.24,8.86;9,104.40,193.18,417.60,8.86;9,104.40,204.70,225.34,8.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,192.55,181.66,311.30,8.86">Rapid Parser Development: A Machine Learning Approach for Korean</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Hermjakob</surname></persName>
		</author>
		<ptr target="http://www.isi.edu/~ulf/papers/kor_naacl00.ps.gz" />
	</analytic>
	<monogr>
		<title level="m" coord="9,104.40,193.18,417.60,8.86;9,104.40,204.70,21.18,8.86">Proceedings of the North American chapter of the Association for Computational Linguistics (NAACL-2000)</title>
		<meeting>the North American chapter of the Association for Computational Linguistics (NAACL-2000)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,222.22,432.32,8.86;9,104.40,233.74,328.75,8.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,180.10,222.22,245.44,8.86">Parsing and Question Classification for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Hermjakob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,445.29,222.22,77.03,8.86;9,104.40,233.74,248.29,8.86">Proceedings of the Workshop on Question Answering at the Conference ACL-2001</title>
		<meeting>the Workshop on Question Answering at the Conference ACL-2001<address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,251.26,432.00,8.86;9,104.40,262.78,265.15,8.86" xml:id="b9">
	<analytic>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Junk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,371.86,251.26,150.13,8.86;9,104.40,262.78,154.44,8.86">Question Answering in Webclopedia. Proceedings of the TREC-9 Conference</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,280.30,432.11,8.86;9,104.40,291.82,400.96,8.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,382.04,280.30,140.08,8.86;9,104.40,291.82,44.36,8.86">Toward Semantics-Based Answer Pinpointing</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ravichandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,160.17,291.82,275.66,8.86">Proceedings of the Human Language Technologies Conference (HLT)</title>
		<meeting>the Human Language Technologies Conference (HLT)<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,309.58,432.12,8.86;9,104.40,321.10,191.86,8.86" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="9,277.28,309.58,244.85,8.86;9,104.40,321.10,42.67,8.86">Managing Gigabytes: Compressing and indexing documents and images</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">C</forename><surname>Bell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Van Nostrand Reinhold</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
