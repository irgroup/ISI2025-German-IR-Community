<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,221.58,129.91,230.90,15.49;1,165.09,151.83,265.73,15.49">The University of Amsterdam&apos;s Textual Question Answering System</title>
				<funder ref="#_N5c8Vzv">
					<orgName type="full">Netherlands Organization for Scientific Research (NWO)</orgName>
				</funder>
				<funder ref="#_3qtGHAv #_NuyGhvR #_hjksPVw">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder>
					<orgName type="full">Physical Sciences Council</orgName>
				</funder>
				<funder ref="#_jBWXKby #_WTbqYxB">
					<orgName type="full">NWO</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,203.55,184.32,74.71,10.76"><forename type="first">Christof</forename><surname>Monz</surname></persName>
							<email>christof@science.uva.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Language &amp; Inference Technology</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<addrLine>Nieuwe Achtergracht 166</addrLine>
									<postCode>1018 WV</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,302.17,184.32,90.19,10.76"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language &amp; Inference Technology</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<addrLine>Nieuwe Achtergracht 166</addrLine>
									<postCode>1018 WV</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,221.58,129.91,230.90,15.49;1,165.09,151.83,265.73,15.49">The University of Amsterdam&apos;s Textual Question Answering System</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AD851B4484DCA31DF623CF7DB7FDAEE4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe our participation in the TREC-10 Question Answering track. All our runs used the Tequesta system; we provide a detailed account of the natural language processing and inferencing techniques that are part of Tequesta. We also summarize and discuss our results, which concern both the main task and the list task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.911" lry="842.737"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.911" lry="842.737"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.911" lry="842.737"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.911" lry="842.737"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.911" lry="842.737"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.911" lry="842.737"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.911" lry="842.737"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.911" lry="842.737"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.911" lry="842.737"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.911" lry="842.737"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Current information retrieval systems allow us to locate documents that might contain the pertinent information, but most of them leave it to the user to extract the useful information from a ranked list. However, users want not whole documents but brief answers to specific questions. Question answering is meant to be a step closer to real information retrieval in that it attempts to facilitate just that.</p><p>For researchers (such as ourselves) who are interested in bringing natural language processing (NLP) and inferencing to bear on real-world tasks, question answering (QA) provides an ideal setting. Many years of experimental research have shown that advanced NLP techniques hurt more than they help for traditional document retrieval <ref type="bibr" coords="1,217.75,558.55,10.58,8.97" target="#b0">[1]</ref>. For any system that aims to address the QA task, however, issues such as question classification, partial parsing, and named entity recognition appear to be essential components. In addition, the best performing systems at the TREC-8 and TREC-9 QA tracks have demonstrated that various forms of inferencing (ranging from the use of semantic relations in WordNet to actually abducing answers from questions) make a significant positive contribution towards the effectiveness of QA systems <ref type="bibr" coords="1,64.97,666.14,15.77,8.97" target="#b19">[20,</ref><ref type="bibr" coords="1,84.36,666.14,11.83,8.97" target="#b18">19]</ref>. The recently released (and deliberately ambitious) vision statement that aims to guide future research in QA calls for approaches that are even more knowledge intensive than the current ones <ref type="bibr" coords="1,146.42,702.01,10.58,8.97" target="#b7">[8]</ref>.</p><p>This paper describes our submissions for the question answering track at TREC-10; we submitted runs for the main task and for the list task. This is the first time that we participated in the QA track (and in TREC, for that matter), and our main focus was on evaluting a basic question answering system that exploits shallow NLP techiques in combination with standard retrieval techniques.</p><p>The remainder of the paper is organized as follows. Section 2 describes the Tequesta system that we developed for the QA track. We outline the underlying retrieval engine, the kind of document analysis that we perform (partial parsing and named entity recognition), as well as our question analysis and answer selection modules. Then, in Section 3 we describe the runs that we submitted to the main QA task, and discuss the outcomes. In Section 4 we do the same for the runs submitted to the list task. Section 5 contains our conclusions and plans for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">System Architecture</head><p>The system architecture of Tequesta is fairly standard; its overall architecture is displayed in Figure <ref type="figure" coords="1,481.82,509.50,3.74,8.97" target="#fig_0">1</ref>. Like most current QA systems, Tequesta is built on top of a retrieval system. The first step is to build an index for the document collection, in this case the TREC-10 collection. Then the question is translated into a retrieval query which is posed to the retrieval system. For retrieval we use FlexIR <ref type="bibr" coords="1,495.33,569.27,15.27,8.97" target="#b12">[13]</ref>, a vectorspace based retrieval system, described in Section 2.1.</p><p>The retrieval system is used to identify a set of documents that are likely to contain the answer to a question posed to the system. The top 100 documents returned by FlexIR are processed by a partial parser described in Section 2.2. Then, named entities are annotated with the appropriate type. Named entity recognition is discussed in Section 2.3.</p><p>Just like the top 100 documents, the question is also parsed. The parsed output is used to determine the focus of the question, i.e., what it is looking for. Question analysis is explained in Section 2.4.</p><p>The document analysis and question analysis are mostly done independently from each other, but in order to generate a top 5 list of answers, document information and question information are combined in the answer selection process, described in Section 2.5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Document Retrieval</head><p>For pre-fetching relevant documents that are likely to contain the answer, Tequesta uses FlexIR, an information retrieval system developed at the University of Amsterdam. The main goal underlying FlexIR's design is to facilitate flexible experimentation with a wide variety of retrieval components and techniques. FlexIR is implemented in Perl; it is built around the standard UNIX pipeline architecture, and supports many types of pre-processing, scoring, indexing, and retrieval methods.</p><p>The retrieval model underlying FlexIR is the standard vector space model. All our official runs for TREC-10 used the Lnu.ltc weighting scheme <ref type="bibr" coords="2,151.13,523.24,11.62,8.97" target="#b3">[4]</ref> to compute the similarity between a question and a document. For the experiments on which we report in this article, we fixed slope at 0.2; the pivot was set to the average number of unique words occurring in the collection.</p><p>To increase precision, we decided to use a lexical-based stemmer, or lemmatizer, because it tends to be less aggressive than rule-based stemmers such as Porter's <ref type="bibr" coords="2,210.86,607.49,16.60,8.97" target="#b13">[14]</ref> or Lovins' <ref type="bibr" coords="2,273.58,607.49,11.62,8.97" target="#b8">[9]</ref> stemmer. The lemmatizer is part of the TreeTagger part-ofspeech tagger <ref type="bibr" coords="2,100.67,631.40,15.27,8.97" target="#b16">[17]</ref>. Each word is assigned its syntactic root through lexical look-up. Mainly number, case, and tense information is removed, leaving other morphological processes such as nominalization intact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Document Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Partial Parsing</head><p>At present, full parsing is still computationally rather expensive, and building a grammar that is able to cope with a large number of phenomena is very laborious. For these reasons we decided to use a partial parser which can at least identify simple phrases of various kinds. Our partial parser is based on finite-state technology and is therefore able to process large amounts of data efficiently.</p><p>We focused on identifying noun phrases (NPs), prepositional phrases (PPs) and verb groups (VGs). A verb group is the verbal complex containing the semantic head of a verb phrase (VP) and its auxiliaries (have, be) and modal modifiers (can, would, should, etc.).</p><p>For each noun phrase, its semantic head is marked. If the noun phrase is complex, the right most noun is identified as the head <ref type="bibr" coords="2,346.53,234.49,15.27,8.97" target="#b20">[21]</ref>, which holds for almost all noun phrases in English. <ref type="foot" coords="2,332.58,244.60,3.69,6.63" target="#foot_0">1</ref> Similarly, the semantic head of a prepositional phrase is the head of its noun phrase and the syntactic head is the preposition.</p><p>NPs, PPs, and VGs form the basic constituents of a dependency structure. A dependency structure is headed by a VG and the NPs and PPs in its vicinity are arguments or modifiers of the verb. We did not exploit subcategorization information derived from the verb in order to deal with ambiguities arising from more complex verb-argument relations such as controlling verbs (e.g., promise, persuade), and anaphoric relations. A disadvantage of this approach is that it is harder to distinguish between arguments and modifiers of a verb. On the other hand, using a flat and underspecified representation, one does not have to cope with these ambiguities. Since we did not try to resolve anaphoric relations, this approach has the additional advantage that noun phrases serving as antecedents to intra-sentential pronouns are considered to be part of the dependency structure. Consider, for instance, the sentence in (1), taken from document AP900416-0132.</p><p>(1) Teachers in Oklahoma City and some other districts said they feared reprisals if they took part in the strike.</p><p>Neglecting any context possibly preceding (1), there are four potential antecedents of the plural pronoun they:</p><p>(2) a. Teachers b. Teachers in Oklahoma City c. some other districts d. Teachers in Oklahoma City and some other districts</p><p>The correct antecedent of they, and therefore the subject of fear and take part is (2.d). Since resolving anaphora, and plural anaphora in particular, can be rather complex, we refrained from this task and relaxed our notion of dependency structure instead. There are three dependency structures that can be identified in <ref type="bibr" coords="2,320.95,681.94,10.58,8.97" target="#b0">(1)</ref>. An abstract representation is given in (3):</p><p>(3) a.</p><p>head A number of things require further explanation, and we will briefly discuss most of the features present in the XML structure above. The CAT feature represents the syntactic category of a word or a phrase. The word categories are based on the Penn Treebank tag set, cf. <ref type="bibr" coords="3,146.13,666.59,15.27,8.97" target="#b15">[16]</ref>. The morphologically normalized form of a word, its lemma, is given by the LEM feature. The SEMHEAD feature marks the semantic head of a phrase, and in case of a PP the SYNHEAD feature marks the preposition as the syntactic head. Each occurrence of a word in a document has a unique identifier, indicated by the ID feature. Similarly, each top-level phrase has a unique identifier indicating its scope. The TYPE feature assigns a semantic type to named entities, SMTH (something) being the default value.</p><p>Named entity annotation is discussed in more detail in the next subsection. Whether a verb is in active or passive voice is marked by the VC feature.</p><p>Returning to the representation of dependency structures, this information is contained in the annotation of the VG phrase. The feature DEP has as its value a list of strings, separated by a comma. For instance, l_222-224(district) says that the phrase 222-224 is within the scope to the left of the verb group and that its semantic head is district. Anaphoric phrases, such as 226-226, are not mentioned in the dependency list.</p><p>In addition to VG phrases some noun phrases can also have dependency relations. Nominalizations, such as (4), behave very much like the verbs from which they are derived.</p><p>(4) Mr Paul Volcker, the former chairman of the US Federal Reserve Board, is considering an offer to serve as an adviser to the Russian government on economic and banking reform.</p><p>In (4), taken from document FT921-10181, adviser, or its underlying verb advise, takes Mr Paul Volcker as subject and Russian government as object. To identify nominalizations, we used CELEX <ref type="bibr" coords="3,379.58,363.03,11.62,8.97" target="#b1">[2]</ref> and NOMLEX <ref type="bibr" coords="3,455.09,363.03,16.60,8.97" target="#b9">[10]</ref> as lexical resources. As we will see in Section 2.5, dependency structures are the basic constituents in the answer selection process for several types of questions. Especially questions of the form Who VP? make use of dependency structures to match the question with dependency structures within the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Named Entity Annotation</head><p>In addition to the syntactic annotation described in the previous subsection, we also annotate some named entities with their semantic types. The set of semantic types that we have used is shown in Table <ref type="table" coords="3,405.94,503.09,3.74,8.97" target="#tab_1">1</ref>. Some of the semantic types, such as PERS and LOC, are further divided into subtypes. To identify companies, organizations, associations, etc. we compiled a list of names and extracted 20 features that occur frequently. For instance, &amp;, Inc., and International are likely to indicate the name of a company. If a company or organization name was followed by an expression between parentheses, adhering to some pattern, we took it to be the company's abbreviation and added this information to the annotation in order to allow for aliases. For instance, the annotation for the Asia Pacific Economic Co-operation Group (APEC) is as follows:</p><formula xml:id="formula_0" coords="4,42.52,219.23,228.64,115.93">&lt;C CAT=NP SEMHEAD= TYPE=COMP ABBR=APEC ID=356-364&gt; &lt;C CAT=DT ID=356 LEM=the&gt;the&lt;/C&gt; &lt;C CAT=NNP ID=357 LEM=Asia&gt;Asia&lt;/C&gt; &lt;C CAT=NNP ID=358 LEM=Pacific&gt;Pacific&lt;/C&gt; &lt;C CAT=NNP ID=359 LEM=Economic&gt;Economic&lt;/C&gt; &lt;C CAT=NN ID=360 LEM=cooperation&gt;Co-operation&lt;/C&gt; &lt;C CAT=NNP ID=361 LEM=Group&gt;Group&lt;/C&gt; &lt;C CAT=( ID=362 LEM=(&gt;(&lt;/C&gt; &lt;C CAT=NNP ID=363 LEM=APEC&gt;APEC&lt;/C&gt; &lt;C CAT=) ID=364 LEM=)&gt;)&lt;/C&gt; &lt;/C&gt;</formula><p>Keeping track of abbreviations does not only allow one to match a name with its abbreviation when a question is matched with a dependency structure, it can also be used for questions concerning abbreviations directly; e.g., questions of the form What does X stand for?.</p><p>Phrases of type NUMERIC, DATE, and TIME, are recognized by pattern matching. The TIPSTER gazetteer, containing a list of more than 240,000 locations, is used to find names of cities, provinces, etc.</p><p>The identification of person names uses the U.S. census list of the 80,000 most frequent last names, 4275 most frequent female first names, and 1219 most frequent male first names in the U.S. as a gazetteer. In addition we look for particular indicators for a person name, including titles, such as Mrs., President, Dr., and relative clauses following an NP with capitalized nouns. If a name was identified by pattern matching, it was dynamically added to the list of known names. Whenever it was possible to identify the gender of a person by looking at the first name or title, the more specific subtype information was recorded. Although we did not yet exploit this distinction in the current version of our system, we plan to do so in the future in order to facilitate anaphora resolution.</p><p>If an NP cannot not be recognized by the techniques above, it receives the default semantic type SMTH.</p><p>Obviously, these techniques are rather simple and error prone. In particular, the use of gazetteers has the disadvantages of being inherently incomplete and causing false alarms; see e.g., <ref type="bibr" coords="4,109.04,666.59,16.60,8.97" target="#b10">[11]</ref> for a discussion of the use of gazetteers in the area of Information Extraction. More sophisticated systems such as IdentiFinder TM <ref type="bibr" coords="4,156.38,690.50,11.62,8.97" target="#b2">[3]</ref> therefore use feature learning techniques for named entity annotation. On the other hand, the use of gazetteers has the advantage of being rather simple to implement, which was the main reason we opted for this solution.</p><p>In the current version of system, false alarms account for the majority of errors made by the name entity recognizer. This is caused mainly by the interference of location names and person names. As we do not allow for multiple typing, this has the effect that once a named entity is falsely recognized as being of type A, it cannot be identified as being of type B. Since it is rather unlikely that we will replace the gazetteer look-up by a feature-learning component in the near future -for the aforementioned reasons -we at least intend to allow for multiple typing. As a consequence, false alarms will continue to have a negative impact on precision, but recall should increase.</p><p>Our final remark on the named entity annotation component concerns the interaction between annotation and document retrieval. Currently, the named entity recognizer is applied to the top 100 documents returned by our retrieval system FlexIR. We did not apply the recognizer to the collection as a whole. Pre-processing the whole collection would have two advantages: First, it results in a more efficient system (although efficiency was not one of our major concerns at the current stage), and second, it is possible to index the collection with respect to the semantic types attached to named entities and exploit this additional information during retrieval, cf., e.g., <ref type="bibr" coords="4,348.69,354.23,15.27,8.97" target="#b14">[15]</ref>. The main reason for not doing so was that we developed the named entity recognizer in tandem with the other components. Since applying it to the whole collection is rather time consuming, it would have increased the duration of each development cycle in a significant way. We are hopeful that once we have enabled multiple typing, we will have a stable and reliable version of the recognizer which can used to assist the retrieval process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Question Analysis</head><p>Just like the top 100 documents, the questions themselves were also part-of-speech tagged, morphologically normalized, and partially parsed. Since there is a significant difference between word order in questions and in declarative sentences, we needed to adjust the tagger for questions. To this end, TreeTagger was trained on a set of 500 questions with part-of-speech tags annotated. We used 300 questions taken from the Penn Treebank II data set together with the 200 TREC-8 questions, which we annotated semi-automatically.</p><p>We used 18 categories to classify the focus or target of a question; the first 16 of these are listed in Figure <ref type="figure" coords="4,508.93,606.25,3.74,8.97" target="#fig_1">2</ref>. The two missing categories (what:X and unknown) will shortly be discussed.</p><p>To identify the target of a question, pattern matching is applied to assign one of the 18 categories to the question. In total, a set of 67 patterns is used to accomplish this. Some of the patterns used are shown in Table <ref type="table" coords="4,457.69,678.17,3.74,8.97" target="#tab_2">2</ref>.</p><p>If more than one pattern matches the question, it was assigned multiple targets. The patterns are ordered so that more specific patterns match first. Also, the answer selection component described in the next subsection obeys the order in which questions were categorized to find answers for more specific targets first.</p><p>Questions of type what:X form a special category. Here If none of the matching strategies described so far is able to assign a target to a question, the question is categorized as unknown. As a consequence, none of the answer selection strategies which are particularly suited for the respective question targets can be applied, and a general fall back strategy is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Answer Selection</head><p>Given the parsed and annotated top documents returned by FlexIR and given the parsed and classified questions, the actual process of identifying the answer starts. Although the top 100 documents are analyzed, earlier experiments on TREC-9 questions have shown that in some cases focusing on the top 25 or top 50 documents in the answer selection process results in a better performance. Therefore, we restricted ourselves to analyzing the top 100 documents and varied the parameter of documents analyzed during selection over the submitted runs.</p><p>While answer selection strongly depends on the question target, a basic strategy common to all question types is to match the dependency structure(s) present in the question with dependency structures in the top documents. More precisely, we try to find a maximally matching segment; in our implementation such a segment can be a sentence or a pair of adjacent sentences. Once such a segment has been found, we check whether it contains constituents that fit the appropriate question target. If this is the case, these constituents are marked as potential answers, and the next best matching segment is analyzed, etc.</p><p>For this strategy to work, it is important to have a proper matching algorithm that allows for partial matching and also assigns a weight or score to a match that allows to compare and rank different matches.</p><p>Matching dependency structures involves three steps: First it has to be checked whether the two heads, i.e., verbs, match, and then the overlap between the arguments of the two structures has to be determined. Since the arguments themselves can be complex phrases, it is necessary to also apply phrase matching on this lower level so as to determine to which extent two arguments match.</p><p>There is a number of ways to devise a phrase matching algorithm, although the literature on phrase matching is rather sparse. To our knowledge, there is only one algorithm described in the literature, viz. <ref type="bibr" coords="5,427.95,628.29,10.58,8.97" target="#b6">[7]</ref>. Note that phrase matching is different from phrase weighting, cf., e.g., <ref type="bibr" coords="5,484.24,640.24,10.79,8.97" target="#b4">[5,</ref><ref type="bibr" coords="5,497.22,640.24,11.83,8.97" target="#b17">18]</ref>, which assigns a weight to a whole phrase but does not deal with partial matches between phrases, which is essential in this context.</p><p>Here, we will briefly describe one of our implementations of a phrase matching algorithm which was used for all submitted runs. Given two phrases p1 and p2, the function phrase_match returns a real between 0 and 1 as the matching score. Stop words, such as a, the, some, all, etc., are removed before the phrases are passed as arguments to phrase_match. A pseudo algorithm for phrase_match is given in Figure <ref type="figure" coords="5,542.03,749.06,3.74,8.97">3</ref>.</p><p>First, the if-than-else statement in lines 2-6 checks  whether the semantic heads of the two phrases are identical. If this is the case, the initial score is set to 0.5, otherwise, phrase_match returns with a matching score of 0. This reflects our strong emphasis on the head of a phrase. Of course   this leaves room for other options, such as choosing a different value or not returning immediately if the heads do not match.</p><p>Lines 8-12 compare the lengths of the two phrases, initializing max_length. Since the heads were already compared, they can be neglected and max_length is decremented by 1 in line 9 and 11. max-length is the maximal number of constituents that the two phrases can have in common. Later on it is used for normalization. If max_length equals 0, this means that no constituents other than the heads are to be compared and phrase_match returns with the value 0.5, see lines <ref type="bibr" coords="6,310.71,567.79,27.40,8.97">14-16.</ref> Then, for each constituent occurring in either one of the phrases we check whether it occurs in both phrases (lines <ref type="bibr" coords="6,336.45,604.71,8.78,8.97" target="#b17">[18]</ref><ref type="bibr" coords="6,345.22,604.71,4.39,8.97" target="#b18">[19]</ref><ref type="bibr" coords="6,345.22,604.71,4.39,8.97" target="#b19">[20]</ref><ref type="bibr" coords="6,345.22,604.71,4.39,8.97" target="#b20">[21]</ref><ref type="bibr" coords="6,349.61,604.71,13.16,8.97">[22]</ref>. If this is the case, score is incremented by 0.5/max_length. Finally, line 23 returns the final matching score.</p><p>A couple of remarks are in order. First, except for the identification of the head, we do not consider word order; i.e., matching phrases of the form ABC and BAC get a score of 1 although they differ in word order. A side effect is that the distance of a constituent to the head of its phrase is not considered, although one might argue that the closer a constituent is to the head, the more important it is.</p><p>Another simplification is the fact that we neglect term importance such as tf.idf weighting. Each constituent or term occurring in both phrases contributes equally to the computation of the matching score, even though some terms are obvi-ously more content bearing than others.</p><p>Finally, in the algorithm as it was described above, two constituents are compared with respect to identity. This is a very strict constraint which was softened in the actual implementation of Tequesta. We used WordNet <ref type="bibr" coords="7,213.33,151.26,11.62,8.97" target="#b5">[6]</ref> relations, such as synonomy and hyponomy, thus allowing for a match between two constituents if they are in linked by chain of these WordNet relations.</p><p>Phrase matching is used in the process of matching dependency structures, which, in turn, helps us to rank matching text segments taken from the top documents. Starting with the highest ranked segment, we apply strategies that depend on the question target to extract the answer string from these segments. In the remainder of the subsection we briefly discuss some of our strategies.</p><p>When selecting the answer to a question, we distinguish between the focus, or target, of a question and its topic. The focus is the element the question is asking for, or put differently, the element lacking. The focus, on the other hand, is the information providing some description or context, the answer should fit into.</p><p>Questions of type pers-def or thing-def ask for the function or role of person and some further explanation or definition of a thing, respectively. Often, this kind of information is contained in an apposition (as illustrated by (8.a)) or a relative clause following the occurrence of this person's name or thing's name (as illustrated by (8.b)). In order to make sure that the apposition or relative clause forming the potential answer contains descriptive information rather than some other information we apply further heuristics. For instance, a potential answer is preferred if it contains superlative adjectives, such as first, highest, most, etc., or nouns ending in -er which are likely to describe some role, e.g., winner, member, etc. Questions of type agent ask for an animate entity, such as a person or organization, being the logical agent of an event described in the question. If the dependency structure from the question matches a dependency structure from a document and there is an animate NP in subject position (positive sentence) or within a PP headed by the preposition by, we take this to be the logical agent. Of course, such an NP is disregarded if it already occurs in the question itself. Questions of type object are dealt with analogously.</p><p>Questions of type what:X are particularly interesting because they are very frequent (at least in the TREC data) and explicitly require some lexical knowledge base. Questions of type what:X ask for something that is a kind of X and that fits the further description expressed in the remainder of the question. For example, question 429, given in (9), asks for something which is a university. In <ref type="bibr" coords="7,322.22,488.92,11.62,8.97" target="#b8">(9)</ref> university is the focus of the question and the further constraint was Woodrow Wilson President of? is the topic of the question. In order to establish the relationship between an entity found in a matching dependency structure and the predicate university it is necessary to access a lexical knowledge base. Tequesta exploits WordNet for this purpose. In particular, WordNet's hyponym relations are used.</p><p>While extracting potential answers, we also keep track of the number of steps that had to been taken while traversing WordNet, and the matching scores that were involved. The higher the matching scores and the smaller the number of lexical relations that had to be used from WordNet, the higher the overall answer score of a potential answer. Finally, the extracted answer strings are ordered and the top five are selected as the final set of answers.</p><p>Tequesta also provides a graphical user interface which we use for evaluation and demonstration purposes. Figure <ref type="figure" coords="7,548.41,690.50,4.98,8.97" target="#fig_6">4</ref> shows the two windows that are used to interact with the user. The top window in Figure <ref type="figure" coords="7,415.77,714.41,4.98,8.97" target="#fig_6">4</ref> is the main window; it allows the user to enter a question and provides information on the status of the subtasks involved in answering the question. The bottom window presents the results; in the upper part the extracted answer strings (at most 50 bytes long) are listed and by clicking on them the answer document is displayed. Words occurring in the answer are high-lightened by reversing foreground and background color, and words occurring in the question are displayed in bold face; this is done to facilitate the search for justifications of the extracted answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Main Task</head><p>The main QA task in TREC-10 is similar to the main tasks in TREC-8 and TREC-9. The document set consists of data sets taken from Disks 1-5 of the TIPSTER/TREC document CDs. A total of 500 questions is provided that seek short, fact-based answers. Some questions are not known to have an answer in the document collection. At least one and no more than five ranked responses per question ranked were to be returned for each question, where the first response is to be preferred over the other responses. A response is either a [answer-string, docid] pair or the string "NIL," where the answer-string may contain no more than 50 bytes and the docid must be the id of a document in the collection that supports the answer-string as an answer.</p><p>An [answer-string, docid] pair is judged correct if the answer-string contains an answer to the question, the answerstring is responsive to the question, and the document supports the answer. If the answer-string is responsive and contains a correct answer, but the document does not support that answer, the pair will be judged "unsupported" and the pair will only contribute towards the "lenient" score, not to the "strict" score. Otherwise, the pair is judged incorrect.</p><p>As with TREC-8 and TREC-9, the score assigned to each question is the reciprocal of the rank for the first response to be judged correct, or 0 if no response is judged correct. The total score for a run is the mean reciprocal rank (MRR) over all questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Submitted Runs</head><p>We submitted three runs for the main task (UAmsT10qaM1, M2, and M3). Each of our runs employed the Tequesta system, which was given a total of 979,678 documents to index. The runs differed along 2 dimensions: the number of documents used as input for the answer selection process (either 25 or 50 documents), and the size of the text segments that were used to match the question during the answer selection process (either a single sentence or 2 consecutive sentences); see Table <ref type="table" coords="8,66.90,661.47,3.74,8.97" target="#tab_4">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results and Discussion</head><p>Of the 500 questions that were originally released, eight questions were removed from the evaluation due to various problems with those questions. Table <ref type="table" coords="8,177.61,738.32,4.98,8.97" target="#tab_4">3</ref> summarizes the statistics for each of our three submitted runs (UAmsT10qaM1, M2, and M3) over the (remaining) 492 questions.  <ref type="table" coords="8,350.33,195.99,4.98,8.97" target="#tab_4">3</ref> indicates, it is unlikely that there are significant differences between the MRRs for the three runs that we submitted for the main task. Despite this, we took a closer look at the difference between UAmsT10qaM2 and UAmsT10qaM3.</p><p>We first ordered the questions with respect to the individual reciprocal ranks from run UAmsT10qaM2 and, in case they were identical, with respect to the question's id. Then, we marked the extent to which run UAmsT10qaM3 differs from run UAmsT10qaM2 for each question. Figure <ref type="figure" coords="8,489.13,291.63,4.98,8.97">5</ref> shows the differences for the first 164 ordered questions.</p><p>Although the overall effectiveness of run UAmsT10qaM3 increased by only 3.86% in comparison to run UAmsT10qaM2, it is by no means consistently spread over the questions. For many questions there is a severe decrease in effectiveness. What causes this decrease for some questions is not clear to us at the moment, but we hope to gain further insights by analyzing the results more carefully. The relative contribution of a question class is the MRR for the class multiplied by the proportion of the questions in that class. For example, if a class has an MRR of 0.25, and 10% of all the questions were in that class, the relative contribution would be 0.25 × 0.10 = 0.025. For development purposes it can be especially helpful to record differences in MRR and/or relative contribution. Differences in MRR give an indication of how well a question class was handled. Changes in relative contribution give an indication of how much this matters, and therefore where efforts should be focussed to alter the system's performance.</p><p>It is clear from Table <ref type="table" coords="9,158.91,485.76,4.98,8.97" target="#tab_5">4</ref> that our overall score for UAmsT10qaM3 is strongly positively influenced by our scores on the following classes: thing-def, thing-ident, number, location, and date, while our performance on pers-ident, agent, object, and, especially, what:X, contributed negatively towards our overall score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">List Task</head><p>TREC-10 featured a new task, the QA list task, where answers are to be collected from multiple documents. The list task consisted of 25 questions in the same format as the main task. Each list question specifies a number of instances to be retrieved; e.g., 10 flavors of ice cream in question 11, shown in <ref type="bibr" coords="9,52.76,661.92,15.27,8.97" target="#b9">(10)</ref>. Participants were not allowed to return more instances than specified in the question.</p><p>We modified Tequesta only minimally for this task. Since questions in the list task are typically looking for instances of some description, all questions were classified as what:X type questions. The major difference with the main task is that answers are collected from several documents. When compiling the list of answers we checked for duplicates and near duplicates by using simple techniques such as word overlap while ignoring stop words.</p><p>In the list task, the answers returned are not ranked. Performance is measured in terms of accuracy, which is computed as the number of distinct correct instances divided by the number of instances requested in the question. Table <ref type="table" coords="9,548.41,390.34,4.98,8.97" target="#tab_6">5</ref> summarizes the results for the two submitted runs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runs</head><p>Avg. Accuracy UAmsT10qaL1 0.12 UAmsT10qaL2 0.13</p><p>The strategies for run UAmsT10qaL1 and UAmsT10qaL2 only differ minimally from each other. Run UAmsT10qaL1 uses the top 50 documents to compile the answer list whereas run UAmsT10qaL2 uses the top 25 documents. This similarity between the runs probably also explains the small difference in performance (+8.33%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper we presented our question answering system Tequesta and evaluated its performance in the TREC-10 QA task. Clearly, Tequesta is still in its early stages and our participation in the TREC-10 QA task was very helpful in revealing aspects that need additional attention in future developments of the system. Most of the shortcomings were already discussed in more detail throughout the paper and we will just summarize some of them here. First, the underlying information retrieval system FlexIR that was used for pre-fetching is not tuned for the overall task of question answering. Integrating further constraints into the retrieval process, such as phrase-indexing, locality, and Boolean operators, might help in formulating more structured queries that will increase the density of documents containing an answer in the set of top documents.</p><p>One of the main problems of the named entity recognizer was that it does not allow for multiple semantic types, which results in a high error rate when using gazetteers to assign certain semantic types, such as locations and person names. In addition, we plan to include the annotated semantic types into the index which is used for retrieval.</p><p>Of course, improving the answer selection component remains the main challenge. Table <ref type="table" coords="10,172.19,198.63,4.98,8.97" target="#tab_5">4</ref> shows that there are significant differences in performance between the question types. Especially the performance for questions of type agent, object, and what:X is far below the average performance of the system.</p><p>In this year's participation, we did not spend much time or effort on customizing Tequesta for the list task, but we plan to further develop this aspect of our question answering system, as the problem of fusing information from different sources -in QA as well as in related areas such as multi-document fusion <ref type="bibr" coords="10,69.92,318.18,16.60,8.97" target="#b11">[12]</ref> -strikes us as an interesting challenge.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,49.74,102.80,141.61,7.46;2,49.74,120.95,228.24,215.93"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Tequesta system architecture.</figDesc><graphic coords="2,49.74,120.95,228.24,215.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,49.80,102.80,212.75,7.46;6,49.80,115.15,88.23,7.46"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Question targets, plus examples from the TREC-9 and TREC-10 questions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,317.99,102.80,132.87,7.46;6,322.48,118.59,222.86,8.07;6,322.48,130.54,144.16,8.07;6,322.48,142.50,4.48,8.07;6,359.93,143.07,60.97,7.04;6,322.48,154.45,57.78,8.07;6,322.48,166.41,4.48,8.07;6,359.93,166.98,45.73,7.04;6,322.48,178.36,27.30,8.07"><head>Figure 3 : 3 score</head><label>33</label><figDesc>Figure 3: Phrase matching algorithm. 1 float phrase_match(phrase p1, phrase p2) { 2 if(head(p1) = head(p2)) { 3 score = 0.5; 4 } else { 5 return 0; 6 };</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,322.48,190.32,4.48,8.07;6,322.48,202.27,164.48,8.07;6,322.48,214.23,4.48,8.07;6,359.93,214.80,142.26,7.04;6,317.99,226.18,62.27,8.07;6,317.99,238.14,8.97,8.07;6,359.93,238.71,142.26,7.04;6,317.99,250.09,31.78,8.07;6,317.99,262.05,8.97,8.07;6,317.99,274.00,123.24,8.07;6,317.99,285.96,8.97,8.07;6,349.77,286.53,66.05,7.04;6,317.99,297.91,31.78,8.07"><head>7 8 9 max_length</head><label>79</label><figDesc>if(length(p1) &gt; length(p2)) {</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,317.99,309.87,8.97,8.07;6,317.99,321.14,195.55,9.30;6,317.99,333.78,8.97,8.07;6,349.77,333.10,153.60,9.30;6,317.99,345.73,8.97,8.07;6,370.10,346.31,121.95,7.04;6,317.99,357.69,8.97,8.07;6,349.77,358.26,10.16,7.04;6,317.99,369.64,31.78,8.07;6,317.99,381.60,87.67,8.07;6,317.99,393.55,16.54,8.07"><head>17 18</head><label>17</label><figDesc>foreach const ∈ (p1 ∪ p2)\head(p1) { 19 if(const ∈ (p1 ∩ p2)\head(p1)) { 20 score += 0.5/max_length;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="7,42.52,436.91,114.39,8.97;7,42.52,452.67,26.84,8.97;7,84.36,452.67,174.41,8.97;7,62.45,465.08,222.75,8.97;7,84.36,477.03,101.38,8.97"><head>( 7 )</head><label>7</label><figDesc>Who is Desmond Tutu? (8) a. Tutu, winner of the 1984 Nobel Peace Prize b. Desmond Tutu, who is a member of Harvard University's governing board</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="7,311.92,102.80,125.72,7.46;7,313.13,116.03,237.85,94.76"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Tequesta's user interface.</figDesc><graphic coords="7,313.13,116.03,237.85,94.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="9,42.52,682.14,240.61,8.97"><head>( 10 )</head><label>10</label><figDesc>Name 10 different flavors of Ben and Jerry's ice cream.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,310.71,537.20,242.68,234.00"><head>Table 1 :</head><label>1</label><figDesc>Types for named entity annotation.</figDesc><table coords="3,317.09,548.54,229.98,176.74"><row><cell>Type</cell><cell>Subtypes</cell><cell>Description</cell></row><row><cell>COMP</cell><cell></cell><cell>companies and organizations</cell></row><row><cell>NUMERIC</cell><cell>MONEY</cell><cell>monetary expressions</cell></row><row><cell></cell><cell cols="2">NUM-RATIO percentages</cell></row><row><cell>DATE</cell><cell></cell><cell>explicit dates</cell></row><row><cell>TIME</cell><cell></cell><cell>time periods</cell></row><row><cell>LOC</cell><cell>COUNTRY</cell><cell>countries</cell></row><row><cell></cell><cell>STATES</cell><cell>U.S. states</cell></row><row><cell></cell><cell>PROVINCE</cell><cell>provinces</cell></row><row><cell></cell><cell>CITY</cell><cell>cities</cell></row><row><cell></cell><cell>PORT</cell><cell>harbors</cell></row><row><cell></cell><cell>ISLAND</cell><cell>islands</cell></row><row><cell>PERS</cell><cell>MALE</cell><cell>male persons</cell></row><row><cell></cell><cell>FEMALE</cell><cell>female persons</cell></row><row><cell>SMTH</cell><cell></cell><cell>other NPs</cell></row></table><note coords="3,310.71,738.32,242.68,8.97;3,310.71,750.28,242.68,8.97;3,310.71,762.23,30.99,8.97"><p>Type recognition is accomplished by fairly simple techniques such as pattern matching, gazetteer look-up, or a combination of both.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,42.52,102.80,439.44,409.48"><head>Table 2 :</head><label>2</label><figDesc>Types for question classification.</figDesc><table coords="5,49.19,114.14,432.77,223.89"><row><cell>Question target</cell><cell>Example patterns</cell></row><row><cell>name</cell><cell>/(W|w)hat( wa| i|\')s the name/</cell></row><row><cell>pers-def</cell><cell>/[Ww]ho( wa| i|\')s [A-Z][a-z]+/</cell></row><row><cell>thing-def</cell><cell>/[Ww]hat( wa| i|\')s an? /, / (was|is|are|were) a kind of what/</cell></row><row><cell>pers-ident</cell><cell>/[Ww]ho( wa| i|\')s the/</cell></row><row><cell>thing-ident</cell><cell>/[Ww](hat|hich)( wa| i|\')s the /</cell></row><row><cell>number</cell><cell>/[Hh]ow (much|many) /</cell></row><row><cell>expand-abbr</cell><cell>/stand(s)? for( what)?\s*?/, /is (an|the) acronym/</cell></row><row><cell>find-abbr</cell><cell>/[Ww]hat( i|\')s (the|an) (acronym|abbreviation) for</cell></row><row><cell>agent</cell><cell>/[Ww]ho /, / by whom[\.\?]/</cell></row><row><cell>object</cell><cell>/[Ww]hat (did|do|does) /</cell></row><row><cell>known-for</cell><cell>/[Ww]hy .+ famous/ /[Ww]hat made .+ famous/</cell></row><row><cell>also-known-as</cell><cell>/[Ww]hat( i|\')s (another|different) name /</cell></row><row><cell>name-instance</cell><cell>/Name (a|one|some|an) /</cell></row><row><cell>location</cell><cell>/[Ww]here(\'s)? /, / is near what /</cell></row><row><cell>date</cell><cell>/([Aa]bout )?(W|w)hen /, /([Aa]bout )?(W|w)(hat|hich) year /</cell></row><row><cell>reason</cell><cell>/[Ww]hy /</cell></row><row><cell>what:X</cell><cell>-</cell></row><row><cell>unknown</cell><cell>-</cell></row></table><note coords="5,42.52,361.62,242.68,8.97;5,42.52,373.57,242.68,8.97;5,42.52,385.53,242.68,8.97;5,42.52,397.48,242.68,8.97;5,42.52,409.44,242.68,8.97;5,42.52,421.39,242.68,8.97;5,42.52,433.35,242.68,8.97;5,42.52,445.30,242.69,8.97;5,42.52,457.26,160.91,8.97;5,42.52,477.32,206.13,8.97;5,42.52,491.36,242.68,8.97;5,62.45,503.31,21.33,8.97"><p><p><p><p><p><p>we use partial parsing to identify the appropriate target, symbolized by X in the type. Usually, what:X questions are of the form What NP VP? or What NP PP VP?. After parsing the question, we use the head of the NP following what as the target, potentially modified by further constituents from the NP or PP modifying the head. For instance, question 934 from the TREC-10 question set, shown in</p><ref type="bibr" coords="5,221.82,433.35,10.58,8.97" target="#b4">(5)</ref></p>, is assigned what:plant, and question 1339, shown in</p><ref type="bibr" coords="5,222.50,445.30,10.58,8.97" target="#b5">(6)</ref></p>, is assigned what:breed:of dog as question target.</p>(5) Material called linen is made from what plant? (6) What breed of hunting dog did the Beverly Hillbillies own?</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,310.71,102.80,236.30,102.15"><head>Table 3 :</head><label>3</label><figDesc>Summary of the results for the main task.</figDesc><table coords="8,310.71,114.14,236.30,90.81"><row><cell>UAmsT10qa. . .</cell><cell>M1</cell><cell>M2</cell><cell>M3</cell></row><row><cell>Top documents used</cell><cell>25</cell><cell>50</cell><cell>25</cell></row><row><cell># Sentences in segments</cell><cell>1</cell><cell>1</cell><cell>2</cell></row><row><cell>MRR strict</cell><cell cols="3">0.185 0.183 0.190</cell></row><row><cell>MRR lenient</cell><cell cols="3">0.197 0.196 0.203</cell></row><row><cell>As Table</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,310.71,413.77,242.68,357.43"><head>Table 4 :</head><label>4</label><figDesc>Analysis of the scores for UAmsT10qaM3. Figure 5: Run UAmsT10qaM2 vs. run UAmsT10qaM3.</figDesc><table coords="8,310.71,425.11,242.68,274.35"><row><cell>Question class</cell><cell>#</cell><cell>MRR</cell><cell>Diff.</cell><cell>Rel. Con.</cell></row><row><cell>name</cell><cell cols="3">9 0.111 -41.5%</cell><cell>0.002</cell></row><row><cell>pers-def</cell><cell>3</cell><cell cols="2">0 -100%</cell><cell>0</cell></row><row><cell>thing-def</cell><cell cols="3">110 0.254 +33.8%</cell><cell>0.057</cell></row><row><cell>pers-ident</cell><cell cols="3">22 0.167 -12.3%</cell><cell>0.007</cell></row><row><cell>thing-ident</cell><cell cols="3">107 0.196 +3.30%</cell><cell>0.043</cell></row><row><cell>number</cell><cell cols="3">35 0.267 +40.4%</cell><cell>0.019</cell></row><row><cell>expand-abbr</cell><cell cols="3">4 0.125 -34.2%</cell><cell>0.001</cell></row><row><cell>find-abbr</cell><cell>0</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>agent</cell><cell cols="3">21 0.071 -62.4%</cell><cell>0.003</cell></row><row><cell>object</cell><cell cols="3">18 0.069 -63.5%</cell><cell>0.003</cell></row><row><cell>known-for</cell><cell>0</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>also-known-as</cell><cell cols="3">11 0.273 +43.5%</cell><cell>0.006</cell></row><row><cell>name-instance</cell><cell>2</cell><cell cols="2">0 -100%</cell><cell>0</cell></row><row><cell>location</cell><cell cols="3">27 0.272 +43.0%</cell><cell>0.015</cell></row><row><cell>date</cell><cell cols="3">41 0.250 +31.6%</cell><cell>0.021</cell></row><row><cell>reason</cell><cell>4</cell><cell cols="2">0 -100%</cell><cell>0</cell></row><row><cell>what:X</cell><cell cols="3">71 0.093 -50.8%</cell><cell>0.013</cell></row><row><cell>unknown</cell><cell>7</cell><cell cols="2">0 -100%</cell><cell>0</cell></row><row><cell>Total</cell><cell cols="2">492 0.190</cell><cell></cell><cell></cell></row><row><cell cols="5">Table 4 provides a closer look at our best run for the main</cell></row><row><cell cols="5">task, UAmsT10qaM3, and a breakdown in terms of the indi-</cell></row></table><note coords="8,310.71,702.46,242.68,8.97;8,310.71,714.41,242.68,8.97;8,310.71,726.37,242.68,8.97;8,310.71,738.32,242.68,8.97;8,310.71,750.28,242.68,8.97;8,310.71,762.23,242.68,8.97"><p>vidual question types. Column 1 lists the question classes as discussed in Section 2.4; column 2 lists how many of the 492 questions belonged to a particular class. According to our question classifier two classes did not have any questions in this year's set of questions: find-abbr and known-for. Column 3 lists the mean reciprocal rank for each class of ques-</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,317.09,425.00,170.28,7.46"><head>Table 5 :</head><label>5</label><figDesc>Summary of the results for the list task.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,325.06,754.13,228.37,7.17;2,310.71,763.59,180.08,7.17"><p>Exceptions of the Right-hand Head Rule (RHR) include some hyphenated noun phrases, such as passer-by and mother-in-law.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Christof Monz was supported by the <rs type="funder">Physical Sciences Council</rs> with financial support from the <rs type="funder">Netherlands Organization for Scientific Research (NWO)</rs>, project <rs type="grantNumber">612-13-001</rs>. <rs type="person">Maarten de Rijke</rs> was supported by the <rs type="projectName">Spinoza</rs> project '<rs type="projectName">Logic in Action</rs>' and by grants from <rs type="funder">NWO</rs> under project numbers <rs type="grantNumber">612-13-001</rs>, <rs type="grantNumber">365-20-005</rs>, <rs type="grantNumber">612.069.006</rs>, <rs type="grantNumber">612.000.106</rs>, and <rs type="grantNumber">220-80-001</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_N5c8Vzv">
					<idno type="grant-number">612-13-001</idno>
					<orgName type="project" subtype="full">Spinoza</orgName>
				</org>
				<org type="funded-project" xml:id="_jBWXKby">
					<idno type="grant-number">612-13-001</idno>
					<orgName type="project" subtype="full">Logic in Action</orgName>
				</org>
				<org type="funding" xml:id="_WTbqYxB">
					<idno type="grant-number">365-20-005</idno>
				</org>
				<org type="funding" xml:id="_3qtGHAv">
					<idno type="grant-number">612.069.006</idno>
				</org>
				<org type="funding" xml:id="_NuyGhvR">
					<idno type="grant-number">612.000.106</idno>
				</org>
				<org type="funding" xml:id="_hjksPVw">
					<idno type="grant-number">220-80-001</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,64.10,491.67,37.92,8.97;10,119.80,491.67,56.74,8.97;10,194.32,491.67,90.87,8.97;10,64.10,503.63,221.11,8.97;10,64.10,515.58,60.88,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,119.80,491.67,52.59,8.97">NLP for IR</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,64.10,503.63,216.91,8.97">NAACL/ANLP language technology joint conference</title>
		<imprint>
			<date type="published" when="2000-04-29">April 29, 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,64.10,530.79,221.10,8.97;10,64.10,542.75,221.10,8.97;10,64.10,554.70,221.10,8.97;10,64.10,566.66,39.57,8.97" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="10,269.70,530.79,15.49,8.97;10,64.10,542.75,221.10,8.97;10,64.10,554.70,111.48,8.97">The CELEX lexical database (release 2). Distributed by the Linguistic Data Consortium</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">H</forename><surname>Baayen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Piepenbrock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gulikers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,64.10,581.86,221.10,8.97;10,64.10,593.82,221.10,8.97;10,64.10,605.77,81.64,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,248.80,581.86,36.39,8.97;10,64.10,593.82,134.93,8.97">An algorithm that learns what&apos;s in a name</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,208.61,593.82,72.26,8.97">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="231" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,64.10,620.98,221.10,8.97;10,64.10,632.93,221.10,8.97;10,64.10,644.89,115.12,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,230.35,620.98,54.85,8.97;10,64.10,632.93,147.70,8.97">New retrieval approaches using SMART: TREC 4</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,235.85,632.93,49.36,8.97;10,64.10,644.89,30.13,8.97">Proceedings TREC-4</title>
		<meeting>TREC-4</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="25" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,64.10,660.09,221.10,8.97;10,64.10,672.05,221.10,8.97;10,64.10,684.00,221.10,8.97;10,64.10,695.96,189.88,8.97" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="10,108.82,660.09,176.37,8.97;10,64.10,672.05,221.10,8.97;10,64.10,684.00,111.11,8.97">Experiments in Automatic Phrase Indexing for Document Retrieval: A Comparison of Syntactic and Non-Syntactic Methods</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fagan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Cornell University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="10,64.10,711.16,221.10,8.97;10,64.10,723.12,113.45,8.97" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="10,153.32,711.16,131.88,8.97;10,64.10,723.12,36.16,8.97">WordNet: An Electronic Lexical Database</title>
		<editor>C. Fellbaum</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,64.10,738.32,221.10,8.97;10,64.10,750.28,221.10,8.97;10,64.10,762.23,100.73,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,113.30,738.32,134.40,8.97">A phrase-based matching function</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Galbiati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,254.46,738.32,30.75,8.97;10,64.10,750.28,200.45,8.97">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="36" to="48" />
			<date type="published" when="1991">1991</date>
			<publisher>JA-SIS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,332.29,102.99,221.10,8.97;10,332.29,114.94,221.10,8.97;10,332.29,126.90,221.10,8.97;10,332.29,138.85,221.10,8.97;10,332.29,150.81,221.10,8.97;10,332.29,162.76,221.10,8.97;10,332.29,174.72,221.11,8.97;10,332.29,186.67,221.10,8.97;10,332.29,198.63,37.63,8.97" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gardie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chaudri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Israel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Jacquemin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Maiorano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ogden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Shrihari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strzalkowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weishedel</surname></persName>
		</author>
		<ptr target="http://www-nlpir.nist.gov/projects/duc/roadmapping.html" />
		<title level="m" coord="10,501.09,150.81,52.30,8.97;10,332.29,162.76,221.10,8.97;10,332.29,174.72,98.77,8.97">Issues, tasks, and program structures to roadmap research in question &amp; answering (Q&amp;A)</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,332.29,214.57,221.10,8.97;10,332.29,226.52,221.10,8.97;10,332.29,238.48,105.44,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,390.54,214.57,158.72,8.97">Development of a stemming algorithm</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">B</forename><surname>Lovins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,332.29,226.52,221.10,8.97;10,332.29,238.48,13.06,8.97">Mechanical Translation and Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="22" to="31" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,332.29,254.42,221.10,8.97;10,332.29,266.37,221.11,8.97;10,332.29,278.33,146.48,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,382.91,266.37,166.35,8.97">NOMLEX: A lexicon of nominalizations</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macleod</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Reeves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,343.08,278.33,105.43,8.97">Proceedings EURALEX&apos;98</title>
		<meeting>EURALEX&apos;98</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,332.29,294.27,221.10,8.97;10,332.29,306.22,221.11,8.97;10,332.29,318.18,221.11,8.97;10,332.29,330.13,162.33,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,498.72,294.27,54.67,8.97;10,332.29,306.22,121.57,8.97">Named entity recognition without gazetteers</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Mikheev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Moens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,476.84,306.22,76.55,8.97;10,332.29,318.18,221.11,8.97;10,332.29,330.13,87.71,8.97">Proceedings of the European Chapter of the Association for Computational Linguistics (EACL&apos;99)</title>
		<meeting>the European Chapter of the Association for Computational Linguistics (EACL&apos;99)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,332.29,346.07,221.10,8.97;10,332.29,358.03,221.10,8.97;10,332.29,369.98,221.10,8.97;10,332.29,381.94,143.87,8.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,378.91,346.07,174.48,8.97;10,332.29,358.03,42.87,8.97">Document fusion for comprehensive event description</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,478.43,358.03,74.96,8.97;10,332.29,369.98,221.10,8.97;10,332.29,381.94,114.03,8.97">Proceedings of the ACL 2001 Workshop on Human Language Technology and Knowledge Management</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Maybury</surname></persName>
		</editor>
		<meeting>the ACL 2001 Workshop on Human Language Technology and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,332.29,397.88,221.11,8.97;10,332.29,409.83,221.10,8.97;10,332.29,421.79,221.10,8.97;10,332.29,433.75,42.34,8.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,449.58,397.88,103.81,8.97;10,332.29,409.83,34.64,8.97">University of Amsterdam at CLEF</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,409.31,409.83,144.08,8.97;10,332.29,421.79,114.53,8.97">Proceedings of the Cross Language Evaluation Forum Workshop</title>
		<meeting>the Cross Language Evaluation Forum Workshop</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001. 2001</date>
			<biblScope unit="page" from="165" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,332.29,449.69,221.10,8.97;10,332.29,461.64,86.62,8.97" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,381.86,449.69,127.33,8.97">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,515.62,449.69,33.05,8.97">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,332.29,477.58,221.10,8.97;10,332.29,489.54,221.10,8.97;10,332.29,501.49,162.96,8.97" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,514.09,477.58,39.29,8.97;10,332.29,489.54,144.58,8.97">Questionanswering by predictive annotation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Coden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,504.04,489.54,49.36,8.97;10,332.29,501.49,68.54,8.97">Proceedings ACM SIGIR 2000</title>
		<meeting>ACM SIGIR 2000</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="184" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,332.29,517.43,221.10,8.97;10,332.29,529.39,221.11,8.97;10,332.29,541.34,221.10,8.97;10,332.29,553.30,42.90,8.97" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="10,388.39,517.43,165.00,8.97;10,332.29,529.39,57.59,8.97">Part-of-speech tagging guidelines for the Penn Treebank</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Santorini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
	<note>3rd revision, 2nd printing edition</note>
</biblStruct>

<biblStruct coords="10,332.29,569.24,221.10,8.97;10,332.29,581.19,221.11,8.97;10,332.29,593.15,213.66,8.97" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,384.23,569.24,169.16,8.97;10,332.29,581.19,53.67,8.97">Probabilistic part-of-speech tagging using decision trees</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,404.79,581.19,148.61,8.97;10,332.29,593.15,184.54,8.97">Proceedings of International Conference on New Methods in Language Processing</title>
		<meeting>International Conference on New Methods in Language Processing</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,332.29,609.09,221.10,8.97;10,332.29,621.04,221.10,8.97;10,332.29,633.00,42.34,8.97" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,399.07,609.09,150.80,8.97">Natural language information retrieval</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strzalkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,332.29,621.04,165.46,8.97">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="397" to="417" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,332.29,648.94,221.10,8.97;10,332.29,660.89,182.71,8.97" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,399.34,648.94,154.05,8.97;10,332.29,660.89,52.71,8.97">Overview of the TREC-9 question answering track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,403.10,660.89,81.97,8.97">Proceedings TREC-9</title>
		<meeting>TREC-9</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,332.29,676.83,221.10,8.97;10,332.29,688.79,221.10,8.97;10,332.29,700.74,82.46,8.97" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="10,463.77,676.83,89.62,8.97;10,332.29,688.79,108.27,8.97">The TREC-8 question answering track evaluation</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Tice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,464.96,688.79,83.42,8.97">Proceedings TREC-8</title>
		<meeting>TREC-8</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="83" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,332.29,716.68,221.10,8.97;10,332.29,728.64,199.13,8.97" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="10,383.77,716.68,169.62,8.97;10,332.29,728.64,35.16,8.97">On the notions &apos;lexically related&apos; and &apos;head of a word</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,379.76,728.64,70.19,8.97">Linguistic Inquiry</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="245" to="274" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
