<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,152.88,101.04,289.08,16.65;1,82.08,126.24,430.96,16.65">More Reflections on &quot;Aboutness &quot; TREC-2001 Evaluation Experiments at Justsystem</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,258.48,149.02,78.32,11.10"><forename type="first">Sumio</forename><surname>Fujita</surname></persName>
							<email>sumio_fujita@justsystem.co.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">JUSTSYSTEM Corporation</orgName>
								<address>
									<addrLine>Brains Park</addrLine>
									<postCode>771-0189</postCode>
									<settlement>Tokushima</settlement>
									<country key="JP">JAPAN</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,152.88,101.04,289.08,16.65;1,82.08,126.24,430.96,16.65">More Reflections on &quot;Aboutness &quot; TREC-2001 Evaluation Experiments at Justsystem</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D9926A70CCDB50768B66ADB4A59B3E71</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Aboutness</term>
					<term>pseudo-relevance feedback</term>
					<term>reference database</term>
					<term>fusion</term>
					<term>attribute-value basis re-ranking</term>
					<term>distributed retrieval</term>
					<term>collection partitioning</term>
					<term>database selection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The TREC-2001 Web track evaluation experiments at the Justsystem site are described with a focus on the "aboutness" based approach in text retrieval. In the web ad hoc task, our TREC-9 approach is adopted again, combining both pseudo-relevance feedback and reference database feedback but the setting is calibrated for an early precision preferred search. For the entry page finding task, we combined techniques such as search against partitioned collection with result fusion, and attribute-value basis re-ranking. As post-submission experiments, distributed retrieval against WT10G is examined and two different database partitioning and three database selection algorithms are combined and evaluated.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>When a web page gives information to readers, readers have already understood what the information is about. Information can be about anything, but should be about something in order to be "information".</p><p>A subject concept comprehended by explicit/implicit pacts between authors and readers is the main instance of the objective position of such "about" phrases.</p><p>In the case of artistic writings, they do not necessarily give any information but give some emotional feelings.</p><p>Even an informative document does not necessarily regard a subject concept. A curriculum vitae, for example, gives some information about someone but does not regard any subject concept. Such functional documents work as information carriers according to the complex social/institutional protocols. A curriculum vitae gives information about the professional history of someone. The problem of information access here is split into 1) whose curriculum vitae it is and 2) if it is a curriculum vitae or not. A curriculum vitae of someone can be compared with that of someone else's but also with the medical examination report of this person. Thus information is located in the lattice of syntagmatic and paradigmatic relations of semantics.</p><p>Information access problems against entity topics are in general the identification of the type and the entity in question, given the source of information as well as information needs.</p><p>In the topic relevance search, aboutness is comprehended as representation of subject concepts, while in the entry page finding task, aboutness is split into "entriness"(entry page or not) and entity correctness (which entity it is about?). Neither "entriness" nor entity correctness can be processed as the bag of word representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SYSTEM DESCRIPTION</head><p>For the TREC-2001 Web track experiments, we utilized the engine of Justsystem ConceptBase Search™ version 2.0 as the base system.</p><p>A dual Pentium III™ server (670MHz) running Windows NT™ server 4.0 with 1024MB memory and 136GB hard disk was used for experiments.</p><p>The document collections are indexed wholly automatically, and converted to inverted index files of terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Term Extraction</head><p>In order to compose possible noun phrases, queries and documents in target databases are analyzed by the same module that decomposes an input text stream into a word stream and parses it using simple linguistic rules.</p><p>Extracted units are single word nouns as well as simple linguistic noun phrases that consist of a sequence of nouns or nouns preceded by adjectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Vector Space Retrieval</head><p>Each document is represented as a vector of weighted terms by tf*idf in inverted index files and the query is converted in similar ways.</p><p>Similarity between vectors representing a query and documents are computed using the dot-product measure, and documents are ranked according to decreasing order of RSV. OKAPI BM25 function is utilized as the TF part of weighting function <ref type="bibr" coords="2,128.40,110.68,11.76,9.07" target="#b5">[7]</ref> so that the retrieval process can be considered as probabilistic ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Passage Retrieval</head><p>Since some pages are extremely long in the wt2g data set, we became aware that using passages rather than whole pages as the indexing unit is appropriate for the sake of retrieval effectiveness.</p><p>Passage delimiting is done such that each passage becomes a similar length rather than looking for thematic/discourse boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Phrasal Indexing and Weighting</head><p>Our approach consists of utilizing noun phrases extracted by linguistic processing as supplementary indexing terms in addition to single word terms contained in phrases. Phrases and constituent single terms are treated in the same way, both as independent terms, where the frequency of each term is counted independently based on its occurrence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Pseudo-Relevance Feedback and Reference Database Feedback</head><p>Automatic feedback strategy using pseudo-relevant documents was adopted for automatic query expansion.</p><p>The system submits the first query generated automatically from topic descriptions against the target or reference databases, and considers the top n documents from the ranked list as relevant.</p><p>The term selection module extracts salient terms from these pseudo-relevant documents and adds them to the query vector.</p><p>Then the expanded query vector is submitted against the target databases again and the final relevance ranking is obtained.</p><p>The whole retrieval procedure is as follows:</p><p>1) Automatic initial query construction from the topic description</p><p>2) 1 st pilot search submitted against the reference database</p><p>3) Term extraction from pseudo-relevant documents and feedback 4) 2 nd pilot search submitted against the target database 5) Term extraction from pseudo-relevant documents and feedback 6) Final search to obtain the final results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Term Selection</head><p>Each term in the example documents is scored by some term frequency and document frequency based heuristics measures described in <ref type="bibr" coords="2,401.04,136.84,10.71,9.07" target="#b2">[4]</ref>.</p><p>The terms thus scored are sorted in decreasing order of each score and cut off at a threshold determined empirically.</p><p>In effect, the following parameters in feedback procedures should be decided:</p><p>1) How many documents to be used for feedback?</p><p>2) Where to cut off ranked terms?</p><p>3) How to weight these additional terms?</p><p>These parameters are carefully adjusted using TREC-9 queries (topic 451-500), wt10g data set and the relevance judgement file provided by NIST. Parameter sets for official runs are calibrated so that the early precision rather than average precision is maximized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Spell Variation</head><p>When the system finds non stop-word terms from the "title" field text of topic description, it is clear that no document is returned. In such a case, the initial queries are expanded automatically by generated spell variations.</p><p>The procedure consists of looking for similar words in the word lists extracted from the database. Spelling similarity is measured by a combination of uni-gram, bi-gram and tri-gram matching scores.</p><p>This query expansion was adopted originally for the TREC-9 Web track runs where the "title" field contained some spell errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8">Another source of "aboutness": Anchor Text of Hyperlinks</head><p>When we are asking what a page is talking about, sometimes anchor texts ( or link texts, the texts on which a hyperlink is set ) indicate an exact and very short answer.</p><p>The anchor text is typically an explanation or denotation of the page that it is linked to. Some commercial-based search engines utilize such information for advanced searches [1] <ref type="bibr" coords="2,357.72,644.20,11.16,9.07" target="#b0">[2]</ref>. We treat anchor texts literally as the part of the linked document.</p><p>In total, 6,077,878 anchor texts are added to 1,173,189 linked pages out of 1,692,096 pages in the wt10g data set. So 69% of document pages in the data set are attributed to anchor text information on top of the original page information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.9">Link Structure Analysis</head><p>There seems to be a misunderstanding about the usage of pagerank <ref type="bibr" coords="3,84.02,124.84,13.42,9.07" target="#b0">[2]</ref> like popularity-based ranking that utilizes indirect-link information propagating rank values through hyper-link networks.</p><p>Such a ranking would not help the information seeking activities of individuals unless the individuals' information needs are strongly correlated with the popularity or the collection is heavily polluted by spam pages. The situation in navigation-oriented search seems to be the same as in the subject-oriented search. In order to show the effectiveness of popularity based ranking, information needs should be arranged according to the popularity.</p><p>Instead of the popularity-based ranking, we apply adequate link analysis according to the nature of the information seeking tasks behind the evaluation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.10">Attribute-Value Basis Re-ranking</head><p>Our approach to the entry page finding task consists of combining the scoring results from different analysis procedures of pages. This is intended to rank the pages according to the following aspects:</p><p>"Entriness": the likelihood that the page is the entry point of a site.</p><p>Entity correctness: the likelihood that the page is about the entity indicated by the information need.</p><p>The following four types of analyses are processed:</p><p>-Bag of words analysis This is mainly intended to gather candidate pages to be examined precisely hereafter.</p><p>The following three analyses are intended for rating both "entriness" and entity correctness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-Link analysis</head><p>This examines the number of inter-server linked, inner-server linked and innerserver linker to rate "entriness" of the page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-URL analysis</head><p>This examines URL form, length and names to rate both "entriness" and entity correctness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-Text analysis</head><p>This examines title, inter/inner-server anchor texts and other page extracts to rate mainly entity correctness but also "entriness" by scored pattern matching.</p><p>Through the experiments, we confirmed our expectation that only a small portion of each page is enough to be indexed for the entry page finding task. In fact, only 500 bytes of plain text including the title, the URL, anchor texts and beginning part of the page are indexed in view of the bag of word analysis. As for the link usage, we adopted the "anchor text" of the hyperlink information as we did in TREC-9 <ref type="bibr" coords="4,228.48,142.84,10.71,9.07" target="#b3">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">WEB AD HOC EXPERIMENTS</head><p>Table <ref type="table" coords="4,74.16,158.92,5.04,9.07" target="#tab_0">1</ref> shows the performance of official runs and Table <ref type="table" coords="4,48.24,170.92,5.04,9.07" target="#tab_1">2</ref> shows the length of the queries utilized in each run.</p><p>Initial queries are very short ( in average, 2.38-2.72 single word terms and 0.56-0.60 phrasal terms, maximum 5 single word terms and 3 phrasal terms , minimum 0 single word terms and 0 phrasal terms ) and they do not contain enough terms.</p><p>Table <ref type="table" coords="4,76.08,251.08,5.04,9.07" target="#tab_2">3</ref> shows the performance comparison combining pseudo-relevance feedback and reference database feedback as well as with/without phrasal terms on the basis of jscbtawtl2 and jscbtawtl4 settings.</p><p>The automatic feedback procedure contributes to 16.1% to 18.3 % of consistent improvements in average precision in all cases.</p><p>The final queries contain 80.4-84.86 single word terms and 34.86-37.76 phrasal terms in average (maximum 184 single word terms and 114 phrasal terms, minimum 0 single word terms and 0 phrasal terms). Note that we added many more terms in the final queries than we did in TREC-9.</p><p>The improvement gained by the combination of pseudorelevance feedback and reference database feedback is 21.4% for N index run and 20.9% for NAV index run. It is natural that N index runs where initial queries are shorter gained more from the feedback process. The improvement gain from combined feedback is larger than our TREC-9 experiments( 17% in link runs ). This is mainly caused by our approach to have taken more terms from feedback and promote some terms to the foreground.</p><p>In TREC-9, we explained our approach utilizing "foreground vs background" metaphor. In other words, foreground terms denote directly the subject concept of the information need while background terms connote the subject topic. If the weighting balance is changed in the query, the information need is also shifted.</p><p>In order to promote some terms to the foreground, we adopted a simple voting from two sources of feedback; one is the target collection and the other is the reference collection.</p><p>Doing such calibration, we intended to make the runs be early precision preferred rather than MAP preferred as our TREC-9 runs. Despite this, the official result showed that our system was still MAP and recall preferred in comparison with other systems.</p><p>Supplemental phrasal indexing runs perform better in average precision as well as in R-precision both with/without pseudo-relevance feedback and with/without reference database feedback. The situation observed here is consistent with our experience in TREC-9 web track experiments, but in this case, the effectiveness of phrasal indexing seems to be more stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ENTRY PAGE FINDING EXPERIMENTS</head><p>As table <ref type="table" coords="4,348.96,660.28,5.04,9.07" target="#tab_3">4</ref> shows, we submitted four entry page search runs: jscbtawep1, jscbtawep2, jscbtawep3 and jscbtawep4.</p><p>These four runs adopt essentially the same configuration but differ in two parameters of final scoring.</p><p>The full phrase match bonus weights and the bag of word analysis weights are changed as shown in table 4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Server Database and the Whole Database</head><p>The server database contains 11680 server pages in the wt10g collection.</p><p>In fact, this covers 78% of 100 pre-test queries, i.e., this database contains at least one answer page against each of 78 queries out of 100 queries. It also covers 66.2% of 145 test queries.</p><p>Ten pages from the server database and 1000 pages from the whole database are merged in the manner that the 10 pages from the server database come to the top of the rank.</p><p>Thus far, we applied normal retrieval processing, utilizing bag of word queries.</p><p>MRR of the 10 page ranked lists against the server database accounts for 0.6409 and that of the 1000 page ranked lists against the whole database accounts for 0.4176.</p><p>Merging them together makes MRR of 0.6462.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Attribute-value Basis Re-ranking</head><p>Thus obtained ranked page lists of 1010 pages are cut off at the top 200 pages and re-ranked by the attribute-value basis analysis modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Basic Text Matching and Scoring in view of "Entity Correctness"</head><p>Text fields are scored by the matching procedure that accumulates each word matching point and adjacency point.</p><p>Such analysis is much more powerful than bag of word analysis and is equivalent to the full sub-phrase indexing against all long phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Augmented Text Matching and Scoring in view of "Entity Correctness"</head><p>It is likely that the URL text contains the entity name as the part of the server name or the directory names.</p><p>But it is sometimes the case that the constituent words are agglutinated. The matching is augmented in order to treat such agglutinated names.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Supplemented Text Matching and Scoring in view of "Entriness"</head><p>Some field are matched against pre-coded patterns as follows:</p><p>The "InterServerAnchorText" field is intended to be matched with anchor texts like "go to the homepage of XXX".</p><p>The "InnerServerAnchorText" field is expected to be matched with "back to the home( of XXX)".</p><p>The "Title" field is something like "Welcome to the homepage of XXX".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Link Analysis</head><p>The number of interserver linked, normalized by maximum number of interserver linked, among the candidate pages simply indicates the "entriness" of the page.</p><p>The entry page is also very likely to have at least one linker to the inner server pages unless his/her/their/its web site consists of only one page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Score Composition</head><p>The final score is computed as the sum of the weighted scores from each analysis. Each analysis weight is calibrated by the 100 pre-test topics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>) verLin S(InnerSer verLinked) S(InterSer h) S(FullMatc eFonts) S(L S(Title) verAnchor) S(InnerSer verAnchor) S(InterSer S(URLText) S(URLType) S(BOW) PageScore</head><formula xml:id="formula_0" coords="5,341.76,423.01,153.10,40.03">+ + + + + + + + + =</formula><p>The full phrase match bonus is added only when all the constituent words of the entity name matches and prevents inclining to partial matching in many fields rather than full matching in one field.</p><p>After such re-ranking processes, the final results of MRR 0.746 to 0.769 are obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">DISTRIBUTED RETRIEVAL AGAINST WT10g</head><p>In view of the trade-offs between efficiency and effectiveness, there might be two possibilities for large collection retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1)Centralized Multi-stage Search</head><p>All the units are indexed in a system and first some important parts of each document like title and large font text parts are searched. If the user is not satisfied with the first results or he/she requests an exhaustive search through the collection, the second search looks through all the text part of the documents. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2)Distributed Selective Search</head><p>The collection is partitioned by some criteria like publication date order, author's name order, original document location or content basis classification, etc., and stored into separate databases. The search process consists of 1) selecting databases to be searched, 2) distributed search in all the databases selected, 3)fusion of the result lists from the selected databases, and 4) if the user requests it, the search result lists from all the databases are presented.</p><p>Many studies on distributed retrieval have been done by researchers of the IR society, but so far, web commercial search engines tend to be implemented as centralized search systems. The problem in distributed IR is the database selection; failing to properly select the target databases causes severe degradation in effectiveness. However, some studies claim that the effectiveness of a distributed search is even better than a centralized search when an adequate selection algorithm is applied <ref type="bibr" coords="6,241.62,319.96,12.01,9.07" target="#b4">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Collection Partitioning</head><p>WT10G collection is partitioned in two ways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">104 Pre-defined directory Partitioning</head><p>Each of 104 directories( WTX001 -WTX104 ) of distribution CD-R is utilized as a single database.</p><p>Each database is almost the same size. Each database contains about 10,000 to 20,000 pages and these sizes account for 60 to 80MB in text file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">326 Category Partitioning</head><p>Content basis classification has been done using 326 categories derived from the Yahoo US categories.</p><p>The highest two level categories of Yahoo US <ref type="bibr" coords="6,271.05,494.68,14.54,9.07">[8]</ref> directories were adopted and Web pages linked from them were downloaded in March 2001. These pages (142MB, 19048pages) are stored in the classifier database and each page in WT10G is submitted as a query against this classifier database. Scores of the best 15 ranked (Yahoo linked) pages are voted for the category from which the (yahoo linked) page is linked. Thus, for each page in WT10G, the category is decided and the WT10G pages are stored in partitioned databases.</p><p>In this case, the database size is diverse, ranging from as small as only one page to the maximum 162595 pages (9.6% of the whole collection). Basic statistics measures of the number of pages in each database are shown in table <ref type="table" coords="6,331.44,258.76,3.78,9.07" target="#tab_5">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Database Selection</head><p>The following three algorithms for selecting databases are examined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">CORI</head><p>The formula proposed in <ref type="bibr" coords="6,412.32,334.12,11.76,9.07" target="#b1">[3]</ref>   p(t|c) is the weight of the term t against the collection c and the each database is ranked by the sum of this weight over all query terms. We utilized the setting of k=200 and b=0.8. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Simple DF*ICF</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiments</head><p>We compared two collection partitioning and three database selection methods. Figure <ref type="figure" coords="7,207.12,124.84,5.04,9.07">1</ref> in Appendix A.</p><p>shows a comparison of the combination of two partitioning and three database selection methods by using MAP, R-precision, precision at 20 docs (PREC@20) and the number of relevant documents retrieved (REL_RET).</p><p>For each of the six combinations, we examined 10 runs, decreasing the number of databases to be searched from 100% down to 10% by 10% of the whole collection. For each of topic 501 to 550, the databases were selected from the top n % of the ranked database list utilizing one out of three methods. No feedback is applied in these experiments. Once the databases to be searched are decided, statistics from each database selected are gathered so that a centralized search against the whole selected database is simulated. Consequently, the problem of result fusion is excluded in these experiments.</p><p>Because of the essential similarity of the method, CORI and simple DF*ICF perform very similarly even though CORI seems to perform better in MAP and REL_RET.</p><p>After examining each database selected, we noticed that CORI tends to select larger databases than other methods. In fact, when selecting 10% databases, CORI searched 39% of the pages while TF*ICF searched 20% of pages (See Figure <ref type="figure" coords="7,97.44,409.00,5.04,9.07">2</ref> in Appendix A.).</p><p>Content basis partitioned databases perform clearly better, especially when the portion of the collection to be searched is reduced. The most notable thing is that using a combination of content basis partitioned databases and CORI or DF*ICF, the early precision(PREC@20) is even getting better when reducing the number of databases.</p><p>DF*ICF especially marked the best PREC@20 when searching only 41% of pages out of the whole collection(20% by database numbers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>TREC-2001 Web track evaluation experiments at Justsystem group are described.</p><p>The following conclusions are drawn from these experiments:</p><p>1)We modified our TREC-9 approach, i.e., longer vectors with background down-weighting and promoting some terms to the foreground, seem to perform well.</p><p>2)A three stage approach, i.e., bag of word analyses, result fusion and attribute-value basis re-ranking, is successfully applied to the entry page finding task.</p><p>3)A distributed selective search performs better than a centralized search in early precision when an adequate database selection method and collection partitioning are applied.</p><p>4)A simple DF*ICF database selection method performs as well as the CORI method.</p><p>5)A distributed selective search performs better with content basis category partitioning of the collection than (near) random partitioning.</p><p>6)Distributed selective search is possibly a good option in early precision preferred retrieval tasks against very large collections.</p><p>In future work, we will examine better partitioning methods by equalizing the number of pages in each database of content basis category partitioning. c ¤d ¢e 9f g h i p q ¤r s t u ¢r v `w x y q ¤r s t u ¢r v ¤ 9 ¢ 9 9 ¤ 9 ¢ 9 9 d 9 9e f ¤g h 9g i ¢j k l m l n l m l o l m l p q r q s q r t q r t u q r t v q r t w q r t s q r u q r u u t q u q x q v q y z { z </p><formula xml:id="formula_1" coords="8,31.60,551.00,206.20,150.70">Q R R R R R R Q S R R R R R Q T R R R R R Q U R R R R R Q V R R R R R Q R S R W X Y X</formula><p>`a b a c d e d f g h g g i ¡p q p r s p t @u ¡v xw y ¡ d e f g h i j ¡k l xm n g ¡h o ¡p q r q p s t q u v w</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,48.24,522.40,321.60,10.80"><head>Figure1:Figure2:</head><label></label><figDesc>Figure1: Performance of DB Selection Runs with topic 501-550</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,380.10,294.25,4.60,5.40;8,359.50,294.25,1.90,5.40;8,400.00,301.25,20.20,5.40;8,521.70,195.35,11.10,5.40;8,521.70,202.05,16.80,5.40;8,521.70,205.95,14.70,8.00;8,521.70,212.45,20.40,8.00;8,521.70,218.95,24.00,8.00;8,521.70,225.35,29.80,8.00;8,149.70,316.85,16.90,5.40;8,45.10,489.35,2.70,5.40;8,40.50,477.15,2.70,5.40;8,40.50,465.15,9.20,5.40;8,42.40,452.95,3.50,5.40;8,40.50,440.65,5.40,5.40;8,42.40,428.65,3.20,5.40;8,40.50,416.45,3.20,5.40;8,40.50,404.15,9.20,5.40;8,42.40,392.15,4.00,5.40;8,40.50,379.95,5.90,5.40;8,42.40,367.95,2.70,5.40;8,40.50,355.75,2.70,5.40;8,40.50,343.45,9.20,5.40;8,42.40,331.45,3.50,5.40;8,234.90,495.65,2.30,5.40;8,215.70,495.65,0.80,5.40;8,193.20,495.65,1.90,5.40;8,172.30,495.65,1.90,5.40;8,151.60,495.65,3.90,5.40;8,130.80,495.65,3.80,5.40;8,110.10,495.65,3.80,5.40;8,89.50,495.65,3.80,5.40;8,68.60,495.65,3.80,5.40;8,47.50,495.65,5.20,5.40;8,130.00,502.55,27.40,5.40;8,35.90,419.45,2.40,5.40;8,35.90,413.75,2.40,5.40;8,35.90,406.85,2.40,5.40;8,35.90,401.55,2.40,5.40;8,251.70,394.15,16.80,5.40;8,251.70,400.85,16.80,5.40;8,251.70,407.35,20.40,5.40;8,251.70,413.75,20.60,5.40;8,251.70,420.25,29.80,5.40;8,251.70,426.75,29.80,5.40;8,420.20,316.85,14.70,5.40;8,316.30,489.35,2.40,5.40;8,312.40,474.95,6.30,5.40;8,312.40,460.55,6.30,5.40;8,312.40,446.15,6.30,5.40;8,312.40,431.75,6.30,5.40;8,311.00,417.65,7.70,5.40;8,311.00,403.25,7.70,5.40;8,311.00,388.85,7.70,5.40;8,311.00,374.45,7.70,5.40;8,311.00,360.05,7.70,5.40;8,310.50,345.85,8.20,5.40;8,310.50,331.45,8.20,5.40;8,504.90,495.65,3.90,5.40;8,483.80,495.65,4.30,5.40;8,463.40,495.65,4.30,5.40;8,442.80,495.65,4.30,5.40;8,422.10,495.65,4.30,5.40;8,401.50,495.65,4.30,5.40;8,380.80,495.65,4.40,5.40;8,360.20,495.65,4.30,5.40;8,339.60,495.65,4.30,5.40;8,318.70,495.65,5.70,5.40;8,400.50,502.55,27.40,5.40;8,305.90,418.55,3.80,5.40;8,305.90,413.95,3.80,5.40;8,305.90,407.95,3.80,5.40;8,521.70,394.15,16.80,5.40;8,521.70,400.85,26.40,5.40;8,529.20,407.35,12.90,5.40;8,521.70,413.75,33.60,5.40;8,528.00,420.70,23.50,4.80;8,521.70,427.20,455.60,4.80;8,122.80,538.00,56.20,4.80;8,42.90,691.10,2.40,4.80;8,33.30,675.50,12.00,4.80;8,33.30,659.90,12.00,4.80;8,33.30,644.60,12.00,4.80;8,33.30,629.00,12.00,4.80"><head></head><label></label><figDesc>¡ ¢ ¡£ ¤ ¥ ý ¢þ ¤ÿ ¡ ¢ ¦ § ©¡ ý ¢ ¡£ ¤ ¥ © ¡ ý ¢ ¦ § ©¡ ¡ ¢ © ¡ ¢ ¡£ ¤ ¥ © ¡ ¡ ¢ © ¡ ¢ ¦ § ¡ "! # $ ¡% &amp; ' ( ) # 0 21 3 "4 5 # 20 # 6 # 7 4 # 8 @9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,244.56,114.31,302.31,577.49"><head>Table 1 : Performance of official runs</head><label>1</label><figDesc></figDesc><table coords="3,312.00,114.31,216.72,69.65"><row><cell>Run tag</cell><cell>Index</cell><cell cols="3">RefTerms Avg. Prec R-Prec</cell></row><row><cell>jscbtawtl1</cell><cell>N</cell><cell>Strong</cell><cell>0.1890</cell><cell>0.2020</cell></row><row><cell>jscbtawtl2</cell><cell>N</cell><cell>Weak</cell><cell>0.1954</cell><cell>0.2150</cell></row><row><cell>jscbtawtl3</cell><cell>NVA</cell><cell>Strong</cell><cell>0.2003</cell><cell>0.2226</cell></row><row><cell>jscbtawtl4</cell><cell>NVA</cell><cell>Weak</cell><cell>0.2060</cell><cell>0.2308</cell></row></table><note coords="3,309.36,340.60,223.12,9.07;3,309.36,372.76,237.24,9.07;3,309.36,384.76,128.48,9.07;3,309.36,400.84,237.24,9.07;3,309.36,412.84,122.72,9.07;3,309.36,428.92,237.51,9.07;3,309.36,440.92,207.20,9.07;3,309.36,457.00,237.51,9.07"><p>We submitted four title-only automatic runs as follows: jscbtawtl1: title only, link run with noun phrase indexing, more weight on reference terms jscbtawtl2: title only, link run with noun phrase indexing, less weight on reference terms jscbtawtl3: title only, link run with noun phrase, adjective and verb indexing, more weight on reference terms jscbtawtl4: title only, link run with noun phrase, adjective</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,246.72,705.31,291.44,21.07"><head>Table 2 : Length of queries measured by number of single word terms and phrasal terms ( without spell variation expansion</head><label>2</label><figDesc></figDesc><table /><note coords="3,504.72,717.31,3.36,9.07;4,48.24,98.68,201.44,9.07"><p><p>)</p>and verb indexing, less weight on reference terms</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,303.12,113.83,238.68,433.04"><head>Table 3 : Performance comparison ( Title only, jscbtawtl2-4 parameter set )</head><label>3</label><figDesc></figDesc><table coords="4,303.12,113.83,230.40,398.45"><row><cell>Run description</cell><cell>Ref</cell><cell cols="3">PFB AvgPrec R-Prec</cell></row><row><cell>N index / SW + phrases</cell><cell>Yes</cell><cell>Yes</cell><cell>0.1954</cell><cell>0.2150</cell></row><row><cell>(jscbtawtl2)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>N index / SW + phrases</cell><cell>Yes</cell><cell>No</cell><cell>0.1730</cell><cell>0.2013</cell></row><row><cell>N index / SW + phrases</cell><cell>No</cell><cell>Yes</cell><cell>0.1903</cell><cell>0.2074</cell></row><row><cell>N index / SW + phrases</cell><cell>No</cell><cell>No</cell><cell>0.1609</cell><cell>0.1898</cell></row><row><cell>N index / Single words</cell><cell>Yes</cell><cell>Yes</cell><cell>0.1854</cell><cell>0.2051</cell></row><row><cell>only</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>N index / Single words</cell><cell>Yes</cell><cell>No</cell><cell>0.1685</cell><cell>0.1915</cell></row><row><cell>only</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>N index / Single words</cell><cell>No</cell><cell>Yes</cell><cell>0.1837</cell><cell>0.2078</cell></row><row><cell>only</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>N index / Single words</cell><cell>No</cell><cell>No</cell><cell>0.1537</cell><cell>0.1841</cell></row><row><cell>only</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NVA index / SW +</cell><cell>Yes</cell><cell>Yes</cell><cell>0.2060</cell><cell>0.2308</cell></row><row><cell>phrases (jscbtawtl4)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NVA index / SW +</cell><cell>Yes</cell><cell>No</cell><cell>0.1824</cell><cell>0.2106</cell></row><row><cell>phrases</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NVA index / SW +</cell><cell>No</cell><cell>Yes</cell><cell>0.1979</cell><cell>0.2417</cell></row><row><cell>phrases</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NVA index / SW +</cell><cell>No</cell><cell>No</cell><cell>0.1704</cell><cell>0.2149</cell></row><row><cell>phrases</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NVA index / Single</cell><cell>Yes</cell><cell>Yes</cell><cell>0.1997</cell><cell>0.2357</cell></row><row><cell>words only</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NVA index / Single</cell><cell>Yes</cell><cell>No</cell><cell>0.1745</cell><cell>0.2083</cell></row><row><cell>words only</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NVA index / Single</cell><cell>No</cell><cell>Yes</cell><cell>0.1894</cell><cell>0.2217</cell></row><row><cell>words only</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NVA index / Single</cell><cell>No</cell><cell>No</cell><cell>0.1641</cell><cell>0.2062</cell></row><row><cell>words only</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,51.84,114.31,233.52,103.04"><head>Table 4 : Performance of official runs of the Entry</head><label>4</label><figDesc></figDesc><table coords="5,51.84,114.31,224.32,80.45"><row><cell>Run tag</cell><cell>full-</cell><cell>bow</cell><cell>MRR</cell><cell>Top10</cell><cell>NF%</cell></row><row><cell></cell><cell>match</cell><cell>wght</cell><cell></cell><cell>%</cell><cell></cell></row><row><cell>jscbtawep1</cell><cell cols="2">moder low</cell><cell>0.754</cell><cell>83.4</cell><cell>9.0</cell></row><row><cell>jscbtawep2</cell><cell cols="2">moder med</cell><cell>0.769</cell><cell>83.4</cell><cell>9.0</cell></row><row><cell>jscbtawep3</cell><cell>high</cell><cell>med</cell><cell>0.752</cell><cell>83.4</cell><cell>9.0</cell></row><row><cell>jscbtawep4</cell><cell cols="2">moder high</cell><cell>0.746</cell><cell>83.4</cell><cell>8.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,310.32,334.12,165.33,151.49"><head></head><label></label><figDesc>is adopted.</figDesc><table coords="6,310.32,399.19,165.33,86.42"><row><cell></cell><cell></cell><cell></cell><cell cols="3">log(</cell><cell>|</cell><cell>|</cell><cell cols="2">. 0</cell><cell cols="2">5</cell><cell>)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">log(|</cell><cell>|</cell><cell>1</cell><cell cols="4">) 0 .</cell><cell>(2.3)</cell></row><row><cell></cell><cell>p(t</cell><cell>|</cell><cell>c)</cell><cell></cell><cell cols="3">d_b</cell><cell cols="3">(1</cell><cell>-</cell><cell>d_b)</cell><cell>*</cell><cell>T</cell><cell>*</cell><cell>I</cell><cell>(2.4)</cell></row><row><cell>d_t,</cell><cell>d_b</cell><cell>:</cell><cell cols="2">0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>cw</cell><cell>:</cell><cell cols="4">number</cell><cell>of</cell><cell cols="5">words</cell><cell>in</cell><cell>a</cell><cell>database</cell></row><row><cell></cell><cell>df</cell><cell>:</cell><cell cols="5">document</cell><cell cols="5">frequency</cell><cell>of</cell><cell>the</cell><cell>term</cell><cell>t</cell><cell>in the</cell><cell>collection</cell><cell>c</cell></row><row><cell></cell><cell>CF</cell><cell>:</cell><cell cols="4">number</cell><cell>of</cell><cell cols="5">collection</cell><cell>s</cell><cell>where</cell><cell>the</cell><cell>term</cell><cell>t</cell><cell>appears</cell></row><row><cell></cell><cell></cell><cell>|</cell><cell>C</cell><cell>|</cell><cell>:</cell><cell cols="5">number</cell><cell cols="2">of</cell><cell>collection</cell><cell>s</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,50.16,515.80,496.80,219.55"><head>Table 5 : Basic Statistics of number of pages in each database of 326 category partitioning</head><label>5</label><figDesc>*AVG-IDF is similar to DF*ICF but instead of ICF, average IDF of the term over all the databases is utilized.</figDesc><table coords="6,309.36,561.16,237.60,143.92"><row><cell cols="12">This is simplest version of DF*ICF, an essential part of</cell></row><row><cell cols="12">the CORI method.</cell></row><row><cell>DF</cell><cell cols="2">=</cell><cell cols="3">d_t</cell><cell cols="2">+</cell><cell cols="3">(1</cell><cell>-</cell><cell>d_t)</cell><cell>MAX_df df</cell><cell>(3.1)</cell></row><row><cell>ICF</cell><cell cols="2">=</cell><cell cols="4">log(</cell><cell cols="3">| CF C</cell><cell cols="2">|</cell><cell>)</cell><cell>(3.2)</cell></row><row><cell>p(t</cell><cell>|</cell><cell></cell><cell>c)</cell><cell cols="2">=</cell><cell></cell><cell cols="2">DF</cell><cell></cell><cell cols="2">*</cell><cell>ICF</cell><cell>(3.3)</cell></row><row><cell>d_t</cell><cell>:</cell><cell></cell><cell cols="2">0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>df</cell><cell>:</cell><cell cols="10">document</cell><cell>frequency</cell><cell>of</cell><cell>the</cell><cell>term</cell><cell>t</cell><cell>in the</cell><cell>collection</cell><cell>c</cell></row><row><cell>MAX_df</cell><cell>:</cell><cell></cell><cell cols="9">maximum</cell><cell>df</cell><cell>of</cell><cell>the</cell><cell>term</cell><cell>through</cell><cell>collection</cell><cell>s</cell></row><row><cell>CF</cell><cell>:</cell><cell></cell><cell cols="6">number</cell><cell cols="3">of</cell><cell>s collection</cell><cell>where</cell><cell>the</cell><cell>term</cell><cell>t</cell><cell>appears</cell></row><row><cell></cell><cell>|</cell><cell></cell><cell>C</cell><cell>|</cell><cell>:</cell><cell></cell><cell cols="5">number</cell><cell>of</cell><cell>s collection</cell></row><row><cell cols="12">5.2.3 DF*AVG-IDF</cell></row></table><note coords="6,309.36,708.04,11.78,9.07"><p>DF</p></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,327.36,319.24,219.51,9.07;7,327.36,331.24,219.60,9.07;7,327.36,343.24,219.60,9.07;7,327.36,355.24,112.92,9.07" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,421.44,319.24,125.44,9.07;7,327.36,331.24,137.45,9.07">The Anatomy of a Large-Scale Hypertextual Web Search Engine</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,485.76,331.24,61.20,9.07;7,327.36,343.24,219.60,9.07;7,327.36,355.24,44.18,9.07">Proceedings of the Seventh International World Wide Web Conference</title>
		<meeting>the Seventh International World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="107" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,327.36,371.32,219.36,9.07;7,327.36,383.32,219.12,9.07;7,327.36,395.32,219.44,9.07;7,327.36,407.32,215.88,9.07" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,505.92,371.32,40.80,9.07;7,327.36,383.32,199.55,9.07">Searching Distributed Collections With Inference Networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,327.36,395.32,219.44,9.07;7,327.36,407.32,72.74,9.07">Proceedings of the 18th Annual International ACM SIGIR Conference</title>
		<meeting>the 18th Annual International ACM SIGIR Conference<address><addrLine>Seattle Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="21" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,327.36,423.40,219.24,9.07;7,327.36,435.40,219.24,9.07;7,327.36,447.40,219.12,9.07;7,327.36,459.40,219.28,9.07;7,327.36,471.40,219.36,9.07;7,327.36,483.40,160.92,9.07" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,327.36,447.40,203.74,9.07">CLARIT TREC Design, Experiments and Results</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">G</forename><surname>Lefferts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">K</forename><surname>Handerson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Archbold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,327.36,459.40,219.28,9.07;7,327.36,471.40,84.69,9.07">Proceedings of the First Text REtrieval Conference(TREC-1)</title>
		<meeting>the First Text REtrieval Conference(TREC-1)<address><addrLine>Washington D.C.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="494" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,327.36,499.48,219.36,9.07;7,327.36,511.48,219.51,9.07;7,327.36,523.48,219.28,9.07;7,327.36,535.48,191.88,9.07" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Fujita</surname></persName>
		</author>
		<title level="m" coord="7,383.52,499.48,163.20,9.07;7,327.36,511.48,219.51,9.07;7,327.36,523.48,219.28,9.07;7,327.36,535.48,84.69,9.07">Reflections on &quot;Aboutness&quot;-TREC-9 Evaluaton Experiments at Justsystem ,in the Notebook version of the Ninth Text REtrieval Conference(TREC-9)</title>
		<meeting><address><addrLine>Gaithersburg MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,327.36,551.56,219.24,9.07;7,327.36,563.56,219.12,9.07;7,327.36,575.56,219.36,9.07;7,327.36,587.56,219.24,9.07;7,327.36,599.56,126.36,9.07" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,395.76,563.56,150.72,9.07;7,327.36,575.56,89.87,9.07">The impact of Database Selection on Distributed Searching</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Connell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Viles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,441.36,575.56,105.36,9.07;7,327.36,587.56,214.82,9.07">Proceedings of the 23rd Annual International ACM SIGIR Conference</title>
		<meeting>the 23rd Annual International ACM SIGIR Conference<address><addrLine>Athens Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="232" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,333.60,615.64,213.12,9.07;7,327.36,627.64,219.12,9.07;7,327.36,639.64,219.28,9.07;7,327.36,651.64,219.36,9.07;7,327.36,663.64,160.92,9.07" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,458.64,627.64,70.73,9.07">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,327.36,639.64,219.28,9.07;7,327.36,651.64,84.69,9.07">Proceedings of the Third Text REtrieval Conference(TREC-3)</title>
		<meeting>the Third Text REtrieval Conference(TREC-3)<address><addrLine>Washington D.C.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">500</biblScope>
			<biblScope unit="page" from="109" to="126" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
