<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,109.12,87.38,393.75,12.64;1,229.60,103.58,152.82,12.64">JHU/APL at TREC 2001: Experiments in Filtering and in Arabic, Video, and Web Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,135.16,132.52,61.73,8.96"><forename type="first">James</forename><surname>Mayfield</surname></persName>
							<email>mayfield@jhuapl.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Research and Technology Development Center</orgName>
								<orgName type="laboratory">Applied Physics Laboratory</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>11100 Johns Hopkins Road</addrLine>
									<postCode>20723-6099</postCode>
									<settlement>Laurel</settlement>
									<region>Maryland</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,203.80,132.52,58.89,8.96"><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
							<email>mcnamee@jhuapl.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Research and Technology Development Center</orgName>
								<orgName type="laboratory">Applied Physics Laboratory</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>11100 Johns Hopkins Road</addrLine>
									<postCode>20723-6099</postCode>
									<settlement>Laurel</settlement>
									<region>Maryland</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,270.76,132.52,97.30,8.96"><forename type="first">Christine</forename><surname>Cash Costello</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research and Technology Development Center</orgName>
								<orgName type="laboratory">Applied Physics Laboratory</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>11100 Johns Hopkins Road</addrLine>
									<postCode>20723-6099</postCode>
									<settlement>Laurel</settlement>
									<region>Maryland</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,370.60,132.52,24.04,8.96;1,418.24,132.52,20.53,8.96"><forename type="first">Amit</forename><surname>Piatko</surname></persName>
							<email>piatko@jhuapl.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Research and Technology Development Center</orgName>
								<orgName type="laboratory">Applied Physics Laboratory</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>11100 Johns Hopkins Road</addrLine>
									<postCode>20723-6099</postCode>
									<settlement>Laurel</settlement>
									<region>Maryland</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,441.28,132.52,35.50,8.96"><surname>Banerjee</surname></persName>
							<email>banerjee@jhuapl.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Research and Technology Development Center</orgName>
								<orgName type="laboratory">Applied Physics Laboratory</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>11100 Johns Hopkins Road</addrLine>
									<postCode>20723-6099</postCode>
									<settlement>Laurel</settlement>
									<region>Maryland</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,109.12,87.38,393.75,12.64;1,229.60,103.58,152.82,12.64">JHU/APL at TREC 2001: Experiments in Filtering and in Arabic, Video, and Web Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">21ECA7B1DB27AE92CAE9D73D16686450</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview</head><p>The outsider might wonder whether, in its tenth year, the Text Retrieval Conference would be a moribund workshop encouraging little innovation and undertaking few new challenges, or whether fresh research problems would continue to be addressed. We feel strongly that it is the later that is true; our group at the Johns Hopkins University Applied Physics Laboratory (JHU/APL) participated in four tracks at this year's conference, three of which presented us with new and interesting problems. For the first time we participated in the filtering track, and we submitted official results for both the batch and routing subtasks. This year, a first attempt was made to hold a content-based video retrieval track at TREC, and we developed a new suite of tools for image analysis and multimedia retrieval. Finally, though not a stranger to cross-language text retrieval, we made a first attempt at Arabic language retrieval while emphasizing a language-neutral approach that has worked well in other languages. Thus, our team found several challenges to face this year, and this paper mainly reports our initial findings.</p><p>We also made a last-minute (really a last 36 hour) effort to participate in the web retrieval track. We unearthed a year-old index and the software that we used for the web task at TREC-9, and very quickly produced some official submissions. Our main interest in the home-page finding task was to submit content-only runs that could serve as a simple baseline to which other group's sophisticated hyperlink-influenced approaches might be compared. We simply did not have the time to seriously investigate the more complex problems being examined by the web track; however, we wanted to be good TREC citizens and contribute to the document pools.</p><p>All of our text-based investigations were based on the Hopkins Automated Information Retriever for Combing Unstructured Text, or HAIRCUT system. HAIRCUT is a Java-based tool developed internally at JHU/APL that was first used to compare tokenization methods during TREC-6. HAIRCUT benefits from a basic design decision to support flexibility throughout the system. For example, the software supports words, stemmed words, character n-grams, and multiword phrases as indexing terms. And, several methods for computing document similarity are supported, though we recently have relied on probabilistic methods based on statistical language modeling techniques.</p><p>In general, we have seen better performance using language models than when using cosine-based vector scoring. In our experiments we used a linguistically motivated probabilistic model. Hiemstra and de Vries describe this model and explain how it relates to both the Boolean and vector space models <ref type="bibr" coords="1,381.88,378.04,10.69,8.96" target="#b3">[4]</ref>. The model has also been cast as a rudimentary Hidden Markov Model <ref type="bibr" coords="1,478.36,389.20,15.43,8.96" target="#b14">[15]</ref>. Although the model does not explicitly incorporate inverse document frequency, it does favor documents that contain more of the rare query terms. The similarity measure can be expressed as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(</head><p>)</p><formula xml:id="formula_0" coords="1,324.04,453.94,203.56,48.14">) , ( ) ( ) 1 ( ) , ( ) , ( q t f terms t t df d t f d q Sim ∏ = ⋅ - + ⋅ = α α Equation 1. Similarity calculation.</formula><p>where <ref type="bibr" coords="1,352.84,515.56,20.36,8.97">(1-)</ref> is the probability that a query word is generated by a generic language model, and ¡ is the probability that it is generated by a document-specific model. df(t) denotes the relative document frequency of term t.</p><p>We conducted all of our work on a set of four Sun Microsystems workstations that are shared among our department (80 physicists, chemists, engineers, and about 25 computer scientists). Two of the machines are 4-node Sun Microsystems Ultra Enterprise 450 servers with 2.5 and 4.0 GB of physical memory, respectively; the other two machines are Sun Ultra-2 workstations with 1.25 of RAM. This cluster has 200GB of dedicated, networked disk space for use in our retrieval work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Filtering Track</head><p>We participated in both the routing and batch tasks for the filtering track. We did not use any of the hierarchy information available with the Reuters categories for either task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Routing Task</head><p>Our goal for the routing task was to evaluate the use of a statistical language model for routing. We submitted two runs, one based on a character n-gram (n=6) index (apl10frn) and one based on a stem index (apl10frs) using a derivative version of the SMART stemmer. We also created an unofficial word-based run (apl10frw). We simulated routing, using a modified version of HAIRCUT system to score indexed test documents using training index statistics -the statistical language model described above was used for scoring. We formed queries using 60 terms per topic that were selected from the positive batch qrels documents. Term selection was accomplished using mutual information based difference statistics with respect to the August 96 training data.</p><p>We were pleased with our official results for our first participation in this task. We were excited to participate in the "routing bet" discussion and we can report that we have 28 queries (exactly 1/3 of the queries) with ≥ 0.9 precision at 1000 docs in both our official runs. The closeness of the results indicates the choice of terms is not critical. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Batch Task</head><p>Our goal for the batch task was to evaluate the effectiveness of Support Vector Machines (SVMs) on the new Reuters data set <ref type="bibr" coords="2,184.72,534.52,15.43,8.96" target="#b17">[22]</ref>. SVMs are used to create classifiers from a set of labeled training data. SVMs find a hyperplane (possibly in a transformed space) to separate positive examples from negative examples. This hyperplane is chosen to maximize the margin (or distance) to the training points. The promise of large margin classification is that it does not overfit the training data and generalizes well to test data of similar distribution. See Hearst <ref type="bibr" coords="2,253.00,623.80,11.72,8.96" target="#b2">[3]</ref> for a general discussion of SVMs.</p><p>For the batch task, we sought to explore the effects of different parameter choices on learning with this Reuters collection. We were interested in the use of tf/idf weighted vectors vs. per-topic binary vectors; the use of radial basis function (RBF) vs. linear kernels in the SVMs; score thresholds on resulting classifier scores; and training skew factors to incur less error on positive examples. This follows earlier work in text batch filtering on the original smaller Reuters collection <ref type="bibr" coords="2,402.16,87.16,11.72,8.96" target="#b1">[2]</ref>  <ref type="bibr" coords="2,418.00,87.16,10.69,8.96" target="#b5">[6]</ref>. We used the SVM-light package (version 3.50, by Thorsten Joachims <ref type="bibr" coords="2,509.44,98.32,16.09,8.96">[19]</ref>) to create classifiers based on the training data for classification of the test data. We used a reduced feature space for both batch submissions. For all runs, we normalized document vectors to unit length.</p><p>Our post-submission results show: tf/idf trainingderived features were better than topic-specific binary ones; RBF kernels were slightly better than linear kernels; aggressive score thresholding hurt our tf/idf runs, while it helped improve our binary runs; fixed skew was not as good as the per-topic skew developed by others in the track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Batch Using Linear SVMs with Binary Vectors</head><p>For the submitted run apl10fbsvml we used 200 terms derived on a per-topic basis to create binary term vectors for each document (our implementation actually created a different document vector for each topic). The terms were selected from each topic's positive qrels documents, using mutual-informationlike difference statistics with respect to the August 96 training sample. Given n positive training documents for a topic, we randomly chose n potentially negative examples from the full training index, and threw away any that were actually positive. We created linear SVMs, weighting positive and negative training examples equally (-j 1 flag in SVM-light). J is a cost or skew factor, by which training errors on positive examples outweigh errors on negative examples (see <ref type="bibr" coords="2,382.96,437.44,10.57,8.96" target="#b4">[5]</ref>).</p><p>We then used the score of the test document using the topic SVM to decide whether to return the document. In experiments reported in the literature, SVMs scores are normally thresholded above zero. However, we had observed many training errors close to zero; many negative examples were misclassifed with a small positive score. We thus experimented with setting higher score thresholds. We debated using a small epsilon to threshold the score, but decided to try to find the "best" scores per topic automatically to maximize the 2R+ -N+ measure for the training data. While the overall approach did not work all that well, thresholding did salvage something out of these particular vectors. Unofficial runs using a zero threshold did worse, for both j=1 and j=5 (runs BINLIN skew1 and BINLIN skew5 in Table <ref type="table" coords="2,386.56,638.32,4.98,8.96" target="#tab_2">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>below).</head><p>We do not know why this approach did not succeed. We considered trying different values of j to weight positive and negative examples differently. Perhaps more negative training data or a greater number of terms would improve the technique. Finally, our main intuition is that binary features are probably not appropriate for this Reuters dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Batch Using RBF SVMs with TFIDF Vectors</head><p>For the submitted run apl10fbsvmr we used a reduced term space of 2000 terms to create all the test and training document vectors, based on all the training data. The terms were selected using the top 2000 stems by document frequency in the training set. Stems were produced using a derivative of the SMART stemmer and stopwords were not removed. We created tf/idf weighted vectors for each document and each vector was normalized to unit length. Given n positive training documents for a topic, we randomly chose 4n potentially negative examples from the training index, and threw away any that were actually positive. We then trained radial basis function SVMs (using the -t 2 -g 1 flags in SVMlight), weighting positive and negative training examples equally (-j 1 flag in SVM-light). Using thresholds higher than zero to classify the test documents, as we did with linear kernels, proved to be a big mistake. It hurt performance significantly. Set precision was good, but set recall was terrible.</p><p>We redid this run using the same RBF models with zero as the score threshold (RBF skew1), and are much happier with the results. We also did some runs using weighted RBF models with j=5 (RBF skew5) and similarly tried linear kernels (LIN skew1 and LIN skew5). These post-hoc experiments confirm that SVMs can work well for the batch task, using either radial basis functions or linear separators with tf/idf weighted vectors normalized to unit length.</p><p>We expect there are many per-topic optimizations (such as the leave-one-out cross-validation on training data Dave Lewis used to find optimal j weights per topic <ref type="bibr" coords="3,160.84,470.92,11.31,8.96" target="#b7">[8]</ref>) that could dramatically improve these initial findings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T10SU</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video Retrieval</head><p>The </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T10SU</head><p>For the shot boundary detection task, we experimented with using color histograms, luminance, and the raw image gradient of frames to locate hard cuts and gradual transitions. Hard cuts were identified using an ad hoc global threshold on the color histogram intersection of consecutive frames <ref type="bibr" coords="4,107.80,109.48,15.43,8.96" target="#b15">[16]</ref>. With gradual transitions, possible dissolves and fades were first detected by looking for abrupt changes in the average luminance of frames. These possible gradual transitions were then evaluated by analyzing the change in the image gradient. Each frame was divided into eight-by-eight blocks. If a large percentage of the blocks had changes in the image gradient greater than some threshold, the presence of a dissolve or fade was confirmed. The same technique was used to locate the start and end of each gradual transition. This approach was based on the work of Zabih, Miller, and Mai <ref type="bibr" coords="4,108.16,243.40,15.43,8.96" target="#b16">[17]</ref>, the major differences being that we did not perform motion compensation and used raw image gradients rather than edges. This resulted in a method that was less computationally expensive than the typical edge entering and exiting method. The method did not perform well in the evaluation, but we did not have sufficient time to experiment with different variations and thresholds. The only interaction between the algorithm for detecting hard cuts and those for detecting gradual transitions was the hard cut algorithm taking precedence if a cut and a transition were detected in close proximity. A summary of the results for shot boundary detection is shown in Table <ref type="table" coords="4,136.36,388.48,3.77,8.96" target="#tab_4">5</ref>. Because of limited time and experience, our approach to video retrieval was to treat a video as a series of still images. We made no attempt to exploit the extra information available with video and not with images, such as the audio track and object motion. The experiments we performed focused on using color histograms and image texture features. Each video was first decomposed into shots using the shot boundary detection algorithms described above. The middle frame was used as the key frame to represent all the content of the frames in the shot. This is not a complete representation, but it fit with the emphasis on simplicity for the sake of expediency. In fact, the index files for the 6.3 GB data set comprised only 31 MB altogether, less than 1% the size of the source data. A key frame was described by a vector that contained color and texture features. Similarly, each query was also represented by one or more of these vectors. For a description of the vectors, see Table <ref type="table" coords="4,276.64,706.36,3.77,8.96">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Keyframes</head><p>Dimensions Color features Texture features 7391 272 256 16 Table <ref type="table" coords="4,349.36,78.52,3.77,8.96">6</ref>. Description of video index and vector features When processing queries, any text or audio was completely ignored. If a video example was provided for a query, just the middle frame was extracted as an image example. A weighted distance measure was used for evaluation with the key frames ranked by minimum distance to the set of query examples. The weights were chosen so that the texture and color features made approximately the same contribution to the distance measure even though there were fewer texture measures. The texture features were calculated using a texture descriptor proposed by Manjunath <ref type="bibr" coords="4,379.24,234.76,10.69,8.96" target="#b8">[9]</ref>. It creates a multiresolution decomposition using a Gabor filter bank. We used code available from the Image Processing and Vision Research Lab at the University of California, Santa <ref type="bibr" coords="4,324.04,279.40,50.96,8.96">Barbara [18]</ref> to calculate these features.</p><p>While our results from the known item task were close to the median, the results from the general search were significantly below average. We have not had time to completely investigate this disparity. One explanation for this is that the general information need queries depend more on the text description of the query than on the image or video examples. Since we discarded this information when parsing the query, we were at a disadvantage when trying to retrieve relevant video clips for general searches. The three queries on which we were above the median would support this hypothesis; the text descriptions were short with little information contained in them. "Other shots of city scapes," which is the text description of a query where we were above the median, is a good example. In the known item task, the queries we scored the best on asked about objects that have a strong color component: "Scenes with a yellow boat" or "Other examples of the surface of the planet Mars." This result agrees with the strong emphasis we placed on color in the representation of video data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Arabic Language Retrieval</head><p>The Cross-Language Retrieval task at TREC 2001 consisted of bilingual retrieval of Arabic newspaper articles given either English or French topic statements. Monolingual submissions were also accepted using the manually produced Arabic translations of the topics.</p><p>The apparent necessity of having quality translation resources available for use in a CLIR system has often been expressed. For example, at the first CLEF workshop, Anne Diekema gave a provocative talk, suggesting that CLIR evaluation was essentially just evaluation of translation resources <ref type="bibr" coords="5,226.72,42.64,10.69,8.96" target="#b0">[1]</ref>. We spent several days searching the Web for extant collections of parallel corpora or bilingual dictionaries that would be helpful for translating to Arabic, with no real success. We finally found one newspaper that published mappable, parallel content in both Arabic and English, only to discover that the Arabic stories were available only as images (a practice that stems from the historic lack of standards and software for displaying Arabic text). Downloading that GIF files, OCRing them, and building a parallel collection was beyond our means.</p><p>Unable to discover or acquire significant translation resources, we relied exclusively on two on-line machine translation systems, Ajeeb [20] and Almisbar <ref type="bibr" coords="5,118.96,221.08,15.43,8.96">[21]</ref>. Recently, Kraaij showed how translation probabilities can be incorporated nicely into a language model for cross-language text retrieval, and he demonstrated the efficacy of this combination at the CLEF-2001 workshop <ref type="bibr" coords="5,273.76,265.72,10.69,8.96" target="#b6">[7]</ref>. However, since we simply used machine translation for query translation we did not have access to translation probabilities that are available when dictionaries and corpus-based approaches are used. All of our work was with fully automated retrieval. This was JHU/APL's first experience with Arabic document processing and we learned quite a lot from the experience. We had no personnel who could read Arabic. This however, did not dampen our enthusiasm for the task in the slightest. Over the last several years, our team at APL has participated in multiple CLIR evaluations, where large document collections in Chinese, Dutch, English, French, German, Italian, Japanese, and Spanish were searched <ref type="bibr" coords="5,109.72,444.28,16.76,8.96" target="#b9">[10]</ref> [11] <ref type="bibr" coords="5,148.60,444.28,16.76,8.96" target="#b11">[12]</ref> [13] <ref type="bibr" coords="5,187.24,444.28,15.43,8.96" target="#b13">[14]</ref>. While these higherdensity languages tend to have many resources available for linguistic analysis and automated translation, these languages are diverse, and use numerous character sets and character encodings. Our approach for combating the inherent scalability issues presented by working with numerous languages has been to focus on simple, language-neutral approaches to text processing. Counterintuitively, we have not found that sophisticated, linguistically-rich approaches demonstrate an appreciable performance advantage over the knowledge-light methods we espouse.</p><p>One example of a language-neutral technique is the use of overlapping character n-grams. We have found that n-grams work well in many languages and a pseudo linguistic normalization occurs in agglutinative languages such as Dutch and German <ref type="bibr" coords="5,72.04,656.32,15.43,8.96" target="#b10">[11]</ref>. N-grams are more widely used for retrieval in Asian languages; we recently showed that 3-grams perform on par with 2-grams in unsegmented Japanese text <ref type="bibr" coords="5,127.84,689.80,15.43,8.96" target="#b11">[12]</ref>, which is not the case with Chinese <ref type="bibr" coords="5,72.04,700.96,15.43,8.96" target="#b13">[14]</ref>. Our use of 6-grams for indexing Arabic was not founded on linguistic principles or empirical evidence -we simply guessed that it would be a good choice as it has been in many other alphabetic languages. In retrospect, shorter n-grams have proven to work better with Arabic. In addition to examining the choice of words or n-grams as indexing terms, we experimented with eliminating or replacing certain Arabic characters that did not appear in a list of 28 letters that we had available. Thus we built four different indexes; summary information about each is shown in Table <ref type="table" coords="5,388.36,154.12,3.77,8.96" target="#tab_5">7</ref> Our submissions were produced by combining multiple base runs using different combinations of the topic statement fields, and different methods for morphological normalization, tokenization, query expansion, and translation. One monolingual run, three bilingual runs from English topics, and one cross-language run using the French topics were submitted. For our monolingual Arabic run, apl10ca1, we relied on eight constituent runs</p><p>• 2 query formats: TD and TDN • 2 choices for relevance feedback (yes or no)</p><p>• 2 tokenization alternatives, words and 6grams • 1 normalization approach, character elimination was used Thus, eight different base runs were created, and merged together to produce apl10ca1. See <ref type="bibr" coords="5,507.28,451.60,16.76,8.96" target="#b12">[13]</ref> for details of the merging strategy.</p><p>Apl10ce1, was our first bilingual run using the English topics. We used the exact same approach as apl10ca1, but had two methods for translating the topics:</p><p>• 2 translation systems (Ajeeb and Almisbar) Thus sixteen different base runs were combined to produce the submitted run.</p><p>Our second and third English bilingual runs only made use of the TD topic fields and used either words, or 6-grams as indexing terms. The second run, apl10ce2 used eight base runs:</p><p>• 1 query format: TD • 2 choices for relevance feedback (yes or no)</p><p>• 1 tokenization alternative: 6-grams • 2 normalization approachs, character elimination was used, or not • 2 translation systems (Ajeeb and Almisbar)</p><p>The third English bilingual run, apl10ce3, was just like apl10ce2, except that words were used in place of n-grams.</p><p>Finally, we submitted one run using the French topic statements, apl10cf1. The base runs for this used:</p><p>• 1 query format: TDN • 2 choices for relevance feedback (yes or no)</p><p>• 2 tokenization alternatives: words and 6grams • 1 normalization approach, the character elimination was used • 2 translation systems (Ajeeb and Almisbar) from English to Arabic • 1 translation system for French to English (Systran) Thus, when using the French queries, we first translated to English using the Systran product, and then translated to Arabic using one of the two online systems (Ajeeb/Almisbar). Interestingly, this second layer of translation did not seem to cause much loss in retrieval effectiveness. This may be due to the generally high performance of the Systran English/French module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Official results</head><p>An overview of APL's five official runs for the Arabic track are shown in We note that run apl10ce1 (bilingual English to Arabic) achieved 94.4% of the monolingual baseline observed in apl10ca1. As yet, we are unable to ascertain whether this is do in part to our particular approach to retrieval, or is more a factor of the quality of the machine translation software we relied on.</p><p>Since the conference workshop in November, we have found better bilingual performance using ngrams of length four instead of the longer six-grams. This yielded an improvement in average precision from 0.2891 (apl10ce1) to 0.3350. But our monolingual baseline also improved when 4-grams were used, from 0.3064 (apl10ca1) to 0.3588. Thus, the relative bilingual performance drops insignificantly to 93.4%. Recall-precision graph for APL's official Arabic track automatic submissions We do observe that the use of n-grams accounted for a 17% relative improvement over words in mean average precision (0.2250 vs. 0.1914) as seen in the results for runs apl10ce2 and apl10ce3.. We are still examining the data from all of our many base runs, and do not report on those runs. However, our preliminary analysis finds that character elimination was helpful, but the effect was not extremely large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Web Retrieval</head><p>All of our work for this task was done in essentially one day using an index file previously created for the wt10g collection and used by APL during TREC-9. We submitted four content-only based runs for the ad hoc task, and produced two submissions for the homepage finding task. Our site finding runs were based entirely on query content; we did not use site popularity (backlink frequency) or any graphtheoretic analysis of the hyperlink structure. Our purpose was to see how well a pitifully underinformed approach would compare to the more sophisticated methods we anticipated others would apply to the problem. We indexed documents using unstemmed words; the resulting dictionary contained over three million entries and the index files consumed roughly 3GB of disk space. Each document was processed in the following fashion. First, we ignored HTML tags and used them only to delimit portions of text. Thus no special treatment was given for sectional tags such as &lt;TITLE&gt; or &lt;H1&gt; and both tags and their attribute values were eliminated from the token stream. The text was lowercased, punctuation was removed, and diacritical marks were retained. Tokens containing digits were preserved; however only the first two of a sequence of digits were retained (e.g., 1920 became 19##). The result is a stream of blank-separated words. Queries were parsed in the same fashion as document, except that tried to remove stop structure from the description and narrative sections of the queries using a list of about 1000 phrases constructed from previous TREC topic statements.</p><p>After the query is parsed each term is weighted by the query term frequency and an initial retrieval is performed followed by a single round of relevance feedback. In performing blind relevance feedback we first retrieve the top 1000 documents. We use the top 20 documents for positive feedback and the bottom 75 documents for negative feedback; however duplicate or near-duplicate documents are removed from these sets. We then select 60 terms for the expanded query.</p><p>For the most part we ignored the web-nature of the documents and relied on textual content alone to rank documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Informational Task</head><p>We submitted four runs for this subtask, three runs that simply used the short (Title) portion of the topic statement, and one run that used all parts of the topic (TDN). The four runs were: Results for our official submissions are shown in Table <ref type="table" coords="7,97.84,645.52,3.77,8.96">9</ref>. The submissions that used pseudo relevance feedback (RF) had much higher precision at 10 docs, mean average precision, and recall at 1000 docs. The run using all parts of the topic statement (apl10wd) had the highest performance across the board, including precision at 5 documents. Runs apl10wb and apl10wc, both of which required all query terms (only terms from the topic titles) to be present in returned documents, had about a 50 percent improvement in precision at 5 documents over apl10wa. This is important, because it suggests that when high precision is desirable, not all documents containing any query term need be examined, a practice common to many web search engines today (instead, the smaller set of documents that contain all of the query terms could be scored). Also, while apl10wc had high performance at higher recall levels than did apl10wb, this was not really true at high precision. This lends support for the practice of not using relevance feedback when only few relevant documents are needed to satisfy a user's need.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Navigational Task</head><p>We submitted just two runs for this subtask, and decided to see how well a purely content-based ranking would perform. As in the informational task, we compared performance between runs where all of the query terms were required to be present in relevant documents. We simply ordered our ranked list of hyperlinks using the similarity scores from the retrieval process. As was mentioned earlier, no use of document popularity or hyperlink structure was attempted. The two runs we submitted were:</p><p>• apl10ha: all terms required, no relevance feedback • apl10hb: all terms not compulsory, no blind relevance feedback used MRR % top 10 % failure apl10ha 0.238 44.8% 22.1% apl10hb 0.220 42.8% 21.4% Table <ref type="table" coords="7,349.36,441.52,8.38,8.96" target="#tab_0">10</ref>. Performance of APL Official TREC-2001 Web submissions (site finding task) On the officially reported measures, mean reciprocal rank, percent of topics with a correct entry page found in the top 10 documents, and the failure percentage (when none was found in the top 100 docs), these two runs were virtually identical. The mean reciprocal rank is just slightly higher for apl10ha, in which all query terms were required to be on the given page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>This year we participated in three tracks that each presented new challenges: filtering, video, and Arabic.</p><p>We investigated the use of Support Vector Machines (SVMs) for batch text classification and noticed a large sensitivity to parameter settings for these classifiers. We also found that we were able to choose reasonable score thresholds for the routing task when using a language model for estimating document relevance.</p><p>Due to a lack of experience with multimedia retrieval (e.g., we had never previously participated in the TREC Spoken Document Retrieval task), the video track was a significant challenge for us. We placed an emphasis on simple techniques to quickly create a retrieval system while planning to add more advanced components such as speech recognition in the future. From our initial analysis, there was a correlation between how we parsed queries and our performance on different types of queries Arabic retrieval was especially interesting for our team, which had no personnel who could read Arabic. The lack of available translation resources left us with little alternative but to use weak machine translation systems; yet, we found bilingual performance rivaled a good monolingual baseline in terms of mean average precision (94%), had equal performance at high precision levels (such as measured in precision at 5 or 10 documents), and even achieved higher recall at 1000. Our results emphasizing language-neutral techniques indicate that excellent performance is attainable without sophisticated linguistic processing.</p><p>While we did not put significant effort into the Web track this year, we did attempt to improve our retrieval performance at high precision levels (in contrast to our previous work attempting to maximize mean average precision). We found support for several techniques currently used in the commercial sector that improve query processing efficiency without impacting high precision performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,324.04,277.12,36.21,8.96;6,378.04,277.12,162.03,8.96;6,324.04,288.28,174.63,8.96"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1.Recall-precision graph for APL's official Arabic track automatic submissions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,72.04,413.54,206.58,69.95"><head>Table 1 .</head><label>1</label><figDesc>APL Routing Results</figDesc><table coords="2,82.00,413.54,196.62,58.30"><row><cell></cell><cell>Avg.</cell><cell cols="2"># bests # ≥ median</cell></row><row><cell></cell><cell>prec.</cell><cell></cell><cell>(84 topics)</cell></row><row><cell>apl10frn</cell><cell>0.121</cell><cell>4</cell><cell>70</cell></row><row><cell>apl10frs</cell><cell>0.104</cell><cell>4</cell><cell>56</cell></row><row><cell>apl10frw</cell><cell>0.113</cell><cell></cell><cell>unofficial run</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,72.04,43.00,480.37,626.84"><head>Table 2 .</head><label>2</label><figDesc>Official Batch Submissions.</figDesc><table coords="3,72.04,520.24,225.97,149.60"><row><cell></cell><cell></cell><cell cols="2">Fbeta SetPrec SetRecall</cell></row><row><cell cols="2">apl10fbsvml 0.115</cell><cell>0.292 0.303</cell><cell>0.627</cell></row><row><cell cols="2">apl10fbsvmr 0.081</cell><cell>0.154 0.380</cell><cell>0.054</cell></row><row><cell></cell><cell cols="3">T10SU Fbeta SetPrec SetRecall</cell></row><row><cell>RBF skew1</cell><cell>0.283</cell><cell>0.459 0.546</cell><cell>0.437</cell></row><row><cell>RBF skew5</cell><cell>0.254</cell><cell>0.430 0.442</cell><cell>0.525</cell></row><row><cell>LIN skew1</cell><cell>0.234</cell><cell>0.413 0.400</cell><cell>0.601</cell></row><row><cell>LIN skew5</cell><cell>0.157</cell><cell>0.341 0.318</cell><cell>0.689</cell></row><row><cell cols="4">Table 3. Unofficial (post hoc) batch runs, unified</cell></row><row><cell cols="2">tf/idf weighted term space.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,324.04,113.08,216.03,407.72"><head>Table 4 .</head><label>4</label><figDesc>Unofficial (post hoc) batch runs, per-topic binary term space.</figDesc><table coords="3,324.04,147.88,216.03,112.53"><row><cell>Summary of Batch Filtering Results</cell></row><row><cell>Chart 1 summarizes the results of our batch filtering</cell></row><row><cell>experiments. SVMs with RBF kernels on TFIDF</cell></row><row><cell>vectors and no thresholding works well, and could</cell></row><row><cell>have performed above median compared to other</cell></row><row><cell>official batch results. Thresholding above zero hurt</cell></row><row><cell>for RBF SVMs on TFIDF vectors (RBF skew1 vs.</cell></row><row><cell>apl10fbsvmr). However thresholding improved a</cell></row><row><cell>poor baseline result of linear SVMs on binary vectors</cell></row><row><cell>(apl10fbsvml vs. BINLIN skew1).</cell></row></table><note coords="3,324.04,500.68,216.03,8.96;3,324.04,511.84,210.69,8.96"><p>Chart 1. SVMs with RBF kernels on TFIDF vectors work well for batch filtering with the T10SU metric.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,72.04,409.34,208.93,82.79"><head>Table 5 .</head><label>5</label><figDesc>Shot boundary detection results</figDesc><table coords="4,72.04,409.34,208.93,71.03"><row><cell></cell><cell>Total</cell><cell># ≥ median</cell><cell># best</cell></row><row><cell></cell><cell>videos</cell><cell></cell><cell></cell></row><row><cell>Cuts-prec</cell><cell>15</cell><cell>12</cell><cell>8</cell></row><row><cell>Cuts-recall</cell><cell>15</cell><cell>4</cell><cell>1</cell></row><row><cell>Graduals-prec</cell><cell>17</cell><cell>0</cell><cell>0</cell></row><row><cell>Graduals-recall</cell><cell>17</cell><cell>4</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="5,324.04,154.12,215.99,101.24"><head>Table 7 .</head><label>7</label><figDesc>. Index statistics for the 869 MB, 384K article TREC-2001 Arabic collection.</figDesc><table coords="5,347.44,176.92,169.18,55.64"><row><cell></cell><cell># terms</cell><cell>index size</cell></row><row><cell>words</cell><cell cols="2">571,798 372 MB</cell></row><row><cell>words -morph</cell><cell cols="2">539,979 351 MB</cell></row><row><cell>6-grams</cell><cell cols="2">6,784,129 2513 MB</cell></row><row><cell cols="3">6-grams -morph 6,081,618 2427 MB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="6,70.24,365.68,219.02,102.32"><head>Table 8</head><label>8</label><figDesc></figDesc><table coords="6,70.24,365.68,219.02,102.32"><row><cell></cell><cell></cell><cell>below.</cell><cell></cell></row><row><cell>MAP Recall</cell><cell>#</cell><cell># ≥</cell><cell>%</cell></row><row><cell>(4122)</cell><cell>best</cell><cell>median</cell><cell>mono</cell></row><row><cell>apl10ca1 0.3064 2669</cell><cell>3</cell><cell>17</cell><cell>100 %</cell></row><row><cell>apl10ce1 0.2891 2819</cell><cell>1</cell><cell>22</cell><cell>94.4</cell></row><row><cell>apl10ce2 0.2250 2593</cell><cell>0</cell><cell>16</cell><cell>73.4</cell></row><row><cell>apl10ce3 0.1914 2350</cell><cell>0</cell><cell>15</cell><cell>62.5</cell></row><row><cell>apl10cf1 0.2415 2574</cell><cell>0</cell><cell>20</cell><cell>78.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="6,72.04,470.68,214.16,8.96"><head>Table 8 .</head><label>8</label><figDesc>Official results for Arabic runs (25 topics)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="6,338.89,53.76,202.27,210.50"><head>TREC-2001 Official Arabic Results</head><label></label><figDesc></figDesc><table coords="6,338.89,81.13,202.27,183.13"><row><cell></cell><cell>1.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Precision</cell><cell>0.40 0.50 0.60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Recall Level</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">apl10ca1 (0.3064)</cell><cell></cell><cell cols="2">apl10ce1(0.2891)</cell><cell></cell><cell cols="2">apl10cf1 (0.2415)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">apl10ce2 (0.2250)</cell><cell></cell><cell cols="2">apl10ce3 (0.1914)</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,90.04,475.48,197.98,8.96;8,72.04,486.64,215.94,8.96;8,72.04,497.80,215.98,9.08;8,72.04,508.96,216.03,8.96;8,72.04,520.00,215.97,9.08;8,72.04,531.16,215.97,8.96;8,72.04,542.32,76.41,8.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,223.64,475.48,64.38,8.96;8,72.04,486.64,215.94,8.96;8,72.04,497.80,43.53,8.96">Cross-Language Information Retrieval Using Dutch Query Translation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Diekema</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W-Y</forename><surname>Hsiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,221.32,497.92,66.70,8.96;8,72.04,508.96,216.03,8.96;8,72.04,520.12,129.23,8.96">Cross-Language Information Retrieval and Evaluation: Proceedings of the CLEF-2000 Workshop</title>
		<title level="s" coord="8,72.04,531.16,148.54,8.96">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2069</biblScope>
			<biblScope unit="page" from="230" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.04,564.64,197.97,8.96;8,72.04,575.80,216.03,8.96;8,72.04,586.96,215.94,8.96;8,72.04,598.12,215.94,8.96;8,72.04,609.28,180.81,8.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,76.27,575.80,211.80,8.96;8,72.04,586.96,96.39,8.96">Inductive Learning Algorithms and Representations for Text Categorization</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sahami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,193.00,586.96,94.98,8.96;8,72.04,598.12,215.94,8.96;8,72.04,609.28,144.41,8.96">Proceedings of the 7th International Conference on Information and Knowledge Management (CIKM 98</title>
		<meeting>the 7th International Conference on Information and Knowledge Management (CIKM 98</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.04,631.60,198.01,8.96;8,72.04,642.76,215.97,8.96;8,72.04,653.92,75.81,8.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,173.92,631.60,114.13,8.96;8,72.04,642.76,100.85,8.96">Trends and controversies: Support vector machines</title>
		<author>
			<persName coords=""><forename type="first">Marti</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,181.48,642.76,102.06,8.96">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="28" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.04,676.24,197.99,8.96;8,72.04,687.40,215.98,8.96;8,72.04,698.56,216.01,8.96;8,72.04,709.72,113.73,8.96" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="8,219.21,676.24,68.82,8.96;8,72.04,687.40,215.98,8.96;8,72.04,698.56,107.79,8.96">Relating the new language models of information retrieval to the traditional retrieval models</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>De Vries</surname></persName>
		</author>
		<idno>TR-CTIT-00-09</idno>
		<imprint>
			<date type="published" when="2000-05">May 2000</date>
		</imprint>
	</monogr>
	<note type="report_type">CTIT Technical Report</note>
</biblStruct>

<biblStruct coords="8,342.04,53.68,197.94,8.96;8,324.04,64.84,216.01,8.96;8,324.04,76.00,215.97,8.96;8,324.04,87.16,121.41,8.96" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="8,396.64,53.68,143.34,8.96;8,324.04,64.84,216.01,8.96;8,324.04,76.00,64.67,8.96">Making large-Scale SVM Learning Practical Advances in Kernel Methods -Support Vector Learning</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<editor>B. Schölkopf and C. Burges and A. Smola</editor>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,342.04,109.48,198.01,8.96;8,324.04,120.64,216.01,8.96;8,324.04,131.80,215.94,9.08;8,324.04,142.96,148.65,9.08" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,403.84,109.48,136.21,8.96;8,324.04,120.64,216.01,8.96;8,324.04,131.80,30.90,8.96">Text categorization with support vector machines: learning with many relevant features</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,381.88,131.92,158.10,8.96;8,324.04,143.08,116.70,8.96">Proc. 10th European Conference on Machine Learning ECML-98</title>
		<meeting>10th European Conference on Machine Learning ECML-98</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,342.04,165.28,197.96,8.96;8,324.04,176.44,215.94,8.96;8,324.04,187.60,215.97,8.96;8,324.04,198.76,66.33,8.96" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="8,395.29,165.28,144.71,8.96;8,324.04,176.44,215.94,8.96;8,324.04,187.60,116.53,8.96">TNO at CLEF-2001&apos;. In Results of the CLEF-2001 Cross-Language System Evaluation Campaign (Working Notes)</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="29" to="40" />
			<pubPlace>Darmstadt, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,342.04,221.08,198.04,8.96;8,324.04,232.24,160.53,8.96" xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Dave</forename><surname>Lewis</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>personal communication, TREC 2001 Batch Filtering Task Experiments</note>
</biblStruct>

<biblStruct coords="8,342.04,254.56,197.99,8.96;8,324.04,265.72,215.94,8.96;8,324.04,276.88,215.98,8.96;8,324.04,288.04,208.17,8.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,534.75,254.56,5.28,8.96;8,324.04,265.72,215.94,8.96;8,324.04,276.88,34.72,8.96">A Texture Descriptor for Browsing and Similarity Retrieval</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,374.32,276.88,165.70,8.96;8,324.04,288.04,62.26,8.96">Journal of Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="43" />
			<date type="published" when="2000-09">September 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,342.04,310.36,197.98,8.96;8,324.04,321.52,215.97,8.96;8,324.04,332.68,215.98,9.08;8,324.04,343.84,215.97,9.08;8,324.04,355.00,61.29,8.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,525.81,310.36,14.21,8.96;8,324.04,321.52,166.08,8.96">The JHU/APL HAIRCUT System at TREC-8</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Piatko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,464.92,332.80,75.10,8.96;8,324.04,343.96,187.67,8.96">Proceedings of the Eighth Text REtrieval Conference (TREC-8)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Eighth Text REtrieval Conference (TREC-8)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="445" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,342.04,377.32,197.99,8.96;8,324.04,388.48,216.01,8.96;8,324.04,399.64,215.98,9.08;8,324.04,410.80,216.03,8.96;8,324.04,421.84,215.97,9.08;8,324.04,433.00,215.97,8.96;8,324.04,444.16,76.41,8.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,534.75,377.32,5.28,8.96;8,324.04,388.48,216.01,8.96;8,324.04,399.64,34.72,8.96">A Language-Independent Approach to European Text Retrieval</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Piatko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,473.32,399.76,66.70,8.96;8,324.04,410.80,216.03,8.96;8,324.04,421.96,129.23,8.96">Cross-Language Information Retrieval and Evaluation: Proceedings of the CLEF-2000 Workshop</title>
		<title level="s" coord="8,324.04,433.00,148.54,8.96">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2069</biblScope>
			<biblScope unit="page" from="129" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,342.04,466.48,197.96,8.96;8,324.04,477.64,215.94,8.96;8,324.04,488.80,216.04,8.96;8,324.04,499.96,69.45,8.96" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,408.55,466.48,131.44,8.96;8,324.04,477.64,215.94,8.96;8,324.04,488.80,37.23,8.96">Experiments in the retrieval of unsegmented Japanese text at the NTCIR-2 workshop</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,381.64,488.80,158.44,8.96;8,324.04,499.96,39.33,8.96">Proceedings of the 2nd NTCIR Workshop</title>
		<meeting>the 2nd NTCIR Workshop</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,342.04,522.28,197.96,8.96;8,324.04,533.44,216.03,8.96;8,324.04,544.60,215.96,8.96;8,324.04,555.76,215.94,8.96;8,324.04,566.92,215.97,8.96;8,324.04,578.08,22.65,8.96" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,501.29,522.28,38.71,8.96;8,324.04,533.44,216.03,8.96;8,324.04,544.60,99.02,8.96">JHU/APL experiments at CLEF 2001: Translation resources and score normalization</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,448.96,544.60,91.04,8.96;8,324.04,555.76,215.94,8.96;8,324.04,566.92,67.09,8.96">Results of the CLEF-2001 Cross-Language System Evaluation Campaign (Working Notes)</title>
		<meeting><address><addrLine>Darmstadt, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="121" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,342.04,600.40,197.97,8.96;8,324.04,611.56,215.97,8.96;8,324.04,622.72,216.01,8.96;8,324.04,633.88,157.77,8.96" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,330.13,611.56,91.81,8.96">HAIRCUT at TREC-9</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Piatko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,397.48,622.72,142.57,8.96;8,324.04,633.88,127.79,8.96">Proceedings of the Ninth Text REtrieval Conference (TREC-9)</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Ninth Text REtrieval Conference (TREC-9)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,342.04,656.20,197.99,8.96;8,324.04,667.36,216.01,8.96;8,324.04,678.52,156.30,8.96;8,480.40,676.83,5.04,4.54;8,488.80,678.52,51.25,8.96;8,324.04,689.68,215.94,8.96;9,72.04,42.64,215.97,8.96;9,72.04,53.68,54.09,8.96" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,534.75,656.20,5.28,8.96;8,324.04,667.36,216.01,8.96;8,324.04,678.52,26.43,8.96">A Hidden Markov Model Information Retrieval System</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">R H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,374.68,678.52,105.66,8.96;8,480.40,676.83,5.04,4.54;8,488.80,678.52,51.25,8.96;8,324.04,689.68,215.94,8.96;9,72.04,42.64,142.54,8.96">the Proceedings of the 22 nd International Conference on Research and Development in Information Retrieval (SIGIR-99)</title>
		<imprint>
			<date type="published" when="1999-08">August 1999</date>
			<biblScope unit="page" from="214" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.04,76.00,197.96,8.96;9,72.04,87.16,215.97,8.96;9,72.04,98.32,51.21,8.96" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,214.25,76.00,65.43,8.96">Colour Indexing</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,72.04,87.16,171.49,8.96">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="32" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.04,120.64,197.94,8.96;9,72.04,131.80,215.98,8.96;9,72.04,142.96,215.94,8.96;9,72.04,154.12,74.37,8.96" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="9,232.77,120.64,55.21,8.96;9,72.04,131.80,215.98,8.96;9,72.04,142.96,25.17,8.96">Feature-based Algorithms for Detecting and Classifying Scene Breaks</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,109.60,142.96,178.38,8.96;9,72.04,154.12,44.70,8.96">Proceedings of the 4th ACM Int. Conf. on Multimedia</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.04,286.96,197.97,8.96;9,72.04,298.12,215.94,8.96;9,72.04,309.28,215.94,8.96;9,72.04,320.44,216.03,8.96;9,72.04,331.60,121.17,8.96" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="9,210.28,286.96,77.73,8.96;9,72.04,298.12,215.94,8.96;9,72.04,309.28,215.94,8.96;9,72.04,320.44,63.62,8.96">English Language, 1996-08-20 to 1997-08-19. We gratefully acknowledge the provision of the research corpus by Reuters Limited</title>
		<imprint>
			<publisher>Reuters Corpus</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>without it, our filtering experiments would not have been possible</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
