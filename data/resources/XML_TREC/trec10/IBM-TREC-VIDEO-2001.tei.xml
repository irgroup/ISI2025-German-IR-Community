<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,100.73,113.75,424.74,15.49;1,281.97,135.66,62.27,15.49">Integrating Features, Models, and Semantics for TREC Video Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,119.28,168.15,67.18,10.76"><forename type="first">John</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
						</author>
						<author>
							<persName coords="1,198.67,168.15,87.85,10.76"><forename type="first">Savitha</forename><surname>Srinivasan</surname></persName>
						</author>
						<author>
							<persName coords="1,298.45,168.15,57.50,10.76"><forename type="first">Arnon</forename><surname>Amir</surname></persName>
						</author>
						<author>
							<persName coords="1,368.51,168.15,59.20,10.76"><forename type="first">Sankar</forename><surname>Basu</surname></persName>
						</author>
						<author>
							<persName coords="1,439.90,168.15,58.06,10.76"><forename type="first">Giri</forename><surname>Iyengar</surname></persName>
						</author>
						<author>
							<persName coords="1,146.07,182.10,78.04,10.76"><forename type="first">Ching-Yung</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName coords="1,235.40,182.10,76.18,10.76"><forename type="first">Milind</forename><surname>Naphade</surname></persName>
						</author>
						<author>
							<persName coords="1,324.73,182.10,79.96,10.76"><forename type="first">Dulce</forename><surname>Ponceleon</surname></persName>
						</author>
						<author>
							<persName coords="1,417.36,182.10,57.82,10.76"><forename type="first">Belle</forename><surname>Tseng</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">†IBM T. J. Watson Research Center</orgName>
								<address>
									<addrLine>30 Saw Mill River Road</addrLine>
									<postCode>10532</postCode>
									<settlement>Hawthorne</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">‡IBM Almaden Research Center</orgName>
								<address>
									<addrLine>650 Harry Road</addrLine>
									<postCode>95120</postCode>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,100.73,113.75,424.74,15.49;1,281.97,135.66,62.27,15.49">Integrating Features, Models, and Semantics for TREC Video Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4E0610A05C8527FFC91DE95BA368A19D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe a system for automatic and interactive content-based retrieval of video that integrates features, models, and semantics. The novelty of the approach lies in the (1) semi-automatic construction of models of scenes, events, and objects from feature descriptors, and (2) integration of content-based and model-based querying in the search process. We describe several approaches for integration including iterative filtering, score aggregation, and relevance feedback searching. We describe our effort of applying the content-based retrieval system to the TREC video retrieval benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The growing amounts of digital video are driving the need for more effective methods for storing, searching, and retrieving video based on its content. Recent advances in content analysis, automatic feature extraction, and classification are improving capabilities for effectively searching and filtering digital video using information based on perceptual features, content structure, models, and semantics. The emerging MPEG-7 multimedia content description standard promises to further improve content-based searching by providing a rich set of standardized tools for describing multimedia content in XML <ref type="bibr" coords="1,202.67,568.89,25.86,8.97" target="#b11">[SS01]</ref>. However, MPEG-7 does not standardize methods for extracting descriptions nor for matching and searching. The extraction and use of MPEG-7 descriptions remains a challenge for future research, innovation, and industry competition <ref type="bibr" coords="1,251.75,616.70,30.76,8.97" target="#b10">[Smi01]</ref>.</p><p>In this paper, we describe a system for automatic and interactive content-based retrieval that integrates features, models, and semantics [SBL + 01]. The system analyzes the video by segmenting it into shots, selecting key-frames, and extracting audio-visual descriptors from the shots. This allows the video to be searched at the shot-level using content-based retrieval approaches. However, we further analyze the video by developing and applying models for classifying content. The approach requires the manual-or semi-automatic annotation of the video shots to provide training data. The models are subsequently used to automatically assign semantic labels to the video shots. In order to apply a small number of models but have at the same time to have large impact on classifying the video shots, we have primarily investigated models that apply broadly to video content, such as indoor vs. outdoor, nature vs. man-made, face detection, sky, land, water, and greenery. However, we have also investigated several specific models including airplanes, rockets, fire, and boats. While the models allow the video content to be annotated automatically using this small vocabulary, the integration of the different search methods together (content-based and model-based) allows more effective retrieval.</p><p>In the paper, we describe the approach for integrating features, models, and semantics in a system for contentbased retrieval of video. We have applied these systems and methods to the NIST TREC video retrieval benchmark, which consists of 74 queries of a video corpus containing approximately 11 hours of video. The queries, which were designed to access video based on semantic contents, permit automatic and/or interactive approaches for retrieving the results. We enhance the automatic retrieval by using the models in conjunction with the features to match the query content with the target video shots. For interactive retrieval, we allow the user to apply several methods of iterative searching that combines features, semantics, and models using different filtering operations and weighting methods. In this paper, we describe more details about the approach and discuss results for the TREC video retrieval benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Content analysis system</head><p>The video content is analyzed through several processes that involve shot detection, feature extraction, and classification, as shown in Figure <ref type="figure" coords="1,398.74,688.56,3.74,8.97">1</ref>. The video is segmented temporally according to shot boundaries, and descriptors are extracted for each shot. The descriptors are ingested into a storage system. The descriptors are used as input into the model-based classification system which assigns semantic labels to each shot. The system also ingests any meta-data related to the content such as title, format, source, and so forth. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPEG-7 Annotation</head><p>Figure <ref type="figure" coords="2,100.41,297.81,3.88,8.97">1</ref>: The video content ingestion engine first segments the video temporally using shot detection and selects keyframes, then extracts descriptors of the audio-visual features and applies models in order to classify the content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Shot detection</head><p>The video content is pre-processed by splitting it into temporal segments using the IBM CueVideo (program cuts.exe with the default settings) <ref type="bibr" coords="2,221.92,419.99,20.98,8.97">[Cue]</ref>. After the shots are detected, key-frames are selected and extracted, and all MPEG I-frames are extracted, as shown in Figure <ref type="figure" coords="2,273.35,443.91,3.74,8.97" target="#fig_0">2</ref>. These images are stored and indexed and are used for accessing the shots. CueVideo uses sampled three dimensional color histograms in RGB color space to compare pairs of frames. Histograms of recent frames are stored in a buffer to allow a comparison between multiple frames. Frame pairs at one, three and seven frames apart and their corresponding thresholds are shown by the three upper graphs in Figure <ref type="figure" coords="2,546.72,146.60,3.74,8.97">3</ref>. Statistics of frame differences are computed in a moving window around the processed frame and are used to compute the adaptive thresholds. Hence the program does not require sensitivity-tuning parameters. A state machine is used to detect and classify the different shot boundaries, shown at the botom of Figure <ref type="figure" coords="2,549.22,493.86,4.98,8.97">3</ref> with all thirteen states listed. At each frame a state transition is made from the current state to the next state, and any required operation is taken (e.g., report a shot, save a key-frame to file). The algorithm classifies shot boundaries into Cuts, Fade-in, Fade-out, Dissolve and Other. It works in a single pass, is robust to possibly uncompliant MPEG streams, and runs about 2X real time on a 800MHz P-III.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feature extraction</head><p>The system extracts several different descriptors for each of the key-frames and i-frames. We have used the following descriptors:</p><p>1. color histogram (166-bin HSV color-space), 2. grid-based color histogram (4x4 grid of the HSV histogram), 3. texture spatial-frequency energy (variance measure of each of 12 bands of quadrature mirror filter wavelet decomposition, and 4. edge histogram (using Sobel filter and quantization to 8 angles and 8 magnitudes).</p><p>Each of these descriptors is stored and indexed separately. However, at retrieval time, the CBR matching function allows the descriptor values to be combined using an arbitrary weighting function in order to determine the similarity of the query and target images based on multiple features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Semi-automatic annotation</head><p>In order to allow a model-based approach to video retrieval, ground-truth data is needed for training the models. In order to create training data, we developed a video annotation tool that allows the users to annotate each shot in the video sequence, as shown in Figure <ref type="figure" coords="3,191.49,293.95,3.74,8.97">4</ref>. The tool allows the user to identify and label scenes, events, and object by applying the labels at the shot-level. The tool also allows the user to associate object-labels with individual regions in a key-frame.</p><p>Figure <ref type="figure" coords="3,100.10,520.71,3.88,8.97">4</ref>: The video annotation tool allows users to label the events, scenes, and objects in the video shots.</p><p>For annotating video content, we created a lexicon for describing events, scenes, and objects; the following excerpt gives some of the annotation terms:</p><p>• Events: water skiing, boat sailing, person speaking, landing, take-off/launch, and explosion;</p><p>• Scenes: outer space (moon, mars), indoors (classroom, meeting room, laboratory, factory), outdoors (nature, sky, clouds, water, snow, greenery, rocks, land, mountain, beach, field, forest, canyon, desert, waterfall), and man-made (road, cityscape);</p><p>• Objects: non-rigid objects (animal, deer, bird, duck, human), rigid objects (man-made structure, building, dam, statue, tree, flower), transportation (rocket, space shuttle, vehicle, car, truck, rover, tractor), and astronomy.</p><p>The video anntotation tool allows the user to process the video shot-by-shot, and assign the labels to each shot. The tool is semi-automatic in that it automatically propagates labels to "similar" shots as described in [NLS + 02]. The system requires the user to confirm or reject the propagated labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Content modeling</head><p>The content modeling system uses the labeled training video content to classify other video content (in our case, the test TREC video corpus). We have investigated several different types of static models including Bayes nets, multinets <ref type="bibr" coords="3,337.93,273.83,42.06,8.97" target="#b4">[NKHR00]</ref>, and Gaussian mixture models. In some cases, we have used additional descriptors in the models, which are not applied for content-based retrieval, such as motion activity and color moments.</p><p>We have developed statistical models for the following concepts:</p><p>• Events: fire, smoke, launch;</p><p>• Scenes: greenery, land, outdoors, rock, sand, sky, water;</p><p>• Objects: airplane, boat, rocket, vehicle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Statistical modeling</head><p>In the statistical modeling approach, the descriptors extracted from the video content are modeled by a multidimensional random variable X. The descriptors are assumed to be independent identically distributed random variables drawn from known probability distributions with unknown deterministic parameters. For the purpose of classification, we assume that the unknown parameters are distinct under different hypotheses and can be estimated. In particular, each semantic concept is represented by a binary random variable. The two hypotheses associated with each such variable are denoted by H i , i ∈ {0, 1}, where 0 denotes absence and 1 denotes presence of the concept. Under each hypothesis, we assume that the descriptor values are generated by the conditional probability density function P i (X), i ∈ {0, 1}.</p><p>In case of scenes, we use static descriptors that represent the features of each key-frame. In case of events, which have temporal characteristics, we construct temporal descriptors using time series of static descriptors over the multiple video frames. We use a one-zero loss function <ref type="bibr" coords="3,522.09,680.38,32.10,8.97" target="#b6">[Poo99]</ref> to penalize incorrect detection. This is shown in Equation <ref type="formula" coords="3,318.08,704.29,3.88,8.97">1</ref>:</p><formula xml:id="formula_0" coords="3,375.13,712.20,179.05,22.58">λ(α i |ω j ) = 0 if i = j 1 otherwise (1)</formula><p>The risk corresponding to this loss function is equal to the average probability of error and the conditional risk with action α i is 1 -P (ω i |x). To minimize the average probability of error, class ω i must be chosen, which corresponds to the maximum a posteriori probability P (ω i |x). This corresponds to the minimum probability of error (MPE) rule.</p><p>In the special case of binary classification, the MPE rule can be expressed as deciding in favor of ω 1 if</p><formula xml:id="formula_1" coords="4,128.96,191.44,179.16,24.93">p(x|ω 1 ) p(x|ω 2 ) &gt; (λ 12 -λ 22 )P (ω 2 ) (λ 21 -λ 11 )P (ω 1 )<label>(2)</label></formula><p>The term p(x|ω j ) is the likelihood of ω j and the test based on the ratio in Equation ( <ref type="formula" coords="4,171.34,240.88,3.87,8.97" target="#formula_1">2</ref>) is called the likelihood ratio test (LRT) <ref type="bibr" coords="4,99.35,252.84,30.16,8.97" target="#b1">[DH73,</ref><ref type="bibr" coords="4,132.00,252.84,26.81,8.97" target="#b6">Poo99]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Parameter estimation</head><p>For modeling the TREC video content, we assume that the conditional distributions over the descriptors X under the two hypotheses -concept present (H 1 ) and concept absent (H 0 ) -have been generated by distinct mixtures of diagonal Gaussians. The modeling of these semantic concepts involves the estimation of the unknown but determinsitic parameters of these Gaussian mixture models (GMMs) using the set of annotated examples in the training set. For this purpose the descriptors associated with training data corresponding to each label are modeled by a mixture of five gaussians. The parameters (mean, covariance, and mixture weights) are estimated by using the Expectation Maximization (EM) <ref type="bibr" coords="4,114.05,446.46,36.52,8.97" target="#b2">[DLR77]</ref> algorithm.</p><p>The rest of the training data is used to build a negative model for each label in a similar way, which corresponds to a garbage model for that label. The LRT is used in each test case to determine which of the two hypotheses is more likely to account for the descriptor values. The likelihood ratio can also be looked upon as a measure of the confidence of classifying a test image to the labeled class under consideration. A ranked list of confidence measures for each of the labels can be produced by repeating this procedure for all the labels under consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3">Region merging</head><p>We use manually assigned bounding boxes encompassing regions of interest obtained during annotations for extracting features. The testing is also done at the regional bounding box level. To fuse decisions from several bounding boxes in a key-frame, we use the following hypothesis: If a concept is to be declared absent in a frame, it must be absent in each and every bounding box tested. We can then compute the product of the probability of the "concept absent" hypothesis to obtain the probability of the concept being absent in the frame. Alternately, we can also use the maximum possible probability of the concept being detected in any region as the probability of its occurrence in the image/frame. For concepts which are global in terms of feature support, this step is not needed. Localized or regional concepts include rocket, face, sky, and so forth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.4">Feature fusion</head><p>The objective of feature fusion is to combine multiple statistical models for the different video features. Separate GMM models are used for each of the different descriptors (e.g., color histogram, edge direction histogram, texture, and so forth). This results in separate classifications and associated confidence for each test image depending on the descriptor. While the classifiers can be combined in a many ways, we explored straightforward methods such as taking sum, maximum,and product of the individual confidences for each descriptor in computing an overall classification confidence.</p><p>While this strategy of "late feature fusion" is fairly simple, one can envision other "early feature fusion" methods such as concatenating different descriptors into a single vector and then building a single GMM. We did not pursue this strategy due to the large dimensionality of the descriptors, especially in view of the paucity of training video content depicting the concepts of interest. However, it may be possible to consider discrimination in reduced dimensional subspaces of the feature space by using techniques such as the principal component analysis (PCA) or by using more sophisticated dimensionality reduction techniques that would allow concatenation and modeling of high-dimensional descriptors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.5">Training</head><p>The performance of statistical models such as the GMM depend to a large extent on the amount of training data. Due to the relatively small amount of labeled training video data beyond the TREC video corpus, we adopted a "leave one clip out strategy." This means that we trained a model for each concept as many number of times as the number of video clips. During each such training, one clip was left out from the training set. The models for the two hypotheses thus trained were used to detect the semantic concept in the clip that was left out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Speech indexing</head><p>In addition to automatic analysis and modeling of the features of the video content, we also investigated the use of speech indexing as an alternative approach for video retrieval <ref type="bibr" coords="4,346.73,700.52,25.86,8.97" target="#b7">[PS01]</ref>. We used the IBM ViaVoice speech recognition engine to transcribe the audio and generate a continuous stream of words. We define a unit-document to be a 100 word temporal segment where consecutive segments overlap partially in order to address the boundary truncation effect. There are several operations performed in sequence in this processing.</p><p>First, the words and times from the recognizer output are extracted to create the unit-document files with associated timestamps. The Julian time at the start of the audio is used as the reference basis. This is followed by tokenization to detect sentence/phrase boundaries and then part-of-speech tagging such as noun phrase, plural noun etc. The morphological analysis uses the part-of-speech tag and a morph dictionary to reduce each word to its morph. For example, the verbs, lands, landing and land will all be reduced to land. Then, the stop words are removed using a standard stopwords list. For each of the remaining words, the number of unit-documents that it belongs to (the inverse document frequency) is computed and is used to weight these word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Video retrieval</head><p>Once the video content is ingested, the descriptors and model results are stored and indexed. This allows the user to carry out the searches in a video query pipeline process as shown in Figure <ref type="figure" coords="5,172.29,380.73,3.74,8.97" target="#fig_3">6</ref>, in which queries are processed in a multi-stage search in which the user selects models and clusters or examples of video content at each stage. By operating on the interim results, the user controls the query refinement. As shown in Figure <ref type="figure" coords="5,232.81,428.55,3.74,8.97" target="#fig_3">6</ref>, at each stage of the search, a query Q i produces a result list R i . The result list R i is then used as input into a subsequent query Q i+1 , and through various selectable operations for combining and scoring R i with the matches for Q i+1 , the result list R i+1 is produced. The user can continue this iterative search process until the desired video content is retrieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Content-based retrieval</head><p>Content-based retrieval is the most amenable to automatic retrieval in the case that the query provides example content. For TREC video retrieval, each of the queries provided example content which included anywhere from a single image to several video clips. For automatic contentbased retrieval, the following approach was adopted: the query content was analyzed using shot detection, key-frame selection, and feature extraction to produce a set of descriptors of the query content. Then, the query descriptors were matched against the target descriptors. We considered two approaches for automatic content-based matching: (1) matching of descriptors of the query and target keyframes, and (2) matching of descriptors for multiple frames (i-frames) from the query and target video, as shown in Figure <ref type="figure" coords="5,87.21,724.43,3.74,8.97" target="#fig_2">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Multi-frame matching</head><p>For multi-frame matching, different semantics of the matching are possible depending on the nature of the query. For example, if all of the individual images in the query content are important for the query ("all" semantics), then the matching semantics is such that the best target video shot from the database should have the best overall score of matching all of the query images to images in the target shot.  Multi-frame matching requires first the determination of the best matches among individual images from the query and target, and then computation of the overall score of all the matches. However, alternatively, if the query images are meant to illustrate different variations of the content ("or" semantics), then the matching semantics is such that the best target video should be the ones that have a single frame that best matches one of the query images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Interactive retrieval</head><p>For interactive retrieval, we enhanced the content-based approach by allowing the user to conduct multiple rounds of searching operations in which each successive round refines or builds on the results of a previous round. Each round consists of the following: (1) a similarity search in which target shots are scored against query content (using single frame or multi-frame search), and (2) a combining of these search results with the previous results list. This way, each successive round combines new results with a current list. We investigated several ways of combining results which involve different ways of manipulating the scores from the successive rounds. We have used a choice of the following aggregation functions for combining the scores:</p><formula xml:id="formula_2" coords="5,378.44,624.28,175.75,11.35">D i (n) = D i-1 (n) + D q (n),<label>(3)</label></formula><p>and</p><formula xml:id="formula_3" coords="5,370.13,657.44,184.05,11.36">D i (n) = min(D i-1 (n), D q (n)),<label>(4)</label></formula><p>where D q (n) gives the score of video shot n for the present query, and D i-1 (n) gives the combined score of video shot n for the previous query, and D i (n) gives the combined score result for the current round. Eq. 3 simply takes the sum of the score of each target video shot for the current query plus the cumulative score of the previous queries. This has the effect of weighting the most recent query equally with the previous queries. Eq. 4 takes the minimum of the current score and the previous scores for each target video shot. This has the effect of ranking most highly the target shots that best match any one of the query images. Although, Eq. 3 and Eq. 4 are simple monotonic functions, other combining functions that use arbitrary join predicates are possible [NCS + 01]. For combining content-based and model-based retrieval, we allow the above methods for combining results, however, we allow additionally a filtering method that computes the intersection of the previous result list with the results from the current query, as described next.</p><formula xml:id="formula_4" coords="6,210.76,168.41,188.36,122.58">Models Content Clusters Q 0 Q 1 ... Q i Q i 1 + R 0 R 1 ...</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model-based retrieval</head><p>The model-based retrieval allows the user to retrieve the target shots based on the semantic labels produced by the models. Each semantic label has an associated confidence score. The user can retrieve results for a model by issuing a query for a particular semantic label. The target video shots are then ranked by confidence score (higher score gives lower rank). Since the models do not assign labels to all of the target shots, only the ones that are positively classified tothe  The models can be applied sequentially or in parallel as shown in Figure <ref type="figure" coords="6,387.02,676.61,3.74,8.97" target="#fig_5">7</ref>. In the case of parallel search, the user defines weighting of multiple models in a single query. In sequential search, the user decides based on interim results which models to apply. For example, a parallel model-based search is as follows: nature = 0.5 * outdoors+0.25 * water+ 0.25 * sky. An example sequential model-based search is as follows: outdoors → no faces → no water.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Video query pipeline</head><p>The integrated search is carried out by the user successively applying the content-based and model-based search methods as shown in Figure <ref type="figure" coords="7,166.12,171.65,3.74,8.97" target="#fig_7">8</ref>.  For example, a user looking for video shots showing a beach scene can issue the following sequence of queries in the case that beach scenes have not been explicitly labeled::</p><p>1. Search for model = "outdoors", 2. Aggregate with model = "sky", 3. Aggregate with query image (possibly selected image) resembling desired video shot, 4. Aggregate with model = "water", 5. Aggregate with selected relevant image, video shot, 6. Repeat.</p><p>The iterative searching allows the users to apply sequentially the content-based and model-based searches. Different options can be used for scoring the results at each stage of the query and combining with the previous results. For TREC video retrieval, a choice of the following different approaches using different aggregation functions were provided for combining the scores:</p><p>1. Inclusive: each successive search operation issues new query against target database:</p><formula xml:id="formula_5" coords="7,165.59,587.07,142.53,11.36">D 0 (n) = D q (n),<label>(5)</label></formula><p>2. Iterative: each successive search operation issues query against current results list and scores by new query:</p><formula xml:id="formula_6" coords="7,166.16,650.51,141.96,11.35">D i (n) = D q (n),<label>(6)</label></formula><p>3. Aggregative: each successive search operation issues query against current results list and aggregates scores from current results and new query results:</p><formula xml:id="formula_7" coords="7,139.35,719.92,168.77,11.35">D i (n) = f (D i-1 (n), D q (n)),<label>(7)</label></formula><p>where f (.) corresponds to min, max, or avg. The distance scores D i (n) are based on feature similarity (for CBR) and label confidence (for models). For the models, D q (n) = 1 -C q (n), where C q (n) gives the confidence of the query label for video shot n, and D i-1 (n), and D i (n) are defined as above. The lossy filtering is accounted for in that some target shots n * have confidence score C q (n * ) = -∞. Eq. 7 combines the label score of each target video shot for the current query plus the cumulative label score of the previous queries, whereas Eq. 6 takes only the latest score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Speech retrieval</head><p>To compute the video retrieval results using speech indexing for the TREC video retrieval, we used the textual statement of information need associated with each topic without any refinement or pruning of the text. The speech retrieval system works as follows: the system first loads the inverted index and precomputed weights of each of the nonstop words. A single pass approach is used to compute a relevancy score with which each document is ranked against a query, where the relevancy score is given by the Okapi formula [RWSJ + 95]. Each word in the query string is tokenized, tagged, morphed and then scored using the Okapi formula above. The total relevancy score for the query string is the combined score of each of the query words. The scoring function takes into account the number of times each query term occurs in the document normalized with respect to the length of the document. This normalization removes bias that generally favor longer documents since longer documents are more likely to have more instances of any given word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Retrieval system</head><p>We have applied this type of iterative and integrated content-based and model-based searching procedure for computing the results for many of the TREC video retrieval topics. Example topics for which this approach was used include: "scenes with sailing boats on a beach,", "scenes with views of canyons," and "scenes showing astronaut driving a lunar rover." The video retrieval system is illustrated in Figure <ref type="figure" coords="7,346.57,595.83,3.74,8.97" target="#fig_8">9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Benchmark</head><p>The TREC video retrieval benchmark<ref type="foot" coords="7,482.22,641.18,3.49,6.27" target="#foot_0">1</ref> was developed by NIST<ref type="foot" coords="7,354.40,653.13,3.49,6.27" target="#foot_1">2</ref> to promote progress in content-based retrieval (CBR) from digital video via open, metrics-based evaluation. The benchmark involves the following tasks:</p><p>• Shot boundary detection • Ground truth assessments (provided by participants for known-item queries)</p><p>• Quantitative metrics for evaluating retrieval effectiveness (i.e., precision vs. recall).</p><p>The benchmark focuses on content-based searching in that the use of speech recognition and transcripts is not emphasized. However, the queries themselves typically involve information at the semantic-level, i.e., "retrieve video clips of Ronald Reagan speaking," and opposed to "retrieve video clips that have this color." The two kinds of queries, known-item and general information need, are distinguished in that the number of matches for the knownitem queries is pre-determined, i.e., it is known that there are only two clips showing Ronald Reagan. On the other hand, for the general searches, the number of matches in the corpus in not known, i.e., "video clips showing nature scenes."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Shot detection benchmark results</head><p>The results of the shot boundary detection on the TREC video corpus is shown in The results in Table <ref type="table" coords="8,411.48,258.13,4.98,8.97" target="#tab_2">1</ref> shows that the results for gradual changes could be improved. We found that in many of the cases, which were reported as errors, there was a detection of a boundary but the reported duration was too short. In such a case, the ISIS-based evaluation algorithm [ISI99] rejects the match, and considers it as both a deletion error and an insertion error. This is an undesired property of the evaluation criteria. If, for example, the system would not find a boundary at all, the evaluation would conider it as just a deletion, and rank the system better. In some other cases, a cut was reported as a short dissolve, with similar consequences.</p><p>Shot detection errors also resulted from the high noise level in the compressed MPEG video. For example, a periodic noisy pattern can be observed in Figure <ref type="figure" coords="8,502.64,425.78,4.98,8.97">3</ref> at a period of 15 frames (one GOP) due to the color coding errors introduced by the MPEG encoding scheme. From our experience this noise level seemed somewhat high, but we have not quantified it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Retrieval benchmark results</head><p>The results of the first retrieval experiment are shown in Table 2, which evaluates the average number of hits over the 46 "general search" queries. The interactive content-based retrieval (CBR) method is compared an automatic speech recognition (ASR) approach in which ASR was applied to the audio, and text indexing was used for answering the queries. The results show a signficant increase in retrieval quality using the interactive CBR approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head><p>Hits/query Automatic speech recognition (ASR)</p><p>1.9 Interactive Content-based retrieval (CBR) 4.3 Specific examples comparing retrieval performance for interactive CBR and ASR approaches are given in Table <ref type="table" coords="8,546.72,724.43,3.74,8.97">3</ref>.</p><p>In some cases, such as topics VT66 and VT47, the ASR approach gave better retrieval results. In these topics, the relevant information was not easily captured by the visual scenes. However, for other topics, such as VT55, VT49, VT43, and VT42, the interactive CBR approach gave better performance than the ASR approach. Table <ref type="table" coords="9,97.11,262.01,3.88,8.97">3</ref>: Video retrieval results (hits/query) comparing interactive CBR and ASR methods for specific queries.</p><p>We also compared the interactive CBR approach to noninteractive (or automatic) CBR in which only a single iteration of searching was allowed. The results for two of the topics given in Table <ref type="table" coords="9,177.69,332.40,4.98,8.97">4</ref> show a significant increase in retrieval performance using the interactive CBR approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic Description</head><p>Automatic Interactive # CBR CBR VT54 Glen Canyon Dam 3 12 VT15 Shots of corn fields 1 5 Table <ref type="table" coords="9,96.90,423.91,3.88,8.97">4</ref>: Video retrieval results (hits/query) comparing automatic and interactive CBR methods for specific queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Summary</head><p>In this paper, we described a system for automatic and interactive content-based retrieval that integrates features, models, and semantics. The system extracts feature descriptors from shots, which allows content-based retrieval, and classifies the shots using models for different events, scenes, and objects. The retrieval system allows the integration of content-based and model-based retrieval in an iterative search process. We developed also an approach based on speech indexing to provide a comparison with the contentbased/model-based approach. We described the results of applying these methods to the TREC video retrieval benchmark.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,72.00,700.40,236.13,8.97;2,72.00,712.36,236.13,8.97;2,72.00,724.32,53.95,8.97"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: The shot detection system automatically segments the video into temporal segments and selects a key-frame for each shot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,318.08,294.89,236.12,8.97;5,318.08,306.84,198.44,8.97"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Content-based retrieval matches multiple query frames against multiple frames in the target shots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,72.00,386.78,482.20,8.97;6,72.00,398.73,246.95,8.97;6,172.19,294.80,61.08,72.44"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The video content retrieval engine integrates methods for searching in an iterative process in which the user successively applies content-based and model-based searches.</figDesc><graphic coords="6,172.19,294.80,61.08,72.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,318.08,430.78,236.12,8.97;6,318.08,442.73,236.12,8.97;6,318.08,454.68,236.12,8.97;6,318.08,466.64,149.29,8.97;6,492.84,538.84,22.82,7.81;6,404.74,503.02,22.92,7.81;6,404.74,591.24,22.92,7.81;6,404.74,539.94,22.92,7.81;6,415.04,559.40,2.17,7.81;6,415.04,568.99,2.17,7.81;6,415.04,564.19,2.17,7.81"><head></head><label></label><figDesc>semantic class, the model-based search does not give a total ranking of the target shots. That is, the model-based search both filters and ranks the target shots, which has implications for its use in iterative searching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,318.08,625.13,236.12,8.97;6,318.08,637.09,120.10,8.97"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Parallel model search allows the user to define weighting of multiple models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="7,72.00,244.15,236.12,8.97;7,72.00,256.11,149.97,8.97"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Integration of content-based and model-based searching in the video query pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="8,84.45,350.78,211.21,8.97;8,82.77,84.69,214.52,250.97"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Screen image of the video retrieval system.</figDesc><graphic coords="8,82.77,84.69,214.52,250.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,318.08,117.99,236.12,124.63"><head>Table 1 .</head><label>1</label><figDesc>The system performed extremely well for shot detection giving very high precision and recall.</figDesc><table coords="8,318.08,163.29,236.12,79.33"><row><cell></cell><cell cols="4">Ins. Rate Del. Rate Precision Recall</cell></row><row><cell>Cuts</cell><cell>0.039</cell><cell>0.020</cell><cell>0.961</cell><cell>0.980</cell></row><row><cell cols="2">Gradual 0.589</cell><cell>0.284</cell><cell>0.626</cell><cell>0.715</cell></row><row><cell>All</cell><cell>0.223</cell><cell>0.106</cell><cell>0.831</cell><cell>0.893</cell></row><row><cell cols="5">Table 1: Shot boundary detection results for TREC video</cell></row><row><cell cols="2">shot detection.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,318.08,674.38,236.12,22.58"><head>Table 2 :</head><label>2</label><figDesc>Video retrieval results (avg. hits/query over 46 general searches).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="7,332.42,716.06,166.45,7.17"><p>http://www-nlpir.nist.gov/projects/t01v/revised.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="7,332.42,725.79,96.67,7.17"><p>http://trec.nist.gov/call01.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,125.10,688.56,183.01,8.97;9,125.10,700.52,171.24,8.97;9,125.10,712.48,41.27,8.97;9,300.92,712.48,7.19,8.97;9,125.10,724.43,133.33,8.97" xml:id="b0">
	<monogr>
		<ptr target="http://www.ibm.com/alphaworks" />
		<title level="m" coord="9,125.10,688.56,179.27,8.97">IBM CueVideo Toolkit Version 2.1</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,371.18,86.82,183.01,8.97;9,371.18,98.77,183.02,8.97;9,371.18,110.73,46.78,8.97" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<title level="m" coord="9,480.01,86.82,74.17,8.97;9,371.18,98.77,94.20,8.97">Pattern Classification and Scene Analysis</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley Eastern</publisher>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,371.18,132.42,183.02,8.97;9,371.18,144.38,183.01,8.97;9,371.18,156.34,183.01,8.97;9,371.18,168.29,175.18,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,539.25,132.42,14.94,8.97;9,371.18,144.38,183.01,8.97;9,371.18,156.34,102.02,8.97">Rubin. Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,480.40,156.34,73.78,8.97;9,371.18,168.29,106.24,8.97">Proceedings of the Royal Statistical Society, B</title>
		<imprint>
			<biblScope unit="issue">39</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,371.18,189.99,183.01,8.97;9,371.18,201.94,183.02,8.97;9,371.18,213.90,95.91,8.97;9,505.48,213.90,48.71,8.97;9,371.18,225.85,174.21,8.97;9,318.08,247.55,22.70,8.97;9,340.78,244.81,6.11,7.32;9,347.39,247.55,206.81,8.97;9,371.18,259.51,183.01,8.97;9,371.18,271.46,183.01,8.97;9,371.18,283.42,183.01,8.97;9,371.18,295.37,67.52,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,440.73,259.51,113.46,8.97;9,371.18,271.46,99.65,8.97">Supporting incremental join queries on ranked inputs</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Vitter</surname></persName>
		</author>
		<ptr target="http://www-asim.lip6.fr/AIM/corpus/aim1/indexE.html" />
	</analytic>
	<monogr>
		<title level="m" coord="9,393.20,189.99,160.99,8.97;9,371.18,201.94,126.34,8.97;9,493.84,271.46,60.35,8.97;9,371.18,283.42,123.76,8.97">European Workshop on Content Based Multimedia Indexing</title>
		<meeting><address><addrLine>Toulouse, FR; Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-10">October 1999. September 2001</date>
		</imprint>
	</monogr>
	<note>Proc. Conf. on Very Large Databases (VLDB)</note>
</biblStruct>

<biblStruct coords="9,371.18,317.07,183.02,8.97;9,371.18,329.02,183.01,8.97;9,371.18,340.98,183.01,8.97;9,371.18,352.94,183.01,8.97;9,371.18,364.89,183.02,8.97;9,371.18,376.84,171.88,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,457.83,329.02,96.36,8.97;9,371.18,340.98,183.01,8.97;9,371.18,352.94,20.52,8.97">A factor graph framework for semantic indexing and retrieval in video</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Kozintsev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ramchandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,413.65,352.94,140.54,8.97;9,371.18,364.89,183.02,8.97;9,371.18,376.84,37.90,8.97">Proc. IEEE Workshop on Contentbased Access to Image and Video Libraries (CBAIVL)</title>
		<meeting>IEEE Workshop on Contentbased Access to Image and Video Libraries (CBAIVL)<address><addrLine>Hilton Head, SC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-06-12">June 12 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,318.08,398.54,22.14,8.97;9,340.22,395.81,6.11,7.32;9,346.83,398.54,207.37,8.97;9,371.18,410.49,183.01,8.97;9,371.18,422.46,183.00,8.97;9,371.18,434.41,183.02,8.97;9,371.18,446.36,183.02,8.97;9,371.18,458.32,183.01,8.97;9,371.18,470.27,55.89,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,467.52,410.49,86.67,8.97;9,371.18,422.46,62.94,8.97">Learning to annotate video databases</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">L</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<idno>NLS + 02</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,459.78,422.46,94.41,8.97;9,371.18,434.41,183.02,8.97;9,371.18,446.36,183.02,8.97;9,371.18,458.32,42.06,8.97">IS&amp;T/SPIE Symposium on Electronic Imaging: Science and Technology -Storage &amp; Retrieval for Image and Video Databases</title>
		<meeting><address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-01">2002. January 2002</date>
			<biblScope unit="volume">4676</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,371.18,491.96,183.01,8.97;9,371.18,503.92,183.01,8.97;9,371.18,515.88,86.91,8.97" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="9,421.67,491.96,132.53,8.97;9,371.18,503.92,78.31,8.97">An Introduction to Signal Detection and Estimation</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">V</forename><surname>Poor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>2 edition</note>
</biblStruct>

<biblStruct coords="9,371.18,537.58,183.02,8.97;9,371.18,549.53,183.01,8.97;9,371.18,561.48,183.01,8.97;9,371.18,573.44,183.01,8.97;9,371.18,585.39,67.52,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,501.02,537.58,53.18,8.97;9,371.18,549.53,183.01,8.97;9,371.18,561.48,24.94,8.97">Structure and content-based segmentation of speech transcripts</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ponceleon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,428.76,561.48,125.43,8.97;9,371.18,573.44,178.76,8.97">Proc. ACM Inter. Conf. Res. and Develop. in Inform. Retrieval (SIGIR)</title>
		<meeting>ACM Inter. Conf. Res. and Develop. in Inform. Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2001-09">September 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,323.72,607.09,22.59,8.97;9,346.32,604.35,6.11,7.32;9,352.93,607.09,201.27,8.97;9,371.18,619.05,183.01,8.97;9,371.18,631.00,183.01,8.97;9,371.18,642.96,101.31,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,371.18,631.00,73.35,8.97">OKAPI at TREC-3</title>
		<author>
			<persName coords=""><forename type="middle">E</forename><surname>Rwsj + 95] S</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Sparck-Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,474.17,631.00,80.02,8.97;9,371.18,642.96,72.04,8.97">Proc. Third Text Retrieval Conference</title>
		<meeting>Third Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,318.08,664.65,21.59,8.97;9,339.66,661.92,6.11,7.32;9,346.28,664.65,207.91,8.97;9,371.18,676.61,183.01,8.97;9,371.18,688.56,183.02,8.97;9,371.18,700.52,183.01,8.97;9,371.18,712.48,183.00,8.97;9,371.18,724.43,129.51,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,444.40,676.61,109.79,8.97;9,371.18,688.56,179.50,8.97">Integrating features, models, and semantics for content-based retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,385.49,700.52,168.71,8.97;9,371.18,712.48,183.00,8.97;9,371.18,724.43,54.61,8.97">Proc. Multimedia Content-based Indexing and Retrieval (MMCBIR) workshop, Rocquencourt, FR</title>
		<meeting>Multimedia Content-based Indexing and Retrieval (MMCBIR) workshop, Rocquencourt, FR</meeting>
		<imprint>
			<date type="published" when="2001-09">September 2001</date>
		</imprint>
	</monogr>
	<note>SBL + 01</note>
</biblStruct>

<biblStruct coords="10,125.10,86.82,183.02,8.97;10,125.10,98.77,183.02,8.97;10,125.10,110.73,183.01,8.97;10,125.10,122.68,60.16,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,174.57,86.82,133.55,8.97;10,125.10,98.77,36.60,8.97">MPEG-7 standard for multimedia databases</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,185.33,98.77,122.79,8.97;10,125.10,110.73,66.86,8.97">ACM Proc. Int. Conf. Manag. Data (SIGMOD)</title>
		<meeting><address><addrLine>Santa Barbara, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Tutorial</publisher>
			<date type="published" when="2001-05">May 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,125.10,142.61,183.02,8.97;10,125.10,154.57,183.02,8.97;10,125.10,166.52,173.65,8.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,245.22,142.61,62.90,8.97;10,125.10,154.57,105.75,8.97">MPEG-7 multimedia description schemes</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Salembier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,239.67,154.57,68.45,8.97;10,125.10,166.52,113.47,8.97">IEEE Trans. Circuits Syst. for Video Technol</title>
		<imprint>
			<date type="published" when="2001-08">August 2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
