<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,132.92,77.00,346.37,16.65;1,219.43,104.84,173.24,16.65">The Bias Problem and Language Models in Adaptive Filtering</title>
				<funder ref="#_xC2azqU">
					<orgName type="full">Air Force Research Laboratory</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,235.51,130.51,48.19,11.10"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15232</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,307.28,130.51,69.43,11.10"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
							<email>callan@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15232</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,132.92,77.00,346.37,16.65;1,219.43,104.84,173.24,16.65">The Bias Problem and Language Models in Adaptive Filtering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1ED595CD4F788139DCD37056883F6527</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We used the YFILTER filtering system for experiments on updating profiles and setting thresholds. We developed a new method of using language models for updating profiles that is more focused on picking informative/discriminative words for query. The new method was compared with the well-known Rocchio algorithm. Dissemination thresholds were set based on maximum likelihood estimation that models and compensates for the sampling bias inherent in adaptive filtering. Our experimental results suggest that using what kind of distribution to model the scores of relevant and non-relevant documents is corpus dependant. The experimental results also show the sampling bias problem of training data while filtering makes the final profile learned biased.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Given an initial description of a profile, a filtering system must sift through a stream of information and deliver the most relevant documents to the profile. Filtering is more like an online classification problem than a traditional search problem, because it is a binary decision process. Text classification algorithms, such as SVM, Rocchio, Boosting and Naive Bayes, can be applied to filtering, especially for batch filtering and routing. A common approach to learning profiles is to use an incremental version of the Rocchio algorithm <ref type="bibr" coords="1,275.84,556.52,11.72,8.96" target="#b6">[7]</ref> </p><formula xml:id="formula_0" coords="1,65.96,573.43,119.84,43.94">            -             + = ′ ∑ ∑ ∈ ∈ | | | | NR D R D Q Q NR D i R D i i i γ β α</formula><p>Where Q is the initial profile vector, Q' is the new profile vector, R is the set of relevant documents, NR is the set of non-relevant documents, D i is a document vector, and α, β, and γ are constants indicating the relative value of each type of evidence.</p><p>Language modeling has been applied to filtering track in TREC8 <ref type="bibr" coords="1,111.80,709.16,10.69,8.96" target="#b3">[4]</ref>. EM algorithm is used to find optimal parameters to maximize the likelihood of joint probability of relevant document and query. Our work introduces another way of using language modeling for the profile learning, which is also usin g EM but maximizing the likelihood of training data. We compares it with Rocchio in TREC10. Our result also shows the sampling bias problem (user only provide feedback for documents delivered) on learned profile terms and term weights/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SYSTEM DESCRIPTION</head><p>The YFILTER information filtering system we used is architecturally similar to InRoute <ref type="bibr" coords="1,464.11,378.80,10.69,8.96" target="#b2">[3]</ref>. It supports both structured Boolean and natural language descriptions of initial profiles. For natural language profiles, it can automatically update the profile according to user relevance feedback. YFILTER provides an option to use the INQUERY stopwords list and the Porter wordstemming algorithm <ref type="bibr" coords="1,406.39,449.36,10.69,8.96" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROFILE LEARNING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Initial Profile and Scoring Method</head><p>Each profile begins with topic words (usually 1-4 words) given by NIST, together with the training documents (usually 2). We used the BM25 tf.idf formula for scoring documents. Idf is initialized based on training data and updated over time as documents are filtered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Profile Updating</head><p>At the very beginning when the number of training data is small, YFILTER has profile-specific anytime updating. That is, it updates a profile (threshold and scoring function) immediately whenever feedback, positive or negative, is available for that profile (Figure <ref type="figure" coords="1,324.07,691.88,3.63,8.96" target="#fig_0">1</ref>). After getting enough positive training examples (30 by default), the system begins to decrease the update frequency, updating the threshold only if:</p><p>• Current number of relevant documents delivered is 2*(number of relevant documents delivered at the last update), or • Current number of non-relevant documents is 2*(number of non-relevant documents delivered at the last update), or • Recent dangerous performance, which we define as 9 non-relevant documents delivered in a row.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Threshold Updating</head><p>We used the algorithm described in our SIGIR01 paper for threshold updating <ref type="bibr" coords="2,160.28,251.60,10.69,8.96" target="#b8">[9]</ref>. We provided a solution to optimize for F-beta measure based on our model. In case the system failed to find the optimal model, some heuristic were used to set the threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Updating Terms and Term Weights</head><p>We tried the following two-term selection and term weight updating algorithms and compared their performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.1">Language Model</head><p>Probabilistic language models, which are used widely in speech recognition and have shown promise for adhoc information retrieval. The strong theoretical foundation of language models enables us to build a variety of new capabilities. Current research on using language models for information retrieval tasks is focused on developing techniques similar to those used in speech recognition.</p><p>However the differing requirements of speech recognition and information retrieval suggest that major adaptation of traditional approaches to language modeling is necessary to develop algorithms that will be highly accurate in the real world.</p><p>One research work in this direction is <ref type="bibr" coords="2,216.92,563.72,10.69,8.96" target="#b3">[4]</ref>. In their work, EM algorithm is used to find optimal relevance weights of each word that maximize the likelihood of joint probability of relevant document and query:</p><formula xml:id="formula_1" coords="2,82.04,614.84,206.00,30.83">∏ ∏ = j j j j j d q P d P q d P ) | ( ) ( ) , (<label>(1)</label></formula><p>In our work, we tried a different way of using language model. We propose a mixture of generative language models, which is more appropriate for the task of query expansion in information filtering and traditional adhoc retrieval task. As shown in Figure <ref type="figure" coords="2,218.84,706.76,3.77,8.96" target="#fig_0">1</ref>, we assume each relevant document is generated by a combination of two language models: A General English Model M E , and a user-specific Topic Model M T . For example, if the user is interested in "Star Wars", in a relevant document we expect words such as "is" and "the" will come from the general English model, and words such as "star" and "wars" will come from the topic model.</p><p>When doing user profile updating for query expansion, the filtering system will focus on learning M T to find words that are very informative for deciding whether the document is on topic or not. Given a fixed value of a (usually a very high value, such as 0.95), we can train M E to maximize the likelihood of all documents processed, and train M T using the EM algorithm to maximize the likelihood of all the relevant documents processed. A sketch of the training algorithm was given in Figure <ref type="figure" coords="2,362.71,453.92,3.77,8.96">2</ref>.</p><p>This new mixture model will pick words where P(w| relevant) / P(w | general English) is very high. Because the task of profile updating is to provide a profile that can separate relevant documents from non-relevant documents, we believe such words are more discriminative, and thus are good candidates for being added to user profiles. Similar techniques are developed for ad-hoc retrieval independently <ref type="bibr" coords="2,506.11,557.00,15.43,8.96" target="#b9">[10]</ref>.</p><p>Although both algorithms are called "Language Model" approach, our work and <ref type="bibr" coords="2,423.31,589.52,11.72,8.96" target="#b3">[4]</ref> are very different in that 1) Our optimization goal is to maximize the likelihood of training data (which is a widely used method when using MLE), while they are maximizing something else (Equation <ref type="formula" coords="2,366.79,636.56,4.18,8.96" target="#formula_1">1</ref>) 2) The parameter our algorithm estimates is the Topic model, which is a multinomial distribution, while the parameters <ref type="bibr" coords="2,417.19,660.08,11.72,8.96" target="#b3">[4]</ref> are estimating is relevance weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M E:</head><p>General English M T: Topic Doc a 1 -a</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.2">Rocchio</head><p>The Rocchio algorithm used this year is very similar to the one we used last year in TREC9. Each time a positive relevance feedback arrives (including those in the training data), all words in that document are added to the profile's candidate list of terms. Then the weight of each word in the candidate list is calculated according to the incremental Rocchio formula:</p><formula xml:id="formula_2" coords="3,84.20,584.41,203.84,15.05">rel non rel q w w w Rocchio - ⋅ - ⋅ + ⋅ = γ β α (1)</formula><p>where ) (t w q : max( term frequency of word t in original topics, 0.5) In order to learn faster, β is set bigger than usual in the relevance feedback formula to emphasize the importance of relevant documents.</p><formula xml:id="formula_3" coords="3,82.64,672.62,167.66,30.90">∑ ∈ + = ) ( _ , , * _ 1 | ) ( _ | 1 ) ( t set</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hierarchical Category Structure</head><p>The filtering profiles correspond to Reuters categories, which are organized hierarchically. We assume that if a document belongs to a child category, it should also belong to the parent category. We used the following rules to take advantage of the hierarchical relationship between profiles when making the decision whether to deliver a document: If C i is a child category of Cj, then:</p><p>• When a document d t comes, we first consider whether it should be delivered to Cj, and if so, then consider whether it should be delivered to Ci.</p><p>• If the document d t was delivered to Cj and the system received negative feedback, do not deliver d t to Ci • If d t was not delivered to Cj previously, but was delivered to Ci and the system received a positive feedback, deliver d t to Cj By doing this, we get more training data for some of the profiles. For example, profiles 17, 34, and 45 get 8 instead of 2 relevant training documents to begin with, which is very helpful at the early stage. The Reuters category assignments (i.e., the training data) are not consistent. Some documents are judged as relevant to a child category but non-relevant to the For each words in relevant documents:</p><formula xml:id="formula_4" coords="3,77.48,349.12,213.96,97.23">) | ( * ) | ( * ) 1 ( ) | ( * ) 1 ( * _ E i T i T i i i M w P M w P M w P rtf tf T α α α + - - = ∑ = documents relevant in words all : _ _ ) | ( i i i T i tf T tf T M w P</formula><p>Figure2 Training Algorithm for Language Model parent category. For example, documents 135639, 24269 26015 belong to R18 (DOMESTIC MARKETS) but do not belong to R17 (MARKETS/MARKETING). After reading those documents, we believe that they are in fact relevant to R17. It is well-known that human category assignments are not perfectly consistent, and any algorithm that uses them must compensate for noise in the training data. Using hierarchical structure of the categories helps to solve this problem to some extend. <ref type="bibr" coords="4,64.88,197.29,4.50,10.80" target="#b3">4</ref>. We submitted 4 runs for the adaptive filtering task using Rocchio ("Roc") or language models ("LM") for profile updating, and our Maximum Likelihood Estimation for setting dissemination thresholds. The results are reported in Table <ref type="table" coords="4,188.48,473.48,3.77,8.96" target="#tab_1">1</ref>. Compared with other groups, the results are not satisfying. Implementation errors were one cause, but we defer a discussion of their impact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ANALYSIS OF RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Figures <ref type="figure" coords="4,97.28,529.52,4.98,8.96">2</ref> and<ref type="figure" coords="4,122.00,529.52,4.98,8.96" target="#fig_3">3</ref> show the system performance over time for run 1. Precision and recall improve over time (Figure <ref type="figure" coords="4,97.04,553.04,3.63,8.96" target="#fig_3">3</ref>), but Utility decreases (Figure <ref type="figure" coords="4,228.44,553.04,3.63,8.96" target="#fig_4">4</ref>). This means the profiles (terms and term weighting) were improving as we got more training data while filtering, but unfortunately the threshold was set too high and got worse over time.</p><p>Despite the bugs and problems with the threshold, we can still analyse the performance of the Rocchio and language model methods of adding terms to profiles. According to Table <ref type="table" coords="4,155.48,655.16,3.77,8.96" target="#tab_1">1</ref>, the simple Rocchio method works much better, which was a surprise. Our hypothesis was that the language model would work well; because the new language model approach does not need a stopwords list. However, the idf weights in the BM25 scoring method penalized words with high idf, thus allowing Rocchio to work well. Possible reason is BM25 scoring method used in Yfilter is good for Rocchio, but not good for language model. We probably should replace BM25 with KL divergence, which is a more natural scoring mechanism to measure distributional similarities. 8 0 0 0 0 1 2 0 0 0 0 1 6 0 0 0 0 2 0 0 0 0 0 2 4 0 0 0 0 2 8 0 0 0 0 3 2 0 0 0 0 3 6 0 0 0 0 4 0 0 0 0 0 4 4 0 0 0 0 4 8 0 0 0 0 5 2 0 0 0 0 5 6 0 0 0 0 6 0 0 0 0 0 6 4 0 0 0 0 6 8 0 0 0 0 7 2 0 0 0 0 7 6 0 0 0 0 8 0 0 0 0 0 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Average Precision and Recall Over Time</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Documents Filtered</head><note type="other">Precision</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Score Density Distribution</head><p>We examined profile 77 on which our thresholding method did a very bad job. We used the learned final profile to score all of the documents in the corpus.</p><p>Figure <ref type="figure" coords="4,355.03,597.08,4.98,8.96" target="#fig_4">4</ref> shows the score density distribution of the relevant documents, which can be approximated by a normal distribution. Figure <ref type="figure" coords="4,441.43,620.60,4.98,8.96" target="#fig_5">5</ref> shows the score density distribution of non-relevant documents that contain at least one profile term.</p><p>We found that using the exponential distribution to approximate the probability density function of nonrelevant documents is problematic in this profile. A Beta distribution seems more appropriate. Since the exponential distribution is a special case of the beta distribution, using a beta distribution will also cover cases where the exponential distribution is right.</p><p>Considering the maximum likelihood estimation method proposed in <ref type="bibr" coords="5,151.40,121.64,11.72,8.96" target="#b8">[9]</ref> does not require any specific distribution, we can plug the beta distribution into the general framework and find the optimal parameters. We</p><p>have not yet implemented the algorithm to find the optimal beta distribution parameters. We simply observe that it may be a better approximation function than the exponential distribution proposed by <ref type="bibr" coords="5,257.84,192.20,11.72,8.96" target="#b0">[1]</ref> and used in our previous experiments <ref type="bibr" coords="5,199.52,203.96,11.72,8.96" target="#b8">[9]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Biased Training Problem for Profile Updating</head><p>We looked at the score distribution of profile 71 on which most of the systems did poorly. We fixed the profile terms and term weights (using the profile learned by the end) and scored all the documents in the corpus. We are surprised that the score density distribution of relevant documents looked more like an exponential distribution (Figure <ref type="figure" coords="5,207.68,330.56,3.63,8.96">6</ref>). Redrawing the score density distribution of the top scoring relevant documents for this profile (Figure <ref type="figure" coords="5,202.88,354.08,4.18,8.96">7</ref>) shows that the real distribution is actually like a mixture model of exponential and normal.</p><p>One possible explanation of this is the biased sampling problem <ref type="bibr" coords="5,105.68,405.08,10.69,8.96" target="#b8">[9]</ref>. In adaptive filtering, the user only provides feedback about documents delivered, so the training data is not sampled randomly, and the profile learned by the system is biased. The score density distribution of Profile 71 provides an extreme real case that illustrates this problem. Although we have proposed an algorithm to solve the sampling bias problem for threshold setting <ref type="bibr" coords="5,188.12,487.40,10.69,8.96" target="#b8">[9]</ref>, we didn't develop a solution to solve the sampling bias problem when terms, term weights, and thresholds are all being adjusted simultaneously. One possible way is to deliver interesting "near miss" documents, so that the learning software gets a broader view of the surrounding information landscape. Theories in other research area, such as active learning (also known as experimental design) and reinforcement learning, are potentially useful considering the similarity of the tasks. Also, the bias problem for threshold learning and profile term updating are correlated and should be solved together.</p><p>Another solution is to explicit modeling the sampling bias while profile term weights and threshold are changing, and more advanced analysis is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Defects and explanation</head><p>Our results are disappointing on TREC10. There are several problems with the runs we submitted:   (1) The methods deciding when to adjust terms and term weights was not integrated with threshold learning. Thus the system could change profiles without also adjusting thresholds to compensate for the changes in document scores.</p><p>(2) We used a heuristic rule to set thresholds when there is no solution based on maximum likelihood estimation. Unfortunately due to a programming error, we set the threshold too high (usually 1, which corresponded to delivering no document). Recovery speed was too slow.</p><p>(3) The words in the original profiles were stemmed before case conversion, but words in documents were stemmed after case conversion. The Porter stemmer is sensitive to case, so this difference produces inconsistent stemming in profiles and documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>We tried a new profile-updating algorithm based on a mixture language model that we believe is more appropriate for the task of query expansion, and compared it with Rocchio. Compared with traditional language models, our new approach does select very discriminative words and requires no stop word list. The performance is encouraging. But what surprised us is how efficient (in terms of running time) and effective (in terms of performance) the old method of Rocchio is.</p><p>We noticed that the Beta distribution might be more appropriate for modeling the non-relevant document scores, although our previous experiments shows on some other dataset exponential distribution works well.</p><p>We hypothesis that what kind of distribution to use is corpus/system dependant, although the Maximum Likelihood estimation we proposed does not require what kind of corpus to use, a real filtering should chose the right approximation function when applying our algorithm. We also noticed the effect of the sampling bias problem not only on profile threshold setting, but also on profile term weighting. Active learning and explicit modeling of the sampling bias while profile is changing are possible solutions for this problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,324.08,224.60,223.18,8.96;2,324.08,236.24,45.75,8.96"><head>Figure 1 A</head><label>1</label><figDesc>Figure 1 A mixture model to generate on topic documents</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,76.52,78.20,205.45,8.96;3,94.52,93.92,68.88,9.77;3,178.28,119.96,12.83,23.48;3,137.36,113.94,6.58,15.62;3,197.36,142.70,23.97,8.09;3,188.36,142.70,7.00,8.09;3,164.92,142.70,21.98,8.09;3,154.28,142.70,8.99,8.09;3,152.24,142.70,2.50,8.09;3,203.84,127.22,5.99,10.79;3,184.64,110.30,5.99,10.79;3,130.28,117.86,3.99,10.79;3,105.32,117.86,2.40,10.79;3,86.36,117.86,3.99,10.79;3,149.84,142.70,2.50,8.09;3,122.60,122.90,5.49,8.09;3,98.60,122.90,2.50,8.09;3,214.28,127.22,3.33,10.79;3,192.44,127.22,6.67,10.79;3,192.44,110.30,3.33,10.79;3,173.12,110.30,6.67,10.79;3,110.60,117.86,9.99,10.79;3,90.92,117.86,8.00,10.79;3,78.68,117.86,7.33,10.79;3,76.52,159.56,218.85,8.96;3,94.52,171.32,200.89,8.96;3,94.52,183.08,77.55,8.96;3,112.52,198.80,182.91,8.96;3,130.52,210.56,31.99,8.96;3,187.88,227.53,3.34,10.80;3,181.16,227.53,6.00,10.80;3,165.56,227.53,4.00,10.80;3,141.32,227.53,2.40,10.80;3,122.36,227.53,4.00,10.80;3,193.40,227.53,6.00,10.80;3,146.60,227.53,10.00,10.80;3,126.92,227.53,8.00,10.80;3,114.68,227.53,7.33,10.80;3,157.76,232.58,5.00,8.10;3,134.60,232.58,2.50,8.10;3,172.76,223.61,6.59,15.64;3,112.52,248.24,182.89,8.96;3,112.52,260.00,136.11,8.96;3,112.52,275.72,54.37,8.96;3,112.52,291.44,182.94,8.96;3,165.56,304.45,4.00,10.80;3,141.32,304.45,2.40,10.80;3,122.36,304.45,4.00,10.80;3,157.76,309.50,5.00,8.10;3,134.60,309.50,2.50,8.10;3,146.60,304.45,10.00,10.80;3,126.92,304.45,8.00,10.80;3,114.68,304.45,7.33,10.80;3,178.52,305.84,116.89,8.96;3,112.52,321.20,36.01,8.96"><head>( 1 )For( 2 )</head><label>12</label><figDesc>Calculate the General English language model: Calculate the Topic language model using the EM algorithm to maximize the likelihood of all relevant the number of unique words that appear in the relevant docuements b. EM step:Iterate the following steps until changes on )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,324.08,507.56,223.16,8.96;4,324.08,519.20,98.48,8.96"><head>Figure 3</head><label>3</label><figDesc>Figure 3 Filtering performances at different stages: T10SU Metric. (Run 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,326.60,253.88,220.64,8.96;5,324.07,265.52,107.01,8.96;5,324.08,72.05,235.08,179.16"><head>Figure 4</head><label>4</label><figDesc>Figure 4 Score density distribution of relevant documents for profile 77.</figDesc><graphic coords="5,324.08,72.05,235.08,179.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="5,324.08,487.76,219.92,8.96;5,324.08,499.40,107.01,8.96;5,324.08,508.37,220.08,186.12"><head>Figure 5</head><label>5</label><figDesc>Figure 5 Score density distributions of non-relevant documents for profile 77.</figDesc><graphic coords="5,324.08,508.37,220.08,186.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="5,324.08,697.16,196.64,8.96;5,324.08,708.80,107.01,8.96"><head>Figure 6 Figure 7</head><label>67</label><figDesc>Figure 6 Score density distribution of relevant documents for profile 71.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,64.88,222.98,206.22,195.99"><head>Table 1 : Submitted runs in TREC-9</head><label>1</label><figDesc></figDesc><table coords="4,64.88,222.98,206.22,172.98"><row><cell></cell><cell cols="2">1 Run2</cell><cell>Run3</cell><cell>Run4</cell></row><row><cell>Profile Updating</cell><cell>Roc.</cell><cell>Roc.</cell><cell>LM</cell><cell>LM</cell></row><row><cell>Threshold Updating</cell><cell>ML</cell><cell>ML</cell><cell>ML</cell><cell>ML</cell></row><row><cell>Optimized for</cell><cell>T10S U</cell><cell>T10F</cell><cell>T10S U</cell><cell>T10F</cell></row><row><cell>T10SU</cell><cell>0.144</cell><cell>0.143</cell><cell>0.081</cell><cell>0.080</cell></row><row><cell>T10F</cell><cell>0.273</cell><cell>0.275</cell><cell>0.158</cell><cell>0.163</cell></row><row><cell>Profile&gt;=Mean</cell><cell>55</cell><cell>41</cell><cell>32</cell><cell>21</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6.">ACKNOWLEDGEMENTS</head><p>This material is based on work supported by <rs type="funder">Air Force Research Laboratory</rs> contract <rs type="grantNumber">F30602-98-C-0110</rs>. Any opinions, findings, conclusions or recommendations expressed in this paper are the authors', and do not necessarily reflect those of the sponsors.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_xC2azqU">
					<idno type="grant-number">F30602-98-C-0110</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,342.08,323.48,205.28,8.96;6,342.07,335.24,205.18,8.96;6,342.07,347.00,146.22,8.96" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="6,495.31,323.48,52.04,8.96;6,342.07,335.24,205.18,8.96;6,342.07,347.00,104.18,8.96">The Score-Distribution Threshold Optimization for Adaptive Binary Classification Task</title>
		<author>
			<persName coords=""><forename type="first">Avi</forename><surname>Arampatzis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hameren</surname></persName>
		</author>
		<idno>SIGIR01</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,342.07,362.72,205.26,8.96;6,342.07,374.48,138.42,8.96" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,410.47,362.72,136.86,8.96;6,342.07,374.48,97.10,8.96">Incremental Relevance Feedback for Information Filtering</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<idno>SIGIR96</idno>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,342.07,390.20,202.14,8.96" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<title level="m" coord="6,407.23,390.20,136.98,8.96">Learning while Filtering. SIGIR98</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,342.07,405.92,205.18,8.96;6,342.07,417.68,205.16,8.96;6,342.07,429.44,205.14,8.96;6,342.07,441.20,176.49,8.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,497.35,405.92,49.90,8.96;6,342.07,417.68,205.16,8.96;6,342.07,429.44,88.47,8.96">Twenty-One at TREC-8: using Language Technology for Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pohlmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,454.75,429.44,92.46,8.96;6,342.07,441.20,144.49,8.96">Proceeding of Eighth Text Retrieval Conference (TREC-8)</title>
		<meeting>eeding of Eighth Text Retrieval Conference (TREC-8)</meeting>
		<imprint>
			<publisher>NIST</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,342.07,456.92,205.18,8.96;6,342.07,468.68,205.16,8.96;6,342.07,480.44,126.54,8.96" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="6,461.83,456.92,85.42,8.96;6,342.07,468.68,205.16,8.96;6,342.07,480.44,84.99,8.96">Document Language Model, Query Models and Risk Minimization for Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<idno>SIGIR01</idno>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,342.07,496.16,205.26,8.96;6,342.07,507.92,37.89,8.96" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="6,440.59,496.16,106.74,8.96;6,342.07,507.92,34.10,8.96">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,342.07,523.64,205.26,8.96;6,342.07,535.40,205.21,8.96;6,342.07,547.16,205.21,8.96;6,342.07,558.92,181.70,8.96" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Rocchio</surname></persName>
		</author>
		<title level="m" coord="6,446.11,523.64,101.22,8.96;6,342.07,535.40,205.21,8.96;6,342.07,547.16,205.21,8.96;6,342.07,558.92,41.50,8.96">Relevance feedback in information retrieval in The SMART Retrieval System-Exp eriments in Automatic Document Processing</title>
		<imprint>
			<publisher>Prentice Hall Inc</publisher>
			<date type="published" when="1971">1971</date>
			<biblScope unit="page" from="313" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,342.07,574.64,170.17,8.96" xml:id="b7">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,425.23,574.64,87.01,8.96">Information Retrieval</title>
		<imprint>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,342.07,590.36,205.14,8.96;6,342.07,602.12,201.90,8.96" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="6,453.07,590.36,94.14,8.96;6,342.07,602.12,93.90,8.96">Maximum Likelihood Estimation for Filtering</title>
		<author>
			<persName coords=""><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<idno>Thresholds ACM SIGIR01</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,344.59,617.84,202.62,8.96;6,342.07,629.60,205.14,8.96;6,342.07,641.36,205.14,8.96;6,342.07,653.12,205.14,8.96;6,342.07,664.88,99.85,8.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,495.07,617.84,52.14,8.96;6,342.07,629.60,205.14,8.96;6,342.07,641.36,88.47,8.96">Model-based Feedback in the Language Modeling Approach to Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,454.75,641.36,92.46,8.96;6,342.07,653.12,205.14,8.96;6,342.07,664.88,99.85,8.96">Proceedings of Tenth International Conference on Information and Knowledge Management</title>
		<meeting>Tenth International Conference on Information and Knowledge Management</meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
