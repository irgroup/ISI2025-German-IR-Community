<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,257.56,75.37,96.78,12.64">IIT at TREC-10</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,235.00,103.22,44.74,9.94"><forename type="first">M</forename><surname>Aljlayl</surname></persName>
							<email>aljlayl@ir.iit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science Illinois Institute of Technology Chicago</orgName>
								<orgName type="laboratory">Information Retrieval Laboratory</orgName>
								<address>
									<postCode>60616</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,286.71,103.22,41.43,9.94"><forename type="first">S</forename><surname>Beitzel</surname></persName>
							<email>beitzel@ir.iit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science Illinois Institute of Technology Chicago</orgName>
								<orgName type="laboratory">Information Retrieval Laboratory</orgName>
								<address>
									<postCode>60616</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,335.16,103.22,41.92,9.94"><forename type="first">E</forename><surname>Jensen</surname></persName>
							<email>jensen@ir.iit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science Illinois Institute of Technology Chicago</orgName>
								<orgName type="laboratory">Information Retrieval Laboratory</orgName>
								<address>
									<postCode>60616</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,232.72,292.94,29.84,9.94"><forename type="first">M</forename><surname>Lee</surname></persName>
							<email>lee@ir.iit.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science Illinois Institute of Technology Chicago</orgName>
								<orgName type="laboratory">Information Retrieval Laboratory</orgName>
								<address>
									<postCode>60616</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,270.16,292.94,55.62,9.94"><forename type="first">D</forename><surname>Grossman</surname></persName>
							<email>grossman@ir.iit.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science Illinois Institute of Technology Chicago</orgName>
								<orgName type="laboratory">Information Retrieval Laboratory</orgName>
								<address>
									<postCode>60616</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,333.82,292.94,45.43,9.94"><forename type="first">O</forename><surname>Frieder</surname></persName>
							<email>frieder@ir.iit.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science Illinois Institute of Technology Chicago</orgName>
								<orgName type="laboratory">Information Retrieval Laboratory</orgName>
								<address>
									<postCode>60616</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,257.56,75.37,96.78,12.64">IIT at TREC-10</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FD36E3C97CCE14307E1690FEBE546E2A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For TREC-10, we participated in the adhoc and manual web tracks and in both the site-finding and cross-lingual tracks. For the adhoc track, we did extensive calibrations and learned that combining similarity measures yields little improvement. This year, we focused on a single highperformance similarity measure. For site finding, we implemented several algorithms that did well on the data provided for calibration, but poorly on the real dataset. For the cross-lingual track, we calibrated on the monolingual collection, and developed new Arabic stemming algorithms as well as a novel dictionary-based means of cross-lingual retrieval. Our results in this track were quite promising, with seventeen of our queries performing at or above the median.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For IIT at TREC-10, we focused on the adhoc tasks (both automatic and manual), the site finding task, and the Arabic cross-lingual tasks. For the adhoc tasks, our system is quite different from last year. We calibrated with different fusion approaches and found that a single similarity measure outperformed our other approaches. We also worked with the NetOwl entity tagger to improve our phrase recognition. In the manual track, we developed a new user interface to assist our manual user.</p><p>Our results for the Arabic cross-lingual track were quite promising. We developed a new stemmer and made use of a dictionary-based algorithm that requires the translation of the term to be equivalent when going from Arabic-English and from English-Arabic. Finally, we participated in the web site finding track. We tested a variety of simple approaches, but unfortunately, our results were not very impressive. We are conducting failure analysis on this track to include in the final paper.</p><p>For TREC-10's ad-hoc task, we focused on effectiveness for short queries. We did a variety of calibrations after TREC-9 on the utility of fusion of various IR approaches. We found that when the stop word lists and parsers are kept constant and effective ranking strategies are used, essentially similar result sets occur for a variety of similarity measures and improvements in average precision due to fusion are negligible. We published this result <ref type="bibr" coords="2,409.84,160.46,11.77,9.94" target="#b6">[7]</ref>, and for TREC-10, focused on a single similarity measure.</p><p>In this section, we briefly describe our query-processing techniques: the use of automatic statistical phrase weighting based on query length and the use of entity tagging for query terms. In the last section, we present our TREC 10 ad-hoc results including some of our results from fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Query Processing</head><p>Many different strategies are used to improve the overall effectiveness of an IR system. Several examples are automatic term weighting <ref type="bibr" coords="2,266.74,289.34,12.00,9.94" target="#b0">[1,</ref><ref type="bibr" coords="2,281.52,289.34,9.22,9.94" target="#b1">2]</ref> and relevance feedback <ref type="bibr" coords="2,400.01,289.34,11.77,9.94" target="#b2">[3]</ref>. Phrases are frequently suggested as a means for improving the precision of an IR system. Prior research with phrases has shown that weighting phrases as importantly as terms can cause query drift <ref type="bibr" coords="2,442.13,314.66,12.91,9.94" target="#b4">[5]</ref> and a reduction in precision. To reduce query drift, static weighting factors are applied to a phrase reducing the contribution of importance to a documents ranking. These static weighting factors were shown to yield slight improvements in effectiveness <ref type="bibr" coords="2,349.65,352.58,11.99,9.94" target="#b3">[4,</ref><ref type="bibr" coords="2,364.41,352.58,7.99,9.94" target="#b4">5]</ref>. This year we applied two techniques to improve phrase processing. The first is an automatic phrase-weighting algorithm based on the query length and the second is entity tagging using SRA's NetOwl tagger to determine what phrases to use for search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Automatic Statistical Phrases Weighting Based on Query Length</head><p>Statistical phrases are frequently identified at index time by identifying two term pairs that occur at least FIX THIS !X times and do not cross stop words or punctuation. Twenty-five is commonly used as a threshold for the number of documents a phrase must occur in before it is considered a statistical phrase <ref type="bibr" coords="2,225.01,468.74,11.78,9.94" target="#b4">[5]</ref>.</p><p>While the use of phrases is a precision enhancing technique, their naïve usage generally reduces IR effectiveness. When multiple phrases are evaluated for a given query, the likelihood of query drift increases. This drift is caused by phrases overemphasizing a given document that does not contain a breadth of the attributes but only a highly weighted phrase. For an example query of "oil company law suits", the phrases: "oil company", "company law" and "law suits" will overemphasize documents not containing all the terms or phrases and cause nonrelevant documents to receive a higher ranking. This overemphasis causes query drift and the precision of a system decreases. To correct this, we introduce a damping factor of (exp(-1*delta*queryLength) and apply it to the actual contribution any phrases can supply to a given document. In Equation 1 the complete weighting for a phrase is given.</p><formula xml:id="formula_0" coords="2,91.72,632.50,368.91,33.66">( ) ( ) ∑         + + + + qtf nidf avgdocsize docsize * * h) queryLengt * delta * exp(-1 * / * 2 . 8 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ln(tf)) ln(1 1 1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Equation 1: Phrase Ranking Algorithm</head><p>Where:</p><p>• tf = frequency of occurrences of the term in the document • qtf = frequency of occurrences of the term in the query • docsize = document length • avgdoclength = average document length • N = is the number of documents in the collection • n = is the number of documents containing the word</p><formula xml:id="formula_1" coords="3,90.04,152.26,96.81,14.39">• nidf = log(N+1/n)</formula><p>Our hypothesis is that as the number of phrases increase for a query, the likelihood of query drift due to a highly weighted phrase increases. Thus, by adaptively weighting phrases based on query length, we can improve precision by reducing the likelihood of drift. We ran tuning experiments with the TREC 6, 7 and 8 short (title only) queries. We measured the effectiveness of the various runs with no phrases and phrases with various static weights and dynamic weights.</p><p>By keeping the phrase weight set to one (equivalent to the weight given to terms) our average precision is reduced by almost 5%. Other researchers have experienced this same result <ref type="bibr" coords="3,482.23,269.66,11.99,9.94" target="#b3">[4,</ref><ref type="bibr" coords="3,497.00,269.66,8.00,9.94" target="#b4">5]</ref>. By reducing our phrase weight by a factor of .5 and .25 our effectiveness improves. While other groups have chosen a fixed static weight of 0.5, short queries continue to improve to 0.25. Table <ref type="table" coords="3,90.03,307.58,5.52,9.94" target="#tab_0">1</ref> shows the average precision for phrase weights of 1, .5, and .25. Our adaptive phrase weighting enables us to avoid tuning for phrases. A dynamic weighting based on query length determines the likelihood that the phrase will contribute to the weight. Our dynamic approach yields an improvement of 12% over the statically tuned approach on average for the 150 queries. All IIT runs this year use the given phrase weighting approached described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No Phr</head><p>Pwt -1 Pwt -. For all queries, we used our new weighted statistical phrase processing. In addition, for indexing, we used a modified porter stemmer and conflation class stemming system. This year's baseline title only experiment was iit01t. For our submitted run, we used a modified pivoted document length ranking strategy. We used Rocchio positive feedback using 15 terms from the top 10 documents selected in pass one and each new query term was given a factor of .25. In addition, we used the TREC disks 4-5 for collection enrichment with Rocchio positive feedback of 15 terms from the top 10 documents and a weighting of 0.15. Our run with feedback and collection enrichment is shown in Figure <ref type="figure" coords="3,227.06,681.86,5.52,9.94">1</ref> below. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Query Entity Tagging</head><p>We also tested the impact of using an entity tagger over statistical phrases. Tagging a large document collection is difficult with existing entity-taggers because they are not designed for scalability. We were able to tag queries very quickly. The idea was to take the entities tagged in the query and derive two-term phrases from these entities. Hence, a query with "federal housing authority" that has this tagged as a single entity would result in the phrases "federal housing" and "housing authority" to be derived from this tag.</p><p>We encountered several problems with this approach. Many queries are not long enough for entity taggers to accurately tag the query terms. Worse, not all queries contain entities that provide useful knowledge of which phrases to use for query processing. To further examine our strategy we used the description of the query instead of only the short titles. Only five of the fifty queries contained entities that were tagged by the NetOwl tagger that could be used for query processing. The five queries and their tags are shown in Table <ref type="table" coords="4,369.89,452.30,4.15,9.94" target="#tab_3">3</ref>. When an entity was encountered, all terms within it were combined as phrases. For query 505 "Edmund Hillary" is identified as a useful phrase, for query 510, "J. Robert Oppenheimer" is found, and for query 527 "Booker T. Washington" is identified as a single phrase. Finally, query 538 has "Federal Housing Authority". Because our index includes only two term phrases, we generate two term phrases from these entities. Future work will focus on tagging the entities in the corpus for indexing. That way, Washington as a name will be distinguished from Washington as a place in both the queries and the index and can be used as a filter.</p><p>QUERY 505: WHO IS/WAS &lt;PERSON TYPE="PERSON" FIRSTNAME="EDMUND" LASTNAME="HILLARY" GENDER="MALE"&gt;EDMUND HILLARY&lt;/PERSON&gt;? Query 510: Find biographical data on &lt;PERSON TYPE="PERSON" FIRSTNAME="J. ROBERT" LASTNAME="OPPENHEIMER" GENDER="MALE"&gt;J. Robert Oppenheimer&lt;/PERSON&gt;. Query 515: What did &lt;PERSON TYPE="PERSON" FIRSTNAME="ALEXANDER GRAHAM" LASTNAME="BELL" GENDER="MALE"&gt;Alexander Graham Bell&lt;/PERSON&gt; invent? Query 527: What biographical data is available on &lt;PERSON TYPE="PERSON" FIRSTNAME="BOOKER T." LASTNAME="WASHINGTON" GENDER="MALE"&gt;Booker T. Washington&lt;/PERSON&gt;? Query 538: Find documents describing the &lt;ENTITY TYPE="ENTITY" SUBTYPE="GOVERNMENT"&gt;Federal Housing Administration&lt;/ENTITY&gt; (&lt;ENTITY TYPE="ENTITY" SUBTYPE="GOVERNMENT"&gt;FHA&lt;/ENTITY&gt;): when and why it was originally established and its current mission. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Summary</head><p>For TREC-10's ad-hoc task, we focused on effectiveness for short queries for the web track. This year we focused on query processing techniques and fusion approaches. Our initial results are both positive and negative in nature with an overall strong performance in the adhoc title-only task Thirty-two queries of fifty were judged over the median.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Manual Task</head><p>For the manual WEB Track, IIT expanded upon work from prior years. Our overall results are summarized in the following chart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Above Median</head><p>At Median Below Median 33 3 14</p><p>Research focused on the use of concepts and manual relevance feedback. Additionally, a new user interface was developed. As with previous years, we implemented required and scoring concepts. All fifty topics had at least one required concept. A concept is represented as a set of words from which a document must contain at least one word. Eighteen topics contained two required concepts (documents must contain at least one entry from each list. Forty-six topics have scoring concepts, or concepts that contribute to relevance but do not identify new documents. Table <ref type="table" coords="5,174.16,606.74,5.52,9.94" target="#tab_3">3</ref> summarizes our experiments related to concepts. While the use of multiple required concepts only provided a modest boost to average precision, the probability of achieving the best average precision doubled. The median average precision for all teams was 0.1665 for our topics with two required concepts, while the median was 0.1997 for topics where we used one topic, indicating the two concept topics were somewhat more difficult. We also tested the effect of manual relevance feedback. Manual relevance feedback involved reading some number of documents and selectively modifying queries based upon what was read.</p><p>To do this, we split the topics into three groups. For the most "top" group, we read at least 100 documents per topic, with a maximum of 156. For the middle group we read between 50 and 99 documents. Finally, we read from zero to 49 documents for the group with minimal relevance feedback. We reviewed a little under 10% of returned documents. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4 Manual Relevance Feedback Results</head><p>Final results were re-ranked based upon user assessment. User assessed "Relevant" documents contained all elements of topic, "Probably Relevant" contained most elements, or loosely addressed all elements. Documents assessed "Probably not relevant" contained some reference to the topic but did not seem related, while "Not Relevant" were completely unrelated. Relevance score lowered by 0.9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 5 Relevance Assessments from our Manual User 4 Homepage Finding</head><p>This year our group participated in the new site finding task. For a baseline run, we indexed the title terms from the document collection and ran an initial query pass using our basic adhoc retrieval strategy. In addition, the source URL's for each result document were cleaned to remove extraneous words and characters so they would adhere to a typical URL format. After having retrieved the results from our initial query pass, we used three techniques to augment and improve the result set: TAND, Co-occurrence Boosting, URL-folding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">TAND Initial Results at Thirty Percent</head><p>The results from the initial query pass were TAND'ed. In order for a candidate result document to remain in the result set, it had to contain a minimum of thirty percent of the query terms. This technique was used as a coarse-grained filter, eliminating result documents that had little chance of being relevant. We arrived at thirty percent and all other thresholds by calibrating with the training site-finding set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Boosting on Result Co-Occurrence</head><p>Along with our primary title-only index, we created several other indexes that were used for a form of collection enrichment. These included:</p><p>• ODP Descriptions -We crawled the hierarchy of the Open Directory Project (www.dmoz.org) and created an index of the description terms for each entry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• ODP Anchor Text -An index of the anchor text used for hyperlinks in the Open</head><p>Directory Project • First-100 -An index of the first one hundred terms from each document in the WT10G.</p><p>After the TAND'ing of the result sets from the initial query pass was complete, we ran a query pass against each of these three indexes, and used the following algorithm to "boost" results in the initial result set: "# For the top thirty results from the ODP description query, we checked the URL for the result document in question against the result set from our initial query pass. "# If it was present in the initial query pass, the score for the document in the initial result set was increased by 85% "# If it was not present in the initial query pass, but a document with the same URL was confirmed to exist in the WT10g collection, that document was added to the initial result set with the unmodified weight from the ODP Description result set. "# This process was repeated for the two additional indexes in the following order, with the following parameters: "# ODP Anchor Text: Examined the top sixty results and boosted matches by 50% "# First-100: Examined the top sixty results and boosted matches by 60% TAND'ing and Boosting improved our baseline mean reciprocal rank by approximately 70%. It should be noted that the order in which the boosting indexes were queried is very important, as potential results could have been boosted multiple times depending on which source located them first.</p><p>The order in which the boosting indexes were queried, and the various boosting factors and number of results examined were determined experimentally by performing a large number of calibrations using the supplied training data for the Homepage finding task. Essentially, the numbers describe the measure of confidence we placed in the ability of each source to yield relevant results. We found that the ODP indexes, potentially due to the large amount of human oversight and interaction, were trustworthy. By contrast, the index of the first one hundred terms was shown to be less likely to contain highly relevant results, probably due to the presence of large quantities of "noise" information that is often present in the first terms of a web page, such as advertisements, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Folding</head><p>The final technique we used on the boosted result set was our URL-folding algorithm. The idea here is to combine results from the same site in the ranked list so as to order them in a reasonable way. We refer to pages on a web site in terms of parent-child relationships. A parent page is shallower in the site hierarchy (e.g.; ir.iit.edu) while a child page is deeper (e.g.; ir.iit.edu/researchers). Folding took place as follows:</p><p>a. Parent occurs higher in the result set than child: child is removed from result set and parent's score is increased b. Child occurs higher than parent: parent score is increased, but child is left in its original position of the result set.</p><p>Relevance score modifications were performed for each parent according to the following equation: After experimenting with this scheme, we found a paradox: Many parent pages had too many children above them in the rankings, but increasing the increments by which parents were weighted caused parents with many children to be ranked too highly. To provide finer-grained tuning of how parents had their ranks increased, we added a final step to our algorithm that occurred after all folding had been completed. In this step, we moved parent pages that had unfolded child pages of within 35% of the parent's score just above those unfolded children in the result ranking. We also guaranteed that parent pages had at most three unfolded children above them in the ranking, regardless of their relevance.</p><formula xml:id="formula_2" coords="8,104.92,346.88,73.04,26.94">( ) ∑ + =</formula><p>After attending TREC, we performed some failure analysis on our techniques, in an effort to discover why there was such a large disparity between our performance on the training queries, and our performance on the supplied topics. This failure analysis revealed some deficiencies in our query and document parsers, and also confirmed that there is a high degree of overlap in the improvements observed from our boosting and folding techniques.</p><p>Our experimental results for both the training data and the actual homepage topics for each approach are shown in Table <ref type="table" coords="8,220.77,608.78,4.15,9.94" target="#tab_6">6</ref>. The improvements resulting from our post-conference failure analysis are also included. All values express the mean reciprocal rank over the query set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Set</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Arabic Monolingual and Cross-lingual Track</head><p>For the Cross-Lingual Arabic Information retrieval, our automatic effort concentrated on the two categories; English-Arabic Cross-Language Information Retrieval (CLIR) and monolingual information retrieval. For the English-Arabic CLIR we used two types of dictionary-based query translation: Machine-Readable Dictionary (MRD) and Machine Translation (MT). The First-Match (FM) technique is used for term selection from a given entry in the MRD <ref type="bibr" coords="9,446.97,276.14,11.76,9.94" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Monolingual</head><p>For the monolingual run, we used two stemming algorithms. The first algorithm is root-based, and second is light stemming. In the root-based algorithm, the main aim is to detect the root of the given word. When no root is detected, the algorithm retains the given word intact. The rootbased algorithm is aggressive. For example, the root of office, library, book, and write is the same, thus, the root-based algorithm places these in the same conflation class. Accordingly, a light-stemming algorithm is developed. It is not as aggressive as the root-based algorithm. The idea of this technique is to strip out the most common affixes to the Arabic words. For example, it returns the plural, dual to their singular form except for irregular pluralization.</p><p>Our monolingual run is described in Table <ref type="table" coords="9,279.92,431.42,4.15,9.94" target="#tab_7">7</ref>. This run did reasonably well, with 21 queries above the median, 1 at the median and three below. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Average Precision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">English-Arabic Cross-Language information Retrieval</head><p>We conducted our experiments by using two approaches for query translation. The first approach is the Machine-Readable Dictionary (MRD). The second approach is Machine Translation (MT).</p><p>In MRD realm, we use the first match in the bilingual dictionary as the candidate translation of the source query term. This approach ignores many noise terms introduced by the MRD. Al-Mawrid English-Arabic is used for the translation process <ref type="bibr" coords="9,348.79,633.02,11.77,9.94" target="#b8">[9]</ref>.</p><p>In MT realm, the translation was performed on every field of the topic individually. We performed our experiment by using a commercial MT system product. It is called Al-Mutarjim Al-Arabey. It is developed by ATA Software Technology Ltd <ref type="bibr" coords="9,365.64,683.54,16.96,9.94">[10]</ref>. The post-translation expansion technique is used to de-emphasize the extraneous terms that are introduced to the source query after translation.</p><p>Our cross-lingual run is described in Table <ref type="table" coords="10,282.00,87.02,4.15,9.94" target="#tab_8">8</ref>. Our run has 17 queries above the median, zero at the median and eight below. There are 3 queries where our run is the best. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Average Precision</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,90.04,385.60,403.69,191.84"><head>Table 1 : Phrase Weighting Evaluation Runs (Short Queries) 2.3 Ad-Hoc TREC 10 Experiments Our</head><label>1</label><figDesc></figDesc><table coords="3,90.04,385.60,397.69,191.84"><row><cell></cell><cell></cell><cell cols="3">5 Pwt -.25</cell><cell>Pwt -Sig</cell><cell>No-&gt;.25</cell><cell>No-&gt;Sig</cell></row><row><cell>T6 22.37%</cell><cell>21.02%</cell><cell>22.59%</cell><cell cols="2">23.03%</cell><cell>23.13%</cell><cell>2.95%</cell><cell>3.40%</cell></row><row><cell>T7 17.57%</cell><cell>15.51%</cell><cell>16.94%</cell><cell cols="2">17.68%</cell><cell>17.73%</cell><cell>0.63%</cell><cell>0.91%</cell></row><row><cell>T8 23.85%</cell><cell>24.09%</cell><cell>24.47%</cell><cell cols="2">24.58%</cell><cell>24.60%</cell><cell>3.06%</cell><cell>3.14%</cell></row><row><cell>Avg 21.26%</cell><cell>20.21%</cell><cell>21.33%</cell><cell cols="2">21.76%</cell><cell>21.82%</cell><cell>2.21%</cell><cell>2.48%</cell></row><row><cell></cell><cell>Above</cell><cell cols="2">At Median,</cell><cell cols="2">Below</cell><cell></cell></row><row><cell></cell><cell>Median</cell><cell></cell><cell></cell><cell cols="2">Median</cell><cell></cell></row><row><cell></cell><cell>32</cell><cell>1</cell><cell></cell><cell>17</cell><cell></cell><cell></cell></row></table><note coords="3,109.94,516.02,383.80,9.94"><p>overall results for Trec-10 Ad Hoc experiments are summarized in the following chart.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,90.04,303.16,148.65,9.94"><head>Table 2 : Entity Tagged Queries</head><label>2</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,90.04,100.22,352.19,93.59"><head>Table 3 : Average Precision for Manual Queries</head><label>3</label><figDesc></figDesc><table coords="6,93.76,100.22,348.47,61.54"><row><cell>Required</cell><cell>Number</cell><cell>Avg</cell><cell>Best</cell><cell>At or Above</cell><cell>Below</cell></row><row><cell>Concepts</cell><cell>of Queries</cell><cell>Precision</cell><cell></cell><cell>Median, not</cell><cell>Median</cell></row><row><cell></cell><cell>in Set</cell><cell></cell><cell></cell><cell>best</cell><cell></cell></row><row><cell>1</cell><cell>32</cell><cell>0.3226</cell><cell>7</cell><cell>17</cell><cell>8</cell></row><row><cell>2</cell><cell>18</cell><cell>0.3499</cell><cell>8</cell><cell>5</cell><cell>5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,90.04,272.18,420.10,125.73"><head></head><label></label><figDesc>Table4summarizes the results for manual relevance feedback. It can be seen that reading numerous documents had an impact on whether or not we had the best query.</figDesc><table coords="6,91.48,323.18,366.47,74.73"><row><cell>Documents</cell><cell>Number of</cell><cell>Avg</cell><cell>Best Avg</cell><cell>At or Above</cell><cell>Below</cell></row><row><cell>Read</cell><cell>Queries in</cell><cell>Precision</cell><cell>Precision</cell><cell>Median, not</cell><cell>Median</cell></row><row><cell></cell><cell>Set</cell><cell></cell><cell></cell><cell>best</cell><cell></cell></row><row><cell>100+</cell><cell>10</cell><cell>0.4714</cell><cell>7</cell><cell>2</cell><cell>1</cell></row><row><cell>50-99</cell><cell>25</cell><cell>0.3187</cell><cell>4</cell><cell>15</cell><cell>6</cell></row><row><cell>0-49</cell><cell>15</cell><cell>0.2628</cell><cell>4</cell><cell>5</cell><cell>6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,90.04,483.02,410.66,138.94"><head>Table 5</head><label>5</label><figDesc>below shows our in-house assessments of the result documents.</figDesc><table coords="6,90.04,521.68,369.47,100.28"><row><cell>User</cell><cell>Documents</cell><cell>Ranking Adjustment</cell></row><row><cell>Assessment</cell><cell></cell><cell></cell></row><row><cell>Relevant</cell><cell>598</cell><cell>Ranked above all other documents returned</cell></row><row><cell>Probably</cell><cell>523</cell><cell>Relevance score boosted by 0.25</cell></row><row><cell>Relevant</cell><cell></cell><cell></cell></row><row><cell>Probably not</cell><cell>612</cell><cell>Relevance score lowered by 0.5</cell></row><row><cell>Relevant</cell><cell></cell><cell></cell></row><row><cell>Not Relevant</cell><cell>1678</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,90.04,87.76,431.99,99.94"><head>Table 6 Results of Site Finding Task (MRR)</head><label>6</label><figDesc></figDesc><table coords="9,90.04,87.76,431.99,61.88"><row><cell></cell><cell></cell><cell>Baseline +</cell><cell>Baseline +</cell><cell>Baseline + Boost</cell></row><row><cell></cell><cell></cell><cell>Boosting</cell><cell>Folding (iit01st)</cell><cell>+ Fold (iit01stb)</cell></row><row><cell>Training Data</cell><cell>.590</cell><cell>.725</cell><cell>.670</cell><cell>.880</cell></row><row><cell cols="2">Homepage Topics .253</cell><cell>.503</cell><cell>.559</cell><cell>.578</cell></row><row><cell cols="2">Topics -Improved .373</cell><cell>.519</cell><cell>.561</cell><cell>.664</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="9,94.24,483.52,406.77,48.94"><head>Table 7 Monolingual run Using Light Stemming</head><label>7</label><figDesc></figDesc><table coords="9,94.24,483.52,406.77,22.88"><row><cell>Best</cell><cell>Median Worst</cell><cell cols="2">iit01mlr Above</cell><cell>At</cell><cell>Below</cell><cell>Best</cell><cell>Worse</cell></row><row><cell>0.5118</cell><cell>0.2516 0.0216</cell><cell>0.4288</cell><cell>21</cell><cell>1</cell><cell>3</cell><cell>3</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="10,90.04,176.78,410.99,55.31"><head>Table 8 CLIR result using Mutarjim Al-Arabey MT system</head><label>8</label><figDesc></figDesc><table coords="10,94.24,176.78,406.79,23.26"><row><cell>Best</cell><cell>Median Worst</cell><cell cols="2">iit01xma Above</cell><cell>At</cell><cell>Below</cell><cell>Best</cell><cell>Worse</cell></row><row><cell>0.5623</cell><cell>0.1701 0.0001</cell><cell>0.3119</cell><cell>17</cell><cell>0</cell><cell>8</cell><cell>3</cell><cell>0</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,105.86,297.74,416.21,9.94;10,90.04,310.46,86.52,9.94" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,263.98,297.74,204.36,9.94">A vector-space model for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,479.20,297.74,42.86,9.94;10,90.04,310.46,35.85,9.94">Comm. of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,107.31,335.66,414.98,9.94;10,90.04,348.38,361.08,9.94" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,295.06,335.66,184.79,9.94">Pivoted Document Length Normalization</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,493.54,335.66,28.75,9.94;10,90.04,348.38,329.14,9.94">ACM-SIGIR Conference on Research and Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,107.18,373.70,414.92,9.94;10,90.04,386.30,419.22,9.94" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,167.48,373.70,204.68,9.94">Relevance Feedback in Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rocchio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,380.68,373.70,141.42,9.94;10,90.04,386.30,143.43,9.94">Smart System -Experiments in Automatic Document Processing</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1971">1971</date>
			<biblScope unit="page" from="313" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.87,411.62,416.25,9.94;10,90.04,424.22,101.12,9.94" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,225.16,411.62,256.74,9.94">Statistical Phrases for Vector-Space Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Turpin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,493.37,411.62,28.75,9.94;10,90.04,424.22,25.98,9.94">ACM-SIGIR</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="309" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,106.83,449.54,415.15,9.94;10,90.04,462.14,432.00,9.94;10,90.04,474.86,24.84,9.94" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,342.48,449.54,179.50,9.94;10,90.04,462.14,31.99,9.94">An Analysis of Statistical and Syntactic Phrases</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,136.12,462.14,105.27,9.94">Fifth RIAO Conference</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.73,500.18,343.47,9.94" xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Nist</forename><surname>Trec</surname></persName>
		</author>
		<idno>trec.nist.gov</idno>
		<imprint/>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.73,525.38,413.32,9.94;10,90.04,538.10,314.60,9.94" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,378.84,525.38,143.21,9.94;10,90.04,538.10,165.91,9.94">Analyses of Multiple-Evidence Combinations for Retrieval Strategies</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Frieder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Mccabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,267.20,538.10,53.90,9.94">ACM-SIGIR</title>
		<imprint>
			<date type="published" when="2001-09">September 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,106.35,563.42,415.69,9.94;10,90.04,576.02,432.04,9.94;10,90.04,588.62,378.21,9.94" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,235.77,563.42,286.27,9.94;10,90.04,576.02,288.41,9.94">Effective Arabic-English Cross-Language Information Retrieval via Machine Readable Dictionaries and Machine Translation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aljlayl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Frieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,393.52,576.02,128.56,9.94;10,90.04,588.62,216.98,9.94">ACM Tenth Conference on Information and Knowledge Managemen (CIKM)</title>
		<meeting><address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-11">November 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.76,619.94,217.56,9.94" xml:id="b8">
	<analytic>
		<title/>
		<ptr target="http://www.malayin.com/" />
	</analytic>
	<monogr>
		<title level="j" coord="10,105.76,619.94,96.92,9.94">Dar El-Ilm Lilmalayin</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
