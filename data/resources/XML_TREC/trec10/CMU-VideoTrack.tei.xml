<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,212.23,76.92,187.40,15.69;1,146.96,97.62,317.94,15.69">Video Retrieval with the Informedia Digital Video Library System</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2002-02-04">2/4/2002</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,90.87,129.11,127.22,12.19"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<country>PA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,229.53,129.11,50.15,12.19"><forename type="first">Rong</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<country>PA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,291.19,129.11,105.20,12.19"><forename type="first">Norman</forename><surname>Papernick</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<country>PA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,407.93,129.11,60.18,12.19"><forename type="first">Dorbin</forename><surname>Ng</surname></persName>
						</author>
						<author>
							<persName coords="1,479.70,129.11,41.25,12.19;1,188.59,145.18,13.96,12.19"><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<country>PA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,214.08,145.18,92.96,12.19"><forename type="first">Ricky</forename><surname>Houghton</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<country>PA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Sonic Foundry -MediaSite Systems Pittsburgh</orgName>
								<address>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,342.20,145.18,76.53,12.19"><forename type="first">Sue</forename><surname>Thornton</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Sonic Foundry -MediaSite Systems Pittsburgh</orgName>
								<address>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,212.23,76.92,187.40,15.69;1,146.96,97.62,317.94,15.69">Video Retrieval with the Informedia Digital Video Library System</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2002-02-04">2/4/2002</date>
						</imprint>
					</monogr>
					<idno type="MD5">F29276EDC4C9569D5C53E0AC8F5256CB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Background: The Informedia Digital Video Library System.</p><p>The Informedia Digital Video Library <ref type="bibr" coords="1,260.44,287.62,11.67,8.74" target="#b0">[1]</ref> was the only NSF DLI project focusing specifically on information extraction from video and audio content. Over a terabyte of online data was collected, with automatically generated metadata and indices for retrieving videos from this library. The architecture for the project was based on the premise that real-time constraints on library and associated metadata creation could be relaxed in order to realize increased automation and deeper parsing and indexing for identifying the library contents and breaking it into segments. Library creation was an offline activity, with library exploration by users occurring online and making use of the generated metadata and segmentation. The goal of the Informedia interface was to enable quick access to relevant information in a digital video library, leveraging from derived metadata and the partitioning of the video into small segments. Figure <ref type="figure" coords="1,505.84,382.64,5.01,8.74">1</ref> shows the IDVLS interface following a query. In this figure, a set of results is displayed at the bottom. The display includes a window containing a headline, and a pictorial menu of video segments each represented with a thumbnail image at approximately ¼ resolution of the video in the horizontal and vertical dimensions. The headline window automatically pops up whenever the mouse is positioned over a result item; the headline window for the first result is shown. IDVLS also supports other ways of navigating and browsing the digital video library. These interface features were essential to deal with the ambiguity of the derived data generated by speech recognition, image processing, and natural language processing. Consider the filmstrip and video playback IDVLS window shown in Figure <ref type="figure" coords="1,192.18,486.07,3.76,8.74">2</ref>. For this actual video in the IDVLS library, the segmentation process failed, resulting in a thirty-minute segment. This long segment was one of the returned results for the query "Mir collision." The filmstrip in Figure <ref type="figure" coords="1,230.24,509.10,5.01,8.74">2</ref> shows that the segment is more than just a story on the Russian space station, but rather begins with a commercial, then the weather, and then coverage of Hong Kong before addressing Mir. By overlaying the filmstrip and video playback windows with match location information, the user can quickly see that matches don't occur until later in the segment, after these other stories that were irrelevant to the query. The match bars are optionally color-coded to specific query words; in Figure <ref type="figure" coords="1,89.97,566.57,5.01,8.74">2</ref> "Mir" matches are in red and "collision" matches in purple. When the user moved the mouse over the match bars in the filmstrip, a text window displayed the actual matching word from the transcript or Video OCR metadata for that particular match; "Mir" is shown in one such text window in Figure <ref type="figure" coords="1,457.17,589.55,3.75,8.74">2</ref>. By investigating the distribution of match locations on the filmstrip, the user can determine the relevance of the returned result and the location of interest within the segment. The user can click on a match bar to jump directly to that point in the video segment. Hence, clicking the mouse as shown in Figure <ref type="figure" coords="1,474.32,624.04,5.01,8.74">2</ref> would start playing the video at this mention of "Mir" with the overhead shot of people at desks. Similarly, IDVLS provided "seek to next match" and "seek to previous match" buttons in the video player allowing the user to quickly jump from one match to the next. In the example of Figure <ref type="figure" coords="1,406.48,658.54,3.76,8.74">2</ref>, these interface features allowed the user to bypass problems in segmentation and jump directly to the "Mir" story without having to first watch the opening video on other topics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>From the 11 hours of video, we extracted about 8000 shots, where a shotbreak was defined as an edited camera cut, fade or dissolve using standard color histogram measures. Instead of documents, the Video TREC track had defined shots as the unit of retrieval. We aggregated the MPEG I-frames for each shot to be alternative images for each shot. Whenever something matched to an image within a shot, the complete shot was returned as relevant. In total, there were about 80,000 images to be searched.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IDVLS Processing Components:</head><p>a. IMAGE PROCESSING SHOT BREAKS: Color histogram analysis is applied to the MPEG-encoded video. This enables the software to identify editing effects such as cuts that mark shot changes. A single representative frame from each shot is chosen for use in poster frames or in the filmstrip view. VIDEO OCR: The majority of traditional image processing techniques like optical character recognition (OCR) assume they work with a single image, but image processing for video works with image sequences where each image in the sequence often changes only slightly from the previous image. An overview of the Informedia Project's Video OCR (VOCR) process illustrates these points; Sato et. al discuss VOCR elsewhere in detail <ref type="bibr" coords="2,167.53,673.17,15.37,8.74" target="#b12">[14]</ref>. The goal of VOCR was to generate an accurate text representation for text superimposed on video frames. The VOCR process is as follows:</p><p>• Identify video frames containing probable text regions, in part through horizontal differential filters with binary thresholding.</p><p>• Filter the probable text region across the multiple video frames where that region is identified as containing approximately the same data. This time-based filter improves the quality of the image used as input for OCR processing.</p><p>• Use commercial OCR software to process the final filtered image of alphanumeric symbols into text.</p><p>Optionally improve the text further through the use of dictionaries and thesauri. The text detection phase can be used in key frame selection heuristics, just as face detection is used. The resulting text from VOCR processing has been used as additional metadata to document the contents of a video segment; this text can be searched just like transcript text. OCR technology has been commercially available for many years. However, reading the text present in the video stream requires a number of processing steps in addition to the actual character recognition. First the text must be detected. The it must be extracted from the image, and finally converted into a binary black and white representation, since the commercially available OCR engines do not recognize colored text on a variably colored background. Since the extraction and binarization steps are quite noisy and do not produce perfect results, we decided to run the OCR engine on every 3 rd frame where text was detected. Thus we obtained over 100 OCR results for a single occurrence of text on the screen that might last for just over 10 seconds. Frequently many of the results would be slightly different from each other, with a very high error rate. On this video collection, the word accuracy for detected text was estimated to be 27%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FACE DETECTION AND MATCHING:</head><p>The Informedia system implements the detection of faces in images as described in <ref type="bibr" coords="3,144.21,318.21,11.69,8.74" target="#b1">[2]</ref> and face matching through 'eigenfaces'. While we experimented with face recognition using a commercial system <ref type="bibr" coords="3,207.14,329.67,16.68,8.74" target="#b20">[22]</ref> as well as an implementation of Eigenfaces <ref type="bibr" coords="3,414.03,329.67,15.29,8.74" target="#b13">[15]</ref>, the accuracy of face recognition in this type of video collection was so poor, that it proved useless. Therefore, we only used a face detector that reported the presence of faces in each key frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IMAGE MATCHING:</head><p>Color histograms have been widely adopted by many image retrieval systems <ref type="bibr" coords="3,487.81,375.68,10.89,8.74" target="#b3">[5,</ref><ref type="bibr" coords="3,501.30,375.68,8.33,8.74" target="#b4">6,</ref><ref type="bibr" coords="3,509.63,375.68,12.50,8.74" target="#b9">11]</ref> and, they served as the initial image query technique available to IDVLS users. While color histograms were applicable to the broad range of images accessible in the IDVLS library, their use in image indexing and retrieval revealed a number of problems. The histograms did not include any spatial information and hence were prone to false positives. Finally, they were unsuited for retrieving images in finer granularities, e.g., particular colors or regions. Referring to Figure <ref type="figure" coords="3,320.00,433.15,3.74,8.74" target="#fig_2">3</ref>, a user looking for a shot of grasslands could instead have retrieved these assorted images of predominantly blue and green colors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b. Audio Processing SPEECH RECOGNITION:</head><p>The audio processing component of our video retrieval system splits the audio track from the MPEG-1 encoded video file, and decodes the audio and downsamples it to 16kHz, 16bit samples. These samples are then passed to a speech recognizer. The speech recognition system we used for these experiments is a stateof-the-art large vocabulary, speaker independent speech recognizer <ref type="bibr" coords="3,360.94,525.12,15.33,8.74" target="#b16">[18]</ref>. For the purposes of this evaluation, a 64000-word language model derived from a large corpus of broadcast news transcripts was used. Previous experiments had shown the word error rate on this type of mixed documentary-style data with frequent overlap of music and speech to be just over 30%. </p><formula xml:id="formula_0" coords="3,89.97,746.96,431.89,8.74">DRAFT -3 - 2/4/2002</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SPEAKER IDENTIFICATION</head><p>Our speaker identification technology is based on standard Gaussian mixture models as defined by <ref type="bibr" coords="4,491.26,631.48,10.61,8.74" target="#b7">[9]</ref>. We use the segmented method with multiple training samples derived from chunks of audio that are 30 seconds in duration. We use weighted rank scoring as defined by Markov and Nakagawa <ref type="bibr" coords="4,413.75,654.46,15.29,8.74" target="#b8">[10]</ref>.</p><p>DRAFT -4 -2/4/2002 c. Text Analysis Titles: Segments are scanned for words that have a high inverse document frequency, and that are strongly distinguishing segments. All text is indexed and searchable. All retrieval of textual material was done using the OKAPI formula <ref type="bibr" coords="5,171.89,109.09,15.36,8.74" target="#b11">[13]</ref>. The exact formula for the Okapi method is shown in Equation ( <ref type="formula" coords="5,448.20,109.09,3.89,8.74" target="#formula_1">1</ref>)</p><formula xml:id="formula_1" coords="5,112.94,119.64,213.18,44.86">∑ ∈               + + + + - = Q qw D qw tf dl avg D qw df qw df N D qw tf D Q Sim ) , ( _ | | 5 . 1 5 . 0 ) 5 . 0 ) ( 5 . 0 ) ( log( ) , ( ) , (<label>(1)</label></formula><p>where tf(qw,D) is the term frequency of word qw in document D, df(qw) is the document frequency for the word qw and avg_dl is the average document length for all the documents in the collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3. Result of a color histogram search</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach to the TREC Video Track</head><p>Our approach to the Video Track in Trec was to use the Informedia system with only minor changes and see how well it would work. We treated general information queries the same as known item queries. Specific modifications are discussed in the sections for the interactive and automatic system. For simplicity, we always assumed that the unit of retrieval was a single shot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Interactive Retrieval System</head><p>Since Informedia only uses static images for image matching, we decided make up for this shortcoming by utilizing multiple image search engines:</p><p>• Histo144v50 Image Search Histo144v50 is based on a simple color histogram of the target image. First, the image is converted to the Munsell color space. We are using the Munsell Color space as described in <ref type="bibr" coords="5,420.28,599.63,10.64,8.74" target="#b6">[8]</ref>. The hue is isolated. Miyahara and Yoshida describe using Godlove's formula to represent the perceptual distance between some colors in the HVC space. We are using Euclidian distance to approximate Godlove's formula. The image is broken into 9 equally sized regions. A 16-bin histogram is taken of each region. The histograms are appended to each other to form a 144 dimensional vector. The vector is then reduced in dimensionality to 50 by multiplying with a previously computed singular value decomposition. Each vector is then placed in a tree data structure that allows K-nearest-neighbors searches.</p><formula xml:id="formula_2" coords="5,89.97,688.77,431.89,66.93">• MCPv50 Image Search DRAFT -5 - 2/4/2002</formula><p>MCPv50 computes the color and texture of the target image. The image is broken into 9 equally sized regions. A 15-bin histogram is taken for the Red, Green, and Blue. Then, six texture histograms of 15 bins each are taken. All of these vectors are append to make a 1215 dim vector. This vector is reduced to 50 dim by multiplying with a previously computed singular value decomposition. Each vector is than placed in a tree data structure that allows K-nearest-neighbors searches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Cuebik Image Search</head><p>Cuebik is based on one of the behaviors of the IBM QBIC image search engine. A palette of 255 colors is chosen for a database by marking the strongest colors found in a large sample of images. The target image is reduced to 256 equally sized regions. Each region is mapped to one of the palette colors, and recorded. A search is done by choosing a set of regions and finding all images that have the same color in the same region.</p><p>In addition to the above image search engines, we also used a downloadable version of the original IBM QBIC system as well as a search engine provided by James Wang from the University of Pennsylvania.</p><p>The search process foe each interactive query was as follows:</p><p>1. Determine key words in the text description of the query and use Video OCR text search to find them. 2. Use the supplied query images to initiate a search for relevant segments.</p><p>3. If a segment key frame or title looks related to the answer, open up its filmstrip and view details. 4. If the segment filmstrip looks related to the topic, but does not provide an answer, look one segment forward and back. If the topic in the adjacent segment is the same, scan the filmstrip of an additional segment forward or back. 5. If a frame answers the query, use that frame for relevance feedback with each of the image search engines to find more like it. 6. If a frame seems to be related, but does not answer the question, use that frame with each of the search engines to find more like it. 7. Repeat all steps as needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Automatic Retrieval</head><p>In the following we will elaborate only on the known item query set, because comprehensive relevance judgments were available for this set allowing automatic estimation of precision and recall for variations of our video retrieval system. The 34 known item queries are distinguished from the remaining 'general search' queries in that the information need tends to be more focused and all instances of query-relevant items in the corpus are known. This allows an experimental comparison of systems without the need for further human evaluations.</p><p>Since the evaluation could be done automatically, the top 100 search results were scored for all systems. The general unit of retrieval was a 'shot', in other words a time range between two shot changes, for example editing cuts and fades. Systems had to determine shot changes automatically. An item was considered relevant if at least 33% of the length of the returned item overlapped with the target item in the list of shots relevant to this query and less than 33% of the time range for the returned item was outside the target range. This requirement ensured a reasonable overlap of the returned shot with the target shot. An example of a typical query is shown in Figure <ref type="figure" coords="6,277.41,566.51,3.77,8.74">4</ref>. This query is to be used for automatic systems, but not for interactive evaluations. It is a known item query, indicating that all results are known inside the video collection. According to the text description, the query is looking for video scenes of water skiing, and gives an example of the type of video that is desired. Two images from the example video are extracted and also shown in Figure <ref type="figure" coords="6,176.07,612.46,3.76,8.74">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Automatic Image Retrieval</head><p>For image similarity matching in the automatic version of the system, we implemented a probabilistic model for image retrieval. To obtain the similarity between the query image I Q and any image I' in the collection, our model computes the probability of generating the image I' given the observation of the query image I Q . The model assumes that images are generated through some stochastic process. Given the observation of an image I, we can find the underlying probabilistic model M that generated this image. The optimal probabilistic model for an image I should maximize the generation probability P(I|M). By assuming that if two images are similar, their underlying generation models should also be similar, we can compute the similarity of image I 1 to image I 2 as P(I 1 | M 2 ), i.e. the probability of generating image I 1 from the statistical model M 2 . Preliminary experiments had shown that this model is more effective for image retrieval from the Video TREC collection than some of the traditional vector methods working on extracted features like e.g. QBIC <ref type="bibr" coords="7,184.95,120.61,11.40,8.74" target="#b4">[6,</ref><ref type="bibr" coords="7,196.35,120.61,11.40,8.74" target="#b9">11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Automatically Combining Metadata</head><p>When the various sources of data were combined for information retrieval, we used a linear interpolation with very high weights on the binary features such as face detection or speaker identification. This allowed these features to function as almost binary filters instead of being considered more or less equal to OCR, speech transcripts or image retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results for the Automatic System Evaluation Metrics</head><p>There are two aspects involved in any retrieval evaluation:</p><p>• Recall. A good retrieval system should retrieve as many relevant items as possible.</p><p>• Precision. A good retrieval system should only retrieve relevant items.</p><p>Many evaluation metrics have been used in information retrieval <ref type="bibr" coords="7,351.23,273.40,16.68,8.74" target="#b19">[21]</ref> to balance these two aspects. In the video retrieval track at TREC, a simple measure of precision at 100 items retrieved was used for scoring the systems. However, since there were only an average of 5.5 items relevant for each query, a perfect retrieval system that returned all relevant items at the top and filled the rest of the top 100 result slots with irrelevant items would only achieve a precision of 5.5 %.</p><p>Because our collection contains only small numbers of relevant items, we adopted the average reciprocal rank (ARR) <ref type="bibr" coords="7,139.89,342.39,16.70,8.74" target="#b21">[23]</ref> as our evaluation metric, similar the TREC Question Answering Track. ARR is defined as follows:</p><p>For a given query, there are a total of N r items in the collection that are relevant to this query. Assume that the system only retrieves k relevant items and they are ranked as r 1 , r 2 , …, r k . Then, the average reciprocal rank is computed as</p><formula xml:id="formula_3" coords="7,226.38,398.67,268.01,26.69">r k i i N r i ARR / / 1       = ∑ = (1)</formula><p>As shown in Equation (1), there are two interesting aspects of the metric: first, it rewards the systems that put the relevant items near the top of the retrieval list and punish those that add relevant items near the bottom of the list. Secondly, the score is divided by the total number of relevant items for a given query. Since queries with more answer items are much easier than those with only a few answer items, this factor will balance the difficulty of queries and avoid the predominance of easy queries. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results for Individual Types of Metadata</head><p>The results are shown in Table <ref type="table" coords="8,215.58,88.39,3.76,8.74" target="#tab_0">1</ref>. The average reciprocal rank (ARR) and recall for retrieval using only the speech recognition transcripts was 1.84% with a recall of 13.2%. Since the queries were designed for video documents, it is perhaps not too surprising that information retrieval using only the OCR transcripts show much higher retrieval effectiveness to an ARR of 5.21% (6.10% recall). The effects of post-processing on the OCR data were beneficial, the dictionary-based OCR post-processing gave a more than 10% boost to 5.93 % ARR and 7.52 % recall. Again, perhaps not too surprisingly, the image retrieval component obtained the best individual result with an ARR of 14.99 % and recall of 24.45 %. Since the face detection could only provide a binary score in the results, we only evaluated its effect in combination with other metadata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results When Combining Metadata</head><p>Combining the OCR and the speech transcripts gave an increase in ARR and recall at </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>What have learned from this first evaluation of video information retrieval? Perhaps it is not too surprising that the results indicate that image retrieval was the single biggest factor in video retrieval for this evaluation. Good image retrieval was the key to good performance in this evaluation, which is consistent with the intuition that video retrieval depends on finding good video images when given queries that include images or video. One somewhat surprising finding was that the speech recognition transcripts played a relatively minimal role in video retrieval for the known-item queries in our task. This may be explained by the fact that discussions among the track organizers and participants prior to the evaluation emphasized the importance of a video retrieval task as opposed to 'spoken document retrieval with pictures'. There was a strong contribution of the OCR data to the final results. The results also underscore the fact that video contains information not available in the audio track. As a previous study noted, only about 50% of the words that appear as written text in the video are also spoken in the audio track <ref type="bibr" coords="8,434.43,472.87,15.27,8.74" target="#b12">[14]</ref>, so the information contained in the text of the pictures is not redundant to the spoken words in the transcripts. Overall, the queries presented a very challenging task for an automatic system. While the overall ARR and recall numbers seem small it should be noted that about one third of the queries were unanswerable by any of the automatic systems participating in the Video Retrieval Track. Thus for these queries nothing relevant was returned by any method or system.</p><p>We would like to caution that the known-item queries do not represent a complete sample of video queries. Video retrieval on general search queries, with less specific information needs, might result in a somewhat different conclusion about the combination of information sources. A preliminary analysis showed that 'general search' queries in the video track tended to be much more 'speech oriented', which is why the best performing system on that set of queries was entirely based on speech recognition transcripts.</p><p>Clearly, we can think of a number of improvements to the speech recognition component, using a parallel corpus for document and query expansion, and relevance feedback. However, the same techniques could be used to improve the OCR transcriptions as well.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.97,481.03,267.30,8.74;2,89.97,71.99,431.87,400.31"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Text Query and Result Set in the Informedia System.</figDesc><graphic coords="2,89.97,71.99,431.87,400.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.97,591.11,380.20,8.74;4,89.97,71.99,321.67,510.39"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Marking the filmstrip of a query with word location in the transcript and OCR.</figDesc><graphic coords="4,89.97,71.99,321.67,510.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,97.58,687.87,281.03,8.74;6,213.43,579.74,105.86,72.71"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. A sample known-item query in the automatic condition.</figDesc><graphic coords="6,213.43,579.74,105.86,72.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="5,89.97,200.49,393.53,229.76"><head></head><label></label><figDesc></figDesc><graphic coords="5,89.97,200.49,393.53,229.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,89.97,492.42,417.12,194.11"><head>Table 1 . Results of video retrieval for each type of extracted data and combinations. Retrieval using: Average Reciprocal Rank Recall</head><label>1</label><figDesc></figDesc><table coords="7,89.97,522.30,417.12,164.23"><row><cell>Speech Recognition Transcripts only</cell><cell>1.84 %</cell><cell>13.2 %</cell></row><row><cell>Raw Video OCR only</cell><cell>5.21 %</cell><cell>6.10 %</cell></row><row><cell>Raw Video OCR + Speech Transcripts</cell><cell>6.36 %</cell><cell>19.30 %</cell></row><row><cell>Enhanced VOCR with dictionary post-processing</cell><cell>5.93 %%</cell><cell>7.52 %</cell></row><row><cell>Speech Transcripts + Enhanced Video OCR</cell><cell>7.07 %</cell><cell>20.74 %</cell></row><row><cell>Image Retrieval only using a probabilistic Model</cell><cell>14.99 %</cell><cell>24.45 %</cell></row><row><cell>Image Retrieval + Speech Transcripts</cell><cell>14.99 %</cell><cell>24.45 %</cell></row><row><cell>Image Retrieval + Face Detection</cell><cell>15.04 %</cell><cell>25.08 %</cell></row><row><cell>Image Retrieval + Raw VOCR</cell><cell>17.34 %</cell><cell>26.95 %</cell></row><row><cell>Image Retrieval + Enhanced VOCR</cell><cell>18.90 %</cell><cell>28.52 %</cell></row><row><cell>Image Retrieval + Face Detection + Enhanced VOCR</cell><cell>18.90 %</cell><cell>28.52 %</cell></row><row><cell>Image Retrieval + Speech Transcripts + Enhanced VOCR</cell><cell>18.90 %</cell><cell>28.52 %</cell></row><row><cell>Image Retrieval + Face Detection + Speech Transcripts</cell><cell>18.90 %</cell><cell>28.52 %</cell></row><row><cell>+Enhanced VOCR</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,89.97,211.67,431.34,112.22"><head></head><label></label><figDesc><ref type="bibr" coords="8,433.94,211.67,4.38,8.74" target="#b4">6</ref>.36 % and 19.30 % respectively. Again post-processing of the OCR improved performance to 7.07 % ARR and 20.74 % recall. Combining speech transcripts and image retrieval showed no gain over video retrieval with just images (14.88 % ARR, 24.45 % recall). However, when face detection was combined with image retrieval, a slight improvement was observed (15.04 % ARR, 25.08 % recall). Combining OCR and image retrieval yielded the biggest jump in accuracy to an ARR of 17.34 % and recall of 26.95 % for raw VOCR and to an ARR of 18.90 % and recall of 28.52 % for enhanced VOCR. Further combinations of image retrieval and enhanced OCR with faces, and speech transcripts yielded no additional improvement. The probably cause for this lack of improvement is the redundancy to the other extracted metadata.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0" coords="7,486.26,746.96,35.60,8.74"><p>2/4/2002</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,107.92,682.11,414.02,8.74;8,107.96,693.63,340.72,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,378.22,682.11,143.72,8.74;8,107.96,693.63,210.67,8.74">Lessons Learned from the Creation and Deployment of a Terabyte Digital Video Library</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">D</forename><surname>Wactlar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Christel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,329.32,693.63,64.12,8.74">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="66" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,107.91,74.60,414.13,8.74;9,107.96,86.11,144.25,8.74" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="9,293.73,74.60,162.42,8.74">Human face detection in visual scenes</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<idno>CMU-CS-95-158</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>CMU</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="9,107.92,128.59,414.01,8.74;9,107.96,140.10,414.04,8.74;9,107.96,151.56,276.89,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,348.29,128.59,173.64,8.74;9,107.96,140.10,146.08,8.74">Artificial Intelligence Techniques in the Interface to a Digital Video Library</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Witbrock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Christel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,261.60,140.10,260.40,8.74;9,107.96,151.56,120.76,8.74">Extended Abstracts of the ACM CHI&apos;97 Conference on Human Factors in Computing Systems</title>
		<meeting><address><addrLine>New Orleans LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-03">March 1997</date>
			<biblScope unit="page" from="2" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,107.96,167.10,413.92,8.74;9,107.96,178.56,107.79,8.74" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="9,154.99,167.10,279.94,8.74">Intelligent Image Databases: Toward Advanced Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Hingham, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,107.92,194.09,414.01,8.74;9,107.96,205.55,393.19,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,469.65,194.09,52.29,8.74;9,107.96,205.55,149.27,8.74">Efficient and Effective Querying by Image Content</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Flickner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Niblack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Petkovic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Equitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,264.00,205.55,169.34,8.74">Journal of Intelligent Information Systems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="231" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,107.94,221.03,414.04,8.74;9,107.96,232.55,205.82,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,280.21,221.03,212.00,8.74">Multimedia Abstractions for a Digital Video Library</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Christel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,499.22,221.03,22.75,8.74;9,107.96,232.55,82.16,8.74">ACM Digital Libraries &apos;97</title>
		<meeting><address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-07">July 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,107.90,248.03,413.93,8.74;9,107.96,259.54,366.81,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,236.21,248.03,285.62,8.74;9,107.96,259.54,15.53,8.74">Mathmatical transform of (R,G,B) color data to Munsell (H,V,C) color data</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Miyahara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,133.79,259.54,20.57,8.74;9,201.13,259.54,186.97,8.74">Visual Communications and Image Processing</title>
		<imprint>
			<biblScope unit="volume">1001</biblScope>
			<biblScope unit="page" from="650" to="657" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
	<note>SPIE</note>
</biblStruct>

<biblStruct coords="9,107.94,275.02,414.04,8.74;9,107.96,286.54,117.49,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,212.54,275.02,161.68,8.74">Text-Independent Speaker Identification</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Gish</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,380.82,275.02,136.50,8.74">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="page" from="18" to="32" />
			<date type="published" when="1994-10">October 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,107.94,302.02,414.09,8.74;9,107.96,313.53,302.43,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,233.88,302.02,288.15,8.74;9,107.96,313.53,182.26,8.74">Frame Level Likelihood Normalization For Text-Independent Speaker Identification using Gaussian Mixture Models</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Markov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,297.35,313.53,27.23,8.74">ICSLP</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="1764" to="1767" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,107.94,329.01,414.07,8.74;9,107.96,340.53,413.99,8.74;9,107.96,352.05,97.60,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,411.06,329.01,110.95,8.74;9,107.96,340.53,151.98,8.74">Efficient Color Histogram Indexing for Quadratic Form Distance</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">S</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Equitz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Flickner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Niblack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,270.79,340.53,220.68,8.74">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="729" to="736" />
			<date type="published" when="1995-07">July, 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,107.94,367.52,414.02,8.74;9,107.96,379.04,221.04,8.74" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kantor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<title level="m" coord="9,232.61,367.52,123.50,8.74;9,112.97,379.04,143.55,8.74">The Fifth Text Retrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note>Report on the Confusion Track. TREC-5</note>
</biblStruct>

<biblStruct coords="9,107.94,394.52,400.03,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,197.71,394.52,67.82,8.74">Okapi at TREC-4</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,284.04,394.52,194.15,8.74">The Fourth Text Retrieval Conference (TREC-4)</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,107.95,410.00,413.94,8.74;9,107.96,421.52,413.98,8.74;9,107.96,433.03,25.84,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,316.94,410.00,158.07,8.74">Video OCR for Digital News Archive</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,499.94,410.00,21.95,8.74;9,107.96,421.52,275.98,8.74">Proc. Workshop on Content-Based Access of Image and Video Databases</title>
		<meeting>Workshop on Content-Based Access of Image and Video Databases<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-01">Jan 1998</date>
			<biblScope unit="page" from="52" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,107.94,448.51,414.01,8.74;9,107.96,460.03,46.46,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,214.70,448.51,208.70,8.74">NAME-IT: Association of Face and Name in Video</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,430.77,448.51,56.80,8.74">IEEE CVPR</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<date type="published" when="1997">1997</date>
			<pubPlace>Puerto Rico</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,107.96,475.51,413.90,8.74;9,107.96,486.96,340.38,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,280.73,475.51,241.13,8.74;9,107.96,486.96,77.45,8.74">GMM sample statistic log-likelihoods for text-independent speaker recognition</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Golden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Gish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,196.03,486.96,53.53,8.74">Eurospeech-9</title>
		<imprint>
			<biblScope unit="page" from="855" to="858" />
			<date type="published" when="1997-09">September 1997</date>
			<pubPlace>Rhodes, Greece</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,107.94,502.50,414.12,8.74;9,107.96,513.96,286.29,8.74" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,268.81,502.50,253.25,8.74;9,107.96,513.96,144.11,8.74">Probabilistic Modeling of Local Appearance and Spatial Relationships of Object Recognition</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schneiderman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,258.78,513.96,46.13,8.74">IEEE CVPR</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<pubPlace>Santa Barbara</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,107.94,529.50,413.96,8.74;9,107.96,540.96,413.87,8.74;9,107.96,552.47,252.82,8.74" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="9,317.29,529.50,204.60,8.74;9,107.96,540.96,266.06,8.74">Speech in Noisy Environments: Robust Automatic Segmentation, Feature Extraction, and Hypothesis Combination</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,388.11,540.96,133.72,8.74;9,107.96,552.47,119.00,8.74">IEEE Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Salt Lake City, UT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-05">May, 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,107.95,567.95,414.03,8.74;9,107.96,579.47,414.05,8.74;9,107.96,590.99,93.61,8.74" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="9,385.14,567.95,136.85,8.74;9,107.96,579.47,105.10,8.74">Content-Based Image Retrieval at the End of the Early Years</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,224.30,579.47,221.20,8.74">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1349" to="1380" />
			<date type="published" when="2000-12">December, 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,107.94,606.46,414.01,8.74;9,107.96,617.98,22.58,8.74" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="9,240.03,606.46,60.64,8.74">Color Indexing</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,313.04,606.46,100.28,8.74">Int&apos;l J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="32" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,107.95,633.46,414.01,8.74;9,107.96,644.98,246.11,8.74" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="9,215.21,633.46,298.80,8.74">The Pragmatics of Information Retrieval Experimentation, revised</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Tague-Sutcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,107.96,644.98,162.81,8.74">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="467" to="490" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,107.92,660.45,375.77,8.74" xml:id="b20">
	<monogr>
		<ptr target="http://www.visionics.com" />
		<title level="m" coord="9,107.92,660.45,246.08,8.74">Visionics Corporate Web Site, FaceIt Developer Kit Software</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,107.94,675.93,414.04,8.74;9,107.96,687.45,153.27,8.74" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="9,247.15,675.93,195.27,8.74">The TREC-8 Question Answering Track Report</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Tice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,454.46,675.93,67.52,8.74;9,107.96,687.45,126.03,8.74">The Eighth Text Retrieval Conference (TREC-8)</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
