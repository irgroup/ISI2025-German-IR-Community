<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,135.84,72.09,339.46,15.68">NTT Question Answering System in TREC 2001</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,177.60,104.64,74.82,10.87"><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
							<email>kazawa@cslab.kecl.ntt.co.jp</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">NTT Communication Science Laboratories</orgName>
								<orgName type="institution" key="instit2">NTT Corporation</orgName>
								<address>
									<addrLine>2-4 Hikaridai</addrLine>
									<settlement>Seikacho, Sorakugun, Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,262.32,104.64,72.34,10.87"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
							<email>isozaki@cslab.kecl.ntt.co.jp</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">NTT Communication Science Laboratories</orgName>
								<orgName type="institution" key="instit2">NTT Corporation</orgName>
								<address>
									<addrLine>2-4 Hikaridai</addrLine>
									<settlement>Seikacho, Sorakugun, Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,361.20,104.64,72.51,10.87"><forename type="first">Eisaku</forename><surname>Maeda</surname></persName>
							<email>maeda@cslab.kecl.ntt.co.jp</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">NTT Communication Science Laboratories</orgName>
								<orgName type="institution" key="instit2">NTT Corporation</orgName>
								<address>
									<addrLine>2-4 Hikaridai</addrLine>
									<settlement>Seikacho, Sorakugun, Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,135.84,72.09,339.46,15.68">NTT Question Answering System in TREC 2001</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">946E0F294DCE0FCD5FAF2A777C2389D2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this report, we describe our question-answering system SAIQA-e (System for Advanced Interactive Question Answering in English) which ran the main task of TREC-10's QA-track. Our system has two characteristics (1) named entity recognition based on support vector machines and ( <ref type="formula" coords="1,166.40,230.66,3.92,8.96">2</ref>) heuristic apposition detection. The MPR score of the main task is 0.228 and experimental results indicate the effectiveness of the above two steps in terms of answer extraction accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>To design a QA system, there are several choices to make. One is about what kind of technology the system should be based on. To date, some research works have attempted the Information Retrieval (IR) approach, assuming that the most relevant passages include the answers of questions. Generally speaking, this IR approach is fast and robust, but is unable to specify the 'exact answer', i.e., what part of the passage is really the answer. Another approach is the Information Extraction (IE) approach, where the system extracts candidate strings from documents and evaluates the validity of the candidates. This approach has the advantage of being able to specify the locations of exact answers although it is usually slow and often fails because of complicated natural language processing.</p><p>For TREC-10's QA track, we adopted the IE approach because we think that knowing the exact locations of answers is one of the important goals of QA. It also seems easier to reach the goal with the IE approach. To avoid 'deep' natural language processing, we decided to use only shallow linguistic analysis, i.e., part-of-speech tagging and base noun phrase (NP) chunking. To proceed with this decision, we mainly focused on the following problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Learning extraction rules</head><p>Shallow linguistic analysis only gives 'low level' information and writing extraction rules manually with this information is quite a complicated job. Additionally, the written rules often lack readability and are hard to maintain. Therefore, we applied a machine learning method, support vector machines (SVM), to learn some extraction rules. SVM has shown a high performance in many pattern recognition and natural language processing tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Detecting apposition</head><p>To answer a certain kind of question, detecting an appositive relation is very important. As we looked further into this issue, however, we found that such detection is not easy because apposition is often determined by long-range constraints in sentences and cannot be identified only by neighborhood information. We therefore created a simple but effective heuristics to detect appositive relations. This paper is organized as follows. In Section 2, we briefly describe the overall structure of our QA system SAIQA-e (System for Advanced Interactive Question Answering in English). Then, we </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System overview</head><p>SAIQA-e first classifies a question into several categories (Figure <ref type="figure" coords="2,376.56,424.79,3.87,9.96" target="#fig_0">1</ref>). Then, it passes each question to an answer extraction subsystem, one that is specific to the question's category. Each extraction subsystem extracts candidate answers from a document set and scores them by heuristic measures. Finally, these candidate answers are merged when multiple candidates are located within a 50-byte length. If a question is categorized as 'unknown' or a specific solver does not extract five candidates, the system evokes a passage retrieval system, which extracts 50-byte passages. These passages are added to the candidates.</p><p>In the following, we describe the details of each component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Question analysis</head><p>In the question analysis stage, each question is classified into one of the categories shown in Figure <ref type="figure" coords="2,91.20,566.63,3.87,9.96" target="#fig_1">2</ref>. The category is determined by a manually created decision tree and the following features of the question: question words (who, what, when,...), positions of the question words (start/middle/end of the question), first verb, head word of the first NP, head word of the second NP, and the word between the first and the second NPs.</p><p>Here, we explain the question categories.  • Description This category has two subcategories, Person and Others.</p><p>The distinction between Name-Person and Description-Person might be a little confusing, so let us present examples. "Who is the U.S. president?" is a Name-Person question because it asks about the name of a person who is the U.S. president. On the other hand, "Who is George W. Bush?" is a Description-Person question because it requires descriptive information about a person whose name is George W. Bush.</p><p>• Quantity This category has nine subcategories: Count, Age, Duration, Length, Money, Ratio, Size, Speed, and Others. As you can see, this category is rather broad and contains few related concepts. However, the expressions of these concepts are usually associated with numerical words and accordingly their extraction steps are expected to be similar. Based on this, we grouped these subcategories into one Quantity category.</p><p>• Date This category has four subcategories: Day, Day of the week, Year, and Others.</p><p>• Paraphrase This category has three subcategories: Abbreviation, Fullname, and Nickname. The category is created because these expressions are often related to the original expressions in unique ways (for example, an abbreviation follows an original expression in parentheses) and can be identified in a unified fashion.</p><p>The questions that are not classified into any above categories are labelled as 'Unknown' questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Answer extraction and evaluation</head><p>After the question is categorized, the answers are extracted from a document set and evaluated by heuristic measures. We have several extraction subsystems, each of which is intended to deal with only restricted types of questions. Each question is passed to the corresponding subsystem, while 'Unknown' questions skip this extraction step and are passed directly to the next answer integration step. Here, we describe the subsystems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Principal Name Solver</head><p>This subsystem deals with Name-Person, Name-Location, and Name-Organization questions. It was separated from the other Name solver because detecting names of person/location/organization in documents is harder than other name detection and we wanted to focus our resources on this problem.</p><p>The subsystem first retrieves articles ranked by the fequency of keywords and their proximity. Then, an SVM-trained named entity detection module extracts person/location/organization names from these articles. These names are evaluated by heuristic measures such as the proximity to the keywords.</p><p>The issue of SVM learning of named entity detection is discussed in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Auxiliary Name Solver</head><p>This subsystem deals with Name-Others questions. The extraction and evaluation are similar to Principal Name Solver's, but the name detection rules are manually created. Additionally, the evaluation heuristics are less accurate because Name-Others questions cover such diverse kinds of entities that it is hard to develop accurate category-specific measures such as those used in Principal Name Solver.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Description Solver</head><p>This subsystem accepts Description-Person and Description-Others questions. The extraction and evaluation are quite different from the name solvers'.</p><p>The subsytem first retrieves all articles including the name of the requested entity. (It is easy to identify the name in the question.) Then, the NPs appositively connected to the name are extracted as the descriptive answers. The answers with the same head are grouped as variant expressions of the same description. Finally, the most specific expressions of the groups are scored by the number of group members. (That is, a more frequent description is considered to be more trustable.) Apposition detection plays the main role in Description Solver. We discuss this in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantity/Date Solver</head><p>These subsystems deal with Quantity and Date questions. They are almost the same as Auxiliary Name Solver and the differences are in the extraction rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paraphrase Solver</head><p>This solver deals with Paraphrase questions and the subsystem is quite different from other solvers.</p><p>For example, for Paraphrase-Abbreviation questions (for example, "What is the abbreviation for the United Nations"), it retrieves all articles in which the fullname (United Nations) appears. Then, a regular expression is used to extract all abbreviations from the articles. Finally, a sequence of upper characters in the fullname (UN) is compared to a sequence of upper characters in the abbreviations. This comparison is done approximately so that some missing characters are tolerated and the matching degree is translated into the score of the abbreviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Answer integration</head><p>After the answer extraction and evaluation stage, the answers are extended to 50 bytes and merged when the 50-byte passages contain multiple answers. Then, we add 'no answer' in the following manner.</p><p>1. If a question is classified into a category other than 'unknown' and the specific solver does not return as many as five answers, then 'no answer' (i.e., 'NIL') is added to the candidates. After that, the output of the passage retrieval system is added.</p><p>2. If a question is classified into the 'unknown' category and the passage retrieval system does not return as many as five answers, then 'no answer' (i.e., 'NIL') is added to the candidates.</p><p>We set all 'final answers' as '1', because SAIQA-e's outputs have already been sorted according to their relevance and we consider the first-ranked answer as the most trustable one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Named Entity Recognition based on Support Vector Machines</head><p>Named entity (NE) recognition systems are useful for determining whether a certain pronoun designates a person or an organization or location. Although we have had our own Japanese NE systems, we did not have any experience on developing English NE systems. Therefore, we decided to develop one by using a corpus-based approach. Since we did not have any training data for English NE tasks, we prepared our own training data. We employed support vector machines (SVM) for the English NE system. Such a system was proposed by Yamada et al. <ref type="bibr" coords="5,208.32,329.27,39.58,9.96" target="#b5">[YKM01]</ref> for Japanese NE recognition. His system is a simple application of Kudo's chunking system <ref type="bibr" coords="5,214.08,341.27,32.11,9.96" target="#b2">[KM01]</ref> that shows the best performance for the CoNLL-2000 shared task. We also implemented an SVM-based NE system for Japanese. This SVM-based NE system employs a different approach, but according to our experiments, this system is better than the other Japanese NE systems we have (a C4.5-based rule generation system [Iso01] and a system based on maximum entropy (ME) modelling). In the following sections, we describe our English NE systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Support Vector Machines</head><p>First, we introduce SVM briefly. The non-linear SVM classifier [SBS99] uses a decision function for an input vector x given by f( x) = sign(g( x))</p><p>where sign(y) = -1 for y &lt; 0 and sign(y) = 1 for y &gt; 0, and </p><formula xml:id="formula_0" coords="5,91.20,505.43,271.51,44.07">g( x) = i=1 w i k( x, -→ z i ) + b. k( x,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The first NE system</head><p>The first English NE system we implemented was a simple variation of ME-based NE systems proposed by <ref type="bibr" coords="5,132.00,634.07,78.45,9.96">Borthwick [Bor99]</ref> and Uchimoto [UMM + 00]. In this sytem, each word is classified into 21 classes: {person,organization,location,facility,artifact} × {single,begin,middle,end} ∪ {other}. Here, (person,single) is a label for a one-word person name like "George." (person,begin) is the first word of a certain multi-word expression for a person's name (e.g., "George" in "George Bush"). (person,middle) indicates an internal word (e.g., "Walker" in "George Walker Bush"). (person,end) is the last word (e.g., "Bush" in "George Bush"). When a word does not belong to any of the named entities defined above, it is labeled as other.</p><p>In ME-based NE systems, the Viterbi algorithm is employed to get the best combination of labels. Since the ME model gives conditional probabilities, this is easy.</p><p>However, SVM does not tell us such probabilities. In addition, ordinary SVM can only solve two-class problems. Therefore, we built 21 SVM classifiers, i.e., one SVM for each class. For the application of the Viterbi algorithm, we used the sum of g( x) instead of the sum of logarithms of probabilities. We used Kudo's TinySVM because of its faster speed over the well-known SVM light <ref type="bibr" coords="6,91.20,166.31,33.59,9.96" target="#b3">[SBS99]</ref> for this kind of task.</p><p>Since this first NE system classifies every word in a given document, the training data for each class has 10 5 -10 6 examples. As a result, its training took a very long time.</p><p>In our case, we applied the NE system to the TREC data after the training. It turned out that it was also too slow in the application phase. Because of this slowness, we could not try various combinations of possible features. In addition, we could not improve the QA system, which depends on the NE system. Therefore, we abandoned the first NE system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The second NE system</head><p>We implemented another NE system in which hand-crafted rules were designed to detect NE candidates (roughly, noun phrases containing capitalized words) and then SVMs classified them into four classes: C = {person, organization, location, other}. For efficiency, we removed two classes, i.e., facility and artifact, because they had only small numbers of positive training examples and their results were not very good.</p><p>In the second NE system, the features for classification include word strings, their memberships in word lists, their part-of-speech tags, word counts, neighbor words, appositive information, information about preceding occurrences in the same documents, and other surface features usually used in other NE systems. Since SVM allows only numerical values in the input, we have to convert features into a set of numerical vector components.</p><p>One example is represented by one numerical vector. Suppose an NE candidate's head word is Washington. Then, we introduce an axis for the feature head word is Washington, and its value is 1. At the same time, the vector's incompatible axes like head word is University have 0 as their values. In TinySVM, we have only to enumerate non-zero components.</p><p>For each candidate, the outputs of four functions, g person , g organization , g location , g other , are compared and the function that gives the largest value is chosen as class (argmax c∈C g c ( x)).</p><p>The second NE system was found to be much faster than the first NE system, but it was still too slow for application to all TREC documents. Instead, we embedded the second NE system into the QA system and to be called on demand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Description solver and Apposition detection</head><p>To determine which parts of documents contain descriptions of entities is difficult even for humans, but we provisionally adopted the following assumption.</p><p>• The description of an entity is expressed as the appositive modifier of the entity.</p><p>For example, in the sentence "George W. Bush, the U.S. president, said...", 'George W. Bush' is appositively modified by 'the U.S. president'. Therefore, 'the U.S. president' should be some description of 'George W. Bush'. This assumption makes the detection of an appositive relation the principal task in answering a description question. In detecting appositive relations, punctuation disambiguation plays an important role. By 'punctuation disambiguation', we mean distinguishing the syntactic roles of commas. For example, in the sentence "When I was a kid, things were simple.", the comma is used as a marker of syntactic movement. On the contrary, in the sentence "George, the son of the former president, is a popular man.", the comma shows an appositive relation between 'George' and 'the son of the former president'. Note that in both examples, the commas are placed between noun phrases. This indicates that we cannot disambiguate this kind of comma usage only from neighbor information and punctuation disambiguation requires 'long-range' information.</p><p>We first used some off-the-shelf parsers to detect apposition. Unfortunately, we found that these parsers often failed around commas. We then created several heuristics to disambiguate punctuations and then to identify appositive relations. These heuristics classify punctuations into appositive markers, movement markers, and coordination markers (such as in "cats, dogs and birds").</p><p>Here are examples of the heuristics.</p><p>1. If a sentence starts with a subordinating conjunction, the leftmost comma in the sentence is a movement marker. (For example, "When I was a kid, TV was not popular.") 2. If a sentence contains the sequence of '(NP ,)+ NP CC NP', these commas are coordination markers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Main task results</head><p>Table <ref type="table" coords="7,118.80,468.71,4.98,9.96" target="#tab_1">1</ref> shows the evaluation returned by NIST for each question category. These categorizations were manually done after the result was submitted. Name-Person/Location/Organization result in the highest score (0.349) among all categories. This provides moderate but convincing evidence that our machine learning approach in NE recognition improves the answer extraction accuracy. The second highest is Description. Actually, this result was a little surprising for us because the extraction of the description was based on only a simple assumption (See Section 4).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,243.60,314.15,124.10,9.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: SAIQA-e overview</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,241.44,290.87,128.34,9.96"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Question categories</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,110.88,539.99,409.05,9.96;5,91.20,551.99,384.75,9.96;5,475.92,551.23,3.97,5.45;5,480.48,551.99,34.98,9.96;5,516.00,551.99,3.93,9.96;5,91.20,563.99,334.50,9.96;5,426.00,563.99,94.07,9.96;5,91.20,575.99,163.07,9.96"><head></head><label></label><figDesc>z) is called a kernel function. Several kernel functions are known. By considering Japanese NE results, we decided to use a second-order polynomial kernel function k( x, z) = (1 + x • z) 2 . The z i s are called support vectors that are representatives of training examples. w i s and b are constants determined by the training examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,194.16,33.59,222.77,138.84"><head>Table 1 :</head><label>1</label><figDesc>Results of TREC10 QA track (main task)</figDesc><table coords="7,194.16,33.59,222.71,106.44"><row><cell>Q. category</cell><cell cols="2">Num. of Q. MPR (strict)</cell></row><row><cell>Name-{Per,Loc,Org}</cell><cell>131</cell><cell>0.349</cell></row><row><cell>Name-Others</cell><cell>76</cell><cell>0.096</cell></row><row><cell>Desc-{Per,Others}</cell><cell>128</cell><cell>0.247</cell></row><row><cell>Quantity</cell><cell>68</cell><cell>0.219</cell></row><row><cell>Date</cell><cell>48</cell><cell>0.119</cell></row><row><cell>Paraphrase</cell><cell>14</cell><cell>0.193</cell></row><row><cell>Others</cell><cell>27</cell><cell>0.151</cell></row><row><cell>Total</cell><cell>492</cell><cell>0.228</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,144.00,595.43,376.07,9.96;7,144.00,607.19,149.64,9.96" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="7,231.60,595.77,260.65,9.18">A Maximum Entropy Approach to Named Entity Recognition</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Borthwick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>New York University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="7,144.00,627.11,376.04,9.96;7,144.00,639.11,375.98,9.96;7,144.00,651.11,63.72,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,210.96,627.11,309.08,9.96;7,144.00,639.11,90.08,9.96">Japanese named entity recognition based on a simple rule generator and decision tree learning</title>
		<author>
			<persName coords=""><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,253.92,639.45,245.23,9.18">Proceedings of Association for Computational Linguistics</title>
		<meeting>Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="306" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,144.00,34.79,376.12,9.96;8,144.00,46.79,96.84,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,292.08,34.79,171.36,9.96">Chunking with support vector machines</title>
		<author>
			<persName coords=""><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,484.08,35.13,36.04,9.18;8,144.00,47.13,64.39,9.18">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,144.00,66.71,376.05,9.96;8,144.00,78.71,194.52,9.96" xml:id="b3">
	<monogr>
		<title level="m" coord="8,504.48,67.05,15.57,9.18;8,144.00,79.05,110.19,9.18">Advances in Kernel Methods</title>
		<editor>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><surname>Smola</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.20,98.63,28.48,9.96;8,119.76,97.87,6.11,5.45;8,126.48,98.63,393.56,9.96;8,144.00,110.63,376.04,9.96;8,144.00,122.63,375.90,9.96;8,144.00,134.39,100.44,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,222.00,110.63,298.04,9.96;8,144.00,122.63,89.41,9.96">Named entity extraction based on a maximum entropy model and transformation rules</title>
		<author>
			<persName coords=""><forename type="first">Kiyotaka</forename><surname>Uchimoto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Masaki</forename><surname>Murata</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hiromi</forename><surname>Ozaku</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hitoshi</forename><surname>Isahara</surname></persName>
		</author>
		<idno>UMM + 00</idno>
	</analytic>
	<monogr>
		<title level="j" coord="8,306.72,122.97,175.53,9.18">Journal of Natural Language Processing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="63" to="90" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>in Japanese</note>
</biblStruct>

<biblStruct coords="8,144.00,154.31,375.90,9.96;8,144.00,166.31,365.28,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,375.12,154.31,144.78,9.96;8,144.00,166.31,132.51,9.96">Japanese named entity extraction using support vector machines</title>
		<author>
			<persName coords=""><forename type="first">Hiroyasu</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
		<idno>NL-142-17</idno>
	</analytic>
	<monogr>
		<title level="s" coord="8,357.84,166.65,68.27,9.18">IPSJ SIG Notes</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>in Japanese</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
