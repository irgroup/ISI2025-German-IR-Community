<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,93.60,109.99,399.52,12.30">MSR-Asia at TREC-10 Video Track: Shot Boundary Detection Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,158.28,139.78,50.24,10.51"><forename type="first">Yu-Fei</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Research</orgName>
								<address>
									<settlement>Asia</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,218.40,139.78,45.68,10.51"><forename type="first">Jia</forename><surname>Sheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,274.20,139.78,51.80,10.51"><forename type="first">Yuan</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Computer and Information Science and Engineering Department</orgName>
								<orgName type="institution">University of Florida</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,335.76,139.78,88.52,10.51"><forename type="first">Hong-Jiang</forename><surname>Zhang</surname></persName>
							<email>hjzhang@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Research</orgName>
								<address>
									<settlement>Asia</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,93.60,109.99,399.52,12.30">MSR-Asia at TREC-10 Video Track: Shot Boundary Detection Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FADDCEF05A9DBCA3BB39DAD97DA84840</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The video track is added into TREC-10, composed of two tasks, automatic shot boundary detection and video retrieval. In this year, we (MSR-Asia) participated in the video track, focusing on shot boundary detection task. Our work is to find out all of boundaries the shot changes by a fast algorithm based on uncompressed domain. In our algorithm, all of non-cut transitions are considered as gradual transition, including dissolve, fade-in, fade-out, and all kinds of wipes. Experimental results indicate that the accuracy and processing speed of our algorithm are all very satisfactory.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Shot is the basic unit of video sequence, hence, is important for digital video processing. Shot boundary detection is the first step for video content analysis. Since there are usually many shots in a video sequence, the automatic algorithm for shot boundary detection is indispensable.</p><p>The shot transition can be classified into two types: abrupt transition (cut) and gradual transition. Gradual transition usually includes dissolve, fade-in, fade-out and all kinds of wipes. Cuts are generated by camera operations, such as starting or stopping recording, or editing operations, while gradual transitions are generated only by editing operations There are so many literatures addressing the algorithms of shot boundary detection <ref type="bibr" coords="1,379.20,441.62,7.33,8.72" target="#b0">[1]</ref><ref type="bibr" coords="1,386.53,441.62,3.66,8.72" target="#b1">[2]</ref><ref type="bibr" coords="1,386.53,441.62,3.66,8.72" target="#b2">[3]</ref><ref type="bibr" coords="1,386.53,441.62,3.66,8.72" target="#b3">[4]</ref><ref type="bibr" coords="1,386.53,441.62,3.66,8.72" target="#b4">[5]</ref><ref type="bibr" coords="1,386.53,441.62,3.66,8.72" target="#b5">[6]</ref><ref type="bibr" coords="1,390.19,441.62,7.33,8.72">[7]</ref>. Two fundamental approaches are used: 1) Compressed domain based methods <ref type="bibr" coords="1,236.64,453.02,10.72,8.72" target="#b2">[3,</ref><ref type="bibr" coords="1,247.36,453.02,7.15,8.72" target="#b3">4,</ref><ref type="bibr" coords="1,254.50,453.02,7.15,8.72" target="#b4">5]</ref>, and 2) Uncompressed domain based methods <ref type="bibr" coords="1,455.88,453.02,10.72,8.72" target="#b1">[2,</ref><ref type="bibr" coords="1,466.60,453.02,7.15,8.72" target="#b5">6,</ref><ref type="bibr" coords="1,473.74,453.02,7.15,8.72">7]</ref>. The former usually is much faster than the latter, but its accuracy is difficult to be improved. Additionally, the former must be adaptive to different compression formats or decoders. Comparing with the former, much more methods could be used for the uncompressed domain based methods. Moreover, with the enhancement of hardware and compression standards, the decoding speed is never a drawback.</p><p>In this paper, we propose an uncompressed domain based approach for fast shot boundary detection. We employ a blockwise comparison based algorithm for cut detection and a run-length based algorithm for gradual transition detection. They are integrated seamlessly under the framework of Finite State Automata. In addition, the self-adaptive thresholds are used for robustness purpose. The experiments were carried out on large amount video sequences. The test set is provided by NIST (National Institute of Standard and Technology). The results are also evaluated by NIST.</p><p>The rest of paper is organized as follows. In section 2, we first introduce the system framework. The details of our algorithms will be described in section 3. Then the experimental results are presented in section 4. Section 5 draws some conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Framework</head><p>Our approach consists of four functional modules which are decoding, feature extraction, inter-frame comparison, and decision, as shown in Fig. <ref type="figure" coords="1,154.02,672.74,3.76,8.72">1</ref>. Since our algorithm is based on uncompressed data, the video sequences would be decoded in the decoding module first, if it is in a compressed format, such as mpeg. Then, the visual features are extracted from each decoded frames and compared in the feature extraction module and the Inter-frame comparison module respectively. In our algorithm, block-based average color and color histograms are used as visual features. With a Finite State Automata, the shot boundaries are detected by self-adaptive threshold in the decision module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>!</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig.1: Framework 3 Algorithms Description</head><p>In our algorithms, we assume that if there is an apparent deviation in the visual feature between two frames, a shot transition will occur there. So the difference between two successive frames is used as a measure of variation of video sequence. The key issues here are 1) what visual features are extracted from frames, ands 2) what similarity measure is adopted. We used different methods to deal with cut and gradual transition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Cut Detection</head><p>The pixel-wise difference between two successive frames could be used as dissimilarity measure. However, it is very sensitive to the motion, including camera movement and object motion. To reduce the disturbance of motion, we employ a block based comparison. In RGB color space, let ) , , (</p><formula xml:id="formula_0" coords="2,251.04,470.35,85.68,16.22">) ( ) ( ) ( ) ( t ij t ij t ij t ij b g r c =</formula><p>denote the color of the pixel at the pixel ) , ( j i in the t-th frame. Then we divide each frame into n m × blocks and comparisons are carried out on blocks instead of pixels. The average color of block ) , ( q p in the t-th frame could be defined as follows:</p><formula xml:id="formula_1" coords="2,208.44,519.69,203.45,27.95">× = × = k t k t k t k k t k t pq b g r n m c n m c ) , , (<label>1 1 )</label></formula><formula xml:id="formula_2" coords="2,214.44,525.18,319.19,11.92">( ) ( ) ( ) ( ) (<label>(1)</label></formula><p>where ) (t k c is the pixel color within a block field. Then the block-wise difference between t and (t-1)-th frame is defined:</p><formula xml:id="formula_3" coords="2,173.64,571.22,359.99,16.50">| | ' ) 1 ( ) ( ) ( - - = t pq t pq t pq c c d | | | | | | ) 1 ( ) ( ) 1 ( ) ( ) 1 ( ) ( - - - - + - + - = t pq t pq t pq t pq t pq t pq b b g g r r (2)</formula><p>Finally, we count the number of blocks in frame t whose difference  </p><formula xml:id="formula_4" coords="2,52.92,638.85,480.71,38.51">n m /( n D t d ) t ( c ) t ( pq × = ≥ (3) If the ) t ( c</formula><p>D is larger than another threshold T c , we will declare the current frame as a cut boundary. Experiment results</p><p>show that this block-wise comparison based method lessens the influence of global and local motion effectively.</p><p>After we got the differences between every consecutive frame pair, we can put them along the time axis to observe the temporal distribution of the frame difference values. Fig. <ref type="figure" coords="2,278.69,715.58,3.76,8.72" target="#fig_2">2</ref>. shows the inter-frame pixel-wise difference values of a video sequence, which includes two cut transitions. The cut position can be clearly observed. Since it is difficult for a global threshold to be suitable for any type of videos, even for different segments in one sequence, we adopt a sliding widow scheme for getting a self-adaptive threshold locally. It could be decided by ( <ref type="formula" coords="3,462.56,292.22,3.80,8.72">4</ref>) considering the local first several maximum values during previous m frames.</p><formula xml:id="formula_5" coords="3,199.20,315.71,334.43,27.38">× × = + LMax n w T t 0 1 ) 1 ( 1 (4)</formula><p>where LMax denotes the first n 0 maximum values during that sliding window, × LMax n 0 1</p><p>is the average difference and w 1 is the weight factor. By self-adaptive threshold, our method could adapt to the variation of motion intensity in video.</p><p>When the motion is intense, the threshold will become higher; and when the motion slows down, the threshold will also drop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Gradual Transition Detection</head><p>Due to the complexity of gradual transition, we extract color histogram from each frame as visual feature. Let</p><formula xml:id="formula_6" coords="3,523.68,443.61,7.54,6.10">) (t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>His</head><p>denotes the color histogram of the t-th frame in RGB color space. Then we define a dissimilarity measure of successive frames based on histogram intersection as <ref type="bibr" coords="3,217.44,470.18,10.66,8.72" target="#b4">(5)</ref>:</p><formula xml:id="formula_7" coords="3,52.92,481.75,480.71,59.26">[ ] [ ] n m ) i His , i His ( MIN D ) t ( N i ) t ( ) t ( g × - = - = 1 1 1 (5) where ] [ ) ( i His t</formula><p>denotes the number of pixels falling into i-th bin, and n m × is the total number of pixels in one frame.</p><p>By observing the dissimilarity sequence in Fig. <ref type="figure" coords="3,238.44,557.30,3.67,8.72" target="#fig_3">3</ref>, we can see that the difference between the frames during the dissolve are higher, although only slightly, than those in the preceding and following frames. Moreover, it is a continuous region. Our task is to find the boundary of this region viz. the start and the end frame numbers. We propose a run-length based method to detect this kind of regions. First, we still need a self-adaptive threshold much lower than cut's threshold to detect the variation during gradual transition. If the dissimilarity of two successive frames is higher than this threshold, we count the frame number until this criterion is not met. We call this number as run-length. If the run-length reaches sufficient length, a gradual transition is declared. Otherwise, no gradual transition occurs. Unlike cut detection, we take into account the all of values in the previous sliding window except for those very high or very low values to decide threshold this time. It could be defined as ( <ref type="formula" coords="3,110.40,648.86,3.84,8.72" target="#formula_8">6</ref>)</p><formula xml:id="formula_8" coords="3,172.92,661.06,360.71,28.90">× × = + LMedian n w T ) t ( g 0 2 1 1<label>(6)</label></formula><p>where LMedian are the median values in the sliding window, × LMedian n 0 1 is the average value and w 2 is the weight factor. A graduate transition occurs from #1040 to #1060.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Integration</head><p>In the final decision module, the two detection algorithms above are integrated by finite state automata (FSA), shown as Fig.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Experiments are carried out on the test video sequences provided by NIST, and the results are also evaluated by NIST. NIST provided participants of video track about 11 hours video data. Among them, more than 5 hours' data are used as final test set including 42 video sequences. In this section, we pick out 16 video sequences with the bigger size from the NIST evaluation results to analysis the performance of our algorithms. As we can see, Table . 1 lists the general evaluation results considering cut and gradual transition as a whole. By observing the evaluation results, it is concluded that:</p><p>1) The average probability of correct cut detection is more than 95% with more than 90% average recall and precision. This result indicates that our cut detection algorithm is very effective. 2) The average probability of correct gradual transition detection is more than 80% with about 60% average recall and precision. This result is satisfying considering the complexity of gradual transition. Because gradual transition consists of all kinds of non-cut transitions. 3) The average correction probability of all of transitions is more than 90% with more than 80% average recall and precision. It proves that integration method with finite state automata is much effective and efficient.</p><p>4) Comparing with the video playback time, the processing speed of our algorithm is much faster. Without any optimization, the processing speed could approach about 1.5 times of real-time on the PIII 450MHz 256MB personal computer.</p><p>On the other hand, we also find some mismatching between our ground truth and those provided by the organizer in the case of graduate transition. These mismatching affect our evaluation results to a certain degree. Some examples are listed in Table .5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we described our work in TREC-10 video track shot boundary detection task. We also reported and analyzed the evaluation results by NIST. The experimental results indicate that our shot boundary detection algorithm based on uncompressed domain is effective and much faster than real-time. By some optimizations, the speed of processing can be further improved.</p><p>However, there still is much room to improve our algorithm, especially for gradual transition. For example, some tolerances should be added into run-length based method. Because if the potential gradual transition state ends when only one interframe difference drops below the threshold Tg, some gradual transitions would be truncated. Another shortcoming is that the spans of gradual transition we detected are much longer than the real transitions sometime. Besides, how to integrate two detection algorithms also can still lead to additional improvements.</p><p>[ </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,360.12,591.28,2.28,6.16;2,354.72,591.28,2.28,6.16;2,352.44,593.28,2.10,10.49;2,357.36,591.28,1.90,6.16;2,355.68,599.44,6.90,6.16;2,345.36,593.28,5.83,10.49;2,368.52,594.62,165.10,9.55;2,52.92,613.94,62.50,8.72;2,136.68,610.78,2.25,6.09;2,132.84,610.78,1.88,6.09;2,129.24,610.78,2.25,6.09;2,128.52,618.94,3.00,6.09;2,120.12,612.76,402.14,10.44"><head>D</head><label></label><figDesc>is thus decided by the proportion of the blocks which are sufficiently different between each other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,324.96,642.68,3.90,10.54"><head>)</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,156.96,246.38,272.51,8.72;3,96.84,257.90,392.78,8.72;3,178.80,129.16,228.96,114.72"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Frame-to-frame Different based on Block-wise Comparison Two cut transitions occur at about frame #70 and frame #155 and the "jump up" is easy to figure out.</figDesc><graphic coords="3,178.80,129.16,228.96,114.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,151.68,233.90,283.10,8.72;4,194.64,245.42,197.18,8.72;4,178.80,117.64,228.84,113.64"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Frame-to-frame Difference based on Histogram Intersection.A graduate transition occurs from #1040 to #1060.</figDesc><graphic coords="4,178.80,117.64,228.84,113.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="4,203.04,604.94,180.29,8.72;4,52.92,627.74,480.74,9.55;4,52.92,639.26,480.77,9.43;4,52.92,650.66,480.77,9.55;4,52.92,662.06,480.83,9.55;4,52.92,673.58,480.70,8.72;4,52.92,684.98,480.83,8.72;4,52.92,696.50,480.88,9.43;4,52.92,707.90,185.30,8.72"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Integration in Finite State AutomataWhen a detection process is started, the frame is assumed in State 0 by default. If D g &lt;T g , FSA will keep in State 0. Otherwise we consider D c . If D c &gt;=T c , FSA will transfer to State 1, a cut is declared and FSA will go back to State 0. When D g &gt;=T g and D c &lt;T c , FSA will transfer to State 2, entering a potential gradual transition, and the run-length detection process will be started. In this state, if the condition D g &gt;=T g and D c &lt;T c is satisfied, the state will be kept. When D g &lt;T g , the runlength detection will finish. If the duration of run-length process is long enough, such as length&gt;=L, FSA will jump to State 3, a gradual transition is declared, then FAS go back to State 0. When FAS is in State 2, there is another path to jump. If D c &gt;=T c , FSA will jump to State 1, a cut will be declared, then FAS go back to State 0. The FSA will resume a new detection process, if only it goes back to Sate 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,52.92,179.78,480.83,540.44"><head></head><label></label><figDesc>Table. 2 and Table. 3 list the evaluation results of cut and gradual transition detection respectively. Besides, we also test processing speed of our algorithm on PC with the configuration of PIII 450MHz, 256MB. The results are listed in Table. 4.</figDesc><table coords="5,74.52,225.62,428.93,494.60"><row><cell></cell><cell></cell><cell cols="5">Table. 3 Evaluation Results: Gradual Transition</cell><cell></cell></row><row><cell>Name</cell><cell>Frame</cell><cell>Reference</cell><cell>Deletion</cell><cell cols="2">Insertion</cell><cell>Recall</cell><cell>Precision</cell><cell>Correction</cell></row><row><cell></cell><cell>Number</cell><cell>Transition</cell><cell>rate</cell><cell>rate</cell><cell></cell><cell></cell><cell></cell><cell>probability</cell></row><row><cell></cell><cell></cell><cell>Count</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ahf1.mpg</cell><cell>15679</cell><cell>45</cell><cell>0.133</cell><cell cols="2">0.177</cell><cell>0.866</cell><cell>0.829</cell><cell>0.933</cell></row><row><cell>anni005.mpg</cell><cell>11364</cell><cell>27</cell><cell>0.333</cell><cell cols="2">0.185</cell><cell>0.666</cell><cell>0.782</cell><cell>0.833</cell></row><row><cell>anni009.mpg</cell><cell>12307</cell><cell>65</cell><cell>0.492</cell><cell cols="2">0.184</cell><cell>0.507</cell><cell>0.733</cell><cell>0.753</cell></row><row><cell>bor03.mpg</cell><cell>48451</cell><cell>11</cell><cell>0.181</cell><cell cols="2">2.090</cell><cell>0.818</cell><cell>0.281</cell><cell>0.908</cell></row><row><cell>bor08.mpg</cell><cell>50569</cell><cell>153</cell><cell>0.241</cell><cell cols="2">0.169</cell><cell>0.758</cell><cell>0.816</cell><cell>0.878</cell></row><row><cell>bor12.mpg</cell><cell>24550</cell><cell>135</cell><cell>0.340</cell><cell cols="2">0.140</cell><cell>0.659</cell><cell>0.824</cell><cell>0.829</cell></row><row><cell>bor17.mpg</cell><cell>49801</cell><cell cols="5">Table. 1 Evaluation Results: as a whole 120 0.6 0.291 0.4</cell><cell>0.578</cell><cell>0.699</cell></row><row><cell>Name eal1.mpg</cell><cell>Frame 16048</cell><cell>Reference 20</cell><cell>Deletion 0.7</cell><cell cols="2">Insertion 1.5</cell><cell>Recall 0.3</cell><cell>Precision 0.166</cell><cell>Correction 0.649</cell></row><row><cell>nad28.mpg</cell><cell>Number 52927</cell><cell>Transition 117</cell><cell>rate 0.162</cell><cell cols="2">rate 0.333</cell><cell>0.837</cell><cell>0.715</cell><cell>probability 0.918</cell></row><row><cell>nad31.mpg</cell><cell>52405</cell><cell>Count 56</cell><cell>0.464</cell><cell cols="2">0.714</cell><cell>0.535</cell><cell>0.428</cell><cell>0.767</cell></row><row><cell>ahf1.mpg nad33.mpg</cell><cell>15679 49768</cell><cell>107 26</cell><cell>0.102 0.307</cell><cell cols="2">0.168 1.115</cell><cell>0.897 0.692</cell><cell>0.842 0.382</cell><cell>0.948 0.845</cell></row><row><cell>anni005.mpg nad53.mpg</cell><cell>11364 25783</cell><cell>65 77</cell><cell>0.153 0.194</cell><cell cols="2">0.107 0.350</cell><cell>0.846 0.805</cell><cell>0.887 0.696</cell><cell>0.922 0.902</cell></row><row><cell>anni009.mpg nad57.mpg</cell><cell>12307 12781</cell><cell>103 23</cell><cell>0.368 0.173</cell><cell cols="2">0.155 0.565</cell><cell>0.631 0.826</cell><cell>0.802 0.593</cell><cell>0.814 0.912</cell></row><row><cell>bor03.mpg pfm1.mpg</cell><cell>48451 14686</cell><cell>237 21</cell><cell>0.067 0.238</cell><cell cols="2">0.139 0.666</cell><cell>0.932 0.761</cell><cell>0.870 0.533</cell><cell>0.965 0.880</cell></row><row><cell>bor08.mpg senses111.mpg</cell><cell>50569 86789</cell><cell>528 16</cell><cell>0.094 0.625</cell><cell cols="2">0.140 2.625</cell><cell>0.905 0.375</cell><cell>0.865 0.125</cell><cell>0.951 0.687</cell></row><row><cell>bor12.mpg ydh1.mpg</cell><cell>24550 22276</cell><cell>135 52</cell><cell>0.340 0.365</cell><cell cols="2">0.140 0.326</cell><cell>0.659 0.634</cell><cell>0.824 0.66</cell><cell>0.829 0.816</cell></row><row><cell>bor17.mpg Average</cell><cell>49801 34136</cell><cell>246 60</cell><cell>0.337 0.346</cell><cell cols="2">0.150 0.714</cell><cell>0.662 0.652</cell><cell>0.815 0.571</cell><cell>0.830 0.826</cell></row><row><cell>eal1.mpg</cell><cell>16048</cell><cell>81</cell><cell>0.271</cell><cell cols="2">0.370</cell><cell>0.728</cell><cell>0.662</cell><cell>0.863</cell></row><row><cell>nad28.mpg</cell><cell>52927</cell><cell>298</cell><cell>0.161</cell><cell cols="2">0.177</cell><cell>0.838</cell><cell>0.825</cell><cell>0.918</cell></row><row><cell>nad31.mpg</cell><cell>52405</cell><cell cols="5">239 Table. 4 Processing Speed Comparison 0.175 0.213 0.824</cell><cell>0.794</cell><cell>0.911</cell></row><row><cell>nad33.mpg Name</cell><cell cols="2">49768 Frame Number 214</cell><cell>0.046 Test time (s)</cell><cell cols="3">0.168 Normal time (s) 0.953</cell><cell>0.85 Speed up</cell><cell>0.976</cell></row><row><cell>nad53.mpg ahf1.mpg</cell><cell>25783 15679</cell><cell>158</cell><cell>0.107 367</cell><cell cols="2">0.183 540</cell><cell>0.892</cell><cell>0.829 1.47</cell><cell>0.945</cell></row><row><cell>nad57.mpg anni005.mpg</cell><cell>12781 11364</cell><cell>67</cell><cell>0.208 254</cell><cell cols="2">0.208 379</cell><cell>0.791</cell><cell>0.791 1.49</cell><cell>0.894</cell></row><row><cell>pfm1.mpg anni009.mpg</cell><cell>14686 12307</cell><cell>82</cell><cell>0.134 287</cell><cell cols="2">0.231 410</cell><cell>0.865</cell><cell>0.788 1.43</cell><cell>0.932</cell></row><row><cell>senses111.mpg bor03.mpg</cell><cell>86789 48451</cell><cell>308</cell><cell>0.090 1115</cell><cell cols="2">0.142 1616</cell><cell>0.909</cell><cell>0.864 1.45</cell><cell>0.954</cell></row><row><cell>ydh1.mpg bor08.mpg</cell><cell>22276 50569</cell><cell>119</cell><cell>0.168 1171</cell><cell cols="2">0.252 1687</cell><cell>0.831</cell><cell>0.767 1.44</cell><cell>0.915</cell></row><row><cell>Average bor12.mpg</cell><cell>34136 24550</cell><cell>187</cell><cell>0.176 565</cell><cell cols="2">0.184 819</cell><cell>0.823</cell><cell>0.817 1.45</cell><cell>0.910</cell></row><row><cell>bor17.mpg</cell><cell>49801</cell><cell></cell><cell>1189</cell><cell></cell><cell>1661</cell><cell></cell><cell>1.40</cell></row><row><cell>eal1.mpg</cell><cell>16048</cell><cell cols="4">Table. 2 Evaluation Results: Cut 378 540</cell><cell></cell><cell>1.43</cell></row><row><cell>Name nad28.mpg</cell><cell>Frame 52927</cell><cell>Reference</cell><cell>Deletion 1177</cell><cell cols="2">Insertion 1766</cell><cell>Recall</cell><cell>Precision 1.50</cell><cell>Correction</cell></row><row><cell>nad31.mpg</cell><cell>Number 52405</cell><cell>Transition</cell><cell>rate 1250</cell><cell>rate</cell><cell>1748</cell><cell></cell><cell>1.40</cell><cell>probability</cell></row><row><cell>nad33.mpg</cell><cell>49768</cell><cell>Count</cell><cell>1152</cell><cell></cell><cell>1660</cell><cell></cell><cell>1.44</cell></row><row><cell>ahf1.mpg nad53.mpg</cell><cell>15679 25783</cell><cell>62</cell><cell>0.080 605</cell><cell cols="2">0.161 860</cell><cell>0.919</cell><cell>0.850 1.42</cell><cell>0.959</cell></row><row><cell>anni005.mpg nad57.mpg</cell><cell>11364 12781</cell><cell>38</cell><cell>0.026 297</cell><cell cols="2">0.052 426</cell><cell>0.973</cell><cell>0.948 1.43</cell><cell>0.986</cell></row><row><cell>anni009.mpg pfm1.mpg</cell><cell>12307 14686</cell><cell>38</cell><cell>0.157 342</cell><cell cols="2">0.105 495</cell><cell>0.842</cell><cell>0.888 1.45</cell><cell>0.920</cell></row><row><cell>bor03.mpg senses111.mpg</cell><cell>48451 86789</cell><cell>226</cell><cell>0.061 1998</cell><cell cols="2">0.044 2986</cell><cell>0.938</cell><cell>0.954 1.45</cell><cell>0.968</cell></row><row><cell>bor08.mpg ydh1.mpg</cell><cell>50569 22276</cell><cell>375</cell><cell>0.034 518</cell><cell cols="2">0.128 743</cell><cell>0.965</cell><cell>0.882 1.43</cell><cell>0.982</cell></row><row><cell>bor17.mpg Average</cell><cell>49801 34136</cell><cell>126</cell><cell>0.087 792</cell><cell cols="2">0.015 1146</cell><cell>0.912</cell><cell>0.982 1.44</cell><cell>0.956</cell></row><row><cell>eal1.mpg</cell><cell>16048</cell><cell>61</cell><cell>0.131</cell><cell>0.0</cell><cell></cell><cell>0.868</cell><cell>1.0</cell><cell>0.934</cell></row><row><cell>nad28.mpg</cell><cell>52927</cell><cell>181</cell><cell>0.160</cell><cell cols="2">0.077</cell><cell>0.839</cell><cell>0.915</cell><cell>0.919</cell></row><row><cell>nad31.mpg</cell><cell>52405</cell><cell>183</cell><cell>0.087</cell><cell cols="2">0.060</cell><cell>0.912</cell><cell>0.938</cell><cell>0.956</cell></row><row><cell>nad33.mpg</cell><cell>49768</cell><cell>188</cell><cell>0.010</cell><cell cols="2">0.037</cell><cell>0.989</cell><cell>0.963</cell><cell>0.994</cell></row><row><cell>nad53.mpg</cell><cell>25783</cell><cell>81</cell><cell>0.024</cell><cell cols="2">0.024</cell><cell>0.975</cell><cell>0.975</cell><cell>0.987</cell></row><row><cell>nad57.mpg</cell><cell>12781</cell><cell>44</cell><cell>0.227</cell><cell cols="2">0.022</cell><cell>0.772</cell><cell>0.971</cell><cell>0.886</cell></row><row><cell>pfm1.mpg</cell><cell>14686</cell><cell>61</cell><cell>0.098</cell><cell cols="2">0.081</cell><cell>0.901</cell><cell>0.916</cell><cell>0.950</cell></row><row><cell>senses111.mpg</cell><cell>86789</cell><cell>292</cell><cell>0.061</cell><cell cols="2">0.006</cell><cell>0.938</cell><cell>0.992</cell><cell>0.969</cell></row><row><cell>ydh1.mpg</cell><cell>22276</cell><cell>67</cell><cell>0.014</cell><cell cols="2">0.194</cell><cell>0.985</cell><cell>0.835</cell><cell>0.992</cell></row><row><cell>Average</cell><cell>34776</cell><cell>135</cell><cell>0.084</cell><cell cols="2">0.067</cell><cell>0.915</cell><cell>0.934</cell><cell>0.957</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,119.28,200.42,304.82,91.40"><head>Table .</head><label>.</label><figDesc></figDesc><table coords="7,119.28,200.42,304.82,91.40"><row><cell></cell><cell>5. Some Mismatched Samples</cell></row><row><cell>Video Sequences</cell><cell>Mismatching in ground truth</cell></row><row><cell>bor08.mpg</cell><cell>#49326 to #49349</cell></row><row><cell>bor12.mpg</cell><cell>#16497 to #16520</cell></row><row><cell>bor17.mpg</cell><cell>#9679 to #9686; #9745 to #9752</cell></row><row><cell>nad28.mpg</cell><cell>#327 to #339, #2035 to #2057, #26591 to #26609,</cell></row><row><cell></cell><cell>#37696 to #37709, #52197 to #52211</cell></row><row><cell>… …</cell><cell>……</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>The authors would like to thank <rs type="person">Dong Zhang</rs> for his helpful work on implementing some useful experimental tools.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="7,67.32,530.46,466.24,7.88;7,52.92,540.66,233.55,7.88" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,213.22,530.46,244.03,7.88">A Survey of Technologies for Parsing and Indexing Digital Video</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ahanger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">C</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,469.08,530.46,64.48,7.88;7,52.92,540.66,149.93,7.88">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">28043</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,66.60,561.30,466.95,7.88;7,52.92,571.50,19.95,7.88" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,245.87,561.30,156.29,7.88">Automatic partitioning of full-motion video</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">W</forename><surname>Smoliar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,412.80,561.30,80.24,7.88">Multimedia Systems, 1</title>
		<imprint>
			<biblScope unit="page" from="10" to="28" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,66.12,592.14,467.43,7.88;7,52.92,602.34,52.59,7.88" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,183.79,592.14,234.77,7.88">Scene Change Detection in a MPEG Compressed Video Sequence</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,429.36,592.14,100.49,7.88">Journal, Publisher, Location</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<pubPlace>Date</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,67.80,622.98,465.78,7.88;7,52.92,633.18,130.47,7.88" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,159.45,622.98,158.23,7.88">Rapid scene analysis on compressed video</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">L</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,329.40,622.98,204.18,7.88;7,52.92,633.18,39.30,7.88">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="533" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,66.36,653.82,467.34,7.88;7,52.92,664.02,181.23,7.88" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,185.76,653.82,347.93,7.88;7,52.92,664.02,17.42,7.88">Dissolve transition detection algorithm using spatio-temporal distribution of MPEG macro-block types</title>
		<author>
			<persName coords=""><forename type="first">S.-B</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,79.56,664.02,81.12,7.88">ACM Multimedia 2000</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="391" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,65.64,684.54,467.91,7.88;7,52.92,694.86,178.95,7.88" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,172.20,684.54,260.36,7.88">Automatic Video Indexing and Full-Video Search for Object Appearances</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nagasaka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,442.80,684.54,87.03,7.88">Visual Database Systems</title>
		<imprint>
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="113" to="127" />
			<date type="published" when="1992">1992</date>
			<publisher>Elsevier Science Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
