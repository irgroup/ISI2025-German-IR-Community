<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,110.57,72.16,390.86,12.91">Retrieving Web Pages using Content, Links, URLs and Anchors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,191.16,109.79,67.40,8.97"><forename type="first">Thijs</forename><surname>Westerveld</surname></persName>
							<email>westerve@cs.utwente.nl</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Twente</orgName>
								<orgName type="institution" key="instit2">CTIT</orgName>
								<address>
									<postBox>P.O. Box 217</postBox>
									<postCode>7500 AE</postCode>
									<settlement>Enschede</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,268.01,109.79,55.36,8.97"><forename type="first">Wessel</forename><surname>Kraaij</surname></persName>
							<email>kraaij@tpd.tno.nl</email>
							<affiliation key="aff1">
								<orgName type="institution">TNO-TPD</orgName>
								<address>
									<postBox>P.O. Box 155</postBox>
									<postCode>2600 AD</postCode>
									<settlement>Delft</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,349.69,109.79,66.68,8.97"><forename type="first">Djoerd</forename><surname>Hiemstra</surname></persName>
							<email>hiemstra@cs.utwente.nl</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Twente</orgName>
								<orgName type="institution" key="instit2">CTIT</orgName>
								<address>
									<postBox>P.O. Box 217</postBox>
									<postCode>7500 AE</postCode>
									<settlement>Enschede</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,110.57,72.16,390.86,12.91">Retrieving Web Pages using Content, Links, URLs and Anchors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FD95E32279571C522DC51E96CBC8B6C9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For this year's web track, we concentrated on the entry page finding task. For the content-only runs, in both the ad-hoc task and the entry page finding task, we used an information retrieval system based on a simple unigram language model. In the Ad hoc task we experimented with alternatieve approaches to smoothing. For the entry page task, we incorporated additional information into the model. The sources of information we used in addition to the document's content are links, URLs and anchors. We found that almost every approach can improve the results of a content only run. In the end, a very basic approach, using the depth of the path of the URL as a prior, yielded by far the largest improvement over the content only results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Entry page searching is different from general information searching, not only because entry pages differ from other web documents, but also because the goals of the tasks are different. In a general information search task we're interested in finding as much information as possible, whereas for entry page searches we're looking for one specific document. Therefore, the entry page task is clearly a high precision task. Because of both the differences in the task and in the documents, information sources other then the document's content can be very useful for locating the relevant entry page even though they didn't do much for general information searching.</p><p>For the content-only runs, in both the ad-hoc task and the entry page finding task, we used an information retrieval system based on a simple unigram language model. This IR model, which we introduced at the TREC-7 conference <ref type="bibr" coords="1,72.00,448.76,11.62,8.97" target="#b3">[4]</ref> and which worked effectively on last year's web task, is presented in section 2. Section 3 describes how we used links, anchors and URLs to improve a content only run and section 4 lists the official results for the submitted runs as well as the results for the additional runs we did. Finally, section 5 lists our conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Basic IR model</head><p>All runs were carried out with an information retrieval system based on a simple statistical language model <ref type="bibr" coords="1,507.51,528.08,10.58,8.97" target="#b2">[3]</ref>. The basic idea is that documents can be represented by unigram language models. Now, if a query is more probable given a language model based on document d 1 , than given e.g. a language model based on document d 2 , then we hypothesise that the document d 1 is more relevant to the query than document d 2 . Thus the probability of generating a certain query given a document-based language model can serve as a score to rank documents with respect to relevance.</p><formula xml:id="formula_0" coords="1,152.60,594.81,387.40,30.32">P (T 1 , T 2 , • • • , T n |D k )P (D k ) = P (D k ) n i=1 (1 -λ)P (T i |C) + λP (T i |D k )<label>(1)</label></formula><p>Equation 1 shows the basic idea of this approach to information retrieval, where the document-based language model is smoothed by interpolation with a background language model to compensate for sparseness. In the equation, T i is a random variable for the query term on position i in the query (1 ≤ i ≤ n, where n is the query length), which sample space is the set {t (0) , t (1) , • • • , t (m) } of all terms in the collection. The probability measure P (T i |C) defines the probability of drawing a term at random from the collection, P (T i |D k ) defines the probability of drawing a term at random from document k; and λ is the interpolation parameter <ref type="foot" coords="2,339.74,73.58,3.49,6.28" target="#foot_0">1</ref> . The a-priori probability of relevance P (D k ) is usually taken to be a linear function of the document length, modelling the empirical fact that longer documents have a higher probability of relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Combining external information</head><p>The basic ranking model is based on the content of the web pages. There is evidence that other sources of information (link structure, anchor text) play a decisive role in the ranking process of entry pages (e.g. Google<ref type="foot" coords="2,471.90,154.63,3.49,6.28" target="#foot_1">2</ref> ). The preferred way to incorporate extra information about web pages is to include this information in the model. A clean method is to incorporate this information in the prior probability of a document. A second manner is to model different types of evidence as different types of ranking models, and combine these methods via interpolation.</p><formula xml:id="formula_1" coords="2,208.81,215.98,331.19,9.81">score combi = αscore content + (1 -α)score features<label>(2)</label></formula><p>Equation 2 shows how two ranking functions can be combined by interpolation. The combined score is based on a weighted function of the unigram document model and the posterior probability given the document feature set and a Bayesian classifier trained on the training set. As features we experimented with the number of inlinks and the URL form. However, for interpolation, scores have to be normalised across queries, because the interpolation scheme is query independent. Therefore, for the interpolation method we normalised the content score by the query length, the ranking models based on other document information that we applied are (discriminative) probabilities and thus need no normalisation. The interpolation method has shown to work well in cases where score normalisation is a key factor <ref type="bibr" coords="2,72.00,315.86,10.58,8.97" target="#b5">[6]</ref>. For the experiments we describe here, we have applied both methods and they yield similar results. In a context where score normalisation is not necessary, we prefer method one. We determined the document priors (documentcontent independent prior probabilities) using various techniques, either postulating a relationship, or learning priors from training data conditioning on e.g. the URL form. This process will be described in more detail in the Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Smoothing variants</head><p>Recent experiments have shown that the particular choice of smoothing technique can have a large influence on the retrieval effectiveness. For title adhoc queries, Zhai and Lafferty <ref type="bibr" coords="2,334.82,408.86,11.62,8.97" target="#b7">[8]</ref> found Dirichlet smoothing to be more effective than linear interpolation <ref type="foot" coords="2,166.94,419.24,3.49,6.28" target="#foot_2">3</ref> Both methods start from the idea that the probability estimate for unseen terms: P u (T i |D k ) is modelled a constant times the collection based estimate: P (T i |C). A crucial difference between Dirichlet and Jelinek-Mercer smoothing is that the smoothing constant is dependent on the document length for Dirichlet, reflecting the fact that probability estimates are more reliable for longer documents. Equation (3) shows the weighting formula for Dirichlet smoothing, where c(T i |D k ) is the term frequency of term T i in document D k , w c(T i ; D k ) is the length of document D k and µ is a constant. The collection specific smoothing constant is in this case µ w c(Ti;D k )+µ , whereas the smoothing constant is (1 -λ) in the Jelinek-Mercer based model.</p><formula xml:id="formula_2" coords="2,166.74,513.73,373.26,30.32">P (T 1 , T 2 , • • • , T n |D k )P (D k ) = P (D k ) n i=1 c(T i ; D k ) + µP (T i |C) w c(T i ; D k ) + µ (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Entry page Search</head><p>To improve the results of a content only run in the entry page finding task, we experimented with various link, URL and anchor based methods. We tested several well-known and novel techniques on the set of 100 training topics provided by NIST and found that each method we tested was more or less beneficial for finding entry pages. This contrasts with last year's findings where link based techniques didn't add anything in an ad hoc search task <ref type="bibr" coords="2,459.85,619.20,10.58,8.97" target="#b6">[7]</ref>. In the following subsections, we subsequently discuss link based methods, URL based methods and anchor based methods, along with our findings on the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Links</head><p>One of the sources of information one can use in addition to the content is the link structure. This is the structure of hyperlinks connecting the documents on the web. We took two different approaches exploiting this structure, both relying on the fact that entry pages tend to have a different link structure than other documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inlinks</head><p>The first link-based approach we experimented with is based on the assumption that entry pages tend to have a higher number of inlinks than other documents (i.e. they are referenced more often). A well known example of a commercial search engine which is based on a similar assumption is Google <ref type="bibr" coords="3,385.11,176.07,10.58,8.97" target="#b0">[1]</ref>. To check whether this assumption holds, we made a of plot of P (entrypage|#inlinks) (See Figure <ref type="figure" coords="3,343.14,188.03,3.60,8.97" target="#fig_0">1</ref>). The probabilities are estimated on half of the training data. The figure shows that indeed documents with more inlinks tend to have a higher probability of being an entry page. Therefore, for an entry page task, the number of inlinks might be a good prior. In fact, as figure <ref type="figure" coords="3,505.14,211.94,4.98,8.97">2</ref> shows, the assumption that longer documents have a higher probability of being relevant does not hold for entry page searches and a prior based on the number of inlinks might be better than one based on the length of the document. As a prior for ad hoc searches, we usually take a document length prior:</p><formula xml:id="formula_3" coords="3,246.44,542.16,293.56,26.56">P (D k ) = doclen(D k ) N j=1 doclen(D j )<label>(4)</label></formula><p>We define the inlink prior as:</p><formula xml:id="formula_4" coords="3,240.26,592.28,299.74,26.56">P (D k ) = #inlinks(D k ) N j=1 #inlinks(D j )<label>(5)</label></formula><p>We compared the two priors of equations 4 and 5 on the training data. We normalised the content score by the query length and interpolated with the inlink prior (cf. eq. 2), the doclen prior is used conform eq. 1. Table <ref type="table" coords="3,493.65,639.08,4.98,8.97" target="#tab_0">1</ref> shows the mean reciprocal ranks (MRR) 4 . The interpolation parameters used in the table gave the best results. The scores show that indeed, the number of inlinks is a better prior than the length of the document. 4 The reciprocal of the rank of the relevant entry page averaged over all queries. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kleinberg</head><p>The second link-based approach we experimented with is based on Kleinberg's hub and authority algorithm <ref type="bibr" coords="4,72.00,407.81,10.58,8.97" target="#b4">[5]</ref>. This algorithm identifies authorities (important sources of information) and hubs (lists of pointers to authorities) by analysing the structure of hyperlinks. Since entry pages can be seen as authorities on a very specific subject (a certain organisation), Kleinberg's algorithm can be useful for the entry page search task. The algorithm works by iteratively assigning hub and authority scores to documents in such a way that good hubs are pages that refer to many good authorities and good authorities are referenced by many good hubs:</p><p>1. Take the top N results from the content run 2. Extend this set S with all documents that are linked to S (either through in or through outlinks) 3. Initialise all hub and authority scores in this set to 1. 4. hub(D) = {i|link D → i exists} auth(i) 5. auth(D) = {i|link i → D exists} hub(i) 6. normalise hub and auth scores such that s∈S hub 2 (s) = s∈S auth 2 (s) = 1 7. repeat steps 4 -6</p><p>We computed hubs and authorities for the top N of the content only run and used the resulting authority scores to rank the documents. Table <ref type="table" coords="4,179.39,585.87,4.98,8.97" target="#tab_1">2</ref> shows the results for different values of N.</p><p>As the results show, taking only the top 5 or top 10 ranks from the content run and computing authority scores starting from those, is sufficient to improve the results. Apparently, if an entry page is not in the top 5 from the content run, it is often in the set of documents linked to these 5 documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">URLs</head><p>Apart from content and links, a third source of information are the document's URLs. Entry page URLs often contain the name or acronym of the corresponding organisation. Therefore, an obvious way of exploiting URL information is trying to match query terms and URL terms. Our URL approach however, is based on the observation that entry page URLs tend to be higher in a server's document tree than other web pages, i.e. the number of slashes ('/') in an entry page URL tends to be relatively small.</p><p>We define 4 different types of URLs:</p><p>-root: a domain name, optionally followed by 'index.html' (e.g. http://trec.nist.gov) -subroot: a domain name, followed by a single directory, optionally followed by 'index.html' name (e.g. http: //trec.nist.gov/pubs/) -path: a domain name, followed by an arbitrarily deep path, but not ending in a file name other than 'index.html' (e.g. http://trec.nist.gov/pubs/trec9/papers/) -file: anything ending in a filename other than 'index.html' (e.g. http://trec.nist.gov/pubs/trec9/ t9_proceedings.html)</p><p>We analysed WT10g and the relevant entry pages for half of the training documents to see how entry pages and other documents are distributed over these URL types. Table <ref type="table" coords="5,313.86,346.07,4.98,8.97" target="#tab_2">3</ref>  From these statistics, we estimated prior probabilities of being an entry page on the basis of the URL type P (entrypage|U RLtype = t) for all URL types t. We then interpolated these priors with the normalised content only scores (cf. eq. 2) and tested this on the other 50 entry page search topics of the training data. This gave a major improvement on the content only results (see table <ref type="table" coords="5,275.35,520.89,3.60,8.97" target="#tab_3">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Anchors</head><p>The fourth source of information is provided by the anchor texts of outlinks. These anchor texts are the underlined and highlighted texts of hyperlinks in web pages. We gathered all anchor texts of the outlinks, combined all texts pointing to the same document to form a new textual representation of that document, and built a separate index on these texts. The texts include the so-called ALT-tags of images as well as the words occurring in the URL.</p><p>Note that the score provided by an anchor run is not a document prior. The anchor texts and the body texts ('contentonly') provide two very different textual representations of the documents. The information retrieval language models are particularly well-suited for combining several document representations <ref type="bibr" coords="6,376.37,123.03,10.58,8.97" target="#b2">[3]</ref>. Our preferred way of combining two representations would be by the following, revised ranking formula.</p><formula xml:id="formula_5" coords="6,109.47,154.21,430.53,30.32">score = log(P prior (D k )) + n i=1 log((1-λ-µ)P (T t |C) + λP content (T i |D k ) + µP anchor (T i |D k ))<label>(6)</label></formula><p>So, the combination of the anchor run with the content run would be done on a 'query term by query term' basis, whereas the document prior (provided by inlinks or URLs) is added separately. Unfortunately, the current implementation of the retrieval system does not support combining document representations like this. Instead, the anchor runs were done separately from the content runs, their document scores being combined afterwards. Table <ref type="table" coords="6,112.54,368.55,4.98,8.97" target="#tab_4">5</ref> shows the MRRs on half of the training topics. Surprisingly, the anchor-only run slightly outperforms the content-only run. Apparently, search engines do not actually have to see entry pages to provide some useful retrieval functionality. Combining the two approaches leads to improved results. Anchors still seem to provide additional information if they are combined with the successful URL priors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ad hoc task</head><p>For the Ad Hoc task, we submitted two runs, based on a Jelinek-Mercer smoothing scheme. We did some post hoc runs, based on the Dirichlet smoothing method and were impressed by their superior performance. All runs used only the title field of the topics. run description m.a.p. tnout10t1 JM smoothing (λ = 0.8) without doclen prior 0.1652 tnout10t2 JM smoothing (λ = 0.5) with doclen prior 0.1891 tnout10t3 Dirichlet smoothing (µ = 1000) without doclen prior 0.2039 Table <ref type="table" coords="6,264.50,592.88,3.36,8.07">6</ref>. Results of the Ad Hoc runs Table <ref type="table" coords="6,112.02,623.53,4.15,8.97" target="#tab_3">4</ref>.1 gives the results (mean average precision) for the title only adhoc runs (offical runs in bold font). We know from previous experiments that the Jelinek-Mercer smoothing scheme as such works better on full queries than on title queries. For title queries, the system has too much preference for shorter documents in comparison with the ideal line depicted by P (rel|dlen) (see fig. <ref type="figure" coords="6,249.29,659.40,3.60,8.97">3</ref>). This can be "compensated" by assuming a prior probability which is linearly dependent on the document length. However, a strict linear relationship will favour long documents too much. An alternative is to use Dirichlet smoothing, such a system yields a P (ret|dlen) curve which has the same shape and P(rel|dlen) P(ret|dlen) di(1000) P(ret|dlen) jm(0.8) P(ret|dlen) jm(0.8)+lenprior Fig. <ref type="figure" coords="7,145.47,317.01,3.36,8.07">3</ref>. Probability of relevance and probability of being retrieved as a function of document length orientation as the ideal P (rel|dlen) curve (fig. <ref type="figure" coords="7,264.48,358.41,3.60,8.97">3</ref>). The Dirichlet smoothing scheme is less sensitive to query length <ref type="bibr" coords="7,72.00,370.37,10.58,8.97" target="#b7">[8]</ref>, and the preference for longer documents is inherent, since less smoothing is applied to longer documents.</p><p>Figure <ref type="figure" coords="7,115.97,382.32,4.98,8.97">3</ref> illustrates the effect. The Dirichlet run follows the shape of the P (rel|dlen) line more closely than the runs based on Jelinek-Mercer smoothing. The JM run based on a document length dependent prior indeed follows the ideal curve better in the lower ranges of document lengths, but overcompensates for the higher document length ranges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Entry page task</head><p>For the entry page task, we submitted four runs: a content only run, a anchor only run, a content run with URL prior <ref type="foot" coords="7,91.37,473.46,3.49,6.28" target="#foot_3">5</ref> and a run with content, anchors and URL priors. We did some additional runs to have results for all sensible combinations of content, anchors and priors, as well as an inlinkprior run and a Kleinberg run. The mean reciprocal ranks for all runs are shown in table 7 (official runs in bold face). Figure <ref type="figure" coords="7,359.62,498.94,4.98,8.97">4</ref> shows the success rate at N for all runs 6 (on a logarithmic scale to emphasise high precision).</p><p>The first thing that should be noted from the results is that each combination of content and another source of information outperforms the content only run. The same holds for combinations with the anchor run. However, the improvement when adding URL information is for the anchor run less impressive than for the content run. This is probably due to the differences in the two runs. Although these runs have similar scores (MRR around 0.33), they have different characteristics. The anchor run is a high precision run, whereas the content run also has a reasonable recall. Therefore, it is hard to improve the anchor run since the entry pages that are retrieved are already in the top ranks and the other entry pages are simply not retrieved at all. Figure <ref type="figure" coords="7,310.34,594.58,4.98,8.97">4</ref> shows the differences between the two runs: the anchor run has a slightly higher success rate for the lower ranks, but as the ranks get higher, the content run takes over.</p><p>As mentioned in section 2.1, our preferred way of combining sources of information when normalisation is not necessary, is to incorporate the additional information in the prior probability of a document. However, in the runs listed in table 7 we interpolated URL priors and inlink priors with the content scores. We did additional runs in which we used the priors exactly as in equation 1; Table <ref type="table" coords="7,270.82,654.36,4.98,8.97" target="#tab_6">8</ref>  Table <ref type="table" coords="9,111.63,195.06,4.98,8.97" target="#tab_6">8</ref> shows that also when we use priors in the clean way(cf. eq 1, they improve our results. Comparing these results to the ones in table 7, we see no difference in performance between the interpolated inlinks and the clean inlinks. The interpolated URL priors are slightly better than the clean ones.</p><p>When we take a combination of inlink and URL information as a prior, by simply multiplying the two priors, our results drop (see table <ref type="table" coords="9,181.31,244.09,3.60,8.97" target="#tab_6">8</ref>). This indicates that the two sources of information are not independent. We therefore dropped the independence assumption and had another look at the training data. Just like with the estimation of the URL priors, we subdivided the collection into different categories and estimated prior probabilities of being an entry page given a certain category. As a basis for the categories, we took the 4 URL types defined in section 3.2, then we subdivided the root type into categories on the basis of the number of inlinks. Again we counted the number of entry pages from the training data and the number of documents from WT10g that fell into each category and estimated the prior probabilities from that. We took the categories from the URL types as a starting point and subdivided the root type into 4 subtypes on the basis of the number of inlinks. Table <ref type="table" coords="9,329.66,327.78,4.98,8.97" target="#tab_7">9</ref> shows the statistics for the different categories. As can be seen in table <ref type="table" coords="9,183.56,506.64,3.74,8.97" target="#tab_6">8</ref>, this proper combination of URL and inlink information (i.e. without the independence assumption) performs as good as or better than the two separate priors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Post hoc runs show that the Dirichlet smoothing technique yields superior performance for title ad hoc queries on the web collection. This is probably due to the document length dependent smoothing constant, but further investigation is needed.</p><p>The Entry page finding task turns out to be very different from an ad hoc task. In previous web tracks link information didn't seem to help for general searches <ref type="bibr" coords="9,264.89,635.49,24.89,8.97">[7] [2]</ref>. This year, we found that in addition to content, other sources of information can be very useful for identifying entry pages. We described two different ways of combining different sources of information into our unigram language model: either as a proper prior or by interpolating results from different ranking models. We used both methods successfully when combining content information with other sources as diverse as inlinks, URLs and anchors. URL info gives the best prior info. Adding inlinks yields marginal improvement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,243.33,476.26,125.35,8.07"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. P (entrypage | #inlinks)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,80.87,84.36,465.98,277.18"><head>Table 1 .</head><label>1</label><figDesc>MRRs Inlink and doclen priors on training data</figDesc><table coords="4,80.87,84.36,465.98,255.76"><row><cell>0.001</cell><cell></cell><cell></cell><cell></cell><cell>0.001</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.0001</cell><cell></cell><cell></cell><cell></cell><cell>0.0001</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1e-05</cell><cell></cell><cell></cell><cell></cell><cell>1e-05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1e-06</cell><cell></cell><cell></cell><cell></cell><cell>1e-06</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10</cell><cell>100</cell><cell>1000</cell><cell>10000</cell><cell>100000</cell><cell>10</cell><cell>100</cell><cell>1000</cell><cell>10000</cell><cell>100000</cell></row><row><cell></cell><cell></cell><cell>Ad hoc search</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Entry page search</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Fig. 2. P (relevant | doclen)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>run</cell><cell></cell><cell>MRR</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>content</cell><cell></cell><cell>0.26</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">content + doclen prior</cell><cell>0.21</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">0.7 * content + 0.3 * inlink 0.38</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,208.47,73.55,195.06,84.68"><head>Table 2 .</head><label>2</label><figDesc>MRRs Kleinberg@10 results on training data</figDesc><table coords="5,281.44,73.55,49.12,63.26"><row><cell>N MRR</cell></row><row><cell>content 0.26</cell></row><row><cell>1 0.18</cell></row><row><cell>5 0.33</cell></row><row><cell>10 0.32</cell></row><row><cell>50 0.30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,187.97,346.07,236.07,105.80"><head>Table 3 .</head><label>3</label><figDesc>shows the statistics. Distributions of entry pages and WT10g over URL types</figDesc><table coords="5,230.97,378.15,150.06,52.30"><row><cell cols="2">URL type #entry pages</cell><cell>#WT10g</cell></row><row><cell>root</cell><cell cols="2">38 (71.7%) 11680 (0.6%)</cell></row><row><cell>subroot</cell><cell cols="2">7 (13.2%) 37959 (2.2%)</cell></row><row><cell>path</cell><cell cols="2">3 (5.7%) 83734 (4.9%)</cell></row><row><cell>file</cell><cell cols="2">3 (5.7%) 1557719 (92.1%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,237.69,520.89,136.61,83.89"><head>Table 4 .</head><label>4</label><figDesc>). URL prior results</figDesc><table coords="5,237.69,552.97,136.61,30.39"><row><cell>run</cell><cell>MRR</cell></row><row><cell>content only</cell><cell>0.26</cell></row><row><cell cols="2">0.7 * content + 0.3 * URL prior 0.79</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,219.98,260.52,172.05,73.66"><head>Table 5 .</head><label>5</label><figDesc>MRRs of anchor runs on training data</figDesc><table coords="6,219.98,260.52,172.05,52.30"><row><cell>run</cell><cell>MRR</cell></row><row><cell>content only</cell><cell>0.26</cell></row><row><cell>anchor only</cell><cell>0.29</cell></row><row><cell>0.9 * content + 0.1 * anchor</cell><cell>0.36</cell></row><row><cell cols="2">0.63 * content + 0.07 * anchor + 0.3 * url 0.82</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,278.29,654.36,70.32,8.97"><head>Table 7 .</head><label>7</label><figDesc>shows the results. Entry Page results</figDesc><table coords="8,123.19,83.10,365.63,172.85"><row><cell>run</cell><cell>scores</cell><cell>description</cell><cell>MRR</cell></row><row><cell>tnout10epC</cell><cell>contentscore</cell><cell>Content only run</cell><cell>0.3375</cell></row><row><cell>tnout10epA</cell><cell>anchorscore</cell><cell>Anchor only run</cell><cell>0.3306</cell></row><row><cell>tnout10epCU</cell><cell>0.7 • contentscore +</cell><cell>Content run combined with URL priors</cell><cell>0.7716</cell></row><row><cell></cell><cell>0.3 • urlprior</cell><cell></cell><cell></cell></row><row><cell>tnout10epAU</cell><cell>0.7 • anchorscore+</cell><cell>Anchor run combined with URL priors</cell><cell>0.4798</cell></row><row><cell></cell><cell>0.3 • urlpriors</cell><cell></cell><cell></cell></row><row><cell>tnout10epCA</cell><cell>0.9 • contentscore+</cell><cell cols="2">Interpolation of Content and Anchor runs 0.4500</cell></row><row><cell></cell><cell>0.1 • anchorscore</cell><cell></cell><cell></cell></row><row><cell cols="2">tnout10epCAU 0.63 • contentscore +</cell><cell cols="2">Interpolation of Content and Anchor runs 0.7745</cell></row><row><cell></cell><cell>0.07 • anchorscore +</cell><cell>combined with URL priors</cell><cell></cell></row><row><cell></cell><cell>0.3 • urlpriors</cell><cell></cell><cell></cell></row><row><cell cols="2">tnout10epInlinks 0.7 • contentscore +</cell><cell cols="2">Content run combined with Inlink priors 0.4872</cell></row><row><cell></cell><cell>0.3inlinkprior</cell><cell></cell><cell></cell></row><row><cell cols="4">tnout10epKlein10 Kleinberg s auth.score @ 10 Authority scores after Kleinberg algorithm 0.3548</cell></row><row><cell></cell><cell></cell><cell>on top 10 ranks from Content run</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,211.65,150.16,188.69,8.07"><head>Table 8 .</head><label>8</label><figDesc>Results with clean (non-interpolated) priors</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="9,169.41,363.38,273.19,106.60"><head>Table 9 .</head><label>9</label><figDesc>Distribution entry pages and WT10g over different document types</figDesc><table coords="9,200.83,363.38,210.34,85.18"><row><cell>Document type</cell><cell>#entry pages</cell><cell>#WT10g</cell></row><row><cell>root with 1-10 inlinks</cell><cell>39 (36.1%)</cell><cell>8938 (0.5%)</cell></row><row><cell cols="2">root with 11-100 inlinks 25 (23.1%)</cell><cell>2905 (0.2%)</cell></row><row><cell cols="2">root with 101-1000 inlinks 11 (10.2%)</cell><cell>377 (0.0%)</cell></row><row><cell>root with 1000+ inlinks</cell><cell>4 (3.7%))</cell><cell>38 (0.0%)</cell></row><row><cell>subroot</cell><cell>15 (13.9%)</cell><cell>37959 (2.2%)</cell></row><row><cell>path</cell><cell>8 (7.4%)</cell><cell>83734 (4.9%)</cell></row><row><cell>file</cell><cell cols="2">6 (5.6%) 1557719 (92.1%)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,81.96,661.64,415.81,8.07"><p>We apply a simplified version of the model developed in<ref type="bibr" coords="2,286.46,661.64,9.52,8.07" target="#b2">[3]</ref>, where λ is term specific, denoting the term importance</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,81.96,673.30,112.97,7.05"><p>http://www.google.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,81.96,683.99,139.45,8.07"><p>Also called Jelinek-Mercer smoothing.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="7,81.96,672.82,212.69,8.07"><p>We recomputed the priors on the whole set of training data.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,75.36,98.75,463.51,8.07" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="10,157.03,98.75,319.20,8.07">The Anatomy of a Large-Scale Hypertextual Web Search Engine Proceedings of WWW98</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<pubPlace>Brisbane</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,75.36,109.71,464.63,8.07;10,83.71,120.67,129.52,8.07" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,94.34,109.71,160.13,8.07">Hawking Overview of the TREC-9 Web Track</title>
		<author>
			<persName coords=""><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,260.73,109.71,210.87,8.07">Proceedings of the 9th Text Retrieval Conference (TREC-9)</title>
		<meeting>the 9th Text Retrieval Conference (TREC-9)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>National Institute for Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,75.36,131.63,464.63,8.07;10,83.71,142.59,291.20,8.07" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="10,133.02,131.63,173.44,8.07">Using language models for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<ptr target="htpp://www.cs.utwente.nl/˜hiemstra/paper/thesis.pdf" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>Centre for Telematics and Information Technology, University of Twente</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="10,75.36,153.55,464.64,8.07;10,83.71,164.51,335.71,8.07" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,187.29,153.55,210.42,8.07">Twenty-One at TREC7: Ad-hoc and Cross-Language track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,405.37,153.55,134.63,8.07;10,83.71,164.51,77.87,8.07">Proceedings of the 7th Text Retrieval Conference (TREC-7)</title>
		<meeting>the 7th Text Retrieval Conference (TREC-7)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="227" to="238" />
		</imprint>
		<respStmt>
			<orgName>National Institute for Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,75.36,175.47,464.64,8.07;10,83.71,186.42,120.29,8.07" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,144.33,175.47,182.31,8.07">Authorative Sources in a Hyperlinked Environment</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,334.82,175.47,205.19,8.07;10,83.71,186.42,39.17,8.07">Proceedings of 9th ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>9th ACM-SIAM Symposium on Discrete Algorithms</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="668" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,75.36,197.38,464.63,8.07;10,83.71,208.34,331.67,8.07" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="10,236.23,197.38,303.77,8.07;10,83.71,208.34,170.87,8.07">Heijden Combining a mixture language model and Naive Bayes for multi-document summarisation Working notes of the DUC</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Spitters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Van Der</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001. 2001. 2001</date>
			<pubPlace>New Orleans</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,75.36,219.30,464.63,8.07;10,83.71,230.26,278.69,8.07" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Westerveld</surname></persName>
		</author>
		<title level="m" coord="10,193.80,219.30,346.20,8.07;10,83.71,230.26,73.59,8.07">TNO/UT at TREC-9: How different are Web documents? Proceedings of the 9th Text Retrieval Conference (TREC-9</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>National Institute for Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,75.36,241.22,464.64,8.07;10,83.71,252.18,416.19,8.07" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<title level="m" coord="10,167.77,241.22,372.23,8.07;10,83.71,252.18,392.06,8.07">A Study of Smoothing Methods for Language Models Applied to Ad Hoc Information Retrieval Proceedings of the 2001 ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;01)</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
