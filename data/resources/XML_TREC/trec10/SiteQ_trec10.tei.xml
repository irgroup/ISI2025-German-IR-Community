<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,78.51,99.13,454.99,15.93;1,143.67,115.09,324.73,15.93">SiteQ: Engineering High Performance QA system Using Lexico-Semantic Pattern Matching and Shallow NLP</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,79.23,142.35,78.69,11.30"><forename type="first">Gary</forename><forename type="middle">Geunbae</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName coords="1,164.89,142.35,54.60,11.30"><forename type="first">Jungyun</forename><surname>Seo</surname></persName>
						</author>
						<author>
							<persName coords="1,231.15,142.35,61.27,11.30"><forename type="first">Seungwoo</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName coords="1,299.38,142.35,58.55,11.30"><forename type="first">Hanmin</forename><surname>Jung</surname></persName>
						</author>
						<author>
							<persName coords="1,365.22,142.35,68.68,11.30"><forename type="first">Bong-Hyun</forename><surname>Cho</surname></persName>
						</author>
						<author>
							<persName coords="1,441.35,142.35,52.49,11.30"><forename type="first">Changki</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName coords="1,500.80,142.35,31.78,11.30;1,107.43,153.74,49.38,11.30"><forename type="first">Byung- Kwan</forename><surname>Kwak</surname></persName>
						</author>
						<author>
							<persName coords="1,164.62,153.74,62.21,11.30"><forename type="first">Jeongwon</forename><surname>Cha</surname></persName>
						</author>
						<author>
							<persName coords="1,234.15,153.74,61.96,11.30"><forename type="first">Dongseok</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName coords="1,303.78,153.74,45.30,11.30"><forename type="first">Joohui</forename><surname>An</surname></persName>
						</author>
						<author>
							<persName coords="1,356.52,153.74,55.36,11.30"><forename type="first">Harksoo</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName coords="1,429.56,153.74,63.68,11.30"><forename type="first">Kyungsun</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science &amp; Engineering</orgName>
								<address>
									<country>POSTECH</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Sogang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,78.51,99.13,454.99,15.93;1,143.67,115.09,324.73,15.93">SiteQ: Engineering High Performance QA system Using Lexico-Semantic Pattern Matching and Shallow NLP</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">24599DA1256C099FE3609AF9AD62DCCA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In TREC-10, we participated in the web track (only ad-hoc task) and the QA track (only main task). In the QA track, our QA system (SiteQ) has general architecture with three processing steps: question processing, passage selection and answer processing. The key technique is LSP's (Lexico-Semantic Patterns) that are composed of linguistic entries and semantic types. LSP grammars constructed from various resources are used for answer type determination and answer matching. We also adapt AAD (Abbreviation-Appositive-Definition) processing for the queries that answer type cannot be determined or expected, encyclopedia search for increasing the matching coverage between query terms and passages, and pivot detection for the distance calculation with answer candidates.</p><p>We used two-level answer types consisted of 18 upper-level types and 47 lower-level types. Semantic category dictionary, WordNet, POS combined with lexicography and a stemmer were all applied to construct the LSP knowledge base. CSMT (Category Sense-code Mapping Table ) tried to find answer types using the matching between semantic categories and sense-codes from WordNet. Evaluation shows that MRR for 492 questions is 0.320 (strict), which is considerably higher than the average MRR of other 67 runs.</p><p>In the Web track, we focused on the effectiveness of both noun phrase extraction and our new PRF (Pseudo Relevance Feedback). We confirmed that our query expansion using PRF with TSV function adapting TF factor contributed to better performance, but noun phrases did not contribute much. It needs more observations for us to make elaborate rules of tag patterns for the construction of better noun phrases.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The goal of the QA track is to foster research on systems that retrieve answers rather than documents in response to a question <ref type="bibr" coords="1,239.03,528.60,16.10,11.30" target="#b11">[11]</ref> <ref type="bibr" coords="1,255.13,528.60,16.10,11.30" target="#b12">[12]</ref>. The focus is on systems that can function in unrestricted open domains <ref type="bibr" coords="1,191.73,551.39,15.02,11.30" target="#b11">[11]</ref>.</p><p>The web track features ad hoc search tasks on a document collection that is a snapshot of the World Wide Web. The main focus of this track is to form a Web test collection using pooled relevance judgments. We will describe our systems and experiences for both QA and Web tracks in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">QA track: Systems and Experiences</head><p>In TREC-10, the QA track consisted of three separate tasks: the main task, the list task and the context task. We participated in only the main task.</p><p>The main task is similar to the task in previous QA tracks (TREC-8, TREC-9). NIST provided 500 questions that seek short, fact-based answers. Some questions may not have a known answer in the document collection. In that case, the response string "NIL" is judged correct. This differs from the previous QA tracks and makes the task somewhat more difficult. The answer-string should contain no more than 50 bytes; 250-byte runs were abandoned this year. Participants must return at least one and no more than five responses per question ranked by preferences.</p><p>The document collection consists of the following six data sets: AP newswire, Wall Street Journal, San Jose Mercury News, Financial Times, Los Angeles Times, and Foreign Broadcast Information Service. The documents are SGML tagged, and each document in this collection has a unique identifier in the field.</p><p>Distinguished from an information retrieval, a QA system must retrieve answers rather than documents as responses to a question. As an ordinary course of step, we focused on what can be a possible answer, how our system can determine the answer type of a question, and how our system can detect instances of each answer type in a document. We classified possible answers and designed a method for determining the answer type of each question and detecting instances of it in a document. We have not constructed the index of document collection this time and instead used the ranked document list provided by NIST for each question.</p><p>Our QA system, SiteQ, consists of three important steps; question processing, passage selection and answer processing, which will be explained in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Question Processing</head><p>In general, a question answering system analyzes an input question at first step. It is important to understand what a user wants to find; whether it is person's name, location, organization, or any other types. To do so, we first classified the types of possible answers <ref type="bibr" coords="2,244.11,394.57,12.76,11.30" target="#b1">[1]</ref>[2][3] <ref type="bibr" coords="2,282.39,394.57,12.76,11.30" target="#b6">[6]</ref> and used Lexico-Semantic Patterns (LSP) to determine the answer type of a question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Answer Type</head><p>We classified the type of answers to fact-seeking questions <ref type="bibr" coords="2,119.71,462.72,16.04,11.30" target="#b12">[12]</ref>. Referring to the types used in FALCON <ref type="bibr" coords="2,115.58,474.12,11.82,11.30" target="#b3">[3]</ref>, we analyzed the questions used in the previous QA tracks and their answers judged correct and constructed 2-level hierarchy of answer types. Hierarchical structure of answer types is useful since only YEAR is available for 'what year' question, but YEAR, MONTH, DAY, or TIME is available for 'when' question. Our answer type has 18 types at top level as shown in the box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Lexico-Semantic Patterns</head><p>Usually an interrogative in a question is an important factor but it is not enough to determine the answer type. LASSO first determined the question class and the question focus, and then determined the answer type by using them <ref type="bibr" coords="2,524.90,133.23,11.91,11.30" target="#b6">[6]</ref>.</p><p>The question class is defined as an interrogative and the question focus is defined as the main information required by the interrogation. We used Lexico-Semantic Patterns (LSP) to determine the type of answer expected. Usually in addition to an interrogative in a question, its surrounding words or their senses are expressed in LSP, which substitutes the question class and focus word.</p><p>LSP grammar is composed of condition part and conclusion part. The conclusion part is the type of answer expected if the LSP in condition part is matched. LSP is composed of lexical entries, POS tag, semantic category and their sequence, and is expressed in regular expression. For example, a grammar "(%who)(%be)(@person)</p><p>PERSON" can be constructed from a question "Who was President Cleveland's wife?". '%who' and '%be' is lexical entries and '@person' is a semantic category for representing the position of a person. We have manually constructed LSP grammar from the questions used in the previous QA tracks and the questions gathered from the Web by ourselves. Among them 361 entry LSP grammar was used for this year's QA track. Scanning the tagged question from right to left, this module detects the boundary of noun phrase and its head noun. To do this, we collected the POS patterns for noun phrases from the questions. A noun phrase almost always ends with a noun, usually starts with a pre-determiner, a determiner, an adjective, a possessive pronoun, or a noun. The rightmost noun in a noun phrase is selected as a head noun. Two noun phrases can be combined into a larger noun phrase by connecting them using a preposition 'of' or a possessive ending. In case of a preposition 'of', the head of its left-side noun phrase is selected as a head of the combined noun phrase, but in case of a possessive ending the head of its right-side noun phrase is selected.</p><p>It is important that detecting a head in a noun phrase since the sense of the head noun plays an important role in determining the expected answer type but its modifiers are useful for justifying final answers. In the above question, "President Cleveland's wife" is detected as a noun phrase, and 'wife' is its head and clarifies the answer type of the question is PERSON. In contrast to this question, the expected answer type of a question "Who is Cleveland?" will be POSITION, which means the position of Cleveland (i.e., president) will be an answer.</p><p>At the third step, based on normalization dictionary (Qnorm dic) and WordNet, each word in a question is converted into LSP code to be matched with the condition part of LSP grammar by regular expression. "President Cleveland's wife" is converted into '@person' since it is a noun phrase and its head is 'wife', of which semantic category is '@person'.</p><p>The following box shows how the answer type of a question "Who was President Cleveland's wife?" is determined as PERSON. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Passage Selection</head><p>We have not constructed an index database from the document collection since we had no enough time and computing resources this year. Therefore we couldn't help using only the document list provided by NIST and selecting relevant passages from them by scanning the whole documents and matching the keywords. The documents were ranked by document similarity because they were retrieved by the PRISE <ref type="bibr" coords="3,516.17,178.70,11.46,11.30" target="#b7">[7]</ref>, a document retrieval system rather than a passage retrieval system. Generally, however, a document does not fit for detecting candidate answers within itself since it is too large and contains too much extra information. By analyzing the previous questions and their answers, we can assume that answers to a question usually occur comparatively near to the matched keywords in a document. This means that the answer can occur in any ranked documents and we had better select passages from each document and rank them by passage similarity. Then we can use top passages to find candidate answers. To do so, we first must define passage and keywords to be used in selecting relevant passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Keywords</head><p>We define keywords to be used in selecting passages from the retrieved documents. We first remove useless words in a question and then use the remained words as three types of keywords considering lexical normalization and semantic similarity. Finally we assign weights to each keyword.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-Removing stop words</head><p>The useless words in a question are removed first by POS tag and stop word list, which has 568 entries. Then the following five heuristics are applied to the remaining words.</p><p>a. When a word like 'kind', 'sort', 'one', 'most', etc. occurs in the left side of a preposition 'of', it is removed; eg) What kind of dog …? Name one of the Seven Wonders …? b. When a word like 'name', 'nickname', etc. occurs in the right side of a possessive ending, it is removed; eg) What was the man's name who was killed …? What is Shakespeare's nickname? c. When a question is expressed in imperative sentence, the imperative verb is removed; eg) Tell me what city …? d. When a verb needs a to-infinitive, the verb is removed; eg) Where do lobsters like to live? e. When an adjective or an adverb follows an interrogative 'how', the adjective or adverb is removed; eg) How wide is the Atlantic Ocean?</p><p>-The type of keyword After removing all stop words, the remaining words are considered as question keywords. We define following three types of keyword to solve the mismatching problem of keywords caused by lexical variants and synonyms.</p><p>a. Lemma form The lemma form of a word is used as a keyword except the superlative adjective or adverb, in which case the word itself is used as a keyword; eg) invented invent, inventers inventer, smallest smallest b. Stemmed form Though the lemma form solves somewhat of the mismatching problem, it is not enough to solve the mismatch between 'inventer' and 'invented'. This can be resolved by using a stemmer like the Porter's stemmer <ref type="bibr" coords="4,245.37,360.49,12.05,11.30" target="#b8">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>c. WordNet sense (in case of noun or noun phrase)</head><p>To match a word 'ship' in a question with a word 'steamship' in a document, we must compute semantic similarity between a question keyword and a document word. Using the WordNet <ref type="bibr" coords="4,193.87,440.04,11.64,11.30">[5]</ref>, the synonym or hyponym of a question keyword occurring in documents is matched with the question keyword.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-The weight of the keyword</head><p>The lemma form is weighted by its part of speech. A proper noun, a common noun starting with a capital letter, and a superlative has higher weight than a verb, an adjective and an adverb. The stemmed form has some of the weights its lemma form has. The keyword (noun or noun phrase) matched by WordNet sense has the lowest weight relative to the number of its component words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Passages</head><p>A passage is composed of more than one sentence segmented by punctuation. We make adjacent two sentences into a passage if they have a lexical chain, which indicates that a sentence has a noun and the other sentence has its anaphora. We however limited a passage to maximum three sentences since the more sentences have the more extra information, which may increase incorrect candidate answers. Each sentence from a document gets scored by matching its terms with query terms (Score1) and by considering the distance and number of the matched terms (Socre2). Score1 consists of sum of the weights of matched terms. Each query term is tried to be matched with document terms in the order of lemma form, WordNet sense and stemmed form, and gets assigned the weight of the first matched term type. Passages are ranked by sum of their sentence scores. Our system selected 1000 passages from 1000 retrieved documents per question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Answer Processing</head><p>Answer processing selects answer candidates matching the answer type from each passage and ranks them. It uses stemmer <ref type="bibr" coords="4,467.71,559.31,15.11,11.30" target="#b8">[8]</ref>, thesaurus (WordNet) <ref type="bibr" coords="4,369.59,570.71,11.63,11.30">[5]</ref>, encyclopedia for its performance elevation. Answer processing is composed of four steps: Answer Matching, Pivot Detection, AAD Processing and Answer Ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">System Architecture</head><p>Figure <ref type="figure" coords="4,366.85,638.86,5.57,11.30">2</ref> shows components of answer processing system. Answer matching (detection) finds answer candidates in POS-tagged passages selected by passage selection using the answer type determined by question processing. A query term, which shows up in various forms in the passage, is called "pivot". Answer ranking uses these pivots in scoring answer candidates. When the answer type of a question is "LANGUAGE_UNIT", AAD processing finds context-based answer candidates that are in abbreviated, appositive and definitive relation with the pivots. Answer ranking calculates the score of each answer candidate with various parameters, filters them according to the range and the type of answer, and finally sorts them.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Answer Matching (detection)</head><p>Figure <ref type="figure" coords="5,117.75,643.66,5.50,11.30" target="#fig_1">3</ref> shows the procedures of answer matching. Answer matching assigns semantic categories to each answer candidate by matching between LSP grammar and the normalized answer form from the following procedure. The procedure first searches semantic category dictionary. In case of its failure, it tries thesaurus matching between the sense-code from WordNet and the semantic categories in the CSMT (Category to Sense code Mapping Table ), and then uses POS combined with lexicography.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-Searching semantic category dictionary</head><p>Semantic category dictionary has about 80,000 entries including single word and compound one. Each entry is assigned a semantic category among 65 ones which are components of LSP abstraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-Trying thesaurus matching</head><p>Sense code retrieved from WordNet <ref type="bibr" coords="5,514.46,269.53,12.71,11.30">[5]</ref> is mapped to each category among 65 semantic categories if it has a similarity greater than a threshold value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-POS combined with lexicography</head><p>In case of failure of searching semantic category dictionary, POS combined with lexicography is used to build normalized form. If "Newton" has "np" (proper noun) POS tag, "Np" is used for normalization. It is because capitalization is important for detecting candidate answers, especially named entities.</p><p>When a normalized form matched with a LSP of the answer type, its terms are chosen as an answer candidate. The followings show some examples of LSP and its actual instances. cd@unit_lengthcd@unit_length length|1|4|4 10 feet 5 inches cd@unit_length%per@unit_time speed|1|4|4 3 km per hour</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Pivot Detection</head><p>Pivots corresponding with query terms emerge in the passage in various way: full matching terms, partial matching ones for multi-words, stem matching ones for inflections and semantic synonyms using WordNet. When answer ranking scores answer candidates, pivots are weighted according to these normalized representations of query terms in a passage. When an answer candidate itself is a pivot, it is excluded from answer candidate set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4">AAD Processing</head><p>In the case that no answer type can be determined in question processing due to short of information ("LANGUAGE_UNIT" answer type), AAD processing finds context-based answer candidates that are in abbreviated, appositive and definitive relation with the pivots. It uses lexicographic patterns for abbreviation, and noun phrase chunking and clue words such as "socalled" and "stand for" for apposition and definition. The followings are examples of questions, of which answer type is LANGUAGE_UNIT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Why does the moon turn orange? What is epilepsy? What imaginary line is halfway between the North and South Poles? What is done with worn or outdated flags?</head><p>For more improvement of performance, AAD processing uses encyclopedia information extracted from WordNet glossary <ref type="bibr" coords="6,220.90,326.41,11.56,11.30">[5]</ref>. We gathered descriptions of about 110,000 words from WordNet glossary and removed stop words from the descriptions. Answer ranking reweighs each answer candidate through its semantic similarity with remaining terms in the descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.5">Answer Ranking</head><p>Score of each answer candidate is mainly calculated by distance between pivots within some window in each selected passage. In addition to basic distance measure, the type and ratio matching each pivot with query terms, mean distance between pivots, and semantic type of answer candidate (especially in case of AAD processing) are all used for scoring each answer candidate: This formula (Eq. 4) reflects some of the following assumptions: (1) Reliable answer candidates would appear near query terms, so called pivots, in a passage. <ref type="bibr" coords="6,449.52,212.78,4.34,11.30">(</ref>2) Reliable answer candidates would show up around pivots which matched with query terms more exactly. (3) In the case of "LANGUAGE_UNIT" answer type, answer candidates extracted from AAD processing are more reliable than the others. ( <ref type="formula" coords="6,470.96,269.53,4.37,11.30" target="#formula_0">4</ref>) The smaller mean distance between pivots is, the more reliable an answer candidate around them would be. ( <ref type="formula" coords="6,522.16,292.33,4.24,11.30">5</ref>) If most of query terms appear in a passage, an answer candidate around their pivots is more reliable. <ref type="bibr" coords="6,358.22,326.41,13.21,11.30" target="#b6">(6)</ref> Finally, reliable answer candidates show up in some limited distance between pivots.</p><formula xml:id="formula_0" coords="6,82.23,534.32,199.02,61.11">∑ = - ⋅ ⋅ ⋅ ⋅ - ⋅ = P N j j j p i pivot pivot avg pivot i dist dist r N AADfactor S dist dist R Score 1 max . max . ) 1 ( 1 ) 1 (<label>(4</label></formula><p>After scoring all answer candidates, answer ranking filters less reliable answer candidates according to the range and type of the answer, sorts remaining answer candidates by their scores and presents N most reliable answer candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Experiments in TREC-10</head><p>We participated in the main task of QA track. 500 questions were given to each participant to evaluate their QA systems. After all evaluation, it was known that 49 questions among them have no known correct answers in the document collection. Eight questions were excluded from the evaluation due to various problems with those questions.</p><p>Table <ref type="table" coords="6,353.92,508.20,5.57,11.30" target="#tab_6">2</ref> shows that, unlike the questions used in the previous QA tracks, questions like "what is X?" were remarkably increased. So, the task became more difficult since the answer types of such questions are often not specified definitely.</p><p>For each question, SiteQ used the top 1000 documents provided by NIST (PRISE search engine <ref type="bibr" coords="6,348.76,587.75,11.41,11.30" target="#b7">[7]</ref>), selected top 1000 passages from those documents, detected top five candidate answers from those passages and picked out 50-byte string including the candidate answer as an answer string. When the score of a candidate answer was lower than a threshold value or less than five candidates were detected, we added "NIL" string in the appropriate rank, which means that there might be no answer.</p><p>We submitted only one run (posqa10a) and it was evaluated by mean reciprocal rank (MRR) like the previous QA tracks <ref type="bibr" coords="7,189.06,110.55,16.09,11.30" target="#b13">[13]</ref>. The unsupported answers were judged incorrect in strict judgment but correct in lenient judgment. Table <ref type="table" coords="7,243.68,133.23,4.28,11.30">1</ref> shows the number of questions judged correct in each judgment and the mean reciprocal rank of 492 questions. Comparing with the average MRR of the 67 other runs submitted this year, our system located correct answers at rank 1 for relatively many questions. The difference between the strict and the lenient MRR arises because a word of the same answer type was added to 50-byte string when we picked out the answer string including a candidate answer.  <ref type="table" coords="7,105.17,383.88,4.88,11.30">1</ref> The number of questions judged correct and MRR Table <ref type="table" coords="7,112.85,664.06,5.57,11.30" target="#tab_6">2</ref> shows the MRR for each type of question. For the questions like "What is X?", our system shows relatively good performance. This means that AAD processing was effective for those questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rank</head><p>According to table 3, we know that the systems in TREC-10 show slightly higher performance than the systems in TREC-9. But this does not necessarily refer to the improvement of the systems. Table <ref type="table" coords="7,349.12,267.73,6.22,11.30">3</ref> The comparison between TREC-10 and TREC-9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TREC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Web track: Systems and Experiences</head><p>This is our first participation in the Web track of TREC. Our system is based on POSNIR/K, Korean natural language information retrieval system <ref type="bibr" coords="7,525.12,347.89,11.93,11.30" target="#b4">[4]</ref>. For TREC-10, we focused on effectiveness in both noun phrase extraction and PRF (Pseudo Relevance Feedback). While query expansion using PRF turned out to contribute to the performance significantly, the noun phrases were used with single terms actually didn't contribute much.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Keyword Extraction</head><p>For keyword extraction, we tagged the document collection, wt10g, and queries using POSTAG/E, the English POS (Part-Of-Speech) tagger based on HMM. The output of POSTAG/E is composed of lexis, POS tag, and lemma. From the result of the tagger, we selected keywords using two-phase extraction. If the lemmas were registered in the dictionary, they were selected. On the other hand, lexes were stemmed by Porter's stemmer <ref type="bibr" coords="7,353.39,563.75,15.78,11.30" target="#b8">[8]</ref> and then the stemmed lexes were selected as keywords. Stop words were eliminated using two kinds of stop list: common stop list containing 569 words, and query-specific stop list containing 28 words which must be removed from the query.</p><p>For constructing noun phrases, we made lexicosyntactic rules based on the POS-tag patterns. Some of the rules are described below. </p><formula xml:id="formula_1" coords="7,347.89,677.38,161.71,22.70">Term1/{NN | NP} Term2/{NN | NP} Term1_Term2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Initial Retrieval</head><p>Our retrieval system uses 2-poisson model based on the probabilistic term distribution. The system retrieves top-raked documents after giving scores to each document of a target data collection with each query term list made from the keyword extraction process. For scoring, a rank system uses Okapi BM25 formula <ref type="bibr" coords="8,166.02,246.86,13.12,11.30" target="#b9">[9]</ref> as shown below.</p><formula xml:id="formula_2" coords="8,74.07,272.44,216.44,123.84">      + + - = 5 . 0 5 . 0 log ) 1 ( n n N w (5)     + + × ×       + × + - × × + = ∑ ∈ ) ,<label>( ) , ( ) 1 ( ) ) 1 (( ) 1 ( ) , ( 3 3</label></formula><formula xml:id="formula_3" coords="8,72.75,320.51,214.55,73.35">) 1 ( 1 1 t q tf k t q tf k w tf avdl dl b b k tf k q d Score q q q t t d t<label>(6)</label></formula><p>, where N is the number of documents in the collection, n is the number of documents containing the term, tft is the term frequency of term t in a document d, dld is the document length, avdl is the average document length, tfq(q,t) is the term frequency of query term t in the query q, and k1, b, k3 are tunable constant parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Query Expansion</head><p>Query expansion is achieved through PRF (Pseudo Relevance Feedback). In the process of PRF, top-ranked documents are regarded as relevant and TSV (Term Selection Value) is given to all single terms except stop words in them. Then, top-ranked single terms are expanded and added to the original query term list. In this process, the weights of both original and expanded query terms are reweighted by Eq.( <ref type="formula" coords="8,174.21,600.35,4.55,11.30" target="#formula_4">7</ref>) reflecting relevance and non-relevance information <ref type="bibr" coords="8,191.22,611.75,16.44,11.30" target="#b10">[10]</ref>. </p><formula xml:id="formula_4" coords="8,72.39,638.61,216.71,54.86">+ - + + - - + - + - + + + - + + = s S s S k S n N n S k k r R r R k R n N N k R k k w (<label>7)</label></formula><p>, where N, n is the same as in the Eq.( <ref type="formula" coords="8,489.60,110.55,4.33,11.30">5</ref>), R is the number of documents known to be relevant to a specific topic, r is the number of relevant documents containing the term, S is the number of documents known to be non-relevant, s is the number of non-relevant documents containing the term, and k5, k6 are tunable constant parameters.</p><p>For TSV function, we developed and compared some TSV formulas adapting diverse TF (Term Frequency) factors.  , where w (1) is Eq. ( <ref type="formula" coords="8,399.33,378.97,3.79,11.30" target="#formula_4">7</ref>).</p><formula xml:id="formula_5" coords="8,318.61,238.04,209.51,131.11">TSV R d d d t ×           × + = ∑ ∈ (8) ) 1 ( , log( w dl avgdl tf TSV R d d d t ×           × = ∑ ∈<label>(9) ) 1 ( , 1 , ) ) 1 (</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Final Retrieval</head><p>Final retrieval process is the same as the initial one except that, this time, each query term has the new weights given by Eq. ( <ref type="formula" coords="8,410.70,438.40,3.89,8.96" target="#formula_4">7</ref>) and the expanded query term list is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Experiments in TREC-10</head><p>Table <ref type="table" coords="8,355.47,481.92,5.63,11.30">4</ref> summarizes the TREC-10 results. The results indicate that when a query was expanded using PRF, the performance was better, but noun phrases didn't give much contribution to the performance. As for TSV function in using PRF, Eq. ( <ref type="formula" coords="8,339.06,538.67,8.93,11.30">10</ref>) which is adapting TF factor of the weight formula of Okapi was better than any others.</p><p>In order to further validate the results, the t-test was performed on the data (Table <ref type="table" coords="8,481.06,572.75,3.94,11.30" target="#tab_7">5</ref>). The table shows the mean difference, the standard deviation difference, the t-statistics and the probability of average precision and recall-precision for no-PRF (baseline) versus PRF (using Eq. ( <ref type="formula" coords="8,494.18,618.23,8.64,11.30">10</ref>)) case. Though there are no significant differences for average precision in TREC-9 topics, the table shows the rest of the performance are all significantly improved when PRF was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No query expansion</head><p>Query expansion title only title+desc title only no phrases phrases phrases phrases baseline Eq. ( <ref type="formula" coords="9,351.84,151.58,4.16,11.30" target="#formula_4">7</ref>) Eq. ( <ref type="formula" coords="9,405.68,151.58,4.45,11.30">8</ref>) Eq. ( <ref type="formula" coords="9,459.76,151.58,4.35,11.30" target="#formula_5">9</ref>)</p><p>Eq. ( <ref type="formula" coords="9,512.07,151.58,8.93,11.30">10</ref>) </p><formula xml:id="formula_6" coords="9,286.34,168.26,26.14,11.30">TREC</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In TREC-10, we participated in the QA track and the Web track.</p><p>We submitted a run for the main task of the QA track and it was judged and evaluated by the reciprocal rank. The MRR for 492 questions is 0.320 (strict), which is considerably higher than the average MRR of other 67 runs.</p><p>In the Web track, we confirmed that our new query expansion using PRF with TSV function adapting TF factor contributed to better performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,115.95,597.59,134.25,11.30"><head>Figure 3</head><label>3</label><figDesc>Figure 3 Answer matching</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,79.95,99.15,460.88,599.86"><head></head><label></label><figDesc>) Scorei: final score of ith Answer Candidate Np: number of Pivots rj: weight factor of match type of jth Pivot distj: distance between jth Pivot and ith Answer Candidate distmax: max value of distj</figDesc><table coords="6,79.95,596.87,214.51,102.14"><row><cell>Rpivot: ratio of matched pivots</cell></row><row><cell>distavg.pivot: average distance between pivots distmax.pivot: maximum of distance between pivots</cell></row><row><cell>Si: intermediate score of ith Answer Candidate</cell></row><row><cell>AADfactor: if question type is language-unit,</cell></row><row><cell>if NE type is AAD, 1</cell></row><row><cell>otherwise 4 otherwise 1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="7,73.35,433.96,209.21,204.81"><head>Table 2 The frequency and MRR in each type of question</head><label>2</label><figDesc></figDesc><table coords="7,73.35,433.96,209.21,174.29"><row><cell>Q-ty pe</cell><cell>freq</cell><cell cols="2">MRR (strict) (lenient) MRR</cell></row><row><cell>how + adj/adv</cell><cell>31</cell><cell>0.31 6</cell><cell>0.332</cell></row><row><cell>how do</cell><cell>2</cell><cell>0.250</cell><cell>0.250</cell></row><row><cell>what do</cell><cell>24</cell><cell>0.050</cell><cell>0.050</cell></row><row><cell>what is</cell><cell>242</cell><cell>0.308</cell><cell>0.320</cell></row><row><cell>w hat/w hich noun</cell><cell>88</cell><cell>0.289</cell><cell>0.331</cell></row><row><cell>when</cell><cell>26</cell><cell>0.362</cell><cell>0.362</cell></row><row><cell>where</cell><cell>27</cell><cell>0.51 5</cell><cell>0.51 5</cell></row><row><cell>who</cell><cell>46</cell><cell>0.464</cell><cell>0.47 1</cell></row><row><cell>wh y</cell><cell>4</cell><cell>0.125</cell><cell>0.125</cell></row><row><cell>name a</cell><cell>2</cell><cell>0.500</cell><cell>0.500</cell></row><row><cell>Total</cell><cell>492</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="9,78.15,168.26,455.75,216.49"><head>Table 5 T</head><label>5</label><figDesc></figDesc><table coords="9,312.48,168.26,13.07,11.30"><row><cell>-9</cell></row></table><note coords="9,142.89,373.45,370.31,11.30"><p>-test: Avg. Precision &amp; R-Precision -no-PRF (baseline) vs. PRF (Eq. (10))</p></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,75.58,555.95,63.94,11.30" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,88.01,578.63,207.35,11.30;9,79.95,590.03,215.39,11.30;9,79.95,601.31,134.28,11.30" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="9,182.68,578.63,112.69,11.30;9,79.95,590.03,210.97,11.30">The process of question answering: A computer simulation of cognition</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Lehnert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978">1978</date>
			<publisher>Lawrence Erlbaum Associates</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.14,612.71,208.09,11.30;9,79.95,624.11,215.42,11.30;9,79.95,635.51,80.09,11.30" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,79.95,624.11,154.53,11.30">A taxonomy of question generation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Graesser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Horgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,241.48,624.11,53.89,11.30;9,79.95,635.51,40.22,11.30">Questioning Exchange</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3" to="16" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,89.64,646.78,205.75,11.30;9,79.95,658.18,215.40,11.30;9,79.95,669.58,215.32,11.31;9,79.95,680.86,97.54,11.30" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,79.95,658.18,215.40,11.30;9,79.95,669.58,31.84,11.30">FALCON: Boosting knowledge for answer engines</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,131.81,669.58,163.46,11.31;9,79.95,680.86,49.72,11.30">Proceedings of the 9 th Text REtrieval Conference</title>
		<meeting>the 9 th Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.68,692.26,207.64,11.30;9,325.57,396.84,215.32,11.30;9,325.57,408.24,215.35,11.30;9,325.57,419.64,215.46,11.30;9,325.57,430.92,211.56,11.30" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,214.32,692.26,81.00,11.30;9,325.57,396.84,215.32,11.30;9,325.57,408.24,130.93,11.30">Statistical Natural Language Query System THE IR based on P-Norm Model; Korean TREC-1</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,478.44,408.24,62.48,11.30;9,325.57,419.64,215.46,11.30;9,325.57,430.92,110.09,11.30">Proceeding of The 5 th Korea Science &amp; Technology Infrastructure Workshop</title>
		<meeting>eeding of The 5 th Korea Science &amp; Technology Infrastructure Workshop</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="189" to="202" />
		</imprint>
	</monogr>
	<note>in Korean</note>
</biblStruct>

<biblStruct coords="9,338.59,442.32,202.36,11.30;9,325.57,453.72,215.22,11.30;9,325.57,465.00,67.97,11.30" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,443.71,442.32,97.24,11.30;9,325.57,453.72,37.04,11.30">WordNet: A lexical database</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,371.75,453.72,126.10,11.30">Communication of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,335.73,476.40,205.24,11.30;9,325.57,487.80,215.29,11.30;9,325.57,499.08,215.30,11.30;9,325.57,510.47,45.62,11.30" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,325.57,487.80,196.21,11.30">LASSO: A tool for surfing the answer net</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,325.57,499.08,215.30,11.30">Proceedings of the 8 th Text REtrieval Conference</title>
		<meeting>the 8 th Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>TREC-8</note>
</biblStruct>

<biblStruct coords="9,352.80,521.87,23.33,11.30;9,399.96,521.87,29.09,11.30;9,452.90,521.87,29.86,11.30;9,506.48,521.87,34.31,11.30;9,325.57,533.15,210.22,11.30;9,325.57,544.55,100.55,11.30" xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Nist</forename><surname>Prise</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Search</forename><surname>Engine</surname></persName>
		</author>
		<ptr target="http://www.itl.nist.gov/div894/894.02/works/papers/zp2/main.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,335.24,555.95,205.69,11.30;9,325.57,567.23,167.38,11.30" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,431.88,555.95,109.05,11.30;9,325.57,567.23,38.28,11.30">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,370.48,567.23,36.02,11.30">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,332.76,578.63,208.13,11.30;9,325.57,590.03,215.43,11.30;9,325.57,601.31,137.76,11.30" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,461.60,578.63,73.84,11.30">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,344.81,590.03,196.19,11.30;9,325.57,601.31,49.73,11.30">Overview of the Third Text Retrieval Conference</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="109" to="126" />
		</imprint>
	</monogr>
	<note>TREC-3</note>
</biblStruct>

<biblStruct coords="9,340.46,612.71,200.34,11.30;9,325.57,624.11,215.30,11.30;9,325.57,635.50,215.46,11.31;9,325.57,646.78,215.45,11.30;9,325.57,658.18,215.38,11.30;9,325.57,669.58,73.96,11.30" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,527.51,612.71,13.29,11.30;9,325.57,624.11,215.30,11.30;9,325.57,635.51,51.06,11.30">On relevance weights with little relevance information</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,399.97,635.50,141.07,11.31;9,325.57,646.78,215.45,11.30;9,325.57,658.18,215.38,11.30;9,325.57,669.58,39.68,11.30">Proceeding of the 20 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>eeding of the 20 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="16" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,342.13,680.86,198.76,11.30;9,325.57,692.26,49.93,11.30;10,79.95,99.15,211.90,11.30;10,79.95,110.55,59.02,11.30" xml:id="b11">
	<monogr>
		<ptr target="http://trec.nist.gov/act_part/guidelines/qa_track_spec.html" />
		<title level="m" coord="9,342.13,680.86,198.76,11.30;9,325.57,692.26,45.39,11.30">TREC 2001 Question Answering Track Guidelines</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.91,121.83,204.45,11.30;10,79.95,133.23,215.36,11.30;10,79.95,144.62,215.34,11.30;10,79.95,156.02,215.21,11.30;10,79.95,167.30,168.46,11.30" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,79.95,133.23,198.52,11.30">Building a question answering test collection</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dawn</forename><forename type="middle">M</forename><surname>Tice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,79.95,144.62,215.34,11.30;10,79.95,156.02,215.21,11.30;10,79.95,167.30,168.46,11.30">Proceedings of the 23 rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 23 rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,91.22,178.70,204.06,11.30;10,79.95,190.10,215.32,11.30;10,79.95,201.38,215.34,11.30;10,79.95,212.78,141.75,11.30" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,79.95,190.10,215.32,11.30;10,79.95,201.38,44.33,11.30">The TREC-8 question answering track evaluation</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dawn</forename><forename type="middle">M</forename><surname>Tice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,154.79,201.38,140.50,11.30;10,79.95,212.78,93.62,11.30">Proceedings of the 8 th Text REtrieval Conference</title>
		<meeting>the 8 th Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>TREC-8</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
