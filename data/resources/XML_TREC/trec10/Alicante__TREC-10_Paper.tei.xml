<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,191.80,108.67,229.37,13.98">University of Alicante at TREC-10</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,202.86,131.14,42.98,9.67"><forename type="first">Jose</forename><surname>Luis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dpto. Lenguajes y Sistemas Informáticos Universidad de Alicante Apartado</orgName>
								<address>
									<postCode>99. 03080</postCode>
									<settlement>Alicante</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,260.50,131.14,102.33,9.67"><forename type="first">Antonio</forename><forename type="middle">&amp;</forename><surname>Ferrández</surname></persName>
							<email>antonio@dlsi.ua.es</email>
							<affiliation key="aff0">
								<orgName type="department">Dpto. Lenguajes y Sistemas Informáticos Universidad de Alicante Apartado</orgName>
								<address>
									<postCode>99. 03080</postCode>
									<settlement>Alicante</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,365.74,131.14,76.83,9.67"><forename type="first">Fernando</forename><surname>Llopis</surname></persName>
							<email>llopis@dlsi.ua.es</email>
							<affiliation key="aff0">
								<orgName type="department">Dpto. Lenguajes y Sistemas Informáticos Universidad de Alicante Apartado</orgName>
								<address>
									<postCode>99. 03080</postCode>
									<settlement>Alicante</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,191.80,108.67,229.37,13.98">University of Alicante at TREC-10</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">433EE9C73387770F849CA9A9045CA145</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the architecture, operation and results obtained with the Question Answering prototype developed in the Department of Language Processing and Information Systems at the University of Alicante. Our system is based on our TREC-9 approach where different improvements have been introduced. Essentially these modifications are twofold: the introduction of a passage retrieval module at first stage retrieval and the redefinition of our semantic approach for paragraph selection and answer extraction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Open domain QA systems are defined as tools capable of extracting the answer to user queries directly from unrestricted domain documents. Question answering systems performance is continuously increasing since recent Text REtrieval Conferences <ref type="bibr" coords="1,365.01,418.06,12.58,9.67" target="#b9">[9]</ref> [10] included a special task for evaluating and comparing this kind of systems. The analysis of current best systems <ref type="bibr" coords="1,453.59,430.42,12.58,9.67" target="#b1">[1]</ref> [3] [4] <ref type="bibr" coords="1,500.52,430.42,12.58,9.67" target="#b7">[7]</ref> allows identifying main QA sub-components:</p><formula xml:id="formula_0" coords="1,117.88,464.59,143.16,53.24">• Question analysis • Document / passage retrieval • Paragraph selection • Answer extraction</formula><p>The system presented to TREC-10 QA task is based on the described structure. It departs from the system presented in last TREC conference <ref type="bibr" coords="1,288.28,544.30,17.98,9.67" target="#b11">[11]</ref> where new tools have been added and existing ones have been updated. Modifications introduced rely on several aspects. First, document retrieval stage has been changed. Instead of using first fifty documents supplied by TREC organisation, we have implemented a passage retrieval module that allows a more successful retrieval. Second, our semantic-based paragraph selection approach has been redefined in order to increase selection process performance. Finally, question analysis and answer extraction modules have been updated by including special modules for managing with definition questions. This year, question answering task has been significantly modified. The organisation has designed three different tasks: main task, list task and context task. Main task is similar to previous years' tasks but only permitting a maximum of 50 bytes as answer length. Besides, there is no guarantee that an answer will actually occur in the document collection and participants have to measure the degree of correctness of its answers. The list task consists of answering questions that will specify a number of instances to be retrieved. In this case, it is guaranteed that the collection contains at least as many instances as the question asks for. Finally, the context task consist of answering a set of related questions in such a way that the interpretation of a question will depend on the meaning of and answers to one or more earlier questions in a series.</p><p>Our participation has been restricted to the main task although we did not face up all the restrictions. In fact, no effort was accomplished to measure which of the returned answers is more likely to be the correct one or to detect questions without correct answers in the document collection.</p><p>This paper is structured as follows: Section 2 describes the structure and operation of our system. Afterwards, we present and analyse the results obtained for TREC-10 task we participated in. Finally, initial conclusions are extracted and directions for future work are discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">System Overview</head><p>Our QA system is structured into the four main modules outlined before: question analysis, document/passage retrieval, paragraph selection and answer extraction. First module processes questions expressed in open-domain natural language in order to analyse the information requested in the queries. This information is used as input by remaining modules. Document retrieval module accomplishes a first selection of relevant passages by using a new passage retrieval approach. Afterwards, the paragraph selection module analyses these passages in order to select smaller text fragments that are more likely to contain the correct answer. Finally, the answer selection module processes these fragments in order to locate and extract the final answer. Figure <ref type="figure" coords="2,443.94,365.38,5.37,9.67" target="#fig_0">1</ref> shows system architecture. Several standard natural language processing techniques have been applied to both questions and documents. These tools compose the Slot Unification Parser for Anaphora Resolution (SUPAR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Questions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">SUPAR NLP tools</head><p>In this section, the NLP Slot Unification Parser for Anaphora Resolution (SUPAR) is briefly described <ref type="bibr" coords="3,127.84,191.98,12.58,9.67" target="#b2">[2]</ref>  <ref type="bibr" coords="3,143.92,191.98,16.45,9.67" target="#b12">[12]</ref>. SUPAR's architecture consists of three independent modules that interact with one other. These modules are lexical analysis, syntactic analysis, and a resolution module for Natural Language Processing problems.</p><p>Lexical analysis module. This module each document sentence or question to parse as input, along with a tool that provides the system with all the lexical information for each word of the sentence. This tool may be either a dictionary or a part-of-speech tagger. In addition, this module returns a list with all the necessary information for the remaining modules as output. SUPAR works sentence by sentence from the input text, but stores information from previous sentences, which it uses in other modules, (e.g. the list of antecedents of previous sentences for anaphora resolution).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Syntactic analysis module.</head><p>This module takes as input the output of lexical analysis module and the syntactic information represented by means of grammatical formalism Slot Unification Grammar (SUG). It returns what is called slot structure, which stores all necessary information for following modules. One of the main advantages of this system is that it allows carrying out either partial or full parsing of the text.</p><p>NLP problems resolution module. In this module, NLP problems (e.g. anaphora, extraposition, ellipsis or PP-attachment) are dealt with. It takes the slot structure (SS) that corresponds to the parsed sentence as input. The output is an SS in which all the anaphors have been solved. In this paper, only the resolution of third person pronouns has been applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Question Analysis</head><p>Question processing module accomplishes several tasks. First, SUPAR system accomplishes part-of-speech tagging and parsing of the question. Afterwards, this module determines question type, classifies non-Wh terms into two categories (keywords or definition terms) and finally, concepts referred into the question are detected and processed to obtain the semantic representation of the concepts appearing in the question.</p><p>Question type is detected by analysing Wh-terms (e.g. What, Which, How, etc). This process maps Wh-terms into one or several of the categories listed in figure <ref type="figure" coords="3,384.31,572.38,3.98,9.67" target="#fig_1">2</ref>. Each of these categories is related to WordNet top concepts <ref type="bibr" coords="3,233.45,584.62,11.38,9.67" target="#b6">[6]</ref>. This module has been updated by including the definition questions as new question type. When no category can be detected by Wh-term analysis, NONE is used (e.g. "What" questions). This analysis gives the system the following information: (1) lexical restrictions that expected answer should validate (e.g. proper noun), (2) how to detect definition terms (if they exist), and (3) top WordNet concepts and related synsets that are compatible with the expected answer. Definition questions are detected by applying a pattern matching process. As example, questions such as "Who was Galileo?", "What are amphibians?" or "What does USPS Once question type has been obtained, the system selects the definition terms. A term in a query is considered a definition term if it expresses semantic characteristics of the expected answer. Definition terms do not help the system to locate the correct answer into the document collection but they usually describe the kind of information requested by a query. Depending on question type, different patterns are used to detect definition terms. For "What", "Which", "How" and similar questions, this terms are detected by selecting noun phrases located next to the Wh-term. When questions such as "Find the number of whales…" or "Name a flying mammal ..." are analysed, noun phrases following the verb are considered definition terms.</p><p>Question type and definition terms are used to generate the expected answer semantic context (EASC). This context defines the lexical characteristics that the expected answer should validate to be considered a probable answer (e.g. proper noun) and the semantic context that the expected answer has to be compatible with. This context is made up by the set of synsets that are semantically related to definition terms and question type. These synsets are obtained by extracting from WordNet all hyperonyms of each definition term (its path to top concepts). These synsets are weighted depending on its level into the WordNet hierarchy and the frequency of its appearance into the path towards top concepts. Intuitively, this set of synsets defines the semantic context that has to be compatible with the expected answer semantic context. Finally, remaining question terms are classified as keywords.</p><p>Last question processing stage builds the semantic representation of the concepts expressed into the query (Semantic Content of a Question -QSC). This process consists of obtaining a general semantic representation of the concepts that appear in the questions and its main aim is to achieve concept representation in such a way that make possible to overcome term-based approach limits into the paragraph selection stage. To obtain this representation we have to deal with two basic requirements: a) Concepts appearing in questions need to be correctly detected and extracted. b) The different ways of expressing a concept have to be obtained and represented.</p><p>First requirement is accomplished by parsing questions. This process obtains all the syntactic structures that made up each question. Structures containing definition terms are discarded. Then, each syntactic structure (noun and verbal phrases) that contains one or more keywords defines a concept. The head of each syntactic structure represents the basic element or idea the concept refers to. Remaining terms pertaining to this structure modify this basic concept by refining the meaning represented by its head.</p><p>Accomplishing the second requirement involves obtaining and representing the different ways of expressing each of the concepts detected in a query. This process starts by associating each term pertaining to a concept, with its synonyms and one level search hyponyms and hyperonyms. These relations are extracted from WordNet lexical database. We define the semantic content of a term t (SCt) as a set of terms made up by the term t and all the terms related with it through the synonym and one level search hyponym and hyperonym relations. The SC of a term is represented using a weighted term vector. The weight assigned to each term pertaining to the SC of a term t is the 80%, 50% and 50% of the idf <ref type="bibr" coords="4,191.80,673.42,12.59,9.67" target="#b8">[8]</ref> value of term t for synonyms, hyponyms and hyperonyms respectively. As a concept is made up by the terms included into the same syntactic structure, we define the semantic content of a concept (SCC) as the set of weighted vectors (HSC, MSC) were HSC is the a vector obtained by adding the SC of the terms that made up the head of the concept and MSC is the vector resulting from adding the SC of terms that modify that head into the same syntactic structure.</p><p>The set of SCCs that stand for the concepts appearing in a question builds the semantic content of a question (QSC). This way, the QSC represent all the concepts referenced into the question and the different ways of expressing each of them. This process is widely explained in <ref type="bibr" coords="5,423.85,131.74,16.36,9.67" target="#b13">[13]</ref>.</p><p>Figure <ref type="figure" coords="5,132.53,156.22,5.37,9.67" target="#fig_2">3</ref> shows the semantic content of an example question First, the system identifies the concepts "manufactures" and "American Girl doll collection" by detecting syntactic structures that contain keywords. Afterwards, the semantic content of each concept is generated.</p><p>Question keywords are used for first stage passage retrieval while QSC information will help paragraph selection module to detect the paragraphs that are more likely to contain the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Passage retrieval module</head><p>First stage retrieval applies the passage retrieval approach described in <ref type="bibr" coords="5,434.37,522.34,11.37,9.67">[5]</ref>. This passage retrieval can be applied over all the document collection, but it has only been applied for the 1000 relevant documents supplied by TREC organisation. Therefore, keywords detected at question processing stage are used for retrieving the 200 most relevant passages from the documents included in this initial list. This process is intended to reduce the amount of text that has to be processed by costly NLP modules since these passages are made up by text snippets of 15 sentences length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Paragraph selection</head><p>This module processes 200 first ranked passages selected at passage retrieval stage in order to extract smaller text fragments that are more likely to contain the answer to the query. As all this process is widely described in <ref type="bibr" coords="5,215.15,668.86,17.99,9.67" target="#b13">[13]</ref> we extract here the basic algorithm:</p><p>What is the name of the company that manufactures the American Girl doll collection?  for all SCCs of the question as defined in previous step. c) The value that measures similarity between a SCC and a syntactic structure of the same type is obtained by adding the weights of terms appearing into SCC vectors and the syntactic structure that is being analysed. If the head of this syntactic structure does not appear into the vector representing the SCC head (HSC), this value will be 0 (even if there are matching terms into MSC vector).</p><formula xml:id="formula_1" coords="5,175.72,328.28,3.43,7.33">(</formula><p>At this stage, only best 100 ranked paragraphs are selected to continue with the remaining processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Answer extraction</head><p>This process consists on analysing selected paragraphs in order to extract and rank the text snippets of the desired length that are considered to contain the correct answer. For this purpose, the system selects a window for each probable answer by taking as centre the term considered a probable answer. Each window is assigned a score (window-score) that is computed as follows:</p><p>where EASC is the vector representing the semantic context of the expected answer and PASC stands for the vector representing the semantic context of the possible answer. PASC is computed as done for EASC but using the terms contained into the syntactic structures the probable answer appear into, as well as surrounding syntactic structures.</p><p>Intuitively, the window-score combines (1) the semantic compatibility between the probable answer and the expected answer (cos(EASC,PASC)) and ( <ref type="formula" coords="6,351.84,618.10,4.19,9.67">2</ref>) the degree of similarity between question and paragraphs (paragraph-score).</p><p>Finally, windows are ranked on window-score and the system returns the first five as answer.</p><p>Answer extraction manages differently with definition questions. This questions look for answers that define or explain the concept expressed in the question. From the analysis of definition questions in TREC-9 question set we derived a set of heuristics for detecting answers to definition questions. Each of these heuristics refers to a different way of expressing definition answers. The Window-score = paragraph-score*(1+cos(EASC,PASC))</p><p>following list shows the main ways in which answers to definition questions are more probably expressed and several examples (the answer is italicised):</p><p>• Noun phrases including the answer ("Italian archbishop Filippo Cune ...").</p><p>• Explanatory appositions (" Filippo Cune, the Italian archbishop, …").</p><p>• Explanatory conjunctions ("Italian archbishops Federico Pane, Filippo Cune and ...").</p><p>• Definition phrases (" Filippo Cune was the Italian archbishop …").</p><p>• Coreference resolution ("Filippo Cune travelled to Pisa. The Italian archbishop desired to renew the …"). • … These heuristics were ordered depending on the probability of obtaining a correct answer to a question (heuristic probability) by applying each of them on TREC-9 definition question set. This order determines the sequence of application of each heuristic over relevant paragraphs. The following algorithm shows how these heuristics are applied: a) Heuristics are applied over each relevant paragraph in an ordered way until one of them (or none) succeeds. b) Answers detected by successful heuristics are extracted. c) These answers are scored (answer-score) as follows: d) For duplicated answers, only the highest ranked is maintained, e) First five ranked answers are returned as final answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head><p>This year we submitted two runs for main task. This task allowed five answers for each question and a maximum answer string length of 50 bytes. Figure <ref type="figure" coords="7,383.12,461.98,5.37,9.67" target="#fig_3">4</ref> shows the results obtained. Applying the whole system described above has produced ALIC01M2 run. ALIC01M1 files contain results obtained applying the same strategy but without solving pronominal anaphora in relevant passages. These results were computed after the organisation decided to get rid of eight questions. Therefore, 492 questions were evaluated.</p><p>Although a detailed results analysis is a very complex task, several conclusions can be extracted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with TREC-9 results.</head><p>Our system has achieved a significant improvement since TREC-9 participation. Comparison between strict best results for 50 bytes answer length at TREC-9 (see figure <ref type="figure" coords="7,409.94,706.54,4.43,9.67">5</ref>) and TREC-10 (figure  Answer-score = paragraph-score * heuristic probability 4) shows that the mean reciprocal rank has increased 0.7 points (from 0.23 to 0.30) and besides, the percentage of correct answers found has increased 5.7 points (from 33.9% to 39.6%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Retrieving relevant documents.</head><p>Correct answer was not included into the top ranked documents supplied by TREC for 61 questions.</p><p>If we discard the 49 questions with no correct answer in the collection this number falls to 12 questions. Figure <ref type="figure" coords="8,160.37,290.86,5.37,9.67" target="#fig_4">6</ref> compares the percentage of questions that could be correctly answered between the two possible approaches: (1) processing a number of top documents and (2) selecting a number of passages.</p><p>As we can notice processing 200 passages produces best results than processing 200 complete documents and besides it dramatically reduces later NLP processing costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paragraph selection.</head><p>Our main objective was to inspect if our new paragraph selection method was more effective than last year proposal. As we expected, this model has achieved a better performance. Strict MRR increased 0.7 points from past results, which corroborates that precision achieved at this process has improved significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pronominal anaphora resolution</head><p>The small benefit obtained last year from applying pronominal anaphora resolution has been corroborated with TREC-10 results. This fact is mainly due to the same reasons described last year <ref type="bibr" coords="8,82.84,603.22,16.45,9.67" target="#b11">[11]</ref>. Nevertheless, although we have not participated into the context task thise kind of questions will surely take more profit form coreference resolution techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Future Work</head><p>Several areas of future work have appeared while analysing results. First, passage retrieval has to be tested over the whole collection to investigate the level of benefit it can produce over current results. Besides, although our paragraph selection module has revealed to be very efficient, several aspects can be improved, especially by incorporating a validation module that could measure the inexistence of the answer. Third, it seems essential to incorporate a Name-Entity tagger to our  answer extraction module since we missed several answers that could have easily been detected. And fourth, the system needs to be adapted to manage with list and context questions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,242.56,695.62,126.98,9.67"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. System architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,223.24,721.90,148.39,9.67"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Question type categories</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,239.68,413.62,115.50,9.67"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Example of QSC</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,209.08,645.82,159.26,9.67"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. TREC-10 main task results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,186.64,419.38,231.20,9.67"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Passage and document retrieval comparison</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,87.24,149.82,67.38,10.51" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,86.89,180.70,4.05,9.67;9,111.02,180.70,401.98,9.67;9,111.04,193.06,402.01,9.67;9,111.04,205.42,213.43,9.67" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,370.84,180.70,142.16,9.67;9,111.04,193.06,201.59,9.67">Question Answering by Passage Selection (MultiText Experiments for TREC-9)</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kisman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">R</forename><surname>Lynam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,333.28,193.06,179.77,9.67;9,111.04,205.42,47.66,9.67">Proceedings of the Nineth Text Retrieval Conference</title>
		<meeting>the Nineth Text Retrieval Conference<address><addrLine>Gaithersburg (US)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-11">November 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,86.89,217.66,4.05,9.67;9,111.01,217.66,402.02,9.67;9,111.04,230.02,402.08,9.67;9,111.04,242.26,402.12,9.67;9,111.04,254.62,18.77,9.67" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="9,316.24,217.66,196.79,9.67;9,111.04,230.02,402.08,9.67;9,111.04,242.26,47.69,9.67">An empirical approach to Spanish anaphora resolution. Machine TranslationSpecial Issue on Anaphora Resolution in Machine Translation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ferrández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Palomar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lidia</forename><surname>Moreno</surname></persName>
		</author>
		<imprint>
			<publisher>Kluwer Academic publishers</publisher>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="191" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,86.89,266.98,4.05,9.67;9,111.02,266.98,402.11,9.67;9,111.04,279.22,402.16,9.67;9,111.04,291.58,326.68,9.67" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,254.08,279.22,175.97,9.67">Boosting Knowledge for Answer Engines</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gîrju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Morarescu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Falcon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,448.93,279.22,64.27,9.67;9,111.04,291.58,160.93,9.67">Proceedings of the Nineth Text Retrieval Conference</title>
		<meeting>the Nineth Text Retrieval Conference<address><addrLine>Gaithersburg (US)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-11">November 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,86.89,303.82,4.05,9.67;9,111.02,303.82,402.01,9.67;9,111.04,316.18,402.05,9.67;9,111.04,328.54,82.85,9.67" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,392.68,303.82,120.35,9.67;9,111.04,316.18,76.98,9.67">IBM&apos;s Statistical Question Answering System</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adwait</forename><surname>Ratnaparkhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,206.88,316.18,226.70,9.67">Proceedings of the Nineth Text Retrieval Conference</title>
		<meeting>the Nineth Text Retrieval Conference<address><addrLine>Gaithersburg (US)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-11">November 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,86.89,340.78,4.05,9.67;9,111.01,340.78,402.17,9.67;9,111.04,353.46,401.99,10.51;9,111.04,366.90,338.58,10.51" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,225.28,340.78,203.89,9.67">IR-n: a passage retrieval system at CLEF-2001</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Llopis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,449.29,340.78,63.89,9.67;9,111.04,353.46,235.62,10.51">proceedings of the second Cross-Language Evaluation Forum</title>
		<title level="s" coord="9,425.03,353.46,88.00,10.51;9,111.04,366.90,83.71,10.51">Lecture Notes in Computer Science</title>
		<meeting>the second Cross-Language Evaluation Forum<address><addrLine>Darmstadt (Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-09">2001. September 2001</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct coords="9,86.89,380.02,4.05,9.67;9,111.01,380.02,402.10,9.67;9,111.04,392.26,72.80,9.67" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,191.33,380.02,180.86,9.67">Wordnet: A Lexical Database for English</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,384.36,380.02,128.75,9.67">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,86.89,404.50,4.05,9.67;9,111.00,404.50,402.01,9.67;9,111.04,416.98,402.04,9.67;9,111.04,429.22,109.84,9.67" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,366.40,404.50,146.62,9.67;9,111.04,416.98,86.79,9.67">One Search Engine or Two for Question-Answering</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krzysztof</forename><surname>Czuba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,220.56,416.98,237.95,9.67">Proceedings of the Nineth Text Retrieval Conference</title>
		<meeting>the Nineth Text Retrieval Conference<address><addrLine>Gaithersburg (US)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-11">November 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,86.89,441.58,4.05,9.67;9,111.01,441.58,402.07,9.67;9,111.04,453.82,289.84,9.67" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="9,186.40,441.58,326.68,9.67;9,111.04,453.82,108.23,9.67">Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Addison Wesley Publishing</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,86.88,466.06,4.05,9.67;9,111.00,466.06,351.44,9.67" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="9,180.32,466.06,208.45,9.67">Call for participation Text Retrieval Conference</title>
		<author>
			<persName coords=""><surname>Trec-8</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999. 1999</date>
			<publisher>TREC-8)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,91.83,478.42,370.61,9.67" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><surname>Trec-9</surname></persName>
		</author>
		<title level="m" coord="9,180.33,478.42,271.76,9.67">Call for participation Text Retrieval Conference 2000 (TREC-9</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,91.84,490.78,421.18,9.67;9,111.04,503.14,393.84,9.67" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,253.24,490.78,241.45,9.67">A semantic approach to Question Answering systems</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ferrandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,111.04,503.14,227.98,9.67">Proceedings of the Nineth Text Retrieval Conference</title>
		<meeting>the Nineth Text Retrieval Conference<address><addrLine>Gaithersburg (US)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-11">November 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,91.84,515.38,421.20,9.67;9,111.04,527.62,402.06,9.67;9,111.04,539.98,327.11,9.67" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,246.52,515.38,266.51,9.67;9,111.04,527.62,83.05,9.67">Importance of Pronominal Anaphora resolution in Question Answering systems</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ferrández</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,218.16,527.62,294.94,9.67;9,111.04,539.98,165.25,9.67">proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL2000)</title>
		<meeting>the 38th Annual Meeting of the Association for Computational Linguistics (ACL2000)<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>Hong Kong</publisher>
			<date type="published" when="2000-10">October 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,91.84,552.34,421.17,9.67;9,111.04,564.70,402.01,9.67;9,111.04,576.94,324.03,9.67" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,167.56,552.34,327.73,9.67">Using semantics for Paragraph selection in Question Answering systems</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,111.04,564.70,402.01,9.67;9,111.04,576.94,49.67,9.67">proceedings of the Proceedings of the Eighth String Processing and Information Retrieval Conference</title>
		<meeting>the Proceedings of the Eighth String Processing and Information Retrieval Conference<address><addrLine>Laguna de San Rafael (Chile)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-11">2001. November 2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
