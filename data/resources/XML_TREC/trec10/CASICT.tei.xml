<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,106.58,98.86,382.87,13.08">TREC-10 Experiments at CAS-ICT: Filtering, Web and QA</title>
				<funder ref="#_dprZ4Pq">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,118.82,139.01,39.23,9.15"><forename type="first">Bin</forename><surname>Wang</surname></persName>
							<email>wangbin@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,166.09,139.01,46.86,9.15"><forename type="first">Hongbo</forename><surname>Xu</surname></persName>
							<email>hbxu@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,220.77,139.01,55.14,9.15"><forename type="first">Zhifeng</forename><surname>Yang</surname></persName>
							<email>zfyang@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,283.08,139.01,31.98,9.15"><forename type="first">Yue</forename><surname>Liu</surname></persName>
							<email>yliu@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,322.04,139.01,53.50,9.15"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,383.23,139.01,46.51,9.15"><forename type="first">Dongbo</forename><surname>Bu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,437.37,139.01,38.94,9.15"><forename type="first">Shuo</forename><surname>Bai</surname></persName>
							<email>bai@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,106.58,98.86,382.87,13.08">TREC-10 Experiments at CAS-ICT: Filtering, Web and QA</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">96A35C257164996D3C1D0E13C7D59299</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>TREC-10</term>
					<term>Filtering</term>
					<term>Web track</term>
					<term>QA</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>CAS-ICT took part in the TREC conference for the first time this year. We have participated in three tracks of TREC-10. For adaptive filtering track, we paid more attention to feature selection and profile adaptation. For web track, we tried to integrate different ranking methods to improve system performance. For QA track, we focused on question type identification, named entity tagging and answer matching. This paper describes our methods in detail.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>CAS-ICT took part in the TREC conference for the first time this year. Among the total six tracks of TREC-10, we choose three of them: Filtering, Web and QA.</p><p>For filtering track, we undertook the adaptive filtering subtask. Our model is still based on vector representation and computation. A topic-term relevance function is defined to guide feature selection. For profile adaptation, we use a Rocchio-like algorithm. Four runs have been submitted for evaluation: three of them are optimized for T10U measure, another one for T10F measure. We use very simple optimization methods in our experiments and we do not use any other resource except the new Reuters Corpus.</p><p>For web track, we undertook the ad-hoc subtask. Our system is based on a general-purpose search engine developed by us alone. We try to improve system performance by integrating different ranking methods. Query expansion technology is used to modify the initial query. The PageRank algorithm is investigated in our experiments. Four runs have been submitted and two of them use hyperlink information.</p><p>For QA track, we undertook the main subtask. We first use SMART search engine to retrieve a set of documents from the TREC data sets. At the same time, a question analyzer is used to analyze the given 500 questions of TREC-10 and generates the question types and keyword lists.</p><p>Then we use GATE to analyze the top 50 retrieved documents and extract the named entities from them. Finally, an answer extractor extracts the relevant answers from the named entities. Three QA runs have been submitted for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Filtering</head><p>In the filtering task, we undertook the adaptive filtering subtask, which we think, is more interesting and realistic than the other two subtasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Description</head><p>This year the filtering task has 84 topics, which are exactly the categories in Reuters Corpus.</p><p>The total documents for this task (new Reuters Corpus) are divided into two parts: 23,307 documents for training (training set) and the remaining about 783,484 documents for testing (testing set). All the documents are Reuters everyday news, dating from August 1996 to August 1997. Two measures are given for adaptive filtering: T10U and T10F, the former is a linear utility measure and the latter is a kind of F-measure. In the adaptive task, only two positive samples in training set are given for each topic, the goal is to retrieve relevant documents one by one from the coming testing documents stream and get maximum T10U or T10F value at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">System Description</head><p>Our adaptive filtering system consists of two components: the initializing component and the adaptation component. The former is used to get the initial data through training and the latter is to adapt these data when retrieving testing documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training</head><p>In training procedure, we first process the training set for basic term statistics, this includes term tokenization, stemming and frequency counting. Then we can select terms from the positive and some pseudo-negative samples. After topic processing, we can get the initial profile vector by summing up the topic and feature vectors with different weight. Finally, we can compute the similarity between the initial profile vector and the positive documents to set the initial threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Training set processing</head><p>In this step, all the training documents are processed. First we tokenize each document into single words, then eliminate the stop words and some other words with low frequency in the training set, and then we stem each word using the Porter Stemmer(http://www.cs.jhu.edu /~weiss/). Finally, we count each word's frequency(TF) within each document and the word's document frequency(DF) across the training set. When processing the training texts, we only use the &lt;title&gt; and &lt;text&gt; fields. Thus each document can be represented with its term frequency vector. Meanwhile, we can get the IDF statistics of the training documents. Since we can't use the IDF statistics of testing set, we use in the following steps the IDF statistics of the training document in term weighting. Ideally, we can update the IDF statistics when retrieving documents from the testing documents stream. But [16] has indicated that doing so does not seem to improve the overall filtering performance. So we use the same IDF statistics of the training documents all over our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Topic set processing</head><p>This year, each topic consists of three short parts: the &lt;num&gt; field, the &lt;Reuters-code&gt; field and the &lt;title&gt; filed. The &lt;title&gt; field includes only one or two words. We can't get more information from such kind of topics than from the topics of TREC-8 or TREC-9, which are described with more words. Of the three parts, we regard the &lt;title&gt; field as the most important. Though the &lt;Reuters-code&gt; fields may provide some information about relationships between different topics, we do not use them at all. Processing the topics is very simple, we only extract and stem the &lt;title&gt; field words to construct the vector for each topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Term selection</head><p>To reduce the computation complexity, we apply a method for feature selection. Here the features are all terms, each term is a word.</p><p>For each topic, we have two positive samples. So for all 84 topics, we have 168 positive samples. Thus, of the 168 samples, each topic has two positive samples, and we can suppose the other 166 samples are negative to the topic. Because one document may be relevant to more topics, our supposition is not very correct. But we try it for lack of information.</p><p>We define a word-topic correlation function as:</p><formula xml:id="formula_0" coords="3,133.34,108.32,352.94,23.09">) ) | ( ) | ( log( ) , ( j i j i j i T w P T w P T w Cor ¬ = (2.1)</formula><p>Where P(w i |T j ) means the probability that word w i exists in the relevant documents of topic T j .</p><p>On the contrary, ) | (</p><formula xml:id="formula_1" coords="3,174.55,156.44,46.76,15.65">j i T w P ¬</formula><p>means the probability that word w j exists in non-relevant documents of topic T j . For each topic, we compute the Cor value of each word in the positive two samples and choose the words with high Cor values as the features. Here we use maximum likelihood estimation. We compute the frequency that a word exists in the two positive documents as the estimation of P(w i |T j ), and the frequency that a word exists in the other "negative" documents as the estimation of ) | (</p><formula xml:id="formula_2" coords="3,231.36,250.02,273.83,15.65">j i T w P ¬ . If the estimation of ) | ( j i T w P ¬ is equal to zero,</formula><p>we give the Cor a big value. After getting the feature words for each topic, we combine them to construct one feature space; the topic vector and the feature vector must be mapped into this space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4">Profile initialization</head><p>For each topic, the profile vector(denoted as P v</p><p>) is the weighted sum of the topic vector(denoted as T v ) and the feature vector(denoted as F v</p><p>), which is the sum of the two positive documents vectors. The formula is:</p><formula xml:id="formula_3" coords="3,134.12,384.38,352.17,22.89">T F P v v v * * β α + = (2.2)</formula><p>In our experiments, we set α=1,β=2 to give prominence to the topic words. So far, each component of the vectors is represented with TF values. Then we change it by multiplying with its IDF coefficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.5">Similarity computation</head><p>To compute the similarity between a topic profile( i P v</p><p>) and a document( j D v</p><p>), we use the vector cosine similarity formula: formula. We also try other formulae in our experiment, but the results are almost the same.</p><formula xml:id="formula_4" coords="3,133.94,523.97,344.28,39.55">| | | | ) , cos( ) , ( j i j i j i j i D P D P D P D P sim v v v v v v v v × = = • (2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.6">Initial threshold setting</head><p>We do not have good idea to set the initial threshold. According to our understanding, we believe we can't use the training documents to train the initial threshold because they are prepared for batch filtering task, not for adaptive task, we cannot use the relevance information of the other documents in training set except the two positive ones. Thus we have to use a very simple method, for each topic, we choose a small fixed value as the initial threshold which is smaller than the similarity between the initial profile vector and the two positive samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Adaptation</head><p>For each topic, after initializing the profile and the threshold, we can scan documents one by one from the testing set. If the similarity between the profile and the document is higher than the threshold, the document is retrieved and meanwhile the system can tell you the document is really relevant or not. With this information, some kind of adaptation may need to take to improve system performance. The adaptation may include threshold updating or profile updating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Threshold adaptation</head><p>In TREC-10, two measures are defined to measure the performance of an adaptive filtering system. One is T10U, which is a linear utility; another is T10F, which is a kind of F-value. The goal of the adaptive filtering system is to get maximum T10U or T10F.</p><p>For T10U, we have two goals: one is to avoid negative T10U as far as we can, another is to improve the precision while the recall can't be greatly reduced. We only apply method for the former goal in our experiments.</p><p>For T10F, we also have two goals: one is to avoid retrieving zero documents, another is also to improve the precision while the recall can't be greatly reduced. We only apply method for the former goal, too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Profile adaptation</head><p>After retrieving more and more relevant or non-relevant documents, we can get more useful information and understand the user's interest better. Thus we can adapt each profile vector, which represents each user's interest. Our profile adaptation includes positive adaptation and negative adaptation. For positive adaptation, we add the document vector of the positive documents to the old profile vector. For negative adaptation, we subtract the document vector of the negative documents from the old profile vector. When retrieving the n+1 th document D n+1 , we can adapt the nth profile to the n+1 th profile according the following formula:</p><formula xml:id="formula_5" coords="4,134.12,445.39,352.16,42.55">   - + = + + + + otherwise * relevant is if * 1 1 1 1 n n n n n n D P D D P P v v v v v β α (2.4)</formula><p>Thus after retrieving n+1 documents, all the retrieving relevant documents make the positive set denoted as {D + }, the other documents set is {D -}. Then the new profile vector become</p><formula xml:id="formula_6" coords="4,134.12,524.70,352.12,54.87">φ β α = ∩ = ∪ - + = + + + ∈ ∈ + ∑ ∑ - + } { } { }, ,..., , { } { } { here * * _ 1 2 1 _ } { } { 0 1 D D D D D D D D D P P n D D j D D i n i i v v v v (2.5)</formula><p>Formula (2.5) is some kind of the Rocchio <ref type="bibr" coords="4,290.75,588.64,11.72,6.12" target="#b18">[19]</ref> algorithm except one point: we do not compute the centroid vector of the positive set or negative set and regard it as one vector. In other words, we pay more attention to the retrieving documents than the initial profile vector. Furthermore, we investigate the values of α and β. We found without negative feedback, the result is worse. In our experiments, we set α=1, β=1.1 or 1.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Evaluation Results and Analysis</head><p>We have submitted four adaptive filtering runs: one for T10F optimization, three for T10U optimization. Because we do not use complex optimization method, the results of the 4 runs are similar to each other. The evaluation results are shown in  We focused on the worst zero results and the best results. We found, on one hand, most of the zero results belong to "small" topics, which have small amount of relevant documents in the whole testing set and we called them "hard" topics. On the other hand, most of the best results belong to "big" topics. That is to say, our method is somehow fitful for "big" topics but not very fitful for "small" topics. This may result from two reasons: First, our feature selection method cannot find the best features of these "hard" topics from only two positive samples. Second, our optimization method is too simple to satisfy different cases.</p><p>In the future work, we will pay more attention to three aspects. For feature selection, we will try more effective methods. For optimization, we will try more complex methods. For profile adaptation, we may add feature reselection module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Web Track</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">System Description</head><p>This year, the goal of the main web track is to retrieve the most relevant documents for each topic in the topic set (501~550) from the WT10G collection. Our system is based on a general-purpose search engine which we have been developing and improving since 1998. The system consists of four basic components: indexer, query generator, search agent, and search server. The indexer scans all documents of the WT10G and generates full text indexes and term statistics. The query generator analyzes the TREC topics and generates real queries. The search agent submits real queries to search server and shows the return results in visible format. The search server receives queries, searches documents and returns results to the search agent</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Searching Process 3.2.1 Query construction</head><p>Query generator reads TREC topics, extracts valuable terms and submits them to the search system. Therefore, the initial query for each topic is a set of some useful terms of the topic.</p><p>At this stage all stop words are removed. In addition to the basic stop words, we have also removed some other stop words that carry little information in the topics, such as find. While processing each topic, we only use the &lt;title&gt; field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Initial retrieval</head><p>For each initial query, the search system retrieves some documents and ranks them using formula (3.1) <ref type="bibr" coords="5,145.94,748.09,8.22,6.12" target="#b6">[7]</ref> . Some top ranked documents are returned as the initial search results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>∑ ∑ ∑</head><formula xml:id="formula_7" coords="6,133.76,82.03,352.52,59.37">+ * + + * + = ∩ ∈ ) 1 ( log ) log 1 ( ) 1 log( ) log 1 ( 2 2 , , , t d t d q t t d t d q f N f f N f S (3.1)</formula><p>Where S q,d is the similarity of document d and query q, f t is the number of documents in which term t occurs in WT10G, f t,d is the frequency term t occurs in document d (within-document frequency), and N is the total number of documents in WT10G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Query expansion</head><p>After first retrieval we can get many ranked documents. Then we can regard some top ranked documents as relevant. All the terms in these documents are weighted according to formula (3.2) <ref type="bibr" coords="6,110.12,245.49,16.38,6.12">[1][8]</ref> . </p><formula xml:id="formula_8" coords="6,132.98,265.56,353.26,62.65">+ + + - + - + = = s S s S k S n N n S k k r R r R k R n N n n N N k R k k r t W r TSV (3.2)</formula><p>Where TSV means Term Selection Value that is used to rank terms. N is the number of documents in WT10G, n is the number of documents in which term t occurs, R is the number of relevant documents, r is the number of relevant documents which contain term t, S is the number of non-relevant documents in WT10G, s is the number of non-relevant documents which contain term t in WT10G. k 4 <ref type="bibr" coords="6,174.67,401.47,2.34,6.12">'</ref> ,k 5 , k 6 are parameters.</p><p>Some top terms with their TSVs are selected for the new query vector construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Second retrieval</head><p>In this stage, the new query is submitted to the search system and new results are retrieved which are ranked by formula (3.3).</p><formula xml:id="formula_9" coords="6,133.94,483.09,352.30,50.29">∑ ∈ + + + + - + = q t t q t q t d d d t d t d t d q f k f k f L avr L b b k b f k w w , 3 , 3 , 1 , , 1 , * ) 1 ( * ] _ * ) 1 [( * * * ) 1 ( (3.3)</formula><p>Where L d is the length of document d, avr_L d is the average document length, b d,t is within-document importance of term t in document d, which includes multiple factors such as the frequency t occurs in document title, bolded text, and hyperlink text. k 1 ,k 2 , k 3 are parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Applying Link Analysis Technology in Web track</head><p>We also investigate link analysis technology that is used to rank web pages using link information. In our experiments, we mainly use an improved PageRank algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Basic PageRank algorithm</head><p>Brin &amp; Page <ref type="bibr" coords="6,164.65,666.62,8.10,6.12" target="#b4">[5]</ref> suggested a link based search model called PageRank that first evaluated the importance of each web page based on its citation pattern. The PageRank algorithm re-ranks the retrieved pages of a traditional search scheme according to their PageRank values.</p><p>In this approach, a web page will have a higher score if more web pages link to it and this value will increase if those web pages' scores increase. The PageRank value of a given web page t, denoted as Pr(t),can be iteratively computed according to formula (3.4) <ref type="bibr" coords="6,389.50,744.61,8.22,6.12" target="#b4">[5]</ref> .</p><formula xml:id="formula_10" coords="7,133.94,81.02,352.34,30.64">∑ = + - = m i i i t c t d d t 1 ) ( ) Pr( * ) 1 ( ) Pr( (3.4)</formula><p>Where t 1 , t 2 , …, t m are the web pages which link to page t, d is a parameter (set to 0.85 as suggested by [5]) and c(t i ) is the number of outgoing links for page t i . . For simplification, all the pages which link to page t are called the pre-set of t, denoted as pre-set(t),and all the pages which page t links to are called the post-set of t, denoted as post-set(t).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Implementation of the PageRank algorithm</head><p>From formula (3.4) we can see that computing PageRank is very simple in itself. But because the numbers of web pages is usually very large (in WT10G the number is 1,692,097) and the number of hyperlinks is even larger, the iteration process may be time-consuming. In order to solve this problem, we do some useful pretreatment.</p><p>First, we try to get rid of all the noisy hyperlinks that have little information before iteration. Meanwhile, we mark the web pages that have empty pre-set and do not participate in the iteration.</p><p>Second, we pretreat the post-sets of all the web pages. From formula (3.4) we can see that, for web page t and one page in its pre-set, t j , if c(t j ) is sufficiently large, the value of Pr(t j )/ c(t j ) will be very small, intuitively speaking, that means the influence of webpage t j to webpage t is very small in the web graph. Thus for web pages like t j , we don't let them only simply take part in the iteration. A threshold is set in advance, if one page's post-set has bigger size than the threshold, we assign a fixed value as the PageRank for this page. In this way we can greatly reduce the iteration cycle in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Convergence properties</head><p>According to the probabilistic meanings of formula (3.4), after each time of iteration, the sum of all pages' PageRank should be equal to 1 after standardization. To avoid that the PageRank values are too small and incomparable, we introduce a new standardization method.</p><p>In addition, we use formula (3.5) to determine whether we should stop or not after the i+1 th iteration.</p><formula xml:id="formula_11" coords="7,134.06,497.42,352.18,21.38">δ &lt; - - - + + ) ) Pr( ) (Pr( ) ) Pr( ) (Pr( ) 1 ( ) ( ) 1 ( ) ( i i t i i t t t Min t t Max (3.5)</formula><p>In order to determine whether the iteration should be stopped or not, we consider not only the decreasing tendency of all the web pages, but also the low changeability between iterations. In our experiments, after 25 times' iteration we get a relatively steady convergence results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Integrating ranking methods</head><p>We integrate the above ranking methods into formula (3.6) to get the integrated rank of one page:</p><formula xml:id="formula_12" coords="7,132.68,628.05,353.60,44.33">d dc dc dl dc d PR k PR W W W W W * log max + = + = (3.6)</formula><p>Where W d is the final weight of a page (document), W dc is the content-based document weight computed by formula (3.3), W dl is the link-based document weight. PR d is the PageRank value of document d computed by formula (3.4), PR max is the maximum PageRank value among all the documents, k is a constant greater than 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluation Results</head><p>We have submitted four runs to NIST. The results are listed in Table <ref type="table" coords="8,400.12,92.22,3.95,9.15">3</ref> Table <ref type="table" coords="8,138.56,282.38,4.37,9.15">3</ref>.1 shows that our query expansion method leads to performance degradation. The reason may be that we first use such a complex query expansion method in our system and some of the parameters need to be revised in future tests.</p><p>From the above table, we have also found that the results integrated with link analysis have little difference with the benchmark. We believe the reason is not algorithm itself but the small size of the web pages set. In our experiments, we use WT10G which is a close set that doesn't link to outside, thus many informative hyperlink are not included. If the experimental data size is large enough, the results should be very good. The PageRank method offers an approach that evaluates the web pages objectively.</p><p>Our future work includes three aspects: First, we will try different probabilistic retrieval models; second, we will try alternative feedback methods; third, we will try new connectivity computation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Questing Answering track 4.1 System Description</head><p>Our TREC-10 question answering system consists of four basic components: IR search engine, question analyzer, named entity tagger and answer extractor.  We first use an IR search engine to retrieve a set of documents from the TREC data sets. At the same time, a question analyzer analyzes the given 500 questions of TREC-10 and generates the question type and keyword-list for each question. Then we use a named entity tagger to analyze the top 50 documents retrieved by the search engine and extract the named entities from them. Finally, an answer extractor determines the relevant answers from the named entities using the question type and keyword-list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SMART Search Engine</head><p>As we know, SMART(ftp://ftp.cs.cornell.edu/pub/smart) is an implementation of the vector-space model of information retrieval proposed by Salton dating back in the 60's. The primary purpose of SMART is to provide a framework in which one conducts information retrieval research. It is (as we heard) distributed for research purpose only. Since TREC-QA allows us to use search engine freely, we choose SMART as our IR search engine because it is easy to use. To meet the need of our QA system, we add some new components into SMART. We also use the feedback function of SMART to generate a set of retrieved documents, based on which we make a run ICTQA10c. But this run becomes the worst of all our three runs, which proves we failed in applying the feedback. This indicates that we need to do further research on feedback, such as using the LCA (Local Context Analysis) feedback technique <ref type="bibr" coords="9,420.87,307.88,11.77,6.13" target="#b13">[14]</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Question Analyzer</head><p>Main answer and question types what we can extract are listed in table 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer type Question type Example</head><p>Step 8:Re-rank the candidate passages by their final scores and output the top 10 or 20 passages to the Named Entity tagger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Answer Extractor</head><p>The answer extractor compares the question type with each named entity in candidate passages. If a candidate passage contains a named entity matching the question type, we add 100 to the score of the passage. When there is more than one matched named entity, we only count once.</p><p>After the process above, we re-rank the candidate passages according to the named entity and question types, then output the top 5 named entities as the final answers. If the question type is unknown, we intercept a snippet with the largest density of keywords in the candidate passage to answer the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Results and Analysis</head><p>There are three subtasks in TREC-10 QA: main, list and context. We only participate in the main task and submit three runs in the 50-byte category. ICTQA10a uses the top 20 candidate passages for each question. Both ICTQA10b and ICTQA10c use only top 10 candidate passages for each question, the difference is that ICTQA10c uses the feedback of SMART in the IR phase.  <ref type="table" coords="11,138.44,675.68,4.37,9.15" target="#tab_3">4</ref>.4 shows some statistical results by question types. Our system is pleasant for the NATIONALITY, DURATION, CURRENCY, LOCATION and No Answer question types, but disappointing on the DATE, PERSON, MONEY and REASON question types, though these question types are easy to determine. The main reason is that some bugs exist in our ranking strategy when there are too many candidate named entities matching these question types. We try to find more detailed ranking strategies to solve this problem in the future work. We also try to introduce some syntactic and semantic parsing technology to solve other problems, especially the NAME(What is NP?) question type, which we badly handle in TREC-10. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question type # of question</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This year we participate in the TREC conference for the first time, the main goal is to understand the process and the ideas of the TREC conference. We spend much more time on this goal than we do in system construction. We think we have achieved this goal though our results are not so satisfactory.</p><p>Before attending TREC-10, we have had some experiences in Chinese information processing. After attending TREC-10, we have got some experiences in English information processing and we will try to apply these useful experiences in our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,478.22,543.82,8.07,9.15;3,110.96,575.74,394.24,9.15;3,173.11,597.90,4.00,10.47;3,135.20,597.90,6.00,10.47;3,116.90,597.90,19.34,10.47;3,109.28,597.90,6.00,10.47;3,169.57,610.48,1.95,6.11;3,105.14,601.18,1.95,6.11;3,153.56,607.20,16.00,10.47;3,158.05,590.40,8.01,10.47;3,91.23,597.90,14.01,10.47;3,142.82,593.65,6.59,15.64"><head>3 )</head><label>3</label><figDesc>Each component of the vectors is represented with TFIDF value. Here we use</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,397.37,531.94,107.95,9.15;8,89.97,547.54,80.76,9.15"><head></head><label></label><figDesc>Figure 4.1 illustrates the whole architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,187.51,703.51,241.32,9.15;8,156.55,562.89,303.07,131.62"><head>Figure 4 . 1 :</head><label>41</label><figDesc>Figure 4.1: Architecture of the ICT TREC-10 QA System</figDesc><graphic coords="8,156.55,562.89,303.07,131.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,110.96,731.71,394.37,24.75"><head>Table 2 .</head><label>2</label><figDesc>1. Table2.1 shows that in each run, the results of about 2/3 of all topics are better than the medians, and most of the remaining results are worse. Unfortunately, about 1/3 of the worse results are worst results and most of the worst results are zero. Thus the overall performance of our runs is not high.</figDesc><table coords="5,90.27,139.87,414.34,88.35"><row><cell>Run ID</cell><cell>MeanT10SU</cell><cell cols="3">T10SU vs. median(topic nums)</cell><cell>MeanT10F</cell><cell cols="3">T10F vs. median(topic nums)</cell></row><row><cell></cell><cell></cell><cell cols="2">&gt;(Best) =</cell><cell>&lt;(Worst/Zero)</cell><cell></cell><cell cols="3">&gt;(Best) = &lt;(Worst/Zero)</cell></row><row><cell>ICTAdaFT10Ua</cell><cell>0.204</cell><cell>55(8)</cell><cell>1</cell><cell>28(11/11)</cell><cell>0.368</cell><cell>59(3)</cell><cell>2</cell><cell>23(8/7)</cell></row><row><cell>ICTAdaFT10Ub</cell><cell>0.205</cell><cell>51(0)</cell><cell>3</cell><cell>30(12/12)</cell><cell>0.366</cell><cell>55(0)</cell><cell>4</cell><cell>25(7/6)</cell></row><row><cell>ICTAdaFT10Uc</cell><cell>0.207</cell><cell>52(3)</cell><cell>2</cell><cell>30(11/11)</cell><cell>0.354</cell><cell>51(1)</cell><cell>4</cell><cell>29(12/9)</cell></row><row><cell>ICTAdaFT10Fa</cell><cell>0.206</cell><cell>55(4)</cell><cell>1</cell><cell>28(14/14)</cell><cell>0.387</cell><cell>62(1)</cell><cell>1</cell><cell>21(6/5)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,193.33,236.07,208.53,9.15"><head>Table 2 .</head><label>2</label><figDesc>1 ICT adaptive filtering runs in TREC-10</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,110.30,92.22,394.80,168.13"><head></head><label></label><figDesc>.1.</figDesc><table coords="8,110.30,123.89,394.80,136.45"><row><cell></cell><cell>Average Precision</cell><cell></cell><cell></cell></row><row><cell>Technology(Run id)</cell><cell>(non-interpolated) over all</cell><cell cols="2">P@20 docs R-Precision</cell></row><row><cell></cell><cell>relevant docs</cell><cell></cell><cell></cell></row><row><cell>Baseline(ICTWeb10n)</cell><cell>0.0860</cell><cell>0.1450</cell><cell>0.1244</cell></row><row><cell>Query Expansion(ICTWeb10f)</cell><cell>0.0464</cell><cell>0.0620</cell><cell>0.0657</cell></row><row><cell>Link Analysis(ICTWeb10nl)</cell><cell>0.0860</cell><cell>0.1410</cell><cell>0.1147</cell></row><row><cell>Query Expansion &amp;Link Analysis(ICTWeb10nfl)</cell><cell>0.0464</cell><cell>0.0620</cell><cell>0.0657</cell></row><row><cell cols="2">Table 3.1 Web track results</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,89.96,357.37,415.43,327.46"><head>Table 4 .</head><label>4</label><figDesc>The evaluation results are presented in table 4.2 and 4.3. Table 4.2 shows the results in strict evaluation while table 4.3 does in lenient evaluation. (MRR means "Mean Reciprocal Rank".) ICTQA10b is better than ICTQA10a, which shows that more candidate passages can't guarantee to generate better results. 3 Lenient performance in TREC-10 Table</figDesc><table coords="11,96.02,435.84,401.93,197.94"><row><cell>Task</cell><cell>Run</cell><cell>Correct #</cell><cell>Correct</cell><cell>MRR(strict)</cell><cell>Correct # of</cell><cell>Correct % of</cell></row><row><cell></cell><cell></cell><cell>(strict)</cell><cell>% (strict)</cell><cell></cell><cell>final answer</cell><cell>final answer</cell></row><row><cell cols="2">main ICTQA10a</cell><cell>63</cell><cell>12.8%</cell><cell>0.090</cell><cell>26</cell><cell>8%</cell></row><row><cell cols="2">main ICTQA10b</cell><cell>67</cell><cell>13.6%</cell><cell>0.100</cell><cell>34</cell><cell>10%</cell></row><row><cell cols="2">main ICTQA10c</cell><cell>56</cell><cell>11.4%</cell><cell>0.077</cell><cell>20</cell><cell>6%</cell></row><row><cell></cell><cell></cell><cell cols="4">Table 4.2 Strict performance in TREC-10</cell><cell></cell></row><row><cell>Task</cell><cell>Run</cell><cell>Correct #</cell><cell>Correct %</cell><cell>MRR(lenient)</cell><cell>Correct # of</cell><cell>Correct % of</cell></row><row><cell></cell><cell></cell><cell>(lenient)</cell><cell>(lenient)</cell><cell></cell><cell>final answer</cell><cell>final answer</cell></row><row><cell cols="2">main ICTQA10a</cell><cell>74</cell><cell>15.0%</cell><cell>0.102</cell><cell>26</cell><cell>8%</cell></row><row><cell cols="2">main ICTQA10b</cell><cell>76</cell><cell>15.4%</cell><cell>0.109</cell><cell>34</cell><cell>10%</cell></row><row><cell cols="2">main ICTQA10c</cell><cell>66</cell><cell>13.4%</cell><cell>0.089</cell><cell>20</cell><cell>6%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="12,104.36,123.89,370.84,241.86"><head>Table 4 .</head><label>4</label><figDesc>4 Lenient performance for each question type</figDesc><table coords="12,302.96,123.89,172.13,9.15"><row><cell>Correct #</cell><cell>Correct %</cell><cell>MRR</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This research is supported by the national <rs type="programName">973 fundamental research program</rs> under contact of <rs type="grantNumber">G1998030413</rs>. We give our thanks to all the people who have contributed to this research and development, in particular <rs type="person">Yanbo Han</rs>, <rs type="person">Li Guo</rs>, <rs type="person">Qun Liu</rs>, <rs type="person">Hai Zhuge</rs>, <rs type="person">Zhihua Yu</rs> and <rs type="person">Lei Cheng</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_dprZ4Pq">
					<idno type="grant-number">G1998030413</idno>
					<orgName type="program" subtype="full">973 fundamental research program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PERSON</head><p>Who What do,does,did @2 cost? : MONEY What person @3? : PERSON Which person @3? : PERSON What is,was @1's birthday? : DATE What is,are,was,were @1's employer? : ORGANIZATION Such rules are useful to determine those questions led by "What", whose question types are hard to determine only by keyword-based rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Named Entity Tagger</head><p>We use GATE(http://gate.ac.uk) as our Named Entity tagger. GATE is developed by Sheffield NLP group. It can extract the named entities of PERSON, LOCATION, MONEY, PERCENT, DATE, ORGANIZATION and so on. But it has some obvious shortcomings, for example, it can't extract the pure NUMBER and most MEASUREMENT entities. Furthermore, many entities are incorrectly identified in GATE. Therefore, we revise the basic GATE system. First, we add several new components to identify the NUMBER and MEASUREMENT entities. Second, we modify GATE to improve the tagging correctness, mainly for the MONEY and PERCENT entities. As to some abstract question types, such as REASON(why), MEANING(what), METHOD(how) and so on, we apply some rules based on certain features to tag a snippet from the candidate passage as the candidate answer.</p><p>Before identifying the named entities, we need to construct candidate passages using the top 50 documents retrieved by SMART search engine. The algorithm <ref type="bibr" coords="10,365.14,463.85,34.92,6.13">[10][11][15]</ref> is as follows:</p><p>Step 1:Parse the sentences of the documents.</p><p>Step 2:Retrieve the sentences that contain keywords in the question.</p><p>Step 3:Construct a candidate passage every two sentences. If one sentence is long enough, it becomes a candidate passage itself.</p><p>Step 4:Assign each candidate passage a initial score equal to the score ranked by SMART search engine, i.e. score(P)=IR(D), D is the document where the candidate passage P lies.</p><p>Step 5:Add the idf value of all matched keywords contained by a candidate passage to its score.</p><p>Step 6:Calculate the number of matched keywords(count_m) in each candidate passage P. Add 0 to score(P) if the number of matched keywords is less than the threshold. Otherwise, add count_m to score(P). The threshold is defined as follows:</p><p>threshold=count_q if count_q&lt;4; threshold=count_q/2.0+1.0 if 4&lt;=count_q&lt;=8; threshold=count_q/3.0+2.0 if count_q&gt;8; here count_q is the number of keywords in the question.</p><p>Step 7:Calculate the size of matching window, then add 40*count_m/size(matching_window) to score(P). The size of matching window is defined as the number of keywords in the candidate passage between the first matched keyword and the last one.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="12,107.36,621.75,397.88,9.15;12,89.97,637.35,324.97,9.15" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,317.92,621.75,187.32,9.15;12,89.97,637.35,81.04,9.15">Structuring and Expanding Queries in the Probabilistic Model</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Narita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Honma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,190.09,637.35,155.39,9.15">The Ninth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,107.05,652.94,398.05,9.15;12,89.97,668.54,327.80,9.15" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,324.15,652.94,180.96,9.15;12,89.97,668.54,79.39,9.15">Structuring and expanding queries in the probabilistic model</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Yasushi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hiroko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Masumi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sakiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,188.23,668.54,203.26,9.15">The Eighth Text REtrieval Conference (TREC 8)</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,108.51,684.14,396.72,9.15;12,89.96,699.73,117.69,9.15" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,237.18,684.14,121.67,9.15">Okapi/Keenbow at TREC-8</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,385.42,684.14,119.82,9.15;12,89.96,699.73,91.40,9.15">The Eighth Text REtrieval Conference (TREC 8)</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.72,715.33,399.50,9.15;12,89.96,730.93,103.34,9.15" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,190.56,715.33,262.08,9.15">The anatomy of a large scale hypertextual web search engine</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,472.58,715.33,32.64,9.15;12,89.96,730.93,75.37,9.15">The 7th WWW Conference</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.01,746.53,400.32,9.15;13,89.97,76.62,327.56,9.15" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="12,375.22,746.53,130.11,9.15;13,89.97,76.62,92.97,9.15">The Pagerank citation ranking: Bring order to the web</title>
		<author>
			<persName coords=""><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Terry</forename><surname>Windograd</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Stanford Digital Libraries working paper, 1997-0072</note>
</biblStruct>

<biblStruct coords="13,108.56,92.22,396.65,9.15;13,89.97,107.81,53.19,9.15" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,165.93,92.22,232.81,9.15">Authoritative sources in a hyperlinked environment</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,409.65,92.22,95.55,9.15;13,89.97,107.81,21.27,9.15">Proc 9th ACM-SIAM SODA</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,107.47,123.41,397.77,9.15;13,89.97,139.01,197.70,9.15" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alistair</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timothy</forename><forename type="middle">C</forename><surname>Bell</surname></persName>
		</author>
		<title level="m" coord="13,331.97,123.41,173.27,9.15;13,89.97,139.01,132.97,9.15">Managing gigabytes: Compressing and indexing documents and images</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,107.50,154.61,397.75,9.15;13,89.97,170.20,277.43,9.15" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,460.36,154.61,44.89,9.15;13,89.97,170.20,33.05,9.15">OKAPI at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,142.88,170.20,198.07,9.15">The Third Text REtrieval Conference (TREC 3)</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,104.90,185.80,400.24,9.15;13,89.97,201.40,211.09,9.15" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,254.22,185.80,216.81,9.15">The TREC-8 Question Answering Track Evaluation</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dawn</forename><forename type="middle">M</forename><surname>Tice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,489.38,185.80,15.76,9.15;13,89.97,201.40,184.83,9.15">The Eighth Text REtrieval Conference (TREC 8)</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,113.79,217.00,391.55,9.15;13,89.97,232.59,398.69,9.15" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,166.82,232.59,72.76,9.15">AT&amp;T at TREC-8</title>
		<author>
			<persName coords=""><forename type="first">Amit</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Abney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michiel</forename><surname>Bacchiani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Hindle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,259.08,232.59,203.29,9.15">The Eighth Text REtrieval Conference (TREC 8)</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.56,248.19,393.73,9.15;13,89.97,263.79,289.50,9.15" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,230.04,248.19,275.24,9.15;13,89.97,263.79,41.29,9.15">Overview of system approach at TREC-8 ad-hoc and question answering</title>
		<author>
			<persName coords=""><forename type="first">Toru</forename><surname>Takaki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ntt</forename><surname>Data</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,149.90,263.79,203.29,9.15">The Eighth Text REtrieval Conference (TREC 8)</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,111.56,279.38,393.66,9.15;13,89.97,294.98,211.12,9.15" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,232.84,279.38,234.93,9.15">Information Extraction Supported Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Rohini</forename><surname>Srihari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,489.50,279.38,15.71,9.15;13,89.97,294.98,184.84,9.15">The Eighth Text REtrieval Conference (TREC 8)</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.40,310.58,392.92,9.15;13,89.97,326.18,415.27,9.15;13,89.97,341.77,117.71,9.15" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,454.03,310.58,51.29,9.15;13,89.97,326.18,266.89,9.15">The Use of Predictive Annotation for Question Answering in TREC-8</title>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anni</forename><surname>Coden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Valerie</forename><surname>Samn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,384.22,326.18,121.02,9.15;13,89.97,341.77,91.42,9.15">The Eighth Text REtrieval Conference (TREC 8)</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,110.48,357.37,394.74,9.15;13,89.97,372.97,240.97,9.15" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,238.20,357.37,262.65,9.15">Query Expansion Using Local and Global Document Analysis</title>
		<author>
			<persName coords=""><forename type="first">Jinxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,101.37,372.97,203.29,9.15">The Eighth Text REtrieval Conference (TREC 8)</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.16,388.57,393.15,9.15;13,89.97,404.16,330.35,9.15" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="13,261.50,388.57,239.26,9.15">Evaluating Question-Answering Techniques in Chinese</title>
		<author>
			<persName coords=""><forename type="first">Xiaoyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<pubPlace>Amherst, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Computer Science Department University of Massachusetts</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="13,110.53,419.76,394.79,9.15;13,89.96,435.36,412.50,9.15" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,449.92,419.76,55.40,9.15;13,89.96,435.36,165.02,9.15">Optimization in CLARIT TREC-8 Adaptive Filtering</title>
		<author>
			<persName coords=""><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Norbert</forename><surname>Roma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emilia</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">A</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,272.94,435.36,203.26,9.15">The Eighth Text REtrieval Conference (TREC 8)</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,110.78,450.96,394.48,9.15;13,89.96,466.55,339.13,9.15" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="13,401.78,450.96,103.48,9.15;13,89.96,466.55,95.82,9.15">FDU at TREC-9: CLIR, QA and Filtering Tasks</title>
		<author>
			<persName coords=""><forename type="first">Lide</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yikun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bingwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuejie</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,204.25,466.55,155.39,9.15">The Ninth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,110.94,482.15,394.21,9.15;13,89.96,497.75,180.51,9.15" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="13,264.71,482.15,177.78,9.15">The TREC-9 Filtering Track Final Report</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">A</forename><surname>Hull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,462.62,482.15,42.53,9.15;13,89.96,497.75,111.05,9.15">The Ninth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,110.36,513.35,394.90,9.15;13,89.96,528.94,194.05,9.15" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="13,170.04,513.35,190.78,9.15">Relevance Feedback in Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Rocchio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,379.30,513.35,121.59,9.15">The SMART Retrieval system</title>
		<meeting><address><addrLine>Englewood NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1971">1971</date>
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
