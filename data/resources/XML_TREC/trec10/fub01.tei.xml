<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,157.68,160.05,317.52,22.20;1,160.68,181.89,311.64,22.20">FUB at TREC-10 Web Track: A probabilistic framework for topic relevance term weighting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,214.08,218.04,70.80,16.40"><forename type="first">Gianni</forename><surname>Amati</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fondazione Ugo Bordoni</orgName>
								<address>
									<settlement>Roma</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,324.84,218.04,94.32,16.40"><forename type="first">Claudio</forename><surname>Carpineto</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fondazione Ugo Bordoni</orgName>
								<address>
									<settlement>Roma</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,270.60,231.96,92.16,16.40"><forename type="first">Giovanni</forename><surname>Romano</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fondazione Ugo Bordoni</orgName>
								<address>
									<settlement>Roma</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,157.68,160.05,317.52,22.20;1,160.68,181.89,311.64,22.20">FUB at TREC-10 Web Track: A probabilistic framework for topic relevance term weighting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">469CE8101DFF5621F99CF37DAE1D808C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>T h e m a i n i n vestigation of our participation in the WEB track of TREC-10 concerns the e ectiveness of a novel probabilistic framework 1 for generating term weighting models of topic relevance retrieval. This approach e n d e a vours to determine the weight of a word within a document in a purely theoretic way a s a c o m bination of di erent probability distributions, with the goal of reducing as much as possible the numberof parameters which m ust be learned and tuned from relevance assessments on training test collections. The framework is based on discounting the probability of terms in the whole collection, modeled as deviation from randomness, with a notion of information gain related to the probabilty of terms in single documents. The framework is made up of three components: the information content" component relative to the entire data collection, the information gain normalization factor" component relative t o a subset of the data collection the elite set of the observed term, and the term frequency normalization function" component relative to the document length and to other collection statistics. Each component i s o b t a i n e d b y computing a suitable probability density function. One advantage of the framework is that we m a y easily compare and test the behaviour of di erent basic models of Information Retrieval under the same experimental conditions and normalization factors and functions. At the same time, we m a y test and compare di erent term frequency normalization factors and functions. In addition to testing the e ectiveness of the term weighting framework, we were interested in evaluating the utility of query expansion on the WT10g collection. We used information theoretic query expansion and focused on careful paremeter selection.</p><p>In our experiments, we did not use link information, partly because of tight s c heduling -the WT10g collection was made available to us as late as late May 2001 -and partly because it has been shown at TREC-9 that it is not bene cial to topic relevance retrieval.</p><p>In the rest of the paper, we rst introduce the term weighting framework. Then we describe how collection indexing was performed, which probability functions were used in the experiments to instantiate the term weighting framework, and which p arameters were chosen for query expansion. A discussion of the main results concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The term weighting framework</head><p>The framework presented here can be found in 1 . The fundamental weighting Formula is the product of two information content functions:</p><formula xml:id="formula_0" coords="2,158.64,320.31,340.80,15.14">w = I n f 1 I n f 2 1</formula><p>Both I n f 1 and I n f 2 are decreasing functions of two probabilities P 1 and P 2 , respectively. The function I n f 1 is related to the whole document collection D, whilst I n f 2 to the elite set E t this notion roots back to Harter's work 5 of the term t, namely the set of all documents in which the term t occurs. P 1 is obtained as follows. We assume that words which bring little information are randomly distributed on the whole set of documents. By contrast, informative w ords diverge from the randomic behaviour and therefore they receive little probability according to a suitable model of randomness for Information Retrieval. This is the the inverse document frequency component" of our model, in the sense that similar to the standard IR models based on the idf measure, the informative words have a small probability to occur within a document. We p r o vide di erent basic models which de nes such a n o t i o n o f randomness in the context of Information Retrieval. A model of randomness is derived by a suitable interpretation of the probabilistic urn models of Types I and II 4 i n to the context of Information Retrieval. Basically, a model of Type I is a model where balls tokens are randomly extracted from an urn, whilst in Type II models balls are randomly extracted from an urn belonging to a collection of urns documents. Among type I models there is the Poisson model, the Bose-Einstein statistics, the Geometric distribution, whilst a type II model is the inverse document frequency model. Therefore, the frequency of a word within a document with the lowest probability P 1 as predicted by s u c h models of randomness or, equivalently, the words whose probability is less expected by t h e c hosen model of urns, are highly informative" words.</p><p>I n f 1 = , log P 1 large for informative words in the collection If we observe the elite set E t of the term, then we m a y derive a second conditional probability P 2 of term occurrence within a document in its elite set. The information content of a highly informative term, as obtained by m e a n s o f I n f 1 , will be tuned according to its elite set. This weight tuning process corresponds to the information gain component" of our model. We will take as weighting formula only a fraction I n f 2 of I n f 1 . This fraction of the information content corresponds to the gain" associated to the decision of accepting the term as an informative descriptor of the document. We assume, as in decision theory, that this information gain and thus inf 2 is inversely related to its odds P 2 . Di erently from P 1 , which is in general very small, P 2 should be in general close to certainty, especially when tf is large. If we observe m a n y occurrences of the term t, then the observed term should have a v ery high probability P 2 of being a descriptor of the document. We assume that P 2 is the conditional probability of observing, within an arbitrary document of the elite set, tf + 1 occurrences of a given word in the hypothesis that one has already observed tf occurrences. The higher the term frequency tf, the higher the conditional P 2 . Since the gain is inversely related to its odds: <ref type="bibr" coords="3,156.84,308.01,3.96,11.00" target="#b0">1</ref> I n f 2 = 1 , P 2 rate of the information content gained with t</p><p>The weight o f a term in a document is thus a function of two probabilities P 1 and P 2 which are related by the following relation:</p><formula xml:id="formula_1" coords="3,158.64,380.64,340.80,16.97">w = I n f 1 I n f 2 = , log 2 P 1 1 , P 2 2</formula><p>The term weight w of Formula 2 can be seen as a function of 4 random variables: w = wF;tf;n;N where tf is the within document term frequency N is the size of the collection n is the size of the elite set E t of the term, F is the term frequency in its elite set However, the size of tf depends on the document length: we have to derive the expected term frequency in a document when the document is compared to a xed length typically the average document length. We should determine what is the distribution that the tokens of a term follow in the documents of a collection at di erent document lengths. Once this distribution is obtained, the normalized term frequency tfn is used in the Formula 2 instead of the non-normalized tf.</p><p>One formula we have formally derived and successfully tested on previous TREC collections is:</p><formula xml:id="formula_2" coords="3,158.64,607.83,340.80,28.28">tfn = tf log 2 1 + c avg l l with c 1 3</formula><p>where avg l and l are the average length of the document collection and the length of the observed document respectively.</p><p>Our term weight w of Formula 2 will be thus a function of 6 random variables: w = wF; tfn; n; N = wF ; t f ; n ; N ; l ; a v g l where l is the document length avg l is the length mean</p><p>We postpone the discussion about the probability functions used to instantiate this framework and the choice of parameter c to Section 4.2. We rst describe, in the next section, how collection indexing was performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Test collection indexing</head><p>Text segmentation. Our system rst identi ed the individual terms occurring in the test collection, ignoring punctuation and case. The whole body of each document w as indexed except for HTML tags, which were removed from documents. Pure single keyword indexing was performed, and link information was not used.</p><p>Document pruning. As we h a d v ery limited storage capabilities, we performed some document pruning. We r e m o ved very long or short documents as well as documents which w ere deemed to be nontextual or nonenglish textual. Speci cally, w e pruned the documents with more than 10,000 words 2,897 or less than 10 words 57,031; also, we removed the documents that contained more than 50 of unrecognized English word 86,146, according to a large morphological lexicon for English Karp et al, 1992. In all, we removed 118.087 documents this is not the exact sum of the three categories due to document o verlap. The price we paid for this computational gain is that some relevant documents were lost. More exactly, w e removed 162 out of 3363 relevant documents 4.81. Thus, it should be emphasized that our actual performance retrieval was probably lower than the performance that we w ould have obtained by considering the whole set of documents.</p><p>Word pruning. Incorrect words a ect collection statistics and query expansion. In order to reduce the inherent web word noise, we removed very rare, ill-formed or exceedingly long words. Speci cally, the words contained in no more than 10 documents, which w ere apparently exclusively mispelled words, were dropped from the document descriptions. The words containing more than three consecutive equal characters or longer than 20 characters were also deleted. In this way, t h e n umber of distint w ords in the collection decreased dramatically, from 1,602,447 after steps 1 and 2 to only 293,484. Stop wording and word stemming. As we w ere primarily interested in early precision, we used a very limited stop list and did not peform word stemming at all.</p><p>The system has been implemented in ESL, a Lisp-like language that is automatically translated into ANSI C and then compiled by gcc compiler. The system indexes two gigabytes of documents per hour and allows sub-seconds search e s o n a 5 5 0 M H z Pentium III with 256 megabytes of RAM running Linux.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Term weighting models</head><p>The term-weighting framework described above w as instant i a t e d t o a n umb e r o f m o dels using the Bose-Eistein statistics and the inverse document frequency expected and non combined with the weight normalization factor I n f 2 and frequency normalization function tfn. We rst describe the basic models and then the 2 normalization factors L and B for I n f 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bose-Einstein statistics</head><p>The operational model of the Bose-Einstein statistics is constructed by a p p r o ximating the factorials by Stirling's formula. The model B E B E stands for Bose-Einstein is:</p><formula xml:id="formula_3" coords="5,133.80,339.00,365.64,51.63">I n f 1 tf = log 2 N + F , tf , 2!F !N , 1 F , tf!N + F , 1! 4 Let = F</formula><p>N be the mean of the frequency of the term t in the collection D, then the Bose-Einstein probability that a term occurs tf times in a document can be approximated by P 1 tf = We assume that the probability that an observed term contributes to select a relevant document is high if the probability of counting one more token of the same term in a relevant document is similarly high. This probability a p p r o a c hes 1 for high values of tf.</p><p>Laplace's normalization L The rst model of P 2 tf is obtained by the conditional probability ptf + 1 jtf; d o f Laplace's Law of Succession: P 2 tf = tf tf + 1 The normalization L for Laplace is:</p><formula xml:id="formula_4" coords="6,158.64,455.79,340.80,27.56">I n f 2 = 1 tf + 1 8</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bernoulli's normalization B</head><p>To obtain an alternative estimate of P 2 with Bernoulli's trials we use the following urn model. Let Bn; F; tf b e Bn; F; tf = F tf p tf q F,tf where p = 1 n and q = n,1 n .</p><p>We a d d a n e w t o k en of the term to the collection, thus having F + 1 tokens instead of F. We then compute the probability Bn; F + 1 ; t f + 1 that this new token falls into the observed document, thus having a within document term frequency tf + 1 instead tf. </p><formula xml:id="formula_5" coords="7,133.80,296.07,365.64,57.53">I n f 2 tf = F + 1 n tf + 1 9 4.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The parameter c for the baseline models</head><p>Independently from the model used, namely independently from the probability distributions P 1 and P 2 chosen, in TREC-9 and TREC-10 the best matching value for c was 7. The parameter c seems to be proportional to the size of the collection and inversely proportional to the size of the indexing vocabulary. A similar observation held also for the TREC-1 to TREC-8 collections.</p><p>We conjecture that the parameter c is connected to the Zip an law which relates the size of vocabulary to the size of the collection. This relationships which is not linear a ects the size of term frequency in the collection and thus the term frequency in the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Query expansion</head><p>For TREC-9, the results about the use of query expansion were not as good as with previous TRECs. Several groups reported that expansion did not improve o r e v en hurt retrieval performance 6 . As groups participating in TREC-9 web track had little opportunity for parameter tuning and the WT10g collection is very di erent from the previous collections, these result may h a ve been in uenced by p o o r c hoice of query expansion parameters. We encountered a similar problem with our own information theoretic-based expansion method 3, 2 . The weight o f a t e r m o f t h e expanded query q of the original query q is obtained as follows:</p><p>weightt 2 q = tfq n + tfn KL I n f 1 I n f 2 where tfq n is the normalized term frequency within the original query q i.e. tfq maxt2qtfq tfn KL is a term frequency in the expanded query induced by using a normalized Kullback-Leibler measure tfn KL = tf KL max t2q tf KL 10 tf KL = P R t log P R t P C t where P X t, with X = R;C, is the probability of occurrence of term t in the set of documents X estimated by the relative frequency of the term in X, R indicates the pseudo-relevant set, C indicates the whole collection. = 1 , = 0 :2 jRj = 3 with the number of terms of the expanded query equal to 10.</p><p>I n f 1 and inf 2 as de ned in Relation 1</p><p>This method was used with good results on TREC-8; however, when we ran it with the TREC-8 parameters against the TREC-9 collection the retrieval performance was badly a ected, whether using the new weighting functions discussed above or the Okapi formula. Thus, we focused on better selection of the values used for query expansion parameters for the WT10g document set, by performing parameter tuning on the TREC-9 test collection. We considered three parameters, namely the number of pseudo relevant documents, the number of expansion terms and the ratio between and in Rocchio's formula. One of the most striking characteristics of the WT10g collection is that the quality of baseline retrieval is lower than that obtained for past TREC collections. In an attempt to reduce the chance to select terms from mostly nonrelevant documents we c hose fewer pseudo-relevant documents than typically used for query expansion. We set the number of pseudo-relevant d o c u m e n ts at 3. In order to compensate for the lower quality of the terms used for expansion, we also adjusted the values of and . Since the original query should become more important as the quality of the expansion terms and their weights diminishes, we set the ratio between and to 5 i.e., = 1 , = 0 :2 and reduced the number of terms used for query expansion to 10. In this way, it should be easier for the expanded query to keep the focus on the original topic, even in the presence of bad term suggestions. Finally, it should be noted that the removal of bad words performed at indexing time see discussion above may h a ve considerably reduced the number of typographical errors in documents, which was pointed out as one of the causes for poor query expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Runs at TREC 10</head><p>We submitted 4 runs, 2 of them with our query expansion technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runs fub01ne and fub01ne2: In exp L</head><p>The baseline model fub01ne for I n f 1 is In exp and the normalization formula for I n f 2 is Laplace's law L namely the term weight i s : </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results and conclusions</head><p>In Table <ref type="table" coords="9,173.16,573.15,5.04,13.80" target="#tab_1">1</ref> we s h o w the retrieval performance of all possible models that can be generated by the term weighting framework using the probability functions introduced above, without and with query expansion. The main conclusions that can be drawn from the experimental results are the following.</p><p>-On the whole, the term weighting framework was e ective, with very good absolute and comparative retrieval performance run fub01be2 achieved the best performance of all o cial submissions in the title-only, automatic topic relevance task, although noteworthy di erences in performance were observed depending on which combination of probabilistic distributions and normalization techniques was used.</p><p>-Query expansion with the chosen parameters improved performance for almost all term weighting models and evaluation measures, with more tangible bene ts for average precision. More work is necessary to investigate the relative strengths and weaknesses of each model as well as to study the relationships to other term weighting approaches. Moreover, further experiments should be performed to control the e ect on performance of a wider range of factors, including word stemming, document pruning, and word pruning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,267.72,403.77,3.96,11.00;5,262.32,411.09,14.76,11.00;5,299.40,411.09,14.76,11.00;5,321.36,398.13,6.84,11.80;5,329.52,405.63,169.92,13.80;5,133.80,425.67,187.44,14.60;5,334.20,419.43,5.04,13.80;5,325.20,433.11,15.00,13.80;5,349.32,426.27,36.24,13.80;5,158.64,456.51,16.08,14.60;5,174.72,455.76,64.56,15.89;5,239.16,461.73,3.96,11.00;5,257.88,455.37,3.96,11.00;5,252.48,462.69,14.76,11.00;5,276.72,455.76,39.48,15.60;5,316.20,461.73,28.08,11.96;5,486.72,457.11,12.72,13.80;5,133.80,482.31,365.76,13.80;5,133.80,494.31,365.76,13.80;5,133.80,506.31,81.48,13.80;5,137.64,533.19,220.32,16.20;5,133.80,555.63,293.52,14.60;5,427.32,556.23,72.12,14.54;5,133.80,568.11,365.64,13.80;5,133.80,579.51,365.76,14.60;5,133.80,592.11,89.04,13.80;5,158.64,616.83,16.08,14.60;5,174.72,616.83,64.56,15.14;5,239.40,621.93,3.96,11.00;5,249.00,610.11,26.28,14.60;5,246.72,623.67,30.96,14.60;6,137.64,122.67,277.08,16.54;6,133.80,145.11,365.64,15.34;6,133.80,157.11,365.64,14.60;6,133.80,169.59,87.96,13.80;6,158.64,196.80,253.44,16.09;6,422.76,190.08,26.40,15.60;6,431.40,204.39,8.04,14.60;6,457.56,187.65,5.04,11.80;6,133.80,226.35,260.28,15.34;6,158.64,254.55,77.28,14.60;6,235.92,259.65,3.96,11.00;6,252.00,247.71,26.40,14.60;6,243.24,261.27,43.92,15.34;6,486.72,255.15,12.72,13.80;6,133.80,287.16,322.92,18.80;6,456.72,294.06,4.20,12.00"><head>1 NF 1</head><label>11</label><figDesc>right h a n d s i d e is known as the geometric distribution with probability p = Equation 4 by Stirling's formula and by Equation 5 were indistinguishable in the experiments, therefore Equation 5 is preferred to Equation 4 for its simplicity. The inverse document frequency model In We use a standard tf-idf probability distribution. The probability P 1 tf is obtained by rst computing the probability o f c hoosing a document containing the given term at random and then computing the probability o f h a ving tf occurrences of the same term in a document: document frequency model In exp A di erent model can be obtained by Bernoulli's law. Let n exp the expected number of documents containing the term under the assumption that there are F tokens in the collection. Then n exp = N P r o b tf 6 = 0 = N 1 , BN;F;0 = N 1 , N ,The third basic model is the tf-Expected idf model In exp : Term frequency normalizations: the probability P 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,158.64,230.07,25.20,14.60;9,201.60,223.83,5.04,13.80;9,187.80,236.91,32.64,14.60;9,236.16,230.07,35.52,14.60;9,271.80,235.17,3.96,11.00;9,287.88,223.23,26.28,14.60;9,279.12,236.91,43.80,15.34;9,481.68,230.67,17.76,13.80;9,133.80,258.27,365.64,14.60;9,133.80,270.87,188.76,13.80;9,133.80,295.71,365.76,16.54;9,133.80,308.91,100.08,14.60;9,233.88,308.91,206.76,15.34;9,440.64,309.51,58.92,14.54;9,133.80,320.91,144.72,14.60;9,158.64,348.87,25.20,14.60;9,201.60,342.75,5.04,13.80;9,187.80,355.71,32.64,14.60;9,236.16,348.12,22.32,15.60;9,258.48,353.97,3.96,11.00;9,282.12,342.75,5.04,13.80;9,273.12,356.31,15.00,13.80;9,306.84,348.12,45.48,15.60;9,352.32,353.97,3.96,11.00;9,367.08,356.31,14.88,13.80;9,481.68,349.47,17.76,13.80;9,133.80,376.83,365.76,14.60;9,133.80,389.43,47.16,13.80;9,133.80,414.39,100.56,16.20;9,133.80,430.35,156.24,14.60;9,290.04,430.35,204.96,15.14;9,495.00,434.49,3.96,11.00;9,133.80,442.35,208.20,14.60;9,158.64,469.47,25.20,14.60;9,198.48,462.75,24.96,14.60;9,187.80,476.31,42.60,14.60;9,242.52,469.47,30.12,14.60;9,272.64,474.57,3.96,11.00;9,282.24,462.75,26.28,14.60;9,279.96,476.31,30.96,14.60;9,481.68,470.07,17.76,13.80;9,133.80,500.19,365.64,14.60;9,133.80,512.67,47.16,13.80"><head></head><label></label><figDesc>ned as in Equation 3 with c = 7. Run fub01ne was performed without query expansion, whilst run fub01ne2 with. Run fub01be2: B E L. This was the best performing run at TREC-10. The baseline model fub01be for I n f 1 is B E and the normalization formula for I n f 2 is Laplace's law L namely the term weight i s : ned as in Equation 3 with c = 7. The automatic query expansion was performed.Run fub01idf: InBThe baseline model fub01idf for I n f 1 is In and the normalization formula for I n f 2 is Bernoulli's rate B namely the term weight i s : ned as in Equation 3 with c = 7 . The automatic query expansion was not performed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,187.56,637.71,312.00,14.60"><head></head><label></label><figDesc>The process Bn; F + 1 ; t f + 1 computes the probability o f obtaining one more token of the term t in the document d out of all n documents in which t</figDesc><table coords="7,133.80,136.47,365.76,157.16"><row><cell>occurs when a new token is added to the elite set. The ratio</cell></row><row><cell>Bn; F + 1 ; t f + 1 Bn; F; tf = F + 1 n tf + 1</cell></row><row><cell>of the new probability Bn; F + 1 ; t f + 1 to the previous one Bn; F; tf tells us whether the probability o f e n c o u n tering a new occurrence by c hance is increased or diminished.</cell></row><row><cell>Instead of using P 2 we normalize with the probability increment rate</cell></row><row><cell>IncrementRate = 1 , Bn; F + 1 ; t f + 1 Bn; F; tf</cell></row><row><cell>that is the normalization B is:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,153.00,122.61,322.92,145.50"><head>Table 1 :</head><label>1</label><figDesc>Method O cial run AvPrec Prec-at-10 Prec-at-20 Prec-at-30 Model performance without query expansion Comparison of performance of models and normalization factors.</figDesc><table coords="10,183.72,138.69,258.84,108.28"><row><cell>BEL InL InexpL BEB InB InexpB BEL InL InexpL fub01ne2 fub01ne fub01idf Model performance with query expansion 0.1788 0.3180 0.2730 0.1725 0.3180 0.2740 0.1790 0.3240 0.2720 0.1881 0.3280 0.2980 0.1900 0.3360 0.2880 0.1902 0.3340 0.2860 fub01be2 0.2225 0.3440 0.2860 0.1973 0.3200 0.2730 0.1962 0.3280 0.2760 BEB 0.2152 0.3400 0.2870 InB 0.2052 0.3380 0.2970 InexpB 0.2041 0.3360 0.2990</cell><cell>0.2413 0.2353 0.2440 0.2487 0.2580 0.2580 0.2513 0.2380 0.2507 0.2527 0.2680 0.2660</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0" coords="1,149.04,645.66,350.52,12.00;1,133.80,655.02,93.60,12.00"><p>The author has been carrying out his work also at the Computing Science Department, University of Glasgow, Scotland</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1" coords="3,149.04,635.46,265.80,12.00"><p>We also used an alternative monotone decreasing function, namely I n f</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2" coords="3,421.80,634.56,28.08,13.20;3,450.12,638.88,14.52,9.42;3,465.12,635.46,34.44,12.00;3,133.80,644.82,288.36,12.00;3,425.40,643.92,22.20,13.20;3,449.64,648.24,9.00,8.46;3,459.12,644.82,40.20,12.00;3,133.80,654.30,50.76,12.00;3,184.56,658.98,3.60,7.20;3,191.04,654.30,308.40,12.00;3,133.80,663.78,125.40,12.00;3,262.08,662.88,27.72,13.20;3,289.92,667.20,14.52,9.42;3,307.80,663.78,54.72,12.00"><p>= ,log 2 P 2 . Experimentally, this decreasing function seems to be a little less e ective t h a n I n f 2 = 1 , P 2 . Also, the function 1,P 2 will be easily generalized below to the increment r a t e o f t wo Bernoulli's trials, whilst a similar generalization with I n f 2 = , log 2 P 2 is problematic.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,149.28,447.27,350.28,13.80;10,149.28,459.15,350.04,13.80;10,149.28,471.15,22.92,13.80" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="10,371.76,447.27,127.80,13.80;10,149.28,459.15,287.29,13.80">Probabilistic models of information retrieval based on measuring divergence from randomness</title>
		<author>
			<persName coords=""><forename type="first">Gianni</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cornelis</forename><surname>Joost Van Rijsbergen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>Manuscript</note>
</biblStruct>

<biblStruct coords="10,149.28,490.23,350.16,13.80;10,149.28,502.23,350.16,13.80;10,149.28,514.23,100.56,13.80" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,389.52,490.23,109.92,13.80;10,149.28,502.23,169.42,13.80">An information theoretic approach to automatic query expansion</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">De</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Bigi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,327.00,502.38,172.44,13.60;10,149.28,514.38,18.34,13.60">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">191</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,149.28,533.31,350.16,13.80;10,149.28,545.46,350.04,13.60;10,149.28,557.19,269.40,13.80" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,288.12,533.31,192.48,13.80">Trec-8 automatic ad-hoc experiments at fub</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Romano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,163.32,545.46,336.00,13.60;10,149.28,557.34,84.11,13.60">Proceedings of the 8th Text REtrieval Conference TREC-8, NIST Special Publication 500-246</title>
		<meeting>the 8th Text REtrieval Conference TREC-8, NIST Special Publication 500-246<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">377</biblScope>
			<biblScope unit="page">380</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,149.28,576.39,349.92,13.80;10,149.28,588.27,313.08,13.80" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="10,230.28,576.54,268.92,13.60;10,149.28,588.27,84.64,13.80">The Estimation of Probabilities: an Essay on Modern Bayesian Methods, v olume 30</title>
		<author>
			<persName coords=""><forename type="first">Irving</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Good</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1968">1968</date>
			<publisher>The M.I.T. Press</publisher>
			<pubPlace>Cambrige, Massachusetts</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,149.28,607.47,350.16,13.80;10,149.28,619.47,350.28,13.80;10,149.28,631.35,170.04,13.80" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,248.16,607.47,251.28,13.80;10,149.28,619.47,346.34,13.80">A probabilistic approach to automatic keyword indexing. part I: On the distribution of specialty words words in a technical literature</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Harter</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,149.28,631.50,85.18,13.60">Journal of the ASIS</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="197" to="216" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,149.28,650.55,350.28,13.80;10,149.28,662.43,254.04,13.80" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,205.56,650.55,142.10,13.80">Overview of the trec-9 web track</title>
		<author>
			<persName coords=""><surname>Wking</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,381.84,650.70,117.72,13.60;10,149.28,662.58,134.78,13.60">Proceedings of the 9th Text Retrieval Conference TREC-9</title>
		<meeting>the 9th Text Retrieval Conference TREC-9<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
