<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,91.95,103.06,428.08,16.20;1,128.31,123.82,355.50,16.20">A prototype Question Answering system using syntactic and semantic information for answer retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,128.31,183.17,80.33,9.94"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of York Heslington</orgName>
								<address>
									<postCode>YO10 5DD</postCode>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,216.02,183.17,66.43,9.94"><forename type="first">Marco</forename><surname>De Boni</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of York Heslington</orgName>
								<address>
									<postCode>YO10 5DD</postCode>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,289.94,183.17,103.71,9.94"><forename type="first">José-Luis</forename><surname>Jara-Valencia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of York Heslington</orgName>
								<address>
									<postCode>YO10 5DD</postCode>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,400.93,183.17,82.75,9.94"><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of York Heslington</orgName>
								<address>
									<postCode>YO10 5DD</postCode>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,91.95,103.06,428.08,16.20;1,128.31,123.82,355.50,16.20">A prototype Question Answering system using syntactic and semantic information for answer retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">00CB0B1D1B5ED74D93A86AEC3AAF1DA5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>This was our first entry at TREC and the system we presented was, due to time constraints, an incomplete prototype. Our main aims were to verify the usefulness of syntactic analysis for QA and to experiment with different semantic distance metrics in view of a more complete and fully integrated future system. To this end we made use of a part-of-speech tagger and NP chunker in conjunction with entity recognition and semantic distance metrics. We also envisaged experimenting with a shallow best first parser but time factors meant integration with the rest of the system was not achieved. Unfortunately due to time constraints no testing and no parameter tuning was carried out prior TREC. This in turn meant that a number of small bugs negatively influenced our results. Moreover it was not possible to carry out experiments in parameter tuning, meaning our system did not achieve optimal performance. Nevertheless we obtained reasonable results, the best score being 18.1% of the questions correct (with lenient judgements).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question-Answering algorithm</head><p>The YorkQA question answering system takes inspiration from the generic question answering algorithm presented in <ref type="bibr" coords="1,194.92,598.69,67.75,9.94" target="#b10">[Simmons 1973</ref>]. Although unacknowledged by more recent research its general structure is very similar to the basic algorithms used in the Question Answering systems built for previous TREC conferences</p><p>The algorithm proceeds as follows: I) Accumulate a database of semantic structures representing sentence meanings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II)</head><p>Select a set of structures that appears relevant to the question. Relevance is measured by the number of lexical concepts in common between the proposed answer and the question. This is done by ordering the candidates according to the number of Token values they have in common with the questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III)</head><p>Match the question structure against each candidate. This is done by: 1. Index the documents using a standard Information Retrieval engine 2. Read in the next question, and repeat the following until there are no more questions: 3. Analyse each question to determine the question category, i.e. the type of entity the answer should contain 4. Decide the query to send to the IR engine and send the query 5. Pass the retrieved documents to the sentence splitter, tagger, chunker and named entity recogniser 6. Analyse the sentences to determine if they contain the correct entity type. 7. Rank the sentences containing the correct entity type according to their semantic distance from the question. 8. Use a parser to determine the answer by finding a match between the question and the ranked sentences 9. If matching through the parser fails look for the best entity in the top ranked sentences or, if an entity of the appropriate type is not found, an appropriate 50 byte window.</p><p>Step 1 of our algorithm corresponds to point I in Simmons' algorithm, 2-7 correspond to II and 8-9 correspond to III. Due to time constraints we were unable to implement 8.</p><p>What follows is a more detailed description of the main parts of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Type Identification</head><p>The question analyser used pattern matching based on wh-words and simple part-of-speech information combined with the use of semantic information provided by WordNet <ref type="bibr" coords="3,461.76,113.69,60.31,9.94" target="#b5">[Miller 1995]</ref> to determine question types. Unlike other question analysers constructed for previous TREC QA tracks (e.g. <ref type="bibr" coords="3,145.23,139.01,78.82,9.94" target="#b2">[Harabagiu 2001]</ref>) our question type analyser only recognised a small number of different types. Question types were limited to time, place, currency, organisation, length, quantity, reason, place, constitution, person, length, mode, recommendation, truth and a fallback category of thing. A number of these, for example recommendation ("Should I do X?"), reason ("Why did X occur") and truth ("Is X Y?") revealed themselves not to be needed for the track as no questions of that type were present. An initial evaluation estimated an accuracy of 83% for the question recogniser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Information Retrieval</head><p>Steps 1 and 3 of our algorithm were carried out by a) using the SMART Information Retrieval system (for an introduction to SMART, see <ref type="bibr" coords="3,285.02,308.67,71.26,9.94" target="#b6">[Paijmans 1999]</ref>) to index the documents and retrieve a set of at most 50 documents using the question as the query; and b) taking the first 50 documents supplied by the Prise Information Retrieval engine, as provided by the TREC organisers. Unfortunately due to time constraints we were unable to tune the SMART IR system for the task at hand and in fact the documents retrieved by the SMART engine were much worse (very low in both precision and recall) than the documents provided by NIST.</p><p>The YorkQA system then transformed the output of the retrieval engine into appropriate XML documents and used a number of tools for linguistically processing them, in particular a tokeniser, sentence splitter, part-of-speech tagger, morphological analyser, entity recogniser, QP-and NPchunker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tagging and chunking</head><p>The texts were processed using the TnT part-of-speech tagger <ref type="bibr" coords="3,370.10,516.50,64.15,9.94" target="#b0">[Brants, 2000]</ref> and a Noun Phrase chunk parser based on Transformation Lists <ref type="bibr" coords="3,290.42,529.10,134.83,9.94" target="#b9">[Ramshaw and Marcus, 1995]</ref> to find non-recursive noun phrases. To improve the accuracy of the chunker, we trained it on a copy of the Wall Street Journal corpus that was manually corrected by a human annotator, so as to improve the quality of the training data. This increased the initial accuracy more than 3% to 95%.</p><p>The sentence splitter was based on <ref type="bibr" coords="3,248.08,593.65,72.41,9.94" target="#b3">[Mikheev, 1999]</ref>. It is divided in two steps of processing dotending words. Firstly, the system decides whether they are or not abbreviations. Nonabbreviations ended with dots all indicate sentential endings; and abbreviations followed by a non-capitalised word are never sentence ends. The second step addresses the difficult case, when an abbreviation is followed by a capitalised word, and several heuristics are used to decide whether a sentence ends there or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Named Entity Recognition</head><p>This was an important step in the processing of the text as the YorkQA system initially tried to find sentences containing an appropriate entity that might answer a determined question. Nevertheless this module was, for this version, the weakest link in the processing pipeline and should, and will, be subject to serious improvement in the future. At present, this tool aims to recognise six types of entities:</p><p>• Currency expressions, such as "$10,000", "2.5 million dollars", etc. The evaluation shows a very high accuracy for this sub-module (4/4), but all the expressions found were in dollars and it is not clear that this performance could be obtained for other types of currencies. Its precision however should remain very high in any stricter evaluation. • Locations, such as "Chicago", "Thames", "Mount Kilimanjaro", etc. This sub-module is programmed to recognise locations by context words, so that it recognises "New York City" but not "New York". This limitation is clearly reflected in its performance as this module missed all location expressions (0/32) in the sentences selected for the manual evaluation.</p><p>• Organisations, such as "Climbax Corp.", "Nobel Prize Committee", "The National Library of Medicine", etc. The approach used by this sub-module is to look for a clear organisational word at the end of a sequence of capitalised words. Therefore, it recognises "National Cancer Institute" but not "Massachusetts Institute of Technology". Consequently, we determined that only 35% (8/23) of the organisations mentioned in the evaluated text were correctly tagged and that most of the error were due to poor recall. • People names, such as "Reagan", "Marilyn Monroe", "G. Garcia Marquez", etc. Again precision is preferred to recall in this sub-module. Therefore, people's names are marked when they are surrounded by clear context words such as personal titles ("Mr.", "Dr.", etc.) and common pre-name positions ("President ...", "Sen. ...", etc.) or both the forename(s) and the surname(s) are found in a gazetteer of common names in several languages. The evaluation is consistent with this bias as most of the 69% of incorrect answer (13/42) from this module are consequence of poor recall. • Quantity expressions, such as "twelve", "11/2 billion", etc. Because of the relative regularity in this kind of expressions, this sub-module gets a fairly good accuracy (60/73) and most errors are misinterpretations of the words "a" and "one". • Time expressions, such as "June, 28 1993", "Today", "late 2000", etc. The accuracy of this sub-tool is around 50%, mainly because it misses many relative time expressions (such as "the day after the great storm") which are difficult to capture by regular expressions only.</p><p>Clearly these expressions help the system to answer questions about people, organisations, money, etc. but we are very aware that much more must be done on this. For example, the system would get better performance if a broader range of entities were recognised, in particular speeds, weights, lengths, duration expression, etc. will certainly increase the focus of the search which at present can only rely on quantity entities. This and other improvements are planned in the future and we hope to have in hand a much better entity recogniser for the next version of YorkQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measuring Semantic distance</head><p>The central part of our system was the semantic distance measuring module, which analysed the tagged and chunked sentences in the documents selected by the Information Retrieval engine, comparing them with the question in order to find the sentence which was most similar.</p><p>To calculate similarity was necessary to calculate the semantic (or conceptual) distance between a sentence and a question. A number of algorithms have been presented to measure semantic (or conceptual) distance (see for example <ref type="bibr" coords="5,269.30,178.37,51.96,9.94" target="#b5">[Miller and</ref><ref type="bibr" coords="5,326.06,178.37,141.16,9.94">Teibel 1991, Rada et al. 1989]</ref>). WordNet <ref type="bibr" coords="5,90.04,190.96,57.10,9.94" target="#b5">[Miller 1995</ref>] has been shown to be useful in this respect, as it explicitly defines semantic relationships between words (see, for example, <ref type="bibr" coords="5,300.38,203.68,139.99,9.94" target="#b8">[Mihalcea and Moldovan 1999]</ref> and <ref type="bibr" coords="5,461.89,203.68,60.30,9.94;5,90.04,216.28,31.87,9.94">[Harabagiu et al. 1990</ref>]). Our system, however, differs from previous approaches in that it does not limit itself to considering the WordNet relationships of synomymity and hyperonymy, but makes use of all semantic relationships available in WordNet (is_a, satellite, similar, pertains, meronym, entails, etc.) as well as information provided by the noun phrase chunker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer identification</head><p>The question types were used to identify a candidate answer within the retrieved sentences in the case of questions of type quantity, person, place, time and currency; in the case of questions of type reason, recommendation, mode and difference, appropriate keywords were searched for (e.g. a part of sentence following the word "because" is probably a reason); finally, if the question analyser failed to recognise any question type (i.e. the answer type was the catch-all category "thing"), the system looked for a portion of text 50 bytes long within a sentence which was closest to the words contained in the question, but contained the lowest number of question words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results, Conclusions and further work</head><p>Given that our system was a simple prototype which had not been tested, nor tuned, the results we got were encouraging, the best score being 18.1% of the questions correct (with lenient judgements), proving that the use of syntactic and semantic information can be fruitfully used for the QA task.</p><p>The current system will have to be fully tested and debugged and a full evaluation will have to be carried out on each separate module to evaluate performance and identify the source of errors.</p><p>Future work will include: generic question type identification (needed since handing coding for each type is cumbersome and time consuming); an improved named-entity recogniser; improved semantic distance metrics; a parser to transform sentences into a simple logical form which can then be manipulated in order to find an answer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,90.06,161.44,432.32,245.00"><head>•</head><label></label><figDesc>Determining if the head verb of the question matches the head verb of the candidate. If there is no direct match, a paraphrase rule is applied to see if the question structure can be transformed into the structure of the answer. Paraphrase rules are stored as part of the lexicon and an examination of the lexical structures of two words will be able to determine if there is a rule (path) connecting the two. If there is not, the set of words that the first transforms into is recursively examined to see if can be transformed into the second word. If this fails, the transformation rules are recursively applied to the second word to see if a match can be found. This procedure continues until either a match is found or an arbitrarily set depth is reached.• Applying the same procedure to the other words in the question and the candidate answer question in order to transform the question structure into the form of the candidate answer.</figDesc><table coords="2,90.06,327.50,432.20,78.94"><row><cell>• Examining quantifiers and modalities to see if quantificational, tense and</cell></row><row><cell>negation relationships are matched. • Examining the question's semantic structure to determine if the question word</cell></row><row><cell>type (the wh-word) is present and satisfied in the answer</cell></row><row><cell>Our algorithm proceeded as follows:</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,90.03,116.09,329.13,9.94" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brants</surname></persName>
		</author>
		<title level="m" coord="6,144.61,116.09,178.81,9.94">TnT -A Statistical Part-of-Speech Tagger</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>User manual</note>
</biblStruct>

<biblStruct coords="6,90.03,142.73,376.89,9.94;6,90.04,155.33,423.68,9.94" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,313.38,142.73,153.55,9.94;6,90.04,155.33,138.32,9.94">WordNet2 -a morphologically and semantically enhanced resource</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">I</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,251.78,155.33,119.28,9.94">Proceedings of SIGLEX-99</title>
		<meeting>SIGLEX-99</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>University of Maryland</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.04,181.97,410.08,9.94;6,90.03,194.68,97.55,9.94" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,179.91,181.97,235.09,9.94">FALCON -Boosting Knowledge for Answer Engines</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,434.29,181.97,65.83,9.94;6,90.03,194.68,34.25,9.94">Proceedings of TREC-9</title>
		<meeting>TREC-9</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.03,221.32,408.33,9.94;6,90.04,233.92,22.08,9.94" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,175.25,221.32,133.23,9.94">Periods, capitalized words, etc</title>
		<author>
			<persName coords=""><forename type="first">Andrei</forename><surname>Mikheev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>Submitted to Computational Linguistics</note>
</biblStruct>

<biblStruct coords="6,90.04,260.56,413.18,9.94;6,90.04,273.28,258.82,9.94" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,211.19,260.56,162.97,9.94">A proposal for lexical disambiguation</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Teibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,397.57,260.56,105.64,9.94;6,90.04,273.28,175.77,9.94">Proceedings of DARPA Speech and natural Language Workshop</title>
		<meeting>DARPA Speech and natural Language Workshop<address><addrLine>California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.04,299.92,409.53,9.94" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,158.07,299.92,130.41,9.94">WordNet: A Lexical Database</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,300.74,299.92,126.83,9.94">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.04,326.55,225.70,9.94;6,90.03,339.34,304.53,10.80" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="6,171.11,326.55,131.43,9.94">SMART Tutorial for beginners</title>
		<author>
			<persName coords=""><forename type="first">Hans</forename><surname>Paijmans</surname></persName>
		</author>
		<ptr target="http://pi0959.kub.nl:2080/Paai/Onderw/Smart/hands.html" />
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.03,366.99,415.17,9.94;6,90.04,379.59,409.17,9.94" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,312.29,366.99,192.91,9.94;6,90.04,379.59,58.64,9.94">Development and application of a metric on semantic nets</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Rada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mili</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Bicknell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blettner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,176.75,379.59,233.16,9.94">IEEE Transactions on Systems, Man and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.04,406.23,418.41,9.94;6,90.04,418.95,266.02,9.94" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,249.62,406.23,258.82,9.94;6,90.04,418.95,18.43,9.94">A Method for Word Sense Disambiguation of Unrestricted Text</title>
		<author>
			<persName coords=""><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,127.11,418.95,104.81,9.94">Proceedings of ACL &apos;99</title>
		<meeting>ACL &apos;99<address><addrLine>Maryland, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-06">June 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.04,445.58,407.73,9.94;6,90.04,458.18,418.40,9.94;6,90.04,470.90,24.84,9.94" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,301.23,445.58,196.53,9.94;6,90.04,458.18,37.63,9.94">Text Chunking Using Transformation-Based Learning</title>
		<author>
			<persName coords=""><forename type="first">Lance</forename><forename type="middle">A</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,139.83,458.18,285.67,9.94">Proceedings of the Third ACL Workshop on Very Large Corpora</title>
		<meeting>the Third ACL Workshop on Very Large Corpora</meeting>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="82" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.04,497.54,431.12,9.94;6,90.04,510.14,384.81,9.94;6,90.04,522.74,73.19,9.94" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,169.38,497.54,342.82,9.94">Semantic Networks: computation and use for understanding English sentences</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">F</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">C</forename><surname>Schank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">M</forename><surname>Colby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,252.85,510.14,192.58,9.94">Computer Models of Thought and Language</title>
		<meeting><address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
