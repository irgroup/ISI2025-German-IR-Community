<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,102.72,84.59,390.17,16.20">Description of NTU System at TREC-10 QA Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,202.32,130.58,79.68,12.53"><forename type="first">Chuan-Jie</forename><surname>Lin</surname></persName>
							<email>cjlin@nlg2.csie.ntu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Taiwan University Taipei</orgName>
								<address>
									<region>R.O.C</region>
									<country key="TW">TAIWAN</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,309.36,130.58,83.52,12.53"><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
							<email>hh_chen@csie.ntu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Taiwan University Taipei</orgName>
								<address>
									<region>R.O.C</region>
									<country key="TW">TAIWAN</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,102.72,84.59,390.17,16.20">Description of NTU System at TREC-10 QA Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3FB042105642E59E401FCB8F82310744</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the past years, we attended the 250-bytes group. Our main strategy was to measure the similarity score (or the informative score) of each candidate sentence to the question sentence. The similarity score was computed by sums of weights of cooccurred question keywords.</p><p>To meet the requirement of shorter answering texts proposed in this year, we adapt our system, and experiment on a new strategy that is focused on named entities only. The similarity score is now measured in terms of the distances to the question keywords in the same document. The MRR score is 0.145. Section 2 will deal with our work in the main task.</p><p>We also attended the list task and the context task this year. In the list task, the algorithm is almost the same as the one in the main task except that we have to avoid duplicate answers and find the new answers at the same time. Positions of the candidates in the answering texts should be considered. We will talk about this in Section 3.</p><p>In the context task, how to keep the context, and what the answers of the previous questions can help are the main issues. In our strategy, the answers of the first question are kept when answering the subsequent questions, but the answers of the other ones (denoted by question i) are kept only if question i has a co-referential relationship to its previous one. Section 4 will describe this strategy in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Main Task</head><p>In the previous 250-bytes task, we measured the similarity of the question sentence and each sentence in the relevant documents, and reported the top 5 sentences with the highest scores and with the question focus words. In our experiment, the real answer sometimes lies in the sentence that is not so "similar" to the question. It becomes harder to extract text shorter than 50 bytes and containing the answer in this manner. Therefore, we experiment on another strategy, which is "candidatefocused" rather than "sentence-focused".</p><p>After reading a question, the system first decides its question type and keywords as usual. Now every named entity in the relevant documents becomes our answer candidate. For each candidate, we find out its distances to the question keywords in the same document, and sum up the reciprocals of these distances. One question keyword only contributes once, i.e., if a keyword occurs more than once, only the one nearest to the candidate contributes the score. Moreover, we assign higher weights to the keywords that are named entities. After scoring all the candidates, the highest top five are proposed, together with the texts surrounding the candidates within 50 bytes. The texts are extracted in such a way that the candidates can be placed in the middle.</p><p>In our experiment, we found that if there is a question keyword right preceding or following the candidate, it will dominate the score despite of the other question keywords. To solve this problem, we divide the distance by three, i.e., we consider three words as a unit to measure the distance. The scoring function is shown as follows:</p><p>( )</p><formula xml:id="formula_0" coords="2,115.92,390.84,388.00,31.87">  ∑ × - = ∩ ∈ D Q t D D t weight x pos t pos x score ) ( 3 ) ( ) ( min 1 ) (<label>(1)</label></formula><p>where</p><p>x is an answer candidate, Q is the question sentence, D is the document currently examined, t is a term occurring in both Q and D, and pos D (t) is one of the occurrence positions of t in D.</p><p>The algorithms of deciding question type and extracting named entities are the same as those in last year, which was proposed in <ref type="bibr" coords="2,354.72,509.16,105.98,10.80" target="#b0">Lin and Chen (2000)</ref>. If we cannot tell which question type a question belongs to, or the question type is not concerned with a named entity, we consider every kind of entities as candidates. To extract different answers as more as possible, we ignore those answering texts whose named entity answers have appeared in the previous answering texts.</p><p>Two runs were submitted this year. When question keywords were prepared in the first run qntuam1, variants of ordinary words (inflections of verbs, plural forms of nouns, etc.) and named entities (adjective forms of country names, abbreviations of organization names, etc.) are added into the keyword bag. Stems of keywords are also added with a lower weight. Note that no matter how many variants or stems of a keyword are matched in a document, only one of them contributes the score. We select the one that can contribute the highest score.</p><p>In the second run qntuam2, the synonyms and explanations provided by WordNet (Fellbaum Ed., 1998) are also added, with lower weight to reduce the noise.</p><p>Moreover, if there are m words in an explanation text, and n words occur in the document, the matching score of this explanation is defined as ) (e weight m n × , where weight(e) is the weight of this explanation.</p><p>MRRs of these two runs are 0.145 and 0.101 under strict strategy, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">List Task</head><p>List task is a new task beginning in this year. A question does not only ask for its information need but also a specified number of answers. Therefore, the system has to offer different answers to the specified number. An example is Question 1:</p><formula xml:id="formula_1" coords="3,114.70,251.85,307.60,18.60">¢¡ ¤£ ¦¥ ¨ § © ! #" $£ &amp;% (' $) 0 ¡ ¤ 1 § 2 3© £ ¦¥ 4 § 65 7 8 § @9 A2 B DC (¡ ¤) 0£ &amp;) 0 E FE G£ H£ 8I</formula><p>In this case, the system is asked to provide 20 names of different countries. Besides deciding which country produces coffee, the system also has to decide if the answer is duplicated, or if two answers are identical to each other.</p><p>The main algorithm to this task is almost the same as the main task. The only difference is that we extract the answering text in the manner that the candidates will be located at the beginning. By this way, if more than one answer appears in the same sentence, the previously proposed candidates will not appear again in the subsequent answering texts. The algorithm of the main task has already ignored the same answers (which is lexically identical), so we do not do other things to check answer identity.</p><p>Two runs were submitted as the same as those in the main task. Scores of the average accuracy are 0.18 and 0.14, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Context Task</head><p>There is another new task this year. A series of questions are submitted, which are somewhat relative to the previous questions. For example, in Question CTX1:</p><p>PI Q SR 1T U HR WV $X @Y a0X ¤V T b dc (e f g 30b 1U H`h 4i qp sr (i #t ui Dv w 0r x y i t $i B x A t $x ¨ ( ¨ d ( Ae (f g ih j ¢k ml on 1p #q @r (p Hs tr Au r uq n 1u Fv 4n 1p #w (w Ax 0k 1y</p><formula xml:id="formula_2" coords="3,114.40,623.45,214.30,74.00">z 8{ | S} 1 H} # 0 3 ¦ 4 0 3 &amp; A D 0 A i ¢ H $ # q t ( H ( o 4 0 3 &amp; ( H ( 8 ¡ S¢ 1 H 3 £ 4 0 3 &amp;¤ ¢ 1 ¦¥ a ¦ A 0 A &amp; § 0 #¤ 6 H ( ¨ ¢ H $© ¤ § 0¢ dª A #¥ ¨ « £ 4 ¬¥ o© !¥ ¨ H (</formula><p>Question CTX1a asks the name of the museum. Question CTX1b continues to ask the date of the event mentioned in Question CTX1a, so this question and its answer are important keys to Question CTX1b. Question CTX1c asks more details of Question CTX1a, but irrelevant to Question CTX1b. So is Question CTX1d. But Question CTX1e refers to both Question CTX1a and CTX1d. We can draw a dependency graph of this series of questions as below:</p><formula xml:id="formula_3" coords="4,114.70,123.85,205.80,52.20">¢¡ ¤£ ¦¥ ¨ § © ¢ ! ¢" ¤# ¦$ ¨% &amp; ' ( 0) 1 ¢2 3 4 5 6 ¢7 ¤8 ¦9 ¨@ A B C 0B 6 ¢7 8 9 D E F ¢G H ¦I P Q ¢R ¤S ¦T ¨U V W X 0Y ¢a b c d</formula><p>If a question is dependent on one of its previous question, it is obvious that the information relative to this previous question is also important to the present question.</p><p>Thus the system has to decide the question dependency.</p><p>We proposed a simple strategy to judge the dependency. Because the first question is the base question of this series, every subsequent question is dependent to the first one. After reading a question, if there is an anaphor or a definite noun phrase whose head noun also appears in the previous question, we postulate that this question is dependent on its previous question.</p><p>Next issue is that how we can use the dependency information in finding answers as well as its context information. After answering a single question, the system has located some answering candidates together with documents and segments of texts in which these candidates appear. Such information can be used to answer its subsequent dependent questions, as well as the keywords of the question itself. Note that context information can be transitive. In the above example, Question CTX1e consults the information that Question CTX1d itself owns, and Question CTX1d refers to, i.e., Question CTX1a.</p><p>In our experiment, we only consider the keywords and their weights as the context information. Furthermore, we assign lower weights to the keywords in the context information so that the importance of recent keywords cannot be underestimated. The answers to the previous question remain their weights because they are new information. The question type is decided by the present question.</p><p>The accompanying issue is that how confident an answer is included in the context information. This is because we may find the wrong answers in the preceding questions and those errors may be propagated to the subsequent questions.</p><p>Moreover, do these five answers have the same weight? Or we trust the answers of the higher ranks than those of the lower ones, or only the top one is considered?</p><p>These issues are worthy of investigating, but not yet implemented in the experiment of this year. We assign weights to the previous answers according to the following equation:</p><p>(</p><p>)</p><formula xml:id="formula_4" coords="4,115.92,733.00,388.00,15.64">) ( _ 5 ) ( 6 ) ( _ ) ( x eAns Pr weight x rank x NE weight x weight × - × =<label>(2)</label></formula><p>where weight_NE(x) assigns higher weight if x is a named entity; rank(x) is the rank of x, and weight_PreAns(x) is a discount to the previous answers because they may be wrong. The square root part tries to assign higher weights to the higher-ranked answers.</p><p>Because only relevant documents to the first questions are provided, and we do not implement an IR system on TREC data, we cannot do a new search when answering the subsequent questions. Our solution is to search the same relevant set of the first question.</p><p>We submitted one run this year. Its main algorithm followed the first run of the main task.</p><p>There is still no formal evaluation of this task. The MRR of all 42 question of our result is 0.139. 4 of the first questions are correctly answered. Answers of at least one of the subsequent questions can also be found in each of these 4 series.</p><p>Only one of the series is fully answered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Comparing the results of two runs of the main task and the two runs of the list task, we can find that synonyms and explanations introduce too much noise, so that the performance is worse. However, paraphrase is an important problem in question answering. Explanation provides only one of the paraphrases, thus we have to do more researches on paraphrases.</p><p>After investigation of the results of the list task, we found that there is a small bug when reporting answers. Although duplicate answers were neglected, equivalent answers were not. In other words, adjective forms of country names were regarded as different answers to their original names, which produced redundancy and lowered the performance.</p><p>In this year, the question types of many questions are not named entities. Many of them in the main task are "definition" questions. For example, ¢¡ ¤£ ¦¥ ¨ § © "! $# &amp;% ' )( 10 2¥ 43 ¢0 65 © 5 £ 7 98 @ ¢A ¤B ¦C ¨D E F G H I P "Q $R &amp;S 'T UD VE WC 1T 6G XT 6D F Y aÌ n our system, we only take named entities as answer candidates, so we cannot answer such type of questions, and the performance is rather worse than that of last year.</p><p>The same problem happened in the context task, too. Therefore, it is not obvious that our proposed model to the context task is good or bad. Further investigation and experiment are needed to verify this point.</p></div>		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,90.00,95.58,415.17,9.94;6,108.00,113.10,310.20,9.94" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,212.76,95.58,224.49,9.94">Description of NTU System at TREC-9 QA Track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,450.24,95.58,54.93,9.94;6,108.00,113.10,213.77,9.94">Proceedings of The Ninth Text REtrieval Conference (TREC-9</title>
		<meeting>The Ninth Text REtrieval Conference (TREC-9</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="389" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.00,134.22,299.64,9.94" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,199.68,134.22,184.96,9.94">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
