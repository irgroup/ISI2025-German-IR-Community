<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,117.28,95.66,377.56,14.36">FDU at TREC-10: Filtering, QA, Web and Video Tasks</title>
				<funder>
					<orgName type="full">NSF of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,165.88,126.47,31.77,9.50"><forename type="first">Lide</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhe Feng Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,205.62,126.47,67.48,9.50"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhe Feng Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,280.99,126.47,41.26,9.50"><forename type="first">Junyu</forename><surname>Niu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhe Feng Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,329.00,126.47,41.60,9.50"><forename type="first">Yikun</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhe Feng Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,378.10,126.47,42.36,9.50"><forename type="first">Yingju</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhe Feng Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,117.28,95.66,377.56,14.36">FDU at TREC-10: Filtering, QA, Web and Video Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5A617DE386B46D8404B8E1D7E87D7063</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This year Fudan University takes part in the TREC conference for the second time. We have participated in four tracks of Filtering, Q&amp;A, Web and Video.</p><p>For filtering, we participate in the sub-task of adaptive and batch filtering. Vector representation and computation are heavily applied in filtering procedure. Four runs have been submitted, which includes one T10SU and one T10F run for adpative filtering, as well as another one T10SU and one T10F run for batch filtering.</p><p>We have tried many natural language processing techniques in our QA system, including statistical sentence breaking, POS tagging, parsing, name entity tagging, chunking and semantic verification. Various sources of world knowledge are also incorporated, such as WordNet and geographic information.</p><p>For web retrieval, relevant document set is first created by an extended Boolean retrieval engine, and then reordered according to link information. Four runs with different combination of topic coverage and link information are submitted.</p><p>On video track, We take part in both of the sub-tasks. In the task of shot boundary detection, we have submitted two runs with different parameters. In the task of video retrieval, we have submitted the results of 17 topics among all the topics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Filtering</head><p>Our research on filtering focuses on how to create the initial filtering profile and set the initial threshold, and then modify them adaptively. In this section, detailed introduction to the training and adaptation module of our adaptive runs is first presented. Then we introduced our batch runs briefly. Final part presents the experiment results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Adaptive filtering</head><p>Figure <ref type="figure" coords="1,123.28,509.03,4.39,9.50">1</ref>.1 shows the architecture of the training in adaptive filtering. At first, feature vectors are extracted from positive and pseudo-positive document samples. The initial profile is the weighted sum of positive and pseudo-positive feature vectors. Then we compute the similarity between the initial profile and all the training documents to find the optimal initial threshold for every topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.1">Feature selection</head><p>Since the total number of all words is very large and it costs much time in similarity computation, we decide to select some important words from them. First, we carry out morphological analysis and stopword removing. Then we compute the logarithm Mutual Information between remaining words and topics:</p><formula xml:id="formula_0" coords="1,181.72,623.82,309.49,36.52">( ) ( )       = i j i j i w P T w P T w MI | log ) , ( log (1.1)</formula><p>Where, w i is the ith word and T j is the jth topic. Higher logarithm Mutual Information means w i and T j are more relevant. P(w i ) and P(w i |T j ) are both estimated by maximal likelihood method.</p><p>For each topic, we select those words with logarithm Mutual Information higher than 3.0 and occurs more than once in the relevant documents. Logarithm Mutual Information is not only used as the selection criterion, but also as the weight of feature words. Where, p j is the profile of the jth topic and d i is the vector representation of the ith document. d ik , the weight of the kth word in d i , is computed as such:</p><formula xml:id="formula_1" coords="2,292.36,476.79,128.67,16.61">) log( 1 dl avdl tf d ik ik * + =</formula><p>, where tf ik is the frequency of the kth word in the ith document , dl is the average number of different tokens in one document, avdl is the average number of tokens in one document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.3">Creating initial profile and Setting initial threshold</head><p>Each topic profile is represented by a vector which is the weighted sum of feature vector from positive (relevant) documents and feature vector from pseudo relevant documents with the ratio of 1: X 0 .</p><p>To make use of the hierarchy of categories, those documents of the same high-level category are considered as pseudo relevant documents. Since the number of low-level categories is different among different high-level categories, we set different X 0 for different categories. In our experiment, X 0 is set to 0.03 for the topics from R1 to R79, and 0.15 for R80~R84. After combining the positive and pseudo-positive feature vectors, we get the initial profile. Once the initial profiles are acquired, the initial thresholds should be set to those values that can result in the largest value of T10U or T10F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.4">Adaptation of threshold and topic profile during filtering</head><p>For adaptive filtering, we adopt an adaptation procedure to modify the initial profile and threshold while filtering documents. Figure <ref type="figure" coords="2,189.07,703.19,4.39,9.50">1</ref>.2 shows the architecture for the adaptation: </p><formula xml:id="formula_2" coords="3,219.28,573.84,173.85,24.44">) ( ) ( ) ( ) ( ) , ( 1 1 1 k k k R k R k k t n t n t n t n t t P - - = + + +</formula><p>Intuitionally, we should increase the threshold if the precision is too low and lower the threshold if too few documents are retrieved. So we can use S -( t k+1 , t k ) and P(t k+1 , t k ) to decide whether to increase or decrease the threshold. When the precision is lower than expected, we should increase the threshold. Otherwise, we can decrease the threshold. In particular, when the threshold is too higher than the similarity with the rejected documents, the threshold should be decreased quickly. The above strategy of threshold adjusting can be written as below: </p><formula xml:id="formula_3" coords="3,93.04,688.32,111.40,17.17">If ) ( ) , ( 1 1 + + ≤ k k k t EP t t p</formula><formula xml:id="formula_4" coords="4,93.04,73.71,248.19,89.20">)) ( 1 ( ) ( ) ( ) ( 1 1 k k k k t T t t T t T - • + = + + α Else If S -( t k ,t k+1 ) &lt; T(t k+1 ) * D then ) 1 ( ) , ( ) ( ) ( 1 1 A t t S A t T t T k k k k - • + • = + - + Else ) ( )) ( 1 ( ) ( 1 1 k k k t T t t T * - = + + β Where ) ( k t α</formula><p>is the coefficient for increasing the threshold, and ) ( k t β is the coefficient for decreasing the threshold, both of them can be considered as the function of n R (t). In our experiment, we use the following linear functions shown in equation 1.3. . The introduction of parameter D aims at increasing the recall. Since the actual number of relevant documents of every topic cannot be observed, we can only acquire some indirect estimation. We believed when the average similarity between the profile and those rejected documents are too small, the similarity threshold should be decreased in order to enhance the recall. In our experiment, we set D = 0.1 and A = 0.8. EP(t k ) means the precision which we wish the system to reach. At first, we regarded this parameter as constant and tried several different values, but the results are not very satisfactory. Since it is impractical to require the system to reach the desired high precision at the beginning of filtering, we adopt a gradual-ascent function. The function is showed in equation 1.4.</p><formula xml:id="formula_5" coords="4,72.88,202.70,431.26,36.70">   &gt; ≤ - * = µ µ µ µ α α ) ( 0, ) ( , )) ( ( ) ( 0 k R k R k R k t n t n t n t ,    &gt; ≤ - * = µ µ µ µ β β ) ( , 0 ) ( , )) ( ( ) ( 0 k R k R k R k t n t n t n t (1.</formula><formula xml:id="formula_6" coords="4,179.20,415.46,333.01,36.70">   &gt; ≤ * - + = + + µ µ µ ) ( 0, ) ( , ) ( ) ( ) ( 1 0 0 1 k R k R k R final k t n t n t n P P P t EP (1.4)</formula><p>Where, P 0 and P final are the desired initial and final precision. In our experiment, P 0 = 0.2 and P final = 0.6.</p><p>(2) Adaptation of profile Once a retrieved document has been judged relevant, it is added to the positive document set otherwise it is added to the negative document set. During profile adaptation, feature vectors are extracted from positive documents and negative documents. The new topic profile is the weighted sum of feature vector from positive documents and negative documents with the ratio of 1:X 1 (Here X 1 = -0.25). For effectiveness and efficiency reason, we adjust the topic profile only after L(L = 5) positive documents have been retrieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Batch filtering</head><p>Since this year's batch filtering task does not include batch-adaptive task, there should be no adaptation in the batch-filtering sub-task. Therefore, the profile and threshold acquired from training should remain the same during filtering.</p><p>There is only a slight difference in the initialization module of our batch and adaptive runs. Full relevance judgments are provided in batch filtering. As a result, for batch run, the given relevant judgments are enough for us to build the initial profile, so pseudo-relevant documents are not used in profile creation. In addition, we adopt the stratified tenfold cross-validation method to avoid the phenomenon of overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Evaluation results</head><p>This year Fudan University has submitted four runs for adaptive and batch filtering. We submit no routing runs. Table <ref type="table" coords="5,125.69,120.47,4.39,9.50" target="#tab_1">1</ref>.1 summarizes our adaptive and batch filtering runs. Four evaluation criteria are calculated, including T10SU, T10F, Set Precision and Set Recall. Underlined value means that the run is optimized for the corresponding criterion. The last columns give the number of topics in which our runs perform better, equal and worse than median ones according to the criteria for which our runs are optimized. From this table we can find that our adaptive runs perform better than median for most of the topics, while our batch runs do not perform as well. Although our batch runs performs better than adaptive runs, the divergence is not very significant. It helps to show that adaptation plays a very important role in filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Question Answering</head><p>It is the second time that we take part in the QA track. We tried many natural language processing techniques, and incorporated many sources of world-knowledge. A novel question answering technique, known as "syntactic constrained semantic verification", has been put forward. In next section, we will describe the architecture of our QA system, followed by a detailed discussion of the main components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Overview of QA system</head><p>Our system contains four major modules, namely question processing module, offline indexing module, online searching and concept filtering module, as well as answer processing module. The online models are represented in Figure <ref type="figure" coords="5,163.91,506.87,3.95,9.50">2</ref>.1.</p><p>Our indexing module creates full-text index for the document collection. However, it is quite different from traditional indexing procedure in that it incorporates several NLP techniques not only to avoid errors due to traditional stemming process, but also to increase both the precision and recall while retrieving proper name.</p><p>Question processing module tries to interpret the meaning of the input question by identifying answer type (the kind of information the question requires) from the question type, and extracting keywords. Next, the searching and filtering module use only non-replaceable keywords to retrieve relevant paragraph. After obtaining the result paragraphs, we use a concept thesaurus to filter and rank those paragraphs according to the number of occurring concepts, which are mainly derived from those replaceable keywords.</p><p>In the Answer Processing module, we use a dependency parser to analyze sentences in which the answer may lie in. Finally, a novel semantic verification scheme is applied after a WordNet-based concept tagging and a name entity tagging are completed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Documents Question</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Document Indexing</head><p>Our document indexing module actually includes two separate indices, i.e. a morphological analysis based full-text index and a proper name index. However, before we build these indexes, a sentence breaking module is applied to get correct sentence boundaries. We use a free sentence breaking tool, based on maximum entropy model, from Adwait Ratnaparkhi's web site. Proper name indexing is carried out to accelerate the online question processing speed. Our proper name tagging module depends heavily on a maximum entropy model based NP chunker. After reading each sentence with its part of speech tag (POS) for every word, it outputs NP chunks for the sentence. Figure <ref type="figure" coords="6,472.16,510.23,4.39,9.50">2</ref>.2 presents one sample sentence after NP chunking. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Question Processing</head><p>The goal for question processing module is to find the user's information needs by examining the question. The query and expected answer type are transformed from every original natural language question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Query Formation</head><p>First, considering synonyms, we define two kinds of keywords, i.e. the replaceable keywords and non-replaceable keywords. The replaceable keywords are referred to those words that could be replaced by other synonym without altering the information request of the question. Only the non-replaceable keywords are transformed into query. The documents returned by search engine will consist of all the candidate segments. Further, those candidates irrelevant to the question will be filtered out by replaceable keywords and their synonyms. POS tagging and NP chunking are carried out to segment each question into segments. After that, we apply a HMM based Name Entity Identification tool to extract the non-replaceable keywords. It can recognize six kinds entity name, including people's name, place name, organization name, time, date and miscellaneous number, from normal NP phrases.</p><p>The NP phrases identified by Name Entity Identification module are regarded as the non-replaceable keywords and then submitted to the search engine, while other components are treated as the replaceable keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Answer Type Concept Identification</head><p>Another task for Question Processing module is to determine the desired answer type concepts. First, we roughly classify 10 question types according to the question interrogatives, as shown in Figure <ref type="figure" coords="7,484.09,270.70,3.95,9.50">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Search and Filtering Module</head><p>We employ the Boolean retrieval engine to find candidate answer paragraphs. We modify the search query to avoid returning too many and too few paragraphs. If too many paragraphs are retrieved, more keywords, such as replaceable keywords, will be added to restrict the number of candidate paragraphs. Otherwise, some of the key phrase will be removed.</p><p>After the paragraphs are retrieved, additional lexicon knowledge (Moby electronic thesaurus) is used to filter out irrelevant ones and sort remaining paragraphs according to the number of the words which appear in the question.</p><p>Moby electronic thesaurus contains about 1,000 concepts and each concept includes several words with similar word meaning [Moby00]. First, the replaceable keywords for each question are matched against the thesaurus to find one or more relevant concepts. Next, if the correspondent concept is found, the candidate paragraphs will be examined to find out the number of the words under the same concept. These words will be called extended query keywords (EQKs). Their number, which reflects the semantic closeness between a question and every paragraph, will be used to sort the paragraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Answer Processing Module</head><p>We put forward a new approach in the Answer Processing module, which is named as "syntax constrained semantics verification". The Answer Processing Module aims to determine and extract answers from the candidate paragraphs retrieved by the Search and Filtering module. Figure <ref type="figure" coords="8,401.39,196.43,4.39,9.50">2</ref>.5 gives the framework of this module.</p><p>Firstly, we determine the answer type of every noun word and noun phrase in candidate paragraphs by Name Entity Tagging, which has been described before. The words whose answer types correspond to the Question Type are marked candidate answer.</p><p>Then the candidate paragraphs are passed through a dependency parser, Minipar <ref type="bibr" coords="8,434.08,267.57,26.56,8.10" target="#b5">[Lin98]</ref>, to get the parsing tree. In this dependency tree, every node corresponds to a syntax category and every word in the candidate paragraph resides in a node. The children of a node are those words that modify it. We try to find a path in the parsing tree connecting EQKs and candidate answer. If there exist such path, we extract the words on the path and the children of them in the parsing tree. Thus we get different word groups for each candidate answers.</p><p>Here we assume that these word groups are more semantically closer to the corresponding candidate answer they extracted from than other words in the same sentence. Now we have a word group for each candidate answer. We firstly extract the content words (noun, verb, adjective and adverb words) from question to form another word group. Both word groups are considered to be relative to the focus words in question and answers. Then we compute their semantic similarity using a approach named extended vector space model. The result of this step is a similarity score which varies from zero to one. This is the basic factor to determine the final answer. For each candidate answer, a 50 byte-long section in the candidate paragraph, named answer-windows, is then created. This answer-window is centralized by the candidate answer. We evaluate each answer-window using the following three scores:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question</head><p>Semantic similarity: This score has been computed using extended vector space model. Syntax pattern score: This score is based on the candidate answer sentence's syntactic structures. It takes several syntactic features into consideration, such as the length of the path in the parsing tree between the answer keywords and candidate answer, POS of EQKs and candidate answer. Indicators score: Some phrases or words, such as 'known as', 'called', 'named as' and some syntax features, such as appositive, strongly indicate a possible answer to specified questions such as 'who', 'what', 'which'. The final score is a linear combination of all of these scores, where the weight of every score depends on the question feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Experiment results</head><p>This year we only take part in the main task of QA, and only submit one 50-byte run. Our results are not satisfactory. Statistics over 492 questions shows the strict mean reciprocal rank of 0.137 and lenient mean reciprocal rank of 0.145. Almost 80% of the questions return no correct response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Web Retrieval</head><p>This year we attend the TREC-10 web subtask for the first time. We submit four runs for the web track. We used different combination of information in the four runs: title only and content only (fdut10wtc01), title with description and narrative information and content only (fdut10wac01), title only with link information (fdut10wtl01) and all title, description, narrative with link information (fdut10wal01).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">System Architecture</head><p>In order to get better performance on web information retrieval, we have modified most of our original search engine, which is based on statistical model, and make a new search kernel that is based on extended Boolean search. Moreover, we split the document set and indices into several parts to efficiently handle the corpus of WT10g which contains 10G HTML documents.</p><p>Figure <ref type="figure" coords="9,126.39,500.15,4.39,9.50">3</ref>.1 shows the architecture of our web retrieval system. The first part in the left side is preprocessing module which can turn HTML pages into plain text. The second module is a preparation part for indexing, it combine all the small HTML documents in one directory of WT10g into a single file. The next step is indexing. We use stopword removing and morphology analysis to select entries of the indexing lexicon. What's more, we do not create index for the whole WT10g corpus due to the huge size of the corpus. Instead, the corpus is split into smaller parts, each of which is to be indexed independently. By this means, we can control the indices more easily than simply creating a larger index of the whole corpus.</p><p>On the right side, the first step transfers the queries into several words, recognizes the phrase, and does some query expansion. The second module searches the index with the algorithm below and builds the ranked relevant document set. In the third step, link information is exploited to reorder the relevant documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Search Algorithm</head><p>The core algorithm is based on an extended Boolean retrieval engine called "short matching passages" <ref type="bibr" coords="9,72.04,691.77,49.70,8.10" target="#b3">[Kleinberg98]</ref>, which intends to find the shortest passage that matching certain words. The assumption is that the shorter the passage is, the more possible that it will be relevant with the query. The following equation shows the basic idea of this algorithm. The I(p,q) represents the intensity of a shortest passage from the pth to qth word which contains certain keywords. </p><formula xml:id="formula_7" coords="10,158.32,107.77,335.68,50.39">     ≥ +         + - = otherwise 1, K 1 p - q if , 1 ) , ( a p q K q p I (3.1)</formula><formula xml:id="formula_8" coords="10,153.88,596.64,346.12,26.09">i i i D qi pi i q p w q p I D S ∑ ∈ = (3.2)</formula><p>Together with the length of short passage, we also consider the position of them (which is calculated in w1 in the equation above) to calculate the score of the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Reordering with link information</head><p>In order to improve Retrieving result by link information, we have tested Kleinberg Algorithm of hubs and authorities <ref type="bibr" coords="10,137.32,709.17,49.81,8.10" target="#b3">[Kleinberg98]</ref>, co-citation and bibliographic coupling <ref type="bibr" coords="10,358.24,709.17,36.55,8.10" target="#b4">[Kraaij99]</ref>. After some experiments, we find that co-citation and bibliographic coupling can lead to the batter result. However, our best result still shows that link information cannot improve the search result.</p><p>The basic principle of co-citation is that if two documents (document A and B) cite the same documents, then document A and B are similar to some degree. The basic principle of bibliographic coupling is that if many documents cite both document A and B, then document A and document B are similar to some degree.</p><p>In our experiment, we use the formula of Wessel Kraaij [Kraaij99]: </p><formula xml:id="formula_9" coords="11,94.48,163.62,405.76,78.51">) ( # ) ( ) ( ) ( d inlinks i relevancy d Inlinkrel d inlinks i ∑ ∈ = , ) ( # ) ( ) ( ) ( d outlinks i relevancy d Outlinkrel d outlinks i ∑ ∈ = ) ( # ) ( ) ( ) ( d inlinks i Outlinkrel d Cociterel d inlinks i ∑ ∈ = , ) ( # ) ( ) ( ) (</formula><formula xml:id="formula_10" coords="11,72.04,280.27,435.01,55.45">NewScore(d)= 1 α * S(D)+ 2 α *InlinkRel(d)+ 3 α *OutlinkRel(d) + 4 α *CociteRel(d)+ 5 α * Bibcouple(d) (3.3) Where, 1 α , 2 α , 3 α , 4 α , 5</formula><p>α are five parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Experiment Results</head><p>We submitted four runs, whose names are fdut10wtc01, fdut10wtl01, fdut10wac01, fdut10wal01. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Video Track</head><p>On Video Track, we take part in both Shot Boundary Detection and Video Retrieval. In the task of Shot Boundary Detection, we have submitted two runs with different parameters. One of them is precision-orientied, and another is recall-orientied. In the task of Video Retrieval, we submitted the results of 17 topics out of 74.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Shot Segmentation</head><p>In our system, we use FFD (Frame-to-Frame Difference) <ref type="bibr" coords="11,338.32,680.13,45.49,8.10" target="#b0">[Hanjalic97]</ref> to detect the shot boundary. But we redefined the difference between the nth frame and (n+k)th frame as Equation 4.1.  θ , it may be caused by Cut Shot Boundary from frame n to frame n+1. We have also trying to reduce the influence of flashlight by compare the frames of both sides. When flashlight comes, it can be assumed that there will be more than one frame whose ) ( 1 n Z is larger than C θ , also, the frames before and after the flashlight are similar. The Gradual shot boundary cannot be easily detected by the FFD of two continuous frames. We use ) (n Z k (k=50) to magnify the frame-to-frame difference. But, it will cause more false alerts by motion in shot. To reduce that, we use motion detection: a sequence of continuous frames whose FFD is larger than G θ will be labeled as Gradual Shot Boundary only if no efficient camera motion is detected and the frames before and after the sequence are dissimilar. Other parameters are adjusted manually based on the 42 training video clips. The results show that the parameter selection is not sensitive in our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>∑∑</head><formula xml:id="formula_11" coords="12,160.24,141.62,351.97,56.82">- + = i j k n j i I k n j i I C n D | ) , , ( ) , , ( | ) ( 1 (4.2) ∑ + - = i k n i H k n i H C n Y )) , ( ), ,<label>( min( 1 )</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Video Retrieval</head><p>We submitted the results of 17 topics in video retrieval. Table <ref type="table" coords="12,355.97,561.59,4.39,9.50">4</ref>.1 shows the type of these topics. In Section 4.2.1, we will describe the architecture of our video retrieval system. Section 4.2.2 is the detailed description about implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">System Architecture</head><p>Our Video Retrieval System includes two parts. One is the off-line Indexing Sub-system, and another is on-line Searching Sub-system. Figure <ref type="figure" coords="13,232.78,151.67,4.39,9.50">4</ref>.1 describes the system architecture. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.1">Qualitative Camera Motion Analysis</head><p>In our system, we analyze Camera Motion by the Motion Vectors obtained from MPEG stream. Each motion is composed of Motion Amplitude and Motion Direction. The system tries to segment shots into sub-shots automatically. We define sub-shot as some continuous frames in one shot with the similar camera motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.2">Face Detection</head><p>Our method is designed mainly for interviewee detection. It has features: (1) The face is quite large, (2) the face has motion but the background is still. The method consists of three steps: Skin-Color based </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,200.80,98.61,210.74,8.10"><head>Figure 1 . 1</head><label>11</label><figDesc>Figure 1.1 Architecture of the training in adaptive filtering</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,504.14,217.19,8.07,9.50;4,93.04,251.99,28.10,9.50;4,135.40,257.41,3.50,6.31;4,127.12,246.50,7.58,16.38;4,146.56,251.99,15.24,9.50;4,176.44,257.41,3.50,6.31;4,169.12,246.50,6.59,16.38;4,181.96,251.99,189.07,9.50;4,378.28,246.55,6.89,16.32;4,388.24,251.99,152.10,9.50;4,72.04,269.99,416.76,9.50;4,523.84,269.00,12.01,10.81;4,520.84,269.00,3.00,10.81;4,514.84,269.00,6.01,10.81;4,497.68,275.41,3.50,6.31;4,505.36,265.07,6.59,15.65;4,489.40,264.50,7.58,16.38;4,537.52,269.99,2.64,9.50;4,108.16,287.00,6.01,10.81;4,105.16,287.00,3.00,10.81;4,99.16,287.00,6.01,10.81;4,81.52,293.41,3.50,6.31;4,89.56,283.07,6.59,15.65;4,74.20,282.50,6.59,16.38;4,115.00,287.99,15.24,9.50;4,152.80,287.02,17.96,10.77;4,143.20,283.11,6.57,15.60;4,132.28,282.55,6.89,16.32"><head></head><label></label><figDesc>β are the initial parameter. The parameter of µ indicates the maximum number of positive documents should be used to adjust the threshold and modify the profile. Here we set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,212.32,171.09,187.53,8.10"><head>Figure 2 . 1</head><label>21</label><figDesc>Figure 2.1 Overview of our QA system (online part)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,224.08,539.13,163.88,8.10"><head>Figure 2 . 2</head><label>22</label><figDesc>Figure 2.2 The output of NP chunking toolkit</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="6,299.20,216.45,53.96,8.10;6,299.20,231.93,68.36,8.10;6,313.60,282.57,65.86,8.10;6,313.60,298.17,52.73,8.10;6,313.60,337.17,36.54,8.10;6,313.60,352.77,63.57,8.10;6,304.60,399.57,76.01,8.10;6,421.60,399.57,27.18,8.10;6,466.24,203.63,43.94,9.50;6,475.60,298.17,30.11,8.10;6,475.60,313.77,29.22,8.10;6,448.60,352.77,47.94,8.10;6,448.60,368.37,29.22,8.10;6,412.60,227.97,33.46,8.10;6,461.87,227.97,43.60,8.10;6,412.60,243.57,78.12,8.10;6,124.96,453.81,74.13,8.10;6,281.68,453.81,84.83,8.10;6,415.48,453.81,69.17,8.10;6,90.64,559.89,430.52,8.10;6,90.64,575.37,396.22,8.10"><head>Full</head><label></label><figDesc>] , [ spokesman ] for [ the District ] of [ Columbia police ] , said [ street crime ] in [ Washington ] has increased in [ recent years ] , but [ there ] have been [ few reports ] of [ assaults ] near [ the Capitol grounds. ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="10,223.00,182.73,184.39,8.10"><head>Figure</head><label></label><figDesc>Figure 3.1 Architecture of our web retrieval system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="11,464.20,231.32,6.01,10.81;11,421.60,231.32,38.04,10.81;11,492.28,212.96,3.34,10.81;11,446.08,212.96,41.37,10.81;11,364.12,221.96,6.01,10.81;11,355.60,221.96,3.34,10.81;11,302.32,221.96,53.38,10.81;11,436.72,219.37,3.50,6.31;11,411.16,219.37,22.19,6.31;11,405.16,219.37,1.95,6.31;11,389.68,210.42,12.83,18.00;11,406.84,217.08,5.00,9.13;11,378.88,218.03,6.59,15.65;11,114.04,253.91,426.26,9.50;11,72.04,269.51,330.98,9.50"><head></head><label></label><figDesc>the dth document, we have five scores: S(D), InlinkRel(d), OutlinkRel(d), CociteRel(d),BibcoupleRel(d). We can use the following formula to calculate the news core:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="12,93.64,424.79,213.87,9.50;12,320.68,430.23,4.66,6.29;12,313.72,419.28,6.27,16.40;12,333.64,424.79,15.24,9.50;12,361.96,430.23,5.05,6.29;12,355.12,419.28,6.27,16.40;12,375.04,424.79,165.16,9.50;12,72.04,442.79,285.21,9.50;12,387.28,441.78,4.01,10.83;12,376.36,441.78,4.01,10.83;12,371.92,448.23,3.50,6.29;12,380.80,441.78,6.01,10.83;12,364.96,441.78,6.69,10.83;12,398.56,442.79,15.24,9.50;12,444.88,441.78,4.01,10.83;12,433.96,441.78,10.33,10.83;12,421.36,441.78,6.69,10.83;12,428.92,448.23,3.10,6.29;12,451.72,442.79,88.46,9.50;12,72.04,460.79,53.78,9.50;12,155.56,459.78,4.01,10.83;12,144.64,459.78,4.01,10.83;12,140.20,466.23,3.50,6.29;12,149.08,459.78,6.01,10.83;12,133.24,459.78,6.69,10.83;12,166.48,460.79,15.24,9.50;12,212.44,459.78,4.01,10.83;12,201.52,459.78,10.33,10.83;12,188.92,459.78,6.69,10.83;12,196.48,466.23,3.10,6.29;12,218.80,460.79,321.26,9.50;12,72.04,478.79,41.79,9.50"><head></head><label></label><figDesc>During the Shot Boundary Detection, threshold C θ and G θ are selected automatically every 500 frames<ref type="bibr" coords="12,106.12,443.85,28.35,8.10" target="#b6">[Zhu00]</ref>. The selection is according to the histogram of calculated, and we find the first low point p. The value on p will be the threshold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="13,218.80,549.93,174.61,8.10"><head>Figure 4 . 1</head><label>41</label><figDesc>Figure 4.1 System architecture of video retrieval</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,109.96,183.81,393.45,104.70"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="5,407.44,183.81,91.52,8.10"><row><cell>Comparison with median</cell></row></table><note coords="5,250.77,280.41,137.06,8.10"><p>.1 Adaptive and batch filtering results</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,72.04,270.70,468.43,279.65"><head></head><label></label><figDesc>.3. Next, 32 answer type concepts are introduced into our system, illustrated in figure 2.4. Among them, six are identified by Name Entity Identification tool, i.e. DATE, LOCATION, MONEY_UNIT, ORGANIZATION, PERCENTAGE and PERSON, while other concepts correspond to some synset in the WordNet noun hierarchies.</figDesc><table coords="7,154.60,347.49,301.45,202.86"><row><cell></cell><cell cols="2">Figure 2.3 Question types</cell><cell></cell></row><row><cell></cell><cell>What</cell><cell>Who/whom</cell><cell>Where</cell></row><row><cell></cell><cell>How</cell><cell>why</cell><cell>how much</cell></row><row><cell></cell><cell>When</cell><cell>which</cell><cell>Name</cell></row><row><cell></cell><cell cols="2">Figure 2.4 Answer type concepts</cell><cell></cell></row><row><cell>ACTIVITY</cell><cell>COLOR</cell><cell cols="2">LINEAR_AMOUNT</cell><cell>PLANT</cell></row><row><cell>AMOUNT</cell><cell>COUNTRY</cell><cell>LOCATION</cell><cell cols="2">PRODUCT</cell></row><row><cell>ANIMAL</cell><cell>DATE</cell><cell>MONEY</cell><cell cols="2">PROVINCE</cell></row><row><cell>ARTIFACT</cell><cell>DEF</cell><cell>MONEY_UNIT</cell><cell></cell><cell>STAR</cell></row><row><cell>BODYPART</cell><cell>DISEASE</cell><cell>ORGANIZATION</cell><cell cols="2">TEMPERATURE</cell></row><row><cell>CAREER</cell><cell>ELEMENT</cell><cell>OTHER</cell><cell></cell><cell>TIME</cell></row><row><cell>CD</cell><cell>FOOD</cell><cell>PERCENTAGE</cell><cell></cell><cell>WEIGHT</cell></row><row><cell>CITY</cell><cell>LANGUAGE</cell><cell>PERSON</cell><cell></cell><cell>WORD</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,72.04,369.59,468.31,182.18"><head></head><label></label><figDesc>The final result is shown in the following table.</figDesc><table coords="11,72.04,401.73,468.31,150.04"><row><cell>Runs</cell><cell>Type</cell><cell>Average Precision</cell><cell>R-Precision</cell></row><row><cell cols="2">Fdut10wtc01 Title-only/content-only</cell><cell>0.1661</cell><cell>0.2061</cell></row><row><cell>Fdut10wtl01</cell><cell>Title-only/content + link</cell><cell>0.1544</cell><cell>0.1939</cell></row><row><cell cols="2">Fdut10wac01 Title + Description + Narritive/content-only</cell><cell>0.1661</cell><cell>0.2061</cell></row><row><cell cols="2">Fdut10wal01 Title + Description + Narritive/content + link</cell><cell>0.1248</cell><cell>0.1607</cell></row><row><cell></cell><cell cols="2">Table 3.1 Results of web track</cell><cell></cell></row><row><cell cols="4">We used many different combinations of parameters to reordering the content-only result, and find finally</cell></row><row><cell cols="4">that 1 α =1, 2 α =0, 3 α =0, 4 α =1, 5 α =0 can lead to the best result. But it still does not improve the</cell></row><row><cell>content-only score.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="12,72.04,177.20,468.28,115.32"><head></head><label></label><figDesc>is the Color Histogram value of frame n on the ith bin.</figDesc><table coords="12,72.04,177.20,468.28,115.32"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(</cell><cell>2</cell><cell>(4.3)</cell></row><row><cell></cell><cell cols="2">Where,</cell><cell>I</cell><cell>(</cell><cell cols="2">i</cell><cell>,</cell><cell>j</cell><cell cols="2">,</cell><cell>n</cell><cell>)</cell><cell>is the average luminance of block (i, j) in frame n, and each block has 8*8 pixels.</cell></row><row><cell>H</cell><cell>, ( n i</cell><cell>)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">We use</cell><cell cols="3">Z</cell><cell cols="4">( 1 n</cell><cell>)</cell><cell>to detect the Cut Shot Boundary and</cell><cell>Z k</cell><cell>(n</cell><cell>)</cell><cell>to detect the Gradual Shot Boundary. C θ</cell></row><row><cell cols="11">and G θ are thresholds for cut and gradual shot boundary. They will be discussed in next paragraph. If</cell><cell>Z</cell><cell>( 1 n</cell><cell>)</cell></row><row><cell cols="11">exceed the threshold C</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This research was partly supported by <rs type="funder">NSF of China</rs> under contracts of 69873011 and 69935010. We are thankful to <rs type="person">Lin Mei</rs>, <rs type="person">Yuefei Guo</rs>, <rs type="person">Weixiang Zhao Yi Zheng</rs>, <rs type="person">Yaqian Zhou</rs>, <rs type="person">Kaijiang Chen</rs>, <rs type="person">Xiaoye Lu</rs>, <rs type="person">Jie Xi</rs>, <rs type="person">He Ren</rs>, <rs type="person">Li Lian</rs>, <rs type="person">Wei Qian</rs>, <rs type="person">Hua Wan</rs> and <rs type="person">Tian Hu</rs> for their help in the implementation.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Segmentation, Motion Segmentation, and Shape Filtering.</p><p>In skin-color based segmentation, we use several skin-color filters in different color spaces and combine them by AND operation. It is found that the result is better than the ones in any single color space. After that we have several skin-color regions. Similarly, we can have several motion regions by motion segmentation. By INTERSECTION operation, we have those regions which have both skin-color and motion. They are the candidates of face. For these candidates we use shape filtering to remove those too small or irregular ones and get the final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.3">Face Recognition</head><p>In the training phase, we normalize all the training samples to 40*40 and make histogram equalization. After that, we clustering some of these samples and transform the face images to column vectors. Then:</p><p>(1) Let (2) Calculate the zero subspace of the within-class matrix w S .</p><p>(3) Calculate the eigenfaces in subspace ( ) </p><p>where,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(</head><p>) . Then the projection feature vectors of two images on the eigenface space can be obtained as Equation 4.5:</p><p>, then we think that the Testing Face and the Example Face belong to same person. (δ is a threshold)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.4">Video Text Detection and Recognition</head><p>There are three main parts in our Video Text Detection System: Text Block Detection, Text Enhancement and Binarization. To reduce the processing time, the system processes only one frame in every ten. On each processed frame, text block detection is first applied to get the position of each possible text line. This detection is based on edge image. Since edges are not sensitive to intensity changes and edges are dense in text line blocks, we calculate the gray scale edges in RGB space horizontally. The edge images are then binarized and run length analysis is applied to find candidate text blocks.</p><p>We use SSD (Sum of Square Difference) based block matching to track the detected text lines. All tracked text blocks are interpolated to a reasonable fixed size and then registered. At last, the tracked, interpolated and registered text blocks are combined by an average operation to reduce the noise and suppress the complex background.</p><p>An improved logical level technique (ILLT) is developed to binarize each candidate text block. This method can deal with different intensities of characters (i.e. characters may be brighter or darker than background) efficiently.</p><p>We have used commercial software: TextBridge Pro Millennium to recognize the binarized text block image. At last, the recognized string of each text blocks will be split into words and those words that are too short (less than 3) are removed in the filtering step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.5">Speaker Recognition and Speaker Clustering</head><p>During the Indexing Phase, Speaker Clustering can ensure the clustered shots that include human face are from the same person. And during the Searching Phase, Speaker Recognition can ensure the audio contained in the returned results is the same as the audio examples.</p><p>We use Vector Quantization (VQ) and Gaussian Mixture Model (GMM) to characterize each speaker. In these two methods, VQ worked faster than GMM while GMM performed better than VQ in some cases. In addition, in order to remedy the disturbance of noise and other backgrounds, a global model was used to modify the output of single speaker model.</p><p>Both VQ and GMM, especially after being integrated with global model, showed satisfactory detection and clustering effect if the speeches have the similar backgrounds. But if there is strong music background in the speech, the result will be worse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.6">Automatic Speech Recognition</head><p>We have used the Speech SDK of Microsoft as Automatic Speech Recognition (ASR) engine [www.Microsoft.com/speech]. There're different parameter sets for male and female speaker. One has to choose a proper parameter set to get better performance. And the engine has capability of background adaptation.</p><p>To increase the recall, we did not use gender detection. Instead, we use different parameter sets to recognize the same piece and give the confidence and time alignment of every recognized word. We have tried speaker change detection and audio classification. However, there is little improvement. The main reason is that the background music is too strong. Nevertheless, it recognized most of the keywords correctly and we use it for Topic Detection.</p><p>For the NIST video, we find that there are some errors in the human transcripts. We use ASR engine to give the result with time alignment. Then the result is aligned with the human transcript by ASR evaluation program. Finally, we get the rough time alignment between the audio and the human transcript. This time alignment is used for Question Answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.7">Gender Detection</head><p>In our system, Gender Detection is made by Gaussian Mixture Model (GMM). We select pieces of audio that contains low background noise or music from the unused videos to train a Male Model and a Female Model. The feature is 12-dimension LPC cepstrum. Each model consists of 128 mixtures. Because the "clean" data of female is not enough, some male speakers are recognized as female. On the other hand, this error is also caused by the background music.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.8">Topic Detection and Question Answering</head><p>In order to make Topic Detection and Question Answering, we should have a document library at first. In our system, the documents of videos are obtained in two ways. One is the manually created information. Another is the transcript created by automatic speech recognition (ASR). After that, Topic Detection and Question Answering module will run. These two modules come from Filtering and Question Answering, which can be found in Section 1 and 2. The training data of Topic Detection comes from the unused videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head><p>Our results of shot boundary detection and video retrieval are presented in the following tables. As for shot boundary detection, our system acquires high performance on cut shot, while the results of gradual shot are not very good. Our performance of know-item search and general search both seems satisfactory. The reason may be attributed to the fact that we only submit the results of 10 know-item search topics and 7 general search topics. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="16,119.71,465.93,412.77,8.10;16,90.04,481.53,374.93,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="16,376.00,465.93,156.48,8.10;16,90.04,481.53,63.84,8.10">Automation of Systems Enabling Search on Stored Video Data</title>
		<author>
			<persName coords=""><forename type="first">Alan</forename><surname>Hanjalic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Macro</forename><surname>Ceccarelli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Reginald</forename><forename type="middle">L</forename><surname>Lagendijk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><surname>Biemand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,157.90,481.53,36.23,8.10;16,235.63,481.53,205.07,8.10">of Storage and Retrieval for Image and Video Database V</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">3022</biblScope>
		</imprint>
	</monogr>
	<note>Proc.SPIE</note>
</biblStruct>

<biblStruct coords="16,131.74,497.13,408.51,8.10;16,90.04,512.73,119.33,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="16,274.24,497.13,144.64,8.10">A Parallel System for Textual Inference</title>
		<author>
			<persName coords=""><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,425.68,497.13,114.57,8.10;16,90.04,512.73,67.59,8.10">IEEE Transactions parallel and distributed systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,133.34,528.33,406.88,8.10;16,90.04,543.93,98.93,8.10" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="16,281.44,528.33,195.04,8.10">FALCON: Boosting Knowledge for Answer Engines</title>
		<author>
			<persName coords=""><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>CSE, Southern Methodist University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="16,125.55,559.53,401.68,8.10;16,90.04,575.13,92.97,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="16,196.00,559.53,188.05,8.10">Authoritative Sources in a Hyperlinked Environment</title>
		<author>
			<persName coords=""><forename type="first">Jon</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,390.41,559.53,136.82,8.10;16,90.04,575.13,72.58,8.10">Proc. 9th ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>9th ACM-SIAM Symposium on Discrete Algorithms</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,111.92,590.73,408.66,8.10" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Kraaij</forename><surname>Wessel</surname></persName>
		</author>
		<title level="m" coord="16,166.27,590.73,349.65,8.10">Thijs Westerveld: TNO/UT at TREC-9: How different are web documents, Proceeding of TREC-9</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="16,103.61,606.33,436.51,8.10;16,90.04,621.93,20.47,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="16,154.12,606.33,261.77,8.10">A Dependency-based Method for Evaluating Broad-Coverage Parsers</title>
		<author>
			<persName coords=""><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,423.64,606.33,112.62,8.10">Natural Language Engineering</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,102.69,653.49,437.43,8.10;16,90.04,668.73,252.31,8.10" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="16,256.36,653.49,253.89,8.10">An Automatic Threshold Detection Method in Video Shot Segmentation</title>
		<author>
			<persName coords=""><forename type="first">Xingquan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lide</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">37</biblScope>
		</imprint>
		<respStmt>
			<orgName>Chinese Journal of Computer Research and Development</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="16,118.79,684.33,421.47,8.10;16,90.04,699.93,367.53,8.10" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="16,464.56,684.33,75.71,8.10;16,90.04,699.93,242.84,8.10">Deriving Very Short Queryies for High Precision and Recall (MultiText Experiments for</title>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">R</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Van Biesbrouck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Clarke</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Proceeding of TREC-7</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
