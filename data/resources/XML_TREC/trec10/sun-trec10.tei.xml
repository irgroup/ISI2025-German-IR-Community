<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,48.48,123.57,514.45,21.11">Aggressive Morphology and Lexical Relations for Query Expansion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,159.48,165.49,68.34,14.66"><forename type="first">W</forename><forename type="middle">A</forename><surname>Woods</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sun Microsystems</orgName>
								<address>
									<addrLine>Laboratories 1 Network Drive Burlington</addrLine>
									<postCode>01803</postCode>
									<settlement>{William.Woods, Stephen.Green, Paul.Martin</settlement>
									<region>MA, Ann</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,269.70,165.49,77.68,14.66"><forename type="first">Stephen</forename><surname>Green</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sun Microsystems</orgName>
								<address>
									<addrLine>Laboratories 1 Network Drive Burlington</addrLine>
									<postCode>01803</postCode>
									<settlement>{William.Woods, Stephen.Green, Paul.Martin</settlement>
									<region>MA, Ann</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,389.14,165.49,63.08,14.66"><forename type="first">Paul</forename><surname>Martin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sun Microsystems</orgName>
								<address>
									<addrLine>Laboratories 1 Network Drive Burlington</addrLine>
									<postCode>01803</postCode>
									<settlement>{William.Woods, Stephen.Green, Paul.Martin</settlement>
									<region>MA, Ann</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,269.88,179.41,72.09,14.66"><forename type="first">Ann</forename><surname>Houston</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sun Microsystems</orgName>
								<address>
									<addrLine>Laboratories 1 Network Drive Burlington</addrLine>
									<postCode>01803</postCode>
									<settlement>{William.Woods, Stephen.Green, Paul.Martin</settlement>
									<region>MA, Ann</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,48.48,123.57,514.45,21.11">Aggressive Morphology and Lexical Relations for Query Expansion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C305E12D5325FC555303ABC096BA2D3B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Our submission to TREC this year is based on a combination of systems. The first is the conceptual indexing and retrieval system that was developed at Sun Microsystems Laboratories <ref type="bibr" coords="1,416.40,316.59,98.81,14.45">( Woods et al., 2000a;</ref><ref type="bibr" coords="1,518.57,317.67,45.04,13.37;1,48.24,330.15,48.11,14.45">Woods et al., 2000b )</ref> . The second is the MultiText system developed at the University of <ref type="bibr" coords="1,425.15,330.15,138.31,14.45">Waterloo ( Clarke et al., 2000;</ref><ref type="bibr" coords="1,48.24,343.71,100.19,14.45" target="#b1">Cormack et al., 2000 )</ref> .</p><p>The conceptual indexing system was designed to help people find specific answers to specific questions in unrestricted text. It uses a combination of syntactic, semantic, and morphological knowledge, together with taxonomic subsumption techniques, to address differences in terminology between a user's queries and the material that may answer them. At indexing time, the system builds a conceptual taxonomy of all the words and phrases in the indexed material. This taxonomy is based on the morphological structure of words, the syntactic structure of phrases, and semantic relations between meanings of words that it knows in its lexicon.</p><p>It was not, however, designed as a question answering system. Our results from last year, while encouraging, showed that we needed more work in the area of question analysis (i.e., "What would constitute an answer to this question?") and answer determination (i.e., "Does this retrieved passage actually answer the question?") to support our relaxation ranking passage retrieval algorithm.</p><p>After conversations with the researchers at the University of Waterloo, we decided to submit a run where we would provide front-end processing consisting of query formulation and query expansion using our automatically derived taxonomy and Waterloo would provide the back-end processing via their MultiText passage retrieval system and their answer selection component. The result is a direct comparison of two question answering systems that differ only in the query formulation component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Conceptual Taxonomy</head><p>As we said earlier, Sun's conceptual indexing system builds a taxonomy of all the words (and possibly phrases) that are encountered during indexing. This taxonomy is built around the generality relationships between terms. More general terms are said to subsume more specific terms. There are three sources of knowledge that are used to build the taxonomy for a set of documents: syntactic structure of phrases, semantic subsumption relationships between words and word senses, and morphological structure and relationships between words. For this experiment, we used only the latter two sources of knowledge to expand terms in the input questions:</p><p>1. Semantic subsumption axioms. These are encoded in the lexicon used by the indexing system. The largest base lexicon currently used by the system system contains semantic subsumption information for something in excess of 15,000 words. This information consists of basic "kind of" and "instance of" information such as the fact that book is a kind of document and washing is a kind of cleaning.</p><p>2. Morphological rules. The current morphology component consists of approximately 1200 knowledgebased morphological rules. These rules cover prefixes, suffixes, lexical compounds, as well as some special cases (e.g., phone numbers).</p><p>The conceptual taxonomy for this experiment is constructed automatically as a byproduct of indexing the TREC material with the conceptual indexing system. As each word is encountered during the indexing process, it is looked up in the system's lexicon and if not found, it is given an entry whose content is determined by the morphological analysis component. This entry is used for any subsequent occurrences of the same term. The rules in the morphological analysis system can infer syntactic parts-of-speech for the word, morphological relationships to other words, and sometimes even semantic relationships to other words.</p><p>After the conceptual indexer has assured that the word has a lexical entry, the word is entered into the conceptual taxonomy if it is not already there. Then all of its root words and any more general concepts that are listed in this lexical entry are also entered into the conceptual taxonomy in the same way, and this word is recorded as being subsumed by those words. This process is carried on recursively so that a word's parents' parents are recorded and so on, until there are no more indirect parents that are not already in the taxonomy.</p><p>Thus the conceptual taxonomy includes all of the terms found in the indexed material, plus all of the more general terms that are known in its lexicon or inferred by morphological rules. Furthermore, the conceptual taxonomy contains only words that were induced by this process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Aggressive Morphology</head><p>The morphological analysis component of this system makes use of a large set of morphological rules that can recognize and analyze words that are derived and inflected forms of known words, as well as words that appear to be derived or inflected forms of unknown words. They can also make plausible inferences about the syntactic categories of unknown words that do not appear to be derived from other words. The morphological analysis system considers both prefixes and suffixes and their interaction, and it also recognizes and analyzes lexical compounds formed from concatenating known words. For example, in the TREC-10 collection, "pointy," "repoint," "repointing," and "standpoint" were analyzed as forms of point."</p><p>The morphological analysis system makes use of different kinds of morphological rules, applied in a preferred order to words that are not already in the lexicon. Generally, the rules are ordered in decreasing order of specificity, confidence and likelihood. Very specific tests are applied in Step 1 to identify and deal with "words" that are not ordinary sequences of alphabetic characters. These include numbers, alphanumeric sequences, and expressions involving special characters. Failing this, an ordered sequence of suffix rules is applied in Step 2 in a first pass that will allow a match only if the proposed root word is "known."</p><p>The same list of rules will be applied later in a second pass without this known-root condition if an earlier analysis does not succeed.</p><p>If no phase-one suffix rules apply, prefix rules are tried in Step 3 to see if an interpretation of this word as a prefix combined with some other "known" word is possible. Failing this, a set of lexical compound rules is tried, in Step 4, to see if the word is interpretable as a compound of two or more words, and failing that, lists of first and last names of people and names of cities are checked in Step 5. All of steps 3-5 are considered more reliable if they succeed than a phase-two pass of the suffix rules without any restriction to known roots that comes in Step 6. This ordering allows prefixes and compounding to be tried before less confident suffix analyses are attempted, and avoids applying weak suffix analyses to known names.</p><p>Lexical compound rules are called by a specialized interpreter that looks for places to divide a word into two pieces of sufficient size. The points of potential decomposition are searched from right to left, and the first such point that has an interpretation is taken, with the following exception: The morph compound analyzer checks for special cases where, for example, the first word is plural and ends in an s, but there is an alternative segmentation in which the singular of the first word is followed by a word starting with the s. In such cases, the decomposition using the singular first word is preferred over the one using the plural. For example, the word minesweeper will be analyzed as mine+sweeper rather than mines+weeper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Interaction of rules with semantic axioms</head><p>It is useful to do full morphology on unknown words and to know morphological relationships for words in the lexicon in order to make connections between derived forms of words and semantic subsumption facts that may be known about their roots. For example, destruction may link morphologically to destroy, which then links semantically to damage. A simple stemming technique would not be able to find such connections (unless all the semantic axioms were similarly stemmed, a process that would result in many false subsumption paths in the taxonomy, due to the kinds of noise and errors that result from stemming algorithms).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Query Formulation and Term Expansion</head><p>Because the taxonomies that the conceptual indexing system builds are specific to a particular document collection, the first step of our query processing was to re-index the TREC Question Answering collection using the latest revision of the conceptual indexer. We then ran the queries through a modified version of the query formulation component of the system we used for TREC-9. The reformulated queries were then passed to the term expansion system.</p><p>The query formulation component is based on the pilot version of our conceptual indexing system. The query formulator interprets the question words and the format of the question to determine the desired answer type. It also either replaces the question word in the query with the desired answer type or simply removes it from the request. In addition, the query formulation component will generalize some terms (e.g., high for tall), substitute base forms for inflected forms (e.g., principle for principles), and drop some "noise" terms.</p><p>The query formulation component that we used this year differs from the one used for TREC-9 in small ways. First, we did not limit the number of terms in the reformulated query as we did last year. This limitation was due to a limitation of our passage retrieval system which is not a problem for the MultiText system. Second, we fixed a limitation to allow a query term to be expanded from each of its roots when there was more than one root for that term (e.g., saw from root see as well as root saw). Finally, we fixed a bug that generated incorrect answer types for certain question forms.</p><p>During a typical query run with the relaxation ranking passage retrieval system, a query term is expanded to include all terms that are subsumed by that term. In some cases the expansion can be quite dramatic. For example, in the AP sub-collection of the TREC QA collection, the query term person expands to more than 17,000 other terms. These terms include morphological variations such as people and persons as well as semantic variations such as blacksmith and lawyer. This set of terms reaches 25 levels deep into the conceptual taxonomy.</p><p>Although such a wide-ranging expansion may seem counter-intuitive, there doesn't seem to be any a priori way to determine a reasonable cutoff level. We can decide on using a small integer, say 2, but this may preclude useful expansions such as counterrevolutionary, which may be crucial in finding just the right document.</p><p>In the end, we decided that we would cut the expansion off after the first level of expansion, since this would get most of the morphological expansions for the term and many of the semantic expansions. Even with this stringent criterion, the query term person still expands to more than 8,000 terms.</p><p>We originally intended to integrate the answer types from our query formulation stage with the answer types from the MultiText system, but that proved not to be possible in the time available, so we ended up providing only the selected query terms and their expansions to the MultiText back end.</p><p>The reformulated, expanded queries were passed to the MultiText system as a conjunction of disjunctions of each of the expanded terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>The results for our two submitted runs as well as the corresponding MultiText runs are shown in table 1. The MultiText system seems to have done better on its own than with the Sun query formulation and expansion engine as the front end. In part, this may be due to the lack of full integration between our front end and the MultiText back end, and in part is may be due to the selection of query terms from the original question that was done by the query formulation stage. It is interesting to note that there are a significant number of questions that each system answered that the other didn't. It is interesting to look at the cases where the combined Sun/MultiText system found answers to questions that were not found by the MultiText system alone, and vice versa. Table <ref type="table" coords="4,93.91,649.47,5.39,13.37" target="#tab_1">2</ref> shows the number of questions for which answers were found by only one of the systems, the number of questions for which answers were found by both systems, and the number of questions for which neither system found an answer. The row labeled "Intersection" shows the number of questions that were found in both of the Sun runs and neither of the MultiText runs, and vice versa. For the rest of the discussion, we will focus on the 26 questions that were found in both of the Sun runs, and neither of the MultiText runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NIST Judgment</head><p>There are two ways that the query expansions can affect whether an answer is found or not. First, the expanded query may give the passage retrieval component enough information to retrieve a passage that would not be found with the unexpanded query. Second, the expanded query may give the answer selection component better information about what words would make up a useful answer to the query.</p><p>A variation of the second type appears to have occurred for question 910, "What metal has the highest melting point?" In both runs, the top passage retrieved by the MultiText system contained the correct answer, but it appears that the answer selection component determined only for the Sun runs that this document held an answer.</p><p>In this case, the Sun query formulation stage transformed the original question to the sequence of terms: (METAL HIGHEST MELT POINT), which expanded to:</p><p>(metal aluminium bimetal chrome copper coppers dimetal gunmetal immetal intermetal lead leads lithium metal's metaled metaler metalic metalist metalled metallic metallist metallize metallized metallizing metallurgy metalor metals metalware nickel nickeled nickeling nickels nonmetal nonmetals palladium polymetal rust rusts silver silvers sliver slivered slivers tin tins) (highest top) (melt melted melter melting melts melty molten re-melt remelt smelt smelted smelting smelts) (point barb barbs breakpoint breakpoints checkpoint checkpoints crosspoint cusp cusps depoint endpoint endpoints gunpoint interpoint isopoint middle middles midpoint midpoints multipoint nib nibs nonpoint outpoint outpointing outpoints point's pointed pointer pointers pointeur pointful pointing pointless points pointy repoint repointing standpoint standpoints subpoint tip tips viewpoint viewpoints wellpoint)</p><p>This example also illustrates a potential negative interaction between our expansions and the MultiText answer selection strategy, since in this case the answer could well be one of the query expansion terms, and could then be rejected by the heuristic of looking for answers that are non-query terms.</p><p>Question 922, "Where is John Wayne airport", shows both effects. The sets of passages that were retrieved are completely disjoint. This query was only slightly expanded, and even though the answer was in passages in both sets, for the MultiText runs, the answer selection component seems to have focussed on other proper names than those in the query. We suspect that this is an effect of the removal of all query words before candidate selection. The query formulation stage transforms this to (JOHN WAYNE AIR-PORT), which expands to:</p><p>(john dejohn demijohn john's johner johni johnie johny saint-john) (wayne dewayne wayne's wayner waynes) (airport airport's airporter airports multiairport)</p><p>With respect to questions that MultiText gets answers for that the joint Sun/MultiText system doesn't, a comparison of the two sets suggests that the joint system does better on questions with more complex descriptions (e.g., 1231 "What fruit is Melba sauce made from?") while the MultiText system does better alone on short direct questions (e.g., 1266 "What are pathogens?"). Another noticable pattern is that many of the questions for which MultiText does better alone contain plurals that the Sun question formulation stage generalizes to singulars (e.g., "Great Lakes" and "x-rays") where the plural is probably a better retrieval clue. These comparisons suggest that there is still a lot of tuning to be done to match the query formulation stage to the retrieval stage and answer selection stage, and that a switch between the two techniques based on the question type would do better than either one alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>We've only done a preliminary analysis of the results at this point. The results show that for some queries, the morphological and semantic expansions help, while for others (unfortunately somewhat more others) they degrade the results. This is typical of the impact of this kind of expansion on many retrieval techniques, with the notable exception of the penalty-based passage retrieval technique used in Sun's conceptual indexing system.</p><p>We haven't yet developed a full picture of how this aggressive expansion might be degrading the retrieved passages of the MultiText system. We have some hope that the MultiText system's passage retrieval method is fairly resistant to degradation, but we will have to further analyze the data to find out if that is the case, and if not, what can be done about it.</p><p>So far, the results above suggest a possible refinement to our front-end/back-end strategy. Since it seems clear that there are instances where the expanded queries aided more in the answer selection than in the passage selection, it would be interesting to try a run where the unexpanded queries are used for passage retrieval and the expanded queries are used during answer selection. Another obvious thing to try is a conditional system that uses the expansion technique for the longer more complex question types and avoids expansion for direct short questions, especially those that look like they are asking for the definition of a term.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,116.16,426.39,379.31,178.25"><head>Table 1 :</head><label>1</label><figDesc>Results for the main QA task.</figDesc><table coords="4,154.44,426.39,20.25,13.37"><row><cell>Run</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,215.64,614.19,180.49,13.37"><head>Table 2 :</head><label>2</label><figDesc>Differences in answers found</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,51.84,491.43,511.28,14.45;6,60.96,506.07,501.65,13.61;6,60.96,519.63,305.16,13.61" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,447.40,492.51,115.71,13.37;6,60.96,506.07,250.51,13.37">Question Answering by Passage Selection (MultiText Experiments for TREC-9</title>
		<author>
			<persName coords=""><forename type="first">Clarke</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,337.13,506.09,225.48,13.59">Proceedings of The Ninth Text REtrieval Conference</title>
		<meeting>The Ninth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="6,48.24,541.11,514.81,14.45;6,60.96,555.75,501.66,13.61;6,60.96,569.31,305.16,13.61" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,488.79,542.19,74.26,13.37;6,60.96,555.75,253.35,13.37">Fast Automatic Passage Ranking (MultiText Experiments for TREC-8)</title>
		<author>
			<persName coords=""><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,334.39,555.77,228.23,13.59">Proceedings of The Eighth Text REtrieval Conference</title>
		<meeting>The Eighth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="6,48.24,590.67,514.93,14.45;6,60.96,605.31,501.92,13.61;6,60.96,618.87,105.15,13.37" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,460.63,591.75,102.53,13.37;6,60.96,605.31,50.88,13.37">Halfway to Question Answering</title>
		<author>
			<persName coords=""><surname>Woods</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,133.79,605.33,228.96,13.59">Proceedings of The Ninth Text REtrieval Conference</title>
		<meeting>The Ninth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="6,48.24,640.35,514.68,14.45;6,60.96,654.99,501.70,13.61;6,60.96,668.55,494.95,13.61" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,515.00,641.43,47.91,13.37;6,60.96,654.99,386.17,13.37">Linguistic Knowledge can Improve Information Retrieval</title>
		<author>
			<persName coords=""><surname>Woods</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,466.61,655.01,96.04,13.59;6,60.96,668.57,179.65,13.59">Sixth Annual Applied Natural Language Processing Conference</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000">2000. 2000</date>
			<biblScope unit="page" from="262" to="267" />
		</imprint>
	</monogr>
	<note>Paul Martin, and Stephen Green</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
