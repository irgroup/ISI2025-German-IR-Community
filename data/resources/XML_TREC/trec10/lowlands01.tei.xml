<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.64,102.09,466.75,15.11">Lazy Users and Automatic Video Retrieval Tools in (the) Lowlands</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,253.86,140.53,72.09,10.48"><forename type="first">The</forename><surname>Lowlands</surname></persName>
						</author>
						<author>
							<persName coords="1,329.85,140.53,28.30,10.48;1,142.00,154.47,24.70,10.48"><forename type="first">Team</forename><surname>Cwi</surname></persName>
						</author>
						<author>
							<persName coords="1,178.58,154.47,26.34,10.48"><surname>Tno</surname></persName>
						</author>
						<author>
							<persName coords="1,502.13,215.01,14.46,7.86;1,95.40,225.97,20.86,7.86"><forename type="first">Jan</forename><surname>Baan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Amsterdam 3</orgName>
								<orgName type="institution" key="instit2">University of Twente</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,127.18,225.97,80.29,7.86"><forename type="first">Alex</forename><surname>Van Ballegooij</surname></persName>
						</author>
						<author>
							<persName coords="1,218.39,225.97,95.54,7.86"><forename type="first">Jan</forename><forename type="middle">Mark</forename><surname>Geusenbroek</surname></persName>
						</author>
						<author>
							<persName coords="1,324.85,225.97,77.52,7.86"><forename type="first">Jurgen</forename><surname>Den Hartog</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Amsterdam 3</orgName>
								<orgName type="institution" key="instit2">University of Twente</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,413.28,225.97,68.21,7.86"><forename type="first">Djoerd</forename><surname>Hiemstra</surname></persName>
							<affiliation key="aff1">
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,492.41,225.97,24.19,7.86;1,95.40,236.93,15.54,7.86"><forename type="first">Johan</forename><surname>List</surname></persName>
						</author>
						<author>
							<persName coords="1,121.40,236.93,68.52,7.86"><forename type="first">Thijs</forename><surname>Westerveld</surname></persName>
							<affiliation key="aff1">
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,200.38,236.93,58.65,7.86"><forename type="first">Ioannis</forename><surname>Patras</surname></persName>
						</author>
						<author>
							<persName coords="1,269.49,236.93,85.99,7.86"><forename type="first">Stephan</forename><surname>Raaijmakers</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Amsterdam 3</orgName>
								<orgName type="institution" key="instit2">University of Twente</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,365.96,236.93,46.17,7.86"><forename type="first">Cees</forename><surname>Snoek</surname></persName>
						</author>
						<author>
							<persName coords="1,422.59,236.93,57.01,7.86"><forename type="first">Leon</forename><surname>Todoran</surname></persName>
						</author>
						<author>
							<persName coords="1,490.07,236.93,26.52,7.86;1,95.40,247.89,31.26,7.86"><forename type="first">Jeroen</forename><surname>Vendrig</surname></persName>
						</author>
						<author>
							<persName coords="1,136.44,247.89,69.08,7.86"><forename type="first">Arjen</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
						</author>
						<author>
							<persName coords="1,230.66,247.89,63.30,7.86"><forename type="first">Marcel</forename><surname>Worring</surname></persName>
						</author>
						<title level="a" type="main" coord="1,72.64,102.09,466.75,15.11">Lazy Users and Automatic Video Retrieval Tools in (the) Lowlands</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6502E68B47C564B64D70EEF4E797288B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was funded (in part) by the ICES/KIS MIA project and the Dutch Telematics Institute project DRUID. The following people have contributed to these results</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper describes our participation in the TREC Video Retrieval evaluation. Our approach uses two complementary automatic approaches (the first based on visual content, the other on transcripts), to be refined in an interactive setting. The experiments focused on revealing relationships between (1) different modalities, <ref type="bibr" coords="1,122.42,377.88,12.73,8.74" target="#b1">(2)</ref> the amount of human processing, and (3) the quality of the results.</p><p>We submitted five runs, summarized in Table <ref type="table" coords="1,293.27,402.07,3.88,8.74" target="#tab_0">1</ref>. Run 1 is based on the query text and the visual content of the video. The query text is analyzed to choose the best detectors, e.g. for faces, names, specific camera techniques, dialogs, or natural scenes. Query by example based on detector specific features (e.g. number of faces, invariant color histograms) yields the final ranking result.</p><p>To assess the additional value of speech content, we experimented with a transcript generated using speech recognition (made available by CMU). We queried the transcribed collection with the topic text combined with the transcripts of video examples. Despite of the error-prone recognition process, the transcripts often provide useful information about the video scenes. <ref type="bibr" coords="1,134.31,581.67,18.41,8.74">Run</ref>  the speech transcripts with (visual-only) run 1 in an attempt to improve its results; run 3 is the obligatory transcript-only run. Run 4 models a user working with the output of an automatic visual run, choosing the best answer-set from a number of options, or attempting to improve its quality by helping the system; for example, finding moon-landers by entering knowledge that the sky on the moon is black or locating the Starwars scene by pointing out that the robot has golden skin.</p><p>Finally, run 5 combines all information available in our system: from detectors, to speech transcript, to the human-in-the-loop. Depending on the evaluation measures used, this leads to slightly better or slightly worse results than using these methods in isolation, caused by laziness expressed in the model for selecting the combination strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Detector-based Processing</head><p>The main research question addressed in run 1 was how to make query processing fully automatic. This includes devising mechanisms that bridge in an automatic way the semantic gap <ref type="bibr" coords="1,449.88,586.44,15.50,8.74" target="#b12">[13]</ref> between (1) the user's information need as specified on the one hand by the topic text description and on the other hand by the video and image examples and (2) the low level features that can be extracted from the video. We propose a unifying approach in which a wide range of detectors and features are combined in a way that is specified by semantic analysis of the topic description. Section 2.1 describes the system's architecture and Section 2.2 the specific detectors and features used. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ranked</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extraction of predicate parameters</head><p>Figure <ref type="figure" coords="2,119.93,205.70,3.88,8.74">1</ref>: Architecture for automatic system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">System's architecture</head><p>A great challenge in automatic retrieval of multimedia material is to determine which aspect of the information carried in the audiovisual stream is relevant for the topic in question. The aspects of information that we restrict to are determined by the specific detectors that our systems employs. Examples are color-based detectors, face detectors or modules that detect the camera technique or the presence of monologues.</p><p>In order to select the relevant detectors we associate them with concepts that exist in the 'is-a' hierarchy of the Wordnet dictionary. For example, the face detectors are associated with the concept 'person, individual, human'. In order to determine if the specific detector is to be used for a topic, we analyze its text 1 in two steps. In the first step, a syntactic analysis discards the words that are not nouns, verbs or adjectives. In the second step, we feed the remaining words to the Wordnet dictionary and detect if the concepts that are associated with the detectors that we have at our disposal are present in the 'is-a' hierarchy of the most common meaning of the words in question. Such an approach makes good associations for most of our detectors. However, it exhibits its limitations in the case that the query word has also other meanings. For example the most common meaning of the word "pan" is cooking utensil, cookware". Such ambiguities are resolved in our current system by maintaining an additional set of keywords for the camera motion detector.</p><p>Once the appropriate set of detectors are selected we proceed to the retrieval of the relevant video clips. In order to do so we need to make a distinction between two different kinds of detectors <ref type="bibr" coords="2,233.12,654.10,16.47,8.74" target="#b12">[13]</ref>:</p><p>• detectors for exact queries that yield a yes/no answer depending if a set of predicates is satisfied 1 We analyzed only the first sentence of the topic description.</p><p>(e.g. does the camera exhibit a zoom-in?). The face detector, the monologue detector, and the camera technique detector fall in this category; • detectors for approximate queries that yield a measure that expresses how similar is the examined video clip with an example video clip.</p><p>In this category fall the module for color-based retrieval.</p><p>The selected detectors of the first category are used to filter-out irrelevant material. Then, a query-byexample based search on the (selected) detectors of the second category produces the final ranked results. In case that the analysis of the topic description determines that no detector of the second category should be selected, the ranking is based on the shot length.</p><p>Let us finally note that some of the detectors of the first category learn some of their parameters from the examples provided in the topic. Such a detector is the face detector which learns from the query example how many persons should appear in a video clip so that it is characterized as relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Detectors</head><p>Another goal in the evaluation was to assess the quality of the detectors discussed in this Section. The results of run 1, in the cases that the right detector was chosen, indicate the techniques perform with fairly high precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Camera technique detection</head><p>To detect the camera technique used in a shot, we use a method based on spatiotemporal slices of the original video to detect whether the apparent motion is due to known camera activities such as pan and tilt, or the scene is static <ref type="bibr" coords="2,424.19,526.42,9.97,8.74" target="#b8">[9]</ref>. In the former case, we estimate the percentage of the apparent motion that is due to camera's pan, tilt and zoom (e.g. 60% zoom, 5% tilt and 35% pan). Clips to which the dominant apparent motion is not caused by camera operations are characterized as "unknown".</p><p>The detector of the camera technique was used for topics 44, 48 and 74 in which the keywords 'zoom' and 'pan' appear. The system categorized successfully apparent motions that are due to pure camera operations (90% precision for topic 44 and 100% precision for query 74), but failed for topic 48 in which the zooming-in is not due to change in camera's focallength. The reason for the latter is that the apparent motion field depends on the distance between camera and scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Face detector</head><p>An off-the-shelf face detector (Rowley <ref type="bibr" coords="3,233.70,93.55,18.31,8.74" target="#b11">[12]</ref>) is used in order to detect how many faces are present in the video clip in question. The result is compared with the number of faces that were detected in the image example. We use five categories of numbers of faces: 'no-face", '1-face', '2-faces', '3-faces', 'manyfaces'. The face detector is associated with the general concepts "person, individual, human" and "people" for the Wordnet hierarchy. It works well for topics requesting humans appearing in (near) frontal view (e.g. 100% precision for topic 41) but, naturally, is not relevant otherwise (e.g. water-skier in topic 31).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Caption retrieval</head><p>For finding given names in the visual content, three steps are taken:</p><formula xml:id="formula_0" coords="3,81.96,297.22,107.19,35.07">• text segmentation; • OCR; • fuzzy string matching.</formula><p>For text segmentation of video frames we use a dual approach. The first approach is a color segmentation method <ref type="bibr" coords="3,109.29,363.54,14.62,8.74" target="#b19">[20]</ref>, to reduce the number of colors, while preserving the characters. The second approach is intensity based, using the fact captions are superimposed. OCR is done by ScanSoft's TextBridge SDK 4.5 library <ref type="bibr" coords="3,142.82,411.36,14.62,8.74" target="#b15">[16]</ref>. Finally, string matching is done using k-differences approximate string matching (see e.g. <ref type="bibr" coords="3,90.26,435.27,10.30,8.74" target="#b0">[1]</ref>).</p><p>The detector worked well in retrieving video based on the text that appears as caption. It has been applied for 24 topics that contain capitalized text (e.g. 'House' and 'Congress' in topic 30) with around 10% and 20% false positives and false negatives respectively. However, the retrieved video (even if it contained the query text as a caption) did not always match with the user's intention (e.g. the result for topic 30 is a shot of a text document). Therefore, we have used the results of such a detector only when the topic consists of a text description only (i.e. no media example is available). Only in that case the shots that are retrieved based on this detector are used to initiate a color-based query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">Monologue detection</head><p>The method for monologue detection <ref type="bibr" coords="3,241.86,646.22,15.50,8.74" target="#b14">[15]</ref> first uses a camera distance heuristic based on Rowley's face detector <ref type="bibr" coords="3,112.31,670.13,14.62,8.74" target="#b11">[12]</ref>. Only shots showing faces appearing in front of the camera within a certain distance are processed. In a post-processing stage all those shots are checked upon using three constraints:</p><p>• shot should contain speech;</p><p>• shot should have a static or unknown camera technique; • shot should have a minimum length. When all constraints are met, a shot is classified as a monologue. Subsequently, the selected shots are ranked based on their length: the longer the shot the higher the likelihood of it being a true monologue.</p><p>This detector has been used for topics 40, 63 and 64 with a very good performance (near 100% precision). The performance is lower for topic 64 (60% precision), because satisfying the information need (male interviewees) requires to distinguish between sexes, a predicate not anticipated in our current system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.5">Detectors based on color invariant features</head><p>Ranking of the shots remaining after filtering using predicate detectors, was accomplished by implementing a query by image example paradigm. For each keyframe a robust estimate of the color content of each keyframe is computed by converting the keyframe to the Gaussian color model as described in <ref type="bibr" coords="3,323.63,369.87,9.96,8.74" target="#b3">[4]</ref>. The Gaussian color model is robust against spatial compression noise, achieved by the Gaussian smoothing involved. Further, the Gaussian color model is an opponent color representation, for which the channels are largely uncorrelated. Hence, the color histograms can be constructed as three separate one-dimensional histograms. The keyframes were stored in a database, together with their color histogram information. Matching of example keyframe against the database targets is efficiently performed by histogram intersection between each of the three (one-dimensional) histograms. Matching time was within a second, ensuring system response to be adequate for interactive retrieval purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Probabilistic Multimedia Retrieval</head><p>This section introduces our probabilistic approach to information retrieval, an approach that unifies models of discrete signals (i.e. text) and models of continuous signals (i.e. images) into one common framework. We usually take for text retrieval an approach based on statistical language models <ref type="bibr" coords="3,442.41,658.17,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="3,455.52,658.17,7.75,8.74" target="#b6">7,</ref><ref type="bibr" coords="3,465.85,658.17,12.73,8.74" target="#b9">10,</ref><ref type="bibr" coords="3,481.16,658.17,7.01,8.74" target="#b2">3]</ref>, which uses a mixture of discrete probability measures. For image retrieval, we experimented with a probabilistic model that uses a mixture of continuous probability measures <ref type="bibr" coords="3,353.74,705.99,14.61,8.74" target="#b17">[18]</ref>.</p><p>The basic model can in principal be used for any type of documents and queries, but for now we assume our documents are shots from a video. In a probabilistic setting, ranking the shots in decreasing order of relevance amounts to ranking the shots by the probability P (Shot i |Q) given that query. Using Bayes' rule we can rewrite this to:</p><formula xml:id="formula_1" coords="4,98.52,177.86,174.78,37.17">P (Shot i |Q) = P (Q|Shot i )P (Shot i ) P (Q) ∝ P (Q|Shot i )P (Shot i )</formula><p>In the above, the right-hand side will produce the same ranking as the left-hand side. In absence of a query, we assume that each shot is equally likely of being retrieved, i.e. P (Shot i ) = constant. Therefore, in a probabilistic model for video retrieval shots are ranked by their probability of having generated the query. If a query consists of several independent parts (e.g. a textual Qt and visual part Qv), then the probability function can be easily expressed as the joint probability of the different parts. Assuming independence between the textual part and the visual part of the query leads to:</p><formula xml:id="formula_2" coords="4,90.83,393.85,210.19,9.65">P (Q|Shot i ) = P (Qt|Shot i )P (Qv|Shot i )<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Text retrieval: the use of speech transcripts</head><p>For text retrieval, our main concern was adapting our standard language model system to the retrieval of shots. More specifically, we were interested in an approach to information retrieval that explicitly models the familiar hierarchical data model of video, in which a video is subdivided in scenes, which are subdivided in shots, which are in turn subdivided in frames. Statistical language models are particularly wellsuited for modeling complex representations of the data <ref type="bibr" coords="4,95.06,563.27,9.96,8.74" target="#b5">[6]</ref>. We propose to rank shots by a probability function that is a linear combination of a simple probability measure of the shot, of its corresponding scene, and of the corresponding video (we ignore frames, because in practice words in transcribed speech are not associated with a particular frame).</p><p>Assuming independence between query terms:</p><formula xml:id="formula_3" coords="4,79.03,657.50,214.96,58.14">P (Qt 1 , • • • , Qt n |Shot) = n j=1 (π 1 P (Qt j ) + π 2 P (Qt j |V ideo) + π 3 P (Qt j |Scene) + π 4 P (Qt j |Shot) )</formula><p>In the formula, Qt 1 , • • • , Qt n is a textual query of length n, π 1 , • • • , π 4 are the probabilities of each representation, and e.g. P (Qt j |Shot) is the probability of occurrence of the term Qt j in the shot: if the shot contains 10 terms in total and the query term in question occurs 2 times then this probability would be simply 2/10 = 0.2. P (Qt j ) is the probability of occurrence of the term Qt j in the collection.</p><p>The main idea behind this approach is that a good shot is one that contains the query terms; one that is part of a scene that has more occurrences of the query terms; and one that is part of a video that has even more occurrences of the query terms. Also, by including scenes in the ranking function, we hope to retrieve the shot of interest, even if the video's speech describes the shot just before it begins or just after it finishes. Depending on the information need of the user, we might use a similar strategy to rank scenes or complete videos instead of shots, that is, the best scene might be a scene that contains a shot in which the query terms (co-)occur.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Image retrieval: retrieving the key frames of shots</head><p>For the visual part, we cut the key frames of each shot into blocks of 8 by 8 pixels. On these blocks we perform the Discrete Cosine Transform (DCT), which is used in the JPEG compression standard. We use the first 10 DCT-coefficients from each color channel<ref type="foot" coords="4,323.71,433.75,3.97,6.12" target="#foot_0">2</ref> to describe the block. If an image consists of n blocks, we have n feature vectors describing the image (each vector consisting of 30 DCT coefficients). Now the probability that a particular feature vector (Qv j ) from our query is drawn from a particular shot (Shot i ) can be described by a Gaussian Mixture Model <ref type="bibr" coords="4,363.56,507.06,14.62,8.74" target="#b17">[18]</ref>. Each shot in the collection is then described by a mixture of C Gaussians. <ref type="foot" coords="4,485.72,517.44,3.97,6.12" target="#foot_1">3</ref> The probability that the a query (Qv) was drawn from Shot i is simply the joint probability for all feature vectors from Qv. We assume independence between the feature vectors</p><formula xml:id="formula_4" coords="4,334.88,589.01,205.12,44.48">P (Qv 1 , . . . , Qv n |Shot i ) = n j=1 C c=1 π i,c G(Qv j , µ i,c , Σ i,c ) (2)</formula><p>where π i,c is the probability of class c from Shot i and G(Qv j , µ i,c , Σ i,c ) is the Gaussian density (or normal density) for class c from shot i with mean vector µ i and co-variance matrix Σ i . If m is the number of DCT features representing a shot, the Gaussian is defined as:</p><formula xml:id="formula_5" coords="5,86.16,113.37,214.86,24.25">G(x, µ, Σ) = 1 (2π) m |Σ| e -1 2 (x-µ) T Σ -1 (x-µ)<label>(3)</label></formula><p>For each of the shots in the collection we estimated the probability, mean and co-variance for each of the Gaussians in the model using the Expectation Maximization algorithm <ref type="bibr" coords="5,161.85,182.04,15.50,8.74" target="#b10">[11]</ref> on the feature vectors from the shots.</p><p>At this stage, equation 2 could be used to rank shots given a query, however, its computational complexity is rather high. Therefore, instead of this Feature Likelihood (the likelihood of drawing all query features from a shot model) we computed the Random Sample Likelihood introduced by Vasconcelos <ref type="bibr" coords="5,72.00,277.68,14.62,8.74" target="#b17">[18]</ref>. The Random Sample Likelihood is defined as the likelihood that a random sample from the query model was drawn from the shot model, which comes down to building a model for your query image(s) and comparing that model to the documents models to rank our shots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experimental setup</head><p>For the textual descriptions of the video shots, we used speech transcripts kindly provided by Carnegie Mellon University. Words that occurred within a transition between two shots were put within the previous shot. We did not have a division of the video into scenes, nor did we build a scene detector. Instead, scenes were simply defined as overlapping windows of three consecutive shots. Because we did not have material available to tune the model, the values of the parameters were determined on a ad-hoc basis. Instead of implementing the model as described, we took a more straightforward approach of doubling artificially the terms in the middle shots to obtain pseudo-documents, and ranked those using the 'standard' model with parameter λ = 0.15 (see <ref type="bibr" coords="5,263.23,550.58,10.30,8.74" target="#b5">[6]</ref>). For the queries, we took both the words from the textual description of the topics and the words occurring in the video examples' time frame, if these were provided.</p><p>Run 2 combines automatically the results of run 1 and run 3. It is produced by applying the ranking strategy determined by query analysis to the results of the speech transcript run, using the latter as a filter; unless query analysis decides the transcripts would be irrelevant. Transcripts are ignored if the video is not expected to contain query words, which is the case of predicate detectors like camera motion techniques and monologues. The results of run 2 did not improve upon run 3, which may be attributed to the ad-hoc approach of combining methods. This motivated additional experiments with a pure probabilistic approach. We evaluated this alternative on the known item search task in an unofficial run. Table <ref type="table" coords="5,448.70,290.78,4.98,8.74" target="#tab_2">2</ref> compares these unofficial results with our submitted runs. A returned fragment is regarded relevant if the intersection between the fragment and a known item contains at least one third of the fragment and one third of the known item.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Unfortunately, the unofficial combined run is not better than run 2. The difference between measured performance of the unofficial image-based run and run 1 may have influenced this result. Although it is too early to draw strong conclusions from our experiments, another plausible explanation is that the assumption of independence between the textual and visual part is not a valid one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Interactive Experiments</head><p>Our interactive topic set consisted -by mistakeof only 30 topics, of which we 'solved' 9, and could not produce any answer for 2. <ref type="foot" coords="5,439.24,523.68,3.97,6.12" target="#foot_2">4</ref> This Section presents mostly positive highlights of our work on the interactive topics for the Video Collection. Note that our interactive users do not identify the correct answers in the retrieved result sets, so precision is not expected to be 100% (see also <ref type="bibr" coords="5,403.25,585.03,42.62,8.74">Section 5)</ref>.</p><p>A quick investigation of behavior of 'standard' image and video analysis techniques on the interactive topics proved our suspicion that purely automatic systems cannot be expected to perform well on most topics: a result of the 'difficult' queries (not just 'sunset' and 'tropical fish') and the low quality of the video data itself. Thus, we focused on the research question how users could improve upon naive query-by-example methods to express their information needs in a more successful manner.</p><p>The retrieval system used for this task is developed on top of Monet, a main-memory database system. It uses a variety of features that are all based on the distribution of color in the keyframes of the shots. Details on the particular features used are provided in a forth-coming technical report <ref type="bibr" coords="6,210.78,273.16,14.62,8.74" target="#b16">[17]</ref>. Note that, even though we participated in the interactive topics, the lack of a proper user interface in our current implementation implies that system interaction consisted mostly of writing scripts in Monet's query language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Color-based Retrieval Techniques</head><p>The results of topics 33 (White fort) and 54 (Glenn Canyon dam) clearly demonstrate that popular colorbased retrieval techniques can indeed be successful, as long as the query example is derived from the same source as the target objects. Figure <ref type="figure" coords="6,265.89,416.50,4.98,8.74" target="#fig_0">2</ref> shows the keyframes representing the example and known item for topic 33; any color-based technique worked out well for this query. Topic 54 was solved using a spatial color histogram retrieval method, implicitly enforcing locality such as blue sky on top, brown rocks on the sides and white water and concrete dam in the center. <ref type="foot" coords="6,129.87,498.61,3.97,6.12" target="#foot_3">5</ref>Topic 53 (Perseus) is an example where we were lucky: the example image provided happens to look surprisingly much like the Perseus footage in the data-set, and spatial color histogram retrieval retrieves a large number of Perseus clips.</p><p>Topic 24 (R. Lynn Bondurant) provides an interesting lesson about the balance between recall and precision using content-based retrieval techniques. Although it is relatively easy to find some other shots showing Dr. Bondurant -those where he sits in the same room wearing the same suit -finding all shots is a completely different question.</p><p>The other topics confirm our intuition that we should not expect too much from 'traditional' content-based retrieval techniques. Although more advanced features based on texture and shape possibly could help in solving more topics directly, we doubt whether a significant improvement over these results would be achieved. If available however, domain-specific detectors (such as the face detectors deployed in run 1) can provide good performance for specific tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Query Articulation</head><p>As an alternative approach, we propose to put more emphasis on the quality of the queries expressing the underlying information need. We aim for the interactive refinement from initial, broad multi-modal examples into relatively precise search requests, in a process we have termed query articulation <ref type="bibr" coords="6,477.43,390.93,9.97,8.74" target="#b1">[2]</ref>. In essence, articulating a query corresponds to constructing a query-specific detector on-the-fly.</p><p>The idea of query articulation is best demonstrated through the idea of a 'color-set'. Users define colorsets interactively by selecting regions from the example images, possibly extending the implied color-set by adding similar colors. Unlike the binary sets introduced in VisualSEEK <ref type="bibr" coords="6,408.41,486.57,14.62,8.74" target="#b13">[14]</ref>, we essentially re-quantize the color space in a smaller number of colors, by collapsing the individual elements of a color-set onto a single new color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic 19: Lunar Rover</head><p>Topic 19 (Lunar Rover) provides 2 example images showing the lunar rover. The visual differences between the (grayish) sample images and (bluish) known-items (shown in Figure <ref type="figure" coords="6,447.17,602.55,4.43,8.74" target="#fig_1">3</ref>) explain why colorbased retrieval techniques are not successful on this topic. Query articulation allows users to circumvent this problem, by making explicit their own world knowledge: in scenes on the moon, the sky is black. This can be expressed in terms of the system using two simple filters based on color-sets:</p><p>• 'Black Sky': The filter is realized by selecting those keyframes for which the top 25% of the  image is at least 95% dark (a color-set shown in Figure <ref type="figure" coords="7,123.36,291.03,3.88,8.74" target="#fig_2">4</ref>). • 'Non-black Bottom': making sure that no completely dark images are retrieved, (a large number of outer-space shots are present in the dataset) this second filter selects only those keyframes that do not have a black bottom as there should be lunar surface with the lunar rover visible. The filter is realized by selecting those keyframes for which the lower half of the image is less than 80% dark.</p><p>Together, these filters effectively reduce the total data-set of approximately 7000 keyframes to only 26, containing three of the four known items. Recall is improved using a follow-up query, ranking the images with a 'Black Sky' using the spatial color histogram method on a seed image drawn from the previous phase. This second step returns the four known items in the top-10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic 8: Jupiter</head><p>The Jupiter topic is another example that benefits significantly from query articulation. At a first thought, this query may seem to be easy to solve, as planets have a typical appearance (a colored circle surrounded by black) and Jupiter should be easily recognized. But, examining the example images shown in Figure <ref type="figure" coords="7,147.24,622.08,3.88,8.74" target="#fig_3">5</ref>, it is apparent that colors in different photos of Jupiter can differ significantly.</p><p>An important characteristic of Jupiter is the distinguishable orange and white lines crossing its surface. Articulating this through color content, we decided to put emphasis on the orange content, the white content, and their interrelationships, expressed as filters on color-set correlograms <ref type="bibr" coords="7,199.86,705.99,9.97,8.74" target="#b7">[8]</ref>. Computing correlo-grams from the color-sets shown in Figure <ref type="figure" coords="7,493.78,75.16,4.98,8.74" target="#fig_4">6</ref> produces 9-dimensional feature vectors, one dimension for each possible transition. To ensure that the results are not dominated by the auto-correlation coefficients, the resulting vectors are weighted using the inverse of their corresponding coefficients in the query images. The derived query finally finds some of the known-items, but recall remains low.</p><p>Another way to emphasize the striped appearance of Jupiter is to detect the actual presence of (horizontal) lines in images and rank the keyframes based on that presence. This was implemented by means of DCT-coefficients, classifying each DCT-matrix in the luminance channel of a keyframe into texture-classes. We used the classes 'horizontal-line', 'vertical-line', 'blank' and 'other'. The cheap method of ranking by simple statistics on these texture-classes proved only slightly worse than the previous (elaborate and expensive) method based on correlograms.</p><p>Although a combination of both results did not retrieve any additional answers, a minor improvement is obtained through a subsequent search, seeded with a retrieved shot found before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic 25: Starwars</head><p>Finding the Starwars scene became a matter of honor, since we submitted the topic ourselves -perhaps a bit over-enthusiastically. After several unfruitful attempts using color histograms and color-sets, we decided to articulate the query by modeling the golden appearance of one of the robots, C3PO. This idea might work well, as we do not expect to find many golden objects in the data-set.</p><p>The appearance of gold does not simply correspond to the occurrence of a range of colors; its most distinguishing characteristic derives from the fact it is a shiny material, implying the presence of small, sharp highlights. We implemented two stages of boolean filters to capture these properties, followed by a custom ranking procedure.</p><p>The first filter selects only those images that  have sufficient amount of golden content. It checks whether images have at least 20% 'golden' pixels, using the gold color-set defined in Figure <ref type="figure" coords="8,247.30,251.62,3.88,8.74" target="#fig_4">6</ref>. Secondly, a set of filters reduces the data-set by selecting those images that contain the color(set)s shown, representing the appearance of gold in different lighting conditions, in a way expected for shiny metallic surfaces: a bit of white, some light-gold, a lot of medium-gold, and some dark-gold. Although the precise percentages to be selected are difficult to choose correctly, we believe the underlying idea is valid, as we modeled expected levels of gold-content for a shiny-gold robot.</p><p>The resulting subset is then ranked using another characteristic of shiny surfaces: the expected spatial relations between those color-sets (white highlights surrounded by light-gold spots, surrounded by medium-gold surfaces, which in turn are surrounded with dark-golden edges). We expressed this property using color correlograms, ranking the relevant transitions.</p><p>Using this elaborate approach, we managed to retrieve one of the correct answers, but no higher than position 30. We retrieve many 'golden' images with elements satisfying our limited definition of shininess (most of them not 'metallic'), but the properties of metal surfaces must be modeled more realistically to get more convincing results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic 32: Helicopter</head><p>The helicopter topic provides three audio examples, and we experimented with the audio analogon of query articulation in an attempt to find scenes with helicopters. We hoped to specify the characteristics of a helicopter sound as a combination of two filters:</p><p>(1) a repetitive pattern using periodicity of the audio spectrum, and (2) a concentration of energy in the lower frequencies, using spectral centroid and bandwidth features. Details of the techniques we tried can be found in <ref type="bibr" coords="8,125.13,705.99,14.62,8.74" target="#b16">[17]</ref>.</p><p>Unfortunately, the helicopter sound in the knownitem can only be noticed in the background, and some characteristics of the speech voice-over overlap with the idea of the second filter. It turns out the combination of filters can detect sounds corresponding to vehicles and airplanes, but we have not managed to tune the filters such that it singles out helicopters only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Reflection</head><p>The highlighted known-item searches illustrate the idea underlying the process of query articulation, and demonstrate how query articulation may improve the results of multimedia retrieval dramatically. Without the elicitation of such relatively exact queries, none of these topics could be solved using our limited feature models. The query articulation process studied for topics 25 and 32 (and even for topic 8) suffered however from the risk of overemphasizing precision, sacrificing overall recall. Especially if the features available in the system do not correspond closely to the particular characteristics of the desired result set, the current system does not provide sufficient support to assess suitability of candidate strategies. But, also if appropriate features are available, the resulting query may 'overlook' other possibilities; for example, our strategy would not find the lunar rover if appearing in a lunar crater or in a hangar on earth (so there is no visible black sky).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Lazy Users</head><p>In our interactive experiments, we assumed a 'lazy user' model: users investing only limited effort to express their information need. Our users view 20 result summaries at a time, after which they choose whether to look at more results from the current strategy, or formulate a new strategy. They are not expected to investigate more than 100 result summaries in total. Lazy users identify result sets instead of correct answers, so our interactive results are not 100% precision.</p><p>The combination strategies used to construct run 5 consisted of:</p><p>• choose the run that looks best; • concatenate or interleave top-N from various runs; • continue with an automatic, seeded search strategy.</p><p>For example, the strategy for topic 24 (Lynn Bondurant) used a seeded search based on run 3, which was interleaved with the results of run 4. Surprisingly, the run with speech transcripts only turns out better than the combined run, although not on all topics. It has proven difficult to combine results of multiple input runs effectively. While lack of time did also play a role (the combination strategies were not tried very systematically), the results for topics 54 and 59 demonstrate that a lazy user can, based on a visual impression of a result set, inadvertently decide to discard the better results (in both cases, run 3 was better but run 4 was chosen as best answer). Tool support for such a combination process seems a promising and worthwhile research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>A major goal of having a video retrieval task at TREC-10 was to research a meta-question: investigate (experimentally, through a 'dry-run') how video retrieval systems should be evaluated. Working on the task, we identified three concerns with the current setup of the evaluation:</p><p>• the inhomogeneity of the topics;</p><p>• the low quality of the data;</p><p>• the evaluation measures used.</p><p>Candidate participants all contributed a small number of multimedia topics, the union of which formed the topic set. Partly as a result of the different angles from which the problem of video retrieval can be approached, the resulting topic set is very inhomogeneous. The topic text may describe the information need concisely, but can also provide a detailed elucidation; topics can test particular detectors, or request very high-level information; and some topic definitions are plainly confusing, like 'sailboat on the beach' which uses a yacht on the sea as image example 6 . Thus, each subtask consisted of a mix of (at least) three distinct classes of topics: detector-testers, precise known-item topics, and generic searches. This inhomogeneity causes two problems: it complicates query analysis for automatic systems, and makes comparison between runs difficult (a single good detector can easily dominate an overall score like average precision).</p><p>The low quality of the video data provided another unexpected challenge. It makes some topics more complex than they seemed at first sight (like 'Jupiter'). Also, the results obtained with the technique discussed in Section 2.2.5 are much lower than the application of the same paradigm on for example 6 Shame on us -we contributed this topic ourselves.</p><p>the Corel photo gallery. In fact, we observed that in many cases the color distributions to a large extent are a better indication of the similarity in age of the data than of the true video content. Of course, this can also be viewed as a feature of this data set rather than a concern. Experiments discussed by Hampapur in <ref type="bibr" coords="9,373.20,146.89,10.51,8.74" target="#b4">[5]</ref> showed as well how techniques behaving nicely on homogeneous, high quality data sets are of little value when applied to finding illegal copies of video footage on the web (recorded and digitized with widely varying equipment).</p><p>The third concern, about the evaluation measures, is based on two slightly distinct observations. First, our lazy user model returns shots as answers for known-item queries, but these are often shorter than 1/3 of the scenes that should be found. The chosen evaluation metric for known-item topics thus deems our answers not relevant, while this could be considered open for discussion: a user could easily rewind to the start of the scene.</p><p>Second, an experimental setup that solves the interactive topics by handpicking correct answers should probably result into 100% precision answer sets. First of all, this indicates that precision is not the right measure to evaluate the results of the interactive task. Lower scores on precision only indicate inter-assessor disagreement (viewing the user as just another assessor), instead of the precision of the result set. Another example of this phenomenon can be found in the judgments for topic 59 on runs 4 and 5, where identical results were judged differently. <ref type="foot" coords="9,513.57,432.38,3.97,6.12" target="#foot_4">7</ref> The significant difference in measured performance indicate that the current topics and relevance judgments should probably not be used as ground truth data for laboratory experiments.</p><p>As a concluding remark, it is not so clear how realistic the task is. First of all, no participant seemed to know how to create 'doable' topics for the BBC data, while those video clips are drawn from a real video archive. Also, it seems unlikely that a user with stateof-the-art video retrieval tools could have beaten a naive user who simply scrolls through the relatively small set of keyframes. A larger collection would give video retrieval systems a fairer chance, but the engineering problems (and cost) arising might discourage participation in the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In spite of the issues raised in the discussion, we believe the TREC video evaluation is a strong initiative that was much needed to advance the field of multimedia retrieval, and it has already pointed us to a range of problems that we may never have thought of without participation.</p><p>Our evaluation demonstrates the importance of combining various techniques to analyze the multiple modalities. The optimal technique depends always on the query; both visual and speech can prove to be the key determining factor, while user interaction is crucial in most cases. The final experiment attempted to deploy all available information, and it seems worthwhile to investigate in research into better techniques to support choosing a good combination of approaches. In some cases, this choice can already be made automatically, as demonstrated in run 1; but, in cases like the known-item searches discussed for run 4, user interaction is still required to decide upon a good strategy.</p><p>Our (admittedly poor) results identify many issues for future research: new and improved detectors (better suited for low-quality data), better combination strategies, and more intelligent use of the user's knowledge. The integration of supervised and unsupervised techniques for query formulation form a particular research challenge.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,72.00,144.55,229.03,8.74;6,72.00,156.50,128.79,8.74"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Topic 33, White fort, example(left) and known-item(right) keyframes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,310.98,169.02,229.01,8.74;6,310.98,180.97,216.68,8.74"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Topic 19, Lunar rover, examples (images on top) and the keyframes of the correct answers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,72.00,101.15,229.02,8.74;7,72.00,113.11,54.19,8.74;7,114.65,132.31,51.48,51.33"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The 'dark' color-set as defined for topic 19, Lunar rover.</figDesc><graphic coords="7,114.65,132.31,51.48,51.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,72.00,234.86,229.03,8.74;7,72.00,246.81,141.19,8.74;7,169.45,133.62,88.92,50.03"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Topic 8, Jupiter, example (on top) and some correct answers keyframes.</figDesc><graphic coords="7,169.45,133.62,88.92,50.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,310.98,691.55,229.02,8.74;7,310.98,703.50,101.48,8.74"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Color-sets used in the Jupiter (left) and the Starwars (right) topics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="8,72.00,183.14,229.01,8.74;8,72.00,195.10,152.75,8.74"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Topic 25, Starwars, examples(left 2 images) and the correct answers keyframes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,80.92,581.67,220.10,121.16"><head>Table 1 :</head><label>1</label><figDesc>2 combines the ranked output of Summary of runs</figDesc><table coords="1,80.92,612.45,211.18,68.91"><row><cell cols="2">Run Description</cell></row><row><cell>1</cell><cell>Detector-based, automatic</cell></row><row><cell>2</cell><cell>Combined 1-3, automatic</cell></row><row><cell>3</cell><cell>Transcript-based, automatic</cell></row><row><cell>4</cell><cell>Query articulation, interactive</cell></row><row><cell>5</cell><cell>Combined 1-4, interactive, by a lazy user</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,310.98,73.56,229.03,126.24"><head>Table 2 :</head><label>2</label><figDesc>Recall @ 100 and precision @ 100 for probabilistic runs</figDesc><table coords="5,447.93,73.56,71.46,8.74"><row><cell>R@100 P@100</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="4,326.22,697.85,132.01,6.99"><p>We work in the YCbCr color space.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="4,326.22,707.35,128.67,6.99"><p>We used a mixture of 8 Gaussians.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="5,326.22,697.89,213.76,6.99;5,310.98,707.35,137.04,6.99"><p>The slightly smaller topic set used was the result of missing a crucial message on the mailing list.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="6,87.24,697.89,213.76,6.99;6,72.00,707.35,82.19,6.99"><p>Obviously, nothing guaranteed the dams found are indeed Glenn Canyon dams...</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4" coords="9,326.22,697.89,213.76,6.99;9,310.98,707.35,20.93,6.99"><p>This may also have been a case of intra-assessor disagreement.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Many thanks go to <rs type="person">Alex Hauptman</rs> of <rs type="affiliation">Carnegie Mellon University</rs> for providing the output of the CMU large-vocabulary speech recognition system.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="10,90.15,494.69,210.87,6.99;10,90.15,504.16,197.42,6.99" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<title level="m" coord="10,226.33,494.69,74.69,6.99;10,90.15,504.16,29.66,6.99">Modern information retrieval</title>
		<meeting><address><addrLine>Wokingham, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.15,518.30,210.85,6.99;10,90.15,527.76,210.87,6.99;10,90.15,537.22,210.87,6.99;10,90.15,546.69,210.85,6.99;10,90.15,556.15,83.53,6.99" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,108.62,527.76,130.79,6.99">Exact matching in image databases</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Van Ballegooij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">L</forename><surname>Kers Ten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,258.38,527.76,42.64,6.99;10,90.15,537.22,210.87,6.99;10,90.15,546.69,95.51,6.99">Proceedings of the 2001 IEEE International Conference on Multi media and Expo (ICME2001)</title>
		<meeting>the 2001 IEEE International Conference on Multi media and Expo (ICME2001)<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">August 22-25 2001</date>
			<biblScope unit="page" from="513" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.15,570.29,210.86,6.99;10,90.15,579.76,169.45,6.99" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,165.98,570.29,115.59,6.99">The Mirror DBMS at TREC-9</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Arjen</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>De Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,90.15,579.76,82.36,6.99">Voorhees and Harman</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="171" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.15,593.90,210.86,6.99;10,90.15,603.36,210.86,6.99;10,90.15,612.82,210.86,6.99;10,90.15,622.29,47.28,6.99" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,205.11,603.36,61.21,6.99">Color invariance</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Geusebroek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Van Den Boomgaard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Geerts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,280.34,603.36,20.67,6.99;10,90.15,612.82,136.76,6.99">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>to appear, November</note>
</biblStruct>

<biblStruct coords="10,90.15,636.43,210.85,6.99;10,90.15,645.89,210.88,6.99;10,90.15,655.36,210.87,6.99;10,90.15,664.82,191.22,6.99" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,194.47,636.43,106.53,6.99;10,90.15,645.89,107.43,6.99">Comparison of distance measures for video copy detection</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hampapur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bolle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,214.59,645.89,86.44,6.99;10,90.15,655.36,210.87,6.99">Proceedings of the 2001 IEEE International Conference on Multi media and Expo</title>
		<meeting>the 2001 IEEE International Conference on Multi media and Expo<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-08-22">2001. August 22-25 2001. 9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.15,678.96,210.87,6.99;10,90.15,688.42,210.87,6.99;10,90.15,697.89,210.86,6.99;10,90.15,707.35,4.23,6.99" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="10,157.14,678.96,143.88,6.99;10,90.15,688.42,29.66,6.99">Using language models for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Djoerd</forename><surname>Hiemstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001. 3, 4, 5</date>
		</imprint>
		<respStmt>
			<orgName>Centre for Telematics and Information Technology, University of Twente</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="10,329.14,76.52,210.86,6.99;10,329.14,85.98,210.86,6.99;10,329.14,95.45,210.87,6.99;10,329.14,104.91,210.86,6.99;10,329.14,114.38,210.85,6.99;10,329.14,123.84,30.10,6.99" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,481.88,76.52,58.12,6.99;10,329.14,85.98,167.00,6.99">Twenty-One at TREC-7: Ad-hoc and cross-language track</title>
		<author>
			<persName coords=""><forename type="first">Djoerd</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wessel</forename><surname>Kraaij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,471.66,95.45,68.34,6.99;10,329.14,104.91,210.86,6.99;10,329.14,114.38,145.15,6.99">Proceedings of the Seventh Text Retrieval Conference TREC-7, number 500-242 in NIST Special publications</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Seventh Text Retrieval Conference TREC-7, number 500-242 in NIST Special publications</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="227" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.14,137.29,210.85,6.99;10,329.14,146.76,210.87,6.99;10,329.14,156.22,195.58,6.99" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,329.14,146.76,151.71,6.99">Spatial Color Indexing and Applications</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zahib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,491.13,146.76,48.88,6.99;10,329.14,156.22,102.40,6.99">International journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="268" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.14,169.67,210.85,6.99;10,329.14,179.13,210.85,6.99;10,329.14,188.60,210.87,6.99;10,329.14,198.06,146.52,6.99" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,468.48,169.67,71.50,6.99;10,329.14,179.13,210.85,6.99;10,329.14,188.60,105.25,6.99">Efficient automatic analysis of camera work and microsegmentation of video using spatiotemporal images</title>
		<author>
			<persName coords=""><forename type="first">Philippe</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hae-Kwan</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,444.31,188.60,95.70,6.99;10,329.14,198.06,56.99,6.99">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.14,211.51,210.86,6.99;10,329.14,220.98,210.85,6.99;10,329.14,230.44,116.94,6.99" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,464.98,211.51,75.01,6.99;10,329.14,220.98,141.60,6.99">TNO/UT at TREC-9: How different are web documents?</title>
		<author>
			<persName coords=""><forename type="first">Wessel</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thijs</forename><surname>Westerveld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,489.70,220.98,50.29,6.99;10,329.14,230.44,29.86,6.99">Voorhees and Harman</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="665" to="671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.14,243.89,210.85,6.99;10,329.14,253.35,210.85,6.99;10,329.14,262.82,210.85,6.99;10,329.14,272.28,43.74,6.99" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,502.60,243.89,37.39,6.99;10,329.14,253.35,207.04,6.99">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,329.14,262.82,174.53,6.99">Journal of the Royal Statistical Society, series B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.14,285.73,210.86,6.99;10,329.14,295.20,210.87,6.99;10,329.14,304.66,187.88,6.99" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,480.45,285.73,59.54,6.99;10,329.14,295.20,77.22,6.99">Neural networkbased face detection</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">A</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<idno>1998. 3</idno>
	</analytic>
	<monogr>
		<title level="j" coord="10,419.55,295.20,120.45,6.99;10,329.14,304.66,127.04,6.99">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.14,318.11,210.85,6.99;10,329.14,327.58,210.85,6.99;10,329.14,337.04,210.86,6.99;10,329.14,346.51,210.20,6.99" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,362.85,327.58,177.14,6.99;10,329.14,337.04,38.48,6.99">Content based image retrieval at the end of the early years</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,374.69,337.04,165.31,6.99;10,329.14,346.51,76.10,6.99">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2000-12">Dec. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.14,359.95,210.85,6.99;10,329.14,369.42,210.86,6.99;10,329.14,378.88,124.66,6.99" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,442.44,359.95,97.55,6.99;10,329.14,369.42,151.61,6.99">VisualSEEk: a fully automated content-based image query system</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,499.43,369.42,40.56,6.99;10,329.14,378.88,37.79,6.99">ACM Multimedia 96</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.14,392.33,210.85,6.99;10,329.14,401.80,210.86,6.99;10,329.14,411.26,159.35,6.99" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="10,389.23,392.33,150.76,6.99;10,329.14,401.80,210.86,6.99;10,329.14,411.26,89.16,6.99">Camera distance classification: Indexing video shots based on visual features. Master&apos;s thesis, Universiteit van Amsterdam</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000-10">October 2000. 3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.14,424.71,188.95,6.99" xml:id="b15">
	<monogr>
		<ptr target="http://www.scansoft.com.3" />
		<title level="m" coord="10,329.14,424.71,74.55,6.99">TextBridge SDK 4.5</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.14,438.16,210.85,6.99;10,329.14,447.63,210.86,6.99;10,329.14,457.09,83.49,6.99" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,329.14,447.63,154.29,6.99">Participating in Video-TREC with Monet</title>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Van Ballegooij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johan</forename><surname>List</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arjen</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,350.08,457.09,15.16,6.99">CWI</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="10,329.14,470.54,210.85,6.99;10,329.14,480.00,210.85,6.99;10,329.14,489.47,210.87,6.99;10,329.14,498.93,210.86,6.99;10,329.14,508.40,106.83,6.99" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,467.18,470.54,72.81,6.99;10,329.14,480.00,210.85,6.99;10,329.14,489.47,61.33,6.99">Embedded mixture modelling for efficient probabilistic content-based indexing and retrieval</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lippman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,410.69,489.47,129.31,6.99;10,329.14,498.93,43.45,6.99">Multimedia Storage and Archiving Systems III</title>
		<imprint>
			<date type="published" when="1998">1998. 3, 4, 5</date>
			<biblScope unit="volume">3527</biblScope>
			<biblScope unit="page" from="134" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.14,521.85,210.87,6.99;10,329.14,531.31,210.86,6.99;10,329.14,540.78,178.56,6.99" xml:id="b18">
	<analytic>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,487.57,521.85,52.43,6.99;10,329.14,531.31,170.72,6.99">Proceedings of the Ninth Text Retrieval Conference (TREC-9</title>
		<meeting>the Ninth Text Retrieval Conference (TREC-9</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="500" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.14,554.23,210.86,6.99;10,329.14,563.69,210.85,6.99;10,329.14,573.15,210.86,6.99;10,329.14,582.62,210.86,6.99;10,329.14,592.08,46.56,6.99" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="10,440.40,554.23,99.60,6.99;10,329.14,563.69,210.85,6.99;10,329.14,573.15,13.54,6.99">Segmentation of color documents by line oriented clustering using spatial information</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Todoran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,360.12,573.15,179.87,6.99;10,329.14,582.62,100.11,6.99">International Conference on Document Analysis and Recognition ICDAR&apos;99</title>
		<meeting><address><addrLine>Bangalore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="67" to="70" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
