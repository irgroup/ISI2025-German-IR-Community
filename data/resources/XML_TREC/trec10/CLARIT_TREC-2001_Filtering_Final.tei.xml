<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">54EE1D8D2CC4850B7DDEB40FF208B11A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The Clairvoyance team participated in the Filtering Track, submitting the maximum number of runs in each of the filtering categories: Adaptive, Batch, and Routing. We had two distinct goals this year: <ref type="bibr" coords="1,181.73,237.31,10.96,8.44" target="#b0">(1)</ref> to establish the generalizability of our approach to adaptive filtering and (2) to experiment with relatively more "radical" approaches to batch filtering using ensembles of filters. Our routing runs served principally to establish an internal basis for comparisons in performance to adaptive and batch efforts and are not discussed in this report.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Adaptive Filtering</head><p>In previous TREC work ( <ref type="bibr" coords="1,154.65,340.75,47.51,8.44">TREC 7 &amp; 8)</ref>, we developed an approach to adaptive filtering that proved to be robust and reasonably effective, as evidenced by the relatively strong performance of our systems <ref type="bibr" coords="1,168.23,371.71,9.91,8.44" target="#b0">[1,</ref><ref type="bibr" coords="1,178.14,371.71,6.61,8.44" target="#b1">2]</ref>. For TREC 2001 we sought to assess the generalizability of the approach, given especially the differences this year in (a) the amount and nature of the training data and (b) the inherently "classification"-oriented (vs. "query"-oriented) task. Indeed, additional differences, such as the large numbers of expected "hits" in the test set, contributed to the special character of this year's task. The CLARIT Filtering system is based on core CLARIT retrieval technology. In brief, the CLARIT approach uses text structures such as noun phrases, sub-phrases, and morphologically normalized words, as features or terms to represent text (or passage) or topic (query) content. Terms, in turn, are weighted based on document and corpus statistics (such as IDF and TF), and additionally can have independent coefficients to adjust weights according to processing requirements (such as updates). Information objects are modeled as vectors in a highdimensional vector space; the Euclidean inner product gives the distance (or closeness) measure (document score) in the space. The system also has a variety of thesaurus (term-extraction) algorithms; these are used to identify characterizing terms for a document or set of documents (e.g., the set of "relevant" documents associated with a topic).</p><p>In addition to core processing, our adaptive-filtering system has several parameters, including (i) the number and type of features used to create a topic profile, (ii) the score/threshold setting, (iii) the frequency of setting updates (driven by feedback), (iv) the selection and number of (new) features added at updates, (v) the resetting of score thresholds, and (vi) the number of documents retained over time as a basis for modeling the topic (historical reference statistics and aging).</p><p>For this year's TREC adaptive filtering task, we used the same system that was used for our TREC-8 experiments <ref type="bibr" coords="1,324.05,206.12,9.46,8.44" target="#b1">[2]</ref>. However, for threshold setting and updating we further experimented with our beta-gamma adaptive threshold-regulation method. The method selects a threshold by interpolating between an "optimal" threshold and "zero" threshold for a specified utility function. This method can be applied both to training or sample document sets, as well as to documents that have been returned and judged during actual filtering.</p><p>The optimal threshold is the threshold that yields the highest utility over the training or accumulated reference data. Operationally, this threshold is determined by using the topic profile as a query over the reference (judged) documents to score and rank them based on their features (terms). Additionally, based on the utility function for the filter, a cumulative utility score is calculated at each rank point in ascending order. Typically, the cumulative utility score at each rank point manifests a well-behaved trend: it ascends, reaches a peak value, and descends again, eventually turning negative (as the remaining documents are mostly non-relevant). The feature score on the document at the lowest rank point where the cumulative utility score reaches its maximum is taken as the optimal threshold. The zero threshold is determined by the score on the document at the highest rank point below the optimal threshold that has a cumulative utility of zero or less.</p><p>The two parameters, beta and gamma, are used to determine the feature score-between the optimal threshold and the zero threshold-that will be used as the actual threshold for the filter. Beta attempts to account for the inherent or systematic (i.e. sampling) bias in optimal threshold calculation. Gamma makes the thresholding algorithm sensitive to the number of documents processed. The inverse (1/gamma) expresses the number of documents needed to gain reasonable confidence in the value of the score threshold (apart from the bias already accounted for through beta). The parameters are  + + determined empirically. See Figure <ref type="figure" coords="2,200.67,57.07,5.07,8.44" target="#fig_0">1</ref> for the formulae that use beta and gamma to calculate a filter-threshold score value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic-Specific Optimization and Structuring</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Pre-Test Experiments and Calibration</head><p>We chose to recalibrate all parameter settings by running the system again on the tasks for TREC-8 and TREC-9 Filtering-the latter task to better approximate the classification-oriented features of the Reuters data. (In particular, we did no adaptive-filtering calibrations on any Reuters data <ref type="bibr" coords="2,108.30,171.07,57.18,8.44">(1987 or 1996)</ref>, given our desire to assess unbiased system performance.) Our results for these preliminary tasks were quite good (actually better than any of the reported results in TREC 8 and equal to the best results in TREC 9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Test Configuration</head><p>Our approach to testing included the following elements.</p><p>• Preprocessing: All documents, including testing documents, training documents, and topic descriptions, were pre-indexed using all single nouns, single words occurring in noun phrases, and two-word noun phrases, as recognized by the CLARIT parser.</p><p>• IDF Statistics: The IDF statistics were collected from all the training data. We did not update initial term statistics in the process of filtering as our past experiments indicate that doing so does not seem to have as much impact on the overall filtering performance as other factors.</p><p>• Term Weighting and Scoring: We used the BM25 TF formula for TF-IDF weighting; the average document length was set to 1,000. We used dot product scoring for matching documents with profiles.</p><p>• Initial Profile Term Vector: An initial profile vector was built from the original topic description and trained using the two training examples for each topic by adding 20 terms to the original profiles with coefficient of 0.1. We used a delivery-ratio estimation method to set the initial score threshold and set both gamma and beta to 0.1 for use in processing test data.</p><p>• Term Vector Updating: We used Rocchio term vector learning, but only positive examples were used to expand the profile. A centroid vector accumulator was updated whenever a profile accepted a relevant document. The top K terms with the highest scores were selected from the centroid and added to the original profile with a uniform coefficient. The vector was updated when a specified number of documents had been delivered since the last update or when the profile had not been updated for a time interval measured by 3,000 documents in the test stream.</p><p>• Threshold Updating: We used the same beta-gamma threshold-regulation algorithm as in TREC-8. To emphasize recent documents, we discarded any documents in the cached set of scored documents for each profile that were older than 30,000 documents, provided the cached set did not fall below a minimum of 1,000 documents. The cached documents included both true-and false-positive examples. At any point when a false-positive document scored below a "reference threshold"-equal to half of the then-current real threshold for the profile-the document was discarded.</p><p>For the official TREC-2001 submission, we used the best parameter settings we discovered in our preliminary experiments (on TREC-8 and TREC-9 data). In particular, we varied only two parameters-the number of terms added at each update and slight differences in threshold convergence rates-to create four different submissions, with configurations as given in Table <ref type="table" coords="2,471.86,428.35,3.67,8.44">1</ref>, optimized for linear utility T10U.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Test Results</head><p>Table <ref type="table" coords="2,349.29,480.19,5.07,8.44">2</ref> gives official results for our submitted runs and Table <ref type="table" coords="2,349.24,490.51,5.07,8.44">3</ref> gives comparative results. Our four official runs have similar performance. Our results were good from the point of view of "conservative" filtering (and delivery of information); we achieved an average utility of 222 for our best run, with only 30 topics scoring slight negative utility (the average of these being -4.37 and the maximum -12). However, in the context of the TREC task and the Reuters data, this is poor-to-mediocre performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Observations</head><p>It seems clear in retrospect that the principal problem in the system was the setting of rather high thresholds (scores), resulting in the delivery of too few documents, especially in the first stages of filtering.</p><p>In our system, the initial threshold setting is determined, in part, by the expected "delivery ratio" or density of relevant documents expected in the stream of data to be processed. In particular, before any filtering can occur, a score threshold must be established based on the available information about the topic. In practice, we use the topic profile (based on terms extracted from the topic description and the two example documents) as a query to score and rank the reference documents. Note that we examine none of the documents in the reference corpus in this process and neither make nor require any information about the relevance of individual documents. We merely use the documents of the collection as an empirical test of the scoring potential of the topic profile. After scoring, we identify the rank point that corresponds to the expected ratio of relevant documents for the collection. The score on that document is used as the initial score threshold for the filter. As a concrete example, if we project the delivery ratio to be 1-in-1,000 and we have 10,000 documents in the reference collection, we would use the score on the document at rank 10 as the initial score for the filter threshold.</p><p>Given that we calibrated on TREC-8 and TREC-9 tasks, where observed delivery ratios average approximately 1in-10,000 (TREC-9 = 0.000173 and TREC-8 = 0.00019), we began the TREC-2001 task with default assumptions of delivery that were far out of line with the actual density of topics in the Reuters 1996 Test Collection. In fact, the average density of topics in Reuters is approximately 1-in-100 (0.0125), nearly two orders of magnitude greater than in the collections we have seen in previous TREC tasks.</p><p>This discrepancy between our initial expectations (and the only ones that we might legitimately make) and the actual topic density in Reuters is an immediate source of error in our processing. It might underscore one criticism of the Reuters collection-or at least the use of Reuters subject categories in that collection-as a test bed for adaptive filtering, namely, that such "topics" with such high densities are poor representatives of real-world adaptive filtering tasks.</p><p>This problem in delivery-ratio expectations can also be regarded as an indication of a flaw in the user model we (as a group) have adopted for TREC adaptive filtering. In that model, we assume that a simple utility functionbalancing the value of true versus false positives, and possibly taking into account false negatives-can represent the target outcome of a process. It is clear, however, that some expectations of delivery are also critical and are very likely a part of any user's set of expectations on filter performance.</p><p>Note, it is possible to criticize a system that requires a delivery-ratio setting to perform well in contrast to one that does not. Any system that can perform well without such a setting is to be preferred to one that cannot-ceteris paribus, by Occam's Razor alone. However, it is not clear that any of the more successful adaptive filtering systems that participated in TREC 2001 experiments are such systems. In fact, these better systems seem to have modeled the delivery ratio quite accurately. One wonders how such a model might have been developed on the basis of the topic statement and two sample documents alone. Of course, it is possible that such systems were simply initialized with expectations of 1-or 2-in-100 documents as candidate density. If so, these were lucky choices, indeed. And, of course, if these were just good guesses, it still remains to determine how such good guessing might be ensured, in principal, in filtering over other streams/collections, such as the ones we saw in TREC-8 and TREC-9 tasks, or such as occur in real-world applications, where information about expected density of a topic in the possibly many data streams that are accessed is not available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Follow-Up Experiments</head><p>Recognizing that our system suffered from the inappropriate expectations of density we used, we decided to re-run the experiments with explicitly different deliveryratio settings. In particular, we wanted to assess the inherent strength (or weakness) of the system without the artificial constraint imposed by inappropriate delivery-ratio assumptions.</p><p>In a set of follow-up experiments, we re-set a variety of parameters to accommodate the special conditions of Reuters topics. We used a delivery-ratio expectation of 2.5% (0.025) to model the relatively frequent occurrence of topics. This was designed to insure that we would commence filtering with a lower expected score threshold. But given the extraordinarily high ratios of relevant documents for many topics, we might well find the lowered thresholds to be still too high. We hypothesize that, when we expect high density of a topic in a stream, we should expect any small number of sample documents (e.g., 2) to be extremely under-representative of the topic and to create a high-score bias. This is because features (terms) extracted from such non-representative documents will emphasize the distinct characteristics of those documents and will tend to select and score highly only the small subset of similar documents that share their biases. In such cases, we should depress the lower-bound score further, at least until we have achieve a feedback sample of sufficient size to insure that topic-representation biases are minimized.</p><p>As a test of this hypothesis we used lowered beta and gamma values to retard the convergence on a stable, high (optimal) threshold score. (Note that a beta = 0 would essentially deliver any document that matched on any of the features in the profile.) We also delayed the profile updates until we had accumulated sufficient judgments to yield nine true positives (along with any false positives that also were delivered in the interval). And, finally, we introduced a new parameter, mu, to serve as a coefficient on the filter threshold. For 0 &lt; mu &lt; 1, this effectively further lowers the threshold to allow more documents to be delivered for judgment. The results of these follow-up experiments, given in Table <ref type="table" coords="4,54.05,67.39,5.07,8.44" target="#tab_2">4</ref> for the Runs labeled CLT10F01-04, demonstrate immediate, dramatic improvements. Compared to the official runs (Table <ref type="table" coords="4,130.14,88.27,3.49,8.44">2</ref>), the improvement in performance is nearly 100% for T10SU, 250%+ for T10F, and approaching 800% for T10U. Note that these results do not reflect the effect of different term selection (or numbers of terms selected), rather derive only from (1) assuming a more appropriate delivery ratio, (2) lowering the rate of convergence on an "optimal" utility point, (3) postponing updates, and (4) further reducing the threshold.</p><p>Still, the results are sub-optimal and not at the level of the best-performing systems. We suspect that several factors are interacting to limit performance, including the fact that our core process is geared to retrieval performance and not classification. Thus, we did not model negative or border cases explicitly in developing topic profiles. In addition, the Reuters topics are quite vague and in some cases diffuse, in the sense of having a variety of subtopics. We believe that such cases are best treated with complex filters, not simple ones, capable of modeling the topic structure directly. We offer more specific thoughts on this point in the following section, in our discussion of topic-specific optimization strategies in batch filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Batch Filtering</head><p>Traditional information retrieval approaches to batch filtering have tended to represent a category or topic using a single or monolithic filter (model) that is extracted from positive examples of the category. However, both empirical and theoretical studies in other fields such as machine learning have shown that using multiple models or ensembles of models can lead to improved performance given some weak assumptions about the constituent models <ref type="bibr" coords="4,131.56,429.79,10.55,8.44" target="#b2">[3,</ref><ref type="bibr" coords="4,142.12,429.79,7.04,8.44" target="#b3">4,</ref><ref type="bibr" coords="4,149.15,429.79,7.04,8.44" target="#b4">5,</ref><ref type="bibr" coords="4,156.19,429.79,7.04,8.44" target="#b5">6]</ref>. Hansen and Salamon <ref type="bibr" coords="4,260.69,429.79,9.98,8.44" target="#b2">[3]</ref> proved that, given an ensemble of models in which the error rate of each constituent model is better than random and where each constituent model makes errors completely independent of any other, the expected ensemble error decreases monotonically with the number of constituent models. As examples of these theoretical claims, empirical studies in the field of machine learning have shown that, when weak or unstable learning algorithms, such as C4.5, are used in conjunction with ensemble techniques, the performance of these approaches can be improved significantly <ref type="bibr" coords="4,221.77,543.56,10.03,8.44" target="#b3">[4,</ref><ref type="bibr" coords="4,231.80,543.56,6.69,8.44" target="#b7">8]</ref>.</p><p>The improved performance gained from using ensemble approaches can be attributed to avoiding risks that arise from using a single model. These risks can be statistical in nature, where more than one statistical solution exists (stability). They can be algorithmic in nature, e.g., with high risk of getting stuck in local minima models. They can be representational in nature, e.g., when the space of representable models is infinite. In addition, some concepts can be very diverse and can be more accurately modeled using multiple models. Though the use of ensemble models is a relatively new, active, and very promising field of research in machine learning, very little work in information retrieval has incorporated the notion of ensemble models.</p><p>Our TREC-2001 experiments were designed explicitly to explore some of the issues in the use of ensemble filters for batch filtering. The arguments for using complex (non-unitary) filters are intuitively compelling. We recognize (a) that no single term-selection method works uniformly well for all topics and (b) that some topics are best modeled as "dispersed"-not based on a single set of features, but possibly a family of distinct sub-features. This would seem to suggest that multiple representations (hence, multiple filters) are needed. Thus, we created an approach that optimized filters on a topic-by-topic basis according to feature extraction method and filter structure.</p><p>In particular, in this heterogeneous approach, filters for each topic were unique: each topic's features were derived by one of five different feature extraction techniques and each was modeled by either (i) a single (monolithic) filter, or (ii) a family of four, parallel (multiplexed) filters, or (iii) a set of n (cascaded) filters sequenced so that each filter after the first considered only the fallout (below-thresholdscoring) documents of the preceding filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">General Description of Ensemble Batch Filtering</head><p>Ensemble filtering explores the general idea of constructing many weak or focused filters and combining these into a single highly accurate filter (using, for example, voting) in order to filter or classify an unlabeled document. Ensemble filters can be constructed and combined using various techniques that have been proposed and empirically demonstrated in the fields of machine learning and statistics. Construction approaches vary widely but can generally be placed into three broad categories: data-related methods (such as bagging and boosting); representation-based methods (such as constructive induction and alternative representations of the output space, such as error correcting output codes); and approaches that differ based upon the hypothesis search strategies employed. When it comes to aggregating the constituent filters of an ensemble, various strategies can be used, such as voting strategies as in multiplexing, a cascade (or waterfall) aggregation strategy, or aggregation strategies that are learned, as in stacked generalization <ref type="bibr" coords="4,382.60,725.97,9.46,8.44" target="#b8">[9]</ref>. A multiplex filter is a filter made up of constituent filters Fi, where the multiplex filter accepts the unlabeled document (and classifies it as positive) based on some interpretation of the independent scoring of each constituent filter Fi. In fact, there are many possible, alternative methods for interpreting the score of a set of filters, ranging from some simple combination of binary outcomes (e.g., the sum of the "votes" of each filter) to a weighted, possibly nonindependent scoring based on the interaction of filters. In our experiments, we chose the simplest approach and accepted a document if the document was accepted by any one of the constituent filters.</p><p>On the other hand, a cascade filter is an ensemble filter that consists of an ordered list of filters {F1, …, Fn}, where each filter, Fi, consists of two outputs: one corresponding to the positive class and the other corresponding to the negative or fallout class. Each constituent filter Fi is linked to the fallout class of the filter Fi-1. An unlabeled document is processed by each filter Fi in left-to-right fashion. Should any filter accept the document, processing terminates and the unlabeled document is accepted by the ensemble filter. Otherwise, the subsequent filter Fi+1 processes the unlabeled document in a similar fashion. This process repeats until either some constituent filter has accepted the document or no filter has.</p><p>In the case of ensemble filters, we used two different construction approaches for our TREC-2001 submitted runs (both of which are outlined below): one based upon the iterative modeling of fallout examples, which is a simplified version of boosting; the other based upon crossvalidation, which is a simplified version of bagging. We used n-fold cross-validation <ref type="bibr" coords="5,436.11,352.99,10.22,8.44" target="#b6">[7]</ref> to choose the construction and aggregation method and make other representational decisions, such as which of several term-extraction methods and term counts to use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Cross-Validation to Construct Monolithic Filters</head><p>For ease of presentation, prior to describing ensemble filter construction algorithms, we review how crossvalidation was used to construct monolithic filters. Monolithic filters served as our baseline submission. The presentation is made more concrete by using the TREC-2001 filtering problem-the Reuters 1996 dataset.</p><p>For all submissions, the training corpus was divided into four folds. More specifically, the Reuters 1996 training corpus of twelve days was partitioned into four subsets denoted by Q1, Q2, Q3, and Q4, where each quarter consisted of a non-overlapping sequential sampling of a subset of the full training dataset.</p><p>Monolithic filters were constructed using either (a) the topic descriptions alone, which we subsequently refer to as "topic filters," or (b) the training corpus. To construct topic filters, we extracted filter terms from the topic descriptions and set thresholds using a beta-gamma optimization on three quarters of the data, while the unseen quarter was used as a blind test. This led to a utility measure for the test quarter. This experiment was repeated for each quarter, thereby generating four utility measures U1, U2, U3, U4. The average was taken of these four utility measures resulting in a utility measure, AvgTopicU, for the corresponding topic filter.</p><p>On the other hand, when constructing monolithic filters from training examples, we examined the results of various thesaurus extraction methods and corresponding term counts and chose an optimal filter representation based upon its cross validation performance. See Table <ref type="table" coords="5,551.34,746.11,5.07,8.44">5</ref> Table <ref type="table" coords="5,344.46,268.51,3.67,8.44">5</ref>. Term Count and Extraction Method Combinations Figure <ref type="figure" coords="5,84.29,743.47,3.78,8.44">3</ref>. Term-Extraction Formulae for a list of all combinations of thesaurus extraction methods and term counts that were examined for our TREC-2001 submissions. Note that the label "CC" in the Table <ref type="table" coords="6,79.26,88.27,26.63,8.44">stands</ref> for "CLARIT Classic," a proprietary termextraction method. Our implementations of the methods we refer to as "Rocchio" ("Roc"), "RocchioFQ" ("RocFQ"), "Prob1," and "Prob2," are given in Figure <ref type="figure" coords="6,218.68,119.23,3.79,8.44">3</ref>.</p><formula xml:id="formula_0" coords="5,59.32,460.87,203.23,123.31">R (t) TF x IDF(t) Rocchio(t) DocSet D D ∑ ∈ = R (t) F x IDF(t) t) RocchioFQ( DocSet D D ∑ ∈ = 1) R 1 R log( 1) 1 R N 2 R N log( Prob1(t) t t t - + - - + - + - = Prob1(t) * 1) log(R Prob2(t) t + =</formula><p>In practice, given a training dataset that is partitioned into four folds, Q1, Q2, Q3, and Q4 (for the purposes of our experiments), this optimization procedure translates into taking each combination of thesaurus extraction method E and number of terms N and performing the following steps:</p><p>For each topic T 1. Repeat steps 2 to 6 for all combinations {E, N} listed in Table <ref type="table" coords="6,139.25,238.99,3.79,8.44">5</ref>, thereby generating an average utility for each combination.</p><p>2. Do thesaurus extraction on Q1+Q2 with the {E, N} combination.</p><p>3. Optimize the threshold for T using beta-gamma threshold optimization over Q3.</p><p>4. Do a blind test on Q4 generating a utility value U4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Repeat steps 1 to 4 for alternative combinations of Q1, Q2, Q3, Q4, insuring that each database subset is used as a blind test at most once. This leads to a utility value for each database subset of U1, U2, U3, U4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Set average utility for this combination of {E, N}</head><p>AvgMonoU to Average(U1, U2, U3, U4).</p><p>7. Select the combination {E, N} that provides the highest average utility as the optimal means of generating a monolithic filter for this topic T.</p><p>A utility measure for each fold of the dataset can be generated using various combinations of extraction folds and optimization folds, however, for our experiments, we limited our exploration to the following: {Q1=3, Q2=4, Q3=2, Q4=1; Q1=1, Q2=3, Q3=4, Q4=2; Q1=2, Q2=4, Q3=1, Q4=3; Q1=1, Q2=2, Q3=3, Q4=4}, where 1, 2, 3, and 4 denote the folds in the training dataset and Qi denote the variables used in the above algorithm. In deciding between modeling a topic using a monolithic filter or a topic filter, we choose the filter with the highest average utility scores on the four-fold datasets (i.e., AvgMonoU and AvgTopicU). Prior to running the selected filters on the eleven months of test data, the system retrains each filter using the entire twelve days of training, where the filter thresholds are set using the beta-gamma method on linear utility, T10U, over the entire training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Ensemble Filter Construction Algorithms</head><p>For our TREC-2001 submissions we developed one construction algorithm for each of the two ensemble-filter types used. Since the construction algorithm for multiplex filters is closely related to that for monolithic filter construction (described above), we begin by presenting multiplex filter construction. This algorithm is a simplified version of bagging, whereby each filter is constructed from a sampled subset of the training data based upon an n-fold partitioning of the data. This is in contrast to the more commonly used approach for bagging, where each filter is constructed from a randomly generated dataset. In this case each filter's training dataset is generated by randomly drawing, with replacement, a specified number of examples from the training dataset (typically equal to the size of the training data). We adapted the simpler strategy based on n-folds due to time and system limitations.</p><p>For our current experiments, each topic was modeled using a multiplex filter consisting of four component filters (unless there was insufficient training data for the topic), where each filter was constructed using steps 2 to 6 in the algorithm for constructing monolithic filters (above), i.e., one filter corresponding to each fold in the training data.</p><p>Unlike the monolithic run (where the final monolithic filters were trained on the entire training dataset), the component filters in the multiplex filter were trained on two quarters of the training data, while the threshold was optimized using the beta-gamma method on the third quarter.</p><p>Figure <ref type="figure" coords="6,352.16,284.83,5.07,8.44">4</ref> gives a screen shot of a multiplex filter that was constructed for topic 39 using the CLARIT AW Toolkit. It presents a multiplex filter consisting of four component filters (each depicted as a node) that were constructed using four different subsets of the training dataset. The folds that were used to generate these subsets are depicted as nodes in the top portion of the screen shot.</p><p>On the other hand, the construction algorithm for cascade filters in our TREC2001 submissions is a simplified version of boosting (a version of boosting based upon sampling). The focus of these methods is to produce a series of filters. The training set used for each filter in the series is chosen based on the performance of earlier filters in the series. In boosting, examples that are incorrectly classified by previous filters in the series are chosen more often to train subsequent filters than examples that were correctly classified. Thus boosting attempts to generate filters that are better able to predict examples for which the current ensemble's performance is poor.</p><p>For our experiments, we adapted a simplified, rather radical, approach to boosting, whereby each example that was correctly modeled using the current ensemble was not used in the construction of subsequent filters. As a result of this simplification, different stopping criteria were required to ensure termination of the algorithm. These are presented subsequently.</p><p>The approach to constructing a cascade filter for a topic T involves a number of steps and assumes as input three datasets D1, D2, and D3, which are respectively used for thesaurus extraction, threshold optimization, and blind testing. These datasets could correspond to the following folds in the training data: Q1+Q2, Q3, and Q4 respectively. In this case the training dataset is split into four subsets/folds. The algorithm consists of two threads: the extraction thread and the threshold setting or optimization thread. Each thread results in the construction of its own cascade filter, namely, CExtraction and COpt. The algorithm is presented in stepwise fashion in Figure <ref type="figure" coords="6,482.68,698.83,3.79,8.44">6</ref>, where the left and right hand sides of the figure depict the extraction thread and optimization thread respectively. The algorithm is iterative in nature, whereby the first filter in the cascade, C1Extraction, is constructed using the positive topic  <ref type="figure" coords="9,276.28,57.07,3.49,8.44">6</ref>). This cascade corresponds to the extraction cascade CExtraction. In order to set the threshold for C1Extraction, a second cascade filter (i.e., the optimization cascade) is constructed. The first constituent filter in this cascade is simply a copy of C1Extraction and is denoted as C1Opt. To avoid clutter in Figure <ref type="figure" coords="9,142.84,119.23,3.67,8.44">6</ref>, the Extraction and Opt suffixes are dropped from the component filters names. The threshold for C1Opt is set using the beta-gamma method based upon the linear utility over the optimization dataset D2 (Step 2 in Figure <ref type="figure" coords="9,137.34,160.51,3.48,8.44">6</ref>). Step in Figure <ref type="figure" coords="9,112.85,326.11,3.49,8.44">6</ref>). Subsequently this cascade filter is applied to the D3 dataset and a utility measure is obtained (Last Step in Figure <ref type="figure" coords="9,135.18,346.99,3.49,8.44">6</ref>). The above procedure is repeated such that each quarter of the training dataset serves as a blind test (D3) at most once. This results in a utility measure for each quarter. An average of these four values is then taken, resulting in an average utility, AvgCascU, for this cascade filter. This value serves as a comparison to other approaches used for modeling a topic. Should the cascade filter be chosen (based upon average utility) to model a topic, the cascade filter is first re-trained using three quarters of the data for extraction (D1), while the fourth quarter is used for threshold optimization (D2) and a blind test is carried out on the test dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Test Configuration</head><p>We submitted two batch filtering runs: one where each topic was represented by a single or monolithic filter (CLT10BFA); the other, which we call the "rainbow run" (CLT10BFB), where each topic was represented by using one (the best) of either a topic, monolithic, multiplex, or cascade filter. Note that for both submission runs, beta was set to 0.1 and gamma was set to 0.05. The minimum number of documents required for filter construction in ensemble filters was set to five. In both cases, we chose the final representation for a topic based on which of the alternative choices yielded the highest average crossvalidation utility.</p><p>In the construction of cascaded filters, the representation of each constituent filter was globally set to the optimally determined representation for monolithic filters. Since the average cross-validation utility for both monolithic and multiplex filters corresponds to the same value, a further evaluation criterion was necessary. To decide between these two approaches, we re-trained a corresponding monolithic filter using the entire twelve days of training. The beta-gamma method was used to optimize the threshold of the resulting monolithic filter. This re-trained filter was subsequently used for retrieval over the twelve days of training data and a corresponding global utility was calculated. Similarly, the extracted multiplex filter was used to filter documents from the twelve days of training and a corresponding global utility calculated. The approach with the highest global utility was chosen as the approach to model the associated topic.</p><p>As a benchmark for our ensemble approaches, we carried out a batch filtering experiment using our traditional information retrieval system in conjunction with an optimization strategy for identifying the term extraction method and term count similar to that outlined above. This experiment modeled each topic using a monolithic filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Test Results</head><p>Table <ref type="table" coords="9,349.24,295.16,5.07,8.44" target="#tab_4">6</ref> presents a summary of various batch filtering runs in terms of the linear utility (T10U) and normalized linear utility (T10SU). Row one of this table presents the median of all submitted runs (from all groups) for TREC-2001 batch filtering. The second and third rows summarize the results for our two submitted runs. Our official heterogeneous or rainbow submission had an average normalized utility of 0.152 and F-utility of 3371. The CLT10BFC run represents the result of our benchmark run, which was inadvertently not submitted.</p><p>In the context of this year's task, our Batch results are weak. In follow-up analyses, we determined that some of the performance shortfalls were due to processing errors under our control. For example, the test data had become corrupted in our translation of the NIST sources into our processing format; this led to our losing actual test documents, which naturally limited our results in some cases. In another more serious case, we inadvertently failed to reset a critical system parameter-one that sets an upper bound on the number of documents that are considered in any set of documents to be compared to a topic profile-when the system moved from training to testing data. In effect, we only considered about one-third of the test documents that should have been considered as candidates for matching each topic. This particular problem affected both our routing-based baseline run and our heterogeneous run. When we corrected these problems and re-ran over the correct version of the testing data, we saw immediate improvement in both runs. In particular the heterogeneous approach improved by about 60% (from 0.152 to 0.239 normalized utility), making it essentially indistinguishable from our routing-based results. The CLT10BFA2, CLT10BFB2, and CLT10BFC2 runs correspond to re-runs of our official submissions and our benchmark run, respectively, where both the dataset errors and the critical retrieval parameter have been corrected. Here, our benchmark run yields a performance of 0.257.</p><p>The remainder of Table <ref type="table" coords="9,420.31,709.17,5.07,8.44" target="#tab_4">6</ref> relates to some post-TREC experiments that addressed various problems that occurred during the preparation of our final submissions.</p><p>These experiments were all conducted on the corrected dataset and with properly set retrieval parameters.</p><p>GlobalCascade relates to a run where each topic is represented using a cascade filter where the extraction method and associated term count for each constituent filter in the cascade has a fixed setting. In particular, all constituent filters in the cascade are given the same settings as were determined to be optimal for the corresponding monolithic filter for the topic (in CLT10BFA2). LocalCascade denotes an experiment where each topic is represented using a cascade filter. In this case the extraction method and associated term count for each constituent filter in the cascade is optimized locally. Multiplex relates to an experiment where each filter is represented using a multiplex filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Observations</head><p>Our work in batch filtering this year represented an initial attempt at the construction of ensemble filters. Due to system limitations and time constraints, our experiments were accomplished using simplified versions of bagging and boosting algorithms for the construction of multiplex and cascade filters respectively. Even though the performance of the current system is only comparable to the TREC median, performance should improve with full implementations of bagging and boosting algorithms along with more comprehensive experimentation. Current work is addressing both of these issues.</p><p>Though our proposed approaches to ensemble filters model subtopic structure, albeit in a limited fashion, a more natural means of identifying and thereby representing topic structure can be achieved using clustering. This forms a very important part to our current work in this area.</p><p>Our analysis suggests that ensemble filters perform better than monolithic filters for certain classes of topics. To take advantage of the potential boost in performance that would come from topic structuring, any operational system would have to be able to predict whether a particular filtering task (topic) is best modeled via a monolithic or an ensemble filter. Our hypothesis is that, if there is sufficient training data (or relevance judgments in a running filter), then multiplex filters will outperform monolithic ones. In such cases, then, the task becomes choosing between multiplex and cascade approaches.</p><p>Our hypothesis is that cascading is optimal when the topic is vague or diffuse. Thus, to make a principled decision about which approach to select, we need a means of diagnosing topic structure. We have begun work on the development of a simple method to accomplish this.</p><p>In our report at the TREC 2001 Meeting, we described retrospective results that simulated such an ideal choiceessentially, the best performing method for each topic as seen in the homogeneous runs represented by our baseline monolithic run (CLT10BFB2), our multiplex run (Multiplex), and our cascade run (GlobalCascade)-which we called "MadMax." A striking feature of the MadMax simulation run was the remarkable performance of cascade filters in the case of twelve topics, significantly exceeding the reported official TREC maximum results.</p><p>We feel obligated to report now that, in work since the time of the Meeting, we have not been able to duplicate the extraordinary performance of the cascade runs and now believe we had corrupted data in reporting those results.</p><p>In repeated experiments with cascade filters-albeit with the limited conditions we employed in our original runswe have achieved individual topic performance that equals or exceeds the reported TREC maximum on six topics; but only one of these is significantly better than the maximum.</p><p>We continue to believe that successful, robust filtering (especially in distinction to classification) will require topicspecific optimizations, including topic structuring. We have only just begun to explore this problem. We will continue to develop topic-structuring techniques and apply them in future TREC experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,326.21,740.35,182.41,8.44"><head>Figure 1</head><label>1</label><figDesc>Figure 1. Beta-Gamma Threshold Regulation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,326.21,284.35,201.60,8.44;4,326.21,294.67,165.02,8.44"><head>FilterFigure 2 .</head><label>2</label><figDesc>Figure 2. Schematic Representation of a Multiplex (Parallel) and Cascade (Sequential) Filter</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,66.53,608.97,25.48,7.33;5,69.64,644.25,205.08,7.57;5,104.68,653.61,81.20,7.33;5,85.00,666.81,182.28,8.05;5,82.37,679.29,159.20,7.57;5,105.17,688.41,90.04,7.33;5,83.80,703.53,130.20,7.33;5,82.12,715.29,193.24,7.33;5,64.12,625.53,60.56,8.77;5,136.61,633.46,1.17,3.91;5,131.57,631.29,5.23,6.70;5,132.04,621.21,5.23,6.70"><head></head><label></label><figDesc>) = Frequency of term t or how many times the term appears in document d N = Number of documents in the reference corpus Nt = Number of documents in the reference corpus that contain term t R = Number of relevant documents Rt = Number of relevant documents containing term t IDF (t) = 1 + log t N N</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,80.21,284.35,414.47,8.44;7,58.13,78.01,504.00,195.12"><head>Figure 4 .Figure 5 . 1 :Step 3 :Step 4 :Step 6 :Figure 6 .</head><label>4513466</label><figDesc>Figure 4. Example of a Multiplex Filter for Topic 39, with Performance Illustrated on the Training Corpus</figDesc><graphic coords="7,58.13,78.01,504.00,195.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="9,54.04,367.63,229.46,8.44;9,54.04,377.95,207.39,8.44;9,54.04,388.27,233.10,8.44;9,54.04,398.59,24.61,8.44"><head>Figure 5</head><label>5</label><figDesc>Figure 5 presents a screen shot of the extraction cascade for topic 39 of the Reuters Corpus. Each filter in the cascade, in this figure, is associated with a precision-recall curve.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,74.93,87.40,461.74,76.18"><head>A Report on CLARIT TREC-2001 Experiments David A. Evans, James Shanahan, Xiang Tong, Norbert Roma, Emilia Stoica, Victor Sheftel, Jesse Montgomery, Jeffrey Bennett, Sumio Fujita * , Gregory Grefenstette Clairvoyance Corporation, Pittsburgh, PA &amp; * Justsystem Corporation, Tokushima, Japan</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,324.05,58.53,239.57,150.02"><head>Table 4 .</head><label>4</label><figDesc>Results of Post-TREC Adaptive-Filtering Experiments</figDesc><table coords="3,325.25,58.53,238.37,114.50"><row><cell></cell><cell cols="4">CLT10F01 CLT10F02 CLT10F03 CLT10F04</cell></row><row><cell>Del-Ratio</cell><cell>0.025</cell><cell>0.025</cell><cell>0.025</cell><cell>0.025</cell></row><row><cell>Beta</cell><cell>0.15</cell><cell>0.15</cell><cell>0.15</cell><cell>0.15</cell></row><row><cell>Gamma</cell><cell>0.01</cell><cell>0.01</cell><cell>0.005</cell><cell>0.005</cell></row><row><cell>Update</cell><cell>9</cell><cell>9</cell><cell>9</cell><cell>9</cell></row><row><cell>Mu</cell><cell>1.0</cell><cell>0.9</cell><cell>1.0</cell><cell>0.9</cell></row><row><cell>T10U</cell><cell>1716.8</cell><cell>1678.9</cell><cell>1714.9</cell><cell>1679.6</cell></row><row><cell>T10SU</cell><cell>0.1003</cell><cell>0.0843</cell><cell>0.0983</cell><cell>0.0813</cell></row><row><cell>T10F</cell><cell>0.1980</cell><cell>0.2097</cell><cell>0.2057</cell><cell>0.2148</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,54.04,160.51,233.79,163.72"><head></head><label></label><figDesc>The threshold of the C1Extraction filter is set to the optimized threshold of C1Opt. Subsequently, the fallout documents from C1 (i.e., positive examples from D1 that are rejected by C1Extraction) are used to construct the second filter C2Extraction in the cascade, provided various continuation conditions are met. These include: the number of documents in the fallout of C1Extraction is greater than a minimum number of documents required to construct a filter; the linear utility of the C1Opt over the optimization dataset is positive. The above steps of constituent filter extraction and threshold optimization (on the fallout of each preceding filter) are repeated as long as the continuation conditions are satisfied. Once any continuation conditions fail, all the positive outputs of the constituent filters of the extraction cascade CExtraction are connected to a union filter (2 nd -Last</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,51.89,58.51,488.88,679.44"><head>Table 6 .</head><label>6</label><figDesc>Results of Post-TREC Batch Experiments</figDesc><table coords="10,56.69,58.51,219.95,141.40"><row><cell cols="2">Description</cell><cell cols="2">T10SU T10U</cell></row><row><cell cols="2">Median for all Submitted Runs</cell><cell>0.256</cell><cell>N/A</cell></row><row><cell>Submitted Results</cell><cell>CLT10BFA CLT10BFB</cell><cell cols="2">0.147 0.152 3371 N/A</cell></row><row><cell></cell><cell>CLT10BFA2</cell><cell cols="2">0.237 5834</cell></row><row><cell></cell><cell>CLT10BFB2</cell><cell cols="2">0.210 4925</cell></row><row><cell></cell><cell>CLT10BFC</cell><cell cols="2">0.234 5453</cell></row><row><cell>Post-TREC Runs</cell><cell>CLT10BFC2</cell><cell cols="2">0.257 5895</cell></row><row><cell></cell><cell>GlobalCascade</cell><cell cols="2">0.220 5323</cell></row><row><cell></cell><cell>LocalCascade</cell><cell cols="2">0.195 4882</cell></row><row><cell></cell><cell>Multiplex</cell><cell cols="2">0.225 5665</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method X Terms</head><note type="other">CC</note></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,342.04,429.80,186.62,8.44;10,342.04,440.12,210.14,8.44;10,342.04,450.44,188.91,8.44;10,342.04,460.76,210.29,8.44;10,342.04,471.08,207.26,8.44;10,342.04,481.40,176.29,8.44" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,346.36,440.12,199.02,8.44">Threshold Calibration in CLARIT Adaptive Filtering</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,352.36,450.44,121.47,8.44;10,515.32,450.44,15.63,8.44;10,342.04,460.76,180.13,8.44">The Seventh Text REtrieval Conference (TREC-7)</title>
		<meeting><address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<publisher>U.S. Government Printing Office</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="149" to="156" />
		</imprint>
	</monogr>
	<note>EM Voorhees and DK Harman</note>
</biblStruct>

<biblStruct coords="10,342.04,491.72,195.75,8.44;10,342.04,502.04,209.68,8.44;10,342.04,512.60,188.91,8.44;10,342.04,522.92,202.84,8.44;10,342.04,533.24,207.26,8.44;10,342.04,543.56,176.29,8.44" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,346.14,502.04,198.78,8.44">Optimization in CLARIT TREC-8 Adaptive Filtering</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Roma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,352.36,512.60,121.47,8.44;10,515.32,512.60,15.63,8.44;10,342.04,522.92,172.69,8.44">The Eighth Text REtrieval Conference (TREC-8)</title>
		<meeting><address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<publisher>U.S. Government Printing Office</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="253" to="258" />
		</imprint>
	</monogr>
	<note>EM Voorhees and DK Harman</note>
</biblStruct>

<biblStruct coords="10,342.04,553.88,195.84,8.44;10,342.04,564.20,215.31,8.44" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,477.40,553.88,60.48,8.44;10,342.04,564.20,41.38,8.44">Neural network ensembles</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Salamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,390.76,564.20,111.48,8.44">IEEE Transactions on PAMI</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="993" to="1001" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,342.04,574.52,175.95,8.44;10,342.04,584.84,195.68,8.44;10,342.04,595.40,73.10,8.44" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
			<affiliation>
				<orgName type="collaboration">C4.5</orgName>
			</affiliation>
		</author>
		<author>
			<persName coords=""><surname>Bagging</surname></persName>
			<affiliation>
				<orgName type="collaboration">C4.5</orgName>
			</affiliation>
		</author>
		<title level="m" coord="10,342.04,584.84,195.68,8.44;10,342.04,595.40,44.21,8.44">Proc.Fourteenth National Conference on Artificial Intelligence</title>
		<meeting>.Fourteenth National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,342.04,605.72,187.81,8.44;10,342.04,616.04,157.81,8.44" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,397.72,605.72,128.62,8.44">The strength of weak learnability</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,342.04,616.04,69.91,8.44">Machine Learning</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="197" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,342.04,626.36,203.65,8.44;10,342.04,636.69,84.13,8.44" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bagging</forename><surname>Predictors</surname></persName>
		</author>
		<title level="m" coord="10,471.88,626.36,69.66,8.44">Machine Learning</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="123" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,342.04,647.01,184.09,8.44;10,342.04,657.33,212.47,8.44;10,342.04,667.65,61.09,8.44" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,415.89,647.01,110.24,8.44;10,342.04,657.33,141.58,8.44">Cross-validatory choice and assessment of statistical predictions</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,493.96,657.33,60.55,8.44;10,342.04,667.65,6.08,8.44">Journal of RSS B</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="111" to="147" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,342.04,678.21,206.77,8.44;10,342.04,688.53,185.54,8.44;10,342.04,698.85,133.81,8.44" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,503.30,678.21,45.51,8.44;10,342.04,688.53,142.86,8.44">A boostingbased system for text categorization</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Boostexter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,493.72,688.53,33.86,8.44;10,342.04,698.85,33.40,8.44">Machine Learning</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="135" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,342.04,709.17,214.32,8.44;10,342.04,719.49,73.58,8.44" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,378.28,709.17,104.73,8.44">DH Stacked generalization</title>
		<author>
			<persName coords=""><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,489.64,709.17,66.72,8.44">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="241" to="259" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
