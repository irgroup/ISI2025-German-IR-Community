<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,170.25,75.02,270.94,12.83">Hummingbird SearchServer™ at TREC 2001</title>
				<funder ref="#_f8e2bdF #_92QRrtr">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2002-02-04">February 4, 2002</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,264.75,103.59,78.61,8.78"><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
						</author>
						<author>
							<persName coords="1,277.50,114.84,55.48,8.78;1,255.75,126.09,62.85,8.78"><roleName>Ontario</roleName><forename type="first">Hummingbird</forename><surname>Ottawa</surname></persName>
						</author>
						<title level="a" type="main" coord="1,170.25,75.02,270.94,12.83">Hummingbird SearchServer™ at TREC 2001</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2002-02-04">February 4, 2002</date>
						</imprint>
					</monogr>
					<idno type="MD5">0FC2FA3B5BA34A6FE29B12EAE8B9BFAB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hummingbird submitted ranked result sets for the topic relevance task of the TREC 2001 Web Track (10GB of web data) and for the monolingual Arabic task of the TREC 2001 Cross-Language Track (869MB of Arabic news data). SearchServer's Intuitive Searching™ tied or exceeded the median Precision@10 score in 46 of the 50 web queries. For the web queries, enabling SearchServer's document length normalization increased Precision@10 by 65% and average precision by 55%. SearchServer's option to square the importance of inverse document frequency (V2:4 vs. V2:3) increased Precision@10 by 8% and average precision by 12%. SearchServer's stemming increased Precision@10 by 5% and average precision by 13%. For the Arabic queries, a combination of experimental Arabic morphological normalizations, Arabic stop words and pseudo-relevance feedback increased average precision by 53% and Precision@10 by 9%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Hummingbird SearchServer<ref type="foot" coords="1,186.00,359.14,3.38,6.08" target="#foot_1">2</ref> is an indexing, search and retrieval engine for embedding in Windows and UNIX information applications.</p><p>SearchServer, originally a product of Fulcrum Technologies, was acquired by Hummingbird in 1999. Founded in 1983 in Ottawa, Canada, Fulcrum produced the first commercial application program interface (API) for writing information retrieval applications, Fulcrum® Ful/Text ™ . The SearchServer kernel is embedded in many Hummingbird products, including SearchServer, an application toolkit used for knowledge-intensive applications that require fast access to unstructured information.</p><p>SearchServer supports a variation of the Structured Query Language (SQL), called SearchSQL™ , which has extensions for text retrieval. SearchServer conforms to subsets of the Open Database Connectivity (ODBC) interface for C programming language applications and the Java Database Connectivity (JDBC) interface for Java applications. Almost 200 document formats are supported, such as Word, WordPerfect, Excel, PowerPoint, PDF and HTML. Many character sets and languages are supported. SearchServer's Intuitive Searching algorithms were updated for version 4.0 which shipped in Fall 1999, and in subsequent releases of other products. SearchServer 5.0, which shipped in Spring 2001, works in Unicode internally <ref type="bibr" coords="1,316.67,512.34,11.64,8.77" target="#b3">[4]</ref> and contains improved natural language processing technology, particularly for languages with many compound words, such as German, Dutch and Finnish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head><p>All experiments were conducted on a single-cpu desktop system, OTWEBTREC, with a 600MHz Pentium III cpu, 512MB RAM, 186GB of external disk space on one e: partition, and running Windows NT 4.0 Service Pack 6. An internal development build of SearchServer 5.0 was used for the official TREC runs in July 2001 (build 5.0.504.156), which for the web and Arabic tasks should give essentially the same rankings as the commercial release version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Setup</head><p>We describe how SearchServer was used to handle the topic relevance task of the TREC 2001 Web Track (10GB of web data) and the monolingual Arabic task of the TREC 2001 Cross-Language Track (869MB of Arabic news data).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data</head><p>The WT10g collection of the Web Track consists of pages downloaded from the World Wide Web in 1997. It was distributed on 5 CDs. We copied the contents of each CD onto the OTWEBTREC e: drive (e:\data\wt10g\cd1e:\data\wt10g\cd5). The cd5\info subdirectory, containing supporting information not considered part of WT10g, was removed to ensure it wasn't indexed. The 5157 .gz files comprising WT10g were uncompressed. No further pre-processing was done on the data. Uncompressed, the 5157 files consist of 11,032,691,403 bytes (10.3GB), about 2MB each. Each file contains on average 328 "documents", for a total of 1,692,096 documents. The average document size is 6520 bytes. For more information on this collection, see <ref type="bibr" coords="2,369.75,285.84,10.60,8.77" target="#b1">[2]</ref>.</p><p>Arabic Newswire A Corpus of the Cross-Language Track consists of articles from the Agence France Presse (AFP) Arabic Newswire from 1994-2000. It was distributed on 1 CD. We copied the contents of its TRANSCRIPTS directory to e:\data\Arabic. The 2337 . gz files comprising the corpus were uncompressed. No further preprocessing was done on the data. Uncompressed the 2337 files consist of 911,555,745 bytes (869 MB), about 370KB each. Each file contains on average 164 "documents", for a total of 383,872 documents. The average document size is 2375 bytes. For more information on this collection, see <ref type="bibr" coords="2,369.75,366.84,10.60,8.77" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Text Reader</head><p>To index and retrieve data, SearchServer requires the data to be in Fulcrum Technologies Document Format (FTDF). SearchServer includes "text readers" for converting most popular formats (e.g. Word, WordPerfect, etc.) to FTDF. A special class of text readers, "expansion" text readers, can insert a row into a SearchServer table for each logical document inside a container, such as directory or library file. Users can also write their own text readers in C for expanding proprietary container formats and converting proprietary data formats to FTDF.</p><p>Last year, for TREC-9, we wrote a custom text reader called cTREC to handle expansion of the library files of WT10g collection and to make a few conversions to the HTML format, described in <ref type="bibr" coords="2,422.08,495.09,15.48,8.77" target="#b10">[10]</ref>. We used cTREC again this year and made no significant changes regarding WT10g. This year, we will just describe how cTREC was extended for the Arabic collection.</p><p>The library files of the Arabic collection, like WT10g, consist of several logical documents, each starting with a &lt;DOC&gt; tag and ending with a &lt;/DOC&gt; tag. After the &lt;DOC&gt; tag, the unique id of the document, e.g. 19940513_AFP_ARB.0001, is included inside &lt;DOCNO&gt;..&lt;/DOCNO&gt; tags. The cTREC /E switch handles expansion of the Arabic library files into logical documents identically as for WT10g.</p><p>The Arabic documents contain SGML tags describing its structure (e.g. the headline is preceded by a &lt;HEADLINE&gt; tag and followed by a &lt;/HEADLINE&gt; tag). The Document Type Definition (DTD) which specified the tags and entities used in the documents was provided in the ldc_arabic.dtd file on the CD. When invoked without the /w or /E switch, cTREC by default inserts control sequences to turn off indexing around all tags listed in the Arabic collection DTD (and additionally tags listed in the TREC disk 1-5 DTDs, as described last year), and converts all entities listed in the DTDs (e.g. "&amp;AMP;" is converted to the ampersand "&amp;"). By default, cTREC also turns off indexing for data delineated by certain tags because its content isn't considered helpful (for the Arabic collection, data delineated by HEADER, FOOTER and TRAILER tags is not indexed). cTREC looks ahead at most 5000 bytes for an end tag when it encounters a tag indicating indexing should be turned off; if the end tag is not found, indexing is not turned off.</p><p>The Arabic documents are in the UTF-8 character set, a variable-width encoding of Unicode, for which ASCII characters are represented with 1 byte (e.g. the Latin letter A, which is hexadecimal value 0x0041 in the UTF-16 encoding of Unicode, is 1 byte in UTF-8 (hexadecimal 0x41 or decimal 65)), and non-ASCII characters are represented with 2 to 4 bytes (e.g. the Arabic letter ALEF, which is 0x0627 in UTF-16, is 2 bytes in UTF-8 (0xd8 0xa7)). cTREC passes through the bytes of the documents unchanged (aside from the control sequences inserted and entities converted as described previously). SearchServer's Translation Text Reader (nti), was chained on top of cTREC and the UTF8_UCS2 translation was specified via its /t option to translate from UTF-8 to the UTF-16 encoding desired by SearchServer's parser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Indexing</head><p>We created a SearchServer table called WT10GW for the web collection and two different SearchServer tables called ARAB01 and ARAB01A for the Arabic collection. For example, the SearchSQL statement to create the ARAB01A table was as follows: The stopfile differed for each table. For WT10GW, we used the same MYTREC.STP stopfile as last year, which contained 101 stopwords to not index, including all letters and single-digit numbers. For ARAB01, we did not use a stopfile. For ARAB01A, the stopfile MYARAB.STP did not actually contain any stopwords, but specified a nondefault option to the parser to apply experimental Arabic morphological normalizations to the words before indexing:</p><formula xml:id="formula_0" coords="3,108.00,306.62,167.85,7.66">CREATE SCHEMA ARAB01A CREATE</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PARSER="unicode/a=1"</head><p>The PARSER line of the stopfile specified the built-in unicode parser with the non-default option of a=1 which enables the experimental Arabic morphological normalizations. A powerful new feature of SearchServer 5.0 is the ability to "plug-in" a custom parser to extend or replace the default parser.</p><p>Into each table, we just inserted one row, specifying the top directory of the data set. e.g. for the ARAB01A table, we used this Insert statement:</p><p>INSERT INTO ARAB01A ( FT_SFNAME, FT_FLIST ) VALUES ( 'ARABIC', 'cTREC/E/d=128:s!nti/t=UTF8_UCS2:cTREC/@:s');</p><p>To index each table, we just executed a Validate Index statement such as the following:</p><formula xml:id="formula_1" coords="3,108.00,581.12,257.85,18.91">VALIDATE INDEX ARAB01A VALIDATE TABLE TEMP_FILE_SIZE 2000000000 BUFFER 256000000;</formula><p>The VALIDATE TABLE option of the VALIDATE INDEX statement causes SearchServer to review whether the contents of container rows, such as directory rows and library files, are correctly reflected in the table. In this particular case, SearchServer initially validated the directory row by inserting each of its sub-directories and files into the table. Then SearchServer validated each of those directory and library file rows in turn, etc. Validating library file rows invoked the cTREC text reader in expansion mode to insert a row for each logical document in the library file, including its document id.</p><p>After validating the table, SearchServer indexed the table, in this case using up to 256MB of memory for sorting (as per the BUFFER parameter) and using temporary sort files of up to 2GB (as per the TEMP_FILE_SIZE parameter). The index includes a dictionary of the distinct words (after some Unicode-based normalizations, such as converting to upper-case and decomposed form, and in the case of the ARAB01A table, Arabic-specific normalizations as previously described) and a reference file with the locations of the word occurrences. Additionally, by default, each distinct word is stemmed and enough information saved so that SearchServer can efficiently find all occurrences of any word which has a particular stem. By default, the stemming is done with an English lexicon which has no effect on Arabic words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Search Techniques</head><p>For the topic relevance task of the Web Track, the 50 "topics" were in a file called "topics.501-550". The topics were numbered from 501-550, and each contained a Title (which was an actual web query taken from a search engine log), a Description (NIST's interpretation of the query), and a Narrative (a more detailed set of guidelines for what a relevant document should or should not contain). We assumed the topics were in the Latin-1 character set, the default on North American Windows systems.</p><p>For the Cross-Language Track, the 25 topics were in 3 different languages (English, French and Arabic). We just used the Arabic topics in a file called "arabic_topics.txt". The Arabic topics were numbered AR1 to AR25. They were encoded in the ISO 8859-6 (Latin-6) character set.</p><p>We created an ODBC application, called QueryToRankings.c, based on the example stsample.c program included with SearchServer, to parse the topics files, construct and execute corresponding SearchSQL queries, fetch the top 1000 rows, and write out the rows in the results format requested by NIST. SELECT statements were issued with the SQLExecDirect api call. Fetches were done with SQLFetch (typically 1000 SQLFetch calls per query).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Character Set</head><p>SearchServer easily handled the issue of the Arabic queries and documents being in different character sets. Before running the Arabic queries, the SearchSQL statement "SET CHARACTER_SET 'ISO_8859_6'" was executed so that SearchServer would transcode the queries from Latin-6 to Unicode. The web queries were assumed to be in the Latin-1 character set, the default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Intuitive Searching</head><p>For all runs, we used SearchServer's Intuitive Searching, i.e. the IS_ABOUT predicate of SearchSQL, which accepts unstructured text. For example, for topic 507 of the Web Track, the Title was "dodge recalls?". A corresponding SearchSQL query would be:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SELECT RELEVANCE('V2:3') AS REL, DOCNO FROM WT10GW WHERE FT_TEXT IS_ABOUT 'dodge recalls?' ORDER BY REL DESC;</head><p>This query would create a working table with the 2 columns named in the SELECT clause, a REL column containing the relevance value of the row for the query, and a DOCNO column containing the document's identifier. The ORDER BY clause specifies that the most relevant rows should be listed first.</p><p>The statement "SET MAX_SEARCH_ROWS 1000" was previously executed so that the working table would contain at most 1000 rows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Stemming</head><p>SearchServer "stems" each distinct word to one or more base forms, called stems, using lexicon-based natural language processing technology. For example, in English, "baby", "babied", "babies", "baby's" and "babying" all have "baby" as a stem. Compound words in languages such as German, Dutch and Finnish produce multiple stems; e.g., in German, "babykost" has "baby" and "kost" as stems.</p><p>By default, Intuitive Searching stems each word in the query, counts the number of occurrences of each stem, and creates a vector. Optionally some stems are discarded (secondary term selection) if they have a high document frequency or to enforce a maximum number of stems, but we didn't discard any stems for our TREC runs this year. The index is searched for documents containing terms which stem to any of the stems of the vector.</p><p>The VECTOR_GENERATOR set option controls which stemming operations are performed by Intuitive Searching.</p><p>For most Web Track runs, we used the default VECTOR_GENERATOR setting ('word!ftelp/base/single | * | word!ftelp/inflect') which assumes the English language, but for one submitted run we disabled stemming (using SET VECTOR_GENERATOR ''). (By default, SearchServer's index supports both exact matching (after some Unicode-based normalizations, such as converting to upper-case and decomposed form) and matching on stems.) For the Arabic runs, we always disabled stemming. When searching SearchServer table ARAB01A, for which Arabic morphological normalizations were applied to each word at index-time, the same normalizations were automatically applied to each query term.</p><p>Besides linguistic expansion from stemming, we did not do any other kinds of query expansion this year. For example, we did not use approximate text searching for spell-correction because the queries were known to be spelled correctly this year. We did not use row expansion or any other kind of blind feedback technique for the official runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Statistical Relevance Ranking</head><p>SearchServer calculates a relevance value for a row of a table with respect to a vector of stems based on several statistics. The inverse document frequency of the stem is estimated from information in the dictionary. The term frequency (number of occurrences of the stem in the row (including any term that stems to it)) is determined from the reference file. The length of the row (based on the number of indexed characters in all columns of the row, which is typically dominated by the external document), is optionally incorporated. The already-mentioned count of the stem in the vector is also used. To synthesize this information into a relevance value, SearchServer dampens the term frequency and adjusts for document length in a manner similar to Okapi <ref type="bibr" coords="5,384.67,504.84,11.67,8.78" target="#b5">[6]</ref> and dampens the inverse document frequency in a manner similar to <ref type="bibr" coords="5,204.85,516.09,10.55,8.77" target="#b7">[7]</ref>. SearchServer's relevance values are always an integer in the range 0 to 1000.</p><p>SearchServer's RELEVANCE_METHOD setting can be used to optionally square the importance of the inverse document frequency (by choosing a RELEVANCE_METHOD of 'V2:4' instead of 'V2:3'). SearchServer's RELEVANCE_DLEN_IMP parameter controls the importance of document length (scale of 0 to 1000) to the ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Query Stop Words</head><p>Our QueryToRankings program removed words such as "find", "relevant" and "document" from the topics before presenting them to SearchServer, i.e. words which are not stop words in general but were commonly used in the topics as general instructions. For our CLEF runs this year <ref type="bibr" coords="5,313.31,645.09,10.66,8.77" target="#b9">[9]</ref>, we expanded the list for several languages based on examining the CLEF 2000 topics (not this year's TREC topics). The full list for English is now as follows: "item", "items", "find", "documents", "document", "relevant", "report", "what", "identify", "about", "discussing". (Some of these words, such as "about", were also in the mytrec.stp stopfile, so removing them was redundant.) Although they were unlikely to appear, corresponding words for other languages, e.g. the German word "dokumente", were removed if encountered. No Arabic words were in the list. This step was found to be only minor benefit for CLEF <ref type="bibr" coords="6,72.00,120.84,10.18,8.78" target="#b9">[9]</ref>.</p><p>If a query returned no results, based on our experience with the TREC-9 Large Web Task last year, the reason was often that the queries consisted entirely of stop words. The most famous stopword query, "to be or not to be", is a philosophical question, so for the Web Track this year we pre-selected document WTX094-B32-167 (the Yahoo Philosophy page) to be returned if otherwise the query would return no results. (It turned out the situation came up this year for topic 531 (who and whom), which was judged to be a grammar question, and hence the Philosophy page was properly judged non-relevant.) As a more general technique, we may just return the results of the query "philosophy" for this situation in future years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>The evaluation measures are explained in an appendix of the conference proceedings. Briefly: Precision is the percentage of retrieved documents which are relevant. Precision@n is the precision after n documents have been retrieved. Average precision for a topic is the average of the precision after each relevant document is retrieved (using zero as the precision for relevant documents which are not retrieved). Recall is the percentage of relevant documents which have been retrieved. Interpolated precision at a particular recall level for a topic is the maximum precision achieved for the topic at that or any higher recall level. For a set of topics, the measure is the average of the measure for each topic (i.e. all topics are weighted equally).</p><p>We use the following abbreviations for the evaluation measures in this paper: AvgP: Average Precision P@5, P@10, P@15, P@20, P@30: Precision after 5, 10, 15, 20 and 30 documents retrieved, respectively Rec0, Rec30: Interpolated Precision at 0% and 30% Recall, respectively AvgH: Average Precision just counting Highly Relevants as relevant H@5, H@10, H@15, H@20, H@30: P@5, P@10, P@15, P@20 and P@30 just counting Highly Relevants as relevant, respectively H0, H30: Rec0 and Rec30 just counting Highly Relevants as relevant, respectively</p><p>We refer to the scores with a fixed cutoff (P@5, P@10, P@15, P@20, P@30) as early precision scores. The other scores (AvgP, Rec0, Rec30), which can be influenced by results later in the result list, we call recall-oriented scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Web Track</head><p>The topic relevance task of the Web Track was to run 50 web queries against 10GB of web data and submit a list of the top-1000 ranked documents to NIST for judging.</p><p>NIST produced a "qrels" file: a list of documents judged to be highly relevant, relevant or not relevant for each topic. From these, the scores were calculated with Chris Buckley's trec_eval program, which counts all relevants the same, including highly relevants. To produce scores which just counted highly relevants as relevant, we ran trec_eval a 2 nd time on a modified version of the qrels file which had the ordinary relevants filtered out, then multiplied by 50/44 (in 6 of the 50 topics, there were no highly relevants). Hence the scores focused on highly relevants are averaged over just 44 topics.</p><p>We submitted 4 runs for the topic relevance task of the Web Track: hum01t, hum01tl, hum01tlx and hum01tdlx.</p><p>The run codes we used are as follows:</p><p>hum: Hummingbird 01: TREC 2001 t: title field used d: description field used n: narrative field used l: linguistic expansion (stemming) enabled x: weighting scheme squared importance of inverse document frequency and increased importance of document length normalization (i.e. RELEVANCE_METHOD 'V2:4', RELEVANCE_DLEN_IMP 750, instead of 'V2:3' and 500 respectively)</p><p>Tables <ref type="table" coords="7,102.56,212.34,4.88,8.78" target="#tab_1">1</ref> and<ref type="table" coords="7,129.55,212.34,4.88,8.78">2</ref> show various scores of our submitted Title-only runs, i.e. runs which just used the original web query. Additionally, for some measures, NIST reported the median scores of the 77 submitted Title-only runs from all groups for each of the 50 topics; we show the average of the median scores: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2: Precision of Submitted Title-only runs just counting Highly Relevants as relevant</head><p>Impact of Stemming (compare hum01t to hum01tl): When counting just highly relevants as relevant, the early precision scores (P@5, P@10, P@15, P@20, P@30) were higher with stemming disabled, and the recall-oriented scores (AvgP, Rec0, Rec30) were higher with stemming enabled. This result fits intuition: stemming ought to increase recall because more word variants are allowed to match, but sometimes the variants may not reflect the query as accurately, hurting precision. When counting all relevants the same, the earliest precision score (P@5) was again higher with stemming disabled; the other early precision scores were modestly higher with linguistic expansion enabled, though by a smaller margin than for the recall-oriented scores. The difference from the result for highly relevants may be from precision suffering less when the quality of the match is not required to be as high.</p><p>None of these differences were statistically significant at the 5% level by the two-sided Wilcoxon signed rank test <ref type="bibr" coords="7,72.00,561.09,10.18,8.77" target="#b4">[5]</ref>.</p><p>If stemming helps recall but hurts early precision, it may be better to give a higher weight to the original query word than the generated variants. We haven't yet run any experiments with this approach.</p><p>Note that all topics were English. In our CLEF 2001 experiments this year, we found that the impact of SearchServer's stemming was normally larger in other European languages, particularly in German and Dutch. <ref type="bibr" coords="7,516.31,630.09,11.58,8.77" target="#b9">[9]</ref> Run AvgP P@5 P@10 P@15 P@20 P@30 Rec0 Impact of including the Narrative field (compare 5a to 5b, and 6a to 6b): When counting all relevants the same, all investigated scores were higher when including the Narrative. When just counting highly relevants as relevant, most of the scores were lower when including the Narrative. None of the differences were statistically significant at the 5% level by the two-sided Wilcoxon signed rank test.</p><p>Table <ref type="table" coords="9,98.53,304.59,4.88,8.78">7</ref> shows per-topic comparisons of our submitted runs with the medians in their category for the measures reported by NIST: Average Precision, Precison@10, Precision@20 and Precision@30, respectively. In each comparison, we show the number of topics on which the run scored higher than the median, lower than the median and tied with the median (Higher-Lower-Tied). Differences statistically significant at the 1% level by the two-sided Wilcoxon signed rank test are marked with two asterisks (**), and differences just significant at the 5% level are marked with a single asterisk (*): Run AvgP P@10 P@20 P@30 hum01tlx </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 7: Per-topic comparison of Submitted runs with Medians</head><p>The per-topic comparisons show a lot of ties in the early precision scores, particularly P@10, because of the small number of documents considered. Still, in each measure, the difference of the hum01tlx and hum01tl runs with the medians is statistically significant at the 1% level by the two-sided Wilcoxon signed rank test (the calculation of the significance level discards the ties, following <ref type="bibr" coords="9,253.98,518.34,10.45,8.77" target="#b4">[5]</ref>).</p><p>The significance level (p-value) for the two-sided Wilcoxon signed rank test defined in <ref type="bibr" coords="9,435.66,540.84,11.78,8.77" target="#b4">[5]</ref> was computed by our own implemented algorithm. The computation is exact (aside from double-precision roundoff errors) even in the case of tied absolute differences. For the Wilcoxon signed rank test we assume that the differences on differing topics are independent, and that the differences are from a distribution which is symmetric about a median difference. The test tests the hypothesis that the median difference is zero. For more details, see <ref type="bibr" coords="9,461.91,587.34,10.65,8.77" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Cross-Language Track</head><p>Table <ref type="table" coords="9,98.49,635.34,4.88,8.78" target="#tab_4">8</ref> shows our submitted Arabic runs, which were all monolingual runs, i.e. used the Arabic versions of the topics. The baseline run was humAR01td, a Title+Description run which applied some experimental Arabic morphological normalizations to the words before indexing. The other runs were the same as the baseline except for one factor. Run humAR01tdm disabled the Arabic-specific normalizations (but not general ones such as case normalization). Run humAR01tdx used a different weighting scheme which squared the importance of inverse document frequency and also increased the adjustment for document length. Impact of Arabic morphological normalizations (compare humAR01tdm to humAR01td): All investigated scores except P@15 were tied or higher when the Arabic morphological normalizations were applied. None of the differences were statistically significant at the 5% level by the two-sided Wilcoxon signed rank test. (The Arabic test collection just contained 25 topics, making it harder to detect significant differences than for the web collection, which had 50 topics.)</p><p>Impact of changing the weighting scheme (compare humAR01td to humAR01tdx): It made little difference.</p><p>Impact of excluding the Description field (compare humAR01td to humAR01t): Some scores were higher when just using the Title field (AvgP, P@5, P@15, Rec0, Rec30). Others were higher when the Description was included (P@10, P@20, P@30). It was noted at the conference that the Titles for these topics were a little longer than usual.</p><p>Impact of including the Narrative field (compare humAR01td to humAR01tdn): Some scores were higher when including the Narrative (P@5, P@10, P@15, P@30, Rec0) but a few were lower (AvgP, P@20, Rec30).</p><p>After the conference, we added approximately 2000 Arabic stop words based on the ISI list <ref type="bibr" coords="10,446.55,426.84,10.70,8.77" target="#b8">[8]</ref>. Table <ref type="table" coords="10,492.05,426.84,4.88,8.78" target="#tab_5">9</ref> shows the new scores when redoing runs humAR01td, humAR01t and humAR01tdn with the stop words (note: an experimental version of SearchServer 5.3 (pre-release) was used for the Arabic diagnostic runs, but its document ranking with respect to Arabic was the same as SearchServer 5.0's except for the experimental differences described in this section): Impact of Arabic stop words (compare 8a to 9td, 8d to 9t, and 8e to 9tdn): While the scores just increased modestly, the increases were consistent across topics. For example, in the Title+Description case (runs 8a vs 9td), 22 topics had a higher score in average precision, just 1 lower and 2 tied, when using the stop words. In the Title+Description case, the difference in average precision was statistically significant at the 1% level, and the differences in P@10 and P@20 were statistically significant at the 5% level, by the two-sided Wilcoxon signed ranked test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>As another experiment, we added some morphological normalizations mentioned by other groups which we weren't already using, particularly the orthographic variation mentioned by BBN <ref type="bibr" coords="11,372.98,86.34,16.67,8.78" target="#b11">[11]</ref> (YEH vs. ALEF MAKSURA at end of word) and removing WAW prefixes as mentioned by Berkeley <ref type="bibr" coords="11,340.60,97.59,10.74,8.78">[3]</ref>. Table <ref type="table" coords="11,385.98,97.59,9.92,8.78" target="#tab_6">10</ref> shows the scores when redoing the runs of Impact of additional Arabic morphological normalizations (compare 9td to 10td, 9t to 10t, and 9tdn to 10tdn): Most of the scores modestly increased from the new rules. Focusing on the Title+Description case, the difference in P@5 was statistically significant at the 5% level by the two-sided Wilcoxon signed rank test, but the other differences were not.</p><p>Combined impact of (updated) Arabic morphological normalizations and stop words (compare 8b to 10td): The combination of Arabic morphological normalizations (including the experimental updates) and the stop words increased average precision by 36% and the difference in average precision was statistically significant at the 1% level by the two-sided Wilcoxon signed rank test. None of the other differences were statistically significant even at the 5% level. Precision@10 just increased 6%. Early precision may benefit less from normalization rules because there may be enough exact matches to find. Some groups found that query expansion worked well on this collection, so we applied the "row expansion" technique described in last year's paper <ref type="bibr" coords="11,240.86,368.34,15.41,8.77" target="#b10">[10]</ref>. Roughly speaking, row expansion is a pseudo-relevance feedback technique in which it is assumed that the top rows of the initial query are relevant and SearchServer's Intuitive Searching uses them to generate new, broader queries. (In practice, a SearchServer user would specify which rows are relevant, which should produce better results than the "blind" automatic technique applied here.) Like last year, we used the top-5 rows; a minor difference is that for the row expansion queries, we used a document frequency parameter of 5% (i.e. RELEVANCE_METHOD 'V2:3:05') instead of the experimental secondary term selection approach used last year. Table <ref type="table" coords="11,198.08,437.34,9.90,8.78" target="#tab_8">11</ref> shows the scores after applying row expansion to the runs of Impact of row expansion (compare 10td to 11td, 10t to 11t, and 10tdn to 11tdn): Most of the scores modestly increased from row expansion; average precision was up 11-17%. Focusing on the Title+Description case, the differences in average precision and Rec30 were statistically significant at the 1% level by the two-sided Wilcoxon signed rank test; the other differences were not statistically significant at even the 5% level. Query expansion techniques such as row expansion may help recall-oriented measures by contributing terms from the top documents which are not automatically generated from the initial query.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,108.00,306.62,251.85,30.16"><head>TABLE ARAB01A</head><label>ARAB01A</label><figDesc></figDesc><table coords="3,108.00,317.87,245.85,18.91"><row><cell>(DOCNO VARCHAR(256) 128) PERIODIC</cell></row><row><cell>BASEPATH 'E:\DATA' STOPFILE 'MYARAB.STP';</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,90.00,260.34,417.59,158.77"><head>Table 1 : Precision of Submitted Title-only runs counting all relevants the same</head><label>1</label><figDesc></figDesc><table coords="7,90.00,260.34,417.59,158.77"><row><cell>Run</cell><cell>AvgP</cell><cell>P@5</cell><cell>P@10</cell><cell>P@15</cell><cell>P@20</cell><cell>P@30</cell><cell>Rec0</cell><cell>Rec30</cell></row><row><cell>1a: hum01tlx</cell><cell>0.1949</cell><cell>38.4%</cell><cell>33.2%</cell><cell>30.53%</cell><cell>28.6%</cell><cell>25.47%</cell><cell>0.6168</cell><cell>0.2708</cell></row><row><cell>1b: hum01tl</cell><cell>0.1784</cell><cell>36.8%</cell><cell>32.2%</cell><cell>28.93%</cell><cell>26.6%</cell><cell>23.73%</cell><cell>0.5940</cell><cell>0.2485</cell></row><row><cell>1c: hum01t</cell><cell>0.1582</cell><cell>39.6%</cell><cell>30.8%</cell><cell>28.13%</cell><cell>26.0%</cell><cell>22.67%</cell><cell>0.5665</cell><cell>0.2210</cell></row><row><cell>Median (77 runs)</cell><cell>0.1402</cell><cell>n/a</cell><cell>26.6%</cell><cell>n/a</cell><cell>22.5%</cell><cell>20.53%</cell><cell>n/a</cell><cell>n/a</cell></row><row><cell>Run</cell><cell>AvgH</cell><cell>H@5</cell><cell cols="2">H@10 H@15</cell><cell cols="2">H@20 H@30</cell><cell>H0</cell><cell>H30</cell></row><row><cell>2a: hum01tlx</cell><cell>0.1909</cell><cell>18.6%</cell><cell>13.0%</cell><cell>11.51%</cell><cell>9.9%</cell><cell>7.81%</cell><cell>0.4186</cell><cell>0.2417</cell></row><row><cell>2b: hum01tl</cell><cell>0.1855</cell><cell>18.6%</cell><cell>13.0%</cell><cell>10.76%</cell><cell>9.3%</cell><cell>7.58%</cell><cell>0.3890</cell><cell>0.2235</cell></row><row><cell>2c: hum01t</cell><cell>0.1684</cell><cell>20.9%</cell><cell>14.3%</cell><cell>11.36%</cell><cell>9.5%</cell><cell>7.73%</cell><cell>0.3807</cell><cell>0.2116</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,90.00,87.84,417.59,120.52"><head>Table 5 : Precision of non-Title-only runs counting all relevants the same</head><label>5</label><figDesc></figDesc><table coords="9,480.00,87.84,26.09,8.78"><row><cell>Rec30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,137.25,223.59,355.39,8.78"><head>Table 6 : Precision of non-Title-only runs just counting Highly Relevants as relevant</head><label>6</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,72.00,108.84,467.86,131.78"><head>Table 8 : Precision of Submitted Monolingual Arabic runs</head><label>8</label><figDesc>Run humAR01t was Title-only. Run humAR01tdn was Title+Description+Narrative. No stop words were applied:</figDesc><table coords="10,90.00,144.84,417.59,71.03"><row><cell>Run</cell><cell>AvgP</cell><cell>P@5</cell><cell>P@10</cell><cell>P@15</cell><cell>P@20</cell><cell>P@30</cell><cell>Rec0</cell><cell>Rec30</cell></row><row><cell>8a: humAR01td</cell><cell>0.2441</cell><cell>50.4%</cell><cell>49.2%</cell><cell>47.73%</cell><cell>46.2%</cell><cell>43.07%</cell><cell>0.7494</cell><cell>0.3149</cell></row><row><cell cols="2">8b: humAR01tdm 0.2087</cell><cell>48.0%</cell><cell>48.4%</cell><cell>48.27%</cell><cell>46.2%</cell><cell>41.20%</cell><cell>0.7238</cell><cell>0.2848</cell></row><row><cell>8c: humAR01tdx</cell><cell>0.2465</cell><cell>51.2%</cell><cell>48.0%</cell><cell>46.13%</cell><cell>43.8%</cell><cell>39.07%</cell><cell>0.7486</cell><cell>0.3235</cell></row><row><cell>8d: humAR01t</cell><cell>0.2663</cell><cell>53.6%</cell><cell>48.8%</cell><cell>48.0%</cell><cell>45.0%</cell><cell>41.33%</cell><cell>0.7755</cell><cell>0.3390</cell></row><row><cell>8e: humAR01tdn</cell><cell>0.2395</cell><cell>62.4%</cell><cell>51.6%</cell><cell>49.33%</cell><cell>45.8%</cell><cell>43.47%</cell><cell>0.8312</cell><cell>0.2823</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,90.00,497.34,417.59,71.03"><head>Table 9 : Precision of runs using Arabic Stop Words</head><label>9</label><figDesc></figDesc><table coords="10,90.00,497.34,417.59,46.28"><row><cell></cell><cell>AvgP</cell><cell>P@5</cell><cell>P@10</cell><cell>P@15</cell><cell>P@20</cell><cell>P@30</cell><cell>Rec0</cell><cell>Rec30</cell></row><row><cell>9td: 8a + stp</cell><cell>0.2617</cell><cell>50.4%</cell><cell>52.0%</cell><cell>48.27%</cell><cell>47.8%</cell><cell>43.87%</cell><cell>0.7558</cell><cell>0.3415</cell></row><row><cell>9t: 8d + stp</cell><cell>0.2692</cell><cell>52.0%</cell><cell>48.0%</cell><cell>45.87%</cell><cell>43.0%</cell><cell>40.27%</cell><cell>0.7624</cell><cell>0.3523</cell></row><row><cell>9tdn : 8e + stp</cell><cell>0.2639</cell><cell>60.8%</cell><cell>54.4%</cell><cell>51.47%</cell><cell>50.6%</cell><cell>44.40%</cell><cell>0.8631</cell><cell>0.3335</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="11,90.00,108.84,417.59,95.78"><head>Table 10 : Precision of runs with Additional Arabic Morphological Normalizations</head><label>10</label><figDesc>Table 9 with the additional rules:</figDesc><table coords="11,90.00,133.59,417.59,46.28"><row><cell>Run</cell><cell>AvgP</cell><cell>P@5</cell><cell>P@10</cell><cell>P@15</cell><cell>P@20</cell><cell>P@30</cell><cell>Rec0</cell><cell>Rec30</cell></row><row><cell>10td: 9td +rules</cell><cell>0.2831</cell><cell>57.6%</cell><cell>51.2%</cell><cell>48.53%</cell><cell>45.8%</cell><cell>43.60%</cell><cell>0.7917</cell><cell>0.3837</cell></row><row><cell>10t: 9t +rules</cell><cell>0.2939</cell><cell>57.6%</cell><cell>49.6%</cell><cell>47.47%</cell><cell>46.6%</cell><cell>42.80%</cell><cell>0.8269</cell><cell>0.3853</cell></row><row><cell cols="2">10tdn: 9tdn+rules 0.2802</cell><cell>62.4%</cell><cell>56.8%</cell><cell>51.47%</cell><cell>49.4%</cell><cell>45.07%</cell><cell>0.8746</cell><cell>0.3646</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="11,90.00,437.34,417.59,70.27"><head>Table 10 :</head><label>10</label><figDesc></figDesc><table coords="11,90.00,461.34,417.59,46.27"><row><cell>Run</cell><cell>AvgP</cell><cell>P@5</cell><cell>P@10</cell><cell>P@15</cell><cell>P@20</cell><cell>P@30</cell><cell>Rec0</cell><cell>Rec30</cell></row><row><cell>11td: 10td +exp</cell><cell>0.3185</cell><cell>59.2%</cell><cell>52.8%</cell><cell>52.53%</cell><cell>50.6%</cell><cell>46.80%</cell><cell>0.7918</cell><cell>0.4430</cell></row><row><cell>11t: 10t +exp</cell><cell>0.3268</cell><cell>58.4%</cell><cell>56.0%</cell><cell>53.33%</cell><cell>50.6%</cell><cell>47.47%</cell><cell>0.8070</cell><cell>0.4262</cell></row><row><cell>11tdn: 10tdn+exp</cell><cell>0.3285</cell><cell>63.2%</cell><cell>60.4%</cell><cell>55.47%</cell><cell>53.8%</cell><cell>50.13%</cell><cell>0.8684</cell><cell>0.4351</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="11,195.75,523.59,238.42,8.77"><head>Table 11 : Precision of Arabic runs after Row Expansion</head><label>11</label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,78.00,621.84,347.84,8.77"><p>Core Technology, Research and Development, stephen.tomlinson@hummingbird.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,78.00,630.65,461.47,12.70;1,72.00,645.84,467.87,8.77;1,72.00,657.09,75.55,8.77"><p>Fulcrum® is a registered trademark, and SearchServer™ , SearchSQL™ , Intuitive Searching™ and Ful/Text ™ are trademarks of Hummingbird Ltd. All other copyrights, trademarks and tradenames are the property of their respective owners.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Combined impact of (updated) Arabic morphological normalizations, stop words and row expansion (compare 8b to 11td): Applying all the techniques described above increased average precision by <rs type="grantNumber">53%</rs>, but increased Precison@10 by just <rs type="grantNumber">9%</rs>. The differences in average precision and Rec30 were statistically significant at the 1% level, and the difference in Precision@5 was statistically significant at the 5% level, by the two-sided Wilcoxon signed rank test. None of the other differences were statistically significant at the 5% level.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_f8e2bdF">
					<idno type="grant-number">53%</idno>
				</org>
				<org type="funding" xml:id="_92QRrtr">
					<idno type="grant-number">9%</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There are two differences in the weighting scheme between the submitted hum01tl and hum01tlx runs. To isolate the impact of each change, Tables <ref type="table" coords="8,212.10,86.34,4.88,8.78">3</ref> and<ref type="table" coords="8,236.69,86.34,4.88,8.78">4</ref> show diagnostic runs whose settings are the same as for hum01tlx except for the document length importance setting (RELEVANCE_DLEN_IMP). Rows 3d and 4d are the same as rows 1a and 1b, respectively (the hum01tlx run). Rows 3c and 4c differ from hum01tl in just the relevance method (V2:4 vs. V2:3):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DLen Importance</head><p>AvgP P@5 P@10 P@15 P@20 P@30 Rec0 Impact of Document Length Normalization: Ignoring document length (rows 3a and 4a) hurt all scores; average precision was 30-55% higher in the other rows, and Precision@10 was 45-65% higher in the other rows. The impact on highly relevants was even larger; average precision was up to 85% higher in the other rows. For most measures, a setting of 500 produced the highest scores of the settings investigated. When comparing the document length importance setting of 500 with 0 (i.e. compare 3c to 3a, and 4c to 4a), the differences in all of the shown measures (i.e. from AvgP through H30) are statistically significant at the 1% level by the two-sided Wilcoxon signed rank test.</p><p>Impact of squaring the importance of inverse document frequency (compare 1b to 3c, and 2b to 4c, which are the same except for the relevance method (V2:3 vs V2:4)): All measures were higher with the importance of inverse document frequency squared (relevance method V2:4). The differences were statistically significant at the 1% level for AvgP, P@10 and P@20, and at the 5% level for P@15, P@30, Rec30, AvgH, H@30 and H30, by the two-sided Wilcoxon signed rank test. Note that on some other test collections, such as the CLEF news collections, we have seen V2:3 receive higher scores.</p><p>For the benefit of the relevance assessment pools, we donated one run with the Description field included (hum01tdlx) and assigned it highest judging priority. Tables <ref type="table" coords="8,324.77,549.84,4.88,8.78">5</ref> and<ref type="table" coords="8,350.46,549.84,4.88,8.78">6</ref> show scores for hum01tdlx and a diagnostic run which is the same as hum01tdlx except that the Narrative field was also included. Table <ref type="table" coords="8,449.61,561.09,4.88,8.78">5</ref> also shows averages of the medians reported by NIST which were based on a group including all submitted non-Title-only runs, including 2 manual runs and some runs using the Narrative.</p><p>Impact of including the Description field (compare hum01tlx to hum01tdlx, i.e. 1a to 5a, and 2a to 6a): All scores were higher when including the Description. The differences were statistically significant at the 5% level for AvgP, P@20, Rec0, H@30 and H30 by the two-sided Wilcoxon signed rank test. None of the differences were statistically significant at the 1% level.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="12,86.25,180.09,450.63,8.78;12,72.00,192.09,229.41,8.78" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,187.80,180.09,261.03,8.78">Linguistic Data Consortium (LDC) catalog number LDC</title>
		<ptr target="http://www.ldc.upenn.edu/Catalog/LDC2001T55.html" />
	</analytic>
	<monogr>
		<title level="j" coord="12,86.25,180.09,88.81,8.78">Arabic Newswire Part</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,86.07,214.59,453.81,8.78;12,72.00,226.59,467.85,8.78;12,72.00,237.84,165.63,8.78" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,291.00,214.59,248.88,8.78;12,72.00,226.59,241.30,8.78">Engineering a multi-purpose test collection for Web retrieval experiments (DRAFT, accepted, subject to revision</title>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Hawking</surname></persName>
		</author>
		<ptr target="http://www.ted.cmis.csiro.au/TRECWeb/" />
	</analytic>
	<monogr>
		<title level="m" coord="12,326.93,226.59,208.08,8.78">by Information Processing and Management)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,87.75,261.09,452.12,8.77;12,72.00,272.34,467.86,8.77;12,72.00,283.59,52.30,8.77" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="12,390.09,261.09,149.78,8.77;12,72.00,272.34,257.65,8.77">) Translation Term Weighting and Combining Translation Resources in Cross-Language Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Aitao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Frederic</forename><surname>Gey</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>University of California at Berkeley.</orgName>
		</respStmt>
	</monogr>
	<note>Notebook paper in draft TREC 2001 Conference Proceedings</note>
</biblStruct>

<biblStruct coords="12,86.61,306.84,453.22,8.77;12,72.00,318.09,224.16,8.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,170.06,306.84,209.68,8.77">Converting the Fulcrum Search Engine to Unicode</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Hodgson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,405.75,306.84,134.08,8.77;12,72.00,318.09,44.49,8.78">Sixteenth International Unicode Conference</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-03">March 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,87.30,341.34,452.58,8.77;12,72.00,352.59,59.76,8.77" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Myles</forename><surname>Hollander</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">A</forename><surname>Wolfe</surname></persName>
		</author>
		<title level="m" coord="12,264.75,341.34,217.42,8.77">Nonparametric Statistical Methods. Second Edition</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,85.50,375.84,453.68,8.77" xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>City University.) Okapi at TREC-</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="12,75.68,387.09,464.24,8.77;12,72.00,399.09,239.17,8.77" xml:id="b6">
	<analytic>
		<ptr target="http://trec.nist.gov/pubs/trec3/t3_proceedings.html" />
	</analytic>
	<monogr>
		<title level="m" coord="12,183.00,387.09,243.00,8.77">Overview of the Third Text REtrieval Conference (TREC-3)</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="500" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,87.00,421.59,452.89,8.77;12,72.00,433.59,468.02,8.77;12,72.00,444.84,319.42,8.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,423.53,421.59,74.06,8.77">AT&amp;T at TREC-7</title>
		<author>
			<persName coords=""><forename type="first">Amit</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Hindle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec7/t7_proceedings.html" />
	</analytic>
	<monogr>
		<title level="m" coord="12,230.25,433.59,274.52,8.77">Proceedings of the Seventh Text REtrieval Conference (TREC-7)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Seventh Text REtrieval Conference (TREC-7)</meeting>
		<imprint>
			<biblScope unit="page" from="500" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,86.45,468.09,453.40,8.77;12,72.00,479.34,438.67,8.77" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Bonnie</forename><surname>Glover</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stalls</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<ptr target="http://www.isi.edu/~yaser/arabic/arabic-stop-words.html" />
		<title level="m" coord="12,182.45,479.34,93.76,8.77">Arabic Stop Words List</title>
		<imprint/>
		<respStmt>
			<orgName>University of Southern California, Information Sciences Institute, Natural Language Group</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="12,85.50,503.34,454.43,8.78;12,72.00,514.59,468.56,8.77;12,72.00,525.84,361.37,8.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,173.73,503.34,326.22,8.78">Stemming Evaluated in 6 Languages by Hummingbird SearchServer™ at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,201.86,514.59,260.31,8.77">Proceedings of the Cross-Language Evaluation Forum (CLEF)</title>
		<title level="s" coord="12,186.75,525.84,242.99,8.77">Springer Lecture Notes for Computer Science (LNCS) series</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<meeting>the Cross-Language Evaluation Forum (CLEF)<address><addrLine>Darmstadt, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-09">2001. 2001. September 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,91.54,549.09,447.60,8.77;12,72.00,560.34,467.90,8.77;12,72.00,572.34,287.17,8.77" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,257.12,549.09,200.03,8.77">Hummingbird&apos;s Fulcrum SearchServer at TREC-9</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tom</forename><surname>Blackwell</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec9/t9_proceedings.html" />
	</analytic>
	<monogr>
		<title level="m" coord="12,192.00,560.34,273.75,8.77">Proceedings of the Ninth Text REtrieval Conference (TREC-9)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Ninth Text REtrieval Conference (TREC-9)</meeting>
		<imprint>
			<biblScope unit="page" from="500" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,90.75,594.84,448.40,8.77;12,72.00,606.84,173.11,8.77" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="12,277.67,594.84,174.81,8.77">TREC 2001 Cross-lingual Retrieval at BBN</title>
		<author>
			<persName coords=""><forename type="first">Jinxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Notebook paper in draft TREC 2001 Conference Proceedings</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
