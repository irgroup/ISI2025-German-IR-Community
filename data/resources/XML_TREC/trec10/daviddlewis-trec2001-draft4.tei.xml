<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,96.50,75.68,418.86,14.47;1,209.05,94.16,193.70,14.47">Applying Support Vector Machines to the TREC-2001 Batch Filtering and Routing Tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,266.65,139.19,78.59,10.80"><forename type="first">David</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
							<email>dave@daviddlewis.com</email>
							<affiliation key="aff0">
								<orgName type="department">Independent Consultant</orgName>
								<address>
									<addrLine>858 W. Armitage Ave., #296 Chicago</addrLine>
									<postCode>60614</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,96.50,75.68,418.86,14.47;1,209.05,94.16,193.70,14.47">Applying Support Vector Machines to the TREC-2001 Batch Filtering and Routing Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6A5D2AA47562E6FE88043C05AF84AB9B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>I modified SVM_Light to accept a comment before each example specifying a document ID, and to output during classification records containing score, predicted class, true class (if present in the test data), and document ID.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>My goal for TREC-2001 was simple: submit some runs (so that I could attend the conference), spend the minimum time necessary (since I've been busy this year with a large client project), and get respectable results (marketing!). The TREC batch filtering task was the obvious choice, since this year it was purely and simply a text categorization task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Learning Algorithm</head><p>Given the large training set available for batch filtering, choosing a supervised learning algorithm that would make effective use of this data was critical. The support vector machine approach (SVM) to training linear classifiers has outperformed competing approaches in a number of recent text categorization studies, particularly for categories with substantial numbers of positive training examples. SVMs require little or no feature selection, since they avoid overfitting by optimizing a margin-based criterion rather than one based on number of features. This minimizes the complexity of the software and processing. Finally, Thorsten Joachims has made publicly available an efficient implementation of SVMs, SVM_Light <ref type="bibr" coords="1,273.85,502.31,71.58,10.80" target="#b1">[Joachims 1999</ref>]:</p><p>http://www.joachims.org/svm_light/ SVM_Light allows training of both linear and, via kernels, nonlinear classifiers. I used linear classifiers in all cases. Indeed, I left all SVM_Light options that affect learning at their default values except -j, which controls the relative weight of positive and negative training examples in computing the margin-based loss criterion that SVM's optimize. A typical approach to Problem 2 would be to train using SVM_Light's usual criterion, producing a linear model with a threshold of 0. In a second phase, one would search for a new threshold value that optimizes the TREC effectiveness measure on the training set, while leaving the rest of the parameters unchanged <ref type="bibr" coords="2,337.45,490.79,88.06,10.80" target="#b2">[Lewis, et al 1996]</ref>. Zhang &amp; Oles recently used this approach with SVMs <ref type="bibr" coords="2,280.33,504.71,102.94,10.80" target="#b2">[Zhang &amp; Oles, 2001]</ref>. This approach assumes, however, that the optimal orientation of the hyperplane is the same for all effectiveness measures, something which is not at all clear. Further, it does nothing about Problem 1. I therefore took the opposite approach, which was to leave the threshold at 0, and try to force the fitting process for the other parameters to adapt to the TREC filtering measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Tuning the Weighting of Positive and Negative Examples</head><p>Happily, SVM_Light has a parameter that controls the relative weight of positive and negative examples in its loss function. Since the T10SU measure corresponds to a weighting of positive examples to be 2 times more important than negative examples, an obvious approach would be to use that same relative weighting in training, at least in producing classifiers for the T10SU measure. I rejected that approach for two reasons:</p><p>1. While it has been proven that equally weighting positive and negative examples in SVM Light's loss function leads to high accuracy (i.e. high utility with an equal weighting on positive and negative examples), this result has not been shown to carry through to unequal weightings of positive and negative examples. It seemed possible, indeed likely, that the scaling factors for the two measures would be different. In any case, as with most computational learning theory results, those for SVMs are too loose to constrain parameter settings tightly.</p><p>2. Something had to be done about the F-measure, which does not correspond to any simple relative weighting of positive to negative examples. (Indeed, the F-measure can't be optimized by any predetermined threshold <ref type="bibr" coords="3,332.17,171.11,56.48,10.80" target="#b1">[Lewis 1995</ref>], but I ignored this problem for TREC-2001.) I therefore took a brute force approach. I did multiple training runs for each topic with relative weightings of positive to negative examples of 0.4, 0.6, 0.8, 0.9, 1.0, 2.0, 4.0, and 8.0. This gave me 8 classifiers for each topic, from which I needed to choose a single classifier for each of the two effectiveness measures. I considered three methods: Method 1. Evaluate on training data: Using each classifier to classify the training data, comparing those classifications with the true labels, computing the effectiveness on the appropriate measure, and picking the classifier with best effectiveness. The obvious problem with this approach is overfitting: the effectiveness estimates will be too optimistic. If the estimates were systematically too optimistic, then the choice of classifier would not be affected. However, that seemed too much to hope for, particularly with a nonlinear effectiveness measure such as T10F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method 2. Leave-one-out cross-validation:</head><p>In cross-validation, one breaks the training data into k subsets. A classifier is trained on the union of k-1 of the subsets, and evaluated on the kth subset. The process is repeated k times, using each of the subsets as the validation subset once. One then combines the results from the validation subsets to get an overall estimate of the effectiveness of the training procedure. The most extreme and most accurate version of cross-validation is leave-one-out cross validation (LOOCV), i.e. doing n-fold cross validation when there are n training examples. Cross validation can be used to choose a parameter setting by making a cross-validated estimate of effectiveness at several values of the parameter, choosing the parameter value with highest estimated effectiveness, and then doing a final training run with all data using the chosen parameter setting. (Or one can train using all training data on all choices of parameter setting in advance, and then use cross-validation to pick the best of the already trained classifiers, as I did.) Method 3. xi-alpha estimation: SVM_Light incorporates a highly efficient approximation to LOOCV called xi-alpha estimation <ref type="bibr" coords="3,274.09,626.63,71.36,10.80" target="#b1">[Joachims 2000</ref>].</p><p>It is important to stress that none of these three methods make any use of the test data.</p><p>Given the computational expense of Method 2, I first investigated Methods 1 and 3. I had high hopes for the xi-alpha estimate but found its predictions of effectiveness seemed both unrealistically pessimistic, and varied with the example weighting parameter in ways that seemed intuitively wrong. On the other hand, the estimates of Method 1 seemed unrealistically optimistic in many cases, as well as disagreeing strongly with the Method 3 estimates.</p><p>I therefore used the more expensive but accurate Method 2. SVM_Light includes support for LOOCV which, in the case of TREC 2001 batch filtering, meant 23,307-fold cross validation. Using this to choose among 8 values of a weighting parameter in theory meant training 8 × 23,307 classifiers, each on 23,306 examples, for each of 84 categories.</p><p>Fortunately, the properties of the SVM algorithm are such that many of the results of LOOCV folds can be predicted from a run on the full training data, without actually doing the training on the subset. SVM_Light incorporates an optimization that prunes away the folds that do not need to be explicitly run, and it meant that typically only a few hundred to a few thousand of the LOOCV folds were actually run for each setting, rather than 23,307 folds. In addition, I used a slightly aggressive version of pruning (the options -x 1 and -o 1) known to work well on text classification problems <ref type="bibr" coords="4,444.01,296.39,50.52,10.80;4,90.01,310.07,28.00,10.80" target="#b1">[Joachims, 2000]</ref> instead of the exactly correct version of pruning (options -x 1 and -o 2). Still, several weeks of computing time on a 700MHz PC were required to generate the results for 8 parameter settings and 84 categories.</p><p>A minor complexity was that SVM_Light output LOOCV and xi-alpha estimates of recall, precision, and error, but I really wanted estimates of T10F and T10U. I therefore wrote code to work backwards from the estimates that were printed (to only 4 digits of accuracy) to the actual contingency table entries, taking into account knowledge of the number of training examples and the number of positive examples for a topic. The code then computed estimates for T10F and T10U from the LOOCV contingency table entries.</p><p>These LOOCV-based effectiveness predictions were used to choose two sets of 84 classifiers, one set submitted for T10SU (run DLewis01bfUa) and one for T10F (run DLewis01bfFa).</p><p>I also used these two sets of classifiers to rank the test documents and submitted rankings of the top 1000 documents for each topic (runs DLewis01rUa and DLewis01rFa) for the routing evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Text Representation</head><p>SVMs, like most approaches in both machine learning and IR, require text to be reduced to vectors of numeric feature values.</p><p>Our text processing was minimal, consisting of downcasing the text, replacing numbers and punctuation with whitespace, and breaking the text into tokens at whitespace. No stemming was used. I discarded words on the SMART stoplist.</p><p>One innovation was motivated by the robustness of SVMs to large feature sets. It is common in IR to give additional prominence in a document representation to words that occur in the document title, for instance by counting them twice. It seems likely that this is a good strategy for some words but a poor one for others. To allow the learning algorithm to choose, I created two sets of features:</p><p>1. A set of binary features corresponding to the presence or absence of each word in the title.</p><p>2. A set of features for the total number of words in both the title and body of the document. Log TF weighting was applied to this feature set.</p><p>These two sets of features were combined into a single feature set used to represent documents. With respect to this feature set, there are linear models that give a variety of ranges of prominence to title words vs. words in the document body.</p><p>No IDF weighting or other corpus weighting was used. I did, however, apply cosine normalization to the feature vectors, as the default settings of SVM_Light are designed with this in mind.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>See the Appendix to these proceedings for the raw data. Comparing each of my runs to the other 18 submitted runs as evaluated by the measure I submitted that run for, I found:</p><p>• DLewis01bfUa, my T10SU run, had greater than or equal to median T10SU on 82 of 84 topics, and equaled the maximum T10SU score on 61 of 84 topics.</p><p>• DLewis01bfFa, my T10F run, had greater than or equal to median T10F on 81 of 84 topics, and equaled the maximum T10F score on 49 of 84 topics.</p><p>• DLewis01rFa, my T10F run as submitted for the routing evaluation, had greater than or equal to median SAP (simple average precision) on 83 of 84 topics, and equaled the maximum SAP score on 61 of 84 topics.</p><p>What I had intended to be quick-and-dirty runs using OPS (other people's software) were unusually dominant in a relatively mature TREC track. I believe the most important factors in this performance were:</p><p>1) The good fit of the SVM approach to text classification problems, 2) Exploring a range of relative weightings of positive and negative examples as a uniform way of addressing both the scaling and thresholding problems, and 3) Use of the computationally expensive but very accurate LOO approach for choosing a distinct relative weighting for each topic. Future work will test the relative importance of these factors versus, for example, choice of text representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Afterword: High Frequency Topics for Dinner? Yum!</head><p>Prior to the submission of TREC Filtering runs, I made two dinner bets on the outcome of the routing evaluation. The outcome of one of these has been determined as of the writing of this paper. Here's the history:</p><p>1. Avi Arampatzis wrote <ref type="bibr" coords="6,210.73,143.51,86.80,10.80">(15-August-2001)</ref> to the TREC filtering mailing list, worrying that using only the top 1000 docs in the routing evaluation wouldn't be meaningful, because there were too many positive test documents.</p><p>2. As part of the discussion of Avi's obvservation, I wrote: "While I have not looked at the test data labels, I'll go out on a limb and predict that many groups will have have <ref type="bibr" coords="6,493.45,212.63,20.80,10.80">[sic]</ref> test set precision @ 1000 over 90% for a nontrivial number of topics. That suggests that any interesting differences between systems will only kick <ref type="bibr" coords="6,367.21,240.23,20.80,10.80">[sic]</ref> among documents well below rank 1000..." 4. Chris Buckley wrote "I'd be surprised with P @ 1000 of over 90% for any topics except those that are defined by a single keyword. That's a comment about reliability of the target categorization, not on system performance. Ie, the system may find 950 documents that should be in the category, but only 850 of those were actually assigned the category." 5. I wrote Chris off the list betting dinner that some system would get P @ 1000 of over 90% for some topic that was not defined by a single keyword. We discussed a bit how "single keyword" would defined and he accepted. (Basically, if Chris can write a single word query that gets P @ 1000 of 90% or more, the topic doesn't count.) 6. Separately, Paul Kantor wrote me and the list that "I will buy you a dinner if any system gets 90% @ 1000 for any topic." These were much looser terms than I'd already proposed to Chris, so I happily accepted.</p><p>7. Paul conceded on the list on September 6, 2001, after the preliminary results were released and several groups reported 90% @ 1000 results on 30 or so of the topics. I had a nice dinner with Paul at TREC 2001. 8. I am eagerly awaiting the result of Chris Buckley's single word queries to see who buys dinner, perhaps in Finland at SIGIR 2002.</p><p>Precision of 90% at 1000 documents seems to conflict with that fact that interindexer consistency rates are often 60% or lower <ref type="bibr" coords="6,288.01,599.03,85.42,10.80" target="#b0">[Cleverdon, 1991]</ref>. If two people can't agree more often than that, how can a machine do better? However, interindexer consistency is measuring agreement on an entire collection of documents. (In practice it is typically estimated by sampling techniques.) I made the above bets based on a suspicion that high scoring documents are different from collections as a whole. That is, if a well-trained statistical classifier is very confident a document belongs to a class, then it must be an "easy" document that almost any human indexer would assign to the class. The interindexer consistency, and thus the possible machine/indexer consistency, would be much higher than average. Since some routing topics in TREC 2001 were clearly going to have 10's of thousands of relevant documents, the top 1000 would consist of documents with very high scores.</p><p>In addition, at the time I made the bets I already knew that SVM_Light's LOOCV estimates of precision on the entire training set were above 90% for some topics. Since LOOCV estimates are (almost) statistically unbiased, I was confident that results on the entire test set would be similar, and that on the top 1000 would be even better. As it turned out, some groups had precision of 100% on their top 1000 test documents for some topics.</p><p>On a serious note, it's clear that Avi Arampatzis' original worry was on the mark: evaluating routing runs on only the top 1000 documents meant that there was very little distinction among systems for high frequency topics. I will go further, and suggest that any ranking-oriented evaluation is of questionable interest when one has both substantial training data and thousands of relevant documents.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,90.01,132.23,425.57,300.48"><head></head><label></label><figDesc>My experiments focused on the relative weighting of positive and negative training examples. This was due to two problems I anticipated with using SVMs: Past text categorization experiments have suggested that SVMs are less dominant over competing algorithms on categories with very few positive training examples. A plausible explanation is that the orientation of the learned hyperplane is being determined almost completely by the negative examples. In some machine learning tasks, the positive and negative classes are equally coherent, and a classifier fit to either will produce good effectiveness on the binary classification problem. This is rarely true in text categorization, however. The positive class is typically a coherent</figDesc><table coords="2,90.01,173.51,425.57,259.20"><row><cell>Problem 1. gives equal weight to positive</cell></row><row><cell>and negative examples. However, the TREC batch filtering task used two effectiveness</cell></row><row><cell>measures, T10SU and T10F, which give unequal weights to positive and negative</cell></row><row><cell>examples and, moreover, were likely to require two different classifiers to optimize. (See</cell></row><row><cell>the TREC-2001 filtering track report in this volume for more on these measures.) This</cell></row><row><cell>again suggested paying attention to the weighting of positive and negative examples.</cell></row></table><note coords="2,90.01,270.23,406.91,10.80;2,90.01,283.91,413.52,10.80;2,90.01,297.83,327.72,10.80;2,90.01,325.19,409.55,10.80;2,90.01,339.11,416.27,10.80;2,90.01,352.79,274.32,10.80"><p><p>subset (e.g. "Retail Sales") of all possible documents, but the negative class is the less well-defined "everything else". Therefore, telling SVM_Light to pay more attention to positive examples for low frequency classes seemed like a good idea.</p>Problem 2. SVM_Light by default optimizes a margin-based loss measure which gives equal weight to positive and negative examples. It has been proven that optimizing this measure will tend to lead to low error rate; error rate also</p></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,95.37,339.11,382.49,10.80;7,90.01,352.79,411.00,10.80;7,90.01,366.71,426.29,10.80;7,90.01,380.39,415.80,10.80;7,90.01,394.31,185.64,10.80" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,247.21,339.11,230.64,10.80;7,90.01,352.79,48.92,10.80">The Significance of the Cranfield Tests on Index Languages</title>
		<author>
			<persName coords=""><forename type="first">;</forename><forename type="middle">C</forename><surname>Cleverdon</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cleverdon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,90.01,366.71,426.29,10.80;7,90.01,380.39,265.43,10.80">SIGIR &apos;91: Proceedings of the Fourteenth Annual International ACM/SIGIR Conference on Research and Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Bookstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Chiaramella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Raghavan</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="1991">1991. 1991</date>
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,174.97,421.91,298.56,10.80;7,90.01,435.59,428.88,10.80;7,90.01,449.51,160.20,10.80;7,90.01,477.11,405.23,10.80;7,90.01,490.79,357.00,10.80;7,90.01,518.39,388.80,10.80;7,90.01,532.31,387.76,10.80;7,90.01,545.99,422.88,10.80;7,90.01,559.91,425.92,10.80;7,90.01,573.59,109.80,10.80" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,238.33,421.91,214.33,10.80;7,235.21,477.11,260.03,10.80;7,90.01,490.79,47.41,10.80;7,206.65,518.39,272.16,10.80;7,90.01,532.31,34.97,10.80">Evaluating and optimizing autonomous text classification systems</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,90.01,435.59,266.11,10.80;7,144.97,490.79,266.13,10.80;7,425.77,532.31,52.00,10.80;7,90.01,545.99,422.88,10.80;7,90.01,559.91,181.67,10.80">SIGIR &apos;95: Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">Edward</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Peter</forename><surname>Ingwersen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Raya</forename><surname>Fidel</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="1995">1999. Joachims 2000. 2000. Lewis 1995. 1995</date>
			<biblScope unit="page" from="246" to="254" />
		</imprint>
	</monogr>
	<note>Advances in Kernel Methods -Support Vector Learning</note>
</biblStruct>

<biblStruct coords="7,95.11,601.19,413.10,10.80;7,90.01,615.11,413.87,10.80;7,90.01,628.79,429.01,10.80;7,90.01,642.71,405.00,10.80;7,90.01,656.39,191.40,10.80;7,90.01,683.99,420.72,10.80;7,90.01,697.91,352.68,10.80" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,414.73,601.19,93.47,10.80;7,90.01,615.11,112.45,10.80;7,316.57,683.99,194.16,10.80;7,90.01,697.91,131.26,10.80">Text categorization based on regularized linear classification methods</title>
		<author>
			<persName coords=""><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,183.13,628.79,335.89,10.80;7,90.01,642.71,323.75,10.80">SIGIR &apos;96: Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">Hans-Peter</forename><surname>Frei</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Peter</forename><surname>Schauble</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ross</forename><surname>Wilkinson</surname></persName>
		</editor>
		<meeting><address><addrLine>Konstanz</addrLine></address></meeting>
		<imprint>
			<publisher>Hartung-Gorre Verlag</publisher>
			<date type="published" when="1996">1996. 1996. 2001. 2001</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="5" to="31" />
		</imprint>
	</monogr>
	<note>Zhang and Oles</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
