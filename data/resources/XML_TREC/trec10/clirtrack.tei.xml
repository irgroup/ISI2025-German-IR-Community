<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,81.72,151.80,434.13,15.49;1,97.56,173.76,402.50,15.49">The TREC-2001 Cross-Language Information Retrieval Track: Searching Arabic using English, French or Arabic Queries</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,263.16,207.59,71.26,10.76"><forename type="first">Fredric</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
							<email>gey@ucdata.berkeley.edu</email>
						</author>
						<author>
							<persName coords="1,257.40,277.31,82.82,10.76"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
							<email>oard@glue.umd.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">UC DATA</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">College of Information Studies and Institute for Advanced Computer Studies</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,81.72,151.80,434.13,15.49;1,97.56,173.76,402.50,15.49">The TREC-2001 Cross-Language Information Retrieval Track: Searching Arabic using English, French or Arabic Queries</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DBC8DDB5228B762B8929A1C67A03E31E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ten groups participated in the TREC-2001 cross-language information retrieval track, which focussed on retrieving Arabic language documents based on 25 queries that were originally prepared in English. French and Arabic translations of the queries were also available. This was the first year in which a large Arabic test collection was available, so a variety of approaches were tried and a rich set of experiments performed using resources such as machine translation, parallel corpora, several approaches to stemming and/or morphology, and both pre-translation and post-translation blind relevance feedback. On average, forty percent of the relevant documents discovered by a participating team were found by no other team, a higher rate than normally observed at TREC. This raises some concern that the relevance judgment pools may be less complete than has historically been the case.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For the 2001 Text Retrieval Conference (TREC-2001), the Cross-Language Information Retrieval (CLIR) task was to utilize English (or French) queries against Arabic documents. Monolingual Arabic experiment designs in which both the queries and the documents were in Arabic were also supported. This was the eighth year in which non-English document retrieval has been evaluated at TREC, and the fifth year in which cross-language information retrieval has been the principal focus of that work. In TREC-3, retrieval of 25 topics against a Mexican newspaper corpus was tested by four groups. Spanish language retrieval was evaluated in TREC-3, TREC-4 (another 25 topics for the same Mexican corpus), and TREC-5 (where an European Spanish corpus was used). In TREC-5, a Chinese language track was introduced using both newspaper (People's Daily) and newswire (Xinhua) sources from People's Republic of China, and 25 Chinese topics with an English translation supplied. The TREC-5 corpus was represented with the GB character set of simplified Chinese. The Chinese monolingual experiments on this collection that were done in TREC-5 and TREC-6 sparked research into the application of Chinese text segmentation to information retrieval using dictionary-based methods and statistical techniques, and simpler overlapping bigram segmentation methods were also found to be effective. TREC-6, TREC-7 and TREC-8 had the first cross language tracks, which focussed upon European languages (English, French, German, and later Italian). Following TREC-8, the venue for European-language retrieval evaluation moved to Europe with the creation of the Cross-Language Evaluation Forum (CLEF), first held in Lisbon in September 2000 <ref type="bibr" coords="2,191.16,139.43,11.61,9.82" target="#b0">[1]</ref>. For TREC-9, the CLIR task used Chinese documents from Hong Kong.</p><p>In distinction from the earlier TREC-5/6 Chinese corpus, these sources were written in the traditional Chinese character set and encoded in BIG5. Following TREC-9 the evaluation of English-Chinese retrieval moved to the NTCIR Evaluation that is coordinated by the National Institute of Informatics in Japan (http://research.nii.ac.jp/ntcir/workshop/work-en.html).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head><p>As in past TREC CLIR evaluations, the principal task for each group was to match topics in one language (English or French, in this case) with documents in another language (Arabic) and return a ranked list of the top 1000 documents associated with each topic. Participating groups were allowed to submit as many as five runs, with at least one using only the title and description field of the topic description. Evaluation then proceeded by pooling ranks and manual examination of the pools by human judges to decide binary (yes/no) relevance for each document in the pool with respect to each topic. A suite of statistics were then calculated, with the mean (over 25 topics) uninterpolated average being the most commonly reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Topics</head><p>Twenty-five topic descriptions (numbered AR1-AR25) were created in English in a collaborative process between the LDC and NIST. An example of a topic description is:  The Linguistic Data Consortium also prepared an Arabic translation of the topics, so participating teams also had the option of doing monolingual (Arabic-Arabic) retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Documents</head><p>The document collection used in the TREC-2001 CLIR track consisted of 383,872 newswire stories that appeared on the Agence France Press (AFP) Arabic Newswire between 1994 and 2000. The documents were represented in Unicode and encoded in UTF-8, resulting in a 896 MB collection. A typical document is shown in Figure <ref type="figure" coords="3,179.88,556.31,4.09,9.82" target="#fig_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Relevance Judgments</head><p>The ten participating research teams shown in Table <ref type="table" coords="3,307.32,616.19,5.45,9.82">1</ref> together produced 24 automatic cross-language runs with English queries, 3 automatic cross-language runs with French queries, 19 automatic monolingual runs with Arabic queries, and 2 manual runs (one with English queries and one with Arabic queries). From these, 3 runs were selected from each team in a preference order recommended by the participants for use in forming assessment pools. The resulting pools were formed from 15 cross-language runs with English queries, 1 cross-language run with French queries, and 14 monolingual runs with Arabic queries. The top-ranked 70 documents for a topic in each of the 30 ranked lists were added to the judgment pool for that topic, duplicates were removed, and the documents then sorted in a canonical order designed to prevent the human judge from inferring the rank assigned to a document by any system. Each document in the pool was then judged for topical relevance, usually by the person that had originally written the topic statement. The mean number of relevant documents that were found for a topic was 165.</p><p>Most documents remain unjudged when pooled relevance assessments are used, and the usual procedure is to treat unjudged documents as if they are not relevant. Voorhees has shown that the preference order between automatic runs in the TREC ad hoc retrieval task would rarely be reversed by the addition of missing judgments, and that the relative reduction in mean uninterpolated average precision that would result from removing "uniques" (relevant documents found by only a single system) from the judgment pools was typically less than 5% <ref type="bibr" coords="4,217.56,440.15,11.61,9.82" target="#b1">[2]</ref>. As Figure <ref type="figure" coords="4,283.20,440.15,5.45,9.82" target="#fig_2">2</ref> shows, this effect is substantially larger in the TREC-2001 Arabic collection, with 9 of the 28 judged automatic runs experiencing a relative reduction in mean uninterpolated average precision of over 10% relative when the "uniques" contributed by that run were removed from the judgment pool.</p><p>Figure <ref type="figure" coords="4,120.48,494.39,5.45,9.82" target="#fig_3">3</ref> helps to explain this unexpected condition, illustrating that many relevant documents were found by only a single participating research team. For 7 of the 25 topics, more than half of the known relevant documents were ranked in the top-70 in runs submitted by only a single research team. For another 6 of the 25 topics, between 40 and 50 percent of their relevant documents were ranked in the top-70 by only one team.</p><p>These results show a substantial contribution to the relevance pool from each site, with far less overlap than has been typical in previous TREC evaluations. This limited degree of overlap could result from the following factors: ¢ A preponderance of fairly broad topics for which many relevant documents might be found in the collection. The average of 165 relevant documents per topic is somewhat greater than the value typically seen at TREC (100 or so).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>¢</head><p>The limitation of the depth of the relevance judgment pools to 70 documents (100 documents per run have typically been judged in prior TREC evaluations).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>¢</head><p>The diversity of techniques tried by the participating teams in this first year of Arabic retrieval experiments at TREC, which could produce richer relevance pools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>¢</head><p>A relatively small number of participating research teams, which could interact with the diversity of the techniques to make it less likely that another team would have tried a technique that would find a similar set of documents.</p><p>The first two factors have occasionally been seen in information retrieval evaluations based on pooled assessment methodologies (TREC, CLEF, and NTCIR) without the high "uniques" effect observed on this collection. We therefore suspect that the dominant factors in this case may be the last two. But until this cause of the high "uniques" effect is determined, relative differences of less than 15% or so in unjudged and post hoc runs using this collection should be regarded as suggestive rather than conclusive. There is, of course, no similar concern for comparisons among judged runs since judgments for their "uniques" are available.</p><p>As has been seen in prior evaluations in other languages, manual and monolingual runs provided a disproportionate fraction of the known relevant documents. For example, 33% of the relevant documents that were found by only one team were found only by monolingual runs, while 63% were found only by cross-language runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Table <ref type="table" coords="5,98.16,608.15,5.45,9.82">1</ref> summarizes the alternative indexing terms, the query languages, and (for cross-language runs) the sources of translation knowledge that were explored by the ten participating teams. All ten participating teams adopted a "bag-of-terms" technique based on indexing statistics about the occurrence of terms in each document. A wide variety of specific techniques were used, including language models, hidden Markov models, vector space models, inference networks, and the PIRCS connectionist network. Four basic types of indexing terms were explored, sometimes separately and sometimes in combination:</p><p>Words. Indexing word surface forms found by tokenizing at white space and punctuation requires no language-specific processing (except, perhaps, for stopword removal), but potentially desirable matches between morphological variants of the same word (e.g., plural and singular forms) are Arabic Terms Indexed</p><formula xml:id="formula_0" coords="6,88.80,113.27,420.03,164.86">Query Translation Resources Used Team Word Stem Root £ -gram Lang MT Lexicon Corpus Translit BBN X A,E X X X Hummingbird X A IIT X X X A,E X X JHU-APL X X A,E,F X NMSU X X A,E X Queens X X A,E X UC Berkeley X A,E X X U Maryland X X X X A,E X X U Mass X X A,E X X U Sheffield X A,E,F X</formula><p>Table <ref type="table" coords="6,208.44,305.15,4.24,9.82">1</ref>: Configurations tested by participating teams.</p><p>precluded. As a result, word indexing yielded suboptimal retrieval effectiveness (by the mean uninterpolated average precision measure). Many participating research teams reported results for word-only indexing, making that condition useful as a baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stems.</head><p>In contrast to English, where stems are normally obtained from the surface form of words by automatically removing common suffixes, both prefixes and suffixes are normally removed to obtain Arabic stems. Participating teams experimented with stemming software developed at three participating sites (IIT, NMSU, and U Maryland) and from two other sources (Tim Buckwalter and Shereen Khoja).</p><p>Roots. Arabic stems can be generated from a relatively small set of root forms by expanding the root using standard patterns, some of which involve introduction of infixes. Stems generated from the same root typically have related meanings, so indexing roots might improve recall (possibly at the expense of precision, though). Although humans are typically able to reliably identify the root form of an Arabic word by exploiting context to choose between alternatives that would be ambiguous in isolation, automatic analysis is a challenging task. Two participating teams reported results based on automatically determined roots.</p><p>Character £ -grams. As with other languages, overlapping character £ -grams offer a useful alternative to techniques based on language-specific stemming or morphological analysis. Three teams explored £ -grams, with values of £ ranging from 3-6.</p><p>Term formation was typically augmented by one or more of the following additional processing steps:</p><p>Character deletion. Some Unicode characters, particularly diacritic marks, are optional in Arabic writing. This is typically accommodated by removing the characters when they are present, since their presence in the query but not the document (or vice-versa) might prevent a desired match.</p><p>Character normalization. Some Arabic letters have more than one Unicode representation because their written form varies according to morphological and morphotactic rules, and in some cases authors can use two characters interchangeably. These issues are typically accommodated by mapping the alternatives to a single normalized form.</p><p>Figure <ref type="figure" coords="7,104.16,378.11,4.24,9.82">4</ref>: Cross-language retrieval effectiveness, English queries formed from title+description fields, automatic runs.</p><p>Stop-term removal. Extremely frequent terms and other terms that system developers judge to be of little use for retrieval are often removed in order to reduce the size of the index. Stop-term removal is most commonly done after stemming or morphological analysis in Arabic because the highly productive morphology would otherwise result in impractically large stopword lists.</p><p>Nine of the ten participating research teams submitted cross-language retrieval runs, with all nine using a query-translation architecture. Both of the teams that tried French queries used English as a pivot language for French-to-Arabic query translation, so English-to-Arabic resources were key components in every case. Each team explored some combination of the following four types of translation resources: Machine Translation Systems. Two machine translation systems were used: (1) a system developed by Sakhr (available at http://tarjim.ajeeb.com, and often referred to simply as "Ajeeb" or "Tarjim"), a system produced by ATA Software Technology Limited (available at http://almisbar.com, and sometimes referred to as "Almisbar" or by the prior name "Al-Mutarjim"). At the time of the experiments, both offered only English-to-Arabic translation. Some teams used a machine translation system to directly perform query translation, others used translations obtained from one or both of these systems as one source of evidence from which a translated query was constructed.</p><p>A mark in the "MT" column of Table <ref type="table" coords="7,271.80,641.51,5.45,9.82">1</ref> indicates that one or more existing machine translation systems were used in some way, not that they were necessarily used to directly perform query translation.</p><p>Translation Lexicons. Three commercial machine readable bilingual dictionaries were used: one marketed by Sakhr (sometimes referred to as "Ajeeb"), one marketed by Ectaco Inc., (typically referred to as "Ectaco"), and one marketed by Dar El Ilm Lilmalayin (typically referred to as "Al Mawrid").</p><p>In addition, one team (NMSU) used a locally produced translation lexicon.</p><p>Figure <ref type="figure" coords="8,104.16,320.63,4.24,9.82">5</ref>: Cross-language topic difficulty, uninterpolated average precision (base of each bar: median over 28 runs, top of each bar: best of the 28 runs).</p><p>Parallel Corpora. One team (BBN) obtained a collection of documents from the United Nations that included translation-equivalent document pairs in English and Arabic. Word-level alignments were created using statistical techniques and then used as a basis for determining frequently observed translation pairs.</p><p>Transliteration. One team (Maryland) used pronunciation-based transliteration to produce plausible Arabic representations for English terms that could not otherwise be translated.</p><p>When multiple alternative translations were known for a term, a number of techniques were used to guide the combination of evidence, including: (1) translation probabilities obtained from parallel corpora, (2) relative term frequency for each alternative in the collection being searched, and (3) structured queries. Pre-translation and/or post-translation query expansion using blind relevance feedback techniques and pretranslation stop-term removal were also explored by several teams.</p><p>To facilitate cross-site comparison, teams submitting automatic cross-language runs were asked to submit at least one run in which the query was based solely on the title and description fields of the topic descriptions. Figure <ref type="figure" coords="8,188.28,561.47,5.45,9.82">4</ref> shows the best recall-precision curve for this condition by team. All of the top-performing cross-language runs used English queries.</p><p>As is common in information retrieval evaluations, substantial variation was observed in retrieval effectiveness on a topic-by-topic basis. Figure <ref type="figure" coords="8,280.80,602.15,5.45,9.82">5</ref> illustrates this phenomenon over the full set of crosslanguage runs (i.e., not limited to title+description queries). For example, half of the runs did poorly on topic 12, which included specialized medical terminology, but at least one run achieved a perfect score on that topic. Topic 5, by contrast, turned out to be problematic for all systems.</p><p>No standard condition was required for monolingual runs, so Figure <ref type="figure" coords="8,382.20,656.39,5.45,9.82">6</ref> shows the best monolingual run by team regardless of the experiment conditions. Several teams observed surprisingly small differences between monolingual and cross-language retrieval effectiveness. One site (JHU-APL) submitted runs under similar conditions for all three topic languages, and Figure <ref type="figure" coords="8,352.80,696.95,4.39,9.82" target="#fig_5">7</ref>(a) shows the resulting recall-precision graphs by topic language. In that case, there is practically no difference between English-topic and Arabic-topic results. There are two possible explanations for this widely observed effect: Figure <ref type="figure" coords="9,102.60,357.59,4.24,9.82">6</ref>: Monolingual retrieval effectiveness, Arabic queries formed from title+description fields (except JHU-APL and UC Berkeley, which also used the narrative field), automatic runs (except U Maryland, which was a manual run).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>¢</head><p>No large Arabic information retrieval test collection was widely available before this evaluation, so the monolingual Arabic baseline systems created by participating teams might be improved substantially in subsequent years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>¢</head><p>The 25 topics used in this year's evaluation might represent a biased sample of the potential topic space. For example, relatively few topic descriptions this year included names of persons. Several teams also observed that longer queries did not yield the improvements in retrieval effectiveness that would normally be expected. One site (Hummingbird) submitted runs under similar conditions for three topic lengths, and Figure <ref type="figure" coords="9,225.60,530.27,4.54,9.82" target="#fig_5">7</ref>(b) shows the resulting recall-precision graphs. In this case, longer queries showed no discernible benefit; indeed, it appears that the best results were achieved using the shortest queries! The reasons for this effect are not yet clear, but one possibility is that the way in which the topic descriptions were created may have resulted in a greater concentration of useful search terms in the title field. For example, the title fields contains an average of about 6 words, which is about twice as long as is typical for TREC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Summary and Outlook</head><p>The TREC-2001 CLIR track focussed this year on searching Arabic documents using English, French or Arabic queries. In addition to the specific results reported by each research team, the evaluation produced the first large Arabic information retrieval test collection. A wide range of index terms were tried, some useful language-specific processing techniques were demonstrated, and many potentially useful translation resources were identified. In this paper we have provided an overview of that work in a way that will help readers recognize similarities and differences in the approaches taken by the  participating teams. We have also sought to explore the utility of the test collection itself, providing aggregate information about topic difficulty that individual teams may find useful when interpreting their results, identifying a potential concern regarding the completeness of the pools of documents that were judged for relevance, and illustrating a surprising insensitivity of retrieval effectiveness to query length.</p><p>The TREC-2002 CLIR track will continue to focus on searching Arabic. We plan to use 50 new topics (in the same languages) and to ask participating teams to also rerun the 25 topics from this year with their improved systems as a way of further enriching the existing pools of documents that have been judged for relevance. We expect that the result with be a test collection with enduring value for post hoc experiments, and a community of researchers that possess the knowledge and resources needed to address this important challenge.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,107.76,435.35,14.24,9.82;2,107.76,448.91,98.78,9.82;2,107.76,462.47,247.96,9.82;2,107.76,476.03,84.53,9.82;2,99.24,489.59,304.11,9.82;2,99.24,503.15,93.48,9.82;2,107.76,516.59,72.44,9.82;2,99.24,530.15,294.81,9.82;2,99.24,543.71,316.87,9.82;2,99.24,557.27,311.05,9.82;2,99.24,570.83,135.94,9.82;2,107.76,584.39,17.24,9.82;2,88.92,620.51,436.63,9.82;2,72.00,633.95,453.40,9.82;2,72.00,647.51,209.51,9.82;2,107.76,670.07,14.24,9.82;2,107.76,683.63,98.78,9.82;2,107.76,697.19,302.16,9.82;2,107.76,710.75,84.53,9.82;2,99.24,724.31,399.07,9.82"><head></head><label></label><figDesc>top¡ num¡ Number: AR22 title¡ Local newspapers and the new press law in Jordan desc¡ Description: Has the Jordanian government closed down any local newspapers due to the new press law? narr¡ Narrative: Any articles about the press law in Jordan and its effect on the local newspapers and the reaction of the public and journalists toward the new press law are relevant. The articles that deal with the personal suffering of the journalists are irrelevant. /top¡ Through the efforts of Edouard Geoffrois of the French Ministry of Defense, the English topics were translated into French and made available to participants which wished to test French to Arabic retrieval. The French version of the topic shown above is: top¡ num¡ Number: AR22 title¡ Les journaux locaux et la nouvelle loi sur la presse en Jordanie desc¡ Description: Le gouvernement jordanien a-t-il interdit un journal local à cause de la nouvelle loi sur la</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,217.44,314.15,162.75,9.82;3,118.80,108.81,360.00,189.27"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example Arabic document.</figDesc><graphic coords="3,118.80,108.81,360.00,189.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,119.76,312.83,358.07,9.82;4,118.80,108.83,359.99,187.93"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Effect on 29 judged runs of removing "uniques" contributed by that run.</figDesc><graphic coords="4,118.80,108.83,359.99,187.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,176.52,343.43,244.61,9.82;5,118.80,108.93,359.99,218.43"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Unique relevant documents, by research team.</figDesc><graphic coords="5,118.80,108.93,359.99,218.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="10,78.00,330.35,232.33,9.82;10,341.40,330.35,172.11,9.82"><head></head><label></label><figDesc>(a) Topic language effect, title+description+narrative. (b) Query length effect, Arabic queries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="10,90.72,364.79,416.08,9.82;10,90.12,109.84,208.80,214.40"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Comparing runs under comparable conditions (T=title, D=Description, N=Narrative).</figDesc><graphic coords="10,90.12,109.84,208.80,214.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="7,118.80,108.81,359.99,253.23"><head></head><label></label><figDesc></figDesc><graphic coords="7,118.80,108.81,359.99,253.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="8,118.80,108.87,360.01,195.69"><head></head><label></label><figDesc></figDesc><graphic coords="8,118.80,108.87,360.01,195.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="9,118.80,108.89,359.99,232.63"><head></head><label></label><figDesc></figDesc><graphic coords="9,118.80,108.89,359.99,232.63" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We are grateful to <rs type="person">Ellen Voorhees</rs> for coordinating this track at <rs type="institution">NIST</rs> and for her extensive assistance with our preliminary analysis, to the participating research teams for their advice and insights along the way, and to <rs type="person">Kareem Darwish</rs> for his comments on an earlier version of this paper.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="10,90.12,655.43,435.45,9.82;10,90.12,668.99,214.38,9.82" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,153.96,655.43,240.20,9.82">Cross-Language Information Retrieval and Evaluation</title>
		<author>
			<persName coords=""><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,451.20,655.43,74.37,9.82;10,90.12,668.99,137.00,9.82">Lecture Notes in Computer Science: LNCS 2069</title>
		<meeting><address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.12,690.71,435.31,9.82;10,90.12,704.27,21.51,9.82" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="10,181.92,690.71,343.52,9.82;10,90.12,704.27,17.21,9.82">Variations in relevance judgments and the mesaurement of retrieval effectiveness</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,115.56,704.27,409.99,9.82;10,90.12,717.83,435.48,9.82;10,90.12,731.39,158.10,9.82" xml:id="b2">
	<monogr>
		<title level="m" coord="10,389.28,704.27,136.28,9.82;10,90.12,717.83,403.85,9.82">Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alistair</forename><surname>Moffat</surname></persName>
		</editor>
		<meeting>the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1998-08">August 1998</date>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
