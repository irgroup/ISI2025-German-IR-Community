<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,98.28,87.46,415.26,12.64;1,267.12,103.54,77.65,12.64">TREC2001 Question-Answer, Web and Cross Language Experiments using PIRCS</title>
				<funder ref="#_YdmPBVC">
					<orgName type="full">Space and Naval Warfare Systems Center San Diego</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,187.92,140.79,50.54,10.80"><forename type="first">K</forename><forename type="middle">L</forename><surname>Kwok</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Queens College</orgName>
								<address>
									<postCode>11367</postCode>
									<settlement>CUNY Flushing</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,247.92,140.79,54.15,10.80"><forename type="first">L</forename><surname>Grunfeld</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Queens College</orgName>
								<address>
									<postCode>11367</postCode>
									<settlement>CUNY Flushing</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,310.20,140.79,44.02,10.80"><forename type="first">N</forename><surname>Dinstl</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Queens College</orgName>
								<address>
									<postCode>11367</postCode>
									<settlement>CUNY Flushing</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,377.52,140.79,42.00,10.80"><forename type="first">M</forename><surname>Chan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Queens College</orgName>
								<address>
									<postCode>11367</postCode>
									<settlement>CUNY Flushing</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,98.28,87.46,415.26,12.64;1,267.12,103.54,77.65,12.64">TREC2001 Question-Answer, Web and Cross Language Experiments using PIRCS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">95345F8A6FB332E8630F5F932DCF6D4A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We applied our PIRCS system for the Question-Answer, ad-hoc Web retrieval using the 10-GB collection, and the English-Arabic cross language tracks. These are described in Sections 2,3,4 respectively. We also attempted to complete the adaptive filtering experiments with our upgraded programs but found that we did not have sufficient time to do so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Question-Answering (QA) Track</head><p>The QA Track requires obtaining 50-byte answer strings to 500 questions (later truncated to 492). The answers are to be retrieved from documents made up from the TREC collections: AP1-3, WSJ1-2, SJMN-3, FT-4, LA-5 and FBIS-5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Approach</head><p>Our QA system is constructed using methods of classical IR, enhanced with simple heuristics. It does not have natural language understanding capabilities, but employs simple pattern matching and statistics. We view QA as a three-step process: 1) retrieving a set of documents that are highly related to the topic of the question; 2) weighing sentences in this document set that are most likely to answer the question according to the query type and its description; and 3) selecting words from the top-scoring sentences to form the answer string. This approach was quite successful for the 250-byte answer task at TREC-9 <ref type="bibr" coords="1,256.80,605.01,10.69,8.96" target="#b0">[1]</ref>. This year we added more heuristics, better pattern recognition and entity recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Methodology</head><p>For the first step, retrieving a set of documents related to the question under focus, we employ both the NIST supplied document list as well as one generated by our PIRCS system. We also use a combination of these two lists that prove to be the best.</p><p>For the second step, weighting prospective sentences in the top ranked list of documents, we continue to employ the methods introduced in TREC-9, which are summarized below:</p><p>1) Coordinate Matching: counting words in common between the question and a document sentence. 2) Stemming: counting stems as opposed to words in 1). We use Porter's algorithm for stemming. 3) Synonyms: matching based on a manually created dictionary of common synonyms. Its size has increased to 420 terms from 300. It also contains unusual word forms, which are not handled well by stemming. Most of the entries were taken directly from Wordnet 4) RSV: use of the retrieval score of a document from PIRCS to resolve ties for sentences that have the same weight based on word or stem matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) ICTF: use of Inverse Collection Term</head><p>Frequency to give more credit to less frequently occurring words. For 'Numeric' answers, heuristics were included to identify the following: a) Units: there are classes of queries, which require units.</p><p>Our system recognizes common units of: length, area, time, speed, currency, temperature and population. b) Date: there are some queries that have a date or year in the question. We require this date to occur in the sentence or within the Date Tag of a document. c) Other entities are recognized such as time, address, telephone number, zip codes and percent. d) Numbers: when no other clues are available.</p><p>Selecting a 50-byte answer from the top sentences is quite a challenge as the third step. We used the proximity to query words criterion in most cases, which misses many answers.</p><p>We also compiled several lists for countries, states, continents and oceans. We felt it may be useful for the list retrieval task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Results and Discussions</head><p>Three runs named pir1Qqa{1,2,3} were submitted: pir1Qqa1 utilized the 50 top documents of the PRISE system; pir1Qqa2 used the top 400 subdocuments retrieved by our PIRCS system; pir1Qqa3 combines the two retrievals. PIRCS preprocesses the original documents and returns subdocuments of about 500 words long. Historically, tag information such as heading and (some) date were not captured in our system, which may result in some small degradation in the final score. Table <ref type="table" coords="2,390.84,213.36,4.18,8.96" target="#tab_1">2</ref>.1 compares the submitted runs to the TREC overall median.</p><p>As shown in Table <ref type="table" coords="2,401.76,247.92,3.77,8.96" target="#tab_1">2</ref>.1, our best entry pir1Qqa3 scored 0.326, 39% above the TREC median. It also demonstrates that combining retrievals is useful and improves over the results from individual retrievals pir1Qqa1 or pir1Qqa2. A new feature of TREC2001 is that a system might mark as NIL for a query that has no definite answer <ref type="bibr" coords="2,348.84,328.44,10.69,8.96" target="#b1">[2]</ref>. Since most correct answers occur at the top positions, a promising strategy is to mark all position 5 answers as NIL. We contemplated doing this but did not do so. The queries may be ranked by the overall performance by all the participants. It is instructive to look at some easy queries that we missed. It happens, that in many cases we retrieved the correct sentences but did not select the correct string. In many cases the correct answer is within the selected answer string, but the other words added (such us names and numbers) make the answer ambiguous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Context and List Tasks</head><p>A week before the deadline we decided to try the context and list tracks by making minor changes. For the context track, we submitted two runs, pir1Qctx2 and pir1Qctx3. They are essentially the same as our main QA system. pir1Qctx1 (unsubmitted) used the PRISE retrieval, pir1Qctx2 used PIRCS retrieval and pir1Qctx3 is a combination as before. <ref type="bibr" coords="3,183.60,581.29,15.58,8.96">The</ref>  different in that it combines the series of questions into one query, aiming to retrieve documents that have all or many of the words in the series.</p><p>Considering all questions to be independent and evaluate as in main QA, we get the results shown in Fig. <ref type="figure" coords="3,343.84,145.33,3.85,8.96">2</ref>.3. It seems retrieving on all query words for pirQctx2 did not substantially improve the results. Combination of retrievals again proved its usefulness as pir1Qctx3 outperformed its individual retrievals. The context task is an interesting and important task and more intelligence must be crafted into a system to take advantage of the knowledge gained from a succession of previous questions (which we did not do).</p><p>We made two changes in the QA system with an eye towards improving performance in the list task. We added a list of countries, states and oceans, and we improved our duplicate answer detection, so that similar forms will be considered equivalent and suppressed. We submitted two runs, pir1Qli1 based on PRISE retrieval and pir1Qli2 based on PIRCS retrieval. There was a bug in the second run output routine that truncated all results to the first word.</p><p>&gt; med = med &lt; med pir1Qli1 14 <ref type="bibr" coords="3,375.92,422.41,13.08,8.96" target="#b1">[2]</ref> 10(1) 1 pir1Qli2</p><p>7 <ref type="bibr" coords="3,371.39,436.21,12.57,8.96" target="#b0">[1]</ref> 11(6) 7(7) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Web Track</head><p>The target collection for the Web track is the W10g disks used last year. We submitted three runs: two for title only queries pir1Wt1 and pir1Wt2, and one for all-section query pir1Wa, which is a long query. Last year <ref type="bibr" coords="3,459.36,646.93,10.69,8.96" target="#b0">[1]</ref>, we noticed that several queries returned no documents because the query words are common words and screened out by our Zipf threshold. Returning a random set of documents usually is fruitless. This year, for these 'zero' queries, we did special processing to bring Query Type back words that were screened out due to high frequency, hoping that we might restore some precision value. Documents having these terms within a distance of 5 words in a sentence are considered. For ranking, the minimum distance and the number of such repeats are used, and no second stage retrieval was performed on these queries. This year, there were only 3 such queries (509, 518, 521), but the process was unsuccessful. This is pir1Wt2. For pir1Wt1, we additionally do this process for queries left with one term below threshold. This turns out to depress effectiveness rather than help. Also, we had no spell-check nor punctuation processing, so that queries like #509 ("steroids;what does it do to your body") was not corrected. Query #531 ("Who and whom") contains all stop words and also returns zero precision. Results of our runs are tabulated in Table <ref type="table" coords="4,115.44,670.92,4.18,8.96" target="#tab_5">3</ref>.1 and 2.</p><p>The result for pir1Wt2 is about median. Using all sections of a query pir1Wa does not perform better -we suspect there may be some parameters set wrong in our processing. With respect to high precision, Table <ref type="table" coords="4,384.36,120.24,3.77,8.96" target="#tab_5">3</ref>.2, it appears our system perform better at precision 20 &amp; 30 compared to median.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Cross Language Track</head><p>For Arabic utf-8 coding, the most prevalent twobyte coding is similar to Chinese GB. We think that our Chinese processing can support Arabic with few changes. A student who knows Arabic expressed interest to help us in forming a stopword list and try to find stemming algorithms from the web. A number of such programs were examined, and we eventually discovered that none can process large volumes in reasonable time without drastic re-programming. We also tried to locate an Arabic-English dictionary without success. However, the website for English to Arabic translation (http://tarjin.ajeeb.com) seems useful and good. We had the given English queries translated by using this site. To meet the deadline, we finally decided to use a mixture of n-grams for indexing so that we do not have to rely on linguistic processing. Our representation is to mix 4-gram, 5-gram and single words without stemming or stopword removal.</p><p>We submitted four runs two for monolingual Arabic: pirXAtdn and pirXAtd using all sections, and title with description section respectively. The corresponding runs for English-Arabic cross language runs are: pirXEtdn and pirXEtd. Results are tabulated in Table <ref type="table" coords="4,406.32,494.04,3.77,8.96" target="#tab_7">4</ref> The results are way below median. Apparently, there was an error in the retrieval in that no year 2000 documents were returned in our retrieval list. We corrected the error but result still does not materially change. It also seems that we may have some system problem related to LINUX v7 where we ran this experiment. We did not pursue this cross language track further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We continued experimenting with our QA system based on classical IR methods enhanced with simple heuristics for locating good sentences. It achieved above average results. This year we used better pattern and entity recognition. In the future, more heuristics, increased use of knowledge bases, exploring part-of-speech information and more careful query analysis will be needed for further progress. The context and list tasks were also prepared using the same methodology. They also give respectable average. It may be because the average is low, or it may perhaps show that an IRbased system is quite robust although it may be less intelligent.</p><p>Our web and cross language results are not up to expectation. For the web track, we did not employ more advanced processing such as collection enrichment, term variety, etc. because of time constraints. This year we transferred these two tasks to work on a Linux-PC platform instead of Solaris-SUN. It is possible that some system error may creep in during processing of the Arabic coding.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,316.80,362.88,205.28,346.17"><head>Table 2 .1 QA Results: MRR Values and Comparison with Median</head><label>2</label><figDesc>The bottom 3 lines of the table show the improvement gained by this NIL strategy.</figDesc><table coords="2,316.80,409.24,205.23,299.81"><row><cell>All</cell><cell></cell><cell>Compare to</cell><cell>not</cell><cell>NIL</cell></row><row><cell cols="2">Queries</cell><cell>TREC</cell><cell>NIL</cell><cell>Queries</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Queries</cell></row><row><cell>TREC2001</cell><cell>0.234</cell><cell cols="3">+0% 0.239 0.193</cell></row><row><cell>Official:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>pir1Qqa1</cell><cell>0.300</cell><cell cols="3">+28% 0.333 0.000</cell></row><row><cell>pir1Qqa2</cell><cell>0.314</cell><cell cols="3">+34% 0.348 0.000</cell></row><row><cell>pir1Qqa3</cell><cell>0.326</cell><cell cols="3">+39% 0.362 0.000</cell></row><row><cell>NIL</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Strategy:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>pir1Qqa1</cell><cell>0.317</cell><cell cols="3">+36% 0.330 0.200</cell></row><row><cell>pir1Qqa2</cell><cell>0.328</cell><cell cols="3">+40% 0.342 0.200</cell></row><row><cell>pir1Qqa3</cell><cell>0.340</cell><cell cols="3">+45% 0.355 0.200</cell></row><row><cell cols="5">Pir1Qqa3 has 126 questions with rank 1 answers</cell></row><row><cell cols="5">correct, 39 with rank 2, 22 rank 3, 14 rank 4, and 5</cell></row><row><cell cols="5">rank 5 correct. Since there are 49 questions for</cell></row><row><cell cols="5">which the correct answer is NIL, the aggressive</cell></row><row><cell cols="5">strategy of making every rank 2 answer NIL would</cell></row><row><cell>do even better!</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,90.00,268.80,205.28,77.73"><head>Table 2 .2 MRR Performance by question type.Table 2 .</head><label>22</label><figDesc>2 shows we did well for 'what' questions, both the definition and the longer types, and 'who' questions. The results are not as good for date ('when'), 'what unit' and 'where' type of questions.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="3,90.00,581.29,209.90,134.59"><head>Table 2 .3: Context Task Results</head><label>2</label><figDesc></figDesc><table coords="3,90.00,581.29,209.90,108.80"><row><cell cols="2">PIRCS retrieval is</cell><cell></cell></row><row><cell>MRR</cell><cell cols="2">Compare to</cell></row><row><cell>Score</cell><cell cols="2">TREC Med</cell></row><row><cell>TREC2001 average</cell><cell>0.298</cell><cell>0%</cell></row><row><cell>pir1Qctx1 (unofficial)</cell><cell>0.310</cell><cell>+4%</cell></row><row><cell>pir1Qctx2</cell><cell>0.314</cell><cell>+5%</cell></row><row><cell>pir1Qctx3</cell><cell>0.329</cell><cell>+10%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="3,316.80,462.48,205.26,92.73"><head>Table 2 .4: List Task:. Comparison with MedianTable 2 .</head><label>22</label><figDesc>4 shows the performance of the submitted runs compared with the median of all runs. The un-bracketed values are the actual number better, worse or same as the median; the numbers in square brackets denote best, and the numbers in parenthesis denote worst scores.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="4,83.28,107.33,218.58,300.66"><head>Table 3 .1: Automatic Web Results for 50 Queries Query Type</head><label>3</label><figDesc></figDesc><table coords="4,83.28,107.33,218.58,300.66"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>¡ ¡</cell></row><row><cell></cell><cell>Title:</cell><cell cols="2">Title:</cell><cell>All sections:</cell></row><row><cell></cell><cell>pir1Wt1</cell><cell cols="2">pir1Wt2</cell><cell>pir1Wa</cell></row><row><cell>Relv.Ret</cell><cell>2263 0</cell><cell cols="2">2275 2</cell><cell>2284 6</cell></row><row><cell>(at most)</cell><cell>(3363)</cell><cell cols="2">(3363)</cell><cell>(3363)</cell></row><row><cell>Avg.Prec</cell><cell cols="3">.1660 0 .1742 5</cell><cell>.1715 12</cell></row><row><cell>P@10</cell><cell cols="3">.2220 0 .2160 3</cell><cell>.2780 11</cell></row><row><cell>P@20</cell><cell cols="3">.2070 0 .2110 4</cell><cell>.2370 15</cell></row><row><cell>P@30</cell><cell cols="3">.2013 0 .2040 8</cell><cell>.2220 18</cell></row><row><cell>R.Prec</cell><cell cols="3">.1700 0 .1894 5</cell><cell>.1968 9</cell></row><row><cell cols="2">Title:</cell><cell>Title:</cell><cell></cell><cell>All sections:</cell></row><row><cell cols="2">pir1Wt1</cell><cell cols="2">pir1Wt2</cell><cell>pir1Wa</cell></row><row><cell>&gt;</cell><cell>= &lt;</cell><cell>&gt;</cell><cell>= &lt;</cell><cell>&gt; = &lt;</cell></row><row><cell cols="4">Avg.Prec 24,4 1 25,5 25,4 1 24,5</cell><cell>13,3 2 26,7</cell></row><row><cell cols="5">prec at 10 13,3 17 20,13 14,4 18 18,14 10,2 17 23,13</cell></row><row><cell cols="5">prec at 20 22,5 10 17,10 23,6 12 15,11 14,4 10 26,14</cell></row><row><cell cols="5">prec at 30 21,5 10 19,9 23,7 9 18,10 16,5 10 21,11</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="4,100.80,426.72,183.62,20.48"><head>Table 3 .2: Web Results -Comparison with Median</head><label>3</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="4,321.48,494.04,195.92,205.40"><head>Table 4 .1: Automatic Mono andCross Language Results for 25 Queries</head><label>4</label><figDesc>.1.</figDesc><table coords="4,321.48,517.32,195.92,142.88"><row><cell></cell><cell></cell><cell cols="2">Query Type</cell><cell></cell></row><row><cell></cell><cell>Mono</cell><cell>Cross</cell><cell>Mono</cell><cell>Cross</cell></row><row><cell></cell><cell>tdn</cell><cell>tdn</cell><cell>td</cell><cell>Td</cell></row><row><cell>Relv.Ret</cell><cell>1254</cell><cell>899</cell><cell>974</cell><cell>802</cell></row><row><cell>(at most)</cell><cell>(4122)</cell><cell>(4122)</cell><cell>(4122)</cell><cell>(4122)</cell></row><row><cell cols="2">Avg.Prec .1036</cell><cell>.0440</cell><cell>.0852</cell><cell>.0360</cell></row><row><cell>P@10</cell><cell>.2440</cell><cell>.1280</cell><cell>.1720</cell><cell>.1040</cell></row><row><cell>P@20</cell><cell>.2120</cell><cell>.1220</cell><cell>.1540</cell><cell>.0920</cell></row><row><cell>P@30</cell><cell>.2000</cell><cell>.1200</cell><cell>.1520</cell><cell>.0867</cell></row><row><cell cols="2">R.Prec .1602</cell><cell>.0768</cell><cell>.1405</cell><cell>.0647</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was partly supported by the <rs type="funder">Space and Naval Warfare Systems Center San Diego</rs>, under grant No. <rs type="grantNumber">N66001-1-8912</rs>. We like to thank <rs type="person">Khalid Yehia</rs> and <rs type="person">Peter Deng</rs> for helping us in the Arabic processing.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_YdmPBVC">
					<idno type="grant-number">N66001-1-8912</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,105.48,610.68,189.69,8.96;5,90.00,622.08,205.28,8.96;5,90.00,633.60,205.18,8.96;5,90.00,645.12,205.05,8.96;5,90.00,656.64,125.97,8.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,107.04,622.08,188.24,8.96;5,90.00,633.60,167.49,8.96">TREC-9 cross language, web and questionanswering track experiments using PIRCS</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">L</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Grunfeld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dinstl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chan</surname></persName>
		</author>
		<idno>NIST SP 500-249</idno>
	</analytic>
	<monogr>
		<title level="m" coord="5,279.60,633.60,15.58,8.96;5,90.00,645.12,174.83,8.96">The Ninth Text Retrieval Conference (TREC-9)</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,110.28,679.68,184.86,8.96;5,90.00,691.08,205.21,8.96;5,316.80,74.28,205.28,8.96;5,316.80,85.80,83.85,8.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,176.76,679.68,118.38,8.96;5,90.00,691.08,109.77,8.96">Overview of the TREC-9 Question-Answering Track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,226.92,691.08,68.29,8.96;5,316.80,74.28,205.28,8.96;5,316.80,85.80,13.21,8.96">The Ninth Text Retrieval Conference (TREC-9). NIST SP 500-249</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="71" to="79" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
