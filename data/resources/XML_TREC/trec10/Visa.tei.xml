<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,116.88,121.08,361.43,15.49">Tampere University of Technology at TREC 2001</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,133.32,164.92,46.00,12.91"><forename type="first">Ari</forename><surname>Visa</surname></persName>
							<email>ari.visa@cs.tut.fi</email>
							<affiliation key="aff0">
								<orgName type="institution">Tampere University of Technology</orgName>
								<address>
									<postBox>P.O. Box 553</postBox>
									<postCode>FIN-33101</postCode>
									<settlement>Tampere</settlement>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,188.64,164.92,89.12,12.91"><forename type="first">Jarmo</forename><surname>Toivonen</surname></persName>
							<email>jarmo.toivonen@cs.tut.fi</email>
							<affiliation key="aff0">
								<orgName type="institution">Tampere University of Technology</orgName>
								<address>
									<postBox>P.O. Box 553</postBox>
									<postCode>FIN-33101</postCode>
									<settlement>Tampere</settlement>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,287.64,164.92,78.56,12.91"><forename type="first">Tomi</forename><surname>Vesanen</surname></persName>
							<email>tomi.vesanen@cs.tut.fi</email>
							<affiliation key="aff0">
								<orgName type="institution">Tampere University of Technology</orgName>
								<address>
									<postBox>P.O. Box 553</postBox>
									<postCode>FIN-33101</postCode>
									<settlement>Tampere</settlement>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,376.20,164.92,85.65,12.91"><forename type="first">Jarno</forename><surname>Mäkinen</surname></persName>
							<email>jarno.makinen¡@cs.tut.fi</email>
							<affiliation key="aff0">
								<orgName type="institution">Tampere University of Technology</orgName>
								<address>
									<postBox>P.O. Box 553</postBox>
									<postCode>FIN-33101</postCode>
									<settlement>Tampere</settlement>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,116.88,121.08,361.43,15.49">Tampere University of Technology at TREC 2001</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2F9CB9C6F63A924A717EFF1D7B6F0FDC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present the prototype based text matching methodology used in the Routing Sub-Task of TREC 2001 Filtering Track. The methodology examines texts on word and sentence levels. On the word level the methodology is based on word coding and transforming the codes into histograms by the means of Weibull distribution. On the sentence level the word coding is done in a similar manner as on the word level. But instead of making histograms we use a more simple method. After the word coding, we transform the sentence vectors to sentence feature vectors using Slant transform. The paper includes also description of the TREC runs and some discussion about the results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A common approach to topic detection and tracking is the usage of keywords, especially, in context of Dewey Decimal Classification <ref type="bibr" coords="1,212.16,489.71,12.95,10.76" target="#b2">[3,</ref><ref type="bibr" coords="1,228.48,489.71,9.86,10.76" target="#b1">2]</ref> that is used in United States to classify books. The approach is based on assumption that keywords given by authors or indexers characterize the text well. This may be true, but then one neglects the accuracy. There are also many automatic indexing approaches. A more accurate method is to use all the words of a document and the frequency distribution of words, but the comparison of frequency distributions is a complicated task. Some theories say that the rare words in the word frequency histograms distinguish documents <ref type="bibr" coords="1,296.16,561.95,12.70,10.76" target="#b5">[6]</ref>. Traditionally, information retrieval has roughly been based on a fixed list of index terms <ref type="bibr" coords="1,253.68,576.35,12.95,10.76" target="#b5">[6,</ref><ref type="bibr" coords="1,270.60,576.35,8.57,10.76" target="#b4">5]</ref>, or vector space models <ref type="bibr" coords="1,404.04,576.35,18.93,10.76" target="#b9">[10,</ref><ref type="bibr" coords="1,426.96,576.35,8.57,10.76" target="#b8">9]</ref>. The latter ones miss the information of co-occurrences of words. There are techniques that are capable of considering the co-occurrences of words, as latent semantic analysis <ref type="bibr" coords="1,303.00,605.27,13.94,10.76" target="#b6">[7]</ref> but they are computationally heavy.</p><p>In this paper, we present our methodology and concentrate on tests of content based topic classification, which is highly attractive in text mining. The evolution of the methodology has been earlier discussed in several publications <ref type="bibr" coords="2,213.84,75.83,18.93,10.76" target="#b10">[11,</ref><ref type="bibr" coords="2,237.00,75.83,14.94,10.76" target="#b11">12,</ref><ref type="bibr" coords="2,256.08,75.83,14.19,10.76" target="#b12">13]</ref>. In the second chapter the applied methodology is described. In the third chapter the experiments with the Reuters database are described. The execution times are presented in chapter four. Finally, the methodology and the results are discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>The methodology we applied to the TREC 2001 Routing runs is a multilevel one. It examines the contents of text documents on word and sentence levels.</p><p>The process starts with preprocessing of the training set text. This includes omitting extra spaces and carriage returns, and separating single words with single spaces. With the Reuters database, the preprocessing also includes the removal of the XML tags. The filtered text is next translated into a suitable form for encoding purposes. The encoding of words is a wide subject and there are several approaches for doing it. The word can be recognized and replaced with a code. This approach is sensitive to new words. The succeeding words can be replaced with a code. This method is language sensitive. Or, each word can be analyzed character by character and based on the characters a key entry to a code table is calculated. This approach is sensitive to capital letters and conjugation if the code table is not arranged in a special way.</p><p>We selected the last alternative, because it is accurate and suitable for statistical analysis. A word ¢ is transformed into a number in the following manner:</p><formula xml:id="formula_0" coords="2,256.70,358.15,288.36,34.30">£ ¥¤ §¦ © ! ¦ "¨<label>(1)</label></formula><p>where # is the length of the character string (the word), is the ASCII value of a character within a word ¢ , and is a constant.</p><p>Example: if the word is "c a t", then £ ¥¤ $ &amp;% "' ( 0) 1) 32 4 65 87 !% "' 9 @) A) B2 1% "5 87 C% "' ( 0) 1) 32 ED F5</p><p>(2)</p><p>The encoding algorithm produces a unique code number for each different word. After each word has been converted to a code number, we consider the distribution of these numbers and try to estimate their statistical distribution. Many distributions, e.g. Gamma distribution, are suitable for this purpose. However, it would be advantageous, if the selected distribution had only few parameters and it matched the observed distribution as well as possible. Based on tests with different types of text databases, we selected the Weibull distribution to estimate the distribution of the code numbers.</p><p>In the training phase the range from the logarithm of the minimum value to the logarithm of the maximum value of code numbers is examined. This range is divided to G IH equal bins. Next, the frequency count of words belonging to each bin is calculated. The bins' counts are normalized with the number of all words. Then the best Weibull distribution corresponding to the data is determined.</p><p>Weibull distribution is compared with empirical distribution by examining both distributions' cumulative distributions. Weibull's Cumulative Distribution Function is calculated by: P RQ TS ¤ VU XW `Y ba ca da ¨$ fe g ih qp sr 1t a su fv Au iw yx 0 d q h q (3)</p><p>There are two parameters that can be varied in Weibull's CDF formula: % and . A set of Weibull distributions are calculated with all the possible combinations of % 's and 's using a selected precision. The possible values for the coefficients are restricted between realistic minimum and maximum values.</p><p>The empirical cumulative distribution and Weibull's cumulative distribution are compared in the least square sum sense.</p><p>The best Weibull distribution is divided into G IH equal probable bins. Every word belongs now to a bin that can be found using the word code number and the best fitting Weibull distribution. Using this type of quantization the word can now be presented as the bin number, i.e. the number of the bin that it belongs to. Due to the selected coding method the resolution will be the best where the words are most typical to text (usually words with 2-5 characters). Rare words (usually long words) are not so accurately separated from each other.</p><p>Similarly at the sentence level every sentence has to be converted to numbers. Every word in a sentence is now replaced with a bin number. The bin number is generated with the method described earlier. Example of encoding a sentence :</p><formula xml:id="formula_1" coords="3,62.16,520.79,125.15,10.76">I have a cat .</formula><p>@ 3 @ $ 3 3</p><p>where 3 = bin number of the word ) . Note, that in the encoding also the punctuation marks are encoded. When the sentence is encoded, it can be considered as a sampled signal. To illustrate this way of thinking, an example sentence and it's encoded form are presented in figure <ref type="figure" coords="3,427.08,592.31,4.48,10.76" target="#fig_0">1</ref>. We use Slant transform matrix for transforming the sentence signals. Slant transform coding is commonly used in image processing. In detail Slant transform is explained e.g. in publications <ref type="bibr" coords="3,482.52,621.11,12.95,10.76" target="#b0">[1,</ref><ref type="bibr" coords="3,498.24,621.11,8.97,10.76" target="#b3">4,</ref><ref type="bibr" coords="3,510.00,621.11,8.57,10.76" target="#b7">8]</ref>. The size of the Slant matrix is, based on experiments, selected to be 32*32. If the sentences length is over 32 words, the rest of the sentence after 32 words is not considered. If the sentence is shorter than 32 words, then the missing words get zero as value.</p><p>First row of Slant matrix is multiplied with sentence vector and the result is stored to . Second row </p><p>creates the result vector of the sentence (l l $ nm cm dm e . Now there are 32 real numbers in a vector that describe the sentence. The numbers can be negative or positive. This method have to go through all the sentence vectors of the text and the result is summed up with previous result. After every sentence of the text is transformed with Slant transform and then normalized, then we have 32-dimensional feature vector that describes the text.</p><p>When examining the test set documents on the word level, we create histograms of the relevant and test set documents' word code numbers. The filtered text from a single document is encoded word by word. Each word number is quantized using word quantization created with all the words of the training set. The quantization value is determined, an accumulator corresponding to the value is increased, and thus a word histogram o pH is created. The histogram o pH consisting of G IH bins is finally normalized by the length of the histogram vector. On the sentence level the relevant and test documents are encoded to straight to feature vectors as described earlier.</p><p>With the histograms and feature vectors derived from all the relevant and test documents in the database it is possible to compare and analyze the test documents' text on the word and sentence levels against the relevant texts. The histogram and vector creation and comparison processes are illustrated in Figure <ref type="figure" coords="4,84.60,572.87,4.48,10.76" target="#fig_1">2</ref>. Note, that it is not necessary to have any prior knowledge of the actual text documents to use this methodology. The training set words define the distribution formula that is used with the quantization of words. This information is transferred to the sentence level, where the relations and order of the words are taken into account. No linguistic methods are used in the process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Runs with Reuters database</head><p>For TREC 2001 we did two runs, one on the word level (VisaWordT10) and one on the sentence level (VisaSentT10). This is our first time in the TREC conference, so we were left with very little time for doing the actual runs. This led to simplifying the training and testing processes.</p><p>In the runs for TREC, 2080 was selected for bin number G qH . The amount of bins was selected on the grounds of earlier experiments with English text databases. On the word level run, every test word histogram is compared with randomly chosen 969 relevant document histograms. The number of relevant documents is reduced from 15261 to 969 because it speeds up the test comparison time. Otherwise, the computing time would have risen unreasonable high. To all 84 topics 13 relevant documents have been chosen by random. If a topic has less than 13 relevant documents the number of chosen relevant documents is the amount that exist in that topic. Euclidean distance metric is used in comparing word histograms. The topic of the test document becomes the topic of the relevant document which have the smallest Euclidean distance with the test document. Only the best match relevant document is considered.</p><p>On the sentence level run, feature vector of every sentence is compared with all 15261 feature vectors of relevant documents. Since the sentence feature vector is only 32-dimensional, the comparisons are very fast. Comparison was done using Euclidean distance between the sentence vectors. Distances to every topic are calculated for each test document. On the sentence level, all the distances between the test document and the relevant documents are taken into consideration.</p><p>On both runs, top thousand test documents are chosen from every topic as the final result. The actual value of the distances are not take into consideration, there are no thresholds for belonging to a topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Execution times</head><p>The applied methodology is very fast even with a database as large as the Reuters database. In table <ref type="table" coords="5,539.16,367.19,5.98,10.76" target="#tab_0">1</ref> we present the execution times we calculated for the two runs. Making histograms (word level) execution time include finding the best Weibull distribution and creating the word histograms for test documents. Making feature vectors (sentence level) execution time consists of the encoding of the sentences and creation of the 32-dimensional feature vectors. The comparing execution times are the times that it took to compare the test histograms and vectors with the relevant documents' histograms and vectors. The There were some general difficulties when using the methodology on the Reuters database. The selection of documents for the given training set turned out to be disadvantageous. Firstly, it seemed that the set was too unevenly distributed in topics for our methodology. When some topics have under ten relevant documents and some hundreds, statistical methods are in trouble. There is not enough information in just few short relevant documents for this type of methods to be successful. Uneven division in topics also lead to give more weight to topics that have more relevant documents. Secondly, because the training set was from few days period of a single month, the vocabulary in the relevant documents seemed not to vary enough. The type of methodology we used requires a good set of representative word and sentence samples from the whole database. The training set vocabulary was restricted in the sense of yearly cycle, to one month in autumn of 1996. This type of difficulties are, on the other hand, very common in real life tasks.</p><p>More precise problematic issues using the methodology include the selection of quantization method and distance metrics. Using the word quantization method the way presented here has also some disadvantages. Some accuracy is lost when the word codes are quantized with the Weibull distribution. Words that are quite different may get the same value in the quantization, since strict division of the distribution is used to quantize the codes. Perhaps this could be prevented by using a quantization method that would use the code number values to create a more natural classification. Such a method would be very simple and would perhaps create a more precise and truthful quantization of words. Selection of distance metrics is perhaps even more problematic area. Euclidean distance, which was used here, seems not to be taking into account the shape of the histograms and feature vectors. Euclidean distance compares just the values that are in the same place in two vectors. A distance metric that would allow more variation in the position of values in the histograms would perhaps be more suitable. This kind of metrics are for example Levenshtein metrics or some metrics utilizing the angle between vectors.</p><p>Our methodology seemed to work well only in few topics. However, the methodology was competitive with other competitors on some topics. The runs were designed so that only a very basic form of the methodology was used. The methods used are very fast and it seems that the speed in these runs was gained at the cost of accuracy. It should be noted, that the methodology uses no external aids in the process. Dictionaries, stemming algorithms, transformation to base form, or linguistic information were not used. The methodology does not depend on the language. The comparisons can be performed as long as the training data and the test data are written in the same language.</p><p>Future improvements in the methodology include finding a way to balance the effects of different amounts of relevant documents in topics. One of our future aims is to more efficiently take into account the advantage from the given relevancy information. This would hopefully help to create a specific view of the topic, i.e. the knowledge of the words and sentences that separate the topics. Also the size of the word histograms and the sentence feature vectors should be more properly optimized for the amount and type of the text in the database.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,175.20,300.77,244.80,9.22"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The transform from a sentence to a signal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,62.04,256.25,471.14,9.22;4,62.04,270.77,38.07,9.22"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Process of comparing and analyzing documents based on extracted histograms and feature vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,50.16,474.05,495.00,138.15"><head>Table 1 . Execution times rounded up to the nearest hour.</head><label>1</label><figDesc></figDesc><table coords="5,50.16,500.63,495.00,111.56"><row><cell cols="2">Making histograms/</cell><cell></cell><cell></cell></row><row><cell cols="2">feature vectors</cell><cell cols="2">Comparing Altogether</cell></row><row><cell>Word level</cell><cell>3 h</cell><cell>30 h</cell><cell>33 h</cell></row><row><cell>Sentence level</cell><cell>4 h</cell><cell>26 h</cell><cell>30 h</cell></row><row><cell cols="4">computer used in the runs was a PC with a Intel R r 550 MHz Pentium R r III processor and 128 Mb of</cell></row><row><cell>memory. The operating system was Linux.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0" coords="1,68.04,668.92,477.13,8.97;1,50.16,680.80,142.68,8.97"><p>This research is supported by TEKES, the National Technology Agency of Finland (grant number 40943/99). The support is gratefully acknowledged.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,75.84,104.03,443.22,9.82" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="7,180.12,104.03,231.31,9.82">Orthogonal Transforms for Digital Signal Processing</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>Springer Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.84,117.11,469.25,9.82;7,75.84,130.67,303.66,9.82" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="7,128.16,117.11,416.93,9.82;7,75.84,130.67,28.53,9.82">A Classification and subject index for cataloguing and arranging the books and pamphlets of a library</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dewey</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">1876</biblScope>
			<pubPlace>Case, Lockwood &amp; Brainard Co., Amherst, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.84,143.87,469.29,9.82;7,75.84,157.43,469.26,9.82;7,75.84,170.99,24.55,9.82" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,132.84,143.87,319.72,9.82">Catalogs and Cataloguing: A Decimal Classification and Subject Index</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dewey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,478.44,143.87,66.69,9.82;7,75.84,157.43,233.32,9.82">U.S. Bureau of Education Special Report on Public Libraries Part I</title>
		<meeting><address><addrLine>Washington DC, USA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page">1876</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.84,184.07,319.26,9.82" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<title level="m" coord="7,113.40,184.07,185.59,9.82">Fundamentals of Digital Image Processing</title>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.84,197.27,469.24,9.82;7,75.84,210.83,463.86,9.82" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="7,136.08,197.27,409.00,9.82;7,75.84,210.83,79.23,9.82">Automatic indexing: an approach using an index term corpus and combining linguistic and statistical methods</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lahtinen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<pubPlace>Finland</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of General Linguistics, University of Helsinki</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="7,75.84,223.91,469.27,9.82;7,75.84,237.47,146.94,9.82" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<title level="m" coord="7,220.80,223.91,244.99,9.82">Foundations of Statistical Natural Language Processing</title>
		<meeting><address><addrLine>Cambridge, Massachusetts</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.84,250.67,469.22,9.82;7,75.84,264.23,155.35,9.82" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="7,223.08,250.67,180.07,9.82">A conceptual framework for text filtering</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Marchionini</surname></persName>
		</author>
		<idno>CS-TR3643</idno>
		<imprint>
			<date type="published" when="1996-05">May 1996</date>
		</imprint>
		<respStmt>
			<orgName>University of Maryland</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="7,75.84,277.31,468.92,9.82;7,75.84,290.87,127.26,9.82" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,263.04,277.31,127.27,9.82">Slant transform image coding</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">K</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">R</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,398.64,277.31,141.04,9.82">IEEE Trans. on Communications</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1075" to="1093" />
			<date type="published" when="1974-08">August 1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.84,304.07,271.74,9.82" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="7,123.72,304.07,113.47,9.82">Automatic Text Processing</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.84,317.15,469.33,9.82;7,75.84,330.71,128.46,9.82" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,233.64,317.15,198.45,9.82">A vector space model for automatic indexing</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,442.68,317.15,102.49,9.82;7,75.84,330.71,19.15,9.82">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="613" to="620" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.84,343.91,469.28,9.82;7,75.84,357.47,468.99,9.82;7,75.84,371.03,469.00,9.82;7,75.84,384.59,451.74,9.82" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,400.20,343.91,144.93,9.82;7,75.84,357.47,98.20,9.82">Data Mining of Text as a Tool in Authorship Attribution</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Visa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Toivonen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Autio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mäkinen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Vanharanta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Back</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,296.88,357.47,113.09,9.82;7,439.56,357.47,105.27,9.82;7,75.84,371.03,469.00,9.82;7,75.84,384.59,197.22,9.82">SPIE 15th Annual International Symposium on Aerospace/Defense Sensing, Simulation and Controls. Data Mining and Knowledge Discovery: Theory, Tools, and Technology III</title>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">V</forename><surname>Dasarathy</surname></persName>
		</editor>
		<meeting><address><addrLine>Orlando, Florida, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-04-16">2001. April 16-20 2001</date>
			<biblScope unit="volume">4384</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of AeroSense</note>
</biblStruct>

<biblStruct coords="7,75.84,397.67,469.31,9.82;7,75.84,411.23,469.14,9.82;7,75.84,424.79,469.32,9.82;7,75.84,438.35,91.73,9.82" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,299.16,397.67,245.99,9.82;7,75.84,411.23,84.26,9.82">Improvements on a Knowledge Discovery Methodology for Text Documents</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Visa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Toivonen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Vanharanta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,180.84,411.23,364.14,9.82;7,75.84,424.79,119.04,9.82">Proceedings of SSGRR 2000 -International Conference on Advances in Infrastructure for Electronic Business</title>
		<meeting>SSGRR 2000 -International Conference on Advances in Infrastructure for Electronic Business<address><addrLine>L&apos;Aquila, Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-08-06">July 31-August 6 2000</date>
		</imprint>
		<respStmt>
			<orgName>CD-ROM</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.84,451.55,469.24,9.82;7,75.84,464.99,469.30,9.82;7,75.84,478.55,428.21,9.82" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,307.92,451.55,237.16,9.82;7,75.84,464.99,50.17,9.82">Prototype Matching -Finding Meaning in the Books of the Bible</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Visa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Toivonen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Vanharanta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Back</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,271.20,464.99,273.94,9.82;7,75.84,478.55,188.62,9.82">Proceedings of the Thirty-Fourth Annual Hawaii International Conference on System Sciences (HICSS-34)</title>
		<editor>
			<persName><forename type="first">J</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ralph</forename><forename type="middle">H</forename><surname>Sprague</surname></persName>
		</editor>
		<meeting>the Thirty-Fourth Annual Hawaii International Conference on System Sciences (HICSS-34)<address><addrLine>Maui, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-06">January 3-6 2001</date>
		</imprint>
		<respStmt>
			<orgName>CD-ROM</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
