<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,132.24,116.35,336.36,15.77">TREC-10 Web Track Experiments at MSRA</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,177.96,146.95,49.90,8.34"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<email>jfgao@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,238.91,146.95,56.93,8.34"><forename type="first">Guihong</forename><surname>Cao #</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<country>China #</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Computer Science. &amp;Tech. Dept</orgName>
								<orgName type="laboratory">State Key Lab. of Intelligent Tech. &amp; Sys</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,300.68,146.95,58.28,8.34"><forename type="first">Hongzhao</forename><surname>He #</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<country>China #</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Computer Science. &amp;Tech. Dept</orgName>
								<orgName type="laboratory">State Key Lab. of Intelligent Tech. &amp; Sys</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,363.96,146.95,51.52,8.34"><forename type="first">Min</forename><surname>Zhang ##</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<country>China #</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Computer Science. &amp;Tech. Dept</orgName>
								<orgName type="laboratory">State Key Lab. of Intelligent Tech. &amp; Sys</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,190.20,158.59,50.10,8.34"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
							<email>nie@iro.umontreal.ca</email>
							<affiliation key="aff1">
								<orgName type="department">Département d&apos;informatique et de recherche opérationnelle</orgName>
								<orgName type="institution">Université de Montréal</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<country>China #</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Computer Science. &amp;Tech. Dept</orgName>
								<orgName type="laboratory">State Key Lab. of Intelligent Tech. &amp; Sys</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,254.75,158.59,61.79,8.34"><forename type="first">Stephen</forename><surname>Walker</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,327.35,158.59,76.17,8.34"><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,132.24,116.35,336.36,15.77">TREC-10 Web Track Experiments at MSRA</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">13A7912B69AA8CB1BE3D3F67554C7345</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In TREC-10, Microsoft Research Asia (MSRA) participated in the Web track (ad hoc retrieval task and homepage finding task). The latest version of the Okapi system (Windows 2000 version) was used. We focused on the developing of content-based retrieval and linkbased retrieval, and investigated the suitable combination of the two.</p><p>For content-based retrieval, we examined the problems of weighting scheme, re-weighting and pseudo-relevance feedback (PRF). Then we developed a method called collection refinement (CE) for QE.</p><p>We investigated the use of two kinds of link information, link anchor and link structure. We used anchor descriptions instead of content text to build index. Furthermore, different search strategies, such as spreading activation and PageRank, have been tested.</p><p>Experimental results show: (1) Okapi system is robust and effective for web retrieval. (2) In ad hoc task, content-based retrieval achieved much better performance, and the impact of anchor text can be neglected; while for homepage finding task, both anchor text and content text provide useful information contributing more on precision and recall respectively. (3) Although query expansion does not show any improvement in our web retrieval experiments, we believe that there are still potential for CE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Microsoft Research Asia (MSRA) participated in the Web track (ad hoc retrieval task and page finding task) at TREC-10. We used, for the first time, the new version of the Okapi system (which is running on <ref type="bibr" coords="1,87.96,534.02,63.34,8.72">Windows-2000)</ref> developed at Microsoft Research Cambridge. We focused our researches on: (1) the use of traditional IR techniques (content-based retrieval) for web retrieval, (2) the use of query expansion (QE) for web retrieval, and (3) the use of link information.</p><p>In this paper, we will explore the following issues:</p><p>(1) Testing the Windows version of the Okapi system using 10GB web collection.</p><p>(2) The impact of query expansion on web retrieval. The expansion terms are chosen from the topranked documents retrieved using the initial queries. We used two types of collections for initial retrieval: the 10G web collection and an external collection, i.e. the MS-Encarta collection.</p><p>(3) The relative contribution of content information and link information to web retrieval. We exploit methods of combining both kinds of information to improve the effectiveness of web retrieval.</p><p>(4) The impact of link information on web retrieval. We investigate the use of two kinds of link information: link anchor text and link connection.</p><p>In the remainder of this paper, we will discuss in turn each problem together with our approaches and results of TREC experiments. The results include official runs we submitted and additional runs that we designed to help us explore the issues. Finally, we give our conclusions and present our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The System</head><p>We used the Okapi system Windows-2000 version for our runs. The system was developed in October 2000. A detailed summary of the contributions to TREC1-9 by the Okapi system is presented in <ref type="bibr" coords="2,467.87,148.82,40.48,8.72;2,87.96,159.86,73.70,8.72">(Roberson and Walker, 2000;</ref><ref type="bibr" coords="2,164.98,159.86,113.17,8.72">Roberson and Walker, 1999)</ref>. In this section, we give a very brief introduction to the system.</p><p>The search engine in Okapi is called the Basic Search System (BSS). It is a set-oriented ranked output system designed primarily for probabilistic-type retrieval of textual material using inverted indexes. There is a family of built-in weighting scheme functions known as BM25 and its variants. In addition to weighting and ranking facilities, it has the usual Boolean and quasi-boolean (positional) operations and a number of non-standard set operations. Indexes are of a fairly conventional inverted type. BSS also provides functions for blind feedback.</p><p>All the TREC-10 processing was done at Microsoft Research China. Most of the experiments were run on two DELL severs. Both machines have four 500MHz Pentinum processors with 2GB RAM, and were running on Windows-2000. The network was 100Mbps Ethernet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data Processing</head><p>The collection we used in Web track is a set of web pages downloaded from the World Wide Web. The size of the original collection is more than 10GB. It is a good challenge for the new version of the Okapi system. Four query sets were used in our experiments:</p><p>(1) TREC-9 ad hoc query set of 50 queries (denoted by T9),</p><p>(2) TREC-10 ad hoc query set of 50 queries (denoted by T10),</p><p>(3) TREC-10 page finding query set of 145 queries (denoted by P10), and</p><p>(4) A page finding training set, which includes 100 queries (the query set is denoted by P9) and the relevance judgment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pre-processing</head><p>Our data pre-processing includes data cleaning and information extraction.</p><p>We first removed junk from the collection. The junk includes mismatched tags, and files which contain non-text material (i.e. compressed data, etc). All lines starting with "Sever:" and "Content-type" etc. were also removed. The resulting collection is of size 6GB.</p><p>We then used an HTML parser developed at Microsoft to extract logical fields, including Title &lt;T&gt;, Subtitle &lt;H1&gt;, &lt;H2&gt; and &lt;H3&gt;, and Passage delimited by tags &lt;P&gt; and &lt;/P&gt;. We also, from the collection, established two tables. One table contains the link connection information; each entry of the table is a pagepair connected by a link. The other contains link anchor text information; each entry includes an anchor text, the page containing the anchor text, and the page pointed by the anchor text. Title, Subtitle and Passage were used for content-based retrieval while the link connection and link anchor text were used for link-based retrieval. We will describe both retrieval methods in detail later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Indexing/Query processing</head><p>For query processing, we first performed stemming using the Okapi stemmer. Stop words were then removed. We used a stop word list of 222 words <ref type="bibr" coords="2,289.56,672.02,117.05,8.72">(Roberson and Walker, 1999)</ref>. For four query sets, we used title-only queries in our experiments.</p><p>For each web page, before indexing, all words were stemmed, and stop words were removed. The term weight is BM2500. It is a variant of BM25 and has more parameters that we can tune. BM2500 is of the form:</p><p>) )( (</p><formula xml:id="formula_0" coords="3,199.56,112.21,299.27,32.32">) 1 ( ) 1 ( 3 3 1 ) 1 ( qtf k tf K qtf k tf k w Q T + + + + ∑ ∈ (1)</formula><p>where Q is a query containing key terms T, tf is the frequency of occurrence of the term within a specific document, qtf is the frequency of the term within the topic from which Q was derived, and w (1) is the Robertson/Spark Jones weight of T in Q. It is calculated by Equation (2):</p><formula xml:id="formula_1" coords="3,214.44,191.22,280.53,26.19">) 5 . 0 /( ) 5 . 0 ( ) 5 . 0 /( ) 5 . 0 ( log + + - - + - + - + r R n N r n r R r (<label>2</label></formula><formula xml:id="formula_2" coords="3,494.97,192.92,3.83,8.64">)</formula><p>where N is the number of documents in the collection, n is the number of documents containing the term, R is the number of documents relevant to a specific topic, and r is the number of relevant documents containing the term. In Equation (1), K is calculated by Equation (3):</p><formula xml:id="formula_3" coords="3,245.76,266.84,253.04,9.37">k 1 ((1-b)+b×dl/avdl) (3)</formula><p>where dl and avdl denote the document length and the average document length measured in some suitable unit, such as word or a sequence of words.</p><p>Parameters k 1 , k 3 , b, and avdl are tuned by experiments to optimize the performance. In our experiments, we set k 1 =1.2, k 3 =1000, b=0.75, and avd=61200.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Basic Content-based Retrieval</head><p>For basic content-based retrieval, only initial retrievals (i.e. without QE) were performed. Only those words in fields of Title/Subtitle/Passage were indexed. The initial retrieval results are summarized in Table <ref type="table" coords="3,486.74,386.60,4.80,8.64" target="#tab_0">1</ref> and<ref type="table" coords="3,87.96,397.76,3.66,8.64" target="#tab_1">2</ref>. We can see that the ad hoc retrieval results for TREC-9 query set are very promising. It is favorably comparable to the best effectiveness achieved in the previous web track experiments. This indicates the robustness and effectiveness of our new version of the Okapi system. The results in Table <ref type="table" coords="3,443.29,420.20,4.80,8.64" target="#tab_0">1</ref> and 2 will also serve as the baseline in all experiments described below. The evaluation metric of ad hoc task is noninterpolated average precision. The evaluation metrics of page finding task includes average reciprocal rank, top-10 precision, and not-found rate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Query Expansion</head><p>The average length of title-only queries is less than 3 words (non stop word). It seems that query expansion is needed to deal with word mismatching problem for web retrieval. We performed query expansion experiments on ad hoc retrieval. The procedure works as follows:</p><p>(1) For each query, retrieve 10 top ranked documents by an initial retrieval;</p><p>(2) Choose 10 expansion terms from the top ranked documents. First, stop words were discarded. Then expansion terms were ranked in decreasing order of a term selection value (TSV) of the form</p><formula xml:id="formula_4" coords="4,269.52,135.18,233.72,13.13">R r w TSV / * ) 1 ( = (4)</formula><p>where w (1) , R, and r are same elements described in Equation ( <ref type="formula" coords="4,373.49,155.84,3.81,8.64">1</ref>) and ( <ref type="formula" coords="4,404.75,155.84,3.44,8.64" target="#formula_1">2</ref>). The top-10 terms were added to the initial query.</p><p>As shown in Table <ref type="table" coords="4,163.89,184.04,3.66,8.64" target="#tab_2">3</ref>, the conventional query expansion (i.e. pseudo-relevance feedback (PFB)) result is not good. We think that there might be two reasons. First, the topics of web pages are diverse. Although the expansion terms were chosen from top ranked documents, the ranking of these terms was based on the statistics over the whole collection as indicated by Equation (4). Second, the quality of documents in the web collection is highly mixed. We adopted two methods to solve the abovementioned two problems.. First, we introduced the local context analysis (LCA) technique as proposed in <ref type="bibr" coords="4,405.16,262.88,82.54,8.64" target="#b9">(Xu and Croft, 1996)</ref>. We used statistics of documents and terms from local collection (i.e. top-10 ranked document collection retrieved by an initial retrieval) to estimate the TSV for each expansion terms. That is, we set R=10, and r is the number of relevant documents containing the term in the local collection (r&lt;10). As shown in Table <ref type="table" coords="4,501.26,296.48,3.60,8.64" target="#tab_2">3</ref>, although the result is a little better than PRF, it is still worse than the initial retrieval.</p><p>Second, we introduced the idea of collection enhancement (CE), which was successfully applied for TREC cross language information retrieval experiments <ref type="bibr" coords="4,286.01,335.84,77.07,8.64">(Kwok et al., 2000)</ref>. The basic idea is: if we can refine web queries by QE using documents from an external high-quality and well-organized collection, we may able to improve the web retrieval. We used MS-Encarta collection as the external collection. That is, in our experiments, the initial retrieval was performed using MS-Encarta collection. The expansion terms were chosen from the top-1 Encarta document. Notice that we did not use top-10 documents because the MS-Encarta collection is relatively small (i.e. less than 200MB). More importantly, MS-Encarta is a wellwritten encyclopedia with each document discussing one specific topic. So terms from multiple documents are likely of different topics and not relevant. Documents in MS-Encarta are categorized by a set of predefine keywords and are well-organized under a domain hierarchy structure. We think that such information will be helpful for navigating the web collection, but we have not found an effective way to make use of them.</p><p>The preliminary result shown in Table <ref type="table" coords="4,253.92,464.84,4.80,8.64" target="#tab_2">3</ref> is not encouraging. We found that it is largely due to the difference between the Encarta collection and the web data. But we do believe it has potential if we can make good use of rich information imbedded in MS-Encarta collection (i.e. pre-defined keywords, domain hierarchy, etc.) or figure out an effective way to fuse the web collection with MS-Encarta collection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Link-based Retrieval and Content-based Retrieval</head><p>Recently, the research of web retrieval has focused on link-based ranking methods. However, none had achieved better results than content-based methods in TREC experiments. We investigated the use of two kinds of link information: link anchor and link connection. Our focus was on finding the effective ways for combining link-based retrieval with content-based retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Using anchor text</head><p>We assumed that the anchor text of a link describes its target web page <ref type="bibr" coords="4,367.59,703.64,85.19,8.64" target="#b2">(Craswell et al., 2001)</ref>. We then, for each web page, built an anchor description document containing all the anchor texts of a page's incoming links.</p><p>We observed that plenty of anchor texts are names of home pages (i.e. URL, or URL-like terms), which are reasonable. Therefore intuitively, they could be very effective for page finding task, in which most queries are also a bunch of URLs or URL-like terms. Our results on TREC-10 page finding tasks confirmed the intuition. In Table <ref type="table" coords="5,169.13,141.80,3.66,8.64" target="#tab_3">4</ref>, row 1 and row 2 show that anchor-text-based retrieval achieved much better performance than content-based retrieval, i.e. more than 96% improvements on average reciprocal rank and 48% improvements on top-10 precision. We then performed experiments on ad hoc retrieval using anchor description only for indexing. The results are much worse than content-based retrieval as shown in Table <ref type="table" coords="5,356.67,314.72,4.80,8.64" target="#tab_4">5</ref> (row 1-3). This is due to the data sparseness problem. As indicated in Figure <ref type="figure" coords="5,268.26,325.76,3.60,8.64" target="#fig_0">1</ref>, statistics showed that about 28% web pages in the web collection have no anchor description at all. Totally 75% web pages have anchor description documents with less than 10 words. Therefore, the information (in terms of keywords) that anchor text provided for ad hoc retrieval is very limited. It is most unlikely that the title-only queries have chances to match words contained in such a short description. So even with query expansion (e.g. chose 30 terms from top-5 ranked documents), anchor-text-based retrieval was still much worse than content based retrieval although it was much better than the result without query expansion as shown in row 1-3 of Table <ref type="table" coords="5,409.41,392.96,3.66,8.64" target="#tab_4">5</ref>.</p><p>Avg. P using T9 Avg. P. using T10  In what follows, we examine three different ways to make use of anchor text and content text.</p><p>First, we simply combined the content text and anchor text for indexing (denoted by Comb-1 in Table <ref type="table" coords="6,487.20,142.40,4.80,8.64" target="#tab_3">4</ref> and<ref type="table" coords="6,87.96,153.56,28.12,8.64" target="#tab_4">Table 5</ref>).</p><p>Second, we merged the two ranking lists obtained by content-based retrieval and anchor-text-based retrieval respectively. The new score, s, of a retrieved page is estimated by Equation ( <ref type="formula" coords="6,422.10,181.76,3.47,8.64">5</ref>).</p><formula xml:id="formula_5" coords="6,245.52,195.85,257.72,13.35">s = λ * sc + (1-λ)*sa (5)</formula><p>where sc and sa are, respectively, the scores of content-based retrieval and anchor-text-based retrieval, and</p><formula xml:id="formula_6" coords="6,87.96,226.45,49.63,13.35">λ (0 ≤ λ ≤ 1)</formula><p>is the interpolation weight tuned on a test set.</p><p>The last method we used is to re-rank the results of content-based retrieval according to the results of anchor-text-based retrieval (denoted by Comb-3 in Table <ref type="table" coords="6,317.40,258.44,4.80,8.64" target="#tab_3">4</ref> and<ref type="table" coords="6,342.61,258.44,3.52,8.64" target="#tab_4">5</ref>). For each retrieved page in the ranking list of content-based retrieval, if it is also included in the ranking list of anchor-text-based retrieval, we set a new score by Equation ( <ref type="formula" coords="6,182.84,281.60,3.41,8.64">6</ref>), where λ ≥ 1.</p><formula xml:id="formula_7" coords="6,267.96,295.57,235.28,13.35">s = λ * sc (6)</formula><p>The page finding results are summarized in Table <ref type="table" coords="6,284.02,319.28,3.66,8.64" target="#tab_4">5</ref>. The ad hoc retrieval results are summarized in Table <ref type="table" coords="6,501.29,319.28,3.66,8.64" target="#tab_5">6</ref>.</p><p>Let us discuss the frustrating ad hoc retrieval results first.</p><p>As we expected, since the addition indexing words provided by anchor text are very limited, the impact of combination is neglectable, as shown in row 4 of Table <ref type="table" coords="6,311.64,358.64,3.65,8.64" target="#tab_4">5</ref>. Similarly, in the second method, we found that the best result is obtained when λ=1. This indicates again the neglectable impact of anchor text information on ad hoc retrieval. For Comb-3, we still found that the best results are obtained when λ approached 1.</p><p>The reason that anchor text is not helpful to ad hoc retrieval is largely due to the sparseness problem we discussed above. The following Figure <ref type="figure" coords="6,243.17,410.72,4.80,8.64" target="#fig_2">2</ref> shows the results of query by query analyses on TREC-10 ad hoc retrieval task. Because most of the anchor texts are too short, they are submerged in the content data. At the same time, what most commonly happens is that the query and the anchor text are mismatched for both of them are extremely short. Since no good result can be achieved by using anchor description of the document only, no improvement may be obtained by combining anchor text retrieval result and content text retrieval results.</p><p>Average Precision on TREC-10 ad hoc task (Anchor text only retrieval v.s. source data retreival)  We applied Comb-1 and Comb-3. Unlike the ad hoc retrieval, experimental results on page finding task are much more encouraging as shown in Table <ref type="table" coords="6,260.90,716.72,3.66,8.64" target="#tab_3">4</ref>. Row 3 shows that when using Comb-1, although we did not get any improvements on average reciprocal rank and top-10 precision, the not found rate dropped dramatically by more than 48%. As shown in row 4, by using Comb-3, we obtained even better performance. We achieved approximately 15% improvement on average reciprocal rank, and 56% improvement on top-10 precision. The not found rate also dropped substantially by more than 40%.</p><p>We give the similar query by query analyses for TREC-10 homepage finding task, the first 50 queries of which are shown in Figure <ref type="figure" coords="7,197.26,170.12,3.65,8.64" target="#fig_3">3</ref>. That is to say, we evaluate the retrieval result by non-interpolated 11 points average precision metric, which shows the performance in terms of both precision and recall. For the remaining queries, the results are most similarly. On a whole, there are 90 queries that can get better performance by using anchor description than using source data for indexing; only 38 queries are worse than source data retrieval; and for the remaining 17 queries, both anchor description retrieval and source date retrieval get the same results. Since anchor text takes limited but precise information of a homepage, especially the URL feature of the page, it can get better performance. Then it is reasonable to make improvements while combining two different ranked lists of retrieval results. The results indicate that (1) anchor text containing less but URL-like terms which contributed more to the precision of page finding;</p><p>(2) content text with more terms might contribute more to the recall; and (3) when we combined anchor text and content text for indexing, both kinds of information really complemented each other, and achieved a better tradeoff between precision and recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Using link connection</head><p>We assumed that links between web pages indicate useful semantic relationships between related web pages. Especially, we tried spreading activation (SA) approach <ref type="bibr" coords="7,356.01,575.72,103.90,8.64" target="#b3">(Crestani and Lee, 2000;</ref><ref type="bibr" coords="7,465.16,575.72,43.21,8.64;7,87.96,587.00,63.79,8.64" target="#b8">Savoy and Rasolofo, 2000)</ref> for ad hoc task using TREC-9 query set. In SA method, the degree of match between a web page D i and a query Q, as initially computed by the IR system (denoted SIM(D i ,Q)), is propagated to the linked documents through a certain number of cycles using a propagation factor. <ref type="bibr" coords="7,428.50,609.44,79.93,8.64;7,87.96,621.44,25.96,8.64" target="#b8">Savoy and Rasolofo (2000)</ref> used a simplified version with only one cycle and a fixed propagation factor λ for k-best incoming links and k-best outgoing links. Our experiments showed that considering outgoing links negatively affects the retrieval results. Therefore only the top-1 similar incoming link is considered in our methods. In this case, the final retrieval value of a document D i with m incoming linked documents is computed as:</p><formula xml:id="formula_8" coords="7,156.84,674.95,346.40,15.22">} ,... 1 | ) , ( max{ ) , ( ) SAscore( m j Q D SIM Q D SIM D j i i = ⋅ + = λ (7)</formula><p>Unfortunately, we found that the best result could be obtained only when λ approached 0.</p><p>In addition to SA, different search strategies, such as PageRank etc have been tested. However, none was able to improve the retrieval effectiveness. This result confirmed the previous results in TREC using link connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Summary of Official Retrieval Results</head><p>In TREC-10, we submitted 5 official runs for ad hoc task, and 4 runs for page finding task. In both tasks, title-only query sets were used.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions and Future Work</head><p>In this paper, we described our work in the Web track (ad hoc retrieval task and page finding task) evaluated at TREC-10. We used the latest version of the Okapi system (Windows 2000 version), and focused our researches on: (1) the use of traditional IR techniques (content-based retrieval) for web retrieval, (2) the use of query expansion, and (3) the use of link information.</p><p>Several conclusions are suggested by our experiments.</p><p>(1) The new version of the Okapi system was shown to be very robust and effective for web retrieval.</p><p>(2) In ad hoc task, content-based retrieval achieved much better performance than link-based retrieval. This confirmed again the previous Web track results at TREC.</p><p>(3) In ad hoc task, the impact of anchor text could be neglected. This might be due to the problem of the nature of the TREC web collection, such as the sparseness problem mentioned in Section 6. Other groups have reported that similar methods can achieve improvements on other web collection than TREC collection <ref type="bibr" coords="8,251.23,682.76,89.63,8.64" target="#b4">(Dumais and Jin, 2001)</ref>.</p><p>(4) In page finding task, anchor-text-based retrieval achieved much better results than content-based retrieval in spite of much less terms contained in the anchor description. This is perhaps because terms in page finding queries and anchor text are very similar (i.e. URL, or URL-like terms).</p><p>(5) Both anchor text and content text provided useful information for page finding. In particular, anchor text contributed more to the precision of page finding, while content text contributed more to the recall. Both kinds of information complemented each other. The combination thus achieved a better tradeoff between precision and recall.</p><p>(6) Although query expansion did not show any improvement in our web retrieval experiments, we think that there are still potential for CE if we can make good use of other rich information imbedded in the well-organized high-quality external collection (MS-Encarta) or figure out an effective way to combine the web collection with the external collection.</p><p>Our future work includes</p><p>(1) Study the nature of the web collection, and exploit the use of link information on a more 'complete' web collection.</p><p>(2) Enrich the anchor description by using context information of the anchor. The context information can be a sentence or a passage that contains the anchor text. The context information may enhance the anchor description from two aspects: (1) providing clues to evaluate the relevance between the anchor text and its target web page; (2) providing richer description of the target web page.</p><p>(3) Exploit the use of the external collection for QE including the use of information of domain hierarchy, pre-defined keywords, and topics etc, and the effective combination of external collection with web collection, etc.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,173.76,721.52,257.31,8.64"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Anchor text description length vs. number of documents</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,129.48,654.32,346.10,8.64"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Average precision on TREC-10 ad hoc task (using anchor text v.s. source data)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,102.60,447.08,390.96,8.64"><head>AverageFigure 3 :</head><label>3</label><figDesc>Figure 3: Average precision on TREC-10 homepage finding task (using anchor text v.s. source data)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,142.20,471.20,302.93,130.80"><head>Table 1 :</head><label>1</label><figDesc>Baseline results of ad hoc task</figDesc><table coords="3,142.20,471.20,302.93,130.80"><row><cell></cell><cell>Query set</cell><cell>Avg. P.</cell><cell></cell></row><row><cell></cell><cell>T9</cell><cell>22.08%</cell><cell></cell></row><row><cell></cell><cell>T10</cell><cell>19.42%</cell><cell></cell></row><row><cell>Query set</cell><cell cols="2">Average reciprocal rank Top-10 precision</cell><cell>Not-found rate</cell></row><row><cell>P9</cell><cell>19.68%</cell><cell>34.00%</cell><cell>25.00%</cell></row><row><cell>P10</cell><cell>22.46%</cell><cell>44.10%</cell><cell>25.52%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,205.80,610.88,176.06,8.64"><head>Table 2 :</head><label>2</label><figDesc>Baseline results of page finding task</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,189.48,532.88,215.60,43.68"><head>Table 3 :</head><label>3</label><figDesc>QE results of TREC-9 ad hoc retrieval</figDesc><table coords="4,189.48,532.88,215.60,26.16"><row><cell>Initial retrieval</cell><cell>PRF</cell><cell>LCA</cell><cell>CE</cell></row><row><cell>22.08%</cell><cell>20.89%</cell><cell>21.55%</cell><cell>18.24%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,106.32,187.64,379.04,96.24"><head>Table 4 :</head><label>4</label><figDesc>Page finding results of TREC-10 query set (P10)</figDesc><table coords="5,106.32,187.64,379.04,78.72"><row><cell></cell><cell cols="4">Average reciprocal rank Top 10 precision Not found Method</cell></row><row><cell>1</cell><cell>22.46%</cell><cell>44.10%</cell><cell>25.52%</cell><cell>Content-based retrieval</cell></row><row><cell>2</cell><cell>44.06%</cell><cell>65.50%</cell><cell>25.52%</cell><cell>Anchor-text-based retrieval</cell></row><row><cell>3</cell><cell>42.40%</cell><cell>65.50%</cell><cell>13.10%</cell><cell>Content + anchor text (Comb-1)</cell></row><row><cell>4</cell><cell>50.50%</cell><cell>69.00%</cell><cell>15.20%</cell><cell>Content + anchor text (Comb-3)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,137.88,416.36,307.76,292.49"><head>Table 5 :</head><label>5</label><figDesc>Ad hoc retrieval results using anchor text</figDesc><table coords="5,320.16,416.36,30.24,8.64"><row><cell>Method</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,87.96,194.00,420.49,264.12"><head>Table 6 :</head><label>6</label><figDesc>Table6 and 7show the results as well as methods we used for our submitted runs. Ad hoc official results of submitted runs</figDesc><table coords="8,87.96,239.84,410.48,218.28"><row><cell></cell><cell>Run #</cell><cell>Avg. P</cell><cell>Method</cell><cell></cell></row><row><cell></cell><cell>Msrcn1</cell><cell>19.42%</cell><cell cols="2">Content-based retrieval</cell></row><row><cell></cell><cell>Msrcn2</cell><cell>19.13%</cell><cell cols="3">Content + anchor text (Comb-1)</cell></row><row><cell></cell><cell>Msrcn3</cell><cell>18.64%</cell><cell cols="3">Content + anchor text (Comb-3)</cell></row><row><cell></cell><cell>Msrcn4</cell><cell>17.79%</cell><cell cols="3">Content + anchor text (Comb-3) +LCA</cell></row><row><cell></cell><cell>Msrcn5</cell><cell>18.80%</cell><cell cols="3">Content + anchor text (Comb-1) + PRF</cell></row><row><cell>Run#</cell><cell cols="5">Average reciprocal rank Top 10 precision Not found Method</cell></row><row><cell>Msrcnp1</cell><cell>22.46%</cell><cell></cell><cell>44.10%</cell><cell>25.52%</cell><cell>Content-based retrieval</cell></row><row><cell>Msrcnp2</cell><cell>42.40%</cell><cell></cell><cell>65.50%</cell><cell>13.10%</cell><cell>Content + anchor text (Comb-1)</cell></row><row><cell>Msrcnp3</cell><cell>44.06%</cell><cell></cell><cell>65.50%</cell><cell>25.52%</cell><cell>Anchor-text-based retrieval</cell></row><row><cell>Msrcnp4</cell><cell>50.50%</cell><cell></cell><cell>69.00%</cell><cell>15.20%</cell><cell>Content + anchor text (Comb-3)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,191.52,467.00,212.99,8.64"><head>Table 7 :</head><label>7</label><figDesc>Page finding official results of submitted runs</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,87.96,380.00,420.51,8.64;9,105.48,391.16,105.96,8.64" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,259.84,380.00,248.63,8.64;9,105.48,391.16,48.82,8.64">Improved Algorithms for Topic Distillation in a Hyperlinked Environment</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Bhrat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Henzinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,171.84,391.16,35.20,8.64">SIGIR-98</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.96,408.08,417.24,8.64" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="9,204.75,408.08,251.67,8.64">The Anatomy of a Large-Scale Hypertextual Web Search Engine</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<idno>WWW7</idno>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.96,425.24,422.92,8.64;9,105.48,436.40,50.28,8.64" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="9,307.67,425.24,199.18,8.64">Effective site finding using link anchor information</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<idno>SIGIR-01</idno>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.96,453.44,420.46,8.64;9,105.48,464.72,192.96,8.64" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,235.24,453.44,219.25,8.64">Searching the web by constrained spreading activation</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,462.00,453.44,46.42,8.64;9,105.48,464.72,104.11,8.64">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="585" to="605" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.96,481.64,357.72,8.64" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="9,210.66,481.64,178.77,8.64">Probabilistic combination of content and links</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename></persName>
		</author>
		<idno>SIGIR-01</idno>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.96,498.68,420.42,8.64;9,105.48,509.84,172.08,8.64" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="9,293.30,498.68,215.08,8.64;9,105.48,509.84,119.93,8.64">TREC-9 cross language, web and question-answering track experiments using PIRCS</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">L</forename><surname>Kowk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Grunfeld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chan</surname></persName>
		</author>
		<idno>TREC-9</idno>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.96,526.88,317.88,8.64" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="9,247.60,526.88,105.86,8.64">Okapi/Keenbow at TREC-8</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<idno>TREC-9</idno>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.96,543.92,398.64,8.64" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="9,247.54,543.92,188.28,8.64">Microsoft Cambridge at TREC-9: Filtering track</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<idno>TREC-9</idno>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.96,561.08,420.45,8.64;9,105.48,572.12,140.16,8.64" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Rasolofo</surname></persName>
		</author>
		<idno>TREC-9</idno>
		<title level="m" coord="9,245.80,561.08,262.61,8.64;9,105.48,572.12,89.13,8.64">Report on the TREC-9 Experiment: Link-Based Retrieval and Distributed Collections</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.96,589.16,396.48,8.64" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,200.30,589.16,227.60,8.64">Query expansion using local and global document analysis</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,444.60,589.16,35.41,8.64">SIGIR-96</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
