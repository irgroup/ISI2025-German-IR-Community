<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,147.12,120.27,302.00,13.57">PiQASso: Pisa Question Answering System</title>
				<funder>
					<orgName type="full">Microsoft Research Cambridge</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,161.76,139.41,78.12,10.48"><forename type="first">Giuseppe</forename><surname>Attardi</surname></persName>
							<email>attardi@di.unipi.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Informatica</orgName>
								<orgName type="institution">Università di Pisa</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,247.20,139.41,88.22,10.48"><forename type="first">Antonio</forename><surname>Cisternino</surname></persName>
							<email>cisterni@di.unipi.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Informatica</orgName>
								<orgName type="institution">Università di Pisa</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,342.72,139.41,86.94,10.48"><forename type="first">Francesco</forename><surname>Formica</surname></persName>
							<email>formicaf@di.unipi.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Informatica</orgName>
								<orgName type="institution">Università di Pisa</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,219.12,152.85,50.47,10.48"><forename type="first">Maria</forename><surname>Simi</surname></persName>
							<email>simi@di.unipi.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Informatica</orgName>
								<orgName type="institution">Università di Pisa</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,277.68,152.85,99.48,10.48"><forename type="first">Alessandro</forename><surname>Tommasi</surname></persName>
							<email>tommasi@di.unipi.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Informatica</orgName>
								<orgName type="institution">Università di Pisa</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,147.12,120.27,302.00,13.57">PiQASso: Pisa Question Answering System</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7132FF10076F1943E497A1C3D46E0052</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>PiQASso is a Question Answering system based on a combination of modern IR techniques and a series of semantic filters for selecting paragraphs containing a justifiable answer. Semantic filtering is based on several NLP tools, including a dependency-based parser, a POS tagger, a NE tagger and a lexical database. Semantic analysis of questions is performed in order to extract keywords used in retrieval queries and to detect the expected answer type. Semantic analysis of retrieved paragraphs includes checking the presence of entities of the expected answer type and extracting logical relations between words. A paragraph is considered to justify an answer if similar relations are present in the question. When no answer passes the filters, the process is repeated applying further levels of query expansions in order to increase recall. We discuss results and limitations of the current implementation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Architecture</head><p>The overall architecture of PiQASso is shown in Figure <ref type="figure" coords="1,70.32,498.76,4.89,8.80">1</ref> and consists in two major components: a paragraph indexing and retrieval subsystem and a question answering subsystem.</p><p>The whole document collection is stored in the paragraph search engine, through which single paragraphs are retrieved, likely to contain an answer to a question.</p><p>Processing a question involves the following steps:</p><p>• question analysis • query formulation and paragraph search • answer type filter • relation matching filter • popularity ranking • query expansion.</p><p>Question analysis involves parsing the question, identifying its expected answer type and extracting relevant keywords to perform paragraph retrieval. The initial query built with such keywords is targeted to high precision and to retrieve a small number of sentences to be evaluated as candidate answers through a series of filters. This approach was inspired by the architecture of the system FALCON <ref type="bibr" coords="1,394.32,233.56,10.11,8.80" target="#b4">[5]</ref>. PiQASso analyzes questions and answer paragraphs by means of a natural language dependency parser, Minipar <ref type="bibr" coords="1,414.96,255.88,10.29,8.80" target="#b1">[2]</ref>.</p><p>The semantic type filter checks whether the candidate answers contain entities of the expected answer type and discards those that do not.</p><p>A semantic filter identifies relations in the question, and looks for similar relations within candidate answers. Relations are determined from the dependency tree provided by Minipar. A matching distance between the question and the answer is computed. Sentences whose matching distance is above a certain threshold are discarded. The remaining sentences are given a score that takes into account the frequency of occurrence among all answers. The highest ranking answers are returned.</p><p>If no sentence passes all filters, query expansion is performed to increase paragraph recall. The whole process is iterated using up to five levels of progressively wider expansions.</p><p>PiQASso is a completely vertical system, made by linking several libraries into a single process, which performs textual analysis, keyword search and the semantic filtering. Only document indexing is performed offline by a separate program.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Paragraph Search Engine</head><p>PiQASso document indexing and retrieval subsystem is based on IXE <ref type="bibr" coords="1,361.44,554.44,10.11,8.80" target="#b0">[1]</ref>, a high-performance C++ class library for building full-text search engines. Using the IXE library, we built a paragraph search engine, which stores the full documents in compressed form and retrieves single paragraphs. However, we do not simply index paragraphs instead of documents: this approach is not suitable for question answering since relevant terms may not all appear within a paragraph, but some may be present in nearby sentences.</p><p>Our solution is to index full documents and to add sentence boundary information to the index, i.e. for each document, the offset to the start of each sentence. A sentence splitting tool is applied to each document before indexing.</p><p>The queries used in PiQASso consist of a proximity query involving the most important terms in the question combined in AND with the remaining terms. Such queries select documents containing both relevant context for the question and paragraphs where the required words occur. The paragraph engine ranks each paragraph individually and extracts them from the source document exploiting sentence boundary information.</p><p>The sentence splitter is based on a maximum entropy learning algorithm as described in <ref type="bibr" coords="2,204.72,456.04,10.47,8.80" target="#b8">[9]</ref>.</p><p>Since sentence splitting is quite time consuming, performing it at indexing time improves significantly PiQASso performance. On a 1 MHz Pentium 3, a paragraph search on the whole Tipster collection takes less than 50 msec. Since documents are stored in compressed form, accessing the individual paragraphs requires an additional amount of time for performing text decompression, which depends on the number of results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Text analysis tools</head><p>Our approach to Question Answering relies on Natural Language Processing tools whose quality and accuracy are critical and influence the overall architecture of the system. We performed experiments with various tools and we had to adapt or to extend some of them for achieving our aims.</p><p>We briefly sketch the main NLP tools deployed in PiQASso:</p><p>• the dependency parser Minipar • WNSense: an interface to WordNet • a Named Entity tagger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Minipar</head><p>Sentences are parsed by means of Minipar <ref type="bibr" coords="2,471.12,408.76,10.11,8.80" target="#b1">[2]</ref>, producing a dependency tree which represents the dependency relations between words in the sentence. A dependency relationship is an asymmetric binary relationship between a word called head, and another word called modifier. A word in the sentence may have several modifiers, but each word may modify at most one word. Figure <ref type="figure" coords="2,332.16,486.76,4.89,8.80">2</ref> shows an example of a dependency tree for the sentence John found a solution to the problem. The links in the diagram represent dependency relationships. The direction of a link is from the head to the modifier in the relationship. Labels associated with the links represent types of dependency relations. Table <ref type="table" coords="2,452.64,542.68,4.89,8.80" target="#tab_2">1</ref> lists some of the dependency relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2. Sample dependency tree.</head><p>The root node does not modify any word, and is given an empty node type. Other empty nodes may be present in the tree. For instance, in the parse tree for sentence "It's the early bird that gets the worm", the word "that" is identified as the subject of the "gets the worm" subordinate phrase, and an empty node is inserted to </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paragraph</head><p>Search Engine</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WNSense</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Indexer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MiniPar</head><p>represent the subject of the verb "gets", including a reference to the word "bird". The parser identifies "that" as the subject of "gets" and "the early bird" as an additional one. This is an instance of the problem of coreference resolution: identifying the entity to which a pronoun refers. From the dependency tree built by Minipar we gather that "the early bird" is the subject of "gets the worm", enabling us to answer a question like "who gets the worm?", even if question and answer are stated in slightly different syntactic forms.</p><p>Minipar has some drawbacks: its parsing accuracy is not particularly high and the selection of dependency relations is somewhat arbitrary, so that two similar phrases may have quite different, although correct, parses. We apply several heuristic rules to normalize the dependency tree and to facilitate identifying the most essential and relevant relations for comparing questions and answers. Minipar is also capable of identifying and classifying named entities (NE). Using its own internal dictionary, plus a few rules, it detects word sequences referring to a person, a geographic location or an amount of money.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">WNSense</head><p>We built WNSense (WordNetSense) as a tool for classifying word senses, assigning a semantic type to a word, and evaluating semantic distance between words based on hyperonymy and synonymy relations. WNSense exploits information from WordNet <ref type="bibr" coords="3,263.04,633.16,10.65,8.80" target="#b6">[7]</ref>, for instance to compute the probability of a word sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Sense Probability</head><p>Senses are organized in WordNet in distinct taxonomies (for instance, the word "crane" has senses in the "animal" taxonomy as well as in the "artifact" one).</p><p>During sentence analysis PiQASso often needs to determine whether a word belongs to a certain category: e.g. it is of the expected answer type. This can be estimated by computing the probability for the word sense to belong to a WordNet category (e.g., the probability of the sense of the word "cat" to fall within the "animal" category). WordNet orders word senses by frequency. Given such ordered list of senses {s 0 , … , s n } for a word w, we compute the probability that the sense for the word belongs to category C as follows:</p><formula xml:id="formula_0" coords="3,303.84,217.72,142.34,82.41">P(w, C) = = - n j j s n j n k 0 ) ( γ where γ(s j ) = ¡ ¢ ¤£ ∈ otherwise C s if j 0 1</formula><p>and k is a parameter of the heuristics, roughly the probability that the first WordNet sense is the correct one (currently, k is at 0.7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Word Type</head><p>The type for a word w is computed as: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Word Distance</head><p>A measure of word distance is used for estimating the distance between two sentences, in particular an answer paragraph and a question.</p><p>Word distance for hyperonyms is based on the distance in depths of their senses in the WordNet taxonomy. The depth differences are normalized dividing them by the taxonomy depth, so that a depth difference of 1 in a detailed taxonomy has less influence than a difference of 1 in a coarser one. The depth differences for all pairs of senses of two words are weighted according to the probabilities of both senses and added together.</p><p>Word distance for two synonyms is also computed over all their senses, weighted according to their probability.The distance between two words, denoted by dist(w 1 , w 2 ), is defined as either their synonym distance, if they are synonyms, or else their hyperonym distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">Word Alternatives</head><p>Alternatives for a word are required during query expansion. They are computed considering the union W of all synsets containing the word w. The set of alternatives for w is defined as:</p><formula xml:id="formula_1" coords="4,97.92,175.16,98.05,12.74">{ s ∈ W | dist(w, s) &lt; th }</formula><p>where th is a fixed ceiling, useful to avoids cases where a synonym of w has meanings not typically related to w (e.g. "machine" for "computer").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Named Entity Tagger</head><p>The NE tagger in Minipar can achieve high precision in NE recognition, since it is dictionary-based, but it has limitations (for instance it does not handle unknown names). Therefore we integrated it with an external tagger, based on a maximum entropy probabilistic approach <ref type="bibr" coords="4,112.32,315.16,11.18,8.80" target="#b2">[3]</ref> that uses both a part-of-speech tagger (TreeTagger <ref type="bibr" coords="4,123.84,326.44,15.66,8.80" target="#b9">[10]</ref>) and a gazetteer to determine word features.</p><p>The Named Entity extractor identifies person names, organizations, locations, quantities and dates, and assigns to them one of the semantic types as defined in MUC <ref type="bibr" coords="4,95.04,382.12,10.47,8.80" target="#b3">[4]</ref>.</p><p>To maintain uniformity of treatment, the tags produced by the NE tagger are integrated within the same tree produced by Minipar as additional semantic features for the corresponding words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Question analysis</head><p>Question analysis extracts or identifies the following information from the question:</p><p>• the keywords to be used in the paragraph search;</p><p>• the expected answer type;</p><p>• the location of the answering entity.</p><p>These pieces of information correspond to three successive steps in the process of question answering: keyword based retrieval, paragraph filtering based on the expected answer type, and logical relation matching between questions and answer paragraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Keyword Extraction</head><p>The first step selects words from the question for generating a suitable paragraph query. PiQASso considers the adjectives, adverbs, nouns and verbs in the question, excluding words from a list, determined experimentally, which includes:</p><p>• nouns such as "type", "sort", "kind", "name", frequently occurring in questions but unlikely to occur in answers; • generic verbs like "be", rhetorical ones like "call", auxiliary verbs;</p><p>• adjectives that qualify "how" (as in "how long", "how far", etc.).</p><p>Words to which the parser does not assign the part-ofspeech tag are discarded: including them did not have a clear effect on performance, according to our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Question Classification</head><p>The expected answer type is the semantic type of the entity expected as the answer to the question. The expected answer type is helpful for factual questions, but not for questions that require complex explanations. Irrelevant sentences can be often discarded simply by checking whether they contain an entity of the expected type. The TREC 2001 QA main task requires answers shorter than 50 characters and this entails that only factual questions are asked (no long explanations can be returned as answers).</p><p>PiQASso uses a coarse-grained question taxonomy consisting of five basic types corresponding to the entity tags provided by Minipar (person, organization, time, quantity and location) extended with the 23 WordNet top-level noun categories. The answer type can be a combination of categories, like in the case of "Who killed John Fitzgerald Kennedy?", where the answer type is person or organization. Categories can often be determined directly from a wh-word: "who", "when", "where".</p><p>The category for "how &lt;adjective&gt;" is determined from the adjective category: "many", "much" for quantity, "long", "old" for time, etc.</p><p>The type for a "what &lt;noun&gt;" question is normally the semantic type of the noun, as determined by WNSense. For instance in "what king signed the Magna Charta?", the semantic type for "king" is person. When feasible, the WordNet category for a word is mapped to one of the basic question types. The other cases are mapped to one of the top-level WordNet categories by means of WNSense: "what metal has the highest melting point" has the semantic type "substance".</p><p>For "what &lt;verb&gt;" questions the answer type is the type of the object of the verb. "What is" questions, which expect a definition as an answer ("what is narcolepsy?", "what is molybdenum?") are dealt specially. The answer type is the answer itself (a disease, a metal). However, it is often not possible to just look up the semantic type of the word, because lack of context does not allow identifying the right sense. Therefore, we treat definition questions as type-less questions: entities of any type are accepted as answers (skipping the semantic type filter), provided they appear as subject in an is-a sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Proper and Common Names</head><p>Questions whose expected answer type is person require special treatment. A question like "who is Zsa Zsa Gabor?" expects a definition, and therefore a common noun as an answer, while a question like "who is the king who signed the Magna Charta?" expects a proper noun. Therefore the rule for the case of a person answer type is: if the question contains a proper noun, a common noun is expected and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Relations between Words</head><p>Relations between words in the question are determined from the dependency tree built by Minipar. In a question like "who killed John F. Kennedy", Minipar identifies the verb "killed" as having "Kennedy" as object, and a missing subject (represented by an empty node -a node with no corresponding word). In a possible answer sentence the verb "kill" may appear with exactly "Kennedy" as object. However the same relation could also be stated in a quite different syntactic form, where the dependencies are not so explicit, but require more complex analysis of the tree, as discussed later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">Identifying the Answer Node</head><p>In the dependency tree of the question we must identify the node that represents the object of the question. We call this the answer node, since it can be considered as a placeholder to be matched with the answer object in the answer paragraph. The answer node will have the answer type as determined above. Often this node exists and is empty: it corresponds to the missing subject of a verb, as in "who killed John F. Kennedy". In other cases the node is not empty: for instance in "What instrument did Glenn Miller play?", the answer node corresponds to the word "instrument". The answer type of the question is "artifact" and the semantic type of the word "instrument". In a direct answer like "Glenn Miller played trombone" the answer entity ("trombone") occurs in the place held by the word "instrument" in the question. By experimenting with a number of questions and analyzing Minipar output, we noticed that the answer node often corresponds to the first empty subject node in the question. This is because the main verb in a question is often the first one, and because an empty subject means that the actual subject is missing. An exception to this rule is when the required entity does not participate in the action as a subject, as in question "in what year did the Titanic sink?". In this case, the answer is a complement, and the answer node is still in relation with the verb, but as a complement instead of as a subject. In such cases, there is no such node in the output of the parser, and a new one must be created.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Head</head><p>These simple heuristics are effective for simple questions: dealing with more involved expressions will require extending such heuristics, since determining the answer node is a critical issue in our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Query Formulation and Expansion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Query formulation</head><p>The first iteration in the question answering process performs keyword extraction and query formulation.</p><p>A keyword search is performed for selecting candidate answer sentences from the whole Tipster document collection. Further iterations perform various level of query expansion, each allowing larger recall in the search.</p><p>PiQASso only addresses the problem of finding answers that are fully justified within a single paragraph. This simplifies textual analysis, as only one sentence at a time needs to be analyzed.</p><p>Although sentence boundaries information is stored in the index, search is performed document-wise, in order to achieve better recall. Consider two sentences like: "Neil Armstrong walked on the moon. Armstrong was the first man to walk on Earth's satellite". A question like "Who was the first man to walk on the moon?" would yield the keywords "first", "walk", "moon" and "man". However, while the two sentences contain all those keywords, none of them does by itself. Instead of looking for keywords within a each individual sentence, PiQASso performs a proximity search, looking for terms within a specified word position distance. For the above example, the query could be:</p><formula xml:id="formula_2" coords="5,303.84,597.32,220.35,6.95">proximity 100 ((first) &amp; (walk*) &amp; (moon))</formula><p>which looks for the term "first", for the prefix "walk" and for the word "moon" within a window of one hundred words. Such window would spans across the two sentences above, which would be both returned, individually, as candidate answers.</p><p>Keyword expansion would hardly propose "satellite" as an alternative to "moon", and therefore the second sentence would not be returned if paragraphs had been indexed separately. When evaluating the second sentence within the last filter, the match between "moon" and "satellite" will be given a certain distance as hyperonym, allowing the paragraph to pass the filter.</p><p>We use the following criterion for choosing the size of the proximity window. In principle question and answer lengths are not strictly related: an answer to a short question may appear within a very long sentence, and vice versa, an answer to a complex question could be much shorter than the question itself. However, it seems reasonable to expect that the keywords in an answer paragraph are not too spread apart. The size of the proximity window is twice the number of nodes in the question parse tree (including irrelevant or empty nodes that may account for complicated sentences).</p><p>The heuristics proposed for keyword extraction is adequate for short questions, but returns too many terms for long questions. Thus, we split keywords into two sets: those that must appear within the proximity window and those that must occur anywhere in the document. The generated query consists of a conjunction of those terms that must be found in the document, and of a proximity query.</p><p>Terms are put in either of the two sets depending on the distance of the term from the root of the parse tree. Terms closer to the root, and therefore more central to the question, are required to be appear within the proximity window, while the others are accessory: they are only requested to occur in the document, but may be missing from the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Query Expansion</head><p>The first expansion step tries to cope with morphological variants of words by replacing each keyword with a prefix search, obtained from stemming the original word. Certain prefixes that appear frequently in questions are discarded: for instance "locate", "find", "situate" in questions expecting a location as an answer, "day", "date", "year" in questions expecting a date and so on. We use about a dozen of such exceptions, which correspond to cases in which the type makes these words superfluous.</p><p>Stemming is performed using Linh Huynh implementation of Lovins's stemmer <ref type="bibr" coords="6,215.04,573.64,10.29,8.80" target="#b5">[6]</ref>.</p><p>In the second expansion cycle we broaden the search by adding (in or) the synonyms of the search terms. Synonyms are looked up in WordNet by means of WNSense. Synonyms are stemmed as well.</p><p>In the third and fourth expansion cycles, we increase recall by dropping some search terms. During the third cycle, adverbs are dropped.</p><p>During the last expansion cycle, if the query contains more than three keywords in conjunctive form, verbs are discarded, as well as person's first names when the last name is present. If after such a pruning there are still more than three keywords in and, then we also drop those keywords whose parent (as from the dependence tree) is already within the keywords to be searched. This has the effect, if looking for a "black cat", to perform a search for a "cat", black being a modifier of cat, and therefore depending on it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Type Matching</head><p>The sentences returned by the query are analyzed and checked for the presence of entities of the proper answer type, as determined by question analysis.</p><p>Sentences are parsed and recognized entities are tagged. The tree is then visited, looking for a node tagged with the expected answer type.</p><p>We also check whether an entity which occurs in the sentence is already present in the question. A question like "Who is George Bush's wife?" expects a proper person name as an answer. The sentence "George Bush and his wife visited Italy" contains a proper person name, but does not answer the question. Such sentences occur frequently (the search keywords being "George", "Bush" and "wife"), so it is convenient to discard them as early as possible.</p><p>Sentences not verifying this condition are rejected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Relation Matching</head><p>Sentences that pass the answer type filter are submitted to the relation matching filter, which performs a more semantic analysis, verifying that the answer sentence contains words that have the same type and relation than corresponding words in the question. The filter analyzes Minipar output in order to:</p><p>• determine a set of relations between nodes present both in the question and in the sentence; • look for relations in the answer corresponding to those in the question; • compute the distance of each candidate answer and select the one with the lower distance.</p><p>In order to simplify the process, not all the nodes in the question and in the answer are considered. The same criterion used for selecting words as search keywords is applied also in this case: nouns, verbs, adjectives and adverbs are relevant (including dates and words with unknown tag). During this analysis, the parser tree is flattened into a set of triples (H, r, M): head node, relation, modifier node. This representation is more general and allows us to turn the dependency tree into a graph.</p><p>In fact it is often useful to make certain relations explicit by adding links to the parser tree. For instance in the phrase "Man first walked on the moon in 1969", "1969" depends on "in", which in turns depends on "moon". According to our criterion, "in" is not a relevant node and so it will not be considered. We can however short circuit the node by adding a direct link between "moon" and "1969". More generally, we follow the rule that whenever two relevant nodes are linked through an irrelevant one, a link is added between them. Similarly, since the NE tagger recognizes "1969" as a date, it is convenient to add a link between the main verb ("walk") and the date, since the date modifies the action expressed by the verb.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Extracting Relations</head><p>Questions and answers are analyzed in order to determine whether relations present in a question appear in a candidate answer as well.</p><p>PiQASso exploits the relations produced by Minipar and infers new relations applying the following rules:</p><formula xml:id="formula_3" coords="7,70.32,272.44,222.44,8.80">Direct link if B is a child of A in Minipar output, A is</formula><p>related to B according to their relation in the parse tree. Conjunctions Relations distribute over conjunctive links. For example, in "Jack and John love Mary", the relation between John and Mary is distributed over the conj link between John and Jack, i.e. a "love" relation between Jack and Mary is inferred. Predicates A and B are related if they are both child of a "to be" verb, the first with the role of subject, the second as predicate (which is the relation between them). This is because a question like "who is the Pope?" is often answered by phrases such as "The Pope, John Paul II, ..." in which the answer does not go through a "to be" verb. Possession A and B are related with the relation of genitive if A is the subject of a verb "to have" and B is the object. The rule enables matching "John's car" with "John has a car". Location A and B are in inside relation if there is a subj-in relation between B and A (phrase of the form "A is in B"). This allows matching "Paris is in France" with "Paris, France". Invertible relations Some relations are invertible, so that A and B are in relation if B and A are in relation as either apposition (a particular case of nominal compound) or by the person relation (a relation between the first and second name of a person). Dates Minipar links a modifier (e.g. "in 1986") to the closest noun: a relation between the main verb (describing the action) and the date is inferred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Empty nodes</head><p>Empty nodes represent an implicit element of the sentence. Minipar sometimes can determine the word they refer to: in this case we add a relation between A and B if there is a relation between A and C, and C is an empty node referring to B (and dually for the first node in the relation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Negation A relation between two nodes is discarded if</head><p>both nodes depend from a node with a negative modifier. This accounts for negative phrases like "John is not a policeman" and avoids inferring a relation between "John" and "policeman". This rule has precedence over the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Finding a match</head><p>Suppose the following relations appear in a question: </p><formula xml:id="formula_4" coords="7,303.84,205.72,222.74,19.84">qr 1 = (A,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Matching Distance</head><p>An answer paragraph can be considered as a close answer to a question if it contains nodes and relations corresponding to all nodes and relations in the question.</p><p>For each missing node the distance is increased by an amount that depends on the relevance of the node. To represent this relevance we associate a mismatch distance, mmd(n), to each node n in the question. For instance the mismatch distance is small for the node corresponding to the question type (e.g. the node "instrument" in a previous example), since it may be missing in the answer. Nodes depending on other relevant nodes have half the mismatch distance of their parents: they may express a specification that a correct answer need not contain. The overall matching distance between a question and a candidate answer is computed by summing, for each node n in the question:</p><formula xml:id="formula_5" coords="7,310.80,557.25,214.70,22.80">mmd(n) ⋅ dist(n, m) if n matches node m in the answer mmd(n) otherwise</formula><p>and similarly for each relation in the question.</p><p>The distance is incremented to account for special situations, e.g. when the answer is too specific: for the question "Who was the first man in Space?", "The first American in space was …" is not a proper answer, contrary to a naïve rule that wants a specific answer correct for a general question.</p><p>If the answer sentence does not contain an entity of the expected answer type, the distance is set to infinity, to ensure that the paragraph is rejected.</p><p>The maximum distance from the question allowed by the filter (distance ceiling) may be set to either: a) very high, so that any sentence matching the answer node will pass the filter (recall-oriented); or b) proportional to the number of nodes in the question. PiQASso implements a simple tightening strategy whereby the ceiling is decreased at each expansion iteration, avoiding too much garbage in the early phases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Answer Popularity</head><p>After the TREC 2001 submission we introduced a criterion for selecting answers based on a measure of answer popularity, which proved quite effective.</p><p>Answers are grouped according to the value contained in their answer node. A score is assigned to each group proportional to the average of the matching distances in the group and inversely proportional to the cardinality of the group. Groups are sorted by increasing score value and only the answer with the smallest matching distance in each group is returned.</p><p>The criterion combines a measure of difference to the question and a measure of likelihood based on how often the same answer was offered. Selecting only one answer per group ensures more variety in the answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Results</head><p>PiQASso achieved the scores summarized in Table <ref type="table" coords="8,285.60,381.64,3.74,8.80" target="#tab_6">3</ref>, expressed as MRR (Mean Reciprocal Rank) of up to five answers per question. The official score, computed from the judgments of NIST assessors at TREC 2001, ranks PiQASSo in 15 th overall position. PiQASso achieved the same score for both strict evaluation (answers supported by a document in the collection) and lenient evaluation (answer not supported), since it does not use any external source of information. The unofficial score was computed by our own evaluation of the results on a new run of the system after the addition of the popularity ranking.  A peculiarity of the TREC 2001 questions was the presence of a higher than usual percentage (almost 25%) of definition questions that could have been answered by simple lookup in a dictionary or from other sources (e.g. the Web), as some other systems did. For PiQASso we concentrated in improving the system ability to analyze and extract knowledge from the given document collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Assessment</head><p>In order to assess the effectiveness of the various filters, and how they affect the overall performance we performed some measurements using a subset of 50 questions of the TREC 2001 set. Results are summarized in Table <ref type="table" coords="8,338.64,130.12,3.74,8.80">4</ref>. For half of the questions (49%), no paragraph passed all filters. The great majority of answers are obtained from the results of the first IR query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>%</head><p>Questions for which no answer was found 49</p><p>Questions answered by first query (over all answers)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>92</head><p>Questions for which no paragraph was retrieved 2 Table <ref type="table" coords="8,382.80,244.60,4.15,8.80">4</ref>: Filter effectiveness.</p><p>The benefits of iterating the process after performing query expansion are less than expected. Overcoming this limit requires improving query expansion to produce more word alternatives or morphological variations. This may however complicate the task of the relation matching filter, which also needs to be refined: the paragraphs retrieved by the initial query (without stemming or synonym expansion) are simpler to match with the question and produce most of the answers. When more complex paragraphs are retrieved by the more complex queries, matching is more difficult and rarely an answer is found.</p><p>The current system is not capable, for example, of matching the sentences "John loves Mary" and "John is in love with Mary", since their main verbs "love" and "to be" are different. Either deeper semantic knowledge would be required or a collection of phrase variants, that might be built automatically with the method suggested by Lin <ref type="bibr" coords="8,333.84,462.28,15.20,8.80" target="#b9">[10]</ref>, discovering similarities in paths within the dependency graph of the parser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Conclusions and Future Work</head><p>PiQASso is engineered as a vertical application, which combines several libraries into a single application. With the exception of Minipar and WordNet, all the components in the architecture were built by our team, including the special purpose paragraph indexing and search engine, up to the tools for lexical analysis, question analysis and semantic filtering.</p><p>PiQASso is based on an approach that relies on linguistic analysis and linguistic tools, except for passage retrieval, where it exploits modern and efficient information retrieval techniques. Linguistic tools provide in principle higher flexibility, but often appear brittle, since implementations must restrict choices to reduce the efffects of combinatorial explosions. One way to improve their performance would be by providing them with large amounts of semantic data in a preprocessed form: for instance generating a large number of variants from the phrases in the document collection and matching them with effective indexing techniques and statistical estimates, rather than performing sophisticated matching algorithms.</p><p>PiQASso is heavily dependent on Minipar since it relies on the dependency relations it creates. Such relations are often too tied to the syntactic form of the sentence for our purposes, so we had to add specific processing rules to abstract from such representation and to work around certain of its idiosyncrasies.</p><p>Question analysis could be improved by adopting a finer-grained taxonomy for the expected answer type. Such granularity requires support by the named entity tagger.</p><p>Keyword extraction/expansion would benefit from a better identification of the sense of a word, so that fewer and more accurate alternatives can be used in the query formulation. Current figures show that present keyword expansion is not effective, for it either does not add results, or it adds too many, returning way too many hits for the system to analyze them all. As for about half of the questions our system did not find any answer at all (which gives us outstanding improvement margins), this seems a necessary step.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,407.04,394.26,3.89,10.51;3,394.56,394.26,2.92,10.51;3,382.80,394.26,3.89,10.51;3,349.44,394.26,20.00,10.51;3,333.12,394.26,14.72,10.51;3,398.64,394.26,7.79,10.51;3,387.12,394.26,7.79,10.51;3,375.36,394.26,7.14,10.51;3,351.12,406.96,12.21,6.11;3,342.48,406.96,9.40,6.11;3,303.84,421.48,222.50,8.80;3,303.84,432.52,222.52,8.80;3,303.84,443.80,222.80,8.80;3,303.84,454.84,222.41,8.80;3,303.84,466.12,222.80,8.80;3,303.84,477.40,70.12,8.80"><head></head><label></label><figDesc>. the category C among those in TLC, to which the word belongs with the highest the probability. The TLC categories used by PiQASso are the 23 top-level categories from WordNet corresponding to nouns, from the total of 45 lexical files into which WordNet organizes synsets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,91.44,331.14,184.39,156.54"><head>Description</head><label></label><figDesc></figDesc><table coords="3,91.44,342.72,184.39,144.96"><row><cell>i</cell><cell>main verb</cell></row><row><cell>subj, s</cell><cell>subject of the verb</cell></row><row><cell>obj, objn</cell><cell>object of the verb</cell></row><row><cell>pcomp-n</cell><cell>prepositional complement</cell></row><row><cell>appo</cell><cell>appositive noun</cell></row><row><cell>gen</cell><cell>genitive</cell></row><row><cell>inside</cell><cell>location specifier</cell></row><row><cell>nn</cell><cell>nominal compound</cell></row><row><cell>lex-mod</cell><cell>lexical modifier</cell></row><row><cell>det</cell><cell>determiners</cell></row><row><cell>mod</cell><cell>Modifiers (adjs, advs, preps)</cell></row><row><cell>pred</cell><cell>Predicate</cell></row><row><cell>aux</cell><cell>Auxiliary verb</cell></row><row><cell>neg</cell><cell>negative particle</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,104.88,496.36,153.17,8.80"><head>Table 1 :</head><label>1</label><figDesc>some relations in Minipar output.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,74.88,597.54,213.44,80.90"><head>Table 2 : relations for the sentence "what instrument did Glenn Miller play?"</head><label>2</label><figDesc></figDesc><table coords="5,85.44,597.54,179.00,52.29"><row><cell></cell><cell>Relation</cell><cell>Modifier</cell></row><row><cell>play</cell><cell>obj</cell><cell>instrument</cell></row><row><cell>play</cell><cell>s</cell><cell>Miller</cell></row><row><cell>play</cell><cell>s</cell><cell>Glenn</cell></row><row><cell>Miller</cell><cell>lex-mod</cell><cell>Glenn</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,303.84,216.76,222.74,114.16"><head></head><label></label><figDesc>r 1 , B) and qr 2 = (A, r 3 , C). Suppose the following relations are present in a candidate answer sentence:ar 1 = (1, R 1 , 2), ar 2 = (2, R 2 , 3) and ar 3 = (1, R 3 ,<ref type="bibr" coords="7,499.44,241.48,7.23,8.80" target="#b2">3)</ref>. All matches between triples in the question and in the answer are considered, provided that no node is put in correspondence with two different nodes: if we match qr 1 with ar 1 , we cannot match qr 2 with ar 2 , or node A in the question would have to match both nodes 1 and 2 in the answer.The match with the smallest distance is selected.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,79.92,571.00,203.32,8.80"><head>Table 3 : Scores in the TREC 2001 QA main task.</head><label>3</label><figDesc></figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>Alessandro Tommasi is supported by a PhD fellowship from <rs type="funder">Microsoft Research Cambridge</rs>.</p><p>Cesare Zavattari contributed to various aspects of the implementation, in particular of the sentence splitter.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,87.36,456.52,205.77,8.80;9,87.36,467.80,205.64,8.80;9,87.36,478.84,205.46,8.80;9,87.36,490.12,205.72,8.80;9,87.36,501.16,147.16,8.80" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,201.84,456.52,91.29,8.80;9,87.36,467.80,148.29,8.80">Reflection support by means of template metaprogramming</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Attardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cisternino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,244.32,467.80,48.68,8.80;9,87.36,478.84,205.46,8.80;9,87.36,490.12,201.50,8.80">Proceedings of Third International Conference on Generative and Component-Based Software Engineering</title>
		<meeting>Third International Conference on Generative and Component-Based Software Engineering<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.36,512.44,205.24,8.80;9,87.36,523.72,205.72,8.80;9,87.36,534.76,14.73,8.80;9,120.96,534.76,26.44,8.80;9,166.08,534.76,41.56,8.80;9,226.32,534.76,25.77,8.80;9,270.72,534.76,22.36,8.80;9,87.36,546.04,70.84,8.80" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,119.28,512.44,169.24,8.80">LaTaT: Language and Text Analysis Tools</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://hlt2001.org" />
	</analytic>
	<monogr>
		<title level="m" coord="9,87.36,523.72,201.44,8.80">Proc. Human Language Technology Conference</title>
		<meeting>Human Language Technology Conference<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-03">March 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.36,556.84,205.78,8.80;9,87.36,568.12,205.46,8.80;9,87.36,579.16,202.84,8.80" xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Della Pietra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,286.08,556.84,7.06,8.80;9,87.36,568.12,205.46,8.80;9,87.36,579.16,148.70,8.80">A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.36,590.44,205.58,8.80;9,87.36,601.72,205.46,8.80;9,87.36,612.76,205.48,8.80;9,87.36,624.04,191.08,8.80" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,212.16,590.44,80.78,8.80;9,87.36,601.72,48.69,8.80">Design of the MUC-6 evaluation</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sundheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,215.52,601.72,77.30,8.80;9,87.36,612.76,152.20,8.80">The Sixth Message Understanding Conference (MUC-6)</title>
		<editor>
			<persName><forename type="first">Nist</forename><surname>In</surname></persName>
		</editor>
		<meeting><address><addrLine>Columbia, MD. NIST</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan-Kauffmann Publisher</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.36,635.08,205.29,8.80;9,87.36,646.36,205.29,8.80;9,87.36,657.40,75.64,8.80" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,212.88,635.08,79.77,8.80;9,87.36,646.36,146.29,8.80">FALCON: Boosting Knowledge for Answering Engines</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Moldovan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,243.84,646.36,23.80,8.80">TREC</title>
		<imprint>
			<date type="published" when="2000">2000 Proceedings, 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.36,668.68,205.29,8.80;9,87.36,679.48,42.76,8.80;9,147.60,679.48,46.40,8.80;9,211.44,679.48,49.40,8.80;9,278.40,679.48,14.25,8.80;9,87.36,690.76,172.60,8.80" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,159.84,668.68,132.81,8.80;9,87.36,679.48,38.49,8.80">Development of a Stemming Algorithm</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">B</forename><surname>Lovins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,147.60,679.48,46.40,8.80;9,211.44,679.48,49.40,8.80;9,278.40,679.48,14.25,8.80;9,87.36,690.76,101.90,8.80">Mechanical Translations and Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="22" to="31" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.36,702.04,205.52,8.80;9,87.36,713.08,152.68,8.80" xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,129.12,702.04,163.76,8.80;9,87.36,713.08,107.06,8.80">Five papers on WordNet. Special issue of International Lexicography</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,320.88,107.80,205.61,8.80;9,320.88,119.08,205.46,8.80;9,320.88,130.12,175.48,8.80" xml:id="b7">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Reyner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ratnaparkhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,473.04,107.80,53.45,8.80;9,320.88,119.08,205.46,8.80;9,320.88,130.12,146.63,8.80">A Maximum Entropy Approachg to Identify Sentence Boundaries. Computational Language</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,320.88,141.40,205.76,8.80;9,320.88,152.44,56.25,8.80;9,397.20,152.44,26.68,8.80;9,443.76,152.44,22.36,8.80;9,486.24,152.44,40.40,8.80;9,320.88,163.72,78.38,8.80;9,320.88,174.76,173.08,8.80" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="9,371.28,141.40,155.36,8.80;9,320.88,152.44,56.25,8.80;9,397.20,152.44,22.87,8.80">TreeTagger -a language independent part-of-speech tagger</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Schmid</surname></persName>
		</author>
		<ptr target="http://www.ims.uni-stuttgart.de/Tools/DecisionTreeTagger.html" />
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,320.88,185.80,205.64,8.80;9,320.88,197.08,205.52,8.80;9,320.88,208.12,148.84,8.80" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,409.44,185.80,117.08,8.80;9,320.88,197.08,92.01,8.80">Discovery of Inference Rules for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,486.24,197.08,40.16,8.80;9,320.88,208.12,120.16,8.80">Journal of Natural Language Engineering</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>To appear in the</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
