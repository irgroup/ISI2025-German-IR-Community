<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,235.84,75.62,140.42,14.36;1,144.28,94.10,323.32,14.36">Question Answering : CNLP at the TREC-10 Question Answering Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,95.08,111.30,66.38,9.94"><forename type="first">Jiangping</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Natural Language Processing</orgName>
								<orgName type="department" key="dep2">School of Information Studies</orgName>
								<orgName type="department" key="dep3">Center for Science and Technology Syracuse</orgName>
								<orgName type="institution">Syracuse University</orgName>
								<address>
									<addrLine>4-206</addrLine>
									<postCode>1324-4100</postCode>
									<settlement>www.cnlp.org</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,169.45,111.30,76.54,9.94"><forename type="first">Anne</forename><forename type="middle">R</forename><surname>Diekema</surname></persName>
							<email>diekemar@syr.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Natural Language Processing</orgName>
								<orgName type="department" key="dep2">School of Information Studies</orgName>
								<orgName type="department" key="dep3">Center for Science and Technology Syracuse</orgName>
								<orgName type="institution">Syracuse University</orgName>
								<address>
									<addrLine>4-206</addrLine>
									<postCode>1324-4100</postCode>
									<settlement>www.cnlp.org</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,254.05,111.30,65.48,9.94"><forename type="first">Mary</forename><forename type="middle">D</forename><surname>Taffet</surname></persName>
							<email>mdtaffet@syr.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Natural Language Processing</orgName>
								<orgName type="department" key="dep2">School of Information Studies</orgName>
								<orgName type="department" key="dep3">Center for Science and Technology Syracuse</orgName>
								<orgName type="institution">Syracuse University</orgName>
								<address>
									<addrLine>4-206</addrLine>
									<postCode>1324-4100</postCode>
									<settlement>www.cnlp.org</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,326.56,111.30,117.00,9.94"><roleName>Necati</roleName><forename type="first">Nancy</forename><surname>Mccracken</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Natural Language Processing</orgName>
								<orgName type="department" key="dep2">School of Information Studies</orgName>
								<orgName type="department" key="dep3">Center for Science and Technology Syracuse</orgName>
								<orgName type="institution">Syracuse University</orgName>
								<address>
									<addrLine>4-206</addrLine>
									<postCode>1324-4100</postCode>
									<settlement>www.cnlp.org</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,446.32,111.30,66.10,9.94"><forename type="first">Ercan</forename><surname>Ozgencil</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Natural Language Processing</orgName>
								<orgName type="department" key="dep2">School of Information Studies</orgName>
								<orgName type="department" key="dep3">Center for Science and Technology Syracuse</orgName>
								<orgName type="institution">Syracuse University</orgName>
								<address>
									<addrLine>4-206</addrLine>
									<postCode>1324-4100</postCode>
									<settlement>www.cnlp.org</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,216.64,123.90,68.49,9.94"><forename type="first">Ozgur</forename><surname>Yilmazel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Natural Language Processing</orgName>
								<orgName type="department" key="dep2">School of Information Studies</orgName>
								<orgName type="department" key="dep3">Center for Science and Technology Syracuse</orgName>
								<orgName type="institution">Syracuse University</orgName>
								<address>
									<addrLine>4-206</addrLine>
									<postCode>1324-4100</postCode>
									<settlement>www.cnlp.org</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.42,123.90,84.08,9.94"><forename type="first">Elizabeth</forename><forename type="middle">D</forename><surname>Liddy</surname></persName>
							<email>liddy@syr.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Natural Language Processing</orgName>
								<orgName type="department" key="dep2">School of Information Studies</orgName>
								<orgName type="department" key="dep3">Center for Science and Technology Syracuse</orgName>
								<orgName type="institution">Syracuse University</orgName>
								<address>
									<addrLine>4-206</addrLine>
									<postCode>1324-4100</postCode>
									<settlement>www.cnlp.org</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,235.84,75.62,140.42,14.36;1,144.28,94.10,323.32,14.36">Question Answering : CNLP at the TREC-10 Question Answering Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C4EB238F6FF5D54B295C478C175190C9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the retrieval experiments for the main task and list task of the TREC-10 questionanswering track. The question answering system described automatically finds answers to questions in a large document collection. The system uses a two-stage retrieval approach to answer finding based on matching of named entities, linguistic patterns, and keywords. In answering a question, the system carries out a detailed query analysis that produces a logical query representation, an indication of the question focus, and answer clue words.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Question-answering systems retrieve answers rather than documents in response to a user's question. In the TREC question-answering track a number of question-answering systems attempt to answer a predefined list of questions by using a previously determined set of documents. The research is carried out in an unrestricted domain.</p><p>The CNLP question answering system uses a two-stage retrieval approach to answer-finding based on matching of named entities, linguistic patterns, and keywords. In answering a question, the system carries out a detailed query analysis that produces a logical query representation, an indication of the question focus, and answer clue words. Then the information is passed on to answer finding modules, which take the documents, retrieved in the first stage, for further processing and answer finding. The answer finding module uses three separate strategies to determine the correct answer. Two strategies are based on question focus, and the third strategy, based on keywords, is used when the question focus is not found or when the first two strategies fail to identify potential answers. A detailed system overview can be found in section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problem description</head><p>CNLP participated in two of the three QA track tasks: the main task and the list task. The main task is a continuation of last year's QA track in that systems are required to answer 500 short, fact-based questions. Two new aspects were introduced this year: unanswerable questions (with no answer present in the collection), and the notion of an answer confidence level. Systems needed to identify unanswerable questions as such, in order for them to be counted as correct. For the confidence level systems needed to state the rank of their final answer or state that they were unsure about their answer. For each question, up to five ranked answer responses were permitted, with the most likely answer ranked first. The maximum length of the answer string for a submitted run was 50 bytes. A response to a question consisted of the question number, the document ID of the document containing the answer, rank, run name, and the answer string itself.</p><p>The list task questions are similar to those of the main task but include an indication as to how many answer instances needed to be provided for an answer to be considered complete. A response to a list task question consisted of an unordered list with each line containing the question number, the document ID of the document containing an answer instance, and the answer string itself. The length of the list or the number of answer instances for each question is specified in the question. As in the main task, the maximum length of each answer string is 50 bytes. The different answer instances could be found within single documents or across multiple documents or a combination of both. There was no guarantee that all requested answer instances could indeed be found in the collection.</p><p>Answers to both main task and list task questions had to be retrieved automatically from approximately 3 gigabytes of data. Sources of the data were: AP newswire 1988-1990 (728 Mb), Wall Street Journal 1987-1992 (509 Mb), San Jose Mercury News 1991 (287 Mb), Financial Times 1991-1994 (564 Mb), Los Angeles Times 1989, 1990 (475 Mb), and Foreign Broadcast Information Service 1996 (470 Mb). The submitted answer strings for all tasks were evaluated by NIST's human assessors for correctness. <ref type="bibr" coords="2,481.37,154.75,16.76,8.96" target="#b9">[10]</ref> Examples of questions for both the main task and list task can be found in table 1. TREC-10 QA questions Main task questions: How much does the human adult female brain weigh? , Who was the first governor of Alaska? , When was Rosa Parks born? , Where is the Mason/Dixon line? , Why is a ladybug helpful? , Where is Milan? , In which state would you find the Catskill Mountains? , What are invertebrates? List task questions: Name 2 U.S. dams that have fish ladders. , What are 6 names of navigational satellites? , Who are 6 actors who have played Tevye in "Fiddler on the Roof"? , Name 20 countries that produce coffee.</p><p>Table <ref type="table" coords="2,116.91,284.32,3.76,8.96">1</ref>. Examples of TREC-10 questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">System overview</head><p>The CNLP question-answering system consists of four different processes: question processing, document processing, paragraph finding, and answer finding. Each of the processes is described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Question processing</head><p>Question processing has two major parts -conversion of questions into a logical query representation and question focus recognition. Our L2L (Language-to-Logic) module was used this year to convert the query into a logical representation suitable for keyword matching and weighting in our answer finder module (see section 3.3. and section 3.4). Last year we used our L2L module for first-stage retrieval but this year we relied solely on the ranked list of documents retrieved and provided by NIST. L2L was modified this year to also include query expansion for nouns and verbs found in WordNet 1.6 <ref type="bibr" coords="2,391.65,423.43,10.68,8.96" target="#b6">[8]</ref>. Based on the parts-ofspeech of the question words, the system added all related synonyms of the first, most frequently used sense (see example at the end of this section ) to the L2L representation.</p><p>Question focus recognition is performed in order to identify the type of answer expected. Expected answers fall into two broad groups -those based on lexical categories, and those based on answer patterns. Expected answers based on lexical categories can be identified from the terms used in the question. For example, in the question "What river flows between Fargo, North Dakota and Moorhead, Minnesota?", we identify that the questioner is looking for the name of a river. The expected answer type, and therefore the question focus, is river. Expected answers based on answer patterns are predicted by the recognition of certain question types. If the question is recognized as a definition question, then the answer sentence is likely to include one of several patterns, such as apposition, presence of a form of the verb be, etc.</p><p>The question focus recognition routine extracted four elements -the question focus, the lexical answer clue, the number of answers required (used for the list task only), and the confidence level (not fully implemented). In an effort to improve question focus recognition this year, we trained the Brill part-ofspeech tagger <ref type="bibr" coords="2,147.18,607.51,11.69,8.96" target="#b0">[2]</ref> on questions from TREC 8, TREC 9 and HowStuffWorks. <ref type="bibr" coords="2,397.84,607.51,11.69,8.96" target="#b5">[7]</ref> The resulting rules were used to tag the TREC 10 questions. The tagged questions were then run through the Collins parser <ref type="bibr" coords="2,485.63,618.91,25.83,8.96">[3] [4]</ref> for a full parse.</p><p>There are three steps to question focus assignment. In the first step, the question type is determined using predefined search patterns based on regular expressions. There are 7 special question types (acronym, counterpart, definition, famous, standfor, synonym, why) and 7 standard question types (name-a, name-of, where, when, what/which, how). If a special question type is recognized, then the question type becomes the question focus. Second, the parsed question is examined to extract the lexical answer clue (word or phrase) using the predefined search patterns. In the third step, which applies only to standard question types, the lexical answer clue is used to assign the question focus based on lexical categories where possible. Table <ref type="table" coords="3,153.13,97.15,4.98,8.96">2</ref>   <ref type="table" coords="3,116.91,427.12,3.76,8.96">2</ref>. Question types Additional processing performed by the question focus assignment routine includes the extraction of the number of answers required (used for the list task only), and assignment of a confidence level. The number of answers required was extracted based on the predefined search patterns for each question type. The confidence level assigned ranged from 0 to 5, with 5 being the highest level of confidence in the question focus. If the question focus could not be determined, the confidence level was 0. Otherwise, the confidence level was set at a value ranging up to 5 depending on the certainty of the question focus. Due to the short time available for development, confidence level assignment was only partially implemented for TREC 10 and therefore not used in the experiments.</p><p>The output resulting from the L2L module and the Question Focus recognition module is passed on to the paragraph finding module, the answer candidate recognition module, and the answer formatting module. A standard question type (in this case what/which), will produce the following output for the question "What is the deepest lake in the US?": Logical representation:</p><p>deep* lake* +US ( "United States" "United States of America" America U.S. USA U.S.A.) Query focus: lake#deepest lake#2#5 Tagged:</p><p>&lt;sentence sid="s0"&gt; what|WP be|VBZ the|DT &lt;CN&gt; deep|JJS lake|NN &lt;/CN&gt; in|IN the|DT &lt;NP cat="cntry" id="0"&gt; US|NP &lt;/NP&gt; ?|. &lt;/sentence&gt; A special question type (in this case definition), will produce the following output for the question "Who is Duke Ellington?":</p><p>Logical representation: +Duke* +Ellington* Query focus: def#Duke Ellington#2#5 Tagged:</p><p>&lt;sentence sid="s0"&gt; who|WP be|VBZ &lt;NP cat="per" id="0"&gt; Duke|NP Ellington|NP &lt;/NP&gt; ?|. &lt;/sentence&gt;</p><p>As can be seen in the examples above, expansions from WordNet are enclosed in parentheses, and the four elements in the question focus are separated by '#'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Document processing</head><p>For document retrieval, we used the ranked document list as provided by NIST. The top 200 documents from the list for each question were extracted from the TREC collection as the source documents for paragraph finding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Paragraph finding</head><p>In the paragraph finding stage, we aim to select the most relevant paragraphs from the top 200 retrieved documents from the first stage retrieval step. Paragraph selection was based on keyword occurrences in the paragraphs. Although we used the same strategy as last year to identify the paragraphs, we decided to experiment with the selection process itself. For one set of runs we took the original document and divided it up into paragraphs, based on textual clues. After selecting the top 300 most relevant paragraphs we tag only those paragraphs. This approach is identical to our TREC9 approach and these runs are labeled "PAR" (paragraph tagging). For the other set of runs we tagged the original document first, then divided it up into paragraphs from which the top 300 paragraphs were selected. These runs are labeled "DOC" (document tagging). Paragraph detection is no longer based on orthographic clues (i.e. indentations) for the "DOC" runs because this information is removed during the tagging process. The tagged document is divided into several sentence groups based on a pre assigned value that specifies the approximate number of words in each sentence group.</p><p>We hypothesized that tagging the whole document versus isolated paragraphs should provide better named entity identification. Named entities are often referred to in their full form early in a document, only to be reduced to a shorter form later on. When an isolated paragraph is presented to our system for tagging, the context information of the preceding paragraphs is not available for entity categorization, thus hindering tagging performance. The complete documents as well as the individual paragraphs were part-of-speech tagged and categorized by &lt;!metaMarker&gt; TM using CNLP's categorization rules.</p><p>[1] The quality of selected paragraphs and the system's categorization capabilities directly impact later processing such as answer finding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Answer finding</head><p>The answer finding process (see sections below) takes the tagged paragraphs from the paragraph finding stage (for "DOC" as well as "PAR" runs) and identifies different paragraph windows within each paragraph. A weighting scheme was used to identify the most promising paragraph window for each paragraph. These paragraph windows were then used to find answer candidates based on the question focus or additional clue words. All answer candidates were weighted and the top 5 (main task) or top n (list task) were selected. The answer finding process expanded answer finding strategies without making major changes to the weighting strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Paragraph-window identification and selection</head><p>Paragraph windows were selected by examining each occurrence of a question keyword in a paragraph. Each occurrence of a keyword in relation to the other question keywords was considered to be a paragraph window. A keyword that occurred multiple times thus resulted in multiple paragraph windows, one for each occurrence. A weight for each window was determined by the position of the keywords in the window and the distance between them. An alternative weighting formula was used for single-word questions. The window with the highest score was selected to represent that paragraph. The process was repeated for all 300 paragraphs resulting in an ordered list of paragraph windows -all potentially containing the answer to the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Answer candidate identification</head><p>This year we focused on expanding the answer candidate identification ability of the system by changing the answer finding strategies and adjusting our weighting schemes based on the TREC9 question set.</p><p>Answer candidate identification involves three separate strategies. Two strategies are based on question focus, and the third strategy, based on keywords, is used when the question focus is not found or when the first two strategies fail to identify potential answers. The two question focus strategies include search for a specific lexical category in the case of standard question types and search for a specific answer pattern in the case of special question types (see section 3.1). Which strategy is initially employed for a particular question is based on the value found in the question focus element in the question focus line. If the question focus value matches one of the special question types, then the specific answer pattern strategy is used. If the question focus has a value of "unknown", the third strategy involving keywords is invoked as a fallback. For all other values of the question focus element, the specific lexical category strategy is employed. For a discussion of the specific lexical category strategy and the keyword strategy see our TREC 9 paper. <ref type="bibr" coords="5,124.84,246.67,11.57,8.96" target="#b3">[5]</ref> For each special question type (acronym, counterpart, definition, famous, standfor, synonym, why), one or more answer patterns have been identified and defined in the answer candidate identification routine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Answer-candidate scoring and answer selection</head><p>The system used a weighting scheme to assign a weight to each answer candidate. Although we intended to change the weighting scheme to accommodate the new answer finding strategies we ran out of time. The weight was based on the keywords (presence, order, and distance), whether the answer candidate matched the question focus, and punctuation near the answer candidate. This resulted in a pool of at least 300 candidates for each question. A new unique-answer-identifier module removed duplicate answers from the answer-candidate list. The top 5 highest scoring answer candidates were selected as the final answers for each question for the main task. The required number of answers, identified during question processing, determined the number of answers for the list task questions. The answer strings were formatted according to NIST specifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>We submitted four runs for the TREC10 QA track: two runs for the main task and two runs for the list task. Each run name can be parsed into four components: 1) organization name, 2) trec, 3) tagging approach (see section 3.3), and 4) task.<ref type="foot" coords="5,188.08,451.49,3.24,5.83" target="#foot_0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Main task results</head><p>Averages over 492 questions<ref type="foot" coords="5,211.24,499.49,3.24,5.83" target="#foot_1">2</ref> (strict evaluation):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUT10DOCMT SUT10PARMT</head><p>Mean  <ref type="table" coords="5,116.91,598.84,3.76,8.96">3</ref>. Question answering results for the main task.</p><p>The evaluation measure for the main task (see Table <ref type="table" coords="5,302.08,621.79,4.16,8.96">3</ref>) is the mean reciprocal answer rank. For each question, a reciprocal answer rank is determined by evaluating the top five ranked answers starting with one. The reciprocal answer rank is the reciprocal of the rank of the first correct answer. If there is no correct answer among the top five, the reciprocal rank is zero. Since there are only five possible ranks, the mean reciprocal answer ranks can be 1, 0.5, 0.33, 0.25, 0.2, or 0. The mean reciprocal answer ranks for all the questions are summed together and divided by the total number of questions to get the mean reciprocal rank for each system run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">List task results</head><p>Averages over 25 questions SUT10DOCLT SUT10PARLT Average Accuracy 0.25 0.33 Questions with no correct answer found 4 (16 %) 5 (20 %) Questions above the median 13 (52 %) 15 (60 %) Questions on the median 9 (36 % 7 (28 %) Questions below the median 3 (12 %) 3 (12 %) Table <ref type="table" coords="6,116.91,227.20,3.76,8.96">4</ref>. Question answering results for the list task.</p><p>The evaluation measure for the list task (see Table <ref type="table" coords="6,294.29,250.15,4.16,8.96">4</ref>) is average accuracy. For each question accuracy is determined by the number of distinct correct answers over the target number of instances to retrieve. Accuracy for all the questions is summed together and divided by the total number of questions to get the average accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Analysis</head><p>The main task analysis examines: (5.1) retrieval performance of first stage retrieval based on the ranked list provided by NIST, (5.2) the Language-to-Logic module, (5.3) question focus assignment, (5.4) query expansion, and <ref type="bibr" coords="6,152.46,342.19,19.23,8.96">(5.5)</ref> the difference between the tagged document and tagged paragraph run performance. The list task analysis (5.6) examines list task performance, instance assignment, and the difference between the tagged document and tagged paragraph run performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">First stage retrieval</head><p>As mentioned previously, we used the ranked document list as provided by NIST for first stage retrieval. The retrieved lists were created using the PRISE system <ref type="bibr" coords="6,316.95,411.19,10.63,8.96" target="#b4">[6]</ref>. For TREC9 NIST used the SMART <ref type="bibr" coords="6,480.22,411.19,11.66,8.96" target="#b8">[9]</ref> information retrieval system (see Table <ref type="table" coords="6,249.78,422.71,3.63,8.96" target="#tab_2">5</ref>). Compared to last year's retrieval results, both the number of known relevant documents as well as the average number of retrieved relevant documents for each question decreased. The TREC10 retrieval results might have increased the difficulty of finding correct answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Question representation</head><p>A logical representation of the question is created in the question processing stage (see section 3.1). The question representation analysis of this year is based on the main task tagged "PAR" run (SUT10PARMT). We noticed that there were much more short questions this year than the previous two years. Even after query expansion, our system still produced 45 (9 %) single word queries and 64 (12.8 %) two-word queries. Many of these questions are "What/Who is/are/was/were" questions which asked for a definition of a person or a thing. Short queries, although represented correctly, may lead to failure in answer finding because the current weighting strategy has not been adapted to them. After excluding short queries, 73 (14.6 %) questions had various representation problems. The major query representation problems include keyword selection problems; part-of-speech errors; and misplaced wildcards (see Table <ref type="table" coords="7,442.31,97.15,3.62,8.96">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem count</head><p>Problems with description 30 16 13 Keyword selection problems Content words such as numbers were erroneously filtered out or truncated, or inappropriate words were selected Part-of-speech tagging errors Wrong tags led to incorrect morphological processing and query expansion error Misplaced wildcards Wildcards placed in the wrong place of single words created bad stems Table <ref type="table" coords="7,116.91,214.84,3.76,8.96">6</ref>. Question representation problems.</p><p>Compared with the query representation of last year, the system has improved the part-of-speech tagging, but did worse on keyword selection. Some important numbers, such as the number in question "What city has the zip code of 35824? " were filtered out by the system, which had a negative impact on answer finding.</p><p>Our conclusion of last year held true -query representation problems only accounted for part of the failure of answer finding. The "PAR" run contained 160 questions that did find the correct answer: 53 (33 %) were short queries, and 19 (12 %) had various query representation problems. The procedure we developed for answer candidate identification helped finding answers for short queries. However, the system did not find the correct answers for most of the questions even when the query representations were correct. Further analysis is needed to identify why this is the case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Question focus</head><p>As described in section 3.1, we determined the question focus based on special question patterns and lexical answer clues. The question focus analysis is based on the main task "PAR" run (SUT10PARMT). Out of 492 answerable questions, our system determined a question focus for 365 (74.17%) of the questions, more than 10 percent better than TREC-9 (see Table <ref type="table" coords="7,345.28,421.87,3.62,8.96" target="#tab_3">7</ref>). <ref type="bibr" coords="7,361.10,421.87,11.69,8.96" target="#b3">[5]</ref> Our efforts to improve focus recognition aided in this increase. Out of these 365 questions, 322 questions (88.2 %) had a correct focus, and 43 questions (11.8 %) had an incorrect focus. Not only did we find a question focus for a greater percentage of questions this year, we also found the correct focus for a greater percentage of questions as well. For 127 (25.8 %) questions, our system could not determine a focus. An analysis of the special question types (see <ref type="bibr" coords="7,273.85,621.79,33.76,8.96">Table 8)</ref> shows that some of the special question routines (definition, standfor) aided in finding the answer. Our ability to find the answers for definition type questions in particular is improved over last year. But since the majority of special question types still failed to find a correct answer, more work is needed.  <ref type="table" coords="8,116.91,182.20,3.77,8.96">8</ref>. Analysis of Special question types <ref type="foot" coords="8,264.76,180.05,3.24,5.83" target="#foot_2">3</ref>An analysis of lexical answer clues (see <ref type="bibr" coords="8,251.86,205.15,33.76,8.96">Table 9)</ref> shows that having the correct lexical answer clue aids in finding the correct question focus. In summary, our efforts to improve focus recognition led to a greater percentage of both identified question focus and correctly identified question focus. Having a question focus is clearly important for finding the answer, as 89.8 % (114/127) of the questions with no determinable focus failed to find an answer. Finding the correct lexical answer clues aids in finding the correct question focus. Special question processing helps, but needs improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correct question focus Incorrect question focus No determinable question</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acronym</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correct question focus</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Incorrect question focus</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No determinable question focus</head><p>Since the majority of the questions with a correct focus (188/322 = 58.4 %) did not retrieve an answer, we need to examine this finding in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Effects of query expansion</head><p>As discussed in section 3.1, we used WordNet 1.6 to expand nouns and verbs in the questions this year. Experiments using the TREC9 questions showed that the expansion helped find more relevant paragraphs, but whether it helped in locating the final answer within those paragraphs was not investigated. Query terms added from WordNet were found in 109 out of 160 (68 %) questions with correct answers in our paragraph run SUT10PARMT.</p><p>Query expansion had an additional, positive, impact. It actually provided correct answers for some short queries. For the question "What does the acronym NASA stand for? ", the phrase "National Aeronautics and Space Administration", was added to the L2L representation. This feature has been used in our procedure for identifying answer candidates for some question types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Document tagging versus paragraph tagging</head><p>Contrary to our expectation, the "DOC" run (see section 3.3) did not achieve better performance, but did worse than the "PAR" run (see Table <ref type="table" coords="8,241.42,610.15,3.63,8.96">3</ref>). This held true for both the main task and the list task. Following is the comparison of the two runs for main task (see <ref type="bibr" coords="8,290.48,621.67,37.26,8.96">Table 10)</ref>. We hypothesized that the low performance of the document run might be caused by a lack of system testing due to time constraints. The analysis provided a good opportunity to find system bugs as well as evaluate our approach. We noticed that in most cases where the two runs got the right answers at the same rank, they found the answers in different documents or different paragraphs. A careful examination of the results for the first 103 questions (questions 894 to 996) demonstrated that the following sources contributed to the poor performance of the "DOC" run and the difference between the two runs:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RunID # of correct answer</head><p>1. A bug in the paragraph finding program truncated some documents when they were split into paragraphs (see section 3.3). The impact of this bug was minor. 2. The keyword weighting strategy used for paragraph finding inadvertently differed slightly between runs, which led to different scores even when the same answer strings were found. The influence of this difference was minor because it did not cause a change in rank. 3. The sentence alignment procedure truncated part of the texts for some documents. The word alignment procedure occasionally failed to record some of the keywords in the paragraph window, which threw out some paragraphs with the correct answers and dramatically changed the weighting score for some answer candidates. The alignment problems were the major cause of the low performance of the document run and the difference between the two runs. 4. The size of the paragraphs also played a role in making the two runs different. In the "PAR" run, we identified paragraphs according to text indentation while the "DOC" run uses a predefined value (400 bytes) to group sentences into paragraphs. Normally the sentence groups are longer than the natural paragraphs. The difference in length changed the position of paragraph windows and led to different scores for the same candidates.</p><p>After fixing the bugs and adjusting the alignment procedures (sources 1 and 3), we ran the "DOC" run again and achieved comparable results between the two runs. For the first 103 questions, both runs found correct answers for 30 questions out of which 23 were identical.</p><p>We also compared the tagging and categorization between complete documents and individual paragraphs.</p><p>No difference between the two was found in this analysis. It might be that the TREC10 questions did not bring out the need for context information in tagging. This issue will need further investigation. Ultimately we need to decide between these two approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">List task evaluation</head><p>Both list task runs (SUT10PARLT and SUT10DOCLT) are based on the same question processing output.</p><p>The list task analysis examines the performance of the answer instance identification as well as the reasons for the large performance difference between main and list tasks.</p><p>A special feature was added to our question processing module this year to handle the extraction of the number of desired instances from the list questions. Analysis revealed that for 22 (88%) of the questions, the number of instances was determined correctly. For three questions the program could not determine the correct number of instances so it defaulted to 2 instances (enough instances to make up a list). Out of the 22 questions that provided the right number of instances none of the questions managed to get all of the desired answers correct.</p><p>The system seemed to perform better on the list task than on the main task (see Tables <ref type="table" coords="9,437.62,686.83,4.98,8.96">3</ref> and<ref type="table" coords="9,462.09,686.83,3.63,8.96">4</ref>). For the SUT10PARLT run only 20% of the questions could not be answered versus 67.5% in the main task counterpart run (SUT10PARMT). In observing the questions themselves it appears that the list task questions are more straightforward compared to the more complicated main task questions where 151 questions required more advanced linguistic pattern searches. The fact that the questions seem to be easier is reflected in the performance of the focus assignment module for the list task. Out of 25 list questions, 13 questions had a correct focus assignment, 3 questions had a wrong focus assignment, and for 9 questions the system correctly indicated that the focus was unknown. For the list task 88 % of the questions had a correct focus assignment versus 78% questions in the main task. Two out of the three questions with the wrong focus assignment were of identical form (Name n people who/from â€¦) and both indicated the answer should be a number instead of a person. The error is due to a clue in the focus program dealing with how many people questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and future research</head><p>The expansion of our question processing module clearly improved the accuracy of our focus assignment although there are still a large number of questions for which the system did not provide the correct answer. It appears that tagging the entire document before splitting it into paragraphs versus splitting it into paragraphs before tagging does not make a lot of difference. The decision on what tagging approach to take will depend on processing speed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,89.32,97.15,431.52,338.93"><head>Question type # of search patterns Example question</head><label></label><figDesc>is a review of the questions types. Predefined search patterns were developed for these question types:</figDesc><table coords="3,89.32,167.35,431.52,185.84"><row><cell>Standard</cell><cell>Name-a</cell><cell>3</cell><cell>Name a food high in zinc. (TREC 10, question 1268)</cell></row><row><cell>question</cell><cell>Name-of</cell><cell>2</cell><cell>What is the name of Neil Armstrong's wife? (TREC 10,</cell></row><row><cell>types</cell><cell></cell><cell></cell><cell>question 1007)</cell></row><row><cell></cell><cell>Where</cell><cell>10</cell><cell>Where is John Wayne airport? (TREC 10, question 922)</cell></row><row><cell></cell><cell>When</cell><cell>9</cell><cell>When is the official first day of summer? (TREC 10, question</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1331)</cell></row><row><cell></cell><cell>What/Which</cell><cell>14</cell><cell>What is the capital of Mongolia? (TREC 10, question 1050)</cell></row><row><cell></cell><cell>Who</cell><cell>9</cell><cell>Who lived in the Neuschwanstein castle? (TREC 10, question</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1281)</cell></row><row><cell></cell><cell>How</cell><cell>12</cell><cell>How tall is the Gateway Arch in St. Louis, MO? (TREC 10,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>question 971)</cell></row><row><cell></cell><cell>Acronym</cell><cell>4</cell><cell>What is the abbreviation for Texas? (TREC 10, question</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1172)</cell></row><row><cell></cell><cell>Counterpart</cell><cell>2</cell><cell>What is the Islamic counterpart to the Red Cross? (TREC 9,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>question 454)</cell></row><row><cell></cell><cell>Definition</cell><cell>7</cell><cell>What is autism</cell></row></table><note coords="3,333.38,344.23,108.11,8.96;3,154.36,356.23,31.59,8.96;3,238.48,356.23,4.98,8.96;3,273.40,356.23,218.53,8.96;3,154.36,368.23,34.43,8.96;3,238.51,368.23,4.98,8.96;3,273.43,368.23,217.53,8.96;3,273.40,379.75,59.70,8.96;3,154.36,391.75,38.24,8.96;3,238.38,391.75,4.98,8.96;3,273.30,391.75,240.62,8.96;3,273.40,403.27,74.81,8.96;3,89.32,297.31,29.41,8.96;3,89.32,308.83,33.78,8.96;3,89.32,320.23,21.03,8.96;3,154.36,415.27,19.45,8.96;3,238.43,415.27,4.98,8.96;3,273.35,415.27,236.66,8.96;3,90.04,427.12,24.33,8.96"><p>? (TREC 10, question 903) Famous 5 Why is Jane Goodall famous? (TREC 9, question 748) Standfor 4 What does the technical term ISDN mean? (TREC 10, question 1219) Synonym 3 What is the colorful Korean traditional dress called? (TREC 10, question 1151) Special question types Why 1 Why does the moon turn orange? (TREC 10, question 902) Table</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,90.04,446.44,417.07,128.64"><head>Table 5 .</head><label>5</label><figDesc>First stage retrieval performance.</figDesc><table coords="6,96.28,446.44,65.59,8.96"><row><cell>Top 200 results</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,90.04,503.08,369.54,104.76"><head>Table 7 .</head><label>7</label><figDesc>Answer rank distribution of question focus status.</figDesc><table coords="7,404.56,503.08,24.98,8.96"><row><cell>Focus</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,90.04,263.83,391.44,67.76"><head>Table 9 .</head><label>9</label><figDesc>Lexical Answer Clue vs. Question Focus</figDesc><table coords="8,95.44,263.83,386.04,55.88"><row><cell>Correct Lexical Answer</cell><cell>295 (91.6 %)</cell><cell>70 (64.8 %)</cell><cell>55 (88.7 %)</cell></row><row><cell>Clue</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Incorrect Lexical</cell><cell>27 (8.4 %)</cell><cell>38 (35.2 %)</cell><cell>7 (11.3 %)</cell></row><row><cell>Answer Clue</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Total = 492</cell><cell>322</cell><cell>108</cell><cell>62</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,97.24,87.64,414.18,67.31"><head>Rank 1 Rank 2 Rank 3 Rank 4 Rank 5</head><label></label><figDesc></figDesc><table coords="9,97.24,110.95,405.42,44.00"><row><cell>Document tagging</cell><cell>111</cell><cell>51</cell><cell>22</cell><cell>20</cell><cell>8</cell><cell>10</cell></row><row><cell>(SUT10DOCMT)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Paragraph tagging</cell><cell>160</cell><cell>78</cell><cell>31</cell><cell>18</cell><cell>19</cell><cell>14</cell></row><row><cell>(SUT10PARMT)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="9,90.04,157.84,340.38,9.00"><head>Table 10 .</head><label>10</label><figDesc>Comparison of correct answers found by document run and paragraph run.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,95.80,676.27,385.97,8.96;5,90.04,687.67,174.64,8.96"><p>SU = Syracuse University, T10 = TREC10, DOC = tag entire document / PAR = tag individual paragraphs, MT = main task / LT = list task</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,95.80,699.19,424.23,8.96;5,90.04,710.71,210.08,8.96"><p>The initial question set of 500 questions was reduced to 492 questions after 8 questions were discarded by the National Institute for Standards and Technology.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="8,95.80,699.19,406.07,8.96;8,90.04,710.71,70.45,8.96"><p>This analysis is based solely on the special question types identified as such; there were a total of 151 special questions.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>After the TREC10 experiments it is clear that a lot of work remains to be done. Our analysis shows that a one-size-fits-all approach to answer-finding does not work well. The system needs alternative answerfinding strategies for different question types and the means to differentiate between these question types. These different strategies also imply more advanced weighting schemes than are currently implemented.</p><p>Our work on answer confidence level assignment needs to be completed and refined. The confidence level work will also include the ability to decide whether an answer can indeed by provided. In addition the system also needs to be adapted to deal with the context specific task (the third TREC Q&amp;A track task) where each answer provides contextual information to help answering the next (related) question.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="10,104.32,396.19,414.48,8.96;10,90.04,407.71,324.94,8.96;10,90.04,419.23,194.77,8.96" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="10,185.82,396.19,332.97,8.96;10,90.04,407.71,150.05,8.96">Transformation-Based Error-Driven Learning and Natural Language Processing: A Case Study in Part of Speech Tagging</title>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Brill</surname></persName>
		</author>
		<ptr target="&lt;http://www.cs.jhu.edu/~brill/CompLing95.ps&gt;" />
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note>Computational Linguistics. Available at</note>
</biblStruct>

<biblStruct coords="10,104.32,430.63,370.12,8.96;10,90.04,442.15,86.81,8.96;10,176.92,439.97,5.04,5.83;10,184.60,442.15,218.47,8.96;10,90.04,453.67,244.78,8.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,212.74,430.63,257.20,8.96">A New Statistical Parser based on Bigram Lexical Dependencies</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<ptr target="www.research.att.com/~mcollins/papers/acl9629.ps&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="10,90.04,442.15,86.81,8.96;10,176.92,439.97,5.04,5.83;10,184.60,442.15,160.39,8.96">Proceedings of the 34 th Annual Meeting of the ACL, Santa Cruz</title>
		<meeting>the 34 th Annual Meeting of the ACL, Santa Cruz</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,104.26,465.19,417.61,8.96;10,90.04,476.71,24.66,8.96;10,114.76,474.53,5.04,5.83;10,122.32,476.71,392.08,8.96;10,90.04,488.23,267.84,8.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,212.73,465.19,240.17,8.96">Three Generative, Lexicalised Models for Statistical Parsing</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<ptr target="www.research.att.com/~mcollins/papers/paper14.short.ps&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="10,462.02,465.19,59.84,8.96;10,90.04,476.71,24.66,8.96;10,114.76,474.53,5.04,5.83;10,122.32,476.71,333.98,8.96">Proceedings of the 35 th Annual Meeting of the ACL (jointly with the 8th Conference of the EACL), Madrid</title>
		<meeting>the 35 th Annual Meeting of the ACL (jointly with the 8th Conference of the EACL), Madrid</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,104.25,499.63,390.27,8.96;10,90.04,511.15,424.33,8.96;10,90.04,522.67,300.97,8.96" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Anne</forename><forename type="middle">;</forename><surname>Diekema</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiangping;</forename><surname>Xiaoyong; Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nancy</forename><forename type="middle">;</forename><surname>Hudong; Mccracken</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Yilmazel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><surname>Ozgur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Elizabeth</forename><forename type="middle">D</forename><surname>Liddy</surname></persName>
		</author>
		<ptr target="&lt;http://trec.nist.gov/pubs/trec9/papers/cnlptrec9.pdf&gt;" />
		<title level="m" coord="10,255.37,511.15,259.00,8.96;10,90.04,522.67,21.56,8.96">Question Answering: CNLP at the TREC-9 Question Answering Track</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,104.25,534.19,410.88,8.96;10,90.04,545.71,294.61,8.96" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="10,423.22,534.19,91.91,8.96;10,90.04,545.71,49.40,8.96">Its installation, use &amp; modification</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dimmick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Rodgers</surname></persName>
		</author>
		<idno>39.50/prise 2.0</idno>
		<ptr target="http://www-nlpir.nist.gov/works/papers/zp2/zp2.html" />
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,104.31,557.11,206.36,8.96" xml:id="b5">
	<monogr>
		<ptr target="http://www.howstuffworks.com/" />
		<title level="m" coord="10,104.31,557.11,63.46,8.96">HowStuffWorks</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,104.32,568.60,414.92,9.00;10,90.04,580.15,22.49,8.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,178.13,568.63,157.48,8.96">WordNet: An On-line Lexical Database</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,342.40,568.60,152.39,8.96">International Journal of Lexicography</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,118.82,580.15,61.20,8.96" xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Special</forename><surname>Issue</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,104.31,591.64,408.82,9.00;10,90.04,603.19,190.59,8.96" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ed</surname></persName>
		</author>
		<title level="m" coord="10,192.40,591.64,316.47,8.96">The SMART Retrieval System: Experiments in Automatic Document Processing</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall, Inc</publisher>
			<date type="published" when="1971">1971</date>
			<biblScope unit="page">556</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,109.30,614.71,402.71,8.96;10,90.04,626.08,430.95,9.00;10,90.04,637.63,330.31,8.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,287.60,614.71,206.66,8.96">The TREC-8 Question Answering Track Evaluation</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dawn</forename><forename type="middle">M</forename><surname>Tice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,253.60,626.08,192.98,8.96">The Eighth Text REtrieval Conference (TREC-8)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-11-17">1999. 1999. November 17-19</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST)</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
