<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.56,43.77,452.88,27.00;1,135.72,65.61,358.32,27.00">Cross-Language Retrieval for the CLEF Collections { Comparing Multiple Methods of Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,158.76,102.48,72.54,16.40"><forename type="first">Fredric</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
							<email>gey@ucdata.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,240.96,102.48,66.38,16.40"><forename type="first">Hailing</forename><surname>Jiang</surname></persName>
							<email>hjiang1@sims.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,316.20,102.48,69.36,16.40"><forename type="first">Vivien</forename><surname>Petras</surname></persName>
							<email>vivienp@sims.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,412.32,102.48,58.80,16.40"><forename type="first">Aitao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.56,43.77,452.88,27.00;1,135.72,65.61,358.32,27.00">Cross-Language Retrieval for the CLEF Collections { Comparing Multiple Methods of Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">14617180A96796AF539181BC02A3FF93</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For our participation in CLEF, the Berkeley group participated in the monolingual, multilingual and GIRT tasks. To help enrich the CLEF relevance set for future training, we prepared a manual reformulation of the original German queries which a c hieved excellent performance, more than 110% better than average of median precision. The GIRT task performed English-German Cross-Language IR by comparing commercial machine translation with thesaurus lookup techniques and query expansion techniques. Combining all techniques using simple data fusion produced the best results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Unlike monolingual retrieval where the queries and documents are in the same language and where mechanistic techniques can be applied, Cross-language information retrieval (CLIR) must combine linguistic techniques (phrase discovery, machine translation, bilingual dictionary lookup) with robust monolingual information retrieval. The Berkeley Text Retrieval Research group has been using the technique of logistic regression from the beginning of the TREC series of conferences. Indeed our primary development has been a result of the U.S. TREC conferences and collections which provided the rst large-scale test collection for modern information retrieval experimentation. In TREC-2 2] we derived a statistical formula for predicting probability of relevance based upon statistical clues contained with documents, queries and collections as a whole. This formula was used for document retrieval in <ref type="bibr" coords="1,208.80,423.15,44.28,13.80">Chinese 3]</ref> and Spanish in TREC-4 through TREC-6. We utilized the identical formula for English queries against German documents in the cross-language track for TREC-6. In TREC-7 the formula was also used for cross-language runs over multiple European languages. During the past year the formula has proven well-suited for Japanese and Japanese-English cross-language information retrieval 7], even when only trained on English document collections. Our participation in the NTCIR Workshop in Tokoyo (http://www.rd.nacsis.ac.jp/Ã±tcadm/workshop/work-en.html) led to di erent techniques for cross-language retrieval, ones which utilized the power of human indexing of documents to improve retrieval via bi-lingual lexicon development and a form of text categorization which associated terms in documents with humanly assigned index terms 1]. These techniques were applied to English-German retrieval for the GIRT-1 task and collection in the TREC-8 conference 5]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Logistic Regression for Document Ranking</head><p>The document ranking formula used by B e r k eley in all of our CLEF retrieval runs was the TREC-2 formula 2]. The ad hoc retrieval results on the TREC test collections have s h o wn that the formula is robust for long queries and manually reformulated queries. Applying the same formula (trained on English TREC collections) to other languages has performed well, as on the TREC-4 Spanish collections, the TREC-5 Chinese collection 6] and the TREC-6 and TREC-7 European languages (French, German, Italian) 4, 5]. Thus the algorithm has demonstrated its robustness independent of language as long as appropriate word boundary detection (segmentation) can be achieved. The logodds of relevance of document D to query Q is given by</p><formula xml:id="formula_0" coords="1,72.00,675.39,486.00,28.32">log O(RjD Q ) = l o g P (RjD Q ) P (RjD Q )<label>(1)</label></formula><p>= ;3:51 + 1 p N + 1</p><formula xml:id="formula_1" coords="1,245.04,709.47,312.96,24.60">+ :0929 N (2) 1 = 37:4 N X i=1 q t f i q l+ 3 5 + 0 :330 N X i=1 log dtf i dl + 8 0 ;0:1937 N X i=1 log ctf i cl (<label>3</label></formula><formula xml:id="formula_2" coords="2,553.76,64.95,4.24,13.80">)</formula><p>where P (RjD Q ) is the probability of relevance of document D with respect to query Q, P (RjD Q ) is the probability of irrelevance of document D with respect to query Q. Details about the derivation of these formulae may be found in our NTCIR workshop <ref type="bibr" coords="2,241.68,117.27,37.28,13.80">paper 7]</ref>. It is to be emphasized that training has taken place exclusively on English documents but the matching has proven robust over seven other languages in monolingual retrieval, including Japanese and Chinese where word boundaries form an additional step in the discovery process.</p><p>3 Submissions for the CLEF main tasks Following is a description for each run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Monolingual Retrieval of the CLEF collections</head><p>BKMOIIA3 (Berkeley Monolingual Italian against Italian Automatic Run 3) The original query topics in Italian were searched against the Italian collection (La Stampa). For indexing this collection, we u s e d a s t o p wordlist, the Italian-to-lower normalizer and the Italian stemmer (from association dictionary) described in Section 4. BKMOFFA2 (Berkeley Monolingual French against French Automatic Run 2)</p><p>The original query topics in French w ere searched against the French collection (Le Monde). For indexing this collection, we used a stopwordlist, the French-to-lower normalizer and the French stemmer (from association dictionary) described in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BKMOGGA1 (Berkeley Monolingual German against German Automatic Run 1)</head><p>The original query topics in German were searched against the German collection (Frankfurter Rundschau and Der Spiegel). For indexing the collection, we used a s t o p wordlist that contained also capitalized versions of words and the German stemmer (from association dictionary) described in Section 3.4. We did not use a normalizer for this collection because all nouns in German are capitalized and hence this clue might be used in retrieval.</p><p>4. BKMOGGM1 (Berkeley Monolingual German against German Manual Run 1) The original query topics in German were extended with additional query terms obtained by searching the German CLEF collection (Frankfurter Rundschau and Der Spiegel) with the original German query topics and looking at the results for these original queries (with the help of Aitao Chen's Cross-language Text Retrieval System Web-interface). The additional query terms were obtained by either directly looking at the documents or looking at the top ranked document terms for the original query text. The searcher spent about 10 to 25 minutes per topic or query depending on familiarity with the context and meaningfulness of the returned documents and top ranked document terms. For indexing the collection, we used a stopwordlist that contained also capitalized versions of words and the German stemmer (from association dictionary) build by Aitao Chen. We didn't use a normalizer for this collection. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Monolingual Performance</head><p>Our monolingual performance can be found in meaningful statistic from which inference can be made, we have found it useful to average the medians of all queries as sent b y CLEF organizers. Comparing our overall precision to this average of medians gives us some fuzzy gauge of whether our performance is better, poorer, or about the same as the median performance. Thus the bottom two r o ws of the table present the Berkeley overall precision over all queries for which performance has been judged and, below it, the average of the median precision for each query over all submitted runs. From this we see that Berkeley's automatic runs are about the same as the overall 'average' while Berkeley's German-German manual run comes in at overall precision 57 percent better than Average of Median precisions for German-German monolingual runs. As we shall see in the next section, an improved German query set had an even greater impact on multilingual retrieval. Another observation to make is that of the skewedness of relevancy. More than twice as many relevant documents come from the German collection than the Italian collection. Thus a better German query set may have an impact on multilingual retrieval more than a better Italian query set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multilingual Retrieval of the CLEF collections</head><p>Several interesting questions have arisen in recent research on CLIR. First, is CLIR merely a matter of a marriage of convenience between machine translation combined with ordinary (monolingual) information retrieval? In our CLEF work we made use of two w i d e l y a vailable machine translation packages, the SYSTRAN system found at the AltaVista site, and the Lernout and Hauspie Power Translator Pro Version 7.0. For the GIRT retrieval we made comparisions to Power Translator. For CLEF multilingual we c o m bined translations and dictionary lookup from multiple sources, having found that di erent p a c kages made di erent m i s t a k es on particular topics. Second, what is the role of language speci c stemming in improved performance? Our experience with the Spanish tracks of TREC have convinced us that some form of stemming will always improve performance. For this particular evaluation we c hose to create a stemmer mechanistically from common leading substring analysis of the entire corpus. The impact of the stemmer on performance will be discussed at the end of the o cial results discussion. Third, is performance improved by c r e a t i n g a m ultilingual index by pooling all documents together in one index or by creating separate language indexes and doing monolingual retrieval for each language followed by data fusion which combines the individual rankings into a uni ed ranking independent of language? This was one of the major focuses of our experiments at CLEF.</p><p>1. BKMUEAA1 (Berkeley Multilingual English against all Automatic Run 1)</p><p>The original query topics in English were translated once with the Systran system (http://babel.altavista.com/translate.dyn) and with L&amp;H Powertranslator. The English topics were translated in French, German, and Italian. The two translated les for each languages were pooled together and then put together in one query le (the English original query topics were multiplied by 2 to gain the same frequency of query terms in the query le). The nal topics le contained 2 English (original), French, German, and Italian versions (one Powertranslator and one Systran) for each topic. During the search, we divided the frequency of the search terms by 2 to avoid over-emphasis of equally translated search terms. The collection consisted of all languages. For indexing the English part of this collection, we used a stopwordlist, the default normalizer and the Porter stemmer. For indexing the French part of this collection, we used a stopwordlist, the French-tolower normalizer and the French stemmer (from association dictionary in section 3.5). For indexing the German part of the collection, we used a stopwordlist that contained also capitalized versions of words and the German stemmer (from association dictionary) build by Aitao Chen. We didn't use a normalizer for this collection. For indexing the Italian part of this collection, we u s e d a s t o p wordlist, the Italian-to-lower normalizer and the Italian stemmer (from association dictionary). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BKMUEAA2 (Berkeley Multilingual English against all Automatic Run 2)</head><p>The original query topics in English were translated once with Systran and with L&amp;H PowerTranslator. The English topics were translated into French, German, and Italian. The 2 translated versions for each language were pooled together in one query le (resulting in 3 topics les, one in German with the Systran and Powertranslator version, one in French with the Systran and Powertranslator version, and one in Italian accordingly). The original English topics le was searched against the English collection (Los Angeles Times). The pooled German topics le was searched against the German collection, the pooled French topics le was searched against the French collection, and the pooled Italian topics le was searched against the Italian collection. The frequency of the search terms was divided by 2 to avoid over-emphasis of equally translated search terms. This resulted in 4 result les with the 1000 top ranked records for each topic. These 4 result les were then pooled together and sorted by w eight (rank) for each record and topic. The pooling method is described below. For a description of the collections see BKMOGGM1, BKMOFFA2, BKMOIIA3, BKMUEAA1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">BKMUGAA2 (Berkeley Multilingual German against all Automatic Run 2)</head><p>The original query topics in German were translated once with Systran and with Powertranslator. The German topics were translated into English, French, and Italian. The 2 translated versions for each language were pooled together in one query le. The original German topics le was multiplied by 2 to gain the same frequency of query terms in the query le searched. The nal topics le contained 2 German (original), English, French, and Italian versions (one Powertranslator and one Systran) for each topic. During the search, we divided the frequency of the search terms by 2 to avoid over-emphasis of equally translated search terms. For a description of the collection see BKMUEAA1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">BKMUGAM1 (Berkeley Multilingual German against all Manual Run 1)</head><p>The manually extended German query topics (see description from BKMOGGM1) were now translated with Powertranslator into English, French and Italian. These translations were pooled together with the German originals in one le. This topics le was searched against the whole collection including all 4 languages. For a description of the collection see BKMUEAA1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Berkeley's CLEF Multilingual Performance</head><p>Our multilingual performance can be found in Table <ref type="table" coords="5,307.32,38.43,3.90,13.80" target="#tab_3">3</ref>.</p><p>As contrasted from the average of medians for monolingual, the values in the last row of the table are the same for all columns. Our automatic runs performed almost identically at about 38 percent better than average of medians, while the run BKMUGAM1 at overall precision 0.39 is 112 percent greater than the average of multilingual query medians.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Building a Simple Stemmer for Cross-Language Information Retrieval</head><p>A stemmer for the French collection was created by rst translating all the distinct French w ords found in the French collection into English using SYSTRAN. The English translations were normalized by reducing verbs to the base form, nouns to the singular form, and adjectives to the positive form. All the French w ords which h a ve the same English translations after normalization were grouped together to form a class. A member from each class is selected to represent the whole class in indexing. All the words in the same class were replaced by the class representative in indexing.</p><p>The German stemmer and Italian stemmer were created alike. We submitted four monolingual runs and four multilingual runs. These eight runs were repeated without the French, German, and Italian stemmers. The overall precision for each o f t h e e i g h t runs without stemming are shown in column <ref type="bibr" coords="5,165.96,240.15,5.04,13.80" target="#b1">3</ref>  The overall precision for pooling queries and without stemming (the method we applied two y ears ago) for the multilingual run using English queries was .2335. With stemming and pooling documents, the overall precision for the same run was .2626, which is 12.46 percent better. This can be considered as additional evidence that adding a stemming capability will result in an improvement in automatic multilingual retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Data fusion or Monolingual Document Pooling</head><p>The second idea centers on pooling documents from monolingual retrieval runs. The brain-dead solution would be to simply combine the retrieval results from four monolingual retrieval runs and sort the combined results by the estimated probability of relevance. The problem with the simple combination approach is that when the estimated probability of relevance is biased toward one document collection (as the above statistics show for German), the documents from that collection will always appear in the top in the combined list of ranked documents. For our nal run, we took a more conservative a p p r o a c h b y making sure the top 50 documents from each of the four monolingual list of documents will appear in top 200 in the combined list of documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Failure Analysis</head><p>A query-by-query analysis can be done to identify problems. We h a ve not had time to do this, but one query stands out. Query 40 about the privatization of the German national railway w as one which seems to have g i v en everyone problems (the median precision over all CLEF runs was 0.0537 for this query). As an American group, we w ere particularly vexed by the use of the English spelling 'privatisation' which couldn't be recognized by either of our machine translation softwares. The German version of the topic was not much better { in translation its English equivalent became 'de-nationalization' a very uncommon synonym for 'privatization,' and one which yielded few relevant documents. By comparison, our German manual reformulation of this query resulted in an average precision of 0.3749 for best CLEF performance for this query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GIRT retrieval</head><p>A special emphasis of our current funding has focussed upon retrieval of specialized domain documents which have been assigned individual classi cation identi ers by h uman indexers. These classi cation identi ers come from what we call "domain ontologies", of which thesauri are a particular case. Since many millions of dollars are expended on developing these classi cation ontologies and applying them to index documents, it seems only natural to attempt to exploit the resources previously expended to the fullest extent possible to improve retrieval. In some cases such thesauri are developed with identi ers translated (or provided) in multiple languages. This has been done in Europe with the GEMET (General European Multilingual Environmental Thesaurus) e ort and with the OECD General Thesaurus (available in English, French, and Spanish). A review of multilingual thesauri can be found in 8].</p><p>The GIRT collection consists of reports and papers (grey literature) in the social science domain. The collection is managed and indexed the GESIS organization (http://www.social-science-gesis.de). GIRT is an excellent example of a collection indexed by a m ultilingual thesaurus, originally German-English, recently translated into Russian. We w orked extensively with a previous version of the GIRT collection in our cross-language work for TREC-8 5]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The GIRT Collection</head><p>There are 76128 German documents in GIRT subtask collection. Of them, about 54275 (72 percent) have English TITLE sections. 5317 documents (7 percent) have also English TEXT sections. Almost all the documents contain manually assigned thesaurus terms. On average, there are about 10 thesaurus terms assigned to each document.</p><p>In our experiments, we indexed only the TITLE and TEXT sections in each document (not the E-TITLE or E-TEXT). The CLEF rules speci ed that indexing any other eld would need to be declared a manual run. For our CLEF runs this year we added a German stemmer similar to the Porter stemmer for the German language. Using this stemmer led to a 15 percent increase in average precision when tested using the GIRT-1 collection of TREC-8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Query translation</head><p>In CLIR, essentially either queries or documents or both need to be translated from one language to another. Query translation is almost always selected for practical reasons of e ciency, and because translation errors in documents can propagate without discovery since the maintainers of a text archive rarely read every document.</p><p>For the CLEF GIRT task, our focus has been to compare the performance of di erent translation strategies. We applied the following three methods to translate the English queries to German: Thesaurus lookup, Entry Vocabulary Module (EVM), machine translation (MT). The resulted German queries were run against the GIRT collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Thesaurus lookup</head><p>The GIRT social science Thesaurus is a German-English bilingual thesaurus. Each German item in this thesaurus has a corresponding English translation. We took the following steps to translate the English query to German by looking up the thesaurus: a. Create an English-German transfer dictionary from the Social Science Thesaurus. This transfer dictionary contains English items and their corresponding German translations. This "vocabulary discovery" approach was taken by Eichmann, Ruiz and Srinivasan for medical information cross-language retrieval using the UMLS <ref type="bibr" coords="6,72.00,604.83,73.84,13.80">Metathesaurus 9]</ref>.</p><p>b. Use the part-of-speech tagger LT-POS developed by U n i v ersity o f E d i n burgh (http://www.ltg.ed.ac.uk/software/pos/index.html) to tag the English query and identify noun phrases in the English query. One problem with thesaurus lookup is how t o m a t c h the phrasal items in a thesaurus. We have t a k en a simple approach to deal with this problem: use POS tagger to identify noun phrases.</p><p>For last year's GIRT task at the TREC-8 evaluation, we extracted an English-German transfer dictionary from the GIRT thesaurus and used it to translate the English queries to German. This approach left about 50 percent of English query words untranslated. After examining the untranslated English query words carefully, w e found that most of them fell into the following two categories: one category contains general terms that are not likely to occur in a domain-speci c thesaurus like G I R T. Examples are "country", "car", "foreign", "industry", "public", etc. The other category are terms that occur in the thesaurus but in a di erent format from the original English query words. For example, "bosnia-herzegovina" does not appear in the thesaurus, but "bosnia and herzegovina" does.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Fuzzy Matching for the Thesaurus</head><p>To deal with the general terms in the rst category, a general-purpose dictioanry was applied after thesaurus lookup. A fuzzy-matching strategy was used to address the problem for the second category. It counts the letter pairs that two strings have in common and uses Dice's coe cient as a means of accessing the similarity b e t ween the two strings. This fuzzy-matching strategy successfully recovered some query terms, for example, original query terms thesaurus terms asylum policy policy on asylum anti-semitism antisemitism bosnia-herzegovina bosnia and herzegovina gypsy gipsy German Democratic Republic German Democratic Republic (gdr) Fuzzy matching also found related terms for some query terms which do not appear in the thesaurus at all, for example, original query terms thesaurus terms nature protection legislation nature protection violent a c t violence bosnia bosnian</p><p>We tested this combined approach using last year's GIRT-1 data. The results showed about 18 percent increase as measured by a verage precision compared with simple thesaurus lookup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Entry Vocabulary Module (EVM)</head><p>In the GIRT collection, about 72 percent of the documents have both German titles and English titles. 7 percent have also English text sections. This feature allows us to build a EVM which maps the English words appearing in English Title and text sections to German thesaurus terms. This mapping can then be used to translate the English queries. More details about this work can be found in 5].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Machine translation (MT)</head><p>For comparison, we also applied the Lernout and Hauspie Power Translator product to translate the English queries into German.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5">Merging results</head><p>While our CLEF Multilingual strategy focussed on merging monolingual results run independently on di erent subcollections, one per language, all our GIRT r u n s w ere done on a single subcollection, the German text part of GIRT. When analyzing the experimental training results, we noticed that di erent translation methods retrieved sets of documents that contain di erent relevant d o c u m e n ts. This implies that merging the results from di erent translation methods may lead to better performance than of any o n e of the methods. Since we use the same retrieval algorithm and data collection for all the runs, the probability that a document is relevant to a query from di erent runs are commensurable. So, for each document retrieved, we used the sum of its probability from the di erent runs as its nal probability to create the ranking for the merged results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results and analysis</head><p>Our GIRT results are summarized in Table <ref type="table" coords="7,272.40,682.83,3.90,13.80" target="#tab_5">5</ref>. The runs can be described as follows: BKGREGA4 used our entry vocabulary method to map from query term to thesaurus term, the top ranked thesaurus term and its translation was used to create the German query. BKGREGA3 used the results of machine translation by the 7</p><p>L&amp;H Power Translator software. The run BKGREGA2 used thesaurus lookup of English terms in the query and a general purpose English German dictionary for not found terms as well as the fuzzy matching strategy described above. The nal run BKGREGA1 pooled the merged results from the other three runs according to the sum of probabilities of relevance. Note that it performs signi cantly better than the other three runs, and about 61 percent better than the average of median precisions for the CLEF GIRT.</p><p>Run </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Summary and Acknowlegments</head><p>Berkeley's participation in CLEF has enabled us to explore re nements in Cross-language information retrieval. Speci cally we h a ve explored two data fusion methods { for the CLEF multilingual we d e v eloped a t e c hnique for merging from monolingual, language speci c rankings for the GIRT English-German task, we obtained improved retrieval by merging the results of multiple methods of mapping from English queries to German. A new stemming method was developed which maps classes of words to a representative w ord in both English and the targeted languages of French, German, and Italian. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,315.96,439.95,242.04,13.80;8,72.00,451.95,409.92,13.80;8,87.00,463.95,471.00,13.80;8,72.00,475.83,486.00,13.80;8,72.00,487.83,486.00,13.80;8,72.00,499.71,486.12,13.80;8,72.00,511.71,472.56,13.80"><head></head><label></label><figDesc>For future research w e are creating a Russian version of the GIRT queries to test strategies for Russian-German retrieva l v i a a m ultilingual thesaurus. This research w as supported in part by the Information and Data Management Program of the National Science Foundation under grant IRI-9630765 from the Information and Data Management program of the Computer and Information Science and Engineering Directorate. Major support was also provided by D ARPA (Department of Defense Advanced Research Projects Agency) under research g r a n t N66001-00-1-8911, Mar 2000-Feb 2003 as part of the DARPA T ranslingual Information Detection, Extraction, and Summarization Program (TIDES).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,72.00,195.51,486.12,190.44"><head>Table 1 :</head><label>1</label><figDesc>Summary of eight o cial CLEF runs.</figDesc><table coords="2,72.00,195.51,486.12,168.60"><row><cell cols="2">For CLEF we submitted 8 runs, 4 for the Monolingual (non-English) task and 4 for the Multilingual task. For the Monolingual task we submitted:</cell></row><row><cell>For the Monolingual task we submitted: Run Name BKMOGGM1 BKMOFFA2 BKMOGGA1 BKMOIIA3 For the Multilingual task we submitted: BKMUEAA1 BKMUGAM1 BKMUEAA2 BKMUGAA3</cell><cell>Language Run type Priority German Manual 1 French Automatic 2 German Automatic 3 Italian Automatic 4 English Automatic 1 German Manual 2 English Automatic 3 German Automatic 4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,152.40,38.43,405.60,240.72"><head>Table 2 .</head><label>2</label><figDesc>While average of medians cannot be considered a</figDesc><table coords="3,152.40,60.99,325.56,218.16"><row><cell>Run ID Retrieved Relevant Rel. Ret Precision at 0.00 at 0.10 at 0.20 at 0.30 at 0.40 at 0.50 at 0.60 at 0.70 at 0.80 at 0.90 at 1.00 Brk. Prec. Med. Prec.</cell><cell>BKMOIIA3 BKMOFFA2 BKMOGGA1 BKMOGGM1 34000 34000 37000 37000 338 528 821 821 315 508 701 785 0.7950 0.7167 0.6342 0.6907 0.7617 0.6824 0.5633 0.6584 0.6601 0.5947 0.5173 0.6442 0.6032 0.5195 0.3999 0.6037 0.5756 0.4825 0.3687 0.5624 0.5336 0.4404 0.3181 0.5428 0.4189 0.3627 0.2731 0.4970 0.3098 0.2960 0.2033 0.4580 0.2417 0.2422 0.1704 0.4006 0.1816 0.1936 0.1364 0.2959 0.1533 0.1548 0.0810 0.2059 0.4601 0.4085 0.3215 0.4968 0.4453 0.4359 0.3161 0.3161</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,191.76,287.19,246.60,13.80"><head>Table 2 :</head><label>2</label><figDesc>Results of four o cial CLEF monolingual runs.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,148.68,150.03,332.88,240.12"><head>Table 3 :</head><label>3</label><figDesc>Results of four o cial CLEF multilingual runs.</figDesc><table coords="4,148.68,150.03,332.88,218.28"><row><cell>ID Retrieved Relevant Rel. Ret. Precision at 0.00 at 0.10 at 0.20 at 0.30 at 0.40 at 0.50 at 0.60 at 0.70 at 0.80 at 0.90 at 1.00 Brk. Prec. Med. Prec.</cell><cell>BKMUEAA1 BKMUEAA2 BKMUGAA2 BKMUGAM1 40000 40000 40000 40000 2266 2266 2266 2266 1434 1464 1607 1838 0.7360 0.7460 0.7238 0.7971 0.5181 0.5331 0.5046 0.6534 0.4287 0.4465 0.4229 0.5777 0.3545 0.3762 0.3565 0.5032 0.2859 0.2929 0.3027 0.4373 0.2183 0.2290 0.2523 0.3953 0.1699 0.1846 0.1990 0.3478 0.1231 0.1454 0.1682 0.3080 0.1020 0.0934 0.1295 0.2238 0.0490 0.0480 0.0622 0.1530 0.0136 0.0081 0.0138 0.0474 0.2502 0.2626 0.2654 0.3903 0.1843 0.1843 0.1843 0.1843</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,72.00,240.15,486.00,178.68"><head>Table 4 :</head><label>4</label><figDesc>of table 4. Column 4 shows the overall precision with the French, German, and Italian stemmers. Column 5 shows the improvement in precision which can be attributed to the stemmers. Results of Stemming Experiments</figDesc><table coords="5,137.88,274.83,354.36,122.16"><row><cell>RUN ID BKMUEAA1 multilingual TASK BKMUEAA2 multilingual BKMUGAA3 multilingual BKMUGAM1 multilingual BKMOFFA2 monolingual BKMOGGA1 monolingual BKMOGGM1 monolingual BKMOIIA3 monolingual</cell><cell>RESULTS OFFICIAL RESULTS Change (unstemmed) (stemmed) Change 0.2335 0.2502 7.15pct 0.2464 0.2626 6.57pct 0.2524 0.2654 5.15pct 0.3749 0.3903 4.10pct 0.3827 0.4085 6.74pct 0.3113 0.3215 3.27pct 0.4481 0.4968 10.86pct 0.4054 0.4601 13.49pct</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,153.36,89.19,323.64,228.12"><head>Table 5 :</head><label>5</label><figDesc>Results of four o cial GIRT English-German runs.</figDesc><table coords="8,153.36,89.19,323.64,206.28"><row><cell>ID Retrieved Relevant Rel. Ret. at 0.00 at 0.10 at 0.20 at 0.30 at 0.40 at 0.50 at 0.60 at 0.70 at 0.80 at 0.90 at 1.00 Brk. Prec. Med. Prec.</cell><cell>BKGREGA1 BKGREGA2 BKGREGA3 BKGREGA4 23000 23000 23000 23000 1193 1193 1193 1193 901 772 563 827 0.7013 0.5459 0.6039 0.6139 0.5610 0.4436 0.3662 0.4482 0.4585 0.4172 0.2881 0.3583 0.4203 0.3576 0.2633 0.3292 0.3774 0.3165 0.2486 0.2465 0.3454 0.2856 0.2266 0.2004 0.2938 0.2548 0.1841 0.1611 0.2025 0.1816 0.1107 0.1477 0.1493 0.1439 0.0663 0.1252 0.0836 0.0829 0.0575 0.0612 0.0046 0.0075 0.0078 0.0003 0.3119 0.2657 0.2035 0.2299 0.1938 0.1938 0.1938 0.1938</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,87.48,566.43,470.76,13.80;8,87.48,578.43,470.52,13.80;8,87.48,590.43,444.48,13.80" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,218.88,566.43,339.36,13.80;8,87.48,578.43,156.34,13.80">Applying text categorization to vocabulary enhancement for japanese-english cross-language information retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,367.56,578.58,190.44,13.60;8,87.48,590.58,258.28,13.60">The Seventh Machine Translation Summit, Workshop on MT for Cross-language Information Retrieval</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Annandiou</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-09">September 1999</date>
			<biblScope unit="page" from="35" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,87.48,610.35,470.52,13.80;8,87.48,622.23,470.76,13.80;8,87.48,634.23,54.00,13.80" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,225.36,610.35,332.64,13.80;8,87.48,622.23,75.15,13.80">Full text retrieval based on probabilistic equations with coe cients tted by logistic regression</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,281.64,622.38,213.33,13.60">The Second Text REtrieval Conference (TREC-2)</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1994-03">March 1 9 9 4</date>
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,87.48,654.15,470.52,13.80;8,87.48,666.15,470.52,13.80;8,87.48,678.03,470.76,13.80;8,87.48,690.03,22.92,13.80" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,283.68,654.15,218.05,13.80">Chinese text retrieval without using a dictionary</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Meggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,362.16,666.30,195.84,13.60;8,87.48,678.18,348.88,13.60">Proceedings of the 20th Annual International ACM SIGIR Conference o n R esearch and Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Desai Narasimhalu Nicholas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Belkin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Peter</forename><surname>Willett</surname></persName>
		</editor>
		<meeting>the 20th Annual International ACM SIGIR Conference o n R esearch and Development in Information Retrieval<address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.48,19.23,470.64,13.80;9,87.48,31.23,470.40,13.80;9,87.48,43.23,147.12,13.80" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,194.40,19.23,281.98,13.80">Phrase discovery for english and cross-language retrieval at trec-6</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,212.28,31.38,205.99,13.60">The Sixth Text REtrieval Conference (TREC-6)</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ellen</forename><surname>Voorhees</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1998-08">August 1998</date>
			<biblScope unit="page" from="637" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.48,63.15,470.52,13.80;9,87.48,75.03,470.40,13.80;9,87.48,87.03,192.24,13.80" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,193.08,63.15,364.92,13.80;9,87.48,75.03,61.79,13.80">English-german cross-language retrieval for the girt collection { exploiting a multilingual thesaurus</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,273.84,75.18,214.15,13.60">The Eighth Text REtrieval Conference (TREC-8)</title>
		<editor>
			<persName><forename type="first">Ellen</forename><surname>Voorhees</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1999-11">November 1999</date>
			<biblScope unit="page" from="219" to="234" />
		</imprint>
	</monogr>
	<note>draft notebook proceedings</note>
</biblStruct>

<biblStruct coords="9,87.48,106.95,470.64,13.80;9,87.48,118.83,470.52,13.80;9,87.48,130.83,263.52,13.80" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,306.60,106.95,251.52,13.80;9,87.48,118.83,25.51,13.80">Berkeley chinese information retrieval at trec-5: Technical report</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Meggs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,321.12,118.98,204.19,13.60">The Fifth Text REtrieval Conference (TREC-5)</title>
		<title level="s" coord="9,534.00,118.98,24.00,13.60;9,87.48,130.98,81.48,13.60">NIST Special Publication</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ellen</forename><surname>Voorhees</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1996">November1996</date>
			<biblScope unit="page" from="191" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.48,150.75,470.64,13.80;9,87.48,162.75,470.40,13.80;9,87.48,174.63,279.96,13.80" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,309.12,150.75,249.00,13.80;9,87.48,162.75,90.70,13.80">Comparing multiple methods for japanese and japaneseenglish text retrieval</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kishida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,285.84,162.90,272.04,13.60;9,87.48,174.78,75.28,13.60">The First NTCIR Workshop on Japanese Text Retrieval and Term Recognition</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Kando</surname></persName>
		</editor>
		<meeting><address><addrLine>Tokoyo Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">September1999</date>
			<biblScope unit="page" from="49" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.48,194.55,526.20,13.80;9,87.48,206.55,22.92,13.80" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="9,128.04,194.70,216.83,13.60">The World of Multilingual Environmental Thesauri</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Purat</surname></persName>
		</author>
		<ptr target="www.sims.berkeley.edu/research/metadata/papers/purat98" />
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.48,226.47,470.64,13.80;9,87.48,238.47,470.64,13.80;9,87.48,250.50,470.40,13.60;9,87.48,262.35,161.16,13.80" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,264.84,226.47,288.56,13.80">Cross-language information retrieval with the umls metathesaurus</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>D E I C Hmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,425.40,238.62,132.72,13.60;9,87.48,250.50,414.16,13.60">Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">W B</forename><surname>Croft</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Mo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C J</forename><surname>Van Rijsbergen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Wilkinson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Zobel</surname></persName>
		</editor>
		<meeting>the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-08">August 1998</date>
			<biblScope unit="page" from="72" to="80" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
