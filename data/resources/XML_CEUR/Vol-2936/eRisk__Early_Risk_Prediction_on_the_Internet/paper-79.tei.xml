<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,369.98,15.42;1,89.29,106.66,386.97,15.42;1,89.29,128.58,115.77,15.43">uOttawa at eRisk 2021: Automatic Filling of the Beck&apos;s Depression Inventory Questionnaire using Deep Learning</title>
				<funder>
					<orgName type="full">Natural Science and Engineering Council of Canada (NSERC)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,89.29,156.89,64.07,11.96"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
							<email>diana.inkpen@uottawa.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">University of Ottawa</orgName>
								<address>
									<addrLine>800 King Edward Avenue</addrLine>
									<postCode>K1N 6N5</postCode>
									<settlement>Ottawa</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,164.68,156.89,52.60,11.96"><forename type="first">Ruba</forename><surname>Skaik</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">University of Ottawa</orgName>
								<address>
									<addrLine>800 King Edward Avenue</addrLine>
									<postCode>K1N 6N5</postCode>
									<settlement>Ottawa</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,228.23,156.89,97.55,11.96"><forename type="first">Prasadith</forename><surname>Buddhitha</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">University of Ottawa</orgName>
								<address>
									<addrLine>800 King Edward Avenue</addrLine>
									<postCode>K1N 6N5</postCode>
									<settlement>Ottawa</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,337.16,156.89,71.23,11.96"><forename type="first">Dimo</forename><surname>Angelov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">University of Ottawa</orgName>
								<address>
									<addrLine>800 King Edward Avenue</addrLine>
									<postCode>K1N 6N5</postCode>
									<settlement>Ottawa</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,170.84,149.54,11.96"><forename type="first">Maxwell</forename><forename type="middle">Thomas</forename><surname>Fredenburgh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">University of Ottawa</orgName>
								<address>
									<addrLine>800 King Edward Avenue</addrLine>
									<postCode>K1N 6N5</postCode>
									<settlement>Ottawa</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,369.98,15.42;1,89.29,106.66,386.97,15.42;1,89.29,128.58,115.77,15.43">uOttawa at eRisk 2021: Automatic Filling of the Beck&apos;s Depression Inventory Questionnaire using Deep Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FB2691064AA1EAB75732979D5B14236C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>depression detection</term>
					<term>social media</term>
					<term>deep learning</term>
					<term>natural language processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the University of Ottawa's participation in Task 3 of the eRisk 2021 shared task at CLEF 2021. We think that this task is important because it allows detecting the level of depression for social media users as often as needed, without having to ask them to spend their time manually filling in the Beck's Depression Inventory questionnaire. Our methods focus on selecting the relevant posts for each question of the questionnaire and using pre-trained deep learning models with or without fine-tuning to make predictions for unseen users.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper described the uOttawa team's participation in Task 3 of the eRisk 2021 <ref type="bibr" coords="1,460.31,404.49,12.96,10.91" target="#b0">[1]</ref> shared task. The task is a continuation of Task 3 at eRisk 2019 and Task 2 at eRisk 2020, and the goal is to automatically estimate the level of depression from a user's social media postings.</p><p>We believe that this task can be very useful for monitoring users in special situations, without having to ask them to manually provide information. For example, a psychologist could postmonitor their patients after their recovery, with their consent. Another example is for people who need to spend long periods in conditions of isolation, such as arctic researchers or astronauts during long space flights (in the future).</p><p>We employed deep learning techniques to classify information extracted from the postings. Our focus was on selecting relevant posts for each type of information. Then we employed zero-shot learning via pre-trained models or we trained models based on sequence-to-sequence models for Question Answering (QA).</p><p>The rest of the paper is organized as follows. Section 2 gives more details about the task and shows statistics about the class distribution in the training data. Section 3 describes the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task description</head><p>For each user, the participants in the shared task <ref type="foot" coords="2,310.68,157.39,3.71,7.97" target="#foot_0">1</ref> were given the postings of that user for a certain time period. The task was to automatically fill in a standard depression questionnaire: the Beck's Depression Inventory (BDI). The questionnaire has 21 questions which assess the presence of feelings like sadness, pessimism, loss of energy, etc. Each question has 4 answers (0,1,2,3). Therefore the task becomes a classification task into 4 classes, for each question. The answers are targeting changes in a user's life.</p><p>There is variation in the expected answers. Two of the questions have seven answers instead of four, namely: 0, 1a, 1b, 2a, 2b, 3a, 3b. Therefore these two classifiers will need to classify into 7 classes. The two questions are question 16 (about sleep patterns) and question 18 (about appetite).</p><p>The training dataset provided by the task organizers for Task 3 is composed of 43,514 Reddit posts and comments written by 90 users who have answered 21-questions of the BDI questionnaire during the past two years. The test dataset consists of 19,803 posts and comments written by 80 users.pub. More details about the construction of the dataset are available in <ref type="bibr" coords="2,458.30,335.28,11.43,10.91" target="#b1">[2]</ref>.</p><p>In table <ref type="table" coords="2,137.25,348.83,3.81,10.91" target="#tab_0">1</ref>, we show statistics about the class distribution in the training data. For the two questions with 7 answers, the answers 1a, 1b were considered equivalent when performing these counts. They were counted as class 1. We did the same for for classes 2a, 2b and 3a, 3b, because they contribute the same number of points when assessing the depression level of a user. We can see for the statistics that class 0 is the biggest (35%), while class 1 is not far behind (33%). Choosing class 0 as the default class would therefore not necessary be a great choice for the classifiers. Class 2 is 19% and class 3 is the smallest (13%). If we look at the depression levels, 15% of the users have minimal depression, 30% mild depression, 24% moderate depression, and 30% severe depression. So, we can say that the data is not very imbalanced.</p><p>The evaluation measures used in the shared task are: the Average Hit Rate (AHR), the Average Closeness Rate (ACR), the Average Difference between Overall Depression Levels (ADODL), and the Depression Category Hit Rate (DCHR).</p><p>The Average Hit Rate (AHR) is the Hit Rate averaged over all the users. It is a strict measure that computes the ratio of cases where the automatically filled questionnaire has exactly the same answer as the real questionnaire.</p><p>The Average Closeness Rate (ACR) is the Closeness Rate averaged over all users. The Closeness Rate measure takes into account that the answers of the depression questionnaire represent an ordinal scale, and not only separate options. For example, if the user answered "0", a system whose answer is "3" should be penalised more than a system whose answer is "1".</p><p>The Average Difference between Overall Depression Levels (ADODL) measures the overall depression level estimated taking all responses as a sum of all the answers, looking for the depression level as a whole instead of some differences on each questionnaire answer prediction. It computes the difference between the overall depression level for the real and automated questionnaire. Then, the absolute difference (ad) between the real and the automated score is computed. Depression levels are integers between 0 and 63. The measure is normalised to be between 0 and 1 with the formula (63 -ad)/63. Then the average over all the users is computed.</p><p>The Depression Category Hit Rate (DCHR) measures the correctness of the estimation achieved over all users according to the well-established depression categories in psychology, with the following four categories of depression: minimal depression (depression levels 0-9) mild depression (depression levels 10-18) moderate depression (depression levels 19-29) severe depression (depression levels 30-63) See the task overview paper for more details about the evaluation measures <ref type="bibr" coords="3,439.55,658.65,11.43,10.91" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>Our methods contained three steps: pre-processing the data, selecting relevant posts, and the classification to predict the answers for each question, based on zero-short learning or supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preprocessing</head><p>Each post was preprocessed as follows: the title of the post and the post text were concatenated. The contractions were expanded, and words between brackets were removed. Then punctuation and special characters were cleaned, and all the text was lowercased. The posts related to the forum monitoring (namely posts that notified users that they "broke the rules") were removed. Finally, all posts that had less than four characters were removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Selecting posts</head><p>For each question, we focused on selecting a subset of posts for each user. The goal was to keep only posts that are relevant to each question, in order to increase the probability of finding an answer about the topic of the question. We call this process filtering of the posts. We also experimented with keeping all posts, with the caveat that training deep learning models on long texts (the concatenation of all posts) is slow or sometimes problematic for BERT-like models.</p><p>We employed two methods for filtering posts. The first method is similarity-based. The similarity-based method utilized pre-trained sentence transformer models based on BERT or RoBERTa to embed each post and all the BDI answers. Then, the relatedness of each post to each answer of BDI answers is measured by computing the cosine distance between the post_embedding (p) and the answer_embedding (a), as shown in the Equation <ref type="formula" coords="4,438.57,413.97,5.07,10.91" target="#formula_1">1</ref><ref type="foot" coords="4,443.64,412.22,3.71,7.97" target="#foot_1">2</ref> .</p><formula xml:id="formula_0" coords="4,195.07,434.42,68.07,27.09">1 - 𝑝 • 𝑎 ‖𝑝‖ 2 •‖𝑎‖ 2</formula><p>where ‖𝑥‖ 2 is the 2-norm of x</p><p>If the similarity value of a post with all questionnaires' answers is less than 𝜃 1 , then the post is excluded, since it means that the post is not related to any of the BDI questionnaire questions. In addition, if the difference between the maximum similarity and the minimum similarity is less than 𝜃 2 , then the post is excluded because it is considered a general post and not assisting in answering any of the BDI questionnaire questions in specific. It should be noted that not all the categories are discussed in the posts. Some categories appear more often than others. If there are very limited posts for a user, we consider all the posts of that user for the learning process (all the posts are included in the top n posts). Table <ref type="table" coords="4,360.18,561.74,5.17,10.91" target="#tab_0">1</ref> shows the number of posts for each BDI question as per RoBERTa similarity with 𝜃 1 = 0.6. It shows that the posts related to eating and sleeping habits are rare, and posts about guilt and punishment feelings are the most frequent.</p><p>The second method is topic-based. We leveraged topic modeling to help identify relevant posts for each question. We used top2vec <ref type="bibr" coords="4,287.99,629.49,13.00,10.91" target="#b2">[3]</ref> to divide all posts into topics that were then used to find relevant posts for each question. The top2vec algorithm automatically finds the number of topics in a corpus. It finds topic vectors from jointly embedded document and word vectors of a corpus. The main idea behind the algorithm is that it finds dense areas of documents in the embedding space. The assumption of the algorithm is that the dense area of document vectors represents an area of highly similar documents which are representative of a topic. A topic vector is calculated from each dense area of documents as the centroid of those document vectors. The topics are then described with the nearest word vectors to the topic vector. At the end, each document is assigned to its nearest topic vector, allowing for the size of each topic to be calculated. As an example, for the first question, we searched for the topics related to 'sad' and 'feel', using two relevant topics computed by top2vec, containing the following words: cry, crying, sad, cried, feeling, upset, emotional, . . . smile, eyes, myself, cry, face, laugh, beautiful, sad, beauty, wish, forgive, . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Zero-Shot Learning</head><p>The current dataset set is relatively small for training 21 classifiers for each question of the BDI questionnaire, especially if we considered only the posts that are related to the BDI question. In addition, it is difficult to relate which posts are answering which question. For that reason, we decided to utilize transfer learning, more precisely zero-shot learning. Zero shot learning is a fast emerging field in machine learning, with a broad range of computer vision and natural language processing applications <ref type="bibr" coords="5,239.27,546.10,11.43,10.91" target="#b3">[4]</ref>. We used a simple technique that relies on estimating the semantic similarity between the BDI answer and each post of the user. We did make use of the training labels to choose the thresholds 𝜃 1 and 𝜃 2 . Therefore the method is not totally unsupervised. Fixed thresholds could be chosen if preferred.</p><p>We used language models based on Sentence Transformers for deep contextual post representations: Sentence-BERT (SBERT) and Sentence-RoBERTa (SRoBERTa) <ref type="bibr" coords="5,404.92,627.40,11.54,10.91" target="#b4">[5]</ref>. SBERT/SRoBERTa are modifications of the pretrained BERT/RoBERTa network that employs siamese and triplet network architectures <ref type="bibr" coords="5,190.05,654.50,12.84,10.91" target="#b5">[6]</ref> as illustrated in Figure <ref type="figure" coords="5,308.15,654.50,3.74,10.91" target="#fig_1">2</ref>. The following runs from table 2 are based on zero-shot learning: uOttawa1_sim_BERT_base+, uOttawa3_sim_BERT_large and uOttawa5_sim_RoBERTa+. uOttawa1_sim_BERT_base+ uses 'bert-base-nli-mean-tokens' model. The model starts by getting the word embeddings for each word in a given sentence using BERT base model, then calculates the average of the word embeddings to produce SBERT embeddings. The weights are updated using siamese and triplet networks to construct semantically relevant sentence embeddings that can be compared using cosine-similarity. The base model contains 12-layers, 768-hidden layers, 12-heads, 110M parameters <ref type="bibr" coords="6,141.65,373.67,11.40,10.91" target="#b6">[7]</ref>. Whereas, uOttawa3_sim_BERT_large uses the large BERT model as the initial word embedding for each word in the sentence. It contains 24-layers, 1024-hidden layers, 16-heads, 340M parameters <ref type="bibr" coords="6,212.28,400.77,11.37,10.91" target="#b6">[7]</ref>. Similarly, uOttawa5_sim_RoBERTa+ is based on 'roberta-basenli-mean-tokens' model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">BERT QA</head><p>One of the key limitations that BERT-based models <ref type="bibr" coords="6,317.19,463.65,12.75,10.91" target="#b6">[7]</ref> face is the length of the input sequence. Due to the full attention mechanism, the computational memory requirement and the model training time is quadratic. In other words, if the sequence length is 𝑛, the memory requirement when training the model will be 𝑛 2 . Due to this reason, the authors of transformers architecture <ref type="bibr" coords="6,89.29,517.84,12.69,10.91" target="#b7">[8]</ref> and similar architectures, such as BERT, have limited the sequence length to 512 tokens, which includes the special tokens [CLS] (classification embedding) and [SEP] (sentence separator). To overcome this sequence length limitation, many researchers have introduced different architectures such as Longformer <ref type="bibr" coords="6,234.58,558.49,11.28,10.91" target="#b8">[9]</ref>, Reformer <ref type="bibr" coords="6,294.39,558.49,17.75,10.91" target="#b9">[10]</ref> and BigBird <ref type="bibr" coords="6,366.72,558.49,16.09,10.91" target="#b10">[11]</ref>. Even though we conducted several experiments in the form of multiple-choice question answering using the Longformer architecture, we still found it resource-intensive, especially when the sequence length increases. To obtain the results for the "uOttawa4_Ensemble_BERT_QA" run (see 2) and "uOttawa6" (see table4), we conducted several preliminary experiments using the BigBird architecture in the form of multiple-choice question answering. For the experiments in this section, we did not filter the posts as mentioned in section 3.2 and tried to identify the impact of using the content published by users soon before submitting the BDI questionnaire (i.e., results under "uOttawa4_Ensemble_BERT_QA"). In addition, we also conducted several experiments by using all the earliest Reddit posts from the collection of posts of users (for the "uOttawa6" run). After pre-processing the data, we concatenated all the posts in the order of posts' date and time. The concatenated posts were then tokenized using the spaCy tokenizer, so that a limited number of tokens can be extracted for training purposes. Due to the resource intensiveness of the transformer-based models, we selected only 512 tokens from each user. It is important to note that these 512 tokens do not reflect the number of tokens that will get generated when using the BigBirdTokenizer <ref type="bibr" coords="7,168.25,168.26,17.76,10.91" target="#b11">[12]</ref> to tokenize the input data. We prepared our input, per user, per question, in the following format, which was then tokenized using the BigBirdTokenizer.</p><p>[</p><formula xml:id="formula_2" coords="7,153.23,216.86,292.35,35.64">[[𝐶𝐿𝑆] 𝑇 1 𝑇 2 𝑇 3 . . . 𝑇 510 𝑇 511 𝑇 512 [𝑆𝐸𝑃 ] 𝑄 1 𝐶 (𝑞 1 ) 1 [𝑆𝐸𝑃 ]] . . . [[𝐶𝐿𝑆] 𝑇 1 𝑇 2 𝑇 3 . . . 𝑇 510 𝑇 511 𝑇 512 [𝑆𝐸𝑃 ] 𝑄 1 𝐶 (𝑞 1 ) 4 [𝑆𝐸𝑃 ]]]</formula><p>Here 𝑇 1 to 𝑇 512 indicates the sequence of tokens (512 for our experiments) extracted from the concatenated posts of a user. 𝑄 1 states the question, which will be from 1-21 that also indicates the number of classifiers trained. 𝐶</p><formula xml:id="formula_3" coords="7,243.79,289.99,15.30,15.86">(𝑞 1 ) 1</formula><p>indicates the first choice of the first question, where the number of choices can vary between questions. According to the BDI questionnaire, except for questions 16 and 18, the questions had four choices, while questions 16 and 18 had seven choices. Based on the number of choices, we changed the number of class labels accordingly. We used the huggingface implementation of the BigBird model <ref type="bibr" coords="7,383.80,347.20,18.06,10.91" target="#b11">[12]</ref> to train the classifiers. Given the "content", "question" and the "choice" as mentioned above, we used the pre-trained BigBird model and fine-tuned it on our task as a multi-class classifier predicting one out of four or seven choices, based on the question. Unlike the BERT model, which uses full attention, the BigBird model uses random, window, and global attention to reduce the quadratic impact on training time and computational memory. When training, we used the Adam optimizer and trained the model for ten epochs with early stopping. We created five stratified shuffle splits <ref type="bibr" coords="7,116.14,442.04,18.07,10.91" target="#b12">[13]</ref> by allocating 80% for training and 20% for validation. For each stratified split, we created separate models and saved the ones that produced the best results on the validation data. During inference, the saved models were used on the test data to generate predictions, and the softmax outputs were aggregated to create an ensembled output. The stratification was based on the level of depression (i.e., minimal, mild, moderate, and severe depression) calculated based on the answer provided for each question.</p><p>It is important to note that even though the level of depression was used as the stratification strategy, the answers provided by the participants within the same depression group were not consistent. Due to resource intensiveness and training time, we trained the model with a batch size of 1. Given the time constraints, we fine-tuned only a few of the hyperparameters of the BigBird model. We set the block size of the model to be 64 and the number of random blocks to be 5. The block size specifies the block size to be used with random, global and window attention, and the number of random blocks specifies how many random blocks to be used with the given block size. We could not use a sequence length larger or equal to 1024 due to computational limitations. Though using sparse attention could be more effective if used with sequences longer than 1024 tokens <ref type="bibr" coords="7,268.78,645.28,16.22,10.91" target="#b11">[12]</ref>. Given these constraints, we will conduct further research in future work to identify more optimal hyperparameters to obtain better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.">Universal Sentence Encoder QA</head><p>Each user has unique Reddit posts which may be on a variety of different topics. In order to train a model to predict the answer of a user for a given BDI question, we would ideally only use the posts that are relevant to answering the question. However given the limited size of the dataset it would be difficult to train a model to learn which posts are relevant to a question and the user's response. There is the additional challenge that some users may have a very large quantity of posts that may not be able to be processed all at once by deep learning models due to computational constraints.</p><p>In order to overcome these challenges, we leverage transfer learning by using the universal sentence encoder <ref type="bibr" coords="8,169.08,215.93,17.91,10.91" target="#b13">[14]</ref> optimized for question answering (USE-QA). We use it to find the most semantically-relevant posts of a user for each BDI question. We accomplish this by using the response and question encoders of the USE-QA. We encode each of a user's posts with the response encoder. Then for each question, we create a question. For example for question 1, which is about sadness, we create "How sad do I feel?", for question 2, which is about pessimism, we create "How discouraged do I feel?". We then embed each one of these questions with the USE-QA question encoder. The question and response embeddings can then be compared using cosine similarity. In order to select the most relevant posts of a user to each question, we took their top 10 most similar posts to each question. Once we have identified the most relevant posts of a user to each BDI question we concatenate them together as they will be used to train a neural network for each BDI question which predicts the user's responses.</p><p>The neural architecture we use is motivated by the deep averaging network <ref type="bibr" coords="8,450.50,364.97,18.06,10.91" target="#b14">[15]</ref> and the need for transfer learning due to the small data size. For the embedding layer, we use USE to embed the concatenated top 10 most relevant posts for a BDI question. This is followed by three fully-connected layers with dropout and a final dense layer of size equivalent to the number of responses for the given BDI question. We use a 90%/%10 training and validation split on the provided training data (90 users) and train the models for 10 epochs. At prediction time for the test data (80 users) the USE-QA method was used to find the top 10 most relevant posts for each BDI question for each user, then those were concatenated and put through the trained neural network to predict user responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4.">Hierarchical Attention Network</head><p>For this method, we also trained 21 classifiers for each of the BDI questions, but we adopted a hierarchical attention network (HAN) for document classification inspired by <ref type="bibr" coords="8,435.82,536.24,16.24,10.91" target="#b15">[16]</ref>. It employs two levels of attention mechanisms at the word and sentence levels as described in Appendix A. A word attention mechanism is utilized to identify keywords then aggregate them to create a sentence vector. Then a sentence attention mechanism is used to emphasize the importance of a sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and Discussion</head><p>Table <ref type="table" coords="8,115.04,649.06,4.97,10.91" target="#tab_1">2</ref> shows the results of the 5 runs we submitted to the shared task, also available in the task overview paper <ref type="bibr" coords="8,161.90,662.61,11.51,10.91" target="#b0">[1]</ref>. Here is a description of the methods we used to obtained the predictions for the test users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>uOttawa1_sim_BERT_base+</head><p>This run used SBERT, which is pre-trained on a natural language inference (NLI) dataset in addition to BERT's Wikipedia pre-training. As described in section 3.3.1, after calculating the cosine-similarity of each post against the BDI questionnaire answers, we filtered the unrelated and general posts based on 𝜃 1 and 𝜃 2 . In this run, we kept all the post, thus we set 𝜃 1 = 0 and we removed general posts by setting 𝜃 2 = 0.1. Then, each post was assigned to the maximum similarity value of the BDI answer, as illustrated in table <ref type="table" coords="9,357.93,338.18,3.81,10.91">3</ref>. Finally, the answers for each user were aggregated using voting for the most frequent answer, for each question of the BDI questionnaire. This is our best submitted run in terms of performance based on the first three evaluation measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>Cosine-similarity with the post "this is so sad i cry every time" 0 i do not feel sad 0.0902 1 i feel sad much of the time 0.8822 2 i am sad all the time 0.9353 3 i am so sad or unhappy that i can't stand it 0.9119 uOttawa2_top2vec_USE+ This used the method described in section 3.3.3, based on the Universal Sentence Encoder with a QA training architecture. Note that top2vec was not used in this method (we put the top2vec in the run's name by mistake).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>uOttawa3_sim_BERT_large+</head><p>This run is based on zero-shot learning using the model 'bert-large-nli-stsb-mean-tokens'. This model is considered suitable for semantic textual similarity as it was fine-tuned on the NLI dataset, then on the sentence similarity STS benchmark train set. In this run, we kept all the posts as the uOttawa1_sim_BERT_base+ run, thus we set 𝜃 1 = 0 but we changed 𝜃 2 to 0.5 to eliminate all relatively ambiguous posts. This run is based on a BigBird model using 512 tokens from the end of the concatenated and tokenized Reddit posts using the spaCy tokenizer (i.e., before tokenizing the sequence using the BigBirdTokenizer). The training uses the QA models described in section 3.3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>uOttawa5_sim_RoBERTa+</head><p>This run is based on zero-shot learning (section 3.3.1) with the use of pre-trained 'roberta-basenli-mean-tokens' model. In this run, we set 𝜃 1 = 0.25, 𝜃 2 = 0, then we performed extra filtering of the posts, by removing any post with a maximum similarity to the BDI answer that is less than 0.6. This is our best submitted run in terms of the forth evaluation measure, the level of depression. Table <ref type="table" coords="10,128.43,373.90,5.17,10.91" target="#tab_2">4</ref> shows results for four more runs, submitted unofficially since only 5 runs were allowed for the official submission. They were kindly evaluated by the task organizers. Here is the description of the methods used to produce these results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>uOttawa6</head><p>Then run "uOttawa6" is based on the architecture described in section 3.3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>uOttawa(7,8,9)_Sim_HAN_cce+</head><p>These three unofficial runs employed post filtering (based on similarity or on top2vec) and deep learning of the questionnaire answers based on hierarchical attention network. The posts filtering was done by either setting 𝜃 1 to 0.5 and, if there are no posts that represent the category of the questionnaire, the posts filtered by top2vec were added. Or, by selecting the top n most-similar posts for each topic, then combining all the posts of the user as one document. We set n to 10 or 20. Table <ref type="table" coords="10,192.62,568.19,5.07,10.91" target="#tab_3">5</ref> shows the number of posts selected for each run.</p><p>We note that adding the supervised deep learning step (HAN), as described in section 3.3.4, helped improve the results (especially for the uOttawa7_Sim_HAN_cce_top_20_20 run). Table <ref type="table" coords="10,89.04,608.84,5.00,10.91" target="#tab_5">7</ref> form the Appendix B shows results for each question for three runs. The questions 16 and 18, about changes in eating and sleeping patterns, respectively, were the most difficult to answer.</p><p>Our unofficially submitted runs obtained better performance for the first measure (see tables 2 and 4), the correctness of the predicted answers (AHR 32.62% for uOttawa7_Sim_HAN_cce_top_20_20 versus 28.39% for uOttawa1_sim_BERT_base+), but not for the other measures. Table <ref type="table" coords="11,457.26,199.02,4.97,10.91" target="#tab_4">6</ref> compares our results with the best results from the shared task, for the four measures. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>This paper presented several methods for the task of filling the BDI questionnaire. We showed that filtering posts by their relevance are beneficial in training classifiers related to answering different questions. We also showed that zero-shot learning with pre-trained models could be utilized for predicting the answers, with similar performance as QA sequence-to-sequence learning. In addition, deep learning models such as HAN on top of the post filtering led to our best results.</p><p>In future work, we plan to investigate possible ways to improve the performance. One direction is to further pre-train generic sentence similarity measures on large amounts of postings about mental health issues. Another direction is to investigate more ways to use top2vec to detect the most relevant posts for each question and to test better linguistic analysis methods for "change" detection to chose the correct answer to each question. Better ways to implement zero-shot learning can also be investigated.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results per Question</head><p>Table <ref type="table" coords="15,115.79,111.28,5.07,10.91" target="#tab_5">7</ref> shows the results for each question for three runs. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,89.29,234.97,300.14,9.43;5,89.29,84.19,416.71,126.26"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Number of posts per BDI-question based on RoBERTa (𝜃 1 = 0.6)</figDesc><graphic coords="5,89.29,84.19,416.71,126.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,89.29,241.21,227.75,8.93;6,192.22,84.19,208.35,144.46"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: SBERT architecture. (https://www.sbert.net/).</figDesc><graphic coords="6,192.22,84.19,208.35,144.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="14,89.29,378.52,193.42,8.93;14,171.38,87.37,250.02,278.58"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Hierarchical Attention Network [16].</figDesc><graphic coords="14,171.38,87.37,250.02,278.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="14,89.29,494.89,278.22,8.93;14,192.22,407.76,208.35,74.56"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Word Encoder for Question 2 (Sim_HAN_cce_top_20_20).</figDesc><graphic coords="14,192.22,407.76,208.35,74.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="14,89.29,659.68,328.47,8.93;14,192.22,524.13,208.34,122.99"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Attention model summary for Question 2 (Sim_HAN_cce_top_20_20).</figDesc><graphic coords="14,192.22,524.13,208.34,122.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.99,90.49,352.58,380.08"><head>Table 1</head><label>1</label><figDesc>Statistics about the class distribution in the training data</figDesc><table coords="3,89.02,128.61,352.55,341.96"><row><cell>90 users</cell></row><row><cell>Question : 0 : 27 (30%) 1 : 47 (52%) 2 : 11 (12%) 3 : 5 (5%)</cell></row><row><cell>Question : 0 : 22 (24%) 1 : 34 (37%) 2 : 20 (22%) 3 : 14 (15%)</cell></row><row><cell>Question : 0 : 22 (24%) 1 : 35 (38%) 2 : 18 (20%) 3 : 15 (16%)</cell></row><row><cell>Question : 0 : 28 (31%) 1 : 33 (36%) 2 : 23 (25%) 3 : 6 (6%)</cell></row><row><cell>Question : 0 : 34 (37%) 1 : 32 (35%) 2 : 12 (13%) 3 : 12 (13%)</cell></row><row><cell>Question : 0 : 60 (66%) 1 : 13 (14%) 2 : 11 (12%) 3 : 6 (6%)</cell></row><row><cell>Question : 0 : 28 (31%) 1 : 17 (18%) 2 : 23 (25%) 3 : 22 (24%)</cell></row><row><cell>Question : 0 : 28 (31%) 1 : 27 (30%) 2 : 23 (25%) 3 : 12 (13%)</cell></row><row><cell>Question : 0 : 41 (45%) 1 : 37 (41%) 2 : 7 (7%) 3 : 5 (5%)</cell></row><row><cell>Question : 0 : 42 (46%) 1 : 23 (25%) 2 : 8 (8%) 3 : 17 (18%)</cell></row><row><cell>Question : 0 : 37 (41%) 1 : 31 (34%) 2 : 14 (15%) 3 : 8 (8%)</cell></row><row><cell>Question : 0 : 28 (31%) 1 : 32 (35%) 2 : 8 (8%) 3 : 22 (24%)</cell></row><row><cell>Question : 0 : 38 (42%) 1 : 21 (23%) 2 : 16 (17%) 3 : 15 (16%)</cell></row><row><cell>Question : 0 : 38 (42%) 1 : 21 (23%) 2 : 20 (22%) 3 : 11 (12%)</cell></row><row><cell>Question : 0 : 17 (18%) 1 : 32 (35%) 2 : 28 (31%) 3 : 13 (14%)</cell></row><row><cell>Question : 0 : 17 (18%) 1 : 36 (40%) 2 : 24 (26%) 3 : 13 (14%)</cell></row><row><cell>Question : 0 : 38 (42%) 1 : 31 (34%) 2 : 16 (17%) 3 : 5 (5%)</cell></row><row><cell>Question : 0 : 32 (35%) 1 : 30 (33%) 2 : 15 (16%) 3 : 13 (14%)</cell></row><row><cell>Question : 0 : 29 (32%) 1 : 25 (27%) 2 : 25 (27%) 3 : 11 (12%)</cell></row><row><cell>Question : 0 : 21 (23%) 1 : 34 (37%) 2 : 21 (23%) 3 : 14 (15%)</cell></row><row><cell>Question : 0 : 51 (56%) 1 : 18 (20%) 2 : 11 (12%) 3 : 10 (11%)</cell></row><row><cell>Total: : 678 (35%) 1 : 609 (32%) 2 : 354 (19%) 3 : 249 (13%)</cell></row><row><cell>Minimal depression 14 (15.5%)</cell></row><row><cell>Mild depression 27 (30%)</cell></row><row><cell>Moderate depression 22 (24.5%)</cell></row><row><cell>Severe depression 27 (30%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,88.99,90.49,359.00,103.52"><head>Table 2</head><label>2</label><figDesc>Results for the submitted runs</figDesc><table coords="9,147.29,119.88,300.70,74.13"><row><cell>Run</cell><cell>AHR</cell><cell>ACR</cell><cell cols="2">ADODL DCHR</cell></row><row><cell>uOttawa1_sim_BERT_base+</cell><cell cols="4">28.39% 65.73% 78.91% 25.00%</cell></row><row><cell>uOttawa2_top2vec_USE+</cell><cell cols="2">28.04% 63.00%</cell><cell>77.32%</cell><cell>27.50%</cell></row><row><cell>uOttawa3_sim_BERT_large+</cell><cell cols="2">25.83% 59.68%</cell><cell>71.23%</cell><cell>27.50%</cell></row><row><cell cols="3">uOttawa4_Ensemble_BERT_QA 27.68% 62.08%</cell><cell>76.92%</cell><cell>20.00%</cell></row><row><cell>uOttawa5_sim_RoBERTa+</cell><cell cols="2">26.31% 62.60%</cell><cell cols="2">76.45% 30.00%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,88.99,90.49,369.31,129.26"><head>Table 4</head><label>4</label><figDesc>Results for unofficial runs</figDesc><table coords="10,89.29,119.88,369.01,99.86"><row><cell>Run</cell><cell>AHR</cell><cell>ACR</cell><cell cols="2">ADODL DCHR</cell></row><row><cell>uOttawa6</cell><cell cols="4">29.46% 63.04% 78.31% 25.00%</cell></row><row><cell cols="4">uOttawa7_Sim_HAN_cce_top_20_20 32.62% 65.99% 77.62%</cell><cell>22.50%</cell></row><row><cell>uOttawa8_Sim_HAN_cce_top_10</cell><cell cols="2">30.48% 63.63%</cell><cell>72.88%</cell><cell>22.50%</cell></row><row><cell>uOttawa9_Sim_HAN_cce</cell><cell cols="2">31.73% 64.62%</cell><cell cols="2">75.22% 25.00%</cell></row><row><cell>uOttawa4_Ensemble_BERT_QA</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,88.99,90.49,403.78,81.83"><head>Table 5</head><label>5</label><figDesc>Number of posts included based on the selection criteria</figDesc><table coords="11,102.50,122.10,390.26,50.21"><row><cell>run</cell><cell cols="3">n/𝜃 1 posts train</cell><cell>test post selection method</cell></row><row><cell cols="2">uOttawa7_Sim_HAN_cce_top_20_20 20</cell><cell cols="3">14727 9043 5776 RoBERTa Sim</cell></row><row><cell>uOttawa8_Sim_HAN_cce_top_10</cell><cell>10</cell><cell>9515</cell><cell cols="2">575 3798 RoBERTa Sim</cell></row><row><cell>uOttawa9_Sim_HAN_cce</cell><cell>0.5</cell><cell cols="3">6003 3570 2441 RoBERTa Sim &amp; top2vec</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,88.99,240.46,376.53,93.79"><head>Table 6</head><label>6</label><figDesc>Our results compared to the best results from the shared task</figDesc><table coords="11,129.76,272.08,335.76,62.17"><row><cell></cell><cell>Run</cell><cell cols="3">Rank Our best Best result</cell></row><row><cell>AHR</cell><cell>uOttawa7_Sim_HAN_cce_top_20_20</cell><cell>12</cell><cell>32.62%</cell><cell>35.36%</cell></row><row><cell>ACR</cell><cell>uOttawa7_Sim_HAN_cce_top_20_20</cell><cell>15</cell><cell>65.99%</cell><cell>73.17%</cell></row><row><cell cols="2">ADODL uOttawa1_sim_BERT_base+</cell><cell>7</cell><cell>78.91%</cell><cell>83.59%</cell></row><row><cell>DCHR</cell><cell>uOttawa5_sim_RoBERTa+</cell><cell>5</cell><cell>30.00%</cell><cell>41.25%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="15,88.98,140.29,401.97,297.02"><head>Table 7</head><label>7</label><figDesc>AHR results per category for our best runs</figDesc><table coords="15,101.83,171.90,389.12,265.40"><row><cell>Question BDI Question</cell><cell cols="3">uOttawa1 AHR uOttawa5 AHR uOttawa7 AHR</cell></row><row><cell>Sadness</cell><cell>31.25</cell><cell>22.50</cell><cell>53.75</cell></row><row><cell>Pessimism</cell><cell>36.25</cell><cell>37.50</cell><cell>28.75</cell></row><row><cell>Past failure</cell><cell>32.50</cell><cell>36.25</cell><cell>38.75</cell></row><row><cell>Loss of pleasure</cell><cell>32.50</cell><cell>12.50</cell><cell>38.75</cell></row><row><cell>Guilty feelings</cell><cell>36.25</cell><cell>35.00</cell><cell>41.25</cell></row><row><cell>Punishment feelings</cell><cell>35.00</cell><cell>28.75</cell><cell>42.50</cell></row><row><cell>Self-dislike</cell><cell>17.50</cell><cell>32.50</cell><cell>25.00</cell></row><row><cell>Self-criticalness</cell><cell>22.50</cell><cell>21.25</cell><cell>28.75</cell></row><row><cell>Suicidal thoughts or wishes</cell><cell>46.25</cell><cell>38.75</cell><cell>36.25</cell></row><row><cell>Crying</cell><cell>32.50</cell><cell>31.25</cell><cell>32.50</cell></row><row><cell>Agitation</cell><cell>23.75</cell><cell>27.50</cell><cell>41.25</cell></row><row><cell>Loss of interest</cell><cell>32.50</cell><cell>26.25</cell><cell>27.50</cell></row><row><cell>Indecisiveness</cell><cell>31.25</cell><cell>22.50</cell><cell>23.75</cell></row><row><cell>Worthlessness</cell><cell>27.50</cell><cell>26.25</cell><cell>31.25</cell></row><row><cell>Loss of energy</cell><cell>23.75</cell><cell>25.00</cell><cell>20.00</cell></row><row><cell>Changes in sleeping pattern</cell><cell>12.50</cell><cell>17.50</cell><cell>23.75</cell></row><row><cell>Irritability</cell><cell>31.25</cell><cell>28.75</cell><cell>30.00</cell></row><row><cell>Changes in appetite</cell><cell>12.50</cell><cell>15.00</cell><cell>23.75</cell></row><row><cell>Concentration difficulty</cell><cell>27.50</cell><cell>32.50</cell><cell>22.50</cell></row><row><cell>Tiredness or fatigue</cell><cell>25.00</cell><cell>22.50</cell><cell>36.25</cell></row><row><cell>Loss of interest in sex</cell><cell>26.25</cell><cell>12.50</cell><cell>38.75</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.93,671.03,80.06,8.97"><p>https://erisk.irlab.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,108.93,671.03,308.97,8.97"><p>https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank to the <rs type="funder">Natural Science and Engineering Council of Canada (NSERC)</rs> for supporting our research.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,112.66,111.28,393.61,10.91;12,112.66,124.83,393.33,10.91;12,112.66,138.38,393.33,10.91;12,112.33,151.93,124.16,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,347.69,111.28,158.59,10.91;12,112.66,124.83,112.47,10.91">Overview of eRisk 2021: Early Risk Prediction on the Internet</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Martin-Rodilla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,247.81,124.83,258.17,10.91;12,112.66,138.38,393.33,10.91;12,112.33,151.93,51.66,10.91">Proceedings of the Twelfth International Conference of the CLEF Association (CLEF 2021)</title>
		<meeting>the Twelfth International Conference of the CLEF Association (CLEF 2021)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="12,112.66,165.48,394.52,10.91;12,112.66,179.03,297.08,10.91" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-44564-9_3</idno>
		<title level="m" coord="12,223.34,165.48,279.25,10.91">A Test Collection for Research on Depression and Language Use</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="28" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,192.57,393.74,10.91" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Angelov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.09470</idno>
		<title level="m" coord="12,167.15,192.57,209.29,10.91">Top2Vec: Distributed Representations of Topics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,206.12,394.52,10.91;12,112.66,219.67,271.01,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,286.79,206.12,220.39,10.91;12,112.66,219.67,71.09,10.91">A survey of zero-shot learning: Settings, methods, and applications</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,192.00,219.67,146.88,10.91">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,233.22,395.17,10.91;12,112.66,246.77,393.33,10.91;12,112.66,260.32,314.94,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,231.26,233.22,276.57,10.91;12,112.66,246.77,41.53,10.91">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,183.50,246.77,322.49,10.91;12,112.66,260.32,91.61,10.91">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,273.87,393.33,10.91;12,112.66,287.42,393.32,10.91;12,112.33,300.97,118.56,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,283.76,273.87,222.22,10.91;12,112.66,287.42,62.67,10.91">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,221.85,287.42,284.13,10.91;12,112.33,300.97,30.22,10.91">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,314.52,393.33,10.91;12,112.33,328.07,393.65,10.91;12,112.66,341.62,393.32,10.91;12,112.66,355.17,393.33,10.91;12,112.66,368.71,256.64,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,321.08,314.52,184.91,10.91;12,112.33,328.07,192.59,10.91">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,330.38,328.07,175.60,10.91;12,112.66,341.62,393.32,10.91;12,112.66,355.17,99.97,10.91">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="12,112.66,382.26,395.17,10.91;12,112.66,395.81,394.53,10.91;12,112.66,409.36,90.72,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,148.47,395.81,105.26,10.91">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,276.42,395.81,226.02,10.91">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,422.91,394.53,10.91;12,112.66,436.46,107.17,10.91" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m" coord="12,284.61,422.91,217.67,10.91">Longformer: The Long-Document Transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,450.01,393.32,10.91;12,112.66,463.56,393.33,10.91;12,112.66,477.11,60.91,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,262.76,450.01,161.38,10.91">Reformer: The Efficient Transformer</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rkgNKkHtvB" />
	</analytic>
	<monogr>
		<title level="m" coord="12,447.64,450.01,58.35,10.91;12,112.66,463.56,182.30,10.91">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,490.66,394.53,10.91;12,112.28,504.21,393.70,10.91;12,112.66,517.76,232.48,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,266.80,504.21,188.45,10.91">BigBird: Transformers for longer sequences</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,463.76,504.21,42.22,10.91;12,112.66,517.76,187.69,10.91">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,531.30,394.53,10.91;12,112.66,544.85,394.53,10.91;12,112.66,558.40,393.33,10.91;12,112.66,571.95,393.33,10.91;12,112.66,585.50,394.53,10.91;12,112.66,599.05,105.64,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,292.01,558.40,213.98,10.91;12,112.66,571.95,46.57,10.91">Transformers: State-of-the-Art Natural Language Processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Le</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,186.22,571.95,319.77,10.91;12,112.66,585.50,390.37,10.91">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,612.60,394.53,10.91;12,112.66,626.15,394.53,10.91;12,112.66,639.70,393.32,10.91;12,112.66,653.25,176.63,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,227.26,639.70,183.69,10.91">Scikit-learn: Machine Learning in Python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,419.87,639.70,86.11,10.91;12,112.66,653.25,82.55,10.91">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,666.80,395.01,10.91;13,112.66,86.97,393.33,10.91;13,112.66,100.52,107.17,10.91" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="13,166.51,86.97,269.33,10.91">Multilingual universal sentence encoder for semantic retrieval</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">H</forename><surname>Abrego</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-H</forename><surname>Sung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04307</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,112.66,114.06,393.33,10.91;13,112.66,127.61,393.33,10.91;13,112.66,141.16,393.33,10.91;13,112.66,154.71,395.00,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="13,369.18,114.06,136.81,10.91;13,112.66,127.61,219.72,10.91">Deep Unordered Composition Rivals Syntactic Methods for Text Classification</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,361.25,127.61,144.73,10.91;13,112.66,141.16,393.33,10.91;13,112.66,154.71,189.83,10.91">Proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing</title>
		<meeting>the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1681" to="1691" />
		</imprint>
	</monogr>
	<note>Long papers</note>
</biblStruct>

<biblStruct coords="13,112.66,168.26,393.53,10.91;13,112.66,181.81,393.33,10.91;13,112.66,195.36,394.53,10.91;13,112.28,208.91,384.23,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,347.46,168.26,158.73,10.91;13,112.66,181.81,104.23,10.91">Hierarchical attention networks for document classification</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,239.68,181.81,266.31,10.91;13,112.66,195.36,394.53,10.91;13,112.28,208.91,186.39,10.91">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,89.29,253.88,228.02,12.85;13,88.96,279.49,417.23,10.91;13,89.29,293.04,416.69,10.91;13,89.29,306.59,418.54,10.91;13,89.29,320.14,416.69,10.91;13,89.29,333.69,416.69,10.91;13,89.29,347.24,416.70,10.91;13,89.29,360.79,416.69,10.91;13,89.29,374.33,416.69,10.91;13,89.29,387.88,417.90,10.91;13,88.91,401.43,417.29,10.91;13,89.29,414.98,91.80,10.91" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="13,110.05,253.88,207.26,12.85;13,88.96,279.49,417.23,10.91;13,89.29,293.04,322.07,10.91">Hierarchical Attention Network This section describes the Hierarchical Attention Network (HAN) architecture we used for the multi-classification task for each category of the BDI questionnaire</title>
		<author>
			<persName coords=""><forename type="first">A</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>The architecture is shown in figure 3. We trained 21 HAN-classifiers using the top-related posts for each categorybased on the parameters described earlier. HAN employs bi-directional GRU on the word level, followed by an attention model, to extract the most informative words, which are then aggregated to generate a sentence vector, as shown in figure 4. Similarly, bi-directional LSTM on the sentence level is used with an attention mechanism to aggregate the most essential sentences to form the user-category vector which is then passed on to a dense layer for text classification using softmax activation as shown in figure 5 . For training, we use batch_size=128, Adam optimizer and categorical cross-entropy as the loss function. We added a dropout layer to avoid over-fitting.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
