<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.03,75.53,450.94,17.00;1,72.03,96.28,95.79,17.00">A RoBERTa-based model on measuring the severity of the signs of depression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,72.03,129.35,73.03,10.80"><forename type="first">Shih-Hung</forename><surname>Wu</surname></persName>
							<email>shwu@cyut.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="institution">Chaoyang University of Technology</orgName>
								<address>
									<settlement>Taichung</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">CLEF</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,172.33,129.35,66.26,10.80"><forename type="first">Zhao-Jun</forename><surname>Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Chaoyang University of Technology</orgName>
								<address>
									<settlement>Taichung</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">CLEF</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.03,75.53,450.94,17.00;1,72.03,96.28,95.79,17.00">A RoBERTa-based model on measuring the severity of the signs of depression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">044924D4B85BDCA223722C68D7CFE06A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Deep Learning, RoBERTa</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe our approach to the CLEF 2021 lab eRisk Task 3: Measuring the severity of the sign of depression. The main purpose of this task is to automatically measure the severity of the user's depression by analyzing the user's posting on social media. We adopt the deep learning pretrained language model, RoBERTa, as the basis of our system and propose two different approaches as the post-processing and submit 3 runs. The two post-processing weighting mechanisms is designed to make the system that will give prediction on higher level of severity. This is according to our observation on the results of last year eRisk lab that systems tend to give lower level of severity. With a fixed weighting approach, our second run gives the best Average Difference between Overall Depressions Levels (ADODL) and Depression Category Hit Rate (DCHR) this year.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Social media is popular, it can be seen that with the spread of mobile networks, people use social media more frequently. According to DIGITAL 2021: GLOBAL OVERVIEW REPORT <ref type="bibr" coords="1,477.95,421.11,11.63,9.90">[1]</ref>, social media users have reached more than half of the global population. People express emotions through social media has become a daily habit.</p><p>Researcher can analyze these postings with natural language processing technology and get useful results. In eRisk Task 3: Measuring the severity of the sign of the sign of depression, systems try to predict the severity of a user's depressive symptoms by analyzing the user's postings on social media. Similar studies have been conduct on other social media, such as Facebook language predicts depression in medical records <ref type="bibr" coords="1,154.58,509.64,12.66,9.90">[2]</ref> and forecasting the onset and course of mental illness with Twitter data [3], which have shown the importance of evaluating user depression levels through social media postings.</p><p>The main mission of eRisk 2021 Task 3 is to explore the feasibility of automatically estimating the severity of multiple symptoms associated with depressive symptoms. The organizers estimate a user's level of depression by the user's response to each question in the questionnaire of Baker's Depression List (BDI), which assesses the existence of feelings such as sadness, pessimism, and lack of energy. The questionnaire has 21 questions, each with four answers (from 0 to 3) or seven answers (0,1a, 1b, 2a, 2b, 3a, 3b). The system performance will be assessed by the overlap between the questionnaire filled out by real users and the questionnaire filled out by the system (number of correct predictions) <ref type="bibr" coords="1,490.95,610.91,11.44,9.90" target="#b0">[4]</ref>. This is the third time that the task of depression prediction is held in eRisk lab. In the past eRisk Tasks on depression prediction, many teams have come up with different ways to study this topic, such as, the USDB team used two different deep learning models (CNN and BiLSTM) <ref type="bibr" coords="1,437.67,648.94,16.60,9.90" target="#b8">[12]</ref>, the iLab team focused on the pre-processing aspects of training data <ref type="bibr" coords="1,315.88,661.44,16.62,9.90" target="#b9">[13]</ref>, the RELAI team used topic model (LDA and Anchor) <ref type="bibr" coords="2,130.30,74.77,18.16,9.90" target="#b10">[14]</ref> to conduct the research, the BioInfo@UAVR teams used the classifier of Yates et al. <ref type="bibr" coords="2,72.03,87.27,18.16,9.90" target="#b11">[15]</ref> and they had trained before to predict whether users were depressed <ref type="bibr" coords="2,394.92,87.27,16.60,9.90" target="#b12">[16]</ref>.</p><p>Most of the previous works focus on training using different models, or pre-processed data. Our approach this time, mainly focus on post-processing, after we used state-of-the-art deep learning pretrained language model, RoBERTa, as the basis of our system. We propose two different approaches as the post-processing and submit 3 runs. According to our observation on the results of last year eRisk lab that systems tend to give lower level of severity. Our post-processing weighting mechanisms is designed to make the system that will give prediction on higher level of severity.</p><p>The rest of this article is organized as follows: Section 2 describes how eRisk Task 3 provides data and how to evaluate system. The methodology is described in Section 3, which reports our research process and our experimental settings. The last two sections explore results we have come up with, as well as the future direction of the study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Data and Observation</head><p>The organizers of 2021 eRisk T3 provide the test dataset of 2019 and 2020, and as the training data. The 2020 dataset has a total of 20 users, during system developing phrase, we use the 2020 data as the training set to train the model and the 2019 dataset as the validation set to test the model. The dataset includes a severity questionnaire assessment of depressive symptoms, as well as postings on social media of a user's daily life. The questionnaire consists of a total of 21 questions and has four answers for each question, excepting that questions 16 and 18 has 7 answers <ref type="bibr" coords="2,372.65,334.84,11.44,9.90" target="#b0">[4]</ref>.</p><p>eRisk T3 uses four different scoring metrics to evaluate the model, namely Average Hit Rate (AHR), Average Closeness Rate (ACR), Difference between Overall Depressions Levels (DODL) Average (ADODL) and Depression Category Hit Rate (DCHR) <ref type="bibr" coords="2,318.88,372.84,11.45,9.90" target="#b4">[8]</ref>. During system development, we focus on the AHR and DCHR metrics, since the other two metrics are relative metrics of these two metrics. We believe that optimize the two metrics will also optimize the other two.</p><p> Average Hit Rate (AHR): For each user in the 21 questions, if the system predicted the actual result of the user in ten questions, the hit rate is 10/21, and AHR is the average hit rate to all users.  Depression Category Hit Rate (DCHR): System predicts the questionnaire results obtained an estimate of recognized depression, which matches the assessment information obtained in the actual questionnaire. According to the 2020 CLEF eRisk results in Fig. 1 <ref type="bibr" coords="3,315.63,88.02,11.45,9.90" target="#b1">[5]</ref>, the all 0' and all 1's prediction results were 36% and 29% in AHR, respectively. However the percentage dropped to 14% and 25% when evaluating DCHR, from which we speculate that the actual forecast data is tent to a higher level of serenity. That is, training set shows that users will give answers to each question with a lower level of serenity, but the overall serenity is not that low.</p><p>Table <ref type="table" coords="3,115.30,151.29,5.50,9.90">1</ref> shows the statistics of the training data, each user's postings is labelled according to the user's answers in the questionnaire, and the chart shows that most of the statistics are slightly biased to lower level of severity, so we expect to weight the results during the post-processing process to make the results more prone to higher level of severity will give better overall result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1 (a)</head><p>The percentage of the posting distribution for the 2020 dataset, each posting is labelled based on the results of the user's answer to the questionnaire. Assuming that the number of postings of the users who answer 0 to question 1 of the questionnaire is 350, and the total number of postings all users is 1000, the percentage is equal to <ref type="bibr" coords="3,254.10,268.58,5.18,11.00">(</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">System Architecture</head><p>Fig. <ref type="figure" coords="3,106.55,554.66,5.50,9.90" target="#fig_1">2</ref> shows our system flowchart. As mentioned in previous sections, we use a pre-trained model to give the Run1 and weighting the Run1 results into other two runs. The Preprocessing is quite simple, our system just delete URL, special characters, and white space from the users' postings. And sent it to BERT or RoBERTa model.</p><p>We build one model for each question, therefore, there are 21 models. Each posting is labelled with the answer of the user to the question. This labeling is assuming that each posting will give the same information on the choice of the user. We train the classifier by BERT/RoBERTa models according to the sentences in the training set. For each question, we train one classifier to decide whether one sentence lead to which answer. Since each author writes a lot of sentences, our system aggregate the vote of each sentence as our system output. In the first run, our system works with a majority vote principle, one answer will be selected if it get most votes. In the second run, we emphasize the weight of the votes by weighting more on the answers with serious results. That is, tend to be more depression. The weights are 1 to 7 for the votes of 0 to 6 respectively. In the third run, we further lower the weight of vote to 0 by rules. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data Processing</head><p>The training data contains data from 70 volunteers with questionnaire results as well as posts or comments on their social networks. The XML file format is shown in Fig. <ref type="figure" coords="4,413.17,500.64,4.13,9.90" target="#fig_2">3</ref>. We extract the TEXT content into a CSV file as our training data. We remove the URL, path, special characters, and each of the comments is organized into one line, saved in the first column. We aggregate all the posts from each anonymous ID and associate them to the user's questionnaire results, the answer to the question in order after the first column. We get a total of 33,155 comments, we use 80% (26524) for training, 20% (6631) for verification during our system development phrase. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-trained Model</head><p>Since BERT gives good results in several natural language processing applications in recent years <ref type="bibr" coords="5,72.03,118.54,16.62,9.90" target="#b13">[17]</ref>, so we adopt the BERT pre-trained model as our basis of our system. At first, we chose the pretrained model "bert-base-uncased" and made improvements by referring to the methods of <ref type="bibr" coords="5,473.45,131.04,12.66,9.90" target="#b2">[6]</ref> and <ref type="bibr" coords="5,507.48,131.04,11.62,9.90" target="#b3">[7]</ref>. Instead of using the final output of the BERT model directly, our system extracts the output of the last four hidden layers as the input vector for linear classification, as shown in Fig. <ref type="figure" coords="5,413.92,156.29,4.13,9.90">4</ref>. The Hyper-parameters of our model is: Hidden size=768, Learning r=1e-5, weight_decay=1e-2, Epoch=5  Later we use RoBERTa as the core of the system <ref type="bibr" coords="6,307.88,75.02,16.60,9.90" target="#b14">[18]</ref>. Since RoBERTa is optimized on the basis of BERT, and the authors have expanded the training dataset, trained with longer sequences, dynamically generated the shields used by MLM. We test them with our training data, and Fig. <ref type="figure" coords="6,492.95,100.77,5.50,9.90" target="#fig_5">6</ref> shows that RoBERTa is significantly more accurate than BERT in our system. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Post-processing</head><p>In the first run, our system output the original prediction result as a baseline for the follow-up runs, there is no any weighting of the predicted results. The prediction of each question is a simple majority vote, the system output the answer with the largest cumulative number. Fig. <ref type="figure" coords="6,420.43,457.36,5.50,9.90">7</ref> shows the prediction distribution of each question. We find that the prediction, affected by the training set, tend to favor less severity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 7: The answer prediction distribution of our Run1</head><p>In the second run, the predictions are adjusted according to the predictions in the first run, with a fixed-weight weighting mechanism to give with a higher severity answers. For example, Q1 has four different level of severity (from 0 to 3), so we give 1 to 4 as the severity weight. That is, if one posting </p><formula xml:id="formula_0" coords="6,151.45,326.10,328.57,9.00">Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q10 Q11 Q12 Q13 Q14 Q15 Q16 Q17 Q18 Q19 Q20</formula><formula xml:id="formula_1" coords="6,123.95,658.87,362.50,9.00">Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q10 Q11 Q12 Q13 Q14 Q15 Q16 Q17 Q18 Q19 Q20 Q21</formula><p>is predicted as 1 by our model, we count it twice; if one posting is predicted as 2 by our model, we count it 3 times; and if one posting is predicted as 3 by our model, we count it 4 times. Our system finally set the prediction of the question according to the maximum number after the weighting. Fig. <ref type="figure" coords="7,72.03,112.52,5.50,9.90">8</ref> shows Run2 prediction distribution, the distribution is pushed to the higher levels of severity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 8:</head><p>The answer prediction distribution of our Run2</p><p>In Run 3, the weighting is adjusted based on the percentage of the training data distribution. We use the percentage of distribution in Table <ref type="table" coords="7,243.35,354.59,5.50,9.90">1</ref> as the threshold value, the answer with the highest percentage of each question is selected as a default answer. Our system modify the weighting in order from the most severe to the slightest order, as long as the percentage of the prediction result is greater than the percentage of the distribution of training data, the prediction results as the final answer. For example, for Q1, the distribution in training data is (0:35%, 1:51%, 2:9%, 3:6%), the answer with the highest percentage is 1 in Q1, then 1 is our default answer. Suppose the original system predict output distribution for some user is (0:36%, 1:51%, 2:10%, 3:3%), our system will first check the percentage of answer 3, in this case 3% does not exceed the 6% threshold, so it is not our choice. Our system then will check the percentage of answer 2, in this case is 10%, which does exceed the 9% threshold, our system will output answer 2. In short, our system tends to choose the answer with higher severity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 9:</head><p>The answer prediction distribution of our Run3 Fig. <ref type="figure" coords="7,106.27,712.69,5.50,9.90">9</ref> shows the Run 3 prediction distribution. Unlike the first two runs, this distribution results are more broadly, less concentrated. However, overall performance is not the best.   Table . 2 shows that the overall performance of Run2 is better than the other two runs. We further show the number of correct prediction of the Q1-Q21 individual results in Fig. <ref type="figure" coords="8,428.67,717.94,9.17,9.90" target="#fig_7">10</ref>. From Fig. <ref type="figure" coords="8,492.95,717.94,9.17,9.90" target="#fig_8">11</ref>, we can see that Run2 give better prediction in 9 out of the 21 questions. We can also find that Q16 and Q18 are real hard to predict, where Q16's Run1 only predicts correctly once. This observation suggests that our weighting post-processing is valid, and most effective in Q16. However over-weighting also results in a decrease in Run3 results, such that in Run3 of Q1, obviously it makes the prediction results worse. We find that the appropriate adjustment gives better results might due to the unbalanced distribution in training data. Adjustment to fit the training data distribution is an effective post-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 11:</head><p>The number of correct prediction of the three runs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Works</head><p>The goal of eRisk T3 is to automatically assess the severity of depressive by analyze the postings of a person. We used a deep learning approach based on pre-trained model RoBERTa to build our system. We submit three runs with different post-processing weighting mechanism. Run2 gives the best ADODL and DCHR this year.</p><p>In our experiments, we assume that each posting will give the same information on the choice of the user. We believe that this is not a good assumption. Since a user might give positing in different emotions in different time, that will be very different from what the user might answer to each of the questions. This is one point that we will improve in the future. It should be that even for a user that shows higher level of severity according to the questionnaire, there will be only some of the sentences might show higher level of severity. Therefore, the sentences should be filtered with other tools. Only the ones that shows higher level of severity should be associated with the higher scores.</p><p>In the future, we plan to optimize the data, by comparing depression articles with non-depressive articles, extract the content of articles that shows depression, and remove the content of over-familiar articles, reducing the impact of useless content on the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head><p>This study was supported by the Ministry of Science and Technology under the grant number MOST 110-2221-E-324-011. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">References</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,72.03,747.98,256.85,11.00;2,140.75,496.56,312.80,248.98"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: CLEF 2020 eRisk: Task2. Performance Results<ref type="bibr" coords="2,316.26,747.98,12.62,11.00" target="#b1">[5]</ref> </figDesc><graphic coords="2,140.75,496.56,312.80,248.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,90.05,416.37,138.42,11.00"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Our System flowchart</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,93.30,737.73,413.90,11.00;4,98.80,751.23,340.80,11.00;4,223.02,586.36,148.95,148.95"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The XML format of each post in the dataset[4], where ID is the anonymous user ID, TITLE is the post title, INFO is the source, and TEXT is the content of the post</figDesc><graphic coords="4,223.02,586.36,148.95,148.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,86.28,181.79,436.65,9.90;5,72.03,194.29,117.81,9.90"><head>Fig. 5</head><label>5</label><figDesc>Shows the result of the model on Q3-Q6, since more epoch do not give better result, we limited our fine-tuning epoch to 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,72.03,514.15,451.01,11.00;5,72.03,527.65,331.97,11.00;5,188.31,204.28,232.69,306.95"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Our system extracts the output vector from the last four layers of the model's hidden layer and joins the four output vectors as the input vector of the linear classifier</figDesc><graphic coords="5,188.31,204.28,232.69,306.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,72.03,373.10,275.18,11.00"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Q1-Q21 accuracy of our system during development</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="7,121.85,261.23,4.58,9.03;7,117.30,248.03,9.06,9.00;7,117.30,234.78,9.06,9.00;7,117.30,221.53,9.06,9.00;7,117.30,208.28,9.06,9.00;7,117.30,195.03,9.06,9.00;7,117.30,181.76,9.08,9.03;7,117.30,168.55,9.06,9.00;7,117.30,155.30,9.06,9.00;7,117.30,142.05,9.06,9.00;7,137.52,272.97,335.35,9.00;7,249.73,292.90,106.43,9.00;7,122.60,631.98,4.58,9.03;7,118.05,609.65,9.06,9.00;7,118.05,587.28,9.06,9.00;7,118.05,564.90,9.06,9.00;7,118.05,542.51,9.08,9.03;7,118.05,520.17,9.06,9.00;7,118.05,497.80,9.06,9.00;7,138.25,643.73,333.89,9.00"><head></head><label></label><figDesc>Q4 Q5 Q6 Q7 Q8 Q9 Q10 Q11 Q12 Q13 Q14 Q15 Q16 Q17 Q18 Q19 Q20 Q21</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="8,72.03,664.70,450.82,11.00;8,72.03,677.95,72.39,11.00;8,154.25,384.01,286.30,278.20"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Histograms of randomly generated submissions with team submissions marked by vertical lines. (2019)<ref type="bibr" coords="8,125.77,677.95,18.65,11.00" target="#b6">[10]</ref> </figDesc><graphic coords="8,154.25,384.01,286.30,278.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="9,90.05,670.69,433.08,9.90;9,114.05,683.44,283.38,9.90;9,90.05,696.19,433.02,9.90;9,114.05,708.69,409.42,9.90;9,114.05,721.46,62.48,9.90;9,90.05,733.96,432.97,9.90;9,114.05,746.71,409.08,9.90;9,114.05,759.46,36.75,9.90;9,118.85,265.90,4.56,9.00;9,118.85,252.33,4.56,9.00;9,114.30,238.75,9.06,9.00;9,114.30,225.17,9.06,9.00;9,114.30,211.60,9.06,9.00;9,114.30,198.01,9.08,9.03;9,114.30,184.47,9.06,9.00;9,114.30,170.90,9.06,9.00;9,114.30,157.33,9.06,9.00;9,114.30,143.75,9.06,9.00;9,114.30,130.17,9.06,9.00;9,134.68,277.60,341.04,9.00;9,260.38,297.55,85.03,9.00"><head>[ 1 ]</head><label>1</label><figDesc>SIMON KEMP.: DIGITAL 2021: GLOBAL OVERVIEW REPORT. URL:https://datareportal.com/reports/digital-2021-global-overview-report (2021). [2] Eichstaedt, J.C., Smith, R.J., Merchant, R.M.: Facebook language predicts depression in medical records. Proceedings of the National Academy of Sciences (PNAS) 115(44), 11203-11208 (2018). [3] Reece, A.G., Reagan, A.J., Lix, K.L.M. et al. Forecasting the onset and course of mental illness with Twitter data. Sci Rep 7, 13006 (2017). URL:https://doi.org/10.1038/s41598-017-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,72.03,244.58,417.19,123.52"><head>Table 2</head><label>2</label><figDesc>System performance of our runs and best results in recent three years</figDesc><table coords="8,77.53,273.58,411.69,94.52"><row><cell></cell><cell>AHR</cell><cell>ACR</cell><cell>ADODL</cell><cell>DCHR</cell></row><row><cell>CYUT RUN1</cell><cell>32.02%</cell><cell>66.33%</cell><cell>75.34%</cell><cell>20.00%</cell></row><row><cell>CYUT RUN2</cell><cell>32.62%</cell><cell>69.46%</cell><cell>83.59%</cell><cell>41.25%</cell></row><row><cell>CYUT RUN3</cell><cell>28.39%</cell><cell>63.51%</cell><cell>80.10%</cell><cell>38.75%</cell></row><row><cell>Best result in this year [8]</cell><cell>35.36%</cell><cell>73.17%</cell><cell>83.59%</cell><cell>41.25%</cell></row><row><cell>Best result in year 2020 [5]</cell><cell>38.30%</cell><cell>69.41%</cell><cell>83.15%</cell><cell>35.71%</cell></row><row><cell>Best result in year 2019 [9]</cell><cell>41.43%</cell><cell>71.27%</cell><cell>81.03%</cell><cell>45.00%</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and Discussion</head><p>Table. 2 shows the results of our three Runs this year in task 3 <ref type="bibr" coords="8,362.90,105.77,11.63,9.90" target="#b4">[8]</ref>, and we compare them to be best results in last two years <ref type="bibr" coords="8,178.08,118.54,26.88,9.90">[5] [9]</ref>. This year, nine teams sent out 36 runs, of which we got the best results in ADODL and DCHR, and our ADODL performed better than the best results in the past 2 years.</p><p>The best DCHR and AHR results in year 2019 still hard to match. The best DCHR's practices can be seen in <ref type="bibr" coords="8,121.30,156.29,16.60,9.90" target="#b6">[10]</ref>, they use unsupervised methods to make the results. The authors also noted that by a simulation in Fig. <ref type="figure" coords="8,152.83,169.04,11.00,9.90">10</ref> that comparing the results of random, the authors felt that although the data were the best, they did not perform better than random. The best AHR practice can be seen in <ref type="bibr" coords="8,486.10,181.79,16.68,9.90" target="#b7">[11]</ref>, the authors first decided each user's depression level then decided the answer to the questionnaire. This approach is totally different from our approach.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,114.05,74.77,375.73,9.90" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><surname>Clef Erisk</surname></persName>
		</author>
		<ptr target="https://erisk.irlab.org/" />
		<title level="m" coord="10,174.28,74.77,156.33,9.90">Early risk prediction on the Internet</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,114.05,87.27,408.48,9.90;10,114.05,100.02,11.00,9.90;10,141.54,100.02,13.39,9.90;10,171.53,100.02,34.36,9.90;10,222.32,100.02,45.47,9.90;10,284.28,100.02,46.88,9.90;10,347.73,100.02,31.98,9.90;10,396.19,100.02,127.19,9.90;10,114.05,112.52,89.14,9.90" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_253.pdf" />
		<title level="m" coord="10,275.94,87.27,246.60,9.90;10,114.05,100.02,11.00,9.90;10,141.54,100.02,13.39,9.90;10,171.53,100.02,34.36,9.90;10,222.32,100.02,45.47,9.90;10,284.28,100.02,46.88,9.90">Overview of eRisk at CLEF 2020: Early Risk Prediction on the Internet (Extended Overview)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,114.05,125.29,409.33,9.90;10,114.05,138.04,252.23,9.90" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Mccormick</surname></persName>
		</author>
		<ptr target="https://mccormickml.c-om/2019/05/14/BERT-word-embeddings-tutorial/" />
		<title level="m" coord="10,214.25,125.29,167.96,9.90">BERT Word Embeddings Tutorial</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,114.05,150.54,409.24,9.90;10,114.05,163.29,407.38,9.90" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="10,114.05,150.54,311.98,9.90">Use the huggingface pre-trained model to solve 80% of nlp problems</title>
		<ptr target="https://www.bilibili.com/video/BV1Dz4y1d7am/?spm_id_from=333.788.videocard.15" />
		<imprint>
			<date type="published" when="2020-10-18">2020. October 18</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,114.05,175.79,409.03,9.90;10,114.05,188.54,409.29,9.90;10,114.05,201.29,409.08,9.90;10,114.05,213.82,72.46,9.90" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,467.72,175.79,55.36,9.90;10,114.05,188.54,213.94,9.90">Overview of eRisk 2021: Early Risk Prediction on the Internet</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Martin-Rodilla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,346.11,188.54,177.23,9.90;10,114.05,201.29,362.15,9.90">Proceedings of the Twelfth International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<meeting>the Twelfth International Conference of the Cross-Language Evaluation Forum for European Languages<address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021-09">2021, September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,114.05,226.57,408.80,9.90;10,114.05,239.07,409.26,9.90" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380/paper_248.pdf" />
		<title level="m" coord="10,277.22,226.57,245.63,9.90;10,114.05,239.07,156.41,9.90">Overview of eRisk at CLEF 2019 Early Risk Prediction on the Internet (extended overview)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,114.05,251.82,408.96,9.90;10,114.05,264.57,409.10,9.90;10,114.05,277.06,293.48,9.90" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="10,114.05,264.57,409.10,9.90;10,114.05,277.06,35.36,9.90">Transfer Learning for Depression: Early Detection and Severity Prediction from Social Media Postings</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Abed-Esfahani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maslej</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Goegan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">French</forename><forename type="middle">L</forename></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380/paper_102.pdf" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,114.05,289.81,409.39,9.90;10,114.05,302.34,409.16,9.90;10,114.05,315.09,190.94,9.90" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">G</forename><surname>Burdisso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Errecalde</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y-G´omez</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380/paper_103.pdf" />
		<title level="m" coord="10,366.84,289.81,156.59,9.90;10,114.05,302.34,340.95,9.90">UNSL at eRisk 2019: a Unified Approach for Anorexia, Self-harm and Depression Detection in Social Media</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,114.05,327.84,408.99,9.90;10,114.05,340.34,408.91,9.90;10,114.05,353.09,349.99,9.90" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="10,114.05,340.34,408.91,9.90;10,114.05,353.09,132.46,9.90">USDB at eRisk 2020: Deep learning models to measure the Severity of the Signs of Depression using Reddit Posts</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Boumahdi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Boukenaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">C</forename><surname>Kritli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hentabli</forename><forename type="middle">H</forename></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_39.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,114.05,365.59,409.33,9.90;10,114.05,378.36,409.17,9.90;10,114.05,390.86,245.23,9.90" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="10,390.51,365.59,132.88,9.90;10,114.05,378.36,404.17,9.90">Early Risk Detection of Self-Harm and Depression Severity using BERT-based Transformers iLab at CLEF eRisk 2020</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Martínez-Castaño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Htait</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Moshfeghi</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_50.pdf" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,114.05,403.61,408.51,9.90;10,114.05,416.36,409.33,9.90;10,114.05,428.86,83.64,9.90" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="10,316.92,403.61,205.63,9.90;10,114.05,416.36,223.80,9.90">Early Mental Health Risk Assessment through Writing Styles, Topics and Neural Models</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Maupomé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Belbahar</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_53.pdf" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,114.05,441.61,409.29,9.90;10,114.05,454.11,409.21,9.90;10,114.05,466.89,341.40,9.90" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,286.05,441.61,237.29,9.90;10,114.05,454.11,29.15,9.90">Depression and self-harm risk assessment in online forums</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,166.01,454.11,357.25,9.90;10,114.05,466.89,45.89,9.90">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2968" to="2978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,114.05,479.64,409.06,9.90;10,114.05,492.14,408.69,9.90;10,114.05,504.89,319.24,9.90" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="10,296.56,479.64,226.55,9.90;10,114.05,492.14,408.69,9.90;10,114.05,504.89,67.42,9.90">BioInfo@UAVR at eRisk 2020: on the use of psycholinguistics features and machine learning for the classification and quantification of mental diseases</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Trifan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Salgado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Oliveira</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_43.pdf" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,114.05,517.39,409.37,9.90;10,114.05,530.14,409.04,9.90;10,114.05,542.89,408.96,9.90;10,114.05,555.41,390.90,9.90" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,403.76,517.39,119.67,9.90;10,114.05,530.14,233.03,9.90">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,365.24,530.14,157.85,9.90;10,114.05,542.89,408.96,9.90;10,114.05,555.41,196.57,9.90">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">June 2-7, (2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,114.05,568.16,408.98,9.90;10,114.05,580.66,408.71,9.90;10,114.05,593.42,409.10,9.90;10,114.05,606.16,32.48,9.90" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="10,367.11,580.66,155.65,9.90;10,114.05,593.42,125.54,9.90">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<author>
			<persName coords=""><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Computing Research Repository</note>
	<note>version</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
