<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.03,76.17,450.59,15.30;1,72.03,95.67,223.78,15.30">Towards transfer learning using BERT for early detection of self-harm of social media users</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.03,128.10,47.47,10.80"><forename type="first">Un</forename><surname>Qamar</surname></persName>
						</author>
						<author>
							<persName coords="1,122.50,128.10,22.07,10.80"><surname>Nisa</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Computer &amp; Emerging Sciences -FAST</orgName>
								<address>
									<addrLine>ST</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">CLEF</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,173.83,128.10,81.47,10.80"><forename type="first">Rafi</forename><surname>Muhammad</surname></persName>
							<email>muhammad.rafi@nu.edu.pk</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Computer &amp; Emerging Sciences -FAST</orgName>
								<address>
									<addrLine>ST</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">CLEF</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<addrLine>Sector 17D Shah Latif Town</addrLine>
									<postCode>75030</postCode>
									<settlement>Karachi, Karachi</settlement>
									<country key="PK">Pakistan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.03,76.17,450.59,15.30;1,72.03,95.67,223.78,15.30">Towards transfer learning using BERT for early detection of self-harm of social media users</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3CECFF72B545B367FF8E882FEF46E5BF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>BERT transformers</term>
					<term>Word Embeddings</term>
					<term>Early Detection of Depression</term>
					<term>Transfer Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Online social media channels are everywhere and almost everyone uses them to post various parts of their lives that they are not comfortable sharing in real. Large amounts of data can be collected from these channels and used to study and development of systems that detect depression and tendency of self-harm in individuals on the internet. It is generally believed that social posts from a time order can be used to predict depression or signs of self-harm for a user. Deep learning models for Natural Language Processing tasks are already producing better results on language-based analysis of sentiment, offensive or depression detection for a collection of text. BERT and transformer-based architectures have proven to achieve state-of-the-art results for Natural Processing Tasks. We will apply transfer learning and supervised learning algorithm Logistic Regression on data to early detect signs of self-harm for a user. BERT is used to generate word embeddings of sentences, has a limitation of processing sentences only 512 tokens long, so we created custom algorithm to break sentences then generate embeddings then concatenate them for supervised learning. Depression leads to various forms of self-harm and our goal is to detect the tendency of self-harm in individuals as soon as possible to intervene.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Globally, depression is a common illness affecting more than 264 million people <ref type="bibr" coords="1,475.20,517.64,11.44,9.90" target="#b0">[1]</ref>. If left unchecked, depression can get worse with time, resulting in self harm or worse; suicide, according to World Health Organization, one person commits suicide every 40 seconds [2] various helplines and awareness campaigns have been introduced to people in the hope of lowering suicide rate but not everyone has that awareness or access to connect with a professional. 15 to 29-year-old people are most affected by depression that leads to suicide, making it 2 nd most subsequent cause of death for people in this age group. Technological evolution has changed habits of people due to advanced devices, social media networks and lifestyle, millennials prefer to text instead of talking to people on call or face to face. According to a survey by OpenMarket, which surveyed 500 millennials, "75% of millennials would rather lose the ability to talk versus text" <ref type="bibr" coords="1,290.13,631.42,11.44,9.90" target="#b1">[3]</ref>. With the rise of social media, there are various websites and channels where people openly talk about their struggles and battles with depression. There are support groups where people share their stories about depression, attempted suicide and other selfharm incidents. People prefer to text/write on social media about depression instead of seeing a professional for help, in some cases even preferring to talk to Bots. Because of social media channels we now have some textual data that can be salvaged for early detection of depression. There are some hidden patterns and styles in textual data that can help identify an author or even gender. Shlomo Argamon et al <ref type="bibr" coords="2,140.05,100.02,12.66,9.90" target="#b2">[4]</ref> did research that stated that the gender of a writer can be identified based on how many pronouns, facts, and noun-specifiers are used by a writer. Female writers use more pronouns like "her, she, him" and male writers use more facts and noun specifiers like "the number of facts" <ref type="bibr" coords="2,507.97,125.29,11.44,9.90" target="#b2">[4]</ref>. Similarly depressed individuals can also be detected by using sophisticated natural language processing techniques. Technology is helping in all areas and with the help of new natural language processing methods and tools, depressed individuals hopefully can be identified using natural language processing techniques before their condition gets worse or they provide any self-harm.</p><p>Idea of Early Depression Detection (EDD) is to monitor texts of a user over time in chronological order and assess using different learning approaches whether a user is depressed, in early stages to intervene and get some help to user. Because of unavailability of different public datasets regarding early detection of depression in internet users based on their comments and texts on social media, there haven't been diverse solutions to the specific problem to check from other datasets. Researchers have used Natural Language approaches along with ensemble classifiers for EDD but very few have tried to solve this with state-of-the-art deep learning models. In this study, we hope to deal with the problem, looking at the previous techniques that yielded good results, work with them along with new models and result in a working solution. Granted, a machine learning model cannot be as effective as a professional counsellor or psychiatrist but not everyone has access to a professional and with the rise of social media and textual data, a sophisticated model can help internet users.</p><p>This paper describes an experiment to work on the early depression detection problem of classifying a user's text from reddit in chronological order and predicting signs of self-harm. The approach used in this experiment is using BERT to create word embeddings and then Logistic Regression used for prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Detection of depression through textual data is a new and challenging field of research as it has been identified 7-8 years back. Assessing depression online through series of text has immense impact, as a lot of people are suffering from depression and don't have access to mental health officers. There is lack of data publicly available for Early Depression Detection (EDD) due to many reasons, including that the data is collected from social media sites which prohibit distribution based on the GDPR laws. Team of engineering students classified anxiety using EEG signals, making use of IoT and Machine Learning but for that every person has to have the EEG headset <ref type="bibr" coords="2,357.15,498.39,11.44,9.90" target="#b3">[5]</ref>, hence the detection of depression through social media is essential. Because of the incremental classification problem elevated by EDD, there aren't any baseline strategies to detect depression or modelling as well. There have been various approaches by different authors for early detection of depression, some using feature extraction, while others using ensemble of classifiers. Some authors have worked on the problem without taking user history and progress over time, while others have used time-aware methodology. David E. <ref type="bibr" coords="2,491.62,561.66,31.63,9.90;2,72.03,574.16,29.40,9.90" target="#b4">Losada (2016)</ref> collected data from different social sites, largely reddit to create a dataset for detection of depression, the dataset is collected from popular social forum "Reddit" <ref type="bibr" coords="2,385.15,586.92,11.45,9.90" target="#b4">[6]</ref>. Losada's dataset is used as baseline dataset for EDD at eRisk conference and other research projects. Losada also worked on creating a baseline method and evaluation metric called "Early Risk Detection Error" (ERDE) for models created to detect depression in texts over time. Different authors have used different types of machine learning, natural language processing or hybrid approaches to solve the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.1.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Learning</head><p>Deep Learning is a type of machine learning inspired by the structure and working of human brain, neurons connected to one another, in deep learning, it is called "Neural Network". Supervised Machine Learning algorithms need sets of features in data to be classified, but in deep learning features are identified and implicitly learned by the Neural Network itself. Deep learning techniques and frameworks like ULMfit, ELMo etc have been used to do word embeddings then passed on to text classifiers <ref type="bibr" coords="2,119.30,757.71,12.66,9.90" target="#b5">[7]</ref> or doing the text classification itself <ref type="bibr" coords="2,297.63,757.71,11.63,9.90" target="#b6">[8]</ref>. Elena Fano et al (2019) <ref type="bibr" coords="2,424.17,757.71,12.66,9.90" target="#b5">[7]</ref> used deep learning framework ELMo to detect features, word embeddings and classified the output using multi-layer perceptrons.</p><p>M <ref type="bibr" coords="3,99.01,100.02,91.12,9.90" target="#b6">Trotzek et al. (2018)</ref> specified that language metadata is important in EDD text sequences, used Neural networks for word embeddings and classification <ref type="bibr" coords="3,331.40,112.52,11.63,9.90" target="#b6">[8]</ref>. Trotzek used pre-trained word vectors taken from Wikipedia and trained on fastText and GloVe. Trotken trained fastText model on reddit dataset that has 1.7 billion user texts and posts <ref type="bibr" coords="3,291.63,138.04,11.44,9.90">[9]</ref>. Trotken pre-processed the data that it also has punctuations, emoticons and also special character words since he believes based on linguistic research that a depressed user has certain textual attributes, and they are important to preserve too. Trotzek used Convolutional neural network for text classification, in addition to the reddit data, Trotzek also used metadata features as secondary input to the CNN. <ref type="bibr" coords="3,86.28,201.29,87.87,9.90" target="#b7">Matero et al. (2019)</ref> pursued that depression detection for just one source of user writings is limited and proposed to use 3 different types of features or dimensions to predict instead of just textual data. He proposed to use Open Vocabulary features (word embeddings) from BERT, Theoretical dimensions like affect, intensity, valence, dominance etc and based on research said age and gender play a role too as the ratio of suicide is different in both genders. Third dimension he proposed is meta features of data like n-grams and statistics to get more inference about data. Matero proposed to use dual-context, one context from Reddit <ref type="bibr" coords="3,163.58,277.06,12.66,9.90" target="#b4">[6]</ref> writings and other features Meta or theoretical dimensions <ref type="bibr" coords="3,438.67,277.06,16.62,9.90" target="#b7">[10]</ref>. Matero didn't use time-aware classification methodology, so it is taking into account different features but not the user text history and used logistic regression to classify the risk of suicide. Matero evaluated his results with Accuracy and F1 score, no ERDE as there wasn't time aware methodology used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Statement -eRisk 2021 Task 2: Early Detection of Signs of Self-Harm</head><p>Consider a collection of social media posts of a user U, user posts several contents P in a time n. Posts of a user are stored in chronological order over time. The frequency of how much a user posts in a day, week etc. and the order of those posts in a time for example each day the posts P get more depressing and gradually have explicit words like 'I had a panic attack today', 'World would be better without me' etc. These type of analysis on the posts can be used to infer depressions and signs of selfharm early in a user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uposts = {P1, P2, P3……. Pn)</head><p>Idea is to propose a model that can identify a depressed user without the situation being critical that a user cause self-harm to themselves, through a sophisticated system, there can be intervention to help the user. Each post P of a user will be passed through the model sequentially and using time-aware methodology (considering past posts of user), user will be predicted by the model to be depressed or not depressed. Due to the nature of task, it is important that the model predicts user to be depressed in time, so not only does the model have to predict correctly but also in time as late prediction will not be useful in the given scenario.</p><p>We participated in Task 2 <ref type="bibr" coords="3,208.10,604.16,18.16,9.90" target="#b8">[11]</ref> for eRisk 2021, this task was also featured in 2019 <ref type="bibr" coords="3,459.45,604.16,18.16,9.90" target="#b9">[12]</ref> as Task 2 and in 2020 [26] as Task 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">DataSet</head><p>The dataset used is following the same format as <ref type="bibr" coords="3,319.38,686.19,11.45,9.90" target="#b4">[6]</ref>. Dataset is a collection of a user's texts, comments on other people's posts, their own posts with titles, collected from social forum website Reddit. Dataset is anonymous and users are denoted by user numbers, additional data is the time and date of posts and comments. There are 2 classes of users "self-harm" and "not self-harm", denoted by 1 and 0 in data. The dataset released by CLEF eRisk 2021 <ref type="bibr" coords="3,328.15,736.96,18.40,9.90" target="#b8">[11]</ref> is used for the experiment and research. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Methodology</head><p>Model that does additional natural language processing on data like bag of words, word embeddings then resulting data passed through classifier or ensemble classifiers has better outcome in past research. Word embeddings are the most important part of text classification tasks, there have been many pretrained deep learning frameworks that provide state-of-the-art word embeddings. Elena Fano <ref type="bibr" coords="4,487.20,646.19,12.66,9.90" target="#b5">[7]</ref> used GLoVe and ELMo for word embeddings. BERT has limitation of processing only 512 tokens, hence not used by everyone and efficiently. Matero et al <ref type="bibr" coords="4,294.88,671.44,18.16,9.90" target="#b7">[10]</ref> used BERT similarly for word embeddings but they truncated the sentences in training due to the 512 token limitations, so all user writings weren't used to training. Trotzek <ref type="bibr" coords="4,191.08,696.69,12.66,9.90" target="#b6">[8]</ref> also experimented with BERT word embeddings but also stated the limitation of tokens and processing only first 512 tokens of sentences, <ref type="bibr" coords="4,401.18,709.44,12.66,9.90" target="#b6">[8]</ref> used fastText for word embeddings as they didn't have any limitation and provided better results. In this study we will use BERT <ref type="bibr" coords="4,103.55,734.71,18.16,9.90" target="#b10">[13]</ref> to generate word embeddings then use the extracted features to train supervised machine learning model, using Logistic Regression Classifier similar to <ref type="bibr" coords="4,350.65,747.21,11.44,9.90" target="#b4">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.1.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Processing</head><p>The training data for Task 2 was collection of reddit user writings in XLM format, since the data was real collected from Reddit (a social networking forum) the data had to be cleaned, prepared to be processed to accommodate the 512-token limitation of BERT. First all URLs from each User Post is removed, then all words in the post are counted. An empty StringS is initialized for each user to store user posts/writing. If the total words in post are less than 300 then we check further to see if concatenating user post with StringS will result in more than 300 words or not. If not, then both are concatenated, and next post is processed and go through the same flow. If the post contains more than 300 words, then it is broken down into small sentences and split using punctuation marks, the same process of checking words again and concatenating with Strings is applied then, if after splitting on punctuations the word count is still more than 300 then it means it's a long paragraph and is not being split using punctuations, we then split the long post using blank space and concatenate with StringS till StringS has 300 words. If StringS and the new post's length after being concatenating is more than 300 then String S is saved in dataframe as a row and then emptied for next sentence. The whole flow of how the data was processed is shown in detail in Figure <ref type="figure" coords="5,300.64,269.57,5.50,9.90" target="#fig_0">2</ref> as Flowchart. Each row in new created Data has UserId, newly processed sentences, Class Label of the user being depressed or not. Newly generated data of 340 users now has 16060 rows. Now prepared Data is then used to generate word embeddings using transfer learning on BERT.</p><p>The motivation behind this approach is that when a sentence is passed to BERT, if a sentence has 15 words it still outputs a vector of 786 dimension. Using this approach, smaller sentences in chronological order are concatenated instead of removing them as each post of a user is important. Other teams have truncated posts or removed small posts from data. Using this approach, we aren't losing any data and are also maintaining the chronological order of posts and the context of sentences.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">BERT Word Embeddings</head><p>Bidirectional Encoder Representations from Transformers (BERT) was created by google <ref type="bibr" coords="6,492.95,212.82,18.16,9.90" target="#b10">[13]</ref> to out-perform various Natural Language Processing models like ELMo, ULMfit, Generative Pre-Training etc as they are all uni-directional and hardly bidirectional for downwards NLP streams. BERT is called as a method of pre-training language representations, we first pre-train a large general-purpose corpus then uses that for downstream NLP tasks that we are targeting like early detection in our study. BERT has pre-trained word embeddings of 30,000-word tokens, each word token having 768 features.</p><p>BERT pre-trained model is available via hugging face transformer library, BERT has different versions that vary in training parameters or cases like lower-case, upper-case, multilingual etc. We used 'bert-base-uncased' for transfer learning to generate word embeddings for the sentences and user writings.</p><p>The prepared training data is passed to BERT in batches of 3000 rows to generate word embeddings. The word embeddings generated are of 16060 x 768 dimensions Some more data processing is applied to the generated word embeddings to prepare for machine learning, all the rows of a single user are concatenated to have one row per user for classification. This resulted in 340 x 627456 dimensions. The columns were huge, so further dimensionality reduction was applied for faster processing and training of model. After reduction the size of columns is 1655.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Depression / Self-harm Classifier</head><p>Depression classifier is trained using processed word embeddings are then sampled on Logistic Regression using cross-validation to find the optimal weight and the optimal value of C for normalization. The optimal values are then applied on training data word embeddings and model is saved to be used for testing and prediction. We created another variation, where classifier is built using AutoML open-source library 'auto-sklearn', which applies different machine learning algorithms on data and does hyperparameter tuning based on the accuracy metric <ref type="bibr" coords="6,368.40,535.39,16.60,9.90" target="#b14">[17]</ref>. Logistic Regression is chosen as classifier to perform experiment on test-server data provided by CLEF eRisk team. The results on training data with different confidence score or threshold values are shown in Table <ref type="table" coords="6,444.18,560.66,4.14,9.90">3</ref>. The confidence value of 0.5 on the logistic regression output gave the most optimal results in our experiment of predicting on test data set during training. The setting of CLEF eRisk states that once a prediction about a user is made, it cannot be changed later on so for first 5 user posts, we decided to use 0.75 confidence value. AutoML took longer time to train model and outputs bad scores as due to the sparsity of training data it is adding 0 in test data set, due to this AutoML model is predicting 0 unless the data is of same dimensions (1655) without the 0 padding at the end. AutoML model predicted just one sample to be positive but that was not true positive. AutoML model was going to be run2 but due to bad results in training we did not submit it and only submitted run of logistic regression model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>Training results on evaluation metrics using Logistic Regression and AutoML classifier and different confidence scores as threshold values for prediction.</p><p>Classifier Threshold Value </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Baseline Classifier</head><p>The research in Early Depression Detection <ref type="bibr" coords="7,292.38,530.39,12.66,9.90" target="#b5">[7]</ref> has been used as a benchmark by researchers in the domain <ref type="bibr" coords="7,124.55,542.89,16.60,9.90">[18]</ref>. Hence, for the experimental setup for this study, we have reproduced the experiment of <ref type="bibr" coords="7,84.03,555.66,12.66,9.90" target="#b5">[7]</ref> to be used as a Baseline Model, the experiment is implemented as follows:</p><p>CLEF eRisk Dataset has negative and positive users in the format discussed in Chapter 4.1. All negative and positive writings are appended to one corpus. Feature extraction on is performed by vectorizing the corpus TfidfVectorizer, having standard stop list in English language and minimum document frequency of 20words that appear in less than 20 user writings are removed. Vectorization is done with sklearn library in python. Resulting vector is stored in a sparse 486x12968 array.</p><p>Logistic regression is used for classification. Vector of ground truth values is created for 486 users, denoting depressed as 1 and not depressed as 0. Implementation of Logistic Regression is done using sklearn library of Logistic Regression, in this experiment logistic regression is taking 4 parameters. Penalty to specify the normalization which is 'L1' in our study. C is the penalty parameter accompanying with the error term of the optimisation of model, like in SVM's C should have smaller value for strong regularization. class_weight is the last parameter to specify weights for classes, this is useful for data which has class imbalance.</p><p>Values of C and class_weight is optimized using the standard protocol by Chih-Wei hsu [34], grid search with exponentially growing sequences of C = 2-10, 2-4, ..., 29 and w = 20, 21, ..., 29 is applied on parameters. 4-fold cross-validation is done on eRisk 2021 t2 training data with the above sequences to find the optimal value. The value of C and w that maximizes F1 score, precision and recall, in <ref type="bibr" coords="8,500.20,112.52,12.66,9.90" target="#b5">[7]</ref> is C=16 and w=4. But we didn't get that value while experimenting. Based on the 4-fold cross-validation the got two sets of values that optimized F1 score, C=256 &amp; w=64 and C=128 &amp; w=32. The respective values are used with other hyperparameters, and logistic regression is applied on training data to build a self-harm classifier. We created two Baseline classifiers with different hyperparameters, to compare with our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Evaluation Metrics</head><p>Several evaluation metrics are proposed and widely used for the specified task, shared in detail below as mentioned in <ref type="bibr" coords="8,174.33,245.32,16.85,9.90" target="#b8">[11]</ref>:</p><formula xml:id="formula_0" coords="8,90.05,284.83,21.49,14.00">6.1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Early Risk Detection Error (ERDE)</head><p>In detecting a serious illness or disease that causes harm to an individual, time is of the essence and detecting at right time to take preventive measures before the condition worsens. ERDE is a metric <ref type="bibr" coords="8,510.48,327.59,12.66,9.90" target="#b4">[6]</ref> to measure the error or accuracy of the model, too early detection can lead to false positive classifications or too late detection can lead to serious harm so the prediction should be at an optimal time for intervention, ERDE measures that for the model.</p><p>The delay in model is measured by the total user posts depression classifier processed to make the classification, denoted by k. Since positive classes in the cases is low, we can suffer "class imbalance" problem, so each error is weighted differently in ERDE.</p><p>Here, EDD system has made a classification decision of a user being depressed or not, d at a given time t. The prediction d can lead one of the four golden truth judgements: True Positive, True Negative, False Positive, False Negative. ERDE measure based on these four cases, is defines as:</p><p>In different studies, sometimes the positive class is majority or sometimes negative is majority, in our study the positive classes are in minority. To build a measure that takes into consideration the class imbalance, cfn here is set to 1 and it should be greater than cfp, cfp can be set to be some proportion of positive examples (e.g., if there are 3% positive examples then set cfp to 0.03). The third factor, latency cost lco(k) (∈ [0, 1]) encodes a penalty if true positives are identified with delay and is set as <ref type="bibr" coords="8,476.45,594.91,12.66,9.90" target="#b4">[6]</ref> (2) This function is parametrized by o, which controls the point where the cost grows more quickly. The error is calculated by taking mean of all classification of ERDE values. <ref type="bibr" coords="8,407.93,663.94,18.15,9.90" target="#b8">[11]</ref> ERDEo is a baseline proposed data stream classification metric where not only do we have to make prediction but also identify when to make it, which is crucial for Early detection of depression and selfharm domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">F-latency</head><p>CLEF eRisk 2021. <ref type="bibr" coords="9,179.58,74.77,18.40,9.90" target="#b8">[11]</ref> proposed to use Flatency, in addition to ERDE metric, to get more interpretable evaluations. Flatency was proposed by Sadeque and colleagues <ref type="bibr" coords="9,422.42,87.27,16.60,9.90" target="#b13">[16]</ref>. F-latency is also called and referred to as Latency-weighted F1. Which is F1 score multiplied by speed, speed is overall factor where if a user is correctly identified and classified in their first post and it is close to 0 for slow systems that have to process hundred or more writings of a user to identify true positives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6.3.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Standard Classification Measures</head><p>Whenever machine learning models are mentioned for supervised learning, the Accuracy of the model is an important metric to look at, but for some cases the accuracy of the model doesn't specify or give a whole view of the model therefore other standard classification measures like Precision, Recall and F-measure are used to evaluate the model further. Precision, Recall and F1 Score classification metrics are used in CLEF eRisk tasks related to depression as well <ref type="bibr" coords="9,367.40,232.82,16.60,9.90" target="#b8">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Result &amp; Discussion</head><p>The experiment delivered good results for ERDE o <ref type="bibr" coords="9,310.88,289.57,18.16,9.90" target="#b8">[11]</ref> and Recall, but the accuracy and efficiency of model is not optimal and leaves room for improvement. The confidence value of 0.5 used as threshold by Logistic Regression Model is increasing F1 Score slightly, the recall is increased drastically as compared to other results of 0.75 and 0.8 confidence score threshold value. ERDE is decreased with 0.5 confidence score as well. Submission of CLEF eRisk 2021 Task 2 submission was done on 6 user writings, which is low number to see the performance of whole model, so we applied the model after the submission to test what the result would have been if more writings were processed and predicted. We also tested our model with Baseline model created using the approach in <ref type="bibr" coords="9,420.49,378.36,11.43,9.90" target="#b5">[7]</ref>. The ERDEo of our runs had better ERDEo than the baseline model, our proposed model had better recall in train stage too as compared to the result of baseline model on train data. Explanation of runs along with predictions and result on metrics are shared in Table <ref type="table" coords="9,253.70,416.36,4.13,9.90" target="#tab_2">4</ref>. Run 1 is not cleaning URLs, punctuation marks and bad formats like #8217 for encoding of apostrophe (') etc., on user posts processed, whereas Run 2 is processing user posts and cleaning data before generating word embeddings. Run 2 isn't much different from Run 1 as there is very slight change in results of them, both not being optimal for Precision, F1 score. The recall on model is good. Run1 classified 1241 users as positive, out of those only 129 are true positives and Run1 classified 1232 as positive, correctly identifying 130 users. Run1 has read 104 user writings to make all predictions, instead of reading all, Run2 has read 107 user writings to make all predictions. In first 5 posts, 1130 users were classified in Run2 making the speed of experiment to be optimal, details shown in Figure <ref type="figure" coords="10,515.14,87.52,4.13,9.90" target="#fig_2">4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7.1.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Future work</head><p>The approach of Data Preparation is unique, due to the limitation of resources and time, we couldn't train BERT or RoBERTa on our generated data and used Transfer Learning instead. The proposed approach can be improved in future by following methodologies:</p><p>• The approach of Data Preparation is unique, due to the limitation of resources and time, we could not train BERT or RoBERTa on our generated data and used Transfer Learning instead. Own model on prepared data using our approach can be built from scratch or fine-tuned on BERT, RoBERTa to get more accurate results specific to the domain of Early Risk Prediction. • Similar to team iLab from CLEF eRisk 2020 <ref type="bibr" coords="10,342.40,525.20,18.40,10.80" target="#b11">[14]</ref>, another experiment can be done to process Test data similar to Train data in our experiment, pass-through classification model and the output probability averaged for all generated sentences and make decision if a user is depressed or not. • Similar to team iLab from CLEF eRisk 2020 <ref type="bibr" coords="10,342.40,580.72,18.40,10.80" target="#b11">[14]</ref>, another experiment can be done to process Test data similar to Train data in our experiment, pass-through classification model and the output probability averaged for all generated sentences and make decision if a user is at risk or not • Instead of using BERT for extracting word embeddings, use BERT and its different architectures for classification and compare for better model. • Data can be experimented and compared with different models of BERT like RoBERTa or more state-of-the-art models like GPT-3.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,72.03,631.92,451.11,9.90;5,72.03,644.69,368.82,9.90;5,73.15,404.04,449.80,224.85"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Flow-chart of Data Preparation Stage, breaking user posts into small sentences of 300 words. Data preparation is used for training classifier, not while testing as seen in Figure 3.</figDesc><graphic coords="5,73.15,404.04,449.80,224.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,72.03,460.61,347.16,9.90;7,86.20,229.79,451.00,228.10"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: High Level Flow Diagram / Architecture Diagram of the experiment</figDesc><graphic coords="7,86.20,229.79,451.00,228.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="10,123.55,346.59,362.10,9.90;10,173.10,111.30,262.59,218.40"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Histogram of user writings read for each user for classification of Run2.</figDesc><graphic coords="10,173.10,111.30,262.59,218.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="4,72.00,204.49,383.70,306.13"><head></head><label></label><figDesc></figDesc><graphic coords="4,72.00,204.49,383.70,306.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,72.03,74.77,451.12,486.54"><head>Table 1</head><label>1</label><figDesc>Statistics and details about the 2021 Task 2 Dataset<ref type="bibr" coords="4,301.13,87.27,16.60,9.90" target="#b8">[11]</ref>.</figDesc><table coords="4,82.78,102.27,432.47,87.17"><row><cell></cell><cell>Training Data</cell><cell>Training Data</cell><cell>Test Data</cell><cell>Test Data</cell></row><row><cell></cell><cell>self-harm</cell><cell>not self-harm</cell><cell>self-harm</cell><cell>not self-harm</cell></row><row><cell>Number of Users</cell><cell>41</cell><cell>299</cell><cell>104</cell><cell>319</cell></row><row><cell>Number of Posts</cell><cell>6927</cell><cell>163506</cell><cell>11691</cell><cell>91136</cell></row><row><cell>Min. Posts per User</cell><cell>8</cell><cell>10</cell><cell>9</cell><cell>9</cell></row><row><cell>Max. Posts per User</cell><cell>997</cell><cell>1992</cell><cell>942</cell><cell>1990</cell></row><row><cell>Mean of Posts per User</cell><cell>168</cell><cell>546</cell><cell>112</cell><cell>285</cell></row></table><note coords="4,72.03,513.39,450.79,9.90;4,72.03,526.14,451.12,9.90;4,72.03,538.64,450.98,9.90;4,72.03,551.41,344.19,9.90"><p>Figure 1: Histogram of all user writings (training and testing). Out of total 763 users, 376 users have writings from 8 to 150. This can also mean that 150 user writings should be processed in training stage if there is limitation of computing resources or time as majority users have writings no more than 150 and an optimal solution should give accurate results on first 150 user writings.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,72.03,721.72,447.15,37.64"><head>Table 2</head><label>2</label><figDesc>Statistics and details about the 2021 T2 training Data, ready to be prepared to be processed on BERT.</figDesc><table coords="5,257.35,749.46,177.10,9.90"><row><cell>Data Stats</cell><cell>Comments</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,72.03,442.86,451.48,228.73"><head>Table 4</head><label>4</label><figDesc>Result of classifier on test-server data predictions. LR-T2 run1 &amp; run2 are the runs submitted for CLEF eRisk 2021 task2 -Early Detection of Signs of Self-Harm. LR-T2 test run1 &amp; run2 are experiments performed using same model but run on more user posts after the task2 submission.</figDesc><table coords="9,106.30,495.64,379.40,175.95"><row><cell>Classifier</cell><cell>Number</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>ERDE5</cell><cell>ERDE50</cell></row><row><cell></cell><cell>of Posts</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LR -T2</cell><cell>6</cell><cell>0.172</cell><cell>0.124</cell><cell>0.283</cell><cell>0.101</cell><cell>0.097</cell></row><row><cell>run1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LR -T2</cell><cell>6</cell><cell>0.172</cell><cell>0.124</cell><cell>0.283</cell><cell>0.101</cell><cell>0.097</cell></row><row><cell>run2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LR -T2</cell><cell>104</cell><cell>0.185</cell><cell>0.103</cell><cell>0.848</cell><cell>0.097</cell><cell>0.096</cell></row><row><cell>test run1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LR -T2</cell><cell>107</cell><cell>0.187</cell><cell>0.105</cell><cell>0.855</cell><cell>0.096</cell><cell>0.095</cell></row><row><cell>test run2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline-</cell><cell>All</cell><cell>0.545</cell><cell>0.419</cell><cell>0.778</cell><cell>0.141</cell><cell>0.122</cell></row><row><cell>w-64[7]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline-</cell><cell>All</cell><cell>0.558</cell><cell>0.441</cell><cell>0.759</cell><cell>0.139</cell><cell>0.12</cell></row><row><cell>w-32[7]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8.">Acknowledgements</head><p>I would like to thank my university <rs type="institution">National University of Computers &amp; Emerging Sciences</rs>, my research supervisor, <rs type="person">Muhammd Rafi</rs> and my colleagues at <rs type="institution">IBM</rs>, <rs type="person">Asna Javed</rs>, <rs type="person">Mohammad Ali Khan</rs> and <rs type="person">Fawaz Siddiqui</rs> for supporting me with my research and helping out with few technicalities.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="11,108.05,157.10,379.81,10.80;11,108.05,170.85,400.33,10.80;11,108.05,184.85,363.19,10.80;11,108.05,198.60,375.05,10.80" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,355.72,170.85,152.66,10.80;11,108.05,184.85,363.19,10.80;11,108.05,198.60,285.08,10.80">and years lived with disability for 354 diseases and injuries for 195 countries and territories, 1990-2017: a systematic analysis for the Global Burden of Disease Study</title>
	</analytic>
	<monogr>
		<title level="m" coord="11,108.05,157.10,336.95,10.80;11,108.05,170.85,247.67,10.80">GBD 2017 Disease and Injury Incidence and Prevalence Collaborators</title>
		<imprint>
			<date type="published" when="2017">2018. 2017</date>
		</imprint>
	</monogr>
	<note>Global, regional, and national incidence, prevalence</note>
</biblStruct>

<biblStruct coords="11,108.05,239.88,322.17,10.80;11,108.05,253.87,412.67,10.80;11,108.05,267.63,69.78,10.80" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Loechner</surname></persName>
		</author>
		<ptr target="https://www.mediapost.com/publications/article/275332/text-vs-talk-gets-millennials-attention.html" />
		<title level="m" coord="11,169.77,239.88,195.03,10.80">Text vs. Talk Gets Millennials&apos; Attention</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.05,281.37,411.02,10.80;11,108.05,295.15,318.12,10.80;11,108.05,308.90,129.74,10.80" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,210.01,281.37,279.08,10.80">Gender, Genre, and Writing Style in Formal Written Texts</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Argamon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fine</surname></persName>
		</author>
		<idno type="DOI">10.1515/text.2003.014</idno>
	</analytic>
	<monogr>
		<title level="j" coord="11,497.01,281.37,22.07,10.80;11,108.05,295.15,253.98,10.80">Text -Interdisciplinary Journal for the Study of Discourse</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.05,322.90,370.86,10.80;11,108.05,336.65,375.23,10.80;11,108.05,350.40,377.88,10.80;11,108.05,364.15,331.37,10.80;11,108.05,377.92,175.80,10.80" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,285.42,322.90,193.49,10.80;11,108.05,336.65,227.45,10.80">Electroencephalography Based Machine Learning Framework for Anxiety Classification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arsalan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Majid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Anwar</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-981-15-5232-8_17</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,178.77,350.40,307.16,10.80;11,108.05,364.15,52.71,10.80">Intelligent Technologies and Applications, Second International Conference</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Bajwa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sibalija</forename><surname>Sarwar</surname></persName>
		</editor>
		<meeting><address><addrLine>INTAP; Bahawalpur, Pakistan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019. 2020</date>
			<biblScope unit="page" from="187" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.05,391.92,368.50,10.80;11,108.05,405.67,412.63,10.80;11,108.05,419.42,320.13,10.80" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,234.99,391.92,241.56,10.80;11,108.05,405.67,66.27,10.80">A Test Collection for Research on Depression and Language Use</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<idno>doi:0.1007/978-3-319-44564-9_3</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,198.49,405.67,322.18,10.80;11,108.05,419.42,117.64,10.80">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.05,433.17,397.46,10.80;11,108.05,446.92,414.70,10.80;11,108.05,460.70,385.65,10.80;11,108.05,474.70,105.98,10.80" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,265.72,433.17,239.79,10.80;11,108.05,446.92,178.86,10.80">Uppsala University and Gavagai at CLEF eRISK: Comparing Word Embedding Models</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Joakim</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="11,311.22,446.92,211.53,10.80;11,108.05,460.70,161.14,10.80">Working Notes of CLEF 2019 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">September 9-12, 2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.05,488.45,412.13,10.80;11,108.05,502.20,389.34,10.80;11,108.05,515.95,352.38,10.80;11,108.05,529.70,167.72,10.80" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,319.96,488.45,200.22,10.80;11,108.05,502.20,354.91,10.80">Utilizing Neural Networks and Linguistic Metadata for Early Detection of Depression Indications in Text Sequences</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Trotzek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2018.2885515</idno>
	</analytic>
	<monogr>
		<title level="j" coord="11,471.33,502.20,26.06,10.80;11,108.05,515.95,241.86,10.80">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="588" to="601" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,144.05,598.72,357.90,10.80;11,108.05,612.73,403.01,10.80;11,108.05,626.48,398.65,10.80;11,108.05,640.25,27.00,10.80" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,322.99,598.72,178.95,10.80;11,108.05,612.73,191.98,10.80">Suicide Risk Assessment with Multilevel Dual-Context Language and BERT</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Matero</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idnani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Son</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-3005</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,325.38,612.73,185.68,10.80;11,108.05,626.48,247.63,10.80">Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology</title>
		<meeting>the Sixth Workshop on Computational Linguistics and Clinical Psychology</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,144.05,654.00,357.53,10.80;11,108.05,667.75,382.38,10.80;11,108.05,681.75,376.40,10.80;11,108.05,695.50,398.83,10.80" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,443.99,654.00,57.59,10.80;11,108.05,667.75,296.31,10.80">eRisk 2021: Pathological Gambling, Self-harm and Depression Challenges</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Martín-Rodilla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,424.41,681.75,60.05,10.80;11,108.05,695.50,162.05,10.80">Advances in Information Retrieval. ECIR 2021</title>
		<title level="s" coord="11,278.50,695.50,168.87,10.80">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Moens</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Mothe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Perego</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Potthast</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12657</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,144.05,709.25,361.84,10.80;11,108.05,723.02,407.09,10.80;11,108.05,736.78,360.88,10.80;11,108.05,750.78,368.33,10.80;12,108.05,75.08,413.06,10.80;12,108.05,88.83,111.03,10.80" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,359.44,709.25,146.45,10.80;11,108.05,723.02,145.49,10.80">Overview of eRisk 2019 Early Risk Prediction on the Internet</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,108.05,750.78,368.33,10.80;12,108.05,75.08,239.91,10.80">Experimental IR Meets Multilinguality, Multimodality, and Interaction. 10th International Conference of the CLEF Association</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Crestani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Savoy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Rauber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Heinatz Bürki</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting><address><addrLine>CLEF; Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09-09">2019. 2019. September 9-12, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,144.05,102.58,349.60,10.80;12,108.05,116.58,414.42,10.80;12,108.05,130.35,24.00,10.80" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="12,357.76,102.58,135.89,10.80;12,108.05,116.58,268.83,10.80">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,144.05,144.10,374.36,10.80;12,108.05,157.85,410.28,10.80;12,108.05,171.60,412.92,10.80;12,108.05,185.60,30.34,10.80" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,471.43,144.10,46.98,10.80;12,108.05,157.85,410.28,10.80;12,108.05,171.60,71.22,10.80">Early risk detection of self-harm and depression severity using BERT-based transformers : iLab at CLEF eRisk</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Martinez-Castano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Htait</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Moshfeghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,228.52,171.60,174.71,10.80">Early Risk Prediction on the Internet</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="2020" to="2029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,144.05,199.35,372.29,10.80;12,108.05,213.01,402.93,10.92;12,108.05,226.88,223.83,10.80" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,429.13,199.35,87.20,10.80;12,108.05,213.13,177.47,10.80">Learning When to Classify for Early Text Classification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Loyola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">L</forename><surname>Errecalde</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-75214-3_3</idno>
	</analytic>
	<monogr>
		<title level="j" coord="12,308.88,213.01,137.05,10.92">Computer Science -CACIC</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="24" to="34" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,144.05,240.63,331.57,10.80;12,108.05,254.63,376.39,10.80;12,108.05,268.38,388.85,10.80;12,108.05,282.13,27.00,10.80" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,294.99,240.63,180.64,10.80;12,108.05,254.63,122.79,10.80">Measuring the Latency of Depression Detection in Social Media</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sadeque</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bethard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,254.97,254.63,229.48,10.80;12,108.05,268.38,213.24,10.80">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Eleventh ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="495" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,144.05,295.90,353.34,10.80;12,108.05,309.65,361.32,10.80;12,108.05,323.65,245.35,10.80" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,108.05,309.65,244.48,10.80">Efficient and Robust Automated Machine Learning</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Feurer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Eggensperger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Springberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,376.41,309.65,92.96,10.80;12,108.05,323.65,166.76,10.80">Neural Information Processing Systems 28</title>
		<imprint>
			<biblScope unit="page" from="2862" to="2970" />
		</imprint>
	</monogr>
	<note>NIPS 2015</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
