<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,403.82,15.42;1,89.29,106.66,84.44,15.42">Universal Passage Weighting Mecanism (UPWM) in BioASQ 9b</title>
				<funder ref="#_ZCskFwt">
					<orgName type="full">EU/EFPIA Innovative Medicines Initiative 2 Joint Undertaking</orgName>
				</funder>
				<funder>
					<orgName type="full">National Funds</orgName>
				</funder>
				<funder ref="#_dTandFc">
					<orgName type="full">FCT -Foundation for Science and Technology</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.93,134.97,71.84,11.96"><forename type="first">Tiago</forename><surname>Almeida</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Aveiro</orgName>
								<orgName type="institution" key="instit2">IEETA</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,173.42,134.97,63.78,11.96"><forename type="first">SÃ©rgio</forename><surname>Matos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Aveiro</orgName>
								<orgName type="institution" key="instit2">IEETA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,403.82,15.42;1,89.29,106.66,84.44,15.42">Universal Passage Weighting Mecanism (UPWM) in BioASQ 9b</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">FE0E34C8C977C352A98578F703822C79</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Neural ranking</term>
					<term>Sentence aggregation</term>
					<term>Document Retrieval</term>
					<term>Snippet Retrieval</term>
					<term>BioASQ 9B</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the participation of the University of Aveiro Biomedical Informatics and Techologies group (BIT) in the ninth edition of the BioASQ challenge for document and snippet retrieval. Our proposed systems follow a two-stage retrieval pipeline, similar to our BioASQ 8B submissions. However, we completely rebuilt our neural ranking model, maintaining the key ideas of its inception while improving its computational efficiency as well as adding interoperability with the transformer architecture. This resulted in a novel universal passage weighting mechanism (UPWM), which offers a more powerful way to derive a document relevance score from the combination of its sentences. More concretely, we have built two variants that use our passage mechanism, the lightning UPWM and the transformer UPWM. The first uses a shallow interaction model and the second uses a BERT model. Additionally, we also propose an effective pairwise joint training mechanism that combines document retrieval with snippet retrieval. Our systems achieved competitive results scoring at the top and close to the top for all the batches, with MAP values ranging from 0.3573 to 0.4236 in the document retrieval task. Although we only submitted for the snippet retrieval task in the last two batches, our system scored at top position in the last batch by using rank reciprocal fusion of pointwise and pairwise joint training approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The BioASQ <ref type="bibr" coords="1,147.35,456.29,12.94,10.91" target="#b0">[1]</ref> challenge is an annual competition on document classification, retrieval and question-answering applied to the biomedical domain. This competition is notorious for continuously fostering the research in automatic and intelligent retrieval systems over the biomedical literature. A fresh example is the difficulty that researchers and biomedical experts have to search the growing literature about the 2019 novel coronavirus, showing us that new and more powerful methods are still needed.</p><p>More concretely, the BioASQ challenge is divided into two tasks (A and B) that are isolated challenges. Task A addresses the biomedical annotation problem, and is concerned with automatic document labelling with terms from the MeSH hierarchy. On the other hand, task B is further subdivided into phase A and phase B, the first addressing the information retrieval problem and the second addressing the answer extraction and answer generation problems.</p><p>In more detail, in phase A the objective is to retrieve, from the PubMed baseline, the most relevant articles and/or document snippets that answer a given biomedical question written in English. In contrast, the objective in phase B is to extract or generate ideal answers from the information retrieved in phase A.</p><p>This paper describes our participation the participation of the Biomedical Informatics and Techologies (BIT) group of the Aveiro University in the BioASQ task B phase A challenge. Our approach is a direct evolution of the document retrieval approach used in our previous participation <ref type="bibr" coords="2,151.59,181.81,11.58,10.91" target="#b1">[2]</ref>. This year we focused on a ground up rebuilt of the previous model by maintaining the key ideas of its inception, while improving the computation flow for efficiency purposes and adding interoperability with the transformer architecture. In the end, we devised a universal passage weighting mechanism (UPWM) that offers a novel way to combine the sentence relevance in order to derive the final document score. This mechanism tries to solve the usual overlooked problem of sentence aggregation. More precisely, it is usually unfeasible to feed the entire document to a neural IR model and a common practice is to do sentence splitting followed by simple level sentence aggregation <ref type="bibr" coords="2,292.53,276.66,11.23,10.91" target="#b2">[3,</ref><ref type="bibr" coords="2,306.37,276.66,7.49,10.91" target="#b3">4]</ref>. However, little attention has been given to this important step.</p><p>We built two variants that use our passage mechanism, the lightning UPWM (L-UPWM) and the transformer UPWM (T-UPWM). The first uses a shallow interaction model, with only 597 trainable parameters, and the later uses a BERT model. These were the cornerstone models of our submissions for this year challenge for the document and snippet retrieval challenge.</p><p>Our submissions achieved the top and close to the top positions for all document retrieval batches. While we only participated in the last two batches for snippet retrieval, we still achieved a top scoring position in the last batch. Finally, our transformer UPWM consistently outperformed the lightning UPWM bringing to evidence the efficacy vs efficiency trade off.</p><p>In the remaining of the paper we start by describing our universal passage weighting mechanism and detailing the two concrete implementations, lightning UPWM and transformer UPWM. We then describe the submissions and present and discuss the results obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">UPWM</head><p>The universal passage weighting mechanism (UPWM) is, as the name suggests, a high-level mechanism that provides other neural architectures the capability to perform sentence score aggregation. In other words, this mechanism can be viewed as a wrapper that encapsulates already existing models offering them the capability to better combine the individual sentence scores in order to derive the final document score.</p><p>The UPWM is inspired by the human judgement process of selecting relevant articles, as proposed in previous works <ref type="bibr" coords="2,219.71,579.17,11.48,10.91" target="#b4">[5,</ref><ref type="bibr" coords="2,234.27,579.17,7.65,10.91" target="#b5">6]</ref>. More precisely, when searching for some information, a person usually scans a document looking for relevant sentences, signalled by the presence of keywords that are similar to their information need. Then the relevance of the entire document will be judged based only on the selected sentences. Therefore, the purpose of the UPWM is to mimic this judgement process and combine it with the neural relevance signal extracted by another model. So, this human judgement process can be viewed as a heuristic to guide neural models during the sentence aggregation step.</p><p>Another aspect taken into consideration by the UPWM is that query terms are not equally relevant. Similarly, in the human judgement process, when scanning a document only a few keywords, the most representative terms, are considered. So, based on this observation we also compute the importance that each query term carries and weigh each sentence by this importance, thus boosting the sentences that contain the most important query terms.  The idea is that the first block will produce the individual sentence scores that carry the sentence relevance. Then the document score will select the most representative features based on these sentences to derive the final document relevance with respect to the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Sentence Relevance Block</head><p>The sentence relevance block has two parallel layers, the Interaction Model layer and the A priori layer. Both layers produces scores for each sentence that will be linearly combined. The intuition here is that the interaction model layer focuses on thoroughly analysing the sentence information, while the a priori layer will act as a heuristic by mimicking the quick human judgement process of finding relevance. In other words, the main idea of the a priori layer is to produce sentence scores without thoroughly processing the sentences, therefore its name, since we will give an "a priori" score without analysing it.</p><p>Then, the final sentence score comes from the linear combination of the a priori scores with the interaction model scores. So, the a priori score will act as a gating mechanism deciding which scores from the interaction model should be considered for the document ranking. Another interpretation can be gained by considering the types of signals that both extract. Specifically, the interaction model layer will focus on analysing the meaning, context and sentence semantics, hence carrying a more semantic interaction signal, while the a priori will only focus on a more exact matching type of signal, weighted by the importance of each individually query term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Interaction Model Layer</head><p>The interaction model layer acts as a placeholder for neural models that are more specialised in analysing the relevance between the query and sentence. For a better fit, the type of models that make sense to adopt in this layer are models that derive the final relevance score by taking into consideration the meaning, context and semantic relations between the query and the sentence. For example, ARC-II <ref type="bibr" coords="4,184.85,161.73,13.00,10.91" target="#b6">[7]</ref> is an example of an earlier and simpler candidate. However, a more powerful transformer based model, like BERT <ref type="bibr" coords="4,294.12,175.28,12.78,10.91" target="#b7">[8]</ref> can also be adopted as the interaction model in this architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">A Priori Layer</head><p>The a priori layer implementation is summarised in Figure <ref type="figure" coords="4,347.62,238.16,4.97,10.91" target="#fig_2">2</ref> and has two major steps, the exact matching signal extraction and the query terms importance weighting. To better understand the layer inner workings, lets us first define a query as a sequence of tokens ð = {ð¢ 0 , ð¢ 1 , ..., ð¢ ð }, where ð¢ ð is the ð-th token of the query and ð the size of the query; a document passage as ð = {ð£ 0 , ð£ 1 , ..., ð£ ð }, where ð£ ð is the ð-th token of the passage and ð the size of the passage; and a document as a sequence of passages ð· = {ð 0 , ð 1 , ..., ð ð }, where ð is the total number of passages in the document. To further simplify the explanation lets consider that this layer is only applied to one sentence query pair, since the extension to ð pairs is trivially achieved by replicating this procedure for each pair.</p><p>Regarding the exact matching signal extraction, an interaction matrix is first created by applying an interaction function, ð ððð¡ððððð¡ððð (ð, ð), to the query and the passage. This function will perform a pairwise combination of all the query terms with all the passage terms. The output is a matrix, ð¼ [0,1] , where the rows are the query terms and the columns are the passage terms. We adopt two types of implementations for this function, an exact interaction and a semantic interaction. The first one directly uses the token index to create the matrix ð¼, therefore can be defined as follows</p><formula xml:id="formula_0" coords="5,244.80,96.14,261.18,29.86">ð¼ ðð = {ï¸ 1 ð¢ ð = ð£ ð 0 ðð¡âððð¤ðð ð .<label>(1)</label></formula><p>The second approach computes the cosine similarity between the embedding of the terms, described as follows,</p><formula xml:id="formula_1" coords="5,249.67,170.59,256.32,28.19">ð¼ ðð = ð¢ ð â ð â¢ ð£ ð â âð¢ ð â â 2 Ã âð£ ð â â 2 .<label>(2)</label></formula><p>In either case, the exact matching signal will always be captured, since it will correspond to the matrix entry that has the corresponding value of 1 or close to. The next step is to filter these exact matching signals. For that, we defined a matching threshold, that when applied over the interaction matrix returns a matrix with all matching terms between the query and the passage, i.e. it returns a matrix where each entry defines if a query term is present in the passage or not, as described in Equation 3</p><formula xml:id="formula_2" coords="5,224.47,294.70,281.52,30.93">ð¼ ðð = {ï¸ 1 ðð ð¼ ðð &gt;= threshold 0 ðð ð¼ ðð &lt; threshold .<label>(3)</label></formula><p>Note, that it only makes sense to apply this equation over the semantic interaction, since the exact interaction directly indicates all the exact matches.</p><p>Then, since we only care about the presence or absence of a term, we collapse the column dimension in ð¼, transforming it into a vector, ð¼ â [0,1] , where each entry indicates if a query-term is present or not in the passage. Note that each entry in ð¼ is a boolean value; we also tried different alternatives, such as using the number of times a query term appeared in a sentence, but this did not improve the results.</p><p>In parallel with this step, this layer also computes the importance of each query-term, since different terms in a query can carry different importance regarding the information need, as addressed by <ref type="bibr" coords="5,151.19,458.21,11.49,10.91" target="#b5">[6,</ref><ref type="bibr" coords="5,165.66,458.21,7.65,10.91" target="#b8">9]</ref>. To accomplishing this, we compute a probabilistic distribution over the query terms, as shown in Equation <ref type="formula" coords="5,247.18,471.76,3.74,10.91" target="#formula_3">4</ref>,</p><formula xml:id="formula_3" coords="5,251.90,494.49,254.09,45.27">ð ð¢ ð = ð¤ â ð â¢ ð¢ ð â , ð ð¢ ð = ð ðð¢ ð âï¸ ð¢ð¤âð ð ðð¢ ð¤ .<label>(4)</label></formula><p>Here, we used the standard softmax operation to compute the probabilistic distribution over a linear combination of each query term embedding, ð¢ ð â , and a trainable vector, ð¤. Finally, ð ð¢ ð corresponds to the probabilistic importance of the query-term ð¢ ð regarding the entire query.</p><p>Finally, the a priori sentence score arises from the linear combination of the query-importance distribution and the presence vector, ð¼ â , as described in Equation <ref type="formula" coords="5,376.78,599.40,3.74,10.91" target="#formula_4">5</ref>,</p><formula xml:id="formula_4" coords="5,255.80,623.88,250.19,25.61">ð ð = âï¸ ðâ ð ð ð¢ ð Ã ð¼ ð â .<label>(5)</label></formula><p>Given this formulation, ð ð , represents the importance of each sentence following the human judgement heuristic. Let us consider a passage containing all the query terms as an intuitive example. Under this condition the a priori sentence score will be 1, and contrarily, if a passage does not contain any term, it will have a score of zero. Similarly, if a sentence contains important query-terms, it will have a score close to 1, and close to zero on the opposite case.</p><p>An important note is to consider the role of the matching threshold. If too high, e.g., ð¡âððð âððð = 0.99 it will only consider exact match signals, i.e., terms that exactly appear in the passage. However, for lower values, it will also include semantically similar terms, e.g., a ð¡âððð âððð = 0.7 can be beneficial since it will dynamically include semantically similar terms, in some cases synonyms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Document Score Block</head><p>The document score block aims to produce the final document score based on the passages score previously obtained. To avoid creating a bias for longer documents, since they have more scores, we employed a feature selection step. More precisely, we construct a feature vector that contains the max sentence score, the normalised summation over the scores, the average over the scores and several top ð-max-average scores. Finally, the document score is computed by combining this feature vector using a multi-layer perceptron.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">UPWM Models</head><p>As previously mentioned, the UPWM is intended to be combined with an neural model that is capable of computing the relevance between the query and a sentence. So, in this section we present the two concrete implementation that we adopted for the BioASQ challenge, the lightning UPWM and the transformer UPWM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Lightning UPWM</head><p>The lightning UPWM, uses a neural interaction model similar to our last year submission <ref type="bibr" coords="6,89.29,461.83,11.58,10.91" target="#b1">[2]</ref>, which was already a fast and lightweight model <ref type="bibr" coords="6,330.08,461.83,16.41,10.91" target="#b9">[10]</ref>. However, with this much cleaner architecture we achieved a 4x speed up making it a lightning fast model, when compared to the current transformer based models. We show in Figure <ref type="figure" coords="6,189.32,626.91,5.17,10.91" target="#fig_3">3</ref> the overview of the neural architecture. Very briefly, an interaction matrix is built using Equation 2. Then 3 by 3 convolution kernels are applied over this matrix to learn context patterns that are then extracted by a pooling layer. This layer is applied over the filter dimension and we combined the max, average and k-max average pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Transformer UPWM</head><p>The transformer UPWM, as the name suggests, uses a transformer based model in the interaction model layer. More precisely, we chose PubMedBERT <ref type="bibr" coords="7,333.45,121.08,16.41,10.91" target="#b10">[11]</ref>, which is a BERT model that was trained from scratch using abstracts from PubMed and full-text articles from PubMed Central. This model keeps the biomedical specific terms, that would be decomposed into subwords if using other BERT models. This is an important aspect since we can directly use, in the a priori layer, the tokens produced by the PubMedBERT model.</p><p>To produce the relevance score between the query and the sentence, we adopted the usual strategy <ref type="bibr" coords="7,127.22,202.38,16.09,10.91" target="#b11">[12]</ref>, described in Figure <ref type="figure" coords="7,235.35,202.38,3.66,10.91" target="#fig_4">4</ref>, of concatenating the query tokens with the sentence tokens, separated by the [SEP] token. Then we feed this to the BERT model, that outputs a sequence of contextualised embedding per tokens. Finally, we feed the [CLS] embedding to a linear layer in order to produce the sentence score. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Overall Architecture</head><p>This section addresses the complete system architecture that we adopted for the BioASQ 9b challenge. Similarly to our last year submission, we used a two-stage reranking mechanism, as presented in Figure <ref type="figure" coords="7,178.16,502.52,3.74,10.91" target="#fig_5">5</ref>.</p><p>For the first stage we adopted a traditional retrieval model, BM25 <ref type="bibr" coords="7,397.69,516.07,16.39,10.91" target="#b12">[13]</ref>, to efficiently select the top-100 scientific articles for each biomedical question. Then this set of documents is fully reranked by our neural model, in this case, the lightning UPWM and transformer UPWM.</p><p>Given this pipeline and the operation of the UPWM as described above, we observe that the UPWM operates a detailed analysis of the exact match signals captured by the BM25. This makes sense, because according to the UPWM, the interaction model will only evaluate sentences that carry some exact match signal, which is the only type of signal used by BM25. Therefore, we can say that, in this pipeline, the UPWM will "look" more in depth to the signals that contributed for the BM25 score. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training</head><p>Regarding the training of the neural models, both were trained with the same exact data collection from BioASQ. In terms of document ranking, similarly to last years submission, we trained both models using a pairwise cross entropy loss, described in Equation <ref type="formula" coords="8,441.46,352.18,3.74,10.91" target="#formula_5">6</ref>,</p><formula xml:id="formula_5" coords="8,175.46,374.65,330.52,28.41">ð¿ ððð (ð, ð + , ð -) = -ððð( ð (ð ðððð(ð,ð + )) ð (ð ðððð(ð,ð + )) + ð (ð ðððð(ð,ð -)) ).<label>(6)</label></formula><p>Here, the loss is computed as a function over a triplet that contains a query, a positive document and a negative document. Since the BioASQ data only provides positive examples, we sampled the negatives examples from the documents in the BM25 ranking order that are not labelled as positives. Additionally, given that the gold-standard was built as a concatenation of the judged documents from different years, we decided to restrict the BM25 search by year so that only the available documents at that time are available for the training of the model.</p><p>Additionally to the document level training, we also tried to perform joint training by using the snippets feedback data available in the training data. Equation 7 describes how the overall loss is computed in the joint training approach, namely as a weighted average between the document loss and the snippet loss.</p><formula xml:id="formula_6" coords="8,230.88,557.83,275.11,11.50">ð¿ = ð¾ð¿ ððð Ã (1 -ð¾)ð¿ ð ðððððð¡ .<label>(7)</label></formula><p>Regarding the snippet loss, we experimented with pointwise and pairwise variants for the loss. Furthermore, we only applied the snippet loss to the sentences from the positive documents that had feedback.</p><p>Following the ideas of joint training presented in <ref type="bibr" coords="8,326.03,618.50,16.41,10.91" target="#b13">[14]</ref>, we further augmented our UPWM to produce new snippet scores, as presented in Figure <ref type="figure" coords="8,331.76,632.05,3.76,10.91" target="#fig_6">6</ref>. Note that the actual solution already produces snippet scores in the sentence relevance block. However, these do not depend on the final document score, therefore the snippet loss would not contribute, through back propagation, to the training of the document score block. So, we added a MLP that computes a new snippet score from the concatenation of the interaction model score, sentence score and document score. This way, the new snippet score is dependent on the final document score, making it a more joint train approach.</p><p>For the pointwise loss, we adopted the binary cross entropy loss described in Equation <ref type="formula" coords="9,488.05,347.27,3.74,10.91" target="#formula_7">8</ref>,</p><formula xml:id="formula_7" coords="9,195.65,374.37,310.33,11.36">ð¿ ð ðððððð¡ (ð¦, ð¦ Ë) = -(ð¦ððð(ð¦ Ë) + (1 -ð¦)ððð(ð¦ Ë)).<label>(8)</label></formula><p>Here, ð¦ correspond to the true relevance of a snippet, 1 for positive and 0 for negative, and ð¦ Ëcorresponds to the probability of a snippet being positive assigned by the model. From this definition, we also explored using label smoothing by considering a wide range of positive and negative values for ð¦.</p><p>For the pairwise loss, we adopted the pairwise cross entropy loss, already described in Equation <ref type="formula" coords="9,131.07,462.14,3.66,10.91" target="#formula_5">6</ref>. This pairwise loss is computed between all the positive snippets and all the negative snippets that belong to the same document. As before, a snippet loss is only computed over the snippets of a positive document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Submission</head><p>In this section, we start by describing the data collection and some pre-processing steps that are common to our official submission for all the batches. Then we describe each run that was submitted for the BioASQ 9b phase A challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Collection and Pre-processing</head><p>In this year's edition, the document collection was the 2020 PubMed/MEDLINE annual baseline consisting of almost 31 million articles. However, all the articles with missing abstract were discarded, meaning that around 21 million articles were indexed with ElasticSearch using the english text analyser, which performs tokenization, stemming and stopword filtering.</p><p>Regarding the neural model, we built a simple Regex based tokenizer and trained 200dimension word embeddings using the GenSim <ref type="bibr" coords="10,298.04,100.52,17.75,10.91" target="#b14">[15]</ref> implementation of the word2vec algorithm <ref type="bibr" coords="10,89.29,114.06,17.82,10.91" target="#b15">[16]</ref> over the 21 million abstracts from the baseline. Furthermore, when using the UPWM each document is split into a set of individual sentences through the Punkt algorithm <ref type="bibr" coords="10,448.31,127.61,16.25,10.91" target="#b16">[17]</ref>.</p><p>For the training data we used the BioASQ dataset with the exception of the last year test set that we used for validation purposes. With respect to the joint training, given that the snippet gold standard does not respect sentence boundaries, e.g. it can be composed of several sentence, we consider a sentence to be relevant (hard label of 1) if its text matches the text in a positive snippet. However, when using soft label, we instead use the overlap between the sentence text and the gold snippet text as the value for the soft label. The intuition is that a sentence that only partially belongs to a gold snippet should not be considered fully relevant, and therefore should not have a label of 1.</p><p>Regarding the first stage of the pipeline, we finetuned the BM25 parameters, ð1 and ð, for each batch by performing an extensive grid search. The validation data used for this process corresponds to the last year's test set for each corresponding batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Runs</head><p>This year, the document/snippet retrieval challenge received, on average across all batches, 27 submission from seven teams. Our group submitted five runs for each batch, which are identified by the prefix "bioinfo" on the official results <ref type="foot" coords="10,289.56,351.73,3.71,7.97" target="#foot_0">1</ref> . Table <ref type="table" coords="10,327.79,353.48,5.17,10.91">1</ref> presents a summarised description of the systems used in each run, where L-UPWM stans for lightning UPWM, T-UPWM stands for transformer UPWM, JT corresponds to joint training and "-&gt; RRF" means that we made an ensemble of several runs using the rank reciprocal fusion (RRF) method <ref type="bibr" coords="10,410.75,394.13,16.25,10.91" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Summary of the submitted runs for each round of the 2021 BioASQ 9B phase A. In more detail, for the first, second and third batches, as observable in Table <ref type="table" coords="10,436.46,541.64,3.70,10.91">1</ref>, we submitted the same base system configuration for each run. In some cases, we made an ensemble run that used several models trained with a slight difference in its hyperparameters. In these three batches we did not employ the joint training technique and therefore we did not submit any run for snippet retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run name</head><p>For the fourth batch, the main difference was the addition of the joint training methodology with pointwise snippet loss. Then in the fifth batch we only added the pairwise snippet loss for some models that used joint training. More precisely, runs "bioinfo-1" and "bioinfo-3" correspond to RRF ensembles of two models, one jointly trained using pointwise and the other using pairwise snippet loss.</p><p>Regarding snippet retrieval, the only runs in which we submitted snippets were the ones using joint training. Furthermore, to produce the ranked snippet list we followed a heuristic where the sentences presented on the top documents should have a higher probability of being relevant. Therefore, during snippet ranking we preserve this document order and only extract the snippets that score above a specific threshold. More precisely, for the forth batch we retrieved snippets from the top-10 retrieved documents, while for the fifth batch we only considered the top-1 retrieved document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results and Discussion</head><p>In this section, we separately address the document results and the snippet results, since we only submitted snippets for the last two batches. Note that at the time of writing only the preliminary results, regarding the systems' performance, were available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Document Retrieval</head><p>The overall results regarding the document retrieval task are shown in Table <ref type="table" coords="11,434.68,330.81,3.76,10.91" target="#tab_0">2</ref>, together with the median of all submissions, the top performing system in each batch, apart from our own submissions, and the baseline score obtained with BM25, corresponding to the ranking order without applying neural reranking. The results are organised in terms of Mean Average Precision at ten (MAP@10), which was the official measure adopted by the organisers to rank all the submissions. There were a total of 16, 30, 29, 27 and 28 submissions respectively for each batch. Looking at the results presented in Table <ref type="table" coords="11,294.15,582.85,3.81,10.91" target="#tab_0">2</ref>, from a general perspective our transformer UPWM model had top performing results, outscoring every other system in terms of MAP@10 in three batches while being competitive in the remaining two batches. On the opposite side, the lightning UPWM model achieved results closer to the median. This observation brings to evidence the trade off between efficacy and efficiency, since our lightning UPWM model is 63 times faster than the transformer approach with a GPU (K80) <ref type="bibr" coords="11,353.22,650.59,16.09,10.91" target="#b9">[10]</ref>. So, although it did not achieve the same kind of retrieval performance as the transformer counterpart, the lightning UPWM model may still be a viable or better solution depending on the perspective or requirements.</p><p>To better understand the effects of the neural ranking solution, we added the row "baseline" in Table <ref type="table" coords="12,128.67,127.61,3.81,10.91" target="#tab_0">2</ref>, which represents the BM25 ranking order that all of our neural solution used for reranking. We present in Figure <ref type="figure" coords="12,235.99,141.16,5.15,10.91" target="#fig_8">7</ref> the comparison between all the submission scores and the baseline score, to visualise the gains achieved by the neural reranking strategy. From an overall point of view, the neural reranking seems to be beneficial in achieving small increases in MAP@10 percentage points. The lightning UPWM model on the last batch was the only case were the neural reranking negatively influenced the baseline ranking order. This may be a consequence of using only the top 100 documents in the reranking phase. Therefore, we leave as future work the analysis of the impact of ð in the top-ð document selection for reranking. Figure <ref type="figure" coords="12,168.07,423.23,4.97,10.91" target="#fig_8">7</ref> also clearly shows the difference in performance between the L-UPWM and the T-UPWM, since the gains of the solutions that used T-UPWM are clearly higher than the submissions that used the L-UPWM, as previously mentioned.</p><p>For a better context regarding the overall ranking positions, we show in Figure <ref type="figure" coords="12,439.78,463.87,4.97,10.91" target="#fig_9">8</ref> the difference between our best system submission at each batch against the median score at that batch. As observable, our best submission achieved results that are clearly superior when compared to the median for all the batches. This is true even when our best submission was not in the top-5 (batch 3), which shows that the top scores were highly competitive and close.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Snippet Retrieval</head><p>The main results regarding the snippet retrieval task are shown in Table <ref type="table" coords="12,417.10,567.80,3.79,10.91" target="#tab_0">2</ref>, together with the median of all submissions, the top performing system in each batch. Moreover, since we only submitted an experimental run for the fourth and fifth batches, we applied the bioinfo-3 system submitted for the fifth batch to the remaining batches, to gain an idea of the performance that our best snippet solution could have achieved. The results are organised according to the harmonic mean of precision and recall at ten (F1@10), which was the official measure adopted by the organisers. There were a total of 11, 19, 17, 18 and 19 submissions respectively for each batch.  As shown in in Table <ref type="table" coords="13,199.49,425.26,3.81,10.91" target="#tab_1">3</ref>, our last approach, in batch five achieved a top scoring result. Additionally, when applying this model to the remaining batches the performance is consistent, with results in the top-3.</p><p>In Figure <ref type="figure" coords="13,142.97,465.91,3.72,10.91" target="#fig_10">9</ref>, we show a comparison between the performance of the bioinfo-3 system (batch 5) in all batches against the median snippet results. Again, these results show the consistent performance of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Joint Training Benefits</head><p>In order to assess the effect of the joint training methodology, we present in Figure <ref type="figure" coords="13,451.94,542.73,9.94,10.91" target="#fig_11">10</ref> the difference between the same model with and without using the joint training approach. When looking at the lightning UPWM (L-UPWM) joint training lead to a marginal improvement on batch fourth and a clear more positive impact on the fifth batch. Otherwise, the transformer UPWM (T-UPWM) implementations clearly underperform when using the joint training methodology. From these results joint training seems only to be beneficial to the smaller model. The main reason to explain this behaviour may be related to the amount of training data available. Since we build the training set to always output documents that had positive snippets, this had a consequence of reducing the amount of training data available when compared with the version that is only trained with document feedback. More precisely, the joint training approach uses  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper we propose a novel sentence aggregation technique, named universal passage weighting mechanism (UPWM), that can be combined with neural interaction based models. We demonstrate two simple implementations, one a fast and lightweight variant called lightning UPWM (L-UPWM) and a larger one, relying in the success of the transformer architecture, the transformer UPWM (T-UPWM). The first uses a simple CNN and pooling architecture, while the later uses the BERT model as the interaction based model in the UPWM architecture.</p><p>We submitted runs with both implementations to the BioASQ 9b phase A challenge, addressing the document and snippet retrieval tasks. For document task our best solution, T-UPWM, was able to outperform all the other system in three of the five batches, while remaining competitive in the others. The L-UPWM showed an inferior performance, which was expected given that is a much lighter model that is 63 times faster than the T-UPWM. In both cases, neural reranking was generally beneficial when looking at the performance gains against the BM25 baseline. Finally, we also propose a joint training approach that showed encouraging results, leaving a clear open path for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,310.67,317.77,8.93"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A general overview of the Universal passage weighting mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,100.20,340.32,405.79,10.91;3,89.29,353.87,418.37,10.91;3,88.96,367.41,417.02,10.91;3,89.29,380.96,416.70,10.91;3,89.29,394.51,333.75,10.91;3,90.13,163.88,412.48,134.22"><head>Figure 1</head><label>1</label><figDesc>Figure1presents an overview of the main architectural concepts of the UPWM, which is divided into two major blocks, the sentence relevance block and the document score block. The idea is that the first block will produce the individual sentence scores that carry the sentence relevance. Then the document score will select the most representative features based on these sentences to derive the final document relevance with respect to the query.</figDesc><graphic coords="3,90.13,163.88,412.48,134.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,89.29,436.89,311.53,8.96;4,129.72,274.41,333.33,149.91"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The main concepts behind the inner workings of the a priori layer</figDesc><graphic coords="4,129.72,274.41,333.33,149.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,89.29,594.81,343.52,8.93;6,129.72,509.20,333.30,73.05"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A simplistic overview of the neural architecture for the interaction model.</figDesc><graphic coords="6,129.72,509.20,333.30,73.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,89.29,400.70,253.68,8.93;7,129.72,265.74,333.36,122.40"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Diagram of the interaction model as a BERT model.</figDesc><graphic coords="7,129.72,265.74,333.36,122.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="8,89.29,266.92,221.09,8.93;8,129.72,84.18,333.31,170.17"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Overview of our two-stage retrieval system.</figDesc><graphic coords="8,129.72,84.18,333.31,170.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="9,89.29,255.47,135.31,8.93;9,98.46,84.20,395.80,158.71"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Pointwise snippet loss.</figDesc><graphic coords="9,98.46,84.20,395.80,158.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="10,300.24,450.62,48.10,8.12;10,150.61,461.49,62.25,8.12;10,270.41,461.49,30.68,8.12;10,412.96,461.49,30.68,8.12;10,101.20,472.76,33.29,8.06;10,163.80,472.76,35.87,8.06;10,254.12,472.76,63.26,8.06;10,396.66,472.76,63.26,8.06;10,101.20,484.00,33.29,8.06;10,150.11,484.00,348.36,8.06;10,101.20,495.23,33.29,8.06;10,163.96,495.23,35.56,8.06;10,254.27,495.23,62.96,8.06;10,396.82,495.23,62.96,8.06;10,101.20,506.46,33.29,8.06;10,150.26,506.46,348.05,8.06;10,101.20,517.69,33.29,8.06;10,154.20,517.69,55.07,8.06;10,238.45,517.69,94.59,8.06;10,381.00,517.69,94.59,8.06"><head></head><label></label><figDesc>-&gt; RRF L-UPWM + JT(Pointwise) -&gt; RRF L-UPWM + JT(Point/Pairwise) -&gt; RRF bioinfo-2 T-UPWM T-UPWM -&gt; RRF T-UPWM -&gt; RRF bioinfo-3 T-UPWM -&gt; RRF T-UPWM + JT(Pointwise) -&gt; RRF T-UPWM + JT(Point/Pairwise) -&gt; RRF bioinfo-4 RRF of all runs T-UPWM + JT(Pointwise) T-UPWM + JT(Pointwise)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="12,89.29,325.83,295.86,8.93;12,89.29,177.43,416.69,123.89"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Neural reranking gains when compared to the BM25 baseline.</figDesc><graphic coords="12,89.29,177.43,416.69,123.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="13,89.29,248.39,416.70,8.93;13,89.29,260.40,25.14,8.87;13,98.46,84.19,395.86,156.50"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: MAP@10 difference between our best run at each batch against the median score at that batch.</figDesc><graphic coords="13,98.46,84.19,395.86,156.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="14,89.29,280.40,418.22,8.93;14,98.46,84.19,395.86,183.65"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: F1@10 difference between our best run at each batch against the median score of that batch.</figDesc><graphic coords="14,98.46,84.19,395.86,183.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="14,89.29,590.14,418.23,8.93;14,98.46,340.72,395.86,236.86"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Comparison of both UPWM implementations with and without the joint train methodology.</figDesc><graphic coords="14,98.46,340.72,395.86,236.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="11,88.99,427.56,409.25,139.34"><head>Table 2</head><label>2</label><figDesc>Summary of the results obtained for the document retrieval task.</figDesc><table coords="11,97.04,455.12,401.20,111.79"><row><cell>Run name</cell><cell cols="2">Batch 1</cell><cell cols="2">Batch 2</cell><cell cols="2">Batch 3</cell><cell cols="2">Batch 4</cell><cell cols="2">Batch 5</cell></row><row><cell></cell><cell cols="10">Rank MAP Rank MAP Rank MAP Rank MAP Rank MAP</cell></row><row><cell>bioinfo-0</cell><cell>15</cell><cell>31.73</cell><cell>18</cell><cell>33.80</cell><cell>16</cell><cell>35.50</cell><cell>12</cell><cell>38.45</cell><cell>21</cell><cell>30.09</cell></row><row><cell>bioinfo-1</cell><cell>8</cell><cell>32.96</cell><cell>15</cell><cell>35.37</cell><cell>13</cell><cell>36.22</cell><cell>11</cell><cell>38.49</cell><cell>15</cell><cell>31.86</cell></row><row><cell>bioinfo-2</cell><cell>3</cell><cell>35.15</cell><cell>6</cell><cell>37.84</cell><cell>10</cell><cell>38.24</cell><cell>1</cell><cell>42.36</cell><cell>1</cell><cell>35.86</cell></row><row><cell>bioinfo-3</cell><cell>1</cell><cell>35.73</cell><cell>4</cell><cell>38.13</cell><cell>8</cell><cell>39.35</cell><cell>7</cell><cell>40.42</cell><cell>8</cell><cell>34.40</cell></row><row><cell>bioinfo-4</cell><cell>2</cell><cell>35.25</cell><cell>5</cell><cell>37.87</cell><cell>9</cell><cell>38.65</cell><cell>8</cell><cell>40.42</cell><cell>7</cell><cell>34.61</cell></row><row><cell>Baseline (BM25)</cell><cell></cell><cell>31.03</cell><cell></cell><cell>33.20</cell><cell></cell><cell>35.94</cell><cell></cell><cell>36.86</cell><cell></cell><cell>32.89</cell></row><row><cell>Median</cell><cell></cell><cell>32.93</cell><cell></cell><cell>34.85</cell><cell></cell><cell>35.64</cell><cell></cell><cell>38.13</cell><cell></cell><cell>32.03</cell></row><row><cell>Top competitor</cell><cell>4</cell><cell>34.60</cell><cell>1</cell><cell>39.90</cell><cell>1</cell><cell>40.40</cell><cell>2</cell><cell>41.92</cell><cell>2</cell><cell>35.37</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="13,88.99,291.28,409.63,110.17"><head>Table 3</head><label>3</label><figDesc>Summary of the results obtained for the snippet retrieval task.</figDesc><table coords="13,96.65,318.71,401.97,82.74"><row><cell>Run name</cell><cell cols="2">Batch 1</cell><cell cols="2">Batch 2</cell><cell cols="2">Batch 3</cell><cell cols="2">Batch 4</cell><cell cols="2">Batch 5</cell></row><row><cell></cell><cell>Rank</cell><cell>F1</cell><cell>Rank</cell><cell>F1</cell><cell>Rank</cell><cell>F1</cell><cell>Rank</cell><cell>F1</cell><cell>Rank</cell><cell>F1</cell></row><row><cell>bioinfo-1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>18</cell><cell>10.67</cell><cell>5</cell><cell>15.57</cell></row><row><cell>bioinfo-3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>14</cell><cell>14.25</cell><cell>1</cell><cell>17.68</cell></row><row><cell>bioinfo-4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>13</cell><cell>15.03</cell><cell>2</cell><cell>16.83</cell></row><row><cell>Post-challenge (bioinfo-3)</cell><cell>(3)</cell><cell>16.78</cell><cell>(1)</cell><cell>19.71</cell><cell>(3)</cell><cell>20.02</cell><cell>(2)</cell><cell>19.24</cell><cell></cell><cell></cell></row><row><cell>Median</cell><cell></cell><cell>11.32</cell><cell></cell><cell>14.86</cell><cell></cell><cell>16.84</cell><cell></cell><cell>17.18</cell><cell></cell><cell>14.28</cell></row><row><cell>Top result</cell><cell>1</cell><cell>18.45</cell><cell>1</cell><cell>18.16</cell><cell>1</cell><cell>20.42</cell><cell>1</cell><cell>20.61</cell><cell>3</cell><cell>16.73</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="10,108.93,671.01,196.22,8.97"><p>http://participants-area.bioasq.org/results/9b/phaseA/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work has received support from the <rs type="funder">EU/EFPIA Innovative Medicines Initiative 2 Joint Undertaking</rs> under grant agreement No <rs type="grantNumber">806968</rs> and from <rs type="funder">National Funds</rs> through the <rs type="funder">FCT -Foundation for Science and Technology</rs>, in the context of the grant <rs type="grantNumber">2020.05784</rs>.BD.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ZCskFwt">
					<idno type="grant-number">806968</idno>
				</org>
				<org type="funding" xml:id="_dTandFc">
					<idno type="grant-number">2020.05784</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="15,112.66,431.77,395.17,10.91;15,112.66,445.32,395.17,10.91;15,112.66,458.87,395.17,10.91;15,111.87,472.42,394.11,10.91;15,112.66,485.97,395.01,10.91;15,112.66,499.52,168.81,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="15,218.64,472.42,287.34,10.91;15,112.66,485.97,214.03,10.91">An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tsatsaronis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Balikas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Malakasiotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Partalas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zschunke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Alvers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>WeiÃenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Polychronopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Almirantis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Baskiotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Artieres</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A.-C</forename><surname>Ngonga Ngomo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Heino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Barrio-Alvers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12859-015-0564-6</idno>
	</analytic>
	<monogr>
		<title level="j" coord="15,343.15,485.97,94.70,10.91">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">138</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,513.06,393.33,10.91;15,112.66,526.61,394.53,10.91;15,112.14,540.16,395.05,10.91;15,112.66,553.71,394.53,10.91;15,112.66,567.26,243.88,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="15,234.28,513.06,271.70,10.91;15,112.66,526.61,119.98,10.91">UA at BioASQ 8: Lightweight neural document ranking with zero-shot snippet retrieval</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Matos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bit</forename></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_161.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="15,112.14,540.16,329.65,10.91">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="15,315.56,553.71,148.93,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>NÃ©vÃ©ol</surname></persName>
		</editor>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25, 2020. 2696. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,580.81,393.33,10.91;15,112.66,594.36,393.32,10.91;15,111.79,607.91,395.49,10.91;15,112.66,621.46,315.54,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="15,202.16,580.81,195.60,10.91">Passage retrieval based on language models</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<idno type="DOI">10.1145/584792.584854</idno>
		<ptr target="https://doi.org/10.1145/584792.584854.doi:10.1145/584792.584854" />
	</analytic>
	<monogr>
		<title level="m" coord="15,421.72,580.81,84.27,10.91;15,112.66,594.36,393.32,10.91;15,111.79,607.91,11.83,10.91">Proceedings of the Eleventh International Conference on Information and Knowledge Management, CIKM &apos;02</title>
		<meeting>the Eleventh International Conference on Information and Knowledge Management, CIKM &apos;02<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="375" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,635.01,395.16,10.91;15,112.66,648.56,393.33,10.91;15,112.66,662.11,395.17,10.91;16,112.66,86.97,395.01,10.91;16,112.66,100.52,155.44,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="15,188.98,635.01,318.85,10.91;15,112.66,648.56,20.81,10.91">Deeper text understanding for ir with contextual neural language modeling</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3331184.3331303</idno>
		<ptr target="https://doi.org/10.1145/3331184.3331303.doi:10.1145/3331184.3331303" />
	</analytic>
	<monogr>
		<title level="m" coord="15,160.60,648.56,345.39,10.91;15,112.66,662.11,228.67,10.91">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR&apos;19</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR&apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machin-ery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="985" to="988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,114.06,393.33,10.91;16,112.66,127.61,394.62,10.91;16,112.31,141.16,218.89,10.91" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="16,338.79,114.06,167.20,10.91;16,112.66,127.61,206.77,10.91">Deeprank: A new deep architecture for relevance ranking in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1710.05649" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,154.71,394.53,10.91;16,112.66,168.26,394.53,10.91;16,112.28,181.81,395.38,10.91;16,112.66,195.36,28.67,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="16,212.99,154.71,289.21,10.91">Calling attention to passages for biomedical question answering</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Matos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,112.28,181.81,155.90,10.91">Advances in Information Retrieval</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>MagalhÃ£es</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Castells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Silva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Martins</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="69" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,208.91,393.33,10.91;16,112.66,222.46,393.33,10.91;16,112.66,236.01,394.53,10.91;16,112.66,249.56,110.27,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="16,244.54,208.91,261.45,10.91;16,112.66,222.46,122.63,10.91">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,263.36,222.46,242.63,10.91;16,112.66,236.01,181.93,10.91;16,343.98,236.01,32.74,10.91">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2042" to="2050" />
		</imprint>
	</monogr>
	<note>NIPS&apos;14</note>
</biblStruct>

<biblStruct coords="16,112.66,263.11,393.33,10.91;16,112.66,276.66,395.01,10.91;16,112.66,290.20,187.21,10.91" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="16,319.43,263.11,186.56,10.91;16,112.66,276.66,180.57,10.91">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1810.04805" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,303.75,394.53,10.91;16,112.66,317.30,203.18,10.91" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="16,245.25,303.75,232.31,10.91">A deep relevance matching model for ad-hoc retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Croft</surname></persName>
		</author>
		<idno type="DOI">10.1145/2983323.2983769</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,330.85,394.53,10.91;16,112.66,344.40,393.53,10.91;16,112.66,357.95,394.53,10.91;16,112.66,371.50,394.96,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="16,218.56,330.85,284.57,10.91">Benchmarking a transformer-FREE model for ad-hoc retrieval</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Matos</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2021.eacl-main.293" />
	</analytic>
	<monogr>
		<title level="m" coord="16,127.76,344.40,378.43,10.91;16,112.66,357.95,390.21,10.91">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Association for Computational Linguistics</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3343" to="3353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,385.05,394.53,10.91;16,112.66,398.60,394.53,10.91;16,112.66,412.15,158.38,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:arXiv:2007.15779</idno>
		<title level="m" coord="16,112.66,398.60,390.01,10.91">Domain-specific language model pretraining for biomedical natural language processing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,425.70,394.61,10.91;16,112.66,439.25,239.89,10.91" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="16,208.82,425.70,135.31,10.91">Passage re-ranking with BERT</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1901.04085" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,452.79,394.53,10.91;16,112.66,466.34,312.26,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="16,239.49,452.79,267.70,10.91;16,112.66,466.34,25.35,10.91">The probabilistic relevance framework: Bm25 and beyond, Found</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<idno type="DOI">10.1561/1500000019</idno>
	</analytic>
	<monogr>
		<title level="j" coord="16,145.80,466.34,71.76,10.91">Trends Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="333" to="389" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,479.89,393.33,10.91;16,112.66,493.44,393.33,10.91;16,112.66,506.99,375.81,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="16,371.47,479.89,134.52,10.91;16,112.66,493.44,89.27,10.91">AUEB at BioASQ 7: Document and snippet retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G.-I</forename><surname>Brokos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,355.09,493.44,150.90,10.91;16,112.66,506.99,101.82,10.91">Machine Learning and Knowledge Discovery in Databases</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Cellier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Driessens</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="607" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,520.54,394.61,10.91;16,112.66,534.09,394.53,10.91;16,112.30,547.64,319.79,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="16,209.74,520.54,276.93,10.91">Software Framework for Topic Modelling with Large Corpora</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>ÅehÅ¯Åek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sojka</surname></persName>
		</author>
		<ptr target="http://is.muni.cz/publication/884893/en" />
	</analytic>
	<monogr>
		<title level="m" coord="16,112.66,534.09,358.88,10.91">Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks</title>
		<meeting>the LREC 2010 Workshop on New Challenges for NLP Frameworks<address><addrLine>Valletta, Malta</addrLine></address></meeting>
		<imprint>
			<publisher>ELRA</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,561.19,393.33,10.91;16,112.26,574.74,393.73,10.91;16,112.66,588.29,393.33,10.91;16,112.28,601.84,249.23,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="16,368.44,561.19,137.54,10.91;16,112.26,574.74,203.16,10.91">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,338.64,574.74,167.34,10.91;16,112.66,588.29,263.27,10.91;16,430.78,588.29,33.49,10.91">Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note>NIPS&apos;13</note>
</biblStruct>

<biblStruct coords="16,112.66,615.39,393.33,10.91;16,112.66,628.93,397.48,10.91;16,112.36,644.93,138.90,7.90" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="16,190.19,615.39,242.43,10.91">Unsupervised multilingual sentence boundary detection</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Strunk</surname></persName>
		</author>
		<idno type="DOI">10.1162/coli.2006.32.4.485</idno>
		<ptr target="https://www.aclweb.org/anthology/J06-4003.doi:10.1162/coli.2006.32.4.485" />
	</analytic>
	<monogr>
		<title level="j" coord="16,440.59,615.39,65.40,10.91;16,112.66,628.93,48.66,10.91">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="485" to="525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,656.03,393.33,10.91;16,112.66,669.58,393.33,10.91;17,112.66,86.97,394.53,10.91;17,112.28,100.52,395.00,10.91;17,112.31,114.06,312.30,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="16,305.76,656.03,200.23,10.91;16,112.66,669.58,169.31,10.91">Reciprocal rank fusion outperforms condorcet and individual rank learning methods</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Buettcher</surname></persName>
		</author>
		<idno type="DOI">10.1145/1571941.1572114</idno>
		<ptr target="https://doi.org/10.1145/1571941.1572114.doi:10.1145/1571941.1572114" />
	</analytic>
	<monogr>
		<title level="m" coord="16,307.29,669.58,198.70,10.91;17,112.66,86.97,390.58,10.91">Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;09</title>
		<meeting>the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="758" to="759" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
