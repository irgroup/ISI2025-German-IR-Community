<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,373.56,15.42;1,89.29,106.66,377.52,15.42;1,89.29,128.58,180.69,15.43">End-to-end Biomedical Question Answering via Bio-AnswerFinder and Discriminative Language Representation Models</title>
				<funder>
					<orgName type="full">NIDDK Information Network</orgName>
				</funder>
				<funder ref="#_k7PhXUx">
					<orgName type="full">NIH&apos;s National Institute of Diabetes and Digestive and Kidney Diseases</orgName>
					<orgName type="abbreviated">NIDDK</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,89.29,156.89,108.05,11.96"><forename type="first">Ibrahim</forename><forename type="middle">Burak</forename><surname>Ozyurt</surname></persName>
							<email>iozyurt@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Neurosciences</orgName>
								<orgName type="laboratory">FDI Lab</orgName>
								<orgName type="institution">University of California at San Diego</orgName>
								<address>
									<settlement>La Jolla</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,373.56,15.42;1,89.29,106.66,377.52,15.42;1,89.29,128.58,180.69,15.43">End-to-end Biomedical Question Answering via Bio-AnswerFinder and Discriminative Language Representation Models</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">EDC1C18BCD022B691806190ABBA40DFF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>question answering</term>
					<term>language representation models</term>
					<term>biomedical information retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative Transformers based language representation models such as BERT and its biomedical domain adapted version BioBERT have been shown to be highly effective for biomedical question answering. Here, discriminative, sample-efficient biomedical language representation models based on ELEC-TRA language representation model architecture were introduced to enhance an end-to-end biomedical question answering system, Bio-AnswerFinder, for the BioASQ challenge. The introduced language representation models outperformed other language models including BioBERT in answer span classification, answer candidate re-ranking and yes/no answer classification tasks. The resulting end-to-end system participated in BioASQ Synergy and both phases of Task 9B with promising results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Transformers based language representation models such as BERT <ref type="bibr" coords="1,400.97,401.50,11.58,10.91" target="#b0">[1]</ref>, XLNet <ref type="bibr" coords="1,453.42,401.50,13.00,10.91" target="#b1">[2]</ref> and AL-BERT <ref type="bibr" coords="1,118.56,415.05,13.00,10.91" target="#b2">[3]</ref> are becoming increasingly popular for many downstream NLP tasks due to their ubiquitous performance advantages over previous methods. Domain adaptation of general language model BERT to the biomedical domain has shown significant performance improvements for downstream biomedical NLP tasks <ref type="bibr" coords="1,261.25,455.70,11.43,10.91" target="#b3">[4]</ref>.</p><p>BERT, XLNet and ALBERT use a masked language modeling (MLM) approach by masking 15% of the training sentences and learning to guess the masked tokens in a generative manner resulting in only learning from 15% of the tokens per example. Recently a new pretraining approach named ELECTRA <ref type="bibr" coords="1,212.66,509.89,12.74,10.91" target="#b4">[5]</ref> is introduced for BERT transformer based encoder architecture, where a discriminative model is trained to detect whether each token in the corrupted input was replaced by a co-trained generator model sample or not. It is shown that ELECTRA is computationally more efficient than BERT and outperforms it given the same model size, data and computation resources <ref type="bibr" coords="1,210.63,564.09,11.30,10.91" target="#b4">[5]</ref>. The improvements over BERT by ELECTRA is most impressive at small model sizes and that effectiveness translates to biomedical domain for domain adapted small ELECTRA models <ref type="bibr" coords="1,198.27,591.19,11.43,10.91" target="#b5">[6]</ref>.</p><p>The development and evaluation of a question answering system without an expert generated training/evaluation question answer data set is impossible. BioASQ, an EU-funded biomedical semantic indexing and question answering challenge <ref type="bibr" coords="2,328.01,114.06,11.37,10.91" target="#b6">[7,</ref><ref type="bibr" coords="2,342.11,114.06,8.96,10.91" target="#b7">8]</ref> yearly provides cumulative sets of biomedical question/gold standard answer data and evaluation platform for the advancement of biomedical question answering.</p><p>In this paper, enhancements to a sentence level end-to-end biomedical question answering system, Bio-AnswerFinder <ref type="bibr" coords="2,213.12,168.26,12.99,10.91" target="#b8">[9]</ref> to provide answers to all four types (factoid, list, yes/no and summary) of BioASQ challenge questions is introduced. To achieve this, three new biomedical domain adapted pretrained ELECTRA models are introduced. The introduced Bio-ELECTRA models are compared against many language representation models for question keyword selection, question answer span classification, answer candidate re-ranking, yes/no answer classification tasks showing superior performance. An abstractive summarization module based on the Transformers based text-to-text generation model T5 <ref type="bibr" coords="2,384.82,249.56,17.84,10.91" target="#b9">[10]</ref> is also introduced. The resulting system can answer any biomedical domain natural language questions and used in the the 9th BioASQ Challenge for the Synergy and 9B tasks.</p><p>The rest of the paper is organized as follows. After a brief overview of Bio-AnswerFinder and proposed enhancements, details of the pretraining of the ELECTRA based biomedical language models are provided. This is followed by the experiments on answer span classification for the BioASQ factoid/list questions, answer candidate re-ranking, search engine keyword selection and yes/no question answer determination. After the introduction of extractive and abstractive summarizer systems, details of the BioASQ Synergy and BioASQ 9B systems are provided. Following this, results of the challenge is discussed together with an error analysis on the BioASQ 8B ground truth data for factoid questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Overview of Bio-AnswerFinder</head><p>Bio-AnswerFinder <ref type="bibr" coords="2,172.46,443.67,12.74,10.91" target="#b8">[9]</ref> is a biomedical question answering system that takes a natural language question and returns a list of sentences from biomedical texts ranked in the order of confidence that they would answer the question. An overview of the system is shown in Figure <ref type="figure" coords="2,476.80,470.77,3.81,10.91" target="#fig_0">1</ref>. The original system is retrofitted with new modules to be able to provide answers for the four types of questions of the BioASQ Task B. The retrofitted modules together with the enhanced existing modules are shown in blue in Figure <ref type="figure" coords="2,254.35,511.42,3.74,10.91" target="#fig_0">1</ref>.</p><p>The modules of the Bio-AnswerFinder can be grouped logically into question processing, document processing and answer processing phases.</p><p>In the question processing phase, the natural language question is parsed, followed by the detection of the focus of the question. Afterwards, search keywords are selected from the words of the question using a supervised long short term memory (LSTM) <ref type="bibr" coords="2,419.04,579.17,17.83,10.91" target="#b10">[11]</ref> based keyword classifier. For BioASQ 9B, this module is replaced by a Bio-ELECTRA++ <ref type="bibr" coords="2,421.17,592.72,13.00,10.91" target="#b5">[6]</ref> based keyword tagger.</p><p>In the document processing phase, query relevant documents are retrieved from a traditional keyword based information retrieval system (Elasticsearch). Bio-AnswerFinder uses an iterative most specific to most generic keyword search guided by the keyword classifier selected keywords to retrieve a relevant set of documents from an Elasticsearch index. The order of keywords dropped from iteration to iteration is learned from a set of annotated BioASQ 5B questions using a ranking classifier based on RankNet <ref type="bibr" coords="3,286.99,100.52,17.91,10.91" target="#b11">[12]</ref> with LSTM using attention <ref type="bibr" coords="3,430.53,100.52,16.25,10.91" target="#b12">[13]</ref>.</p><p>In the answer processing phase, the question type (focus, definition question or other) is detected. Definition questions are handled by definition pattern based filtering of the sentences from the retrieved documents. For questions with a focus a detected entity type, the entity type is used for filtering out candidate sentences not having entities of the focus entity type. For both focus and other non-definition questions, the answer candidate sentences are ranked by a weighted version of the relaxed word mover's distance <ref type="bibr" coords="3,330.78,181.81,16.09,10.91" target="#b13">[14]</ref>. Afterwards, up to first 100 of these sentences are further re-ranked by a fine-tuned BERT <ref type="bibr" coords="3,330.65,195.36,12.84,10.91" target="#b0">[1]</ref> classifier.</p><p>For BioASQ 9, the BERT re-ranker is replaced by a better performing Bio-ELECTRA reranker. For factoid and list questions, a Bio-ELECTRA based question answer span classifier is used. For yes/no questions, two different Bio-ELECTRA based classification approaches are introduced. For summary questions, both extractive and abstractive summarization approaches are introduced. These approaches are explained in more detail in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ELECTRA Based Biomedical Language Representation Models</head><p>For pretraining corpus both PubMed abstracts and PubMed Central (PMC) open access fulllength papers were used. The main pretraining corpus was built using 21.2 million PubMed abstracts from the January 2021 baseline distribution. From the abstracts, title and abstract text sentences were extracted resulting in a corpus of 3.6 billion words. The second 12.3 billion words corpus was built using the sentences extracted from the sections of PMC open access papers excluding the references sections, which, unlike the other paper sections, do not have a regular sentence format. A domain specific word piece vocabulary was generated using SentencePiece byte-pair-encoding (BPE) model <ref type="bibr" coords="3,234.48,434.51,17.88,10.91" target="#b14">[15]</ref> from PubMed abstract texts. The Bio-ELECTRA Mid and Base models were pretrained for one million steps on the PubMed abstracts corpus followed by 200,000 steps training on the PMC open access papers corpus. The Bio-ELECTRA Mid Combined model was pretrained on the combination of abstract and full text paper corpus for 1.2 million steps.</p><p>Since the improvements over BERT and other transformers based language models by ELEC-TRA are pronounced at small model sizes <ref type="bibr" coords="3,273.73,515.80,11.32,10.91" target="#b4">[5]</ref>, a mid sized model with a hidden layer size in the middle of the small and base ELECTRA architectures is introduced to investigate its competitiveness against the base model with a more than twice training parameters size. The Bio-ELECTRA model architectures are summarized in Table <ref type="table" coords="3,285.43,556.45,3.66,10.91" target="#tab_0">1</ref>. All the mid and base sized Bio-ELECTRA models were pretrained on a single 8 core version 3 tensor processing unit (TPU) with 128 GB of RAM. The small Bio-ELECTRA++ model <ref type="bibr" coords="3,245.84,583.55,12.95,10.91" target="#b5">[6]</ref> was pretrained on a consumer grade 8 GB Nvidia RTX 2070 GPU. The hardware and pretraining times for all Bio-ELECTRA models were summarized in Table <ref type="table" coords="3,127.71,610.65,3.74,10.91" target="#tab_1">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments with Factoid/List Question Answer Span Detection</head><p>Since most factoid and list questions can be answered by a word or phrase (multiple word/phrases for list questions), the answers can be detected by learning to estimate scores for spans in the sequence of tokens of an answer candidate passage. To this end, the training/testing sets were generated from the factoid and list questions of the BioASQ 8b training data. From about 30% of the list and factoid questions which cannot aligned to their exact answers, 152 more questions were recovered via manual inspection for synonyms and transliterations. The labeled data set is split into 85%/15% training/testing data sets of size 9557 and 1809, respectively. To increase performance over the smaller BioASQ data, the training set was combined with the out-of-domain SQuAD <ref type="bibr" coords="5,193.20,324.79,17.91,10.91" target="#b15">[16]</ref> v1.1 data set. All together ten language representation models including BERT based biomedical domain specific BioBERT model were evaluated for factoid and list question answer span detection task. The performance of the models are evaluated by the standard SQUAD evaluation metrics, exact match and ùêπ 1 score. Ten randomly initialized answer span classifiers are fine-tuned for each language representation model. The experiment results are summarized in Table <ref type="table" coords="5,481.39,392.54,3.81,10.91" target="#tab_2">3</ref>. All non-small Bio-ELECTRA had significantly outperformed BioBERT on this task. Given that mid sized Bio-ELECTRA models have less than half of the parameters of BioBERT, the results are very encouraging. The best performing Bio-ELECTRA Mid pretrained for 1.2 million steps was chosen to be used in the final system.</p><p>Snippets provided by BioASQ challenge were first passed through Bio-AnswerFinder bypassing the candidate document retrieval section. The re-ranked candidate sentences were then used as input for the factoid/list question classifier. The answer candidate word sequences were scored by a combination of their span classification probabilities, number of occurrence and rank of the sentence in which they have occurred first. The answer candidates were normalized and filtered to remove sub-phrases, singular/plural differences and acronyms. For list questions, answers candidates were enriched by coordinated phrase detection and processing. A classifier score threshold of 0.65 was selected to maximize ùêπ 1 performance on a holdout set of questions for selecting a subset of list question answer candidate span of words for the BioASQ challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments with Answer Candidate Re-ranking</head><p>In Bio-AnswerFinder, answer candidate sentences are first ranked by the inverse document frequency weighted relaxed word mover's distance on PubMed abstract trained GloVe word and phrase embeddings. While this ranking usually results in decent results, supervised re-ranking improves performance as measured on blind, multiple curator tests <ref type="bibr" coords="5,387.58,667.95,11.36,10.91" target="#b8">[9]</ref>. By casting the ranking problem as a 0/1 loss classification problem, the learned probability estimates can be used to rank the candidate sentences by relevance. For Bio-AnswerFinder, up to 100 answer candidates per question as returned by the weighted rWMD ranker were annotated as relevant or not (up to the first occurrence of a correct answer). The questions were selected from the BioASQ 5b training set. In total, 44933 sentences for 492 training questions and 9064 sentences for 100 testing questions were annotated.</p><p>Nine language representation models were tested. Due to highly unbalanced nature of the data set (on average one positive example per 99 negative examples), a weighted loss function where the errors for the positive examples weighted 99 times more than a negative example error was also used. The mean reciprocal rank (MRR) results averaged on ten randomly initialized runs using 14 language representation models (including weighted models) are summarized in Table <ref type="table" coords="6,128.16,455.66,3.79,10.91" target="#tab_3">4</ref>. Based on the results, Bio-ELECTRA Mid (1M) was chosen for BioASQ 9 challenge since it had more stable score distribution than the Bio-ELECTRA++ model and twice as fast as the larger Bio-ELECTRA Base re-ranking models. While all Bio-ELECTRA models were significantly better than both BioBERT and BERT Base, the performance differences among the best performing Bio-ELECTRA models were not statistically significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Search Engine Keyword Selection via Bio-ELECTRA++</head><p>Selection of keywords is a vital step in the question answering step, since missing of even a single important keyword could prevent retrieval of relevant candidate documents. The original Bio-AnswerFinder had used a LSTM based multi class classifier using GloVe word embeddings trained on PubMed abstracts. To minimize out-of-vocabulary (OOV) effects on GloVe embeddings LSTM based model uses also inputs from part of speech tags of the question words encoded by a separate LSTM layer.</p><p>Encouraged by the performance of the discriminative language representation models, a Bio-ELECTRA++ model based approach was introduced. The keyword selection from a question  task is cast as a sequence tagging problem. Bio-ELECTRA++ was selected over other larger Bio-ELECTRA models, because of its inference time performance is up to eight times better than the larger models. From BioASQ 5b, 752 training questions and 100 test questions were annotated for each word in the question being a keyword or not. The performance of both models averaged over ten randomly initialized training/testing phases is shown in Table <ref type="table" coords="7,499.57,500.76,3.81,10.91" target="#tab_4">5</ref>, which shows that Bio-ELECTRA++ based keyword selection significantly outperforms LSTM based multi-input model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Yes/No Question Answer Determination</head><p>Yes/no question answer determination from provided passages can be cast as a binary classification similar to sentiment classification to determine the implicit sentiment positive (yes) or negative (no) from the given context. However, some of the candidate passages might not provide enough evidence for either of the sentiments. In these cases due to the binary nature of the decision making, spurious decisions can be introduced. This is especially a problem with sentence level operation nature of the Bio-AnswerFinder. To remedy this, a third label (neutral) is introduced.</p><p>Negative sampling for neutral label was done using weighted rWMD based sentence similarity where sentences from the snippets are selected based on their weighted rWMD score being less than or equal to 0.6 compared to the sentences of the ideal answer. Snippet sentences having weighted rWMD score grater than or equal to 0.8 were chosen as additional label support sentences besides ideal answers. The thresholds were selected by minimizing the number of questions without any neutral sentence given the threshold values. While random sampling from other questions could be easily used for negative (neutral) sampling, the goal is to differentiate between candidate sentences related to the question but not provide an answer. The neutral sentences selected this way were afterwards checked and labeled manually.</p><p>From BioASQ 8b training data, 727 yes/no questions were selected for training and 128 for testing. Training/testing instances were prepared from sentences of the ideal answers and snippets. For yes/no classification there were 727 training instances and 128 test instances. For yes/no/neutral classification there were 2938 training instances and 539 testing instances. Nine language representation models were evaluated for yes/no classification to decide on the model for further yes/no/neutral answer classification. Test results for the average of ten randomly initialized classifiers per language representation model together with their standard deviations are shown in Table <ref type="table" coords="8,176.66,303.75,3.74,10.91" target="#tab_5">6</ref>. The best performing Bio-ELECTRA Base model pretrained for 1M steps was selected for comparison experiments between yes/no and yes/no/neutral classifiers together with three different voting strategies for final decision.</p><p>During inference time, either the first ten highest ranked answer candidate sentences selected by Bio-AnswerFinder or the snippets as they are provided by BioASQ challenge is passed to the classifiers to make yes/no decision on each one of the candidate sentences/snippets. The final decision is made by a voting strategy. To this end, three voting strategies were used. The majority voting strategy uses the most common yes/no decision as the final decision. The best score strategy uses the decision of the answer candidate with the highest score as the final decision. The score voting strategy uses the highest sum of scores for the yes and no predicted answer candidates as the final decision. For evaluation, snippets provided for 128 test questions were scored by both types of classifiers. For yes/no/neutral classifier, any snippets with a neutral score greater than 0.5 were excluded from the voting. The test results are shown in Table <ref type="table" coords="8,478.62,466.34,3.66,10.91" target="#tab_6">7</ref>. The yes/no/neutral classifier with score voting was the best performing classifier, which was chosen to be used for BioASQ 9 challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Summary Question Handling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">Extractive Summarization for BioASQ Summary Questions</head><p>In extractive summarization, a summary is generated by selecting sentences for the documents/snippets to be summarized. The introduced salient sentence selection strategy leverages the ranked sentences outputted by the Bio-AnswerFinder where the top 10 ranked answer candidate sentences are used. To minimize repetition, a hierarchical agglomerative clustering using weighted relaxed word mover's distance (wRWMD) similarity is introduced to group sentences where the cluster merge stop similarity threshold to maximize ROGUE-2 score was determined on training set answer summaries. From each cluster, the highest Bio-AnswerFinder ranked sentence is selected. The selected sentences are then ordered by their abstract occurrence </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">Abstractive Summarization for BioASQ Summary Questions</head><p>Unlike extractive summarization where the summary is generated from the sentences of candidate documents/snippets, in abstractive summarization new content summarization the candidate documents/snippets is generated. To this end, a unified text-to-text transformer model called T5 <ref type="bibr" coords="9,160.47,621.44,17.76,10.91" target="#b9">[10]</ref> is trained with combined snippets as the document and the ideal answer as the summary for all summary questions from the BioASQ 8B training data. As a preprocessing step, any overlapping snippets are detected and only the longest of the overlapping snippets are included in generating document to be summarized. A T5 Base model is fine-tuned with a maximum input sequence length of 512, batch size of 2 for 2 epochs to generate summaries of 150 tokens maximum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">BioASQ 2021 Synergy Task Systems</head><p>In the BioASQ 9 Synergy task, all questions were on the developing problem of COVID-19 without any guarantee that all them could be answered at the moment. There are no separate information retrieval and question answering from provided snippets, making the task only suitable to end-to-end systems. Also, feedback from the domain experts is provided after each round of the task allowing the participating systems to take advantage of the provided feedback in the next round. For the BioASQ Synergy document retrieval and snippets selection, Bio-AnswerFinder <ref type="bibr" coords="10,175.87,240.44,13.00,10.91" target="#b8">[9]</ref> was used. Instead of the LSTM based keyword selection classifier, a Bio-ELECTRA++ <ref type="bibr" coords="10,167.32,253.99,12.81,10.91" target="#b5">[6]</ref> model based keyword selection classifier described in Section 6 was used for better performance.</p><p>Starting from Synergy round 2, provided expert feedback data was used to augment the training data used for the BERT <ref type="bibr" coords="10,234.28,294.63,12.91,10.91" target="#b0">[1]</ref> based reranker classifier the Bio-AnswerFinder uses after weighted relaxed word mover's distance (wRWMD) similarity based ranking and focus word based filtering. At each round the BERT Base based reranker was retrained with the cumulative Synergy expert feedback.</p><p>Also for the rounds 2 and 3, an alternative keyword search engine (instead of Elasticsearch) was used after keyword query generation which was based on Pyserini search engine with MonoT5 based document re-ranking <ref type="bibr" coords="10,254.21,375.93,16.25,10.91" target="#b16">[17]</ref>.</p><p>For round 4, a GloVe <ref type="bibr" coords="10,199.36,389.48,18.06,10.91" target="#b17">[18]</ref> embedding vector similarity based boolean search engine was developed where an approximate KNN GloVe vector similarity index was used for efficient similarity based retrieval of expansions for query keywords. The candidate set of abstracts retrieved by this search engine was combined with the Elasticsearch retrieved results for downstream processing by the Bio-AnswerFinder. The results of this system was entered as 'bio-answerfinder-2' to the Synergy task web site.</p><p>GloVe vectors used for round 1 and 2 were generated from the 2017 PubMed abstracts thus having no COVID-19 related terms resulting Bio-AnswerFinder excluding COVID-19 related terms from selected keywords for abstract retrieval, weighted relaxed word mover's distance (wRWMD) similarity based ranking resulting in system degradation. After this was noticed, new GloVe vectors were trained on the 2021 base PubMed abstracts and used to retrain affected classifiers in the Bio-AnswerFinder which were used in rounds 3 and 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1.">Exact Answers/Ideal Answers</head><p>The re-ranked candidate sentences from the Bio-AnswerFinder are the input to the Synergy challenge subsystems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1.1.">Factoid and List Questions</head><p>For factoid and list questions, answer span classifier and post-processing described in Section 4 was used. Since, at the time of Synergy challenge Bio-ELECTRA models were not pretrained, ELECTRA_Base <ref type="bibr" coords="11,163.37,86.97,12.85,10.91" target="#b4">[5]</ref> were fine-tuned using the combined SQuAD v1.1 and BioASQ 8b training data.</p><p>For factoid ideal answers, the highest Bio-AnswerFinder re-ranked sentence that contains the highest scored exact answer was selected. For list ideal answers, the sentence containing the most number of highest scored exact answers was selected among the top ten Bio-AnswerFinder re-ranked sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1.2.">Yes/No Questions</head><p>For yes/no questions, both binary and ternary classifiers described in Section 7 were used for different rounds. Similar to the factoid and list questions ELECTRA Base <ref type="bibr" coords="11,411.22,217.59,12.75,10.91" target="#b4">[5]</ref> models were used. Top 10 Bio-AnswerFinder selected sentences were passed to the binary classifier for the round 1 and yes/no decision was based on majority voting. The three-way ELECTRA Base based classifier for yes, no and neutral sentences was used using majority voting for rounds 2, 3 and 4. The highest re-ranked sentence from Bio-AnswerFinder was selected as the ideal answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1.3.">Summary Questions</head><p>For summary questions, the extractive system described in Section 8.1 was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">BioASQ 2021 9B Systems</head><p>Similar to the Synergy task, for BioASQ 9B Phase A task, Bio-AnswerFinder <ref type="bibr" coords="11,447.06,379.74,13.00,10.91" target="#b8">[9]</ref> was used with the Bio-ELECTRA++ based keyword classifier and Bio-ELECTRA Mid based re-ranker as described in Section 2. The iterative keyword query against Elasticsearch based document retrieval mechanism of the Bio-AnswerFinder was enhanced by a word embeddings based keyword synonym expansion mechanism for the batches 4 and 5. For each keyword selected by the Bio-ELECTRA++ based keyword classifier up to four most similar (by cosine similarity of GloVe word vectors) were added as synonyms to the Elasticsearch query which was iteratively refined until enough documents are returned. This approach was used for the challenge Task A system "bio-answerfinder-2".</p><p>For Task 9B Phase B, snippets provided by BioASQ challenge were first passed through Bio-AnswerFinder after bypassing retrieval section. The re-ranked candidate sentences were the input to the challenge subsystems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1.">Factoid and List Questions</head><p>For factoid and list questions, Bio-ELECTRA Base based answer span classifier and postprocessing described in Section 4 was used. For ideal answers, the same mechanism as in the Synergy Task was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.2.">Yes/No Questions</head><p>For yes/no questions, the best performing Bio-ELECTRA Mid model based ternary yes/no/neutral classifier described in Section 7 was used. Final decision was made by score voting. For ideal answers, also the same mechanism as in the Synergy Task was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.3.">Summary Questions</head><p>For the summary questions, both the extractive and abstractive systems described in the Section 8.1 and Section 8.2, respectively, were used. The abstractive summarization system was used for the BioASQ challenge Task B system "bio-answerfinder-2".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Discussion</head><p>Bio-AnswerFinder together with the extensions introduced in this paper is one of the few end-to-end systems participating BioASQ challenges that can handle Synergy and both phases of Task B for all question types.</p><p>For the Synergy task the systems described in Section 9, GloVe vectors used in the rounds 1 and 2 did not had any COVID-19 related terms since they were generated from 2017 PubMed abstracts. This had detrimental effects to the document retrieval and ranking which relies on GloVe vectors for keyword ranking for greedy iterative retrieval and wRWMD based ranking. Since documents with a feedback from previous rounds needs to be excluded from the eligible document pool for the questions in the subsequent rounds, the detrimental effect from the first two rounds adversely affected the other rounds also. Also, because of a misunderstanding of the instructions for the Synergy challenge, only abstracts with a PubMed ID were indexed for search leaving out all preprint abstracts that make about the half of the CORD-19 corpus, This was not noticed until the Synergy version 2 challenge, Even after these setbacks, the system performance was decent based on official BioASQ Synergy Task results (on average, 12th out of the 23 individual systems on documents ùêπ 1 , 12th out of the 24 systems on snippets ùêπ 1 , 6th out of the 24 systems on yes/no overall ùêπ 1 , 6th out of the 24 systems on factoid MRR and 5th out of the 24 systems on list ùêπ 1 ). The GloVe embedding vector similarity based boolean search engine introduced for the round 4 to increase coverage over the iterative keyword query based document retrieval improved performance over the default retrieval based on the official Synergy Task test results.</p><p>In the BioASQ 9B Phase A, the introduced system was the best system on document retrieval in four out of the five test batches based on ùêπ 1 score and second on the remaining batch. In snippets, the system was second best in two batches and third in three batches. The keyword synonym expansion approach described in Section 10 ('bio-answerfinder-2') used in batches 4 and 5 had slightly worse performance on document retrieval. For snippets, the results were more mixed, the expansion approach had better performance than the original system in batch 4 while performing worse in batch 5.</p><p>In the 9B Phase B, the introduced systems were second for yes/no questions in test batches 2 and 3. For the list questions, the performance was better than last year. While factoid question performance was decent, there is room for improvement. However, based on the factoid question error analysis for last years' submissions described in the next section, the observed near-miss issue is suspected this year also. This will be investigated once the gold standard annotations will be available. For the ideal questions, only automatic ROUGE scores are available. Based on both ROUGE-2 (F1) and ROUGE-SU4 (F1) scores, the introduced system was the best scoring system for the test batches 1 and 2. Despite the terse nature of its results (usually a single sentence), T5 <ref type="bibr" coords="13,150.02,100.52,17.81,10.91" target="#b9">[10]</ref> based abstractive summarization system 'bio-answerfinder-1' seems to work well outperforming extractive summarization in batches 2 and 4.</p><p>In BioASQ 8B, Bio-AnswerFinder won the second place for human evaluated ideal answers in three test batches. Since Bio-AnswerFinder was mainly designed as a practical knowledge discovery tool for biomedical researchers who prefer ideal answers (answer with evidence and context), this result was very encouraging validation towards the main design goal of Bio-AnswerFinder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.1.">Error Analysis for Factoid Questions</head><p>Based on the analysis of the BioASQ 8b factoid question 'bio-answerfinder' system submissions against the ground truth answers in the BioASQ 9B training data, it was identified that about 53% of the errors can be attributed to near misses, i.e. singular/plural differences, differences in stop words (e.g. articles), single special character differences, acronym versus its expansion, and other transliterations or paraphrasings. Another common issue is the provided ground truth being a paraphrased sentence, more akin to an ideal answer than factoid answer, not occurring in any of the supplied documents for the question. Representative near miss error of different types are shown in Table <ref type="table" coords="13,243.23,326.38,3.72,10.91">8</ref>. For examples 4 and 5, the ground truth does not exists in the provided phrases. Even though these were errors for the automatic evaluation, for a human user the predicted answers would be the correct answers. QA systems are designed for human usage and while automatic evaluation provides fast, systematic evaluation of QA systems, more than 50% near miss emphasizes importance of human evaluation for QA systems even though they are more costly than automatic evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.">Conclusions</head><p>In this paper, extensions to an end-to-end biomedical QA system, Bio-AnswerFinder <ref type="bibr" coords="13,476.97,452.75,12.99,10.91" target="#b8">[9]</ref> for BioASQ biomedical question answering challenge were introduced. To this end, three ELEC-TRA <ref type="bibr" coords="13,111.03,479.85,12.69,10.91" target="#b4">[5]</ref> discriminative language representation models were pretrained from scratch on PubMed abstracts and PMC open access papers. Based on performance comparison against numerous other language representation models including BioBERT, the introduced Bio-ELECTRA models had shown superior performance for the classifiers used in the Bio-AnswerFinder sub-systems. The resulting system(s) had shown very good performance in BioASQ 9B Phase A and good performance for yes/no questions and ideal answers in Phase B based on the official automatic evaluation results. In the future, sensitivity of some subsystems such as keyword ranking and weighted relaxed WMD measure based answer candidate ranking to the out-of-vocabulary terms will be addressed. Based on the insights of the in-depth analysis of questions that cannot be properly answered from BioASQ Synergy and 9B Tasks, Bio-AnswerFinder will be further improved with the eventual goal of answering all answerable biomedical domain questions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,482.03,215.38,8.93;4,89.29,84.19,416.70,373.32"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the Bio-AnswerFinder system</figDesc><graphic coords="4,89.29,84.19,416.70,373.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,88.99,90.49,402.67,91.56"><head>Table 1</head><label>1</label><figDesc>ELECTRA Model Architectures for Biomedical Domain</figDesc><table coords="5,103.61,119.83,388.05,62.21"><row><cell>Model</cell><cell>Params</cell><cell>Architecture</cell></row><row><cell>Bio-ELECTRA++ [6]</cell><cell>11M</cell><cell>hidden:256, layers:12, batch:64, attention heads:4</cell></row><row><cell>Bio-ELECTRA Mid</cell><cell>50M</cell><cell>hidden:512, layers:12, batch:256, attention heads:8</cell></row><row><cell>Bio-ELECTRA Base</cell><cell>110M</cell><cell>hidden:768, layers:12, batch:256, attention heads:12</cell></row><row><cell>Bio-ELECTRA Mid Combined</cell><cell>50M</cell><cell>hidden:512, layers:12, batch:256, attention heads:8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,88.99,204.30,352.66,93.78"><head>Table 2</head><label>2</label><figDesc>Pretraining of ELECTRA Models for Biomedical Domain</figDesc><table coords="5,153.62,235.87,288.03,62.21"><row><cell>Model</cell><cell cols="2">Params Steps</cell><cell>Train Time/Hardware</cell></row><row><cell>Bio-ELECTRA++</cell><cell>11M</cell><cell cols="2">3.6M 48 days on RTX 2070 8GB GPU</cell></row><row><cell>Mid</cell><cell>50M</cell><cell>1.2M</cell><cell>6.5 days on 8 TPUv3s</cell></row><row><cell>Base</cell><cell>110M</cell><cell>1.2M</cell><cell>12.5 days on 8 TPUv3s</cell></row><row><cell>Mid Combined</cell><cell>50M</cell><cell>1.2M</cell><cell>6.5 days on 8 TPUv3s</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,88.99,90.49,346.09,177.47"><head>Table 3</head><label>3</label><figDesc>Biomedical Question Answering Test Results</figDesc><table coords="6,157.71,133.79,277.37,134.16"><row><cell>Model</cell><cell>Exact Match</cell><cell>ùêπ 1</cell></row><row><cell>Bio-ELECTRA++</cell><cell>57.93 (0.66)</cell><cell>67.48 (0.44)</cell></row><row><cell>ELECTRA Small++</cell><cell>57.78 (0.64)</cell><cell>67.10 (0.55)</cell></row><row><cell>BERT</cell><cell>59.98 (0.66)</cell><cell>70.25 (0.48)</cell></row><row><cell>BioBERT</cell><cell>63.58 (0.66)</cell><cell>72.72 (0.48)</cell></row><row><cell>ELECTRA Base</cell><cell>65.01 (0.84)</cell><cell>72.82 (0.70)</cell></row><row><cell>Bio-ELECTRA Mid (1M)</cell><cell>68.71 (0.76)</cell><cell>75.52 (0.49)</cell></row><row><cell>Bio-ELECTRA Mid (1.2M)</cell><cell>69.50 (0.54)</cell><cell>75.82 (0.40)</cell></row><row><cell>Bio-ELECTRA Base (1M)</cell><cell>68.44 (0.56)</cell><cell>75.02 (0.60)</cell></row><row><cell>Bio-ELECTRA Base (1.2M)</cell><cell>68.44 (0.38)</cell><cell>75.50 (0.35)</cell></row><row><cell>Bio-ELECTRA Mid Combined (1.2M)</cell><cell>66.46 (0.65)</cell><cell>74.05 (0.44)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,88.99,90.49,317.96,225.29"><head>Table 4</head><label>4</label><figDesc>Biomedical Question Answer Candidate Re-ranking Test Results</figDesc><table coords="7,185.83,134.01,221.13,181.77"><row><cell>Model</cell><cell>MRR</cell></row><row><cell>ELECTRA Small++</cell><cell>0.281 (0.014)</cell></row><row><cell>ELECTRA Small++ (weighted)</cell><cell>0.281 (0.008)</cell></row><row><cell>Bio-ELECTRA++</cell><cell>0.335 (0.017)</cell></row><row><cell>Bio-ELECTRA++ (weighted)</cell><cell>0.332 (0.013)</cell></row><row><cell>BERT Base</cell><cell>0.246 (0.007)</cell></row><row><cell>BioBERT</cell><cell>0.283 (0.020)</cell></row><row><cell>ELECTRA Base</cell><cell>0.294 (0.017)</cell></row><row><cell>Bio-ELECTRA Mid (1M)</cell><cell>0.333 (0.017)</cell></row><row><cell>Bio-ELECTRA Mid (1M) (weighted)</cell><cell>0.336 (0.017)</cell></row><row><cell>Bio-ELECTRA Mid (1.2M)</cell><cell>0.316 (0.015)</cell></row><row><cell>Bio-ELECTRA Mid (1.2M) (weighted)</cell><cell>0.322 (0.015)</cell></row><row><cell>Bio-ELECTRA Base (1M)</cell><cell>0.333 (0.024)</cell></row><row><cell>Bio-ELECTRA Base (1.2M)</cell><cell>0.328 (0.013)</cell></row><row><cell cols="2">Bio-ELECTRA Base (1.2M) (weighted) 0.336 (0.023)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,88.99,349.99,349.53,69.88"><head>Table 5</head><label>5</label><figDesc>Test Performance for Keyword Selection Classifiers</figDesc><table coords="7,156.75,381.34,262.97,9.65"><row><cell>Model</cell><cell>Precision</cell><cell>Recall</cell><cell>ùêπ 1</cell></row></table><note coords="7,156.75,399.04,280.67,8.87;7,156.75,410.99,67.38,8.87;7,272.14,410.94,166.39,8.93"><p>LSTM Multi-input Model 91.72 (0.99) 89.39 (1.70) 90.53 (0.67) Bio-ELECTRA++ 97.58 (0.47) 96.93 (0.54) 97.25 (0.24)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,88.99,90.49,409.49,164.60"><head>Table 6</head><label>6</label><figDesc>Biomedical Yes/No Question Answer Classification Test Results</figDesc><table coords="9,94.30,132.87,404.18,122.21"><row><cell>Model</cell><cell>P (Yes)</cell><cell>R (Yes)</cell><cell>ùêπ 1 (Yes)</cell><cell>P (No)</cell><cell>R (No)</cell><cell>ùêπ 1 (No)</cell></row><row><cell>Bio-ELECTRA++</cell><cell cols="6">91.24 (1.57) 95.29 (2.31) 93.19 (0.75) 78.91 (7.41) 63.85 (7.92) 69.84 (3.87)</cell></row><row><cell cols="7">ELECTRA Small++ 88.18 (0.71) 94.31 (1.74) 91.14 (1.00) 69.92 (7.34) 50.38 (3.19) 58.40 (3.61)</cell></row><row><cell>BERT Base</cell><cell cols="6">87.02 (2.57) 95.49 (2.64) 90.99 (1.00) 65.15 (22.99) 43.46 (15.20) 51.71 (17.49)</cell></row><row><cell>BioBERT</cell><cell cols="6">92.94 (1.19) 93.04 (1.55) 92.63 (0.91) 71.94 (4.10) 69.23 (5.16) 70.42 (3.46)</cell></row><row><cell>ELECTRA Base</cell><cell cols="6">94.73 (1.67) 96.19 (0.82) 95.44 (0.88) 82.02 (3.08) 76.32 (8.01) 78.86 (5.06)</cell></row><row><cell>Mid (1M)</cell><cell cols="6">98.07 (0.71) 94.71 (1.40) 96.36 (0.96) 81.83 (4.15) 92.69 (2.69) 86.89 (3.23)</cell></row><row><cell>Base (1M)*</cell><cell cols="6">97.43 (1.14) 95.98 (1.20) 96.69 (0.58) 85.31 (3.64) 90.00 (4.62) 87.46 (2.31)</cell></row><row><cell>Mid (1.2M)</cell><cell cols="6">95.71 (1.97) 94.80 (1.76) 95.23 (0.89) 80.69 (4.44) 83.08 (8.28) 81.52 (4.16)</cell></row><row><cell>Base (1.2M)</cell><cell cols="6">97.22 (1.21) 95.49 (1.26) 96.34 (0.83) 83.61 (3.78) 89.23 (4.80) 86.23 (3.17)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,88.99,289.29,368.50,252.68"><head>Table 7 Yes</head><label>7</label><figDesc></figDesc><table coords="9,89.29,336.60,361.97,205.38"><row><cell></cell><cell cols="4">Yes/No classifier Bio-ELECTRA Base (1M)</cell><cell></cell><cell></cell></row><row><cell>majority voting</cell><cell>77.78</cell><cell>95.79</cell><cell>85.85</cell><cell>88.57</cell><cell>54.39</cell><cell>67.39</cell></row><row><cell>best score</cell><cell>80.91</cell><cell>93.68</cell><cell>86.83</cell><cell>85.71</cell><cell>63.16</cell><cell>72.73</cell></row><row><cell>score voting</cell><cell>80.91</cell><cell>93.68</cell><cell>86.83</cell><cell>85.71</cell><cell>63.16</cell><cell>72.73</cell></row><row><cell cols="6">Yes/No/Neutral classifier Bio-ELECTRA Base (1M)</cell><cell></cell></row><row><cell>majority voting</cell><cell>84.62</cell><cell>93.62</cell><cell>88.89</cell><cell>86.67</cell><cell>70.91</cell><cell>78.00</cell></row><row><cell>best score</cell><cell>84.85</cell><cell>89.36</cell><cell>87.05</cell><cell>80.00</cell><cell>72.73</cell><cell>76.19</cell></row><row><cell>score voting</cell><cell>85.44</cell><cell>94.68</cell><cell>89.90</cell><cell>88.89</cell><cell>72.73</cell><cell>79.21</cell></row><row><cell cols="7">Yes/No/Neutral classifier Bio-ELECTRA Base (1M) seq length: 256</cell></row><row><cell>majority voting</cell><cell>84.76</cell><cell>94.68</cell><cell>89.45</cell><cell>86.64</cell><cell>70.91</cell><cell>78.79</cell></row><row><cell>best score</cell><cell>85.29</cell><cell>92.55</cell><cell>88.78</cell><cell>85.11</cell><cell>72.73</cell><cell>78.43</cell></row><row><cell>score voting</cell><cell>85.58</cell><cell>94.68</cell><cell>89.90</cell><cell>88.89</cell><cell>72.73</cell><cell>80.0</cell></row><row><cell>order and concatenated.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note coords="9,103.45,301.30,221.48,8.87;9,135.29,319.11,28.86,8.93;9,210.64,318.89,246.86,9.65"><p>/No versus Yes/No/Neutral Classification Performance Model P (Yes) R (Yes) ùêπ 1 (Yes) P (No) R (No) ùêπ 1 (No)</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="14.">Acknowledgments</head><p>This work was supported by the <rs type="funder">NIDDK Information Network</rs> (dkNET; http://dknet.org) via <rs type="funder">NIH's National Institute of Diabetes and Digestive and Kidney Diseases (NIDDK)</rs> award <rs type="grantNumber">U24DK097771</rs>. I would like also to thank <rs type="institution">Google TensorFlow Research Cloud (TFRC)</rs> <rs type="programName">program</rs> for providing me with free TPUs which allowed me to pretrain Bio-ELECTRA models.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_k7PhXUx">
					<idno type="grant-number">U24DK097771</idno>
					<orgName type="program" subtype="full">program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="14,112.66,652.22,393.33,10.91;14,112.66,665.77,393.33,10.91;15,112.66,86.97,393.32,10.91;15,112.66,100.52,393.33,10.91;15,112.66,114.06,394.03,10.91;15,112.66,127.61,234.20,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,323.15,652.22,182.83,10.91;14,112.66,665.77,186.91,10.91">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://www.aclweb.org/anthology/N19-1423.doi:10.18653/v1/N19-1423" />
	</analytic>
	<monogr>
		<title level="m" coord="14,327.87,665.77,178.11,10.91;15,112.66,86.97,393.32,10.91;15,112.66,100.52,99.97,10.91">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="15,112.66,141.16,393.33,10.91;15,112.66,154.71,393.33,10.91;15,112.66,168.26,303.41,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="15,420.76,141.16,85.22,10.91;15,112.66,154.71,233.31,10.91">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="15,367.25,154.71,138.74,10.91;15,112.66,168.26,86.91,10.91">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="5753" to="5763" />
			<date type="published" when="2019">2019</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,181.81,393.53,10.91;15,112.66,195.36,361.04,10.91" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<title level="m" coord="15,407.44,181.81,98.74,10.91;15,112.66,195.36,231.08,10.91">Albert: A lite bert for self-supervised learning of language representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,208.91,393.33,10.91;15,112.66,222.46,393.33,10.91;15,112.66,236.01,394.51,10.91;15,112.66,252.00,127.03,7.90" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="15,398.01,208.91,107.98,10.91;15,112.66,222.46,316.05,10.91">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btz682</idno>
		<ptr target="https://doi.org/10.1093/bioinformatics/btz682.doi:10.1093/bioinformatics/btz682" />
	</analytic>
	<monogr>
		<title level="j" coord="15,439.07,222.46,66.92,10.91">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,263.11,393.33,10.91;15,112.66,276.66,295.16,10.91" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<title level="m" coord="15,334.34,263.11,171.65,10.91;15,112.66,276.66,165.13,10.91">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,290.20,395.17,10.91;15,112.66,303.75,393.61,10.91;15,112.66,317.30,394.61,10.91;15,112.66,330.85,391.96,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="15,166.34,290.20,341.49,10.91;15,112.66,303.75,165.47,10.91">On the effectiveness of small, discriminatively pre-trained language representation models for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">B</forename><surname>Ozyurt</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.sdp-1.12</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.sdp-1.12.doi:10.18653/v1/2020.sdp-1.12" />
	</analytic>
	<monogr>
		<title level="m" coord="15,300.28,303.75,205.98,10.91;15,112.66,317.30,93.38,10.91">Proceedings of the First Workshop on Scholarly Document Processing</title>
		<meeting>the First Workshop on Scholarly Document Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="104" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,344.40,395.17,10.91;15,112.66,357.95,394.53,10.91;15,112.66,371.50,394.53,10.91;15,112.66,385.05,393.33,10.91;15,112.66,398.60,393.33,10.91;15,112.41,412.15,395.25,10.91;15,112.66,425.70,168.81,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="15,333.85,385.05,172.14,10.91;15,112.66,398.60,294.30,10.91">An overview of the bioasq large-scale biomedical semantic indexing and question answering competition</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tsatsaronis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Balikas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Malakasiotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Partalas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zschunke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Alvers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Polychronopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Almirantis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Baskiotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Artieres</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ngonga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Heino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Barrio-Alvers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12859-015-0564-6</idno>
		<ptr target="http://www.biomedcentral.com/content/pdf/s12859-015-0564-6.pdf.doi:10.1186/s12859-015-0564-6" />
	</analytic>
	<monogr>
		<title level="j" coord="15,415.50,398.60,90.48,10.91">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">138</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,439.25,394.62,10.91;15,112.66,452.79,393.33,10.91;15,112.66,466.34,393.33,10.91;15,112.66,479.89,295.48,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="15,367.13,439.25,140.15,10.91;15,112.66,452.79,259.81,10.91">Overview of bioasq 8a and 8b: Results of the eighth edition of the bioasq tasks a and b</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Bougiatiotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_164.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="15,401.26,452.79,104.73,10.91;15,112.66,466.34,393.33,10.91;15,112.66,479.89,43.99,10.91">Proceedings of the 8th BioASQ Workshop A challenge on large-scale biomedical semantic indexing and question answering</title>
		<meeting>the 8th BioASQ Workshop A challenge on large-scale biomedical semantic indexing and question answering</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,493.44,393.33,10.91;15,112.66,506.99,394.04,10.91;15,112.66,520.54,268.07,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="15,303.01,493.44,202.97,10.91;15,112.66,506.99,154.96,10.91">Bio-AnswerFinder: a system to find answers to questions from biomedical texts</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">B</forename><surname>Ozyurt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bandrowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Grethe</surname></persName>
		</author>
		<idno type="DOI">10.1093/database/baz137</idno>
		<ptr target="https://doi.org/10.1093/database/baz137.doi:10.1093/database/baz137,baz137" />
	</analytic>
	<monogr>
		<title level="j" coord="15,275.74,506.99,41.37,10.91">Database</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,534.09,394.53,10.91;15,112.66,547.64,395.01,10.91;15,112.66,563.63,97.35,7.90" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<title level="m" coord="15,112.66,547.64,363.43,10.91">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,574.74,393.98,10.91;15,112.41,588.29,397.73,10.91;15,112.66,604.28,49.88,7.90" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="15,261.91,574.74,114.46,10.91">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="https://doi.org/10.1162/neco.1997.9.8.1735.doi:10.1162/neco.1997.9.8.1735" />
	</analytic>
	<monogr>
		<title level="j" coord="15,391.60,574.74,74.22,10.91">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,615.39,393.33,10.91;15,112.66,628.93,393.33,10.91;15,112.66,642.48,394.52,10.91;15,112.66,656.03,397.48,10.91;15,112.36,672.02,43.94,7.90" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="15,466.87,615.39,39.12,10.91;15,112.66,628.93,133.83,10.91">Learning to rank using gradient descent</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Renshaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lazier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Deeds</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hullender</surname></persName>
		</author>
		<idno type="DOI">10.1145/1102351.1102363</idno>
		<ptr target="https://doi.org/10.1145/1102351.1102363.doi:10.1145/1102351.1102363" />
	</analytic>
	<monogr>
		<title level="m" coord="15,269.62,628.93,236.37,10.91;15,112.66,642.48,124.33,10.91">Proceedings of the 22nd International Conference on Machine Learning, ICML &apos;05</title>
		<meeting>the 22nd International Conference on Machine Learning, ICML &apos;05<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,86.97,393.53,10.91;16,112.66,100.52,393.33,10.91;16,112.33,114.06,312.05,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="16,222.56,86.97,283.62,10.91;16,112.66,100.52,140.99,10.91">Iterative document retrieval via deep learning approaches for biomedical question answering</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">B</forename><surname>Ozyurt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Grethe</surname></persName>
		</author>
		<idno type="DOI">10.1109/eScience.2019.00072</idno>
	</analytic>
	<monogr>
		<title level="m" coord="16,284.20,100.52,221.79,10.91;16,112.33,114.06,42.59,10.91">2019 15th International Conference on eScience (eScience)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="533" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,127.61,394.53,10.91;16,112.66,141.16,393.32,10.91;16,112.66,154.71,394.53,10.91;16,112.66,168.26,80.57,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="16,304.35,127.61,198.60,10.91">From word embeddings to document distances</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kolkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,227.37,141.16,278.61,10.91;16,112.66,154.71,38.37,10.91;16,222.94,155.73,183.99,9.72">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>PMLR, Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="957" to="966" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct coords="16,112.66,181.81,393.33,10.91;16,112.66,195.36,393.32,10.91;16,112.66,208.91,394.52,10.91;16,112.66,222.46,397.48,10.91;16,112.36,238.45,103.79,7.90" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="16,262.37,181.81,243.61,10.91;16,112.66,195.36,20.72,10.91">Neural machine translation of rare words with subword units</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1162</idno>
		<ptr target="https://www.aclweb.org/anthology/P16-1162.doi:10.18653/v1/P16-1162" />
	</analytic>
	<monogr>
		<title level="m" coord="16,156.17,195.36,349.81,10.91;16,112.66,208.91,49.24,10.91">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="16,112.66,249.56,393.33,10.91;16,112.66,263.11,393.33,10.91;16,112.66,276.66,394.52,10.91;16,112.66,290.20,394.51,10.91;16,112.36,306.20,68.18,7.90" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="16,320.68,249.56,185.30,10.91;16,112.66,263.11,98.54,10.91">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
		<ptr target="https://www.aclweb.org/anthology/D16-1264.doi:10.18653/v1/D16-1264" />
	</analytic>
	<monogr>
		<title level="m" coord="16,233.64,263.11,272.35,10.91;16,112.66,276.66,324.64,10.91">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,317.30,393.33,10.91;16,112.66,330.85,395.17,10.91;16,112.66,344.40,394.53,10.91;16,112.41,357.95,394.76,10.91;16,112.66,373.94,133.46,7.90" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="16,327.76,317.30,178.23,10.91;16,112.66,330.85,128.92,10.91">Document ranking with a pretrained sequence-to-sequence model</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.63</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.findings-emnlp.63.doi:10.18653/v1/2020.findings-emnlp.63" />
	</analytic>
	<monogr>
		<title level="m" coord="16,270.66,330.85,237.18,10.91;16,112.66,344.40,289.18,10.91">Findings of the Association for Computational Linguistics: EMNLP 2020, Association for Computational Linguistics</title>
		<meeting><address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="708" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,385.05,394.61,10.91;16,112.66,398.60,393.33,10.91;16,112.33,412.15,395.33,10.91;16,112.66,425.70,363.36,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="16,283.23,385.05,204.21,10.91">GloVe: Global vectors for word representation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
		<ptr target="https://www.aclweb.org/anthology/D14-1162.doi:10.3115/v1/D14-1162" />
	</analytic>
	<monogr>
		<title level="m" coord="16,112.66,398.60,393.33,10.91;16,112.33,412.15,236.37,10.91">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
