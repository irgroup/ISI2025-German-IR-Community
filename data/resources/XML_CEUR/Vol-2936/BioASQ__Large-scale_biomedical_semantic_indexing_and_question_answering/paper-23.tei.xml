<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,413.12,15.42;1,89.29,106.66,408.87,15.42;1,89.29,128.58,59.57,15.43">CoLe and LYS at BioASQ MESINESP Task: large scale multilabel text categorization with sparse and dense indices</title>
				<funder>
					<orgName type="full">Centro de Investigación de Galicia &quot;CITIC</orgName>
				</funder>
				<funder ref="#_ZTkBkTC #_7qch4QM">
					<orgName type="full">Galician Regional Government (Xunta de Galicia)</orgName>
				</funder>
				<funder>
					<orgName type="full">El-Yurt-Umidi Foundation</orgName>
				</funder>
				<funder ref="#_yasWkRg">
					<orgName type="full">SCANNER-UDC</orgName>
				</funder>
				<funder>
					<orgName type="full">European Union</orgName>
				</funder>
				<funder ref="#_e3n7UPz">
					<orgName type="full">European Regional Development Fund</orgName>
				</funder>
				<funder ref="#_Z2bhcCK">
					<orgName type="full">Xunta de Galicia</orgName>
				</funder>
				<funder>
					<orgName type="full">Cabinet of Ministers of the Republic of Uzbekistan</orgName>
				</funder>
				<funder ref="#_RCrZwQD #_gMbGU9p #_fzCVPGu">
					<orgName type="full">ERDF/MICINN-AEI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,156.89,124.39,11.96"><forename type="first">Francisco</forename><forename type="middle">J</forename><surname>Ribadas-Pena</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Departamento de Informática</orgName>
								<orgName type="laboratory">Grupo COLE</orgName>
								<orgName type="institution">Universidade de Vigo E.S. Enxeñaría Informática</orgName>
								<address>
									<addrLine>Campus As Lagoas</addrLine>
									<postCode>32004</postCode>
									<settlement>Ourense</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,226.33,156.89,65.23,11.96"><forename type="first">Shuyuan</forename><surname>Cao</surname></persName>
							<email>shuyuan.cao@uvigo.es</email>
							<affiliation key="aff0">
								<orgName type="department">Departamento de Informática</orgName>
								<orgName type="laboratory">Grupo COLE</orgName>
								<orgName type="institution">Universidade de Vigo E.S. Enxeñaría Informática</orgName>
								<address>
									<addrLine>Campus As Lagoas</addrLine>
									<postCode>32004</postCode>
									<settlement>Ourense</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,322.55,156.89,95.80,11.96"><forename type="first">Elmurod</forename><surname>Kuriyozov</surname></persName>
							<email>e.kuriyozov@udc.es</email>
							<affiliation key="aff1">
								<orgName type="department">Departamento de Computación y Tecnologías de la Información</orgName>
								<orgName type="laboratory">Grupo LYS</orgName>
								<orgName type="institution">Universidade de A Coruña Facultade de Informatica</orgName>
								<address>
									<addrLine>Campus de Elviña, A Coruña</addrLine>
									<postCode>15071</postCode>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,413.12,15.42;1,89.29,106.66,408.87,15.42;1,89.29,128.58,59.57,15.43">CoLe and LYS at BioASQ MESINESP Task: large scale multilabel text categorization with sparse and dense indices</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">98ADDF90CC81C5334DDB604CB712D39D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information Retrieval</term>
					<term>Dense Representation</term>
					<term>Sparse Textual Representation</term>
					<term>Multi-Label Classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe our participation in the second edition of mesinesp shared-task in the BioASQ biomedical semantic indexing challenge. The system employed in this participation tries to exploit different strategies for the use of similarity between documents to build a multi-label classifier that assigns DeCS descriptors to new documents from the descriptors previously assigned to similar documents. We have implemented and evaluated two complementary proposals: (1) the use of sparse document representations, based on the extraction of linguistically motivated index terms and their subsequent indexing using Apache Lucene and (2) the use of indices storing dense representations of training documents obtained by means of sentence level embeddings. The results obtained in official runs were far from the best performing systems, but we believe that our approach offers an acceptable performance taking into account the minimum processing requirements that the proposed document similarity scheme supposes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The mesinesp2 <ref type="bibr" coords="1,156.72,469.25,12.68,10.91" target="#b0">[1]</ref> shared-task on medical semantic indexing in Spanish is part of the BioASQ <ref type="bibr" coords="1,493.30,469.25,12.68,10.91" target="#b4">[5]</ref> 2021 challenge. Content indexing using structured vocabularies is a critical task in the management of large textual collections in scientific and technical domains and it is essential to make possible sophisticated search engines to help researchers in accessing to relevant information. Although Spanish is one of the most spoken languages, most of the previous efforts and advances in semantic indexing have been oriented exclusively to English texts and the aim of the mesinesp challenge series was to evaluate the state-of-art and to promote the research in semantic indexing of Spanish scientific literature.</p><p>The second edition of mesinesp shared-task asked participant teams to label test documents with codes from DeCS (Descriptores en Ciencias de la Salud), a controlled hierarchical vocabulary which is a translation and extension of the MeSH (Medical Subjects Headings) thesaurus. This edition was composed of three sub-tracks, dealing with scientific literature (Sub-track 1, mesinesp-l), clinical trials (Sub-track 2, mesinesp-t), and biomedical patents (Sub-track3, mesinesp-p).</p><p>Our team has participated in the three sub-tracks evaluating the adequacy of various approaches based on textual similarity. The methods used in the three sub-tracks have been essentially the same and are an extension of those used in our participation at the previous edition of this challenge <ref type="bibr" coords="2,204.58,168.26,11.49,10.91" target="#b7">[8,</ref><ref type="bibr" coords="2,219.63,168.26,7.65,10.91" target="#b6">7]</ref>. The starting idea of our method is to identify the training documents most similar to a given test document. Using the set of descriptors assigned to these similar documents we construct the list of candidate labels to be returned as a result.</p><p>In our experiments and in the submitted runs we have evaluated different approaches for identifying this list of similar training documents. As in previous editions of the BioASQ challenge, we have used several natural language processing (NLP) techniques to extract linguistically motivated representations of the training documents that are stored in an Apache Lucene textual index. This index is later queried with the contents of each test document to retrieve the most similar documents. In addition to using this kind of sparse document representations we have proposed the use of dense representations based on sentence-level embeddings. The dense vectors extracted from train documents are indexed in order to locate, during the categorization phase, the set of vectors closest to the dense vectors extracted from the sentence-level embeddings of the test documents to be annotated. Additionally, we have tried to improve the performance of our sparse method based on Apache Lucene using an alternative type of index which is based on the creation of inverse DeCS code profiles that link index terms extracted from the documents with the DeCS tags with which they have a high co-occurrence level.</p><p>The rest of this paper is organized as follows. Section 2 describes the details of our method based on sparse representations on Apache Lucene indices. The generation of inverse DeCS codes profiles is also described in this section. Section 3 details the use of dense representations extracted from sentence-level embeddings. Section 4 provides the preliminary experiments with these methods that were used to carry out the parameterization of the official runs sent to the challenge. Finally, in section 5 we present the details of these official runs and provide a discussion of the results obtained by our approaches in the challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Similarity with sparse representations</head><p>Methods following 𝑘 nearest neighbors (𝑘-NN) approaches have been widely used in the context of large scale multi-label categorization.The sparse representation approach we have followed in our BioASQ challenge participation<ref type="foot" coords="2,258.87,563.86,3.71,7.97" target="#foot_0">1</ref> is essentially a large multi-label 𝑘-NN classifier backed by an Apache Lucene<ref type="foot" coords="2,187.63,577.41,3.71,7.97" target="#foot_1">2</ref> index.</p><p>Our annotation scheme starts by indexing the contents of the mesinesp training articles. For each new article to be annotated, the created index index is queried using its contents as query terms. The list of similar articles returned by the indexing engine and their corresponding similarity measures are exploited to determine the following results:</p><p>• predicted number of descriptors to be assigned</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• ranked list of predicted DeCS codes</head><p>The first aspect is a regression problem, which aims to predict the number of descriptors to be included in the final list, depending on the number of descriptors assigned to the most similar articles identified by the indexing engine and on their respective similarity scores. The other task is a multi-label classification problem, which aims to predict a descriptors list based on the descriptors manually assigned to the most similar mesinesp articles. In both cases, regression and multi-label classification, similarity scores calculated by the indexing engine are exploited. Query terms employed to retrieve the similar articles are extracted from the original article contents and linked using a global OR operator to conform the final query sent to the indexing engine.</p><p>In our case, the scores provided by the indexing engine are actually similarity measures computed according to the weighting scheme being employed, which do not have an uniform and predictable upper bound and do not behave like a real distance. In order to ensure these similarity scores own the properties of a real distance metric, we have applied a normalization procedure, where the most similar document retrieved from the index will have a new score close to 0.0 and the scores of the rest of similar documents are adjusted in accordance with it.</p><p>With this information the number of descriptors to be assigned to the article being annotated is predicted using a weighted average scheme, where the weight of each similar article is the inverse of normalized distance cubed, that is, 1 𝑑 3 . To create the ranked list of descriptors a distance weighted voting scheme is employed, associating the same weight values (the inverse of normalized distances cubed) to the respective similar articles. Since this is actually a multi-label categorization task, there are as many voting tasks as candidate descriptors were extracted from the articles retrieved by the indexing engine. For each candidate label, positive votes come from similar articles annotated with it and negative votes come from articles not including it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Text representations</head><p>Regarding article representation we have evaluated several index term extraction approaches. Our aim was to determine whether linguistic motivated index term extraction could help to improve annotation performance in the 𝑘-NN based method we have described. We employed the following methods:</p><p>Stemming based representation (STEMS). This was the simplest approach which employs stop-word removal, using a standard stop-word list for Spanish, and the default Spanish stemmer from the Snowball project <ref type="foot" coords="3,273.95,574.25,3.71,7.97" target="#foot_2">3</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Morphosyntactic based representation (LEMMAS).</head><p>In order to deal with morphosyntactic variation in Spanish we have employed a lemmatizer to identify lexical roots and we also replaced stop-word removal with a content-word selection procedure based on part-of-speech (PoS) tags.</p><p>We have delegated the linguistic processing tasks to the tools provided by the spaCy Natural Language Processing (NLP) toolkit <ref type="foot" coords="4,316.66,98.76,3.71,7.97" target="#foot_3">4</ref> . In our case we have employed the PoS tagging and lemmatization information provided by spaCy, using the standard Spanish models without any specific data for biomedical related contents.</p><p>Only lemmas from tokens tagged as a noun, verb, adjective, adverb or as unknown words are taken into account to constitute the final article representation, since these PoS are considered to carry the sentence meaning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nominal phrases based representation (NPS).</head><p>In order to evaluate the contribution of more powerful NLP techniques, we have employed a surface parsing approach to identify syntactic motivated nominal phrases from which meaningful multi-word index terms could be extracted.</p><p>Noun Phrase (NP) chunks identified by spaCy are selected and the lemmas of the constituent tokens are joined together to create a multi-word index term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dependencies based representation (DEPS).</head><p>We have also employed as index terms triples of dependence-head-modifier extracted by the dependency parser provided by spaCy. In our case spaCy provides a dependency parsing model for Spanish that identify syntactic dependency labels following the Universal Dependencies(UD) scheme. The complex index terms were extracted from the following UD relationships<ref type="foot" coords="4,383.84,342.45,3.71,7.97" target="#foot_4">5</ref> : acl, advcl, advmod, amod, ccomp, compound, conj, csuj, dep, flat, iobj, nmod , nsubj, obj, xcomp, dobj and pobj.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Named entities representation (NERS).</head><p>Another type of multi-word representations taken into account are named entities. We have employed the NER module in spaCy to extract general named entities (location,misc , organization, person) from articles content. We also added to this representation the set of named entities (disease, medication, procedure, symptom) made available as additional resources by the mesinesp organizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Keywords representation (KEYWORDS).</head><p>The last kind of multi-word representation we have included are keywords extracted with statistical methods from articles textual content. We have employed the implementation of TextRank algorithm <ref type="bibr" coords="4,437.82,484.08,12.80,10.91" target="#b3">[4]</ref> provided by the textacy library <ref type="foot" coords="4,201.35,495.87,3.71,7.97" target="#foot_5">6</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exact matches of DeCS labels (MATCHES).</head><p>In addition to these representations, we also have employed a pattern matching approach to extract exact matches of DeCS labels and of their corresponding synonyms from the abstract text. In our case we have added to the document representation as index term each one of those matches in order to maintain its absolute occurrence frequency. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Inverted DeCS code profiles</head><p>Apache Lucene provides a general information retrieval engine that implements a vector space model with different well-known scoring algorithms, such as TF-IDF, BM25 variants, and others.</p><p>Lucene maintains an inverted index where it links the index terms extracted by its analyzers with the documents where they appear and maintains information about occurrence frequencies of these index terms in order to calculate the query scores.</p><p>As a complementary experiment to our proposal of sparse similarity, instead of using a conventional retrieval system we have proposed our own simplified version of an inverted index at descriptor level. Each possible index term is linked to a list of DeCS codes with which it maintains a degree of co-correlation greater than a certain threshold. The intuition behind this approach is that the presence of certain indexing terms in a given document is a good predictor of the convenience of labeling that document with DeCS codes strongly linked, from a co-occurrence point of view, with those terms.</p><p>To implement this idea we have used as a co-occurrence metric between index terms and DeCS codes the Normalized Pointwise Mutual Information (NPMI), calculated on the training set as follows, being 𝑡 an index term and 𝑑 a DeCS code:</p><formula xml:id="formula_0" coords="6,229.31,203.34,135.47,24.43">𝑁 𝑃 𝑀 𝐼(𝑡, 𝑐) = 𝑃 𝑀 𝐼(𝑡, 𝑑) -𝑙𝑜𝑔(𝑃 (𝑡, 𝑐))</formula><p>where 𝑃 𝑀 𝐼 is the Pointwise Mutual Information computed by For the construction of these inverse DeCS code profiles, we have treated separately the single index terms, corresponding to representations of type lemmas, and the compound index terms, which correspond to the multi-word terms extracted by ners, nps and keywords representations. As thresholds for the NPMI co-occurrence metric we have used the values 0.25, 0.50 and 0.75, linking with each index term, both single and compound, the DeCS codes whose co-occurrence measured according to NPMI exceeds these thresholds.</p><formula xml:id="formula_1" coords="6,88.89,258.70,282.58,50.31">𝑃 𝑀 𝐼(𝑡, 𝑐) = 𝑙𝑜𝑔 (︂ 𝑃 (𝑡, 𝑐) 𝑃 (𝑡) • 𝑃 (𝑐) )︂ with 𝑃 (𝑡, 𝑐) = |docs.</formula><p>With these inverted descriptor profiles we have implemented a simple matching scheme to annotate an input document. Given a document to be annotated, its simple and compound terms are extracted, using the methods described in the preceding section. Using the described term-to-code profiles, the NPMI co-occurrence scores of each possible candidate DeCS code are accumulated in a table every time one of the terms related to a given DeCS code appears.</p><p>To build the final list of DeCS code candidates to be assigned to a given test document we use as a reference the set of codes predicted by the sparse similarity method described in the previous section. This reference set determines the number of DeCS codes to predict, 𝑛, and provides additional codes needed to fulfill that number of output codes whether the number of DeCS codes with higher accumulated co-occurrence scores predicted with the DeCS codes profiles are less than 𝑛.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Similarity with dense representations</head><p>In recent years we are experiencing the rise of powerful language models such as BERT and other similar approaches that have increased the performance of multiple language processing tasks and have allowed that solutions based on Transformer models to dominate the stateof-the-art in many NLP fields today. A natural evolution of these word embeddings is to In this context we have evaluated the possibility of taking advantage of these dense semantic representations of whole sentences as a basis for an approach similar to the one described in the previous section. We replace the use of text indexers to match similar documents with the search for similar vectors in the dense vector space where documents from the training dataset are represented. The procedure that we follow to generate the dense vectors that will represent a document as a whole, either from training or test collections, is the following:</p><p>• The paragraphs of the document are split into sentences and the dense vector that represents every sentence is calculated using Sentence Transformers models. • The dense representation of the whole document is calculated as the mean vector of the dense vectors extracted from the sentences that the abstract is comprised of.</p><p>Once we have the dense representations of the training documents using this procedure, we use the FAISS <ref type="bibr" coords="7,168.74,489.69,12.93,10.91" target="#b2">[3]</ref> library <ref type="foot" coords="7,217.59,487.93,3.71,7.97" target="#foot_7">8</ref> to create a searchable index on these dense vectors. This index allows us to efficiently calculate distances between dense vectors and determine for the dense vector associated with a given test abstract (our query vector) the list of 𝑘 closest training dense vectors using the Euclidean distance or other similarity metrics on vectors.</p><p>By having this mechanism of similarity between dense vectors, the procedure used to annotate the test documents is analogous to that one used with the sparse similarity approach with Lucene indices. In this case we can directly use the real distances between the query vector generated from the text to be annotated and the most similar 𝑘 dense vectors provided by FAISS library. With these distances, the number of labels to be assigned is estimated and the output DeCS codes are selected by means of the weighted voting scheme already described in section 2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Premilinary results</head><p>In this section we briefly present the results of a series of preliminary experiments carried out to validate the methods described in the previous sections and to characterize the parameters to be used in our official runs submitted to the challenge. All of these experiments have used the data provided by the organization of the mesinesp2 challenge for sub-track-1 <ref type="bibr" coords="8,435.64,363.21,11.45,10.91" target="#b1">[2]</ref>, with a train dataset with 237,574 articles annotated with DeCS codes and a development dataset with 1,065 documents.</p><p>In the case of assigning DeCS codes using similarity over sparse representations supported by an Apache Lucene index, we have separately evaluated the performance of the different methods of extraction of index terms introduced in section 2. We also tested different values for the parameter 𝑘, the number of neighbors considered to predict the number of labels and to vote the final list of output labels. Table <ref type="table" coords="8,263.84,458.05,4.97,10.91" target="#tab_0">1</ref> shows the results obtained in this previous evaluation.</p><p>Regarding the use of the inverse profiles of DeCS codes, we have evaluated the use of simple index terms, compound index terms and a mixture of both to build the DeCS codes profiles. Using in all cases the three co-occurrence thresholds previously indicated, 0.25, 0.5, 0.75. To determine the number of DeCS codes to predict in each test document and to provide additional codes, the result list from best execution of the sparse similarity scheme in Table <ref type="table" coords="8,458.15,525.80,5.17,10.91" target="#tab_0">1</ref> has been used as reference. The results of these experiments with inverse profiles are shown in Table <ref type="table" coords="8,500.19,539.35,3.74,10.91" target="#tab_2">2</ref>.</p><p>Finally, in the case of assigning DeCS codes through similarity over dense representations, we have evaluated the use of Sentence Transformers with two different pretrained language models, one multilingual model<ref type="foot" coords="8,199.35,578.24,3.71,7.97" target="#foot_8">9</ref> and a Spanish monolingual model<ref type="foot" coords="8,366.10,578.24,7.41,7.97" target="#foot_9">10</ref> . We also evaluated different values for the 𝑘 parameter. The obtanied results are detailed in Table <ref type="table" coords="8,398.22,593.54,3.74,10.91" target="#tab_3">3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Official runs and discussion</head><p>Although our team has submitted results to all the sub-tracks of the mesinesp challenge, a parameterization adapted to the specific characteristics of each sub-track has not been carried out. All the configurations used in the official runs have been identical in the three sub-tracks only with adjustments in the number of neighbors considered according to the results of previous experiments with the provided development datasets. The only exception is sub-track-3 where a substantially different configuration has been used in one of the submitted runs.</p><p>In table <ref type="table" coords="9,137.73,587.09,5.17,10.91" target="#tab_4">4</ref> the official performance measures obtained by our runs in the three mesinesp2 sub-tasks are shown. The official runs submitted during our participation were created using the following configurations: iria1. This run followed the sparse similarity approach described in section 2. The sparse representation of mesinesp articles was created using all of the index term extraction methods described in section 2.1. During indexing and querying, terms appearing in 5 or less abstracts and terms used in more than 50% of training documents were discarded.</p><p>The number of neighbors used by the 𝑘-NN classifier was 20 and the predicted number of descriptors to be returned was increased by a 10% in order to ensure slightly better values in recall related measures.</p><p>iria2. For this run in Sub-track-1 the same setup as iria1 was employed, but instead of using the original train dataset this run applied a sort of Label Powerset approach proposed in our previous participation at mesinesp challenge <ref type="bibr" coords="10,337.37,177.23,11.48,10.91" target="#b7">[8]</ref>. A new training dataset annotated with those "metalabels" was created by joining pairs of DeCS labels with NPMI cooccurrence scores above 0.25. This dataset was indexed and processed as described for run iria1.</p><p>In Sub-track-3 iria2 setup followed the inverse DeCS codes profile approach from section 2.2. The employed profiles were a mix of single and compound profiles with a threshold of 0.75 for the co-occurrence scores. Instead of using the results of a sparse method as reference this runs was created directly over the set of exact matches extracted from the abstract text (matches representation).</p><p>iria3. This run followed the dense similarity approach introduced in section 3. We employed the multilingual word model to create dense vectors for every training document and indexed those vectors in a FAISS index. The number of neighbors used by the 𝑘-NN classifier was 30 and, as in iria1 run, the predicted number of descriptors to be returned was increased by a 10%.</p><p>iria4. This run employed the inverse DeCS codes profile approach introduced in section 2.2. The employed profiles were a mix of single and compound profiles created using a threshold of 0.75 for the co-occurrence scores between terms and DeCS codes. The reference results employed by this approach were those from iria1.</p><p>iria-mix. This run mixed the predictions of iria1 and the predictions of iria3, adding the exact matches extracted from the textual content of the labeled abstract (matches representation). Predictions from iria1 and iria3 had a weight of 1.0 and the DeCS labels matched on the abstract text were weighted by 1.5.</p><p>The results of our participation in the mesinesp task of the BioASQ biomedical semantic indexing challenge were not very competitive, far from the performance of the winner teams. In any case, we think that our experience confirms the suitability of methods based on similarity as a viable alternative for large scale text categorization in rich domains such as biomedical document collections.</p><p>We have evaluated different classification methods based on similarity over sparse and dense representations. In our experiments the best results have been obtained using sparse representations where different index term extraction techniques were combined. That confirmed the results in our previous BioASQ participation, with small performance improvements mainly due to improvements in the quality of the employed NLP tools and models.</p><p>Results with similarity over dense representations were generally disappointing. The proposed method, a simple mean of sentence based dense vectors, was extremely simple and it remains for future to evaluate better approaches that could improve the dense representation of the documents as a whole. In the same way, it is expected that fine-tuning the language models using biomedical texts will improve their performance and this is precisely one of the lines of future work to experiment with.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,88.99,90.49,371.91,434.90"><head>Table 1</head><label>1</label><figDesc>Performance comparison of term extraction approaches in sparse representations.</figDesc><table coords="5,134.37,133.41,326.54,391.98"><row><cell></cell><cell>𝑘</cell><cell>MiF</cell><cell>MiP</cell><cell>MiR</cell><cell>MaF</cell><cell>MaP</cell><cell>MaR</cell><cell>Acc</cell></row><row><cell>all</cell><cell>5</cell><cell>0.3830</cell><cell>0.4030</cell><cell>0.3650</cell><cell>0.2625</cell><cell>0.3718</cell><cell>0.2640</cell><cell>0.2502</cell></row><row><cell></cell><cell>10</cell><cell>0.4011</cell><cell>0.4230</cell><cell>0.3814</cell><cell>0.2727</cell><cell>0.4275</cell><cell>0.2724</cell><cell>0.2633</cell></row><row><cell></cell><cell>20</cell><cell>0.4117</cell><cell>0.4349</cell><cell>0.3909</cell><cell>0.2592</cell><cell>0.5206</cell><cell>0.2571</cell><cell>0.2684</cell></row><row><cell></cell><cell>30</cell><cell>0.4110</cell><cell>0.4343</cell><cell>0.3901</cell><cell>0.2690</cell><cell>0.4902</cell><cell>0.2667</cell><cell>0.2690</cell></row><row><cell></cell><cell>40</cell><cell>0.4052</cell><cell>0.4283</cell><cell>0.3845</cell><cell>0.2424</cell><cell>0.5253</cell><cell>0.2416</cell><cell>0.2617</cell></row><row><cell>stems</cell><cell>5</cell><cell>0.3768</cell><cell>0.3976</cell><cell>0.3581</cell><cell>0.2579</cell><cell>0.3686</cell><cell>0.2577</cell><cell>0.2464</cell></row><row><cell></cell><cell>10</cell><cell>0.3985</cell><cell>0.4210</cell><cell>0.3784</cell><cell>0.2683</cell><cell>0.4246</cell><cell>0.2667</cell><cell>0.2618</cell></row><row><cell></cell><cell>20</cell><cell>0.4065</cell><cell>0.4302</cell><cell>0.3854</cell><cell>0.2619</cell><cell>0.4759</cell><cell>0.2588</cell><cell>0.2654</cell></row><row><cell></cell><cell>30</cell><cell>0.4059</cell><cell>0.4297</cell><cell>0.3846</cell><cell>0.2467</cell><cell>0.5032</cell><cell>0.2445</cell><cell>0.2635</cell></row><row><cell></cell><cell>40</cell><cell>0.4032</cell><cell>0.4264</cell><cell>0.3823</cell><cell>0.2391</cell><cell>0.5217</cell><cell>0.2380</cell><cell>0.2601</cell></row><row><cell>lemmas</cell><cell>5</cell><cell>0.3762</cell><cell>0.3963</cell><cell>0.3581</cell><cell>0.2526</cell><cell>0.3656</cell><cell>0.2538</cell><cell>0.2454</cell></row><row><cell></cell><cell>10</cell><cell>0.3963</cell><cell>0.4181</cell><cell>0.3766</cell><cell>0.2656</cell><cell>0.4222</cell><cell>0.2658</cell><cell>0.2595</cell></row><row><cell></cell><cell>20</cell><cell>0.4057</cell><cell>0.4280</cell><cell>0.3855</cell><cell>0.2621</cell><cell>0.4765</cell><cell>0.2599</cell><cell>0.2648</cell></row><row><cell></cell><cell>30</cell><cell>0.4045</cell><cell>0.4271</cell><cell>0.3841</cell><cell>0.2483</cell><cell>0.5067</cell><cell>0.2473</cell><cell>0.2616</cell></row><row><cell></cell><cell>40</cell><cell>0.4009</cell><cell>0.4235</cell><cell>0.3807</cell><cell>0.2399</cell><cell>0.5274</cell><cell>0.2388</cell><cell>0.2580</cell></row><row><cell>ners</cell><cell>5</cell><cell>0.2811</cell><cell>0.3080</cell><cell>0.2584</cell><cell>0.1682</cell><cell>0.2675</cell><cell>0.1690</cell><cell>0.1712</cell></row><row><cell></cell><cell>10</cell><cell>0.2974</cell><cell>0.3270</cell><cell>0.2727</cell><cell>0.1681</cell><cell>0.3149</cell><cell>0.1670</cell><cell>0.1805</cell></row><row><cell></cell><cell>20</cell><cell>0.3072</cell><cell>0.3386</cell><cell>0.2811</cell><cell>0.1612</cell><cell>0.3687</cell><cell>0.1602</cell><cell>0.1847</cell></row><row><cell></cell><cell>30</cell><cell>0.3064</cell><cell>0.3381</cell><cell>0.2801</cell><cell>0.1542</cell><cell>0.3950</cell><cell>0.1526</cell><cell>0.1826</cell></row><row><cell></cell><cell>40</cell><cell>0.3033</cell><cell>0.3348</cell><cell>0.2772</cell><cell>0.1487</cell><cell>0.4091</cell><cell>0.1484</cell><cell>0.1791</cell></row><row><cell>keywords</cell><cell>5</cell><cell>0.3346</cell><cell>0.3514</cell><cell>0.3194</cell><cell>0.1991</cell><cell>0.3027</cell><cell>0.2001</cell><cell>0.2164</cell></row><row><cell></cell><cell>10</cell><cell>0.3507</cell><cell>0.3689</cell><cell>0.3342</cell><cell>0.2010</cell><cell>0.3460</cell><cell>0.1994</cell><cell>0.2261</cell></row><row><cell></cell><cell>20</cell><cell>0.3580</cell><cell>0.3770</cell><cell>0.3408</cell><cell>0.1952</cell><cell>0.3992</cell><cell>0.1908</cell><cell>0.2300</cell></row><row><cell></cell><cell>30</cell><cell>0.3592</cell><cell>0.3789</cell><cell>0.3415</cell><cell>0.1863</cell><cell>0.4454</cell><cell>0.1805</cell><cell>0.2290</cell></row><row><cell></cell><cell>40</cell><cell>0.3579</cell><cell>0.3775</cell><cell>0.3402</cell><cell>0.1749</cell><cell>0.4743</cell><cell>0.1684</cell><cell>0.2265</cell></row><row><cell>nps</cell><cell>5</cell><cell>0.2111</cell><cell>0.2497</cell><cell>0.1828</cell><cell>0.0895</cell><cell>0.1735</cell><cell>0.0875</cell><cell>0.1185</cell></row><row><cell></cell><cell>10</cell><cell>0.2257</cell><cell>0.2681</cell><cell>0.1949</cell><cell>0.0889</cell><cell>0.2145</cell><cell>0.0853</cell><cell>0.1265</cell></row><row><cell></cell><cell>20</cell><cell>0.2305</cell><cell>0.2744</cell><cell>0.1987</cell><cell>0.0811</cell><cell>0.2571</cell><cell>0.0756</cell><cell>0.1273</cell></row><row><cell></cell><cell>30</cell><cell>0.2289</cell><cell>0.2728</cell><cell>0.1971</cell><cell>0.0713</cell><cell>0.2697</cell><cell>0.0659</cell><cell>0.1247</cell></row><row><cell></cell><cell>40</cell><cell>0.2282</cell><cell>0.2723</cell><cell>0.1964</cell><cell>0.0650</cell><cell>0.2811</cell><cell>0.0596</cell><cell>0.1232</cell></row><row><cell>deps</cell><cell>5</cell><cell>0.3483</cell><cell>0.3648</cell><cell>0.3332</cell><cell>0.2138</cell><cell>0.3144</cell><cell>0.2170</cell><cell>0.2261</cell></row><row><cell></cell><cell>10</cell><cell>0.3630</cell><cell>0.3808</cell><cell>0.3468</cell><cell>0.2180</cell><cell>0.3633</cell><cell>0.2170</cell><cell>0.2359</cell></row><row><cell></cell><cell>20</cell><cell>0.3702</cell><cell>0.3899</cell><cell>0.3524</cell><cell>0.2122</cell><cell>0.4167</cell><cell>0.2095</cell><cell>0.2385</cell></row><row><cell></cell><cell>30</cell><cell>0.3654</cell><cell>0.3849</cell><cell>0.3479</cell><cell>0.1981</cell><cell>0.4444</cell><cell>0.1939</cell><cell>0.2325</cell></row><row><cell></cell><cell>40</cell><cell>0.3628</cell><cell>0.3823</cell><cell>0.3452</cell><cell>0.1894</cell><cell>0.4681</cell><cell>0.1840</cell><cell>0.2292</cell></row><row><cell>matches</cell><cell>-</cell><cell>0.2574</cell><cell>0.2016</cell><cell>0.3559</cell><cell>0.3171</cell><cell>0.3815</cell><cell>0.3674</cell><cell>0.1517</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,89.29,295.46,416.90,55.72"><head></head><label></label><figDesc>labeled with 𝑐 containing term 𝑡| |docs. in training collection| , 𝑃 (𝑡) = |docs. containing term 𝑡| |docs. in training collection| and 𝑃 (𝑐) = |docs. labeled with 𝑐| |docs. in training collection| . The measure 𝑁 𝑃 𝑀 𝐼(𝑡, 𝑐) normalizes the values of 𝑃 𝑀 𝐼(𝑡, 𝑐) in [-1, 1], resulting in -1 for a term 𝑡 and a DeCS code 𝑐 never occurring together, 0 for independence, and +1 for complete co-occurrence of term 𝑡 and code 𝑐.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,88.99,90.49,417.27,228.23"><head>Table 2</head><label>2</label><figDesc>Performance results with inverse DeCS code profiles.</figDesc><table coords="7,89.29,133.62,416.97,185.10"><row><cell>terms type</cell><cell>threshold</cell><cell>MiF</cell><cell>MiP</cell><cell>MiR</cell><cell>MaF</cell><cell>MaP</cell><cell>MaR</cell><cell>Acc</cell></row><row><cell>single</cell><cell>025</cell><cell>0.1707</cell><cell>0.1803</cell><cell>0.1620</cell><cell>0.1593</cell><cell>0.2692</cell><cell>0.1752</cell><cell>0.0973</cell></row><row><cell></cell><cell>050</cell><cell>0.2860</cell><cell>0.3022</cell><cell>0.2716</cell><cell>0.2192</cell><cell>0.2906</cell><cell>0.2594</cell><cell>0.1716</cell></row><row><cell></cell><cell>075</cell><cell>0.4147</cell><cell>0.4381</cell><cell>0.3937</cell><cell>0.2858</cell><cell>0.4694</cell><cell>0.2990</cell><cell>0.2689</cell></row><row><cell>compound</cell><cell>025</cell><cell>0.2174</cell><cell>0.2297</cell><cell>0.2064</cell><cell>0.1958</cell><cell>0.2413</cell><cell>0.2285</cell><cell>0.1276</cell></row><row><cell></cell><cell>050</cell><cell>0.3052</cell><cell>0.3224</cell><cell>0.2897</cell><cell>0.2557</cell><cell>0.2713</cell><cell>0.2939</cell><cell>0.1870</cell></row><row><cell></cell><cell>075</cell><cell>0.4247</cell><cell>0.4486</cell><cell>0.4032</cell><cell>0.2838</cell><cell>0.4959</cell><cell>0.2901</cell><cell>0.2768</cell></row><row><cell>both</cell><cell>025</cell><cell>0.2417</cell><cell>0.2553</cell><cell>0.2295</cell><cell>0.2224</cell><cell>0.3025</cell><cell>0.2492</cell><cell>0.1432</cell></row><row><cell></cell><cell>050</cell><cell>0.3183</cell><cell>0.3362</cell><cell>0.3021</cell><cell>0.2694</cell><cell>0.3157</cell><cell>0.3126</cell><cell>0.1960</cell></row><row><cell></cell><cell>075</cell><cell>0.4191</cell><cell>0.4427</cell><cell>0.3979</cell><cell>0.2950</cell><cell>0.4554</cell><cell>0.3123</cell><cell>0.2721</cell></row><row><cell cols="9">move them towards embeddings at the sentence-level with approaches as those provided by</cell></row><row><cell cols="9">SentenceTransformers [6] project 7 that allows to convert sentences in natural languages into</cell></row><row><cell cols="4">dense vectors with enriched semantics.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,88.99,90.49,364.53,155.83"><head>Table 3</head><label>3</label><figDesc>Performance results with dense representations.</figDesc><table coords="8,141.75,133.41,311.77,112.91"><row><cell></cell><cell>𝑘</cell><cell>MiF</cell><cell>MiP</cell><cell>MiR</cell><cell>MaF</cell><cell>MaP</cell><cell>MaR</cell><cell>Acc</cell></row><row><cell>mono</cell><cell>5</cell><cell>0.2818</cell><cell>0.2916</cell><cell>0.2726</cell><cell>0.1390</cell><cell>0.2234</cell><cell>0.1415</cell><cell>0.1790</cell></row><row><cell></cell><cell>10</cell><cell>0.3019</cell><cell>0.3130</cell><cell>0.2915</cell><cell>0.1470</cell><cell>0.3364</cell><cell>0.1446</cell><cell>0.1917</cell></row><row><cell></cell><cell>20</cell><cell>0.3097</cell><cell>0.3206</cell><cell>0.2995</cell><cell>0.1477</cell><cell>0.4344</cell><cell>0.1419</cell><cell>0.1965</cell></row><row><cell></cell><cell>30</cell><cell>0.3103</cell><cell>0.3213</cell><cell>0.3000</cell><cell>0.1466</cell><cell>0.4872</cell><cell>0.1394</cell><cell>0.1969</cell></row><row><cell></cell><cell>40</cell><cell>0.3103</cell><cell>0.3213</cell><cell>0.3000</cell><cell>0.1461</cell><cell>0.5208</cell><cell>0.1377</cell><cell>0.1971</cell></row><row><cell>multi</cell><cell>5</cell><cell>0.3443</cell><cell>0.3595</cell><cell>0.3302</cell><cell>0.1861</cell><cell>0.2926</cell><cell>0.1883</cell><cell>0.2221</cell></row><row><cell></cell><cell>10</cell><cell>0.3642</cell><cell>0.3793</cell><cell>0.3504</cell><cell>0.1970</cell><cell>0.3744</cell><cell>0.1961</cell><cell>0.2373</cell></row><row><cell></cell><cell>20</cell><cell>0.3716</cell><cell>0.3869</cell><cell>0.3574</cell><cell>0.1948</cell><cell>0.4377</cell><cell>0.1931</cell><cell>0.2420</cell></row><row><cell></cell><cell>30</cell><cell>0.3731</cell><cell>0.3879</cell><cell>0.3593</cell><cell>0.1935</cell><cell>0.4780</cell><cell>0.1904</cell><cell>0.2429</cell></row><row><cell></cell><cell>40</cell><cell>0.3709</cell><cell>0.3853</cell><cell>0.3574</cell><cell>0.1880</cell><cell>0.4937</cell><cell>0.1841</cell><cell>0.2408</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,88.99,90.49,381.16,342.06"><head>Table 4</head><label>4</label><figDesc>Official results for BioASQ mesinesp2 Task.</figDesc><table coords="9,125.13,115.68,345.03,316.86"><row><cell></cell><cell></cell><cell></cell><cell cols="2">Sub-track 1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>system</cell><cell>rank MiF</cell><cell>EBP</cell><cell>EBR</cell><cell>EBF</cell><cell>MaP MaR MaF</cell><cell>MiP</cell><cell>MiR</cell><cell>Acc.</cell></row><row><cell>best</cell><cell cols="8">1/26 0.4837 0.5077 0.4736 0.4763 0.5237 0.3990 0.3926 0.5077 0.4618 0.3261</cell></row><row><cell>iria-mix</cell><cell cols="8">15/26 0.3725 0.4245 0.3402 0.3662 0.5345 0.2326 0.2354 0.4193 0.3351 0.2341</cell></row><row><cell>iria-4</cell><cell cols="8">17/26 0.3656 0.3938 0.3481 0.3585 0.4476 0.2877 0.2760 0.3909 0.3435 0.2279</cell></row><row><cell>iria-1</cell><cell cols="8">18/26 0.3406 0.3670 0.3238 0.3339 0.4236 0.2348 0.2315 0.3641 0.3199 0.2089</cell></row><row><cell>iria-2</cell><cell cols="8">19/26 0.3389 0.3650 0.3218 0.3319 0.4214 0.2327 0.2293 0.3622 0.3185 0.2073</cell></row><row><cell cols="9">mesinesp baseline 20/26 0.2876 0.2449 0.3839 0.2841 0.3720 0.3787 0.3438 0.2335 0.3746 0.1710</cell></row><row><cell>iria-3</cell><cell cols="8">21/26 0.2537 0.2758 0.2337 0.2460 0.2869 0.0854 0.0817 0.2729 0.2369 0.1480</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Sub-track 2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>system</cell><cell>rank MiF</cell><cell>EBP</cell><cell>EBR</cell><cell>EBF</cell><cell>MaP MaR MaF</cell><cell>MiP</cell><cell>MiR</cell><cell>Acc.</cell></row><row><cell>best</cell><cell cols="8">1/21 0.3640 0.3666 0.3655 0.3558 0.4177 0.3391 0.3102 0.3666 0.3614 0.2242</cell></row><row><cell>iria-1</cell><cell cols="8">12/21 0.2454 0.2303 0.2625 0.2379 0.3167 0.1863 0.1534 0.2289 0.2644 0.1411</cell></row><row><cell>iria-4</cell><cell cols="8">14/21 0.2003 0.1919 0.2142 0.1958 0.1620 0.2049 0.1571 0.1868 0.2158 0.1132</cell></row><row><cell>iria-mix</cell><cell cols="8">15/21 0.2003 0.1919 0.2142 0.1958 0.1620 0.2049 0.1571 0.1868 0.2158 0.1132</cell></row><row><cell>iria-3</cell><cell cols="8">16/21 0.1562 0.1422 0.1681 0.1502 0.1617 0.0730 0.0505 0.1419 0.1736 0.0857</cell></row><row><cell cols="9">mesinesp baseline 17/21 0.1288 0.0971 0.3791 0.1452 0.0977 0.3619 0.2403 0.0781 0.3678 0.0802</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Sub-track 3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>system</cell><cell>rank MiF</cell><cell>EBP</cell><cell>EBR</cell><cell>EBF</cell><cell>MaP MaR MaF</cell><cell>MiP</cell><cell>MiR</cell><cell>Acc.</cell></row><row><cell>best</cell><cell cols="8">1/21 0.4514 0.4487 0.4662 0.4494 0.5041 0.4271 0.4138 0.4487 0.4541 0.3005</cell></row><row><cell>iria-2</cell><cell cols="8">7/21 0.3203 0.3509 0.2878 0.3061 0.4980 0.3166 0.3171 0.3657 0.2849 0.1910</cell></row><row><cell cols="9">mesinesp baseline 8/21 0.2992 0.4117 0.2298 0.2827 0.5290 0.2497 0.2518 0.4293 0.2296 0.1779</cell></row><row><cell>iria-mix</cell><cell cols="8">13/21 0.2542 0.2790 0.2414 0.2528 0.4659 0.2284 0.2213 0.2750 0.2364 0.1526</cell></row><row><cell>iria-4</cell><cell cols="8">16/21 0.2169 0.2251 0.2085 0.2119 0.3105 0.2442 0.2289 0.2232 0.2109 0.1288</cell></row><row><cell>iria-1</cell><cell cols="8">18/21 0.1871 0.1941 0.1826 0.1844 0.2589 0.1966 0.1825 0.1926 0.1820 0.1093</cell></row><row><cell>iria-3</cell><cell cols="8">19/21 0.0793 0.0824 0.0758 0.0777 0.1120 0.0598 0.0501 0.0822 0.0765 0.0437</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.93,660.07,211.34,8.97"><p>Source code available at https://github.com/..../mesinesp2.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,108.93,671.03,94.91,8.97"><p>https://lucene.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,108.93,671.02,100.89,8.97"><p>http://snowball.tartarus.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,108.93,649.03,103.85,8.97"><p>Available at https://spacy.io/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="4,108.93,659.99,314.07,8.97"><p>Detailed list of UD relationships available at https://universaldependencies.org/u/dep/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="4,108.93,670.94,107.56,8.97"><p>https://textacy.readthedocs.io</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="7,108.93,660.02,83.11,8.97"><p>https://www.sbert.net/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="7,108.93,670.98,153.35,8.97"><p>https://github.com/facebookresearch/faiss</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="8,108.93,638.11,398.43,8.97;8,89.29,649.07,163.08,8.97"><p>Using pretrained sentence-level model stsb-xlm-r-multilingual from Sentence Transformers project. Provides dense vectors with 768 dimensions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9" coords="8,108.93,660.03,397.06,8.97;8,89.29,670.99,205.02,8.97"><p>Using pretrained word-level model mrm8488/electricidad-base-generator from Hugging Face models repository. Provides dense vectors with 256 dimensions.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p><rs type="person">F.J. Ribadas-Pena</rs> and <rs type="person">S. Cao</rs> have been supported by <rs type="funder">ERDF/MICINN-AEI</rs> (<rs type="grantNumber">TIN2017-85160-C2-2-R</rs> and <rs type="grantNumber">PID2020-113230RB-C22</rs>), and by the <rs type="funder">Galician Regional Government (Xunta de Galicia)</rs> under projects <rs type="grantNumber">ED431D-2018/50</rs> and <rs type="grantNumber">ED431D-2017/12</rs>. E. Kuriyozov received funding from <rs type="funder">ERDF/MICINN-AEI</rs> (<rs type="projectName">ANSWER-ASAP</rs>, <rs type="grantNumber">TIN2017-85160-C2-1-R</rs>, and <rs type="funder">SCANNER-UDC</rs>, <rs type="grantNumber">PID2020-113230RB-C21</rs>), from <rs type="funder">Xunta de Galicia</rs> (<rs type="grantNumber">ED431C 2020/11</rs>) and from <rs type="funder">Centro de Investigación de Galicia "CITIC</rs>", funded by <rs type="funder">Xunta de Galicia</rs> and the <rs type="funder">European Union</rs> (<rs type="funder">European Regional Development Fund</rs><rs type="programName">-Galicia 2014-2020 Program</rs>), by grant <rs type="grantNumber">ED431G 2019/01</rs>. He is also funded for his PhD by <rs type="funder">El-Yurt-Umidi Foundation</rs> under the <rs type="funder">Cabinet of Ministers of the Republic of Uzbekistan</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_RCrZwQD">
					<idno type="grant-number">TIN2017-85160-C2-2-R</idno>
				</org>
				<org type="funding" xml:id="_gMbGU9p">
					<idno type="grant-number">PID2020-113230RB-C22</idno>
				</org>
				<org type="funding" xml:id="_ZTkBkTC">
					<idno type="grant-number">ED431D-2018/50</idno>
				</org>
				<org type="funding" xml:id="_7qch4QM">
					<idno type="grant-number">ED431D-2017/12</idno>
				</org>
				<org type="funded-project" xml:id="_fzCVPGu">
					<idno type="grant-number">TIN2017-85160-C2-1-R</idno>
					<orgName type="project" subtype="full">ANSWER-ASAP</orgName>
				</org>
				<org type="funding" xml:id="_yasWkRg">
					<idno type="grant-number">PID2020-113230RB-C21</idno>
				</org>
				<org type="funding" xml:id="_Z2bhcCK">
					<idno type="grant-number">ED431C 2020/11</idno>
				</org>
				<org type="funding" xml:id="_e3n7UPz">
					<idno type="grant-number">ED431G 2019/01</idno>
					<orgName type="program" subtype="full">-Galicia 2014-2020 Program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="11,112.66,353.26,395.01,10.91;11,112.66,366.81,395.01,10.91;11,112.66,380.36,393.33,10.91;11,112.66,393.91,101.78,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,306.55,366.81,196.84,10.91">Overview of BioASQ 2021-MESINESP track</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gasco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Estrada-Zavala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R-T</forename><surname>Murasaki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Primo-Peña</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bojo-Canales</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,112.66,380.36,355.17,10.91">Evaluation of advance hierarchical classification techniques for scientific literature</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>patents and clinical trials</note>
</biblStruct>

<biblStruct coords="11,112.66,407.46,393.33,10.91;11,112.66,421.01,393.33,10.91;11,112.66,434.55,360.09,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,279.64,407.46,226.35,10.91;11,112.66,421.01,127.46,10.91">MESINESP2 Corpora: Annotated data for medical semantic indexing in Spanish</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gasco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Antonio</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.4722925</idno>
		<ptr target="https://doi.org/10.5281/zenodo.4722925" />
	</analytic>
	<monogr>
		<title level="m" coord="11,272.75,421.01,233.24,10.91;11,112.66,434.55,113.43,10.91">Funded by the Plan de Impulso de las Tecnologías de las del Lenguaje (Plan TL)</title>
		<meeting><address><addrLine>Zenodo</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,448.10,393.32,10.91;11,112.66,461.65,274.54,10.91" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<ptr target="https://arxiv.org/abs/1702.08734" />
		<title level="m" coord="11,254.44,448.10,179.47,10.91">Billion-scale similarity search with GPUs</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,475.20,393.33,10.91;11,112.66,488.75,76.23,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,210.43,475.20,154.22,10.91">TextRank: Bringing order into texts</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,371.26,475.20,134.72,10.91;11,112.66,488.75,46.58,10.91">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,502.30,394.61,10.91;11,112.66,515.85,395.17,10.91;11,112.66,529.40,211.94,10.91" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="11,112.66,515.85,395.17,10.91;11,112.66,529.40,183.78,10.91">Overview of BioASQ 2021: The ninth BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Katsimpras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Vandorou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gasco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,542.95,393.33,10.91;11,112.66,556.50,393.33,10.91;11,112.66,570.05,351.45,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,225.20,542.95,280.79,10.91;11,112.66,556.50,103.65,10.91">Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,223.76,556.50,282.22,10.91;11,112.66,570.05,128.11,10.91">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,583.60,393.33,10.91;11,112.66,597.15,393.33,10.91;11,112.66,610.69,126.20,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,426.04,583.60,79.94,10.91;11,112.66,597.15,312.59,10.91">CoLe and UTAI at BioASQ 2015: Experiments with Similarity Based Descriptor Assignment</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">J</forename><surname>Ribadas-Pena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">M</forename><surname>De Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">M</forename><surname>Darriba-Bilbao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">E</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="11,432.50,597.15,73.49,10.91;11,112.66,610.69,51.81,10.91">CEUR Workshop Proceedings</title>
		<imprint>
			<biblScope unit="volume">1391</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,624.24,394.62,10.91;11,112.66,637.79,395.01,10.91;11,112.66,651.34,22.69,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,302.75,624.24,204.53,10.91;11,112.66,637.79,213.21,10.91">CoLe and LYS at BioASQ MESINESP8 Task: Similarity based Descriptor Assigment in Spanish</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">J</forename><surname>Ribadas-Pena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kuriyozov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="11,332.96,637.79,127.00,10.91">CEUR Workshop Proceedings</title>
		<imprint>
			<biblScope unit="volume">2696</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
