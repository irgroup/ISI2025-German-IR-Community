<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,348.96,15.42;1,89.29,106.66,383.73,15.42">MDS_UNCC Question Answering System for Biomedical Data with Preliminary Error Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,151.58,11.96"><forename type="first">Seethalakshmi</forename><surname>Gopalakrishnan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computing and Informatics</orgName>
								<orgName type="institution">University of North Carolina at Charlotte</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,253.52,134.97,82.14,11.96"><forename type="first">Swathi</forename><surname>Padithala</surname></persName>
							<email>spaditha@uncc.edu</email>
							<affiliation key="aff1">
								<orgName type="department">School of Data Science</orgName>
								<orgName type="institution">University of North Carolina at Charlotte</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,348.30,134.97,80.03,11.96"><forename type="first">Hilmi</forename><surname>Demirhan</surname></persName>
							<email>hdemirha@uncc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computing and Informatics</orgName>
								<orgName type="institution">University of North Carolina at Charlotte</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,88.72,148.92,87.95,11.96"><forename type="first">Wlodek</forename><surname>Zadrozny</surname></persName>
							<email>wzadrozn@uncc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computing and Informatics</orgName>
								<orgName type="institution">University of North Carolina at Charlotte</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Data Science</orgName>
								<orgName type="institution">University of North Carolina at Charlotte</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,348.96,15.42;1,89.29,106.66,383.73,15.42">MDS_UNCC Question Answering System for Biomedical Data with Preliminary Error Analysis</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">039722E741F6DD54048B29F8A657C173</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>BioASQ</term>
					<term>Question Answering</term>
					<term>Error Analysis</term>
					<term>Factoid</term>
					<term>Yes/No</term>
					<term>List</term>
					<term>Modal Verbs</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we are describing the submission that we made for the 9th year BioASQ competition. The BioASQ challenge aims to promote methodologies and systems for large-scale biomedical semantic indexing and question answering. There are shared tasks that are the benchmark datasets through the BioASQ website yearly. The dataset represents the information needed from biomedical experts. This paper has worked on the 9th task with the BioASQ BioBERT model based on the Bidirectional Encoder Representations from the Transformers (BERT) model. We have fine-tuned the BioASQ BioBERT model and submitted our results both with training and without training. The results show that fine-tuning with training the model gives better results. For the Batch5 factoid submission, we have got an MRR of 0.52, which is higher than the original version of BioASQ BioBERT. For Yes/No, we got the F1 score of 0.81, and for the list, the F1 was 0.26.</p><p>We also present preliminary results of our error analysis, where we hypothesize about the causes of some errors, and run simple experiments to confirm or disprove them. For example, we see that the presence of the natural language modalities -which are quite common in questions, answers and snippets -influences the accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Question Answering (QA) focuses the Information Retrieval (IR) system on the actual information need of the user. Performance on a QA task reflects domain knowledge and natural language understanding ability of a system.</p><p>BioASQ, Biomedical Semantic Question Answering Challenge, is one of the famous competitions that ask practitioners to develop a system for unstructured biomedical text and decision support from PubMed articles. The system is supposed to automatically respond to biomedical questions with relevant concepts, articles, snippets, exact answers and summaries. BioASQ challenge is a competition of document classification, information retrieval and question answering. The participants are expected to achieve the goal of submitting ideal and user-understandable answers to the four types of natural language questions by combining the information from biomedical articles and ontologies. The four types of questions from benchmark datasets are Yes/No questions, Factoid questions, List questions, and Summary questions. One example of Yes/No questions is that "Do CpG islands colocalize with transcription start sites?". One example for a Factoid question is that "Which virus is best known as the cause of infectious mononucleosis?" . One example for a List question is "Which are the Raf kinase inhibitors?". Also one example for a Summary question is that "What is the treatment of infectious mononucleosis?". There are two phases in the challenge: Phase A and Phase B. During phase A, participating systems have to reply with related concepts which are from designated terminologies and ontologies, related articles in English, related snippets and RDF triples. During phase B, participating systems need to have responses with exact and ideal answers in English.</p><p>This project builds on the 2019 BioASQ experiments reported in <ref type="bibr" coords="2,391.57,222.46,11.55,10.91" target="#b0">[1]</ref>. In the project, we are using Task 9b data (the 9th year BioASQ competition shared task). In Task 9a, participants are asked to classify new abstracts written in English, as they became available online. The classes came from the MESH hierarchy, i.e., the subject headings that are currently used to manually index the abstracts. As new manual annotations became available, they were used to evaluate the classification performance of participating systems (that classified articles before they were manually annotated), using standard information retrieval (IR) measures (e.g., precision, recall, accuracy), as well as hierarchical variants of these measures. The models we are using are BioBERT which is based on Bidirectional Encoder Representations from Transformers (BERT) model <ref type="bibr" coords="2,190.29,344.40,12.73,10.91" target="#b1">[2]</ref> which is pre-trained on Wikipedia articles. The BioBERT model was pre-trained with biomedical text using the PubMed and PMC articles <ref type="bibr" coords="2,401.12,357.95,11.51,10.91" target="#b2">[3]</ref>. As the participants, we had to annotate input natural language questions with biomedical concepts, and retrieve relevant documents, snippets and triples (Phase A). Eventually, we need to find and report the answers to the questions (Phase B), given as additional input the golden responses of the Phase A.</p><p>Our contributions in this paper are as follows:</p><p>-We describe our systems, which placed 4th in Task 9b Phase B. In data preparation, we created a new way of converting the BioASQ format to BioBERT format, and we make our code available https://github.com/seetagopal/BioASQ-2021.git -We performed error analysis (on training data) of the Yes/No questions, and hypothesized about causes of some errors. These, e.g. include the presence of modal verbs/auxiliaries such as 'may'.</p><p>-We performed a statistical analysis of Wh-questions and of the modal verbs/auxiliaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BioASQ Related Work and the Competition Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">BERT and BioBERT</head><p>BERT stands for "Bidirectional Encoder Representations from Transformers" is a contextual word embedding model which was developed in 2018 by Google <ref type="bibr" coords="2,372.37,613.68,11.28,10.91" target="#b1">[2]</ref>. It is a contextualized word representation model that is pre-trained using bidirectional transformers <ref type="bibr" coords="2,408.70,627.23,11.28,10.91" target="#b3">[4]</ref>. The model takes a sentence as an input and outputs a contextual embedding of the word. Currently Google Search Engine uses BERT model for over 70 languages <ref type="bibr" coords="2,306.33,654.33,11.58,10.91" target="#b4">[5]</ref>. In addition to search, BERT can be used for additional tasks such as question answering and language inference. BERT's pre-trained deep bidirectional representations from unlabeled text can be modified with an additional output layer for these different tasks. Bidirectional representations are crucial in biomedical text mining to represent relationships in a biomedical corpus <ref type="bibr" coords="3,368.74,114.06,11.58,10.91" target="#b5">[6]</ref>. In this work, we are using BERT for question answering tasks. Question and paragraph (context) are given as an input to the model. In this work, we used BioBERT models. BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining) is a domain-specific language representation model pre-trained on large-scale biomedical corpora developed by Lee et al. <ref type="bibr" coords="3,428.37,181.81,11.40,10.91" target="#b6">[7]</ref>. BioBERT and BERT have very similar architecture. Since it is pre-trained on biomedical corpora, it achieves a better performance than BERT in biomedical text mining tasks.</p><p>Lee et al. <ref type="bibr" coords="3,147.03,222.46,12.99,10.91" target="#b6">[7]</ref> have fine-tuned BioBERT for question answering by using the same BERT architecture used for SQuAD <ref type="bibr" coords="3,225.34,236.01,11.58,10.91" target="#b7">[8]</ref>. They used the BioASQ factoid datasets for fine-tuning as factoid datasets have similarity to that of SQUAD. Some of the BioASQ factoid questions were unanswerable as exact answers were not present in the given texts. The unanswerable questions were removed from the training sets. They used the same pre-training process of Wiese et al. <ref type="bibr" coords="3,103.93,290.20,12.99,10.91" target="#b8">[9]</ref> as they have also used SQuAD. They have used strict accuracy, lenient accuracy and mean reciprocal rank as evaluation metrics. Yoon et al. <ref type="bibr" coords="3,354.51,303.75,18.07,10.91" target="#b9">[10]</ref> have also used BioBERT for answering biomedical questions including factoid, list, and yes/no type questions. Jin et al. <ref type="bibr" coords="3,488.22,317.30,17.76,10.91" target="#b10">[11]</ref> have reviewed the Biomedical Question Answering approaches by classifying them in 6 major methodologies namely open-domain, knowledge base, information retrieval, machine reading comprehension, question entailment and visual QA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Prior BioASQ work</head><p>This year ninth edition of the BioASQ Challenge is being held. The previous year, 8th BioASQ challenge can be summarized as follows <ref type="bibr" coords="3,273.56,421.23,16.41,10.91" target="#b11">[12]</ref>. There were three tasks last year: Task 8a, task 8b, task MESINESP. Task 8a was a large-scale biomedical semantic indexing task. Task 8b was a biomedical question answering task. Task MESINESP was a new task on medical semantic indexing in Spanish (task MESINESP).</p><p>Some of the task 8b submissions were as follows. Kazaryan et al. <ref type="bibr" coords="3,394.81,475.42,18.03,10.91" target="#b12">[13]</ref> have participated as ITMO team. They used BERT fine-tuned on SQUAD <ref type="bibr" coords="3,333.62,488.97,11.58,10.91" target="#b7">[8]</ref>. They also used a model based on BioMed-RoBERTa <ref type="bibr" coords="3,169.79,502.52,17.76,10.91" target="#b13">[14]</ref> to improve the produced answers. Ozyurt et al. <ref type="bibr" coords="3,395.15,502.52,17.76,10.91" target="#b14">[15]</ref> used Electra <ref type="bibr" coords="3,469.97,502.52,17.75,10.91" target="#b15">[16]</ref> and BioBERT <ref type="bibr" coords="3,132.22,516.07,12.82,10.91" target="#b6">[7]</ref> on SQuAD and BioASQ datasets combined. Pappas et al. <ref type="bibr" coords="3,401.27,516.07,17.89,10.91" target="#b16">[17]</ref> experimented with a SciBERT-based model for exact answer extraction <ref type="bibr" coords="3,324.74,529.62,18.05,10.91" target="#b17">[18]</ref> modelled for cloze-style biomedical machine reading comprehension <ref type="bibr" coords="3,237.61,543.17,17.91,10.91" target="#b18">[19]</ref> (MRC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">BioASQ Data</head><p>For the 9th BioASQ tasks, training dataset for Task 9a is about semantic indexing. Task 9a data contains MeSH terms MEDLINE curators annotated in the biomedical articles from PubMed. Different years version means different MeSH terms used in the articles from PubMed besides different sizes. Task 9b is about Question Answering; and as part of the task in the data we get concepts, articles, snippets, RDF triples, "exact" answers and "ideal" answers in JSON format.</p><p>Task 9a, named "Large-scale online biomedical semantic indexing", has 15,559,157 articles.</p><p>In each article, there are an average 12.68 MeSH terms annotated. 29,369 MeSH covered total. The dataset is 7.9 GB zipped and 25.6 GB unzipped. Task 9b, named "Introductory biomedical semantic QA, " uses benchmark datasets containing development and test questions, in English, along with gold standard (reference) answers. The benchmark datasets are being constructed by a team of biomedical experts from around Europe. Below is the number of questions in each category in the 9b training data. Yes/No: 1033, Factoid: 1092, List: 719, Summary: 899. The number of questions in each category in Task 9b Batch5 test data: Yes/No: 19, Factoid: 36, List: 18, Summary: 27. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data Preparation and Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data Preparation</head><p>The main purpose of our data preparation is to convert the BioASQ data into the format which is accepted by the BioBERT. For the factoid questions we are creating a dictionary in which the question id, question, answer, context will be added. BioBERT will expect the start index of the answers during the training. So, one of the main tasks of this data preparation is to find the start of the answers. In order to find the start index, we are performing the set operation on the ideal and exact answers to find the unique answers for that question. Once we have the answers the next step is to find out what is the index of those answer in the given snippet. If the answer is present in the snippet, then that will be set as the start index which we are doing by setting flags. Apart from creating a dictionary to append all the information given above we are also adding three more numbers to the given id. This is necessary because the given id will be of length 24 but the evaluation script in BioBERT to convert the BioBERT predictions into BioASQ format will expect the id of length 28. The same procedure is followed for the list type questions. For Yes/No same procedure is followed except for finding the answer start index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experiments: Finetuning BioBERT</head><p>We are using the pretrained weights provided by the BioASQ BioBERT model <ref type="bibr" coords="5,434.73,163.04,16.38,10.91" target="#b19">[20,</ref><ref type="bibr" coords="5,453.85,163.04,7.54,10.91" target="#b2">3]</ref>. The first step for training this model is to convert the given data into the BioBERT format which includes the answer start index. The process of how we are doing this is explained in the data preparation (Section 3). In the BioASQ BioBERT model, they have released the pretrained weights for the factoid, list and Yes/No questions separately. These weights are pretrained on SQUAD dataset on top of the BioBERT model. We are using those weights to train the BioBERT model for the BioASQ training 9b data. Once the training is done, we are using the weights obtained as a result of training for the prediction. We are following this procedure for factoid, list and Yes/No question answer. Once we got the predictions we ran the script provided in the BioASQ BioBERT model to convert the predictions into the BioASQ format. The overview of the model is given in Figure <ref type="figure" coords="5,169.16,298.53,3.74,10.91" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>We have submitted our results to the 9th Batch of BioASQ competition by finetuning the BiOASQ BioBERT model on the BioASQ Task9B -Phase B dataset. We have submitted our predictions in the Batch 5 of the BioASQ competition and we are placed 4th in the leader board. Our system name is MDS_UNCC. The results that we got for our Test Batch5 submission is given in Table <ref type="table" coords="5,500.35,397.05,3.66,10.91" target="#tab_0">1</ref>. BioASQ competition results for the Batch5 submission. KU-DMIS is placed first in the leaderboard. MDS_UNCC is our submission which is placed 4th in the competition. For the List questions we got an Mean Precision of 0.3259, recall of 0.2963 and F-measure of 0.2678 (which is about 12% below the best system).</p><formula xml:id="formula_0" coords="5,170.96,430.97,14.45,8.87">Yes</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Error Analysis</head><p>In this section we discuss some aspects of our error analysis. We start with Batch 3, but we mostly focus on Batch 5. We perform both error analysis and data analysis in this section. In particular we discuss the presence of the modalities in the data, types of questions, and the accuracy on these types.</p><p>The Yes/No test data prediction file from the Batch3 has identified 87% Yes Accuracy and 54% No Accuracy; that is, most of the Yes were identified correctly but almost half of No answers were identified wrong. Upon doing the further analysis, we see that the probability of No questions was underestimated. We hypothesize that corpus expansion <ref type="bibr" coords="6,407.83,127.61,16.51,10.91" target="#b20">[21,</ref><ref type="bibr" coords="6,427.08,127.61,14.08,10.91" target="#b21">22]</ref> would help in improving the accuracy, but we have not yet tested this hypothesis.</p><p>For Factoid questions, we observed a pattern of lower probabilities for the best and longest match, and the higher probability for initial sentences was higher. For List questions, the probability was again smaller for the long text and the probability for initial sentences higher; moreover, the best matching words were often left out. Thus, we need to find a solution to include the best match words and for matching the longest sentence. In our BioASQ Batch5 submission we got an accuracy of 78% for Yes/No questions. In order to find out where the machine fails with the Yes/No questions, we are presenting a more detailed error analysis for the Yes/No questions in this section. Since the golden data for the task9b</p><p>is not yet published, we have divided the training data in train and test data using the scikit learn library with train data 80% and test data 20%. We had 835 Yes/No questions in the train data and 207 questions as test data. We ran BioASQ-BioBERT on this train data and made some predictions. Out of 207 Yes/No questions, we got 184 questions answered correctly and 23 questions answered incorrectly. For further analysis, we have checked these 23 questions manually along with the snippets. Our analysis show that in many of the questions, the machine cannot understand the synonyms and the antonyms. Also, for few complex questions coreference resolution is needed. For example:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 1:</head><p>Question: Does dronedarone affect T3 and T4 levels? Actual answer: No Predicted answer: Yes Snippet: Amiodarone resulted in increased T4, T4/T3 and rT3, whereas dronedarone did not alter the thyroid hormone profile in normal animals.</p><p>In the above example the expected answer is No but the predicted answer was Yes. In the snippet, the first part talks about Amiodarone, whereas the second part gives the answer for the above question and says that the dronedarone did not alter the thyroid hormone profile. This question can be answered correctly only if the machine can understand that T3, T4 refers to the thyroid hormone.</p><p>In order to check whether our hypothesis is correct, we have changed the above question into "Does dronedarone alter the thyroid hormone level?" and ran BioASQ-BioBERT. Changing the question predicted this answer correctly. We can infer that the proper dealing of coreference resolution is needed.  One other example which explains the understanding of modality is important is given below: Example 2: Question: Is cardiac magnetic resonance imaging indicated in the pre-participation screening of athletes? Actual answer: No Predicted answer: Yes Snippet: As modern imaging further enhances our understanding of the spectrum of athlete's heart, its role may expand from the assessment of athletes with suspected disease to being part of comprehensive pre-participation screening in apparently healthy athletes.</p><p>In the above example the expected answer was No and it was predicted as Yes. Looking into the snippet, we can understand that modern imaging may be used for pre-participation but this is not necessary. If the machine can understand the modal verb "may" then this question can be answered correctly.</p><p>To further analyze the role of modals in BioASQ data we have collected the count of modals in Accuracy of different type of questions and the over all accuracy. There are no "wh" questions in the Yes/No type. There are no "What is X" and "What is of/using.. " questions in the list, Yes/No type. In most of the categories with 0% accuracy the number of questions is very less with a minimum count of 1 and a maximum count of 10 questions.</p><p>the training data which is given in Table <ref type="table" coords="8,269.96,335.91,3.68,10.91" target="#tab_1">2</ref>. This count shows that there are a number of modal verbs present in the training data. If the machine can understand the modals during the train, this may help in improving the accuracy. Also, from Table <ref type="table" coords="8,357.60,363.01,5.17,10.91" target="#tab_1">2</ref> we can see that the percentage of modals in the answers is higher than that of the questions. A better understanding of the relationship between the modals in the questions and answers may help improve the accuracy further.</p><p>In order to investigate our claim about the role of the modalities, we have changed the above question in Example 2 into "May cardiac magnetic resonance imaging be indicated in the pre-participation screening of athletes?". After changing the question the model has predicted the answer correctly which suggests the importance of understanding the modality. Obviously, we need to do more work on a larger set of examples to prove or disprove this hypothesis.</p><p>Example 3: Question: Does the BRAFV600E mutation have an effect on clinical response to radioiodine therapy? Actual answer: Yes Predicted answer: No Snippet: Preclinical studies showed that BRAF mutation significantly reduced radioiodine uptake and decreased the sensitivity to radioactive iodine (RAI) therapy.</p><p>In the above example, the question asks about the effect of BRAFV600E. In the snippet, it shows that BRAF significantly reduced radioiodine which is an effect of BRAF. This question can be answered correctly if the machine can understand that 'effect' and 'reduced' are synonyms here. To investigate this claim we have changed the question to "Does the BRAFV600E mutation have reduced on clinical response to radioiodine therapy?". However, the system still predicts the answer wrongly. We can infer from this result that a better understanding about how the machine interprets these synonyms and antonyms is necessary.</p><p>Another aspect of the analysis is shown in Table <ref type="table" coords="9,321.80,86.97,3.80,10.91" target="#tab_3">3</ref>. Namely we see that the distribution of types of questions is different in the training and test data.</p><p>The accuracy results for the three types of questions (Yes/No, List, and Factoid) are computed on the test data we obtained by splitting the original training data, as described earlier. We calculate the accuracy by finding the exact match between the correct answers in the training data and the predictions we got for the List and Factoid questions. Out of the obtained list of predictions, even if one of the predictions is correct, we consider that question to be answered correctly. The results of the accuracy we got for the different types of questions are given in the Table <ref type="table" coords="9,115.79,208.91,3.74,10.91" target="#tab_4">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Summary and Conclusions</head><p>In this article, we described our contribution to the 9th BioASQ competition. We showed the process of retraining the BioASQ-BioBERT model used in our experiments leads to better results. However, to improve our results further, we need better understanding of the models, as well as of the data; that is, questions, answers, and snippets. Regarding the models, in accordance with common knowledge, we believe that both larger data sets for training and corpus expansion for background knowledge <ref type="bibr" coords="9,212.04,335.28,16.43,10.91" target="#b20">[21,</ref><ref type="bibr" coords="9,231.20,335.28,14.03,10.91" target="#b21">22]</ref> should lead to improved results. Based on a few examples, we also predict that deeper language understanding, e.g. coreference and synonym resolution could have an impact on accuracy. As for the data, we showed that modalities are potentially important, and are present surprisingly frequently in both questions and answers. Again, this suggests that we should experiment with deeper NLP methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,404.88,416.69,8.93;4,88.93,416.89,417.05,8.87;4,89.29,428.84,416.70,8.87;4,89.29,440.80,416.70,8.87;4,89.29,452.75,134.76,8.87;4,113.96,214.75,364.88,182.70"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the model. First step is to prepare the data (Section 3) into BioBERT format which will then be used by the BioASQ-BioBERT model for training and prediction. The last step is the error analysis which is explained in Section 5. The major finding of our error analysis of Yes/No questions was that understanding coreference resolution, modality, synonyms and antonyms may help in improving the model accuracy</figDesc><graphic coords="4,113.96,214.75,364.88,182.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,88.99,430.97,398.11,86.12"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="5,105.68,430.97,381.42,68.64"><row><cell></cell><cell>/No</cell><cell></cell><cell></cell><cell></cell><cell>Factoid</cell><cell></cell></row><row><cell>System</cell><cell cols="7">Macro F1 F1 Yes F1 No Accuracy Strict Acc. Lenient Acc. MRR</cell></row><row><cell>KU-DMIS-2</cell><cell>0.8246</cell><cell>0.88</cell><cell cols="2">0.7692 0.8421</cell><cell>0.3889</cell><cell>0.6111</cell><cell>0.4722</cell></row><row><cell>KU-DMIS-3</cell><cell>0.8246</cell><cell>0.88</cell><cell cols="2">0.7692 0.8421</cell><cell>4167</cell><cell>0.5556</cell><cell>0.4745</cell></row><row><cell>KU-DMIS-5</cell><cell>0.8246</cell><cell>0.83</cell><cell cols="2">0.7692 0.8421</cell><cell>0.3889</cell><cell>0.5833</cell><cell>0.4583</cell></row><row><cell cols="2">MDS_UNCC 0.7841</cell><cell cols="2">0.8182 0.75</cell><cell>0.7895</cell><cell>0.4167</cell><cell>0.6944</cell><cell>0.5204</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,88.99,233.85,417.24,210.04"><head>Table 2</head><label>2</label><figDesc>Number of modals present in the BioASQ training9b dataset. In this table, column indicates the modal verb and the row indicates the count of the modal verb for questions, answers and snippets in the dataset. The last row indicates the percentage of modals present in each of the questions, answers and snippets. We hypothesize that a better understanding of the role of modals can improve the accuracy of question answering.</figDesc><table coords="6,185.22,233.85,222.35,132.40"><row><cell></cell><cell cols="3">Questions Answers Snippets</cell></row><row><cell>Can</cell><cell>58</cell><cell>319</cell><cell>140</cell></row><row><cell>Could</cell><cell>0</cell><cell>55</cell><cell>45</cell></row><row><cell>May</cell><cell>6</cell><cell>201</cell><cell>107</cell></row><row><cell>Might</cell><cell>0</cell><cell>22</cell><cell>21</cell></row><row><cell>Should</cell><cell>5</cell><cell>30</cell><cell>16</cell></row><row><cell>Will</cell><cell>0</cell><cell>24</cell><cell>33</cell></row><row><cell>Would</cell><cell>4</cell><cell>12</cell><cell>9</cell></row><row><cell>Must</cell><cell>0</cell><cell>11</cell><cell>4</cell></row><row><cell>Possible</cell><cell>8</cell><cell>19</cell><cell>23</cell></row><row><cell cols="2">Percent of modals 7%</cell><cell>59%</cell><cell>33%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,88.99,413.27,418.53,44.78"><head>Table 3</head><label>3</label><figDesc>Percentage allocation of most frequent question types in training for Task 9b and Batch5 test dataset.Here the Complement of "Wh" indicates the questions which do not start with "Wh. " (The percentages are rounded-off; and some types of "what is .... " questions are not counted here).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,88.99,86.42,330.62,174.98"><head>Table 4</head><label>4</label><figDesc></figDesc><table coords="8,173.18,86.42,246.44,157.11"><row><cell></cell><cell cols="2">Yes/No List</cell><cell>Factoid</cell></row><row><cell>Starts with "Wh"</cell><cell>N/A</cell><cell>37%</cell><cell>30%</cell></row><row><cell>starts with "Which"</cell><cell>N/A</cell><cell>44%</cell><cell>43%</cell></row><row><cell>starts with "List"</cell><cell>N/A</cell><cell>53%</cell><cell>N/A</cell></row><row><cell>starts with "What is"</cell><cell>N/A</cell><cell>0%</cell><cell>32%</cell></row><row><cell>starts with "What are"</cell><cell>N/A</cell><cell>19%</cell><cell>0%</cell></row><row><cell>starts with "Where"</cell><cell>N/A</cell><cell>0%</cell><cell>33%</cell></row><row><cell cols="2">other type of "What questions" N/A</cell><cell cols="2">12.5% 16%</cell></row><row><cell>complement of wh and list</cell><cell>N/A</cell><cell>42%</cell><cell>N/A</cell></row><row><cell>starts with "What is X?"</cell><cell>N/A</cell><cell>N/A</cell><cell>0%</cell></row><row><cell>starts with what is of/using ..?</cell><cell>N/A</cell><cell>N/A</cell><cell>13%</cell></row><row><cell>Complement of wh questions</cell><cell>89%</cell><cell>N/A</cell><cell>24%</cell></row><row><cell>Overall accuracy</cell><cell>89%</cell><cell>43%</cell><cell>31%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors acknowledge the help of <rs type="person">David Ruddell</rs> in data preparation.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,506.73,393.33,10.91;9,112.66,520.28,393.33,10.91;9,112.66,533.83,304.58,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,293.38,506.73,212.61,10.91;9,112.66,520.28,153.87,10.91">UNCC biomedical semantic question answering systems. BioASQ: Task-7B, Phase-B</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">K</forename><surname>Telukuntla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kapri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,289.70,520.28,216.29,10.91;9,112.66,533.83,173.81,10.91">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="695" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,547.38,393.33,10.91;9,112.66,560.93,290.70,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,323.34,547.38,182.64,10.91;9,112.66,560.93,181.08,10.91">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,317.08,560.93,54.78,10.91">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,574.48,393.33,10.91;9,112.66,588.02,397.48,10.91;9,112.36,604.02,156.70,7.90" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,361.64,574.48,144.35,10.91;9,112.66,588.02,254.86,10.91">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btz682</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,376.31,588.02,65.15,10.91">Bioinformatics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,615.12,395.17,10.91;9,112.66,628.67,326.92,10.91" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m" coord="9,149.36,628.67,107.76,10.91">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,642.22,394.04,10.91;9,112.66,655.77,259.48,10.91" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="9,220.93,642.22,95.85,10.91">Bert (language model)</title>
		<ptr target="https://en.wikipedia.org/wiki/BERT_" />
		<imprint>
			<date type="published" when="2021-05">2021. 11-May-2021</date>
		</imprint>
	</monogr>
	<note>Wikipedia contributors</note>
</biblStruct>

<biblStruct coords="10,112.66,86.97,394.52,10.91;10,112.66,100.52,393.33,10.91;10,112.66,114.06,350.24,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,112.66,100.52,288.03,10.91">Overview of the biocreative vi chemical-protein interaction track</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Rabal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Akhondi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Santamaría</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">P</forename><surname>Rodríguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,423.42,100.52,82.57,10.91;10,112.66,114.06,215.81,10.91">Proceedings of the sixth BioCreative challenge evaluation workshop</title>
		<meeting>the sixth BioCreative challenge evaluation workshop</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="141" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,127.61,393.33,10.91;10,112.66,141.16,393.98,10.91;10,112.41,154.71,48.96,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,361.64,127.61,144.35,10.91;10,112.66,141.16,268.25,10.91">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,394.29,141.16,66.92,10.91">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,168.26,393.33,10.91;10,112.66,181.81,280.07,10.91" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<title level="m" coord="10,324.88,168.26,181.11,10.91;10,112.66,181.81,98.46,10.91">Squad: 100,000+ questions for machine comprehension of text</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,195.36,393.32,10.91;10,112.66,208.91,226.59,10.91" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="10,280.85,195.36,225.13,10.91;10,112.66,208.91,43.99,10.91">Neural domain adaptation for biomedical question answering</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wiese</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Neves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03610</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,222.46,393.33,10.91;10,112.66,236.01,393.33,10.91;10,112.66,249.56,232.59,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,310.77,222.46,195.22,10.91;10,112.66,236.01,83.70,10.91">Pre-trained language model for biomedical question answering</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,219.53,236.01,286.45,10.91;10,112.66,249.56,101.82,10.91">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="727" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,263.11,393.33,10.91;10,112.66,276.66,383.17,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05281</idno>
		<title level="m" coord="10,455.55,263.11,50.44,10.91;10,112.66,276.66,200.90,10.91">Biomedical question answering: A comprehensive review</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,290.20,394.62,10.91;10,112.66,303.75,277.56,10.91" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="10,367.13,290.20,140.15,10.91;10,112.66,303.75,245.64,10.91">Overview of bioasq 8a and 8b: Results of the eighth edition of the bioasq tasks a and b</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Bougiatiotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,317.30,393.33,10.91;10,112.66,330.85,210.69,10.91" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kazaryan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Sazanovich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Belyaev</surname></persName>
		</author>
		<title level="m" coord="10,302.87,317.30,203.12,10.91;10,112.66,330.85,180.08,10.91">Transformer-based open domain biomedical question answering at bioasq8 challenge</title>
		<imprint/>
	</monogr>
	<note>????</note>
</biblStruct>

<biblStruct coords="10,112.66,344.40,394.53,10.91;10,112.66,357.95,393.33,10.91;10,112.66,371.50,107.17,10.91" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Marasović</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10964</idno>
		<title level="m" coord="10,112.66,357.95,316.96,10.91">Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,385.05,393.32,10.91;10,112.66,398.60,243.66,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,298.36,385.05,207.62,10.91;10,112.66,398.60,140.29,10.91">Bio-answerfinder: a system to find answers to questions from biomedical texts</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">B</forename><surname>Ozyurt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bandrowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Grethe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,260.83,398.60,40.56,10.91">Database</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,412.15,393.33,10.91;10,112.66,425.70,347.38,10.91" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<title level="m" coord="10,335.14,412.15,170.85,10.91;10,112.66,425.70,165.13,10.91">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,439.25,393.33,10.91;10,112.66,452.79,214.74,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,319.00,439.25,186.98,10.91;10,112.66,452.79,90.54,10.91">Aueb-nlp at bioasq 8: Biomedical document and snippet retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Stavropoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,211.10,452.79,24.17,10.91">CLEF</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>cit. on p. 41</note>
</biblStruct>

<biblStruct coords="10,112.66,466.34,393.33,10.91;10,112.66,479.89,222.60,10.91" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00051</idno>
		<title level="m" coord="10,307.05,466.34,198.93,10.91;10,112.66,479.89,40.39,10.91">Reading wikipedia to answer open-domain questions</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,493.44,393.60,10.91;10,112.66,506.99,146.44,10.91" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10676</idno>
		<title level="m" coord="10,234.39,493.44,239.83,10.91">Scibert: A pretrained language model for scientific text</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,520.54,393.33,10.91;10,112.66,534.09,393.32,10.91;10,112.66,547.64,375.81,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="10,310.77,520.54,195.22,10.91;10,112.66,534.09,84.78,10.91">Pre-trained language model for biomedical question answering</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,353.21,534.09,152.77,10.91;10,112.66,547.64,101.82,10.91">Machine Learning and Knowledge Discovery in Databases</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Cellier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Driessens</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="727" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,561.19,393.33,10.91;10,112.66,574.74,393.32,10.91;10,112.66,588.29,335.76,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="10,461.80,561.19,44.19,10.91;10,112.66,574.74,179.45,10.91">Statistical source expansion for question answering</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Schlaefer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zadrozny</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ferrucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,315.49,574.74,190.49,10.91;10,112.66,588.29,247.00,10.91">Proceedings of the 20th ACM international conference on Information and knowledge management</title>
		<meeting>the 20th ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="345" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,601.84,393.33,10.91;10,112.66,615.39,280.12,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="10,359.92,601.84,146.07,10.91;10,112.66,615.39,158.42,10.91">Source expansion for information retrieval and information extraction</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">M</forename><surname>Schlaefer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">W</forename><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,303.55,615.39,43.80,10.91">US Patent</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">550</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
