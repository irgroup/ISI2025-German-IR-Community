<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.03,75.53,451.56,17.00;1,72.03,96.28,451.55,17.00;1,72.03,117.05,76.99,17.00">NCU-IISR/AS-GIS: Results of Various Pre-trained Biomedical Language Models and Linear Regression Model in BioASQ Task 9b Phase B</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.03,150.10,48.27,10.80"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Central University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">CLEF</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,133.30,150.10,71.51,10.80"><forename type="first">Jen-Chieh</forename><surname>Han</surname></persName>
							<email>joyhan@cc.ncu.edu.tw</email>
						</author>
						<author>
							<persName coords="1,232.09,150.10,71.54,10.80"><forename type="first">Richard</forename><surname>Tzong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Central University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">CLEF</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,309.75,150.10,41.91,10.80"><forename type="first">Han</forename><surname>Tsai</surname></persName>
							<email>thtsai@csie.ncu.edu.tw</email>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">IoX Center</orgName>
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Research Center for Humanities and Social Sciences</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.03,75.53,451.56,17.00;1,72.03,96.28,451.55,17.00;1,72.03,117.05,76.99,17.00">NCU-IISR/AS-GIS: Results of Various Pre-trained Biomedical Language Models and Linear Regression Model in BioASQ Task 9b Phase B</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B29F7F812B8BD81C03246A7469BC2A88</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Biomedical Question Answering</term>
					<term>Pre-trained Language Model</term>
					<term>Linear Regression</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformer has been widely applied in Natural Language Processing (NLP) field, and it also results in an amount of pre-trained language models like BioBERT, SciBERT, NCBI_Bluebert, and PubMedBERT. In this paper, we introduce our system for the BioASQ Task 9b Phase B. We employed various pre-trained biomedical language models, including BioBERT, BioBERT-MNLI, and PubMedBERT, to generate "exact" answers for the questions, and a linear regression model with our sentence embedding to construct the top-n sentences as a prediction for "ideal" answers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.25" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.25" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.25" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.25" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.25" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.25" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.25" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.25" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.25" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Given the rapid growth of people's interest in Artificial Intelligence (AI), and biomedical questionanswering has been receiving attention <ref type="bibr" coords="1,249.35,432.36,8.18,9.90" target="#b0">[1]</ref><ref type="bibr" coords="1,257.53,432.36,4.09,9.90" target="#b1">[2]</ref><ref type="bibr" coords="1,261.62,432.36,8.18,9.90" target="#b2">[3]</ref>. Is AI able to answer a biomedical question, like "Does metformin interfere thyroxine absorption?", correctly? Is AI able to give textual evidence for its answer? To facilitate answering these questions, we participated in BioASQ Task 9b Phase B (QA task), where participants should return either an exact answer or an ideal answer based on the given biomedical question and List of question-relevant articles/snippets. BioASQ Task 9b PhaseB task provided 3743 training questions, including the previous year's test set with gold annotations, plus 500 test questions for evaluation, divided into five batches of 100 questions each. All questions and answers were constructed by a team of biomedical experts from across Europe and were classified into four types: Yes/no, Factoid, List, and Summary. Three types of questions required accurate answers: Yes/no, Factoid and List. For all four types of questions, participants were asked to submit the ideal answers. In Task 9b, each participant was allowed to submit up to five results per batch.</p><p>Figure <ref type="figure" coords="1,120.51,571.66,5.50,9.90" target="#fig_0">1</ref> illustrates four examples of QA types for BioASQ Task 9b Phase B (QA task). As shown in Figure <ref type="figure" coords="1,116.51,584.42,4.14,9.90" target="#fig_0">1</ref>, the BioASQ QA example gives a question and several relevant PubMed abstract fragments as relevant snippets. Therefore, we formulated the task as a query-based multi-document a. extraction for the exact answer and b. summarization for the ideal answer. Last year, we used the BioBERT model combined with logistic regression to achieve the best result in generating ideal answers at batch 5 <ref type="bibr" coords="1,503.98,622.16,11.44,9.90" target="#b3">[4]</ref>.</p><p>In this paper, we employed pre-trained language models to improve our results, including BioBERT <ref type="bibr" coords="1,72.03,647.69,11.44,9.90" target="#b4">[5]</ref>, BioBERT-MNLI <ref type="bibr" coords="1,168.83,647.69,11.63,9.90" target="#b5">[6]</ref>, and PubMedBERT <ref type="bibr" coords="1,274.38,647.69,11.44,9.90" target="#b6">[7]</ref>. BioBERT-MNLI is a fine-tuned model of BioBERT on the MultiNLI (The Multi-Genre Natural Language Inference) corpus, which is a dataset created for sentence understanding task <ref type="bibr" coords="2,198.58,407.36,11.45,9.90" target="#b7">[8]</ref>. BioBERT related models achieved the best performance in extracting exact answers last year <ref type="bibr" coords="2,181.33,420.11,11.44,9.90" target="#b5">[6]</ref>. PubMedBERT is the latest BERT model pre-trained on the biomedical corpus, which outperformed BioBERT on the BLURB (Biomedical Language Understanding and Reasoning Benchmark).</p><p>We applied Pre-trained models' [CLS] embeddings as input to a linear regression model for predicting ideal answers. The sections are organized as follows. Section 2 briefly reviews recent works on biomedical QA and pretrained model. The details of our methods are described separately in Section 3. Section 4 describes our configurations submitted to the BioASQ 9b Phase B challenge and the results. Section 5 is the discussion and summary of our system's performance in the BioASQ QA task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The acquisition of biomedical knowledge is often carried out by reading academic papers. This process is time-consuming and labor-intensive, and has a high professional threshold. Biomedical professionals cannot quickly obtain the required knowledge in a short period of time. The general public is also unable to complete the acquisition of biomedical knowledge in the absence of expert assistance. QA in natural language processing tasks has the potential to solve these problems by providing direct answers to users' questions. This tests the ability of machine learning systems to semantically understand, retrieve, and generate answers from existing text. Many QA models based on deep learning have been developed and even applied in the past <ref type="bibr" coords="2,292.63,655.94,11.44,9.90" target="#b8">[9]</ref>.</p><p>Biomedical QA Task: Biomedical QA tasks require a large amount of annotated corpus to train the model. This is a prerequisite for deep learning. In addition to BioASQ, many QA datasets annotated by biomedical experts have been published recently <ref type="bibr" coords="2,289.63,693.94,11.75,9.90" target="#b0">[1,</ref><ref type="bibr" coords="2,304.13,693.94,8.00,9.90" target="#b1">2]</ref>. The PubMedQA dataset is a research question set, and each question has a reference text from a PubMed abstract and the span of the text providing the answer (yes/maybe/no) to the research question. BioBERT generally outperformed other deep learning methods such as BiLSTM and ESIM on the PubMedQA dataset <ref type="bibr" coords="2,398.17,731.71,11.44,9.90" target="#b0">[1]</ref>. Another biomedical QA task that deserves our attention is the COVID-QA. COVID-QA is a SQuAD-like Question Answering dataset consisting of 2,019 question/answer pairs annotated by volunteer biomedical experts on scientific articles related to COVID-19. This dataset differs from traditional MRC datasets such as SQuAD in that the answers to the questions come from a much longer context <ref type="bibr" coords="3,418.67,87.27,11.44,9.90" target="#b1">[2]</ref>.</p><p>PubMedBERT: Following the successful application of BERT to natural language processing tasks in various fields, more and more specialized pre-trained language models are being developed in the biomedical field, including BioBERT, SciBERT <ref type="bibr" coords="3,315.63,125.29,16.62,9.90" target="#b9">[10]</ref>, ClinicalBERT <ref type="bibr" coords="3,415.93,125.29,16.60,9.90" target="#b10">[11]</ref>, BlueBERT <ref type="bibr" coords="3,502.22,125.29,16.80,9.90" target="#b11">[12]</ref>, PubMedBERT, and so on. Among them, PubMedBERT is the state-of-art model developed by Microsoft. Its pre-training method is different from the existing biomedical language models, PubMedBERT adopts the method of training professional texts (PubMed papers) from scratch, instead of continuing training on the basis of texts in the general domain <ref type="bibr" coords="3,359.90,175.79,11.63,9.90" target="#b6">[7]</ref>. It has outperformed BioBERT in many biomedical NER, QA, and Relation Extraction tasks.</p><p>Sequential Learning with BioBERT: The pre-trained language model effectively improves the performance of target tasks, while sequential transfer learning based on transfer learning can be used to further improve the performance of biomedical question answering. In the general QA domain, learning relationships between sentence pairs first is effective in sequential transfer learning <ref type="bibr" coords="3,448.95,239.07,16.60,9.90" target="#b12">[13]</ref>. BioBERT's research team has also found that this approach can be applied to biomedical QA. They demonstrated that fine-tuning on the language comprehension dataset and the question-answer dataset can improve BioBERT's performance on BioASQ tasks and released new fine-tuned models such as BioBERT-MNLI, BioBERT-MNLI-SQuAD <ref type="bibr" coords="3,223.60,289.81,11.44,9.90" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>For ideal answer, We basically used the similar method that we used to participate in the BioASQ 8B last year and tried to test this method on different pre-trained language models to boost performance. The goal of our method is to select the most relevant segments for each question in the BioASQ QA instance, and our work was inspired by the logistic regression model framework proposed by Diego Mollá <ref type="bibr" coords="3,100.80,399.11,16.60,9.90" target="#b13">[14]</ref>. The approach follows the two steps of his summarization process. In the first step, the input text is segmented into candidate sentences and each candidate sentence is scored. In the 2nd step, the top n sentences with the highest scores are returned. We use a pre-trained language model and replace its features with word embeddings. The training steps are as follows: 1. For the snippets and ideal answer from training set released from BioASQ organizers, we used NLTK's sentence tokenizer to divide snippets into sentences.</p><p>2. We calculated ROUGE-SU4 F1 scores <ref type="bibr" coords="4,285.63,74.77,18.16,9.90" target="#b14">[15]</ref> between each sentence and the associated ideal answer. 3. All the sentences from snippets with different generated scores were considered as candidate answers. The candidate answers, their corresponding questions and scores became the training set for our linear regression model. 4. We input a candidate answer (sentence) and the corresponding question at the same time, using score as the prediction target. The pre-trained BERT language model is used for fitting the task.</p><p>We used [CLS] embeddings representing the relation between a sentence and a question as the feature and appended a dense layer with ReLU activation after the output layer of BERT model. Mean squared error was used as the loss function. Our script is modified from Google BERT's official TensorFlow code and took default settings from BERT trained on SQuAD <ref type="bibr" coords="4,142.80,213.82,16.62,9.90" target="#b15">[16]</ref>.</p><p>For inference, we used the fine-tuned model from step 3 to predict the scores of the test data and then re-rank the candidate sentences for each question. Because the ideal answer in training set mostly consist of only one sentence, we selected only the top 1 sentence as our system output (ideal answer).</p><p>The improvement of the above method is mainly focused on the replacement of the pretrained language models. We used BioBERT-MNLI (NCU-IISR/AS-GIS-2) and PubMedBERT (NCU-IISR/AS-GIS-3) to replace BioBERT (NCU-IISR/AS-GIS-1) in the above method in an attempt to improve the performances. BioBERT-MNLI is a fine-tuned BioBERT model on the MultiNLI dataset. MultiNLI (Multi-Type Natural Language Inference), published by New York University, is a text entailment task that requires determining whether a hypothesis holds given a premise (Premise), or determining whether the hypothesis is contradictory and neutral to the premise. MNLI's main feature is that it is a collection of texts in many different domains. We believe that the task of MultiNLI dataset has a high similarity to the ideal answer selection task. On the one hand, the questions and ideal answers in the BioASQ 9b training set are usually one sentence, and the premises and assumptions in the MultiNLI data set are also one sentence. Therefore, the data lengths are basically the same. On the other hand, the questions and the ideal answers need to maintain a logical entailment relationship. We can analogize the question to the premise and the ideal answer to the hypothesis. Only answers that are logically related should be considered.</p><p>PubMedBERT is similar to BioBERT in that both are trained using the PubMed corpus. However, BioBERT adopts a continuous pre-training approach based on BERT. So it uses vocabulary from Wikipedia and the book corpus. PubMedBERT, on the other hand, is pre-trained from scratch using the PubMed text. This means that PubMed is less influenced by general domain texts and focuses on the biomedical research corpus. In addition, to test the applicability of PubMedBERT in BioASQ tasks, we also used PubMedBERT with KU-DMIS's method <ref type="bibr" coords="4,302.38,517.39,12.66,9.90" target="#b5">[6]</ref> for the exact answer task (similar to SQuAD). This method converts BioASQ's List, Factoid question and answer data format to a format similar to SQuAD. Then, it uses a fine-tuning method similar to Google's BERT on SQuAD for model training. For the Yes/No problem, it adds a linear regression layer to the BERT model for sequence binary classification. These methods have performed well in past challenges. To simplify the parameters adjustment process, We used Microsoft's open source AutoML system NNI <ref type="bibr" coords="4,403.17,580.66,18.40,9.90" target="#b16">[17]</ref> to automatically adjust the parameters of this task. However, in the ideal answer task, we did not perform multiple experiments because of the time limit.</p><p>For the hardware, we used an NVIDIA GeForce GTX 1080 GPU for Factoid, List question exact answer tasks. Ideal answer tasks and Yes/no question exact answer tasks were trained using an NVIDIA Tesla T4 GPU provided by Google Colab. Because of the limitation of GPU memory, we reduce the batch size for Factoid, List type question tasks to 4, which may affect the performance of following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Submission</head><p>Our submitted configurations are summarized in Table <ref type="table" coords="4,339.15,727.96,4.13,9.90" target="#tab_0">1</ref>. We tested the performance of the pretrained language models by conducting experiments with BioASQ 9b data for each task of the exact answer. Since the results of PubMedBERT are not as good as the BioBERT-related models in the experiments, we only submit the best KU-DMIS BioBERT-related model results. The BioBERT-MNLI model was used for the Yes/no questions, while the BioBERT-MNLI-SQuAD model was used for both the Factoid and List questions. We should additionally mention that all three systems use the same answer for the exact answer submitted. Model performances in predicting exact answers are shown in Table <ref type="table" coords="5,387.42,726.22,4.13,9.90" target="#tab_1">2</ref>. Our system performed better than the median system score for all three question types in batch 5. In particular, our system generally performed higher in the Yes/no category than on the other two question types and scored near the best Macro F1 scores for both batch 4 and batch 5. Among them, we ranked third in the fifth batch in terms of Yes/no type questions. Table <ref type="table" coords="6,99.80,112.03,4.22,11.00">3</ref>. Results (ROUGE-2 and ROUGE-SU4 F1 scores and Recall scores) of test batch 4,5 for ideal answers in the BioASQ QA task. Total Systems counts the number of participants in each batch. In batch 4 and 5, our system "NCU-IISR/AS-GIS-2" took first place out of submitted systems in both F1 scores. However, the Recall Score of our systems are lower than the best score. The performance of the model in predicting the ideal answer is shown in Table <ref type="table" coords="6,433.42,727.71,4.13,9.90">3</ref>. For ideal answers, BioASQ used two evaluation metrics: ROUGE and human evaluation. Roughly speaking, ROUGE calculates the n-gram overlap between an automatically constructed summary and a set of human-written (golden) summaries, with higher ROUGE scores being better. Specifically, ROUGE-2 and ROUGE-SU4 were used to evaluate ideal answers. These automatic evaluations are the most widely used versions of ROUGE and have been discovered to correlate well with human judgments when multiple reference summaries are available for each question. The organizers have not yet reported the results of the human evaluation (manual scoring). All ideal system answers will also be evaluated by biomedical experts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>In batch 4 and 5, our system "NCU-IISR/AS-GIS-2" took first place out of submitted systems in ROUGE-2 F1 and ROUGE-SU4 F1. In particular, in batch 4, the "NCU-IISR/AS-GIS-2" system scored 0.0664 (ROUGE-2) and 0.0721 (ROUGE-SU4) higher than the second-ranked system in terms of F1 score. However, the Recall Score of our systems are lower than the best score. This may be related to the fact that we ended up submitting only the top 1 sentence. We considered increasing the number of sentences submitted, but in the end, we did not have time to test it.</p><p>Results of internal experiments for exact answers on the BioASQ 9b dataset are shown in Table <ref type="table" coords="7,515.48,226.57,4.13,9.90" target="#tab_3">4</ref>. Both Factoid and List type problem experiments were performed using NNI to fine-tune the parameters. Each model was experimented at least 20 times to find the best performance. We conducted this experiment to examine whether PubMedBERT could achieve better results than BioBERT on the BioASQ task. The results are generally in line with our expectations. For Factoid and List type questions, PubMedBERT (especially the Fulltext version) outperformed the basic version of BioBERT. But for Yes/no questions, PubMedBERT was not even as good as the basic version of BioBERT. This result is similar to what we have seen in ideal answer, where the ROUGE-related scores of the system using PubMedBERT "NCU-IISR/AS-GIS-3" are worse than the system of the version using BioBERT "NCU-IISR/AS-GIS-1".  <ref type="bibr" coords="7,460.62,633.95,12.37,11.00" target="#b3">[4,</ref><ref type="bibr" coords="7,472.98,633.95,8.25,11.00" target="#b5">6]</ref> for batch size and <ref type="bibr" coords="7,110.48,647.45,12.38,11.00" target="#b1">[2,</ref><ref type="bibr" coords="7,122.86,647.45,8.25,11.00" target="#b2">3,</ref><ref type="bibr" coords="7,131.11,647.45,8.25,11.00" target="#b3">4]</ref> for epoch. *** Parameter search space for List type question task: [1e-6 -1e-5] for learning rate, <ref type="bibr" coords="7,450.35,660.70,12.36,11.00" target="#b3">[4]</ref> for batch size and <ref type="bibr" coords="7,91.51,674.20,12.37,11.00" target="#b0">[1,</ref><ref type="bibr" coords="7,103.87,674.20,8.25,11.00" target="#b1">2]</ref> for epoch.</p><p>Although PubMedBERT has better results than BioBERT for some tasks, it still has a gap with BioBERT-MNLI and BioBERT-MNLI-SQuAD, which have been fine-tuned with external datasets. Therefore, we did not use the PubMedBERT trained exact answer system for formal submissions. In addition, we also tried to fine-tune PubMedBERT on MultiNLI and SQuAD datasets to get better results. However, we were not able to make any progress until the end of the competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussions and Conclusions</head><p>In the 9th BioASQ QA task, we used pre-trained models including BioBERT, BioBERT-MNLI, BioBERT-MNLI-SQuAD, PubMedBERT to generate both the exact and ideal answers. In generating exact answers, we use the KU-DMIS approach to find the offsets (both start and end positions) of the answer within the given passage (snippets). Although PubMedBERT outperforms the basic version of BioBERT in Factoid, List type questions, it still cannot reach the performance of BioBERT-MNLI-SQuAD which has been fine-tuned with external datasets. This result indicates the significant effect of sequential learning using existing datasets.</p><p>When it comes to the ideal answer, the most relevant fragment or sentence was selected in order to maintain the integrity of the ideal answer, rather than taking the fragment offset approach, which may focus on the wrong location and produce imperfect answers. Our results combining BioBERT-MNLI with linear regression ranked first for both ROUGE-2 F1 and ROUGE-SU4 F1 scores in batch 4 and 5. Our results show that using the linear regression model to select sentences can yield excellent results in generating ideal answers. At the same time, BioBERT's performance on this task was significantly improved after fine-tuning with the MultiNLI dataset, which means that the sentence entailment relationships contained in the MultiNLI dataset is useful for finding the ideal answer.</p><p>However, we also found that the combined PubMedBERT scored worse than the basic version of BioBERT in generating answers for the ideal answer and the exact answer to the Yes/no question. We speculate that this may be related to the difference between pre-trained corpus of PubMedBERT and BioBERT. PubMedBERT was not trained on the general field corpus such as Wikipedia and Books, but pre-trained from scratch on the PubMed research papers corpus. Are differences between the general field corpus and the research paper corpus likely to contribute to the differences in the predictive results of these two tasks? Are there any linguistic elements that are present in general field texts but missing in biomedical research texts? We do not have sufficient evidence to answer these questions now. Future research could further explore the possible reasons for this discrepancy and conduct more experiments.</p><p>Directions for improvement for our system also include expanding the range of snippets to include full abstracts, and comparing activation or loss functions to find a better one. In the regression method, we only processed snippet context and did not use the complete PubMed abstracts. Thus, these can be utilized in the future. All told, we hope to keep the base of pre-trained language model and make an effort to combine it with different approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgments</head><p>This study is supported by the Ministry of Science and Technology, Taiwan (No.: MOST 109-2221-E-008-062-MY3).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,143.55,377.62,304.31,11.00;2,87.28,85.45,417.19,287.10"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The QA examples of the BioASQ Task 9b Phase B (QA task).</figDesc><graphic coords="2,87.28,85.45,417.19,287.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,72.03,689.44,451.42,9.90;3,72.03,701.94,358.91,9.90;3,85.65,459.48,420.65,227.05"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. How a candidate answer (sentence) and the corresponding question obtains the contextual embeddings in the last layer of the BERT model (BioBERT, PubMedBERT etc.).</figDesc><graphic coords="3,85.65,459.48,420.65,227.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,72.03,137.55,431.09,279.57"><head>Table 1 .</head><label>1</label><figDesc>Descriptions of our three systems</figDesc><table coords="5,103.55,169.55,399.57,247.57"><row><cell>System Name</cell><cell>System Description</cell><cell>Participating Batch</cell></row><row><cell></cell><cell>Exact answers: Using KU-DMIS BioBERT</cell><cell></cell></row><row><cell></cell><cell>related models.</cell><cell></cell></row><row><cell>NCU-IISR/AS-GIS-1</cell><cell>Ideal answers: Using BioBERT with</cell><cell>4,5</cell></row><row><cell></cell><cell>predicted ROUGE-SU4 scores to select</cell><cell></cell></row><row><cell></cell><cell>the top 1 sentences of snippets.</cell><cell></cell></row><row><cell></cell><cell>Exact answers: Using KU-DMIS BioBERT</cell><cell></cell></row><row><cell></cell><cell>related models.</cell><cell></cell></row><row><cell>NCU-IISR/AS-GIS-2</cell><cell>Ideal answers: Using BioBERT-MNLI with</cell><cell>4,5</cell></row><row><cell></cell><cell>predicted ROUGE-SU4 scores to select</cell><cell></cell></row><row><cell></cell><cell>the top 1 sentences of snippets.</cell><cell></cell></row><row><cell></cell><cell>Exact answers: Using KU-DMIS BioBERT</cell><cell></cell></row><row><cell></cell><cell>related models.</cell><cell></cell></row><row><cell>NCU-IISR/AS-GIS-3</cell><cell>Ideal answers: Using PubMedBERT with</cell><cell>4,5</cell></row><row><cell></cell><cell>predicted ROUGE-SU4 scores to select</cell><cell></cell></row><row><cell></cell><cell>the top 1 sentences of snippets.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,72.03,439.37,451.04,264.58"><head>Table 2 .</head><label>2</label><figDesc>Results of test batch 4,5 for exact answers in the BioASQ QA task. Total Systems counts the number of participants for each batch in the given category. For example, our system ranked third in batch 5 in Yes/no questions. Best Score indicates the best result across all participants, and Median Score the median result.</figDesc><table coords="5,77.53,504.15,437.40,199.80"><row><cell>Batch</cell><cell cols="6">Yes/no System Name Macro F1 System Name MRR System Name F-Measure Factoid List</cell></row><row><cell></cell><cell>Best Score</cell><cell>0.9480</cell><cell>Best Score</cell><cell>0.6929</cell><cell>Best Score</cell><cell>0.7061</cell></row><row><cell>4</cell><cell>Ours</cell><cell>0.8441</cell><cell>Ours</cell><cell>0.4232</cell><cell>Ours</cell><cell>0.4261</cell></row><row><cell></cell><cell>Median Score</cell><cell>0.4186</cell><cell cols="3">Median Score 0.5030 Median Score</cell><cell>0.4960</cell></row><row><cell>Total systems</cell><cell>52</cell><cell></cell><cell>41</cell><cell></cell><cell>30</cell><cell></cell></row><row><cell></cell><cell>Best Score</cell><cell>0.8246</cell><cell>Best Score</cell><cell>0.5880</cell><cell>Best Score</cell><cell>0.5175</cell></row><row><cell>5</cell><cell>Ours</cell><cell>0.7738(#3)</cell><cell>Ours</cell><cell>0.5287</cell><cell>Ours</cell><cell>0.3673</cell></row><row><cell></cell><cell>Median Score</cell><cell>0.5522</cell><cell cols="3">Median Score 0.4722 Median Score</cell><cell>0.3438</cell></row><row><cell>Total systems</cell><cell>56</cell><cell></cell><cell>45</cell><cell></cell><cell>38</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,72.03,368.10,451.65,276.85"><head>Table 4 .</head><label>4</label><figDesc>Results of internal experiments for exact answers on the BioASQ 9b dataset. Because of the difference in problem types, not all BioBERT-related models have been used in the experiments. Although PubMedBERT-Fulltext outperformed the basic version of BioBERT for Factoid, List type questions, the score was still much lower than the BioBERT-MNLI-SQuAD results.</figDesc><table coords="7,72.03,434.87,451.65,210.08"><row><cell>Pretrained Model Name</cell><cell>Yes/no* Macro F1</cell><cell>Factoid** MRR</cell><cell>List*** F-Measure</cell></row><row><cell>BioBERT</cell><cell>0.7659</cell><cell>0.3990</cell><cell>0.3518</cell></row><row><cell>BioBERT-MNLI</cell><cell>0.8671</cell><cell>-</cell><cell>-</cell></row><row><cell>BioBERT-MNLI-SQuAD</cell><cell>-</cell><cell>0.4509</cell><cell>0.3740</cell></row><row><cell>PubMedBERT-Abstract</cell><cell>0.7199</cell><cell>0.4020</cell><cell>0.3470</cell></row><row><cell>PubMedBERT-Fulltext</cell><cell>0.6960</cell><cell>0.4248</cell><cell>0.3548</cell></row><row><cell cols="4">* Because results of Yes/no questions were too disparate, we did not conduct enough experiments to</cell></row><row><cell cols="4">adjust the parameters to achieve the best performance. Except for the BioBERT-MNLI, we run the</cell></row><row><cell cols="2">experiments for only three times for each model.</cell><cell></cell><cell></cell></row><row><cell cols="4">**Parameter search space for Factoid type question task: [1e-6 -5e-5] for learning rate,</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,108.05,603.66,388.87,9.90;8,108.05,621.66,148.21,9.90" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="8,169.58,603.66,294.31,9.90">PubMedQA: A dataset for biomedical research question answering</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06146</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,108.05,639.69,407.67,9.90;8,108.05,657.69,280.35,9.90" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,182.83,639.69,258.17,9.90">COVID-QA: A Question Answering Dataset for COVID-19</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Möller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,460.70,639.69,55.02,9.90;8,108.05,657.69,248.14,9.90">Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020</title>
		<meeting>the 1st Workshop on NLP for COVID-19 at ACL 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.05,675.69,408.80,9.90;8,108.05,693.69,338.62,9.90" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,206.85,675.69,310.00,9.90;8,108.05,693.69,157.91,9.90">An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tsatsaronis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,273.38,693.69,91.42,9.90">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">138</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.05,711.69,402.93,9.90;8,108.05,729.71,240.60,9.90" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="8,235.35,711.69,275.63,9.90;8,108.05,729.71,208.18,9.90">NCU-IISR: Using a Pre-trained Language Model and Logistic Regression Model for BioASQ Task 8b Phase B</title>
		<author>
			<persName coords=""><forename type="first">J.-C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">T</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">-H</forename><surname>Tsai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,108.05,77.52,375.30,9.90;9,108.05,95.52,296.60,9.90" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,169.58,77.52,313.77,9.90;9,108.05,95.52,98.99,9.90">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,214.60,95.52,64.18,9.90">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,108.05,113.52,379.00,9.90;9,108.05,131.54,227.46,9.90" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="9,184.33,113.52,302.72,9.90;9,108.05,131.54,43.64,9.90">Transferability of natural language inference to biomedical question answering</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00217</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,108.05,149.54,406.97,9.90;9,108.05,167.54,229.21,9.90" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="9,170.08,149.54,344.94,9.90;9,108.05,167.54,45.90,9.90">Domain-specific language model pretraining for biomedical natural language processing</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.15779</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,108.05,185.54,412.68,9.90;9,108.05,203.54,326.38,9.90" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05426</idno>
		<title level="m" coord="9,306.63,185.54,214.10,9.90;9,108.05,203.54,143.34,9.90">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,108.05,221.57,390.89,9.90;9,108.05,239.57,110.73,9.90" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="9,169.58,221.57,258.97,9.90">Biomedical Question Answering: A Comprehensive Review</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05281</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,108.05,257.57,403.97,9.90;9,108.05,275.57,175.95,9.90" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="9,257.35,257.57,250.92,9.90">SciBERT: A pretrained language model for scientific text</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10676</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,108.05,293.59,370.58,9.90;9,108.05,311.59,321.49,9.90" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Altosaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05342</idno>
		<title level="m" coord="9,295.88,293.59,182.75,9.90;9,108.05,311.59,137.78,9.90">Clinicalbert: Modeling clinical notes and predicting hospital readmission</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,108.05,329.59,415.49,9.90;9,108.05,347.59,339.15,9.90;9,108.05,365.59,110.73,9.90" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="9,235.85,329.59,287.69,9.90;9,108.05,347.59,269.14,9.90">Transfer learning in biomedical natural language processing: an evaluation of BERT and ELMo on ten benchmarking datasets</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05474</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,108.05,383.61,410.12,9.90;9,108.05,401.61,148.21,9.90" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10044</idno>
		<title level="m" coord="9,180.58,383.61,305.06,9.90">BoolQ: Exploring the surprising difficulty of natural yes/no questions</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,108.05,419.61,390.20,9.90;9,108.05,437.61,413.18,9.90;9,108.05,455.61,293.81,9.90" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,214.85,419.61,283.40,9.90;9,108.05,437.61,217.24,9.90">Classification betters regression in query-based multi-document summarisation techniques for question answering</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mollá</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,344.15,437.61,177.08,9.90;9,108.05,455.61,217.91,9.90">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,108.05,473.64,405.85,9.90;9,108.05,491.64,87.53,9.90" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="9,190.81,473.64,217.06,9.90">A package for automatic evaluation of summaries</title>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Rouge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>in Text summarization branches out</note>
</biblStruct>

<biblStruct coords="9,108.05,509.64,368.43,9.90;9,108.05,527.64,245.21,9.90" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="9,183.08,509.64,293.40,9.90;9,108.05,527.64,61.52,9.90">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,108.05,545.64,278.09,9.90;9,108.05,563.66,145.30,9.90" xml:id="b16">
	<monogr>
		<ptr target="https://github.com/microsoft/nni" />
		<title level="m" coord="9,157.08,545.64,151.83,9.90">Neural Network Intelligence (NNI)</title>
		<imprint/>
		<respStmt>
			<orgName>Microsoft</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
