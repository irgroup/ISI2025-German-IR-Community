<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,404.08,15.42">Large Biomedical Question Answering Models with</title>
				<funder>
					<orgName type="full">Tensorflow Research Cloud (TFRC)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,192.91,106.66,76.99,15.43;1,89.29,134.97,31.02,11.96"><forename type="first">Electra</forename><surname>Sultan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>Delaware</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,176.27,134.97,83.27,11.96"><forename type="first">K</forename><surname>Vijay-Shanker</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>Delaware</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,404.08,15.42">Large Biomedical Question Answering Models with</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">72A6E3E327FA0CC806FA088D9DCA5107</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>BERT</term>
					<term>ELECTRA</term>
					<term>ALBERT</term>
					<term>BioASQ</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The majority of systems that participated in the BioASQ8 challenge are based on BioBERT model <ref type="bibr" coords="1,479.23,201.18,9.39,8.97" target="#b0">[1]</ref>. We adopt a different approach in our participation in the BioASQ9B challenge by taking advantage of large biomedical language models that are built on ELECTRA [2] and ALBERT [3] architectures, including both BioM-ELECTRA and BioM-ALBERT [4]. Moreover, we examine the advantage of transferability [5] between BioASQ and other text classification tasks such as The Multi-Genre Natural Language Inference (MultiNLI) [6]. Our results show that both BioM-ELECTRA and BioM-ALBERT significantly outperform the BioBERT model on the BioASQ9B task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>BioBERT model <ref type="bibr" coords="1,164.13,368.40,13.00,10.91" target="#b6">[7]</ref> represents the early success of domain adaptation of BERT <ref type="bibr" coords="1,450.18,368.40,13.00,10.91" target="#b7">[8]</ref> model in the biomedical domain. BioBERT model shows impressive results on the BioASQ7B challenge by taking the lead on most five batches of BioASQ7B challenge <ref type="bibr" coords="1,370.15,395.50,11.37,10.91" target="#b8">[9]</ref>. Furthermore, the BioBERT model is used in the majority of biomedical models that competed in the BioASQ8 challenge <ref type="bibr" coords="1,89.29,422.60,11.53,10.91" target="#b0">[1]</ref>. However, since the introduction of BERT model in 2018, new Transformer-based models have been introduced to NLP community including RoBERTa <ref type="bibr" coords="1,362.71,436.14,16.17,10.91" target="#b9">[10]</ref>, ELECTRA <ref type="bibr" coords="1,433.46,436.14,11.36,10.91" target="#b1">[2]</ref>, XLNET <ref type="bibr" coords="1,486.96,436.14,16.18,10.91" target="#b10">[11]</ref>, MegaTron-LM <ref type="bibr" coords="1,155.73,449.69,16.12,10.91" target="#b11">[12]</ref>, and ALBERT <ref type="bibr" coords="1,238.34,449.69,11.31,10.91" target="#b2">[3]</ref>. An adaptation of some of these models to the biomedical domain have been introduced later as BioRoBERTA <ref type="bibr" coords="1,319.16,463.24,16.17,10.91" target="#b12">[13]</ref>, BioMegaTron <ref type="bibr" coords="1,405.26,463.24,17.83,10.91" target="#b13">[14]</ref> and PubMedBERT <ref type="bibr" coords="1,89.29,476.79,16.41,10.91" target="#b14">[15]</ref>. Additionally, we have introduced both BioM-ELECTRA and BioM-ALBERT models <ref type="bibr" coords="1,492.22,476.79,11.58,10.91" target="#b3">[4]</ref>. Both models are large-scale models that are adapted to the biomedical domain by pretraining both on Pubmed abstracts.</p><p>As noted earlier, a majority of participant systems in the BioASQ8B challenge are based on the BioBERT base-scale model. This motivates us to examine the effectiveness of large-scale biomedical models. The main findings of our investigations are that:</p><p>(i) Both BioM-ALBERT and BioM-ELECTRA, models that we have recently developed are effective in addressing both BioASQ factoid and list questions. (ii) Treating BioASQ yes/no question as a classification problem is an effective approach that can lead to competitive performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">System Description</head><p>We use large-scale biomedical language models, which is one of the primary differences between our system and other prior systems that participated in the BioASQ8B challenge. In our participation in the BioASQ9B challenge, we use both our models BioM-ELECTRA and BioM-ALBERT models <ref type="bibr" coords="2,164.43,151.93,11.43,10.91" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">BioM-ELECTRA</head><p>ELECTRA is a model that was built upon the idea of Transformer encoder, and attention mechanism <ref type="bibr" coords="2,143.37,215.20,17.99,10.91" target="#b15">[16]</ref> that BERT model uses. However, the ELECTRA model introduces novelty to the loss function by eliminating Next Sentence Prediction (NSP) objective, which is a similar decision taken by the RoBERTA model <ref type="bibr" coords="2,266.10,242.30,16.35,10.91" target="#b9">[10]</ref>. Moreover, ELECTRA improves the loss function by incorporating ideas from GAN model <ref type="bibr" coords="2,277.00,255.85,18.07,10.91" target="#b16">[17]</ref> where it generates corrupted (fake) tokens by employing a small Masked Language Model (MLM). Then, a discriminator model will judge those corrupted tokens and decide if they are "original" or "replaced" tokens.</p><p>To shift the contextual representation of ELECTRA, we pretrain ELECTRA on PubMed abstracts using specific domain vocabulary learned from PubMed abstracts. We pretrain our BioM-ELECTRA for 434K steps using TPUv3-512 units with a batch size of 4096.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">BioM-ALBERT</head><p>ALBERT model <ref type="bibr" coords="2,158.43,373.32,12.69,10.91" target="#b2">[3]</ref> takes a similar decision to ELECTRA regarding the loss function by dropping Next Sentence Prediction (NSP) function. Furthermore, ALBERT introduces a self-supervised loss for sentence-order prediction (SOP) objective. Additionally, the ALBERT model improves the efficiency of the Transformer model by introducing both parameter-sharing and factorization of embedding layers techniques. The Parameter-sharing technique improves the architecture by reducing the parameters redundancy inside the model.</p><p>On the other hand, factorization of embedding layers allows the model to increase its hidden layer size up to 4096 while having only 235M parameters in the case of ALBERT-xxlarge. We build BioM-ALBERTxxlarge by pretraining ALBERTxxlarge on PubMed Abstracts using TPUv3-512 unit for 264K steps and a batch size of 8192. Similar to BioM-ELECTRA, we also pretrain BioM-ALBERT on PubMed abstracts only.</p><p>Table <ref type="table" coords="2,126.10,522.36,4.97,10.91" target="#tab_0">1</ref> shows the architecture design and the reported results <ref type="bibr" coords="2,370.19,522.36,12.69,10.91" target="#b3">[4]</ref> of our models on SQuAD2.0 <ref type="bibr" coords="2,89.29,535.91,18.07,10.91" target="#b17">[18]</ref> and BioASQ7B-Factoid tasks against other SOTA models. We include this table to show a head-to-head comparison between different architectures that have have been used by participants' systems in the BioASQ9B challenge <ref type="bibr" coords="2,298.85,563.01,16.41,10.91" target="#b18">[19]</ref>. We should also note that it is a common practice in the literature to fine-tune the biomedical language model on the the SQuAD dataset first and then on the BioASQ dataset. The reason to follow this approach because SQuAD2.0 dataset has more than 130K examples, which is much larger than the BioASQ dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pre-Processing phase</head><p>For BioASQ9B factoid and list questions, we converted all questions to SQuADv1.1 format. Therefore, we duplicate the snippet (context) for each question in the training and test dataset instead of having a group of snippets and one corresponding question. For yes/no questions, we adopted a binary classification approach to solve this task by having the context (snippet) as "sentence 1", questions as "sentence 2" and the answer (yes/no) as a "label." We use a preprocessing script developed by <ref type="bibr" coords="3,228.95,395.74,17.91,10.91" target="#b14">[15]</ref> to generate the BioASQ classification dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Environmental Design</head><p>We fine-tune our models on factoid and list questions using Google Cloud Compute Engine with TPUv3-8 units and TensorFlow 1.15. For the yes/no task, we use the Hugging-face Transformers library <ref type="bibr" coords="3,122.12,472.56,17.91,10.91" target="#b19">[20]</ref> and V100 GPU on the Google Colab Pro environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Hyperparameters</head><p>For factoid and list questions, we use the same hyperparameters settings that we use in our previous work <ref type="bibr" coords="3,158.78,535.84,12.99,10.91" target="#b3">[4]</ref> as shown in Table <ref type="table" coords="3,261.76,535.84,3.81,10.91" target="#tab_1">2</ref>. We made this decision to examine the consistency and reproducibility of both BioM-ELECTRA and BioM-ALBERT on the BioASQ9B challenge.</p><p>For the yes/no question, we use the training and testing dataset of the BioASQ8B challenge to determine our choices of hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Task-to-Task Transfer Learning</head><p>The early work done by <ref type="bibr" coords="3,204.18,626.22,13.00,10.91" target="#b4">[5]</ref> and <ref type="bibr" coords="3,240.96,626.22,18.07,10.91" target="#b20">[21]</ref> shows that the transferability (Task-to-Task Transfer Learning) between general domain tasks such as MultiNLI <ref type="bibr" coords="3,356.94,639.76,12.99,10.91" target="#b5">[6]</ref> and SQuAD helps to improve the results on SQuAD and BioASQ8B tasks. We did a similar approach by fine-tuning both BioM-ALBERT and BioM-ELECTRA on the MNLI task, then SQuAD, and later on the BioASQ training dataset. We investigate and report the impact of this transferability on BioASQ9B in the result section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and Discussion</head><p>We participated in the BioASQ9B challenge under the name "UDEL-LAB". Our reported results in this section are obtained from the BioASQ9B official leader board. We participate in the BioASQ9B-Factoid challenge starting from batch 3, and we use batch 2 to test the format of our submission. Therefore, we only include results of BioASQ-Factoid challenge starting from batch 3. We participated in yes/no, and list questions on batch five only since both types of tasks require extra pre-processing that we could not develop at early stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Factoid Task</head><p>Table <ref type="table" coords="4,115.76,401.02,5.07,10.91" target="#tab_2">3</ref> shows the results of our system on the BiASQ9B-Factoid challenge. We show only the top five systems for each batch based on the mean reciprocal rank (MRR) score. The Fudan University team participated with four systems under the name of ir_sys <ref type="bibr" coords="4,419.31,428.12,16.35,10.91" target="#b18">[19]</ref>. Their systems combined SpanBERT <ref type="bibr" coords="4,188.27,441.67,16.42,10.91" target="#b21">[22]</ref>, PubMedBERT <ref type="bibr" coords="4,278.66,441.67,18.07,10.91" target="#b14">[15]</ref> and XLNet models <ref type="bibr" coords="4,388.12,441.67,16.41,10.91" target="#b10">[11]</ref>. On other hand, "bioanswerfinder" system uses the BioELECTRA model <ref type="bibr" coords="4,313.81,455.22,16.08,10.91" target="#b22">[23]</ref>, which they have developed early based on ELECTRA architecture. The result of BioM-ALBERT and BioM-ELECTRA against other models on both batch three and batch five suggests that our models has more consistency on the BioASQ performance than other models. Results also highlight that language model scale is a dominant factor on the performance of BioASQ-Factoid questions. Only large-scale models that are based on ALBERT-xxlarge, ELECTRA-large, and XLNET are taking the lead in all three batches. On the other hand, using the transferability between MNLI and SQuAD tasks improves the score of our systems in the third batch by almost 2% in MRR score. However, this improvement is not consistent in both batch 4 and 5. We attribute this inconsistency to the fact that the finetuning layer of BERT-like models is randomly initialized. This randomness causes a fluctuation in the results, especially if we have small evaluation data set <ref type="bibr" coords="4,356.88,604.26,16.14,10.91" target="#b14">[15]</ref>. On the other hand, the score of BioM-ALBERT and BioM-ELECTRA in both batches 3 and 5 suggest that having an ensemble model could help further improve the results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">List and Yes/No Tasks</head><p>Table <ref type="table" coords="5,115.89,575.93,5.09,10.91" target="#tab_3">4</ref> shows the results of our system on the BiASQ9B List and Yes/No challenge. In the list task, our systems ranked in first and second place. We achieved this score for list questions despite using the same hyperparameters that we use for the factoid task. On yes/no task, BioM-ALBERT performs significantly better than BioM-ELECTRA but falls behind the performance of "KU-DMIS-2" system, which uses BioBERT-Large <ref type="bibr" coords="5,320.04,630.13,16.15,10.91" target="#b18">[19]</ref>. We should also note that the number of both list questions <ref type="bibr" coords="5,183.10,643.68,16.31,10.91" target="#b17">(18)</ref> and yes/no <ref type="bibr" coords="5,252.25,643.68,16.31,10.91" target="#b18">(19)</ref> questions are relatively smaller than factoid questions (36). Tasks with small data sets usually are sensitive to hyperparameter choice and fluctuate between each fine-tuning run, especially in the case of binary classification (yes/no) task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>We demonstrate that BioM-ELECTRA and BioM-ALBERT models are effective in addressing the BioASQ challenge. Our systems take the lead in two batches of factoid tasks and by a significant margin (2%) in batch 5. Additionally, we show that applying transferability between MNLI and SQuAD led our systems to score at first place on factoid (batch 3) and list (batch 5) questions.</p><p>For future work, we plan to build a large ensemble QA system based on both BioM-ELECTRA and BioM-ALBERT to address the BioASQ and pandemic challenges.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.61,90.49,419.04,165.12"><head>Table 1</head><label>1</label><figDesc>Results of BioM-ALBERT and BioM-ELECTRA on BioASQ7B-Factoid Task. Evaluation metrics are F1 score for SQuAD task and Mean Reciprocal Rank MRR for BioASQ task. We use reported results of BioBERT-Base, BioBERT-Large, and BioMegaTron<ref type="bibr" coords="3,309.73,126.40,14.92,8.87" target="#b13">[14]</ref>; PubMedBERT, BioM-ELECTRA and BioM-ALBERT<ref type="bibr" coords="3,125.36,138.36,10.51,8.87" target="#b3">[4]</ref>.</figDesc><table coords="3,110.25,157.57,374.78,98.03"><row><cell>Model</cell><cell cols="4">#Parameters #Hidden Size SQuAD2.0 BioASQ7B-Factoid</cell></row><row><cell>BioBERT-Base</cell><cell>110M</cell><cell>768</cell><cell>-</cell><cell>41.1</cell></row><row><cell>BioM-ELECTRA-Base</cell><cell>110M</cell><cell>768</cell><cell>84.4</cell><cell>52.3</cell></row><row><cell>PubMedBERT-PMC-base</cell><cell>110M</cell><cell>768</cell><cell>80.9</cell><cell>51.9</cell></row><row><cell>BioBERT-Large</cell><cell>335M</cell><cell>1024</cell><cell>-</cell><cell>50.1</cell></row><row><cell>BioMegaTron345m</cell><cell>345M</cell><cell>1024</cell><cell>84.2</cell><cell>52.5</cell></row><row><cell>BioM-ELECTRA-Large</cell><cell>335M</cell><cell>1024</cell><cell>88.3</cell><cell>54.1</cell></row><row><cell>BioM-ALBERT-xxLarge</cell><cell>235M</cell><cell>4096</cell><cell>87.0</cell><cell>56.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,88.99,90.49,418.53,94.18"><head>Table 2</head><label>2</label><figDesc>Details of fine-tuning hyperparameters that we use for both BioM-ALBERT and BioM-ELECTRA.</figDesc><table coords="4,88.99,114.45,360.80,70.22"><row><cell>(MSL=Max Seq. Length)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Task</cell><cell>Model</cell><cell cols="4">Learning Rate Batch Epochs MSL</cell></row><row><cell cols="2">Factoid/List BioM-ELECTRA</cell><cell>2e-5</cell><cell>24</cell><cell>4</cell><cell>512</cell></row><row><cell cols="2">Factoid/List BioM-ALBERT</cell><cell>1e-5</cell><cell>128</cell><cell>3</cell><cell>384</cell></row><row><cell>Yes/No</cell><cell>All our models</cell><cell>3e-5</cell><cell>8</cell><cell>5</cell><cell>256</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,88.99,90.49,416.99,249.99"><head>Table 3</head><label>3</label><figDesc>Results of BioM-ALBERT and BioM-ELECTRA on BioASQ9B-Factoid Task. Strict Acc. is based on the evaluation of the first predicted answer by the system. Lenient Acc. is based on whether the system returns the exact answer in the top five predicted answers.</figDesc><table coords="5,95.43,146.01,404.42,194.47"><row><cell>Batch</cell><cell>Model</cell><cell cols="2">Strict Acc. Lenient Acc.</cell><cell>MRR</cell></row><row><cell></cell><cell>BioM-ALBERTxxlarge+MNLI+SQuAD+BioASQ</cell><cell>0.5405</cell><cell>0.7027</cell><cell>0.6149</cell></row><row><cell></cell><cell>Ir_sys2</cell><cell>0.5946</cell><cell>0.6486</cell><cell>0.6135</cell></row><row><cell cols="2">9B Batch 3 BioM-ALBERTxxlarge+SQuAD+BioASQ</cell><cell>0.5405</cell><cell>0.6757</cell><cell>0.5946</cell></row><row><cell></cell><cell>BioM-ELECTRA-large+SQuAD+BioASQ</cell><cell>0.5135</cell><cell>0.7027</cell><cell>0.5923</cell></row><row><cell></cell><cell>bio-answerfinder</cell><cell>0.5676</cell><cell>0.5946</cell><cell>0.5811</cell></row><row><cell></cell><cell>Ir_sys1</cell><cell>0.6429</cell><cell>0.7857</cell><cell>0.6929</cell></row><row><cell></cell><cell>Ir_sys2</cell><cell>0.6071</cell><cell>0.7500</cell><cell>0.6464</cell></row><row><cell cols="2">9B Batch 4 BioM-ELECTRA-large+SQuAD+BioASQ</cell><cell>0.5357</cell><cell>0.7857</cell><cell>0.6351</cell></row><row><cell></cell><cell>BioM-ELECTRA-large+MNLI+SQuAD+BioASQ</cell><cell>0.5000</cell><cell>0.7857</cell><cell>0.6321</cell></row><row><cell></cell><cell>BioM-ALBERTxxlarge+SQuAD+BioASQ</cell><cell>0.5357</cell><cell>0.7143</cell><cell>0.5982</cell></row><row><cell></cell><cell>BioM-ELECTRA-large+SQuAD+BioASQ</cell><cell>0.5000</cell><cell>0.7222</cell><cell>0.5880</cell></row><row><cell></cell><cell>BioM-ELECTRA-large+MNLI+SQuAD+BioASQ</cell><cell>0.4722</cell><cell>0.6944</cell><cell>0.5694</cell></row><row><cell cols="2">9B Batch 5 finetuning1</cell><cell>0.5000</cell><cell>0.6667</cell><cell>0.5671</cell></row><row><cell></cell><cell>BioM-ALBERTxxlarge+SQuAD+BioASQ</cell><cell>0.4444</cell><cell>0.7222</cell><cell>0.5588</cell></row><row><cell></cell><cell>BioM-ALBERTxxlarge+MNLI+SQuAD+BioASQ</cell><cell>0.4722</cell><cell>0.6667</cell><cell>0.5556</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,88.99,362.74,418.53,165.91"><head>Table 4</head><label>4</label><figDesc>Results of BioM-ALBERT and BioM-ELECTRA on BioASQ9B challenge for a list and yes/no questions. Official evaluation metrics for Yes/No task is Macro-F1 score, and for List questions is F-Measure. We only participated in batch five for these type of questions.</figDesc><table coords="5,142.42,418.27,310.45,110.38"><row><cell>Task</cell><cell>Model</cell><cell cols="2">#Rank Score</cell></row><row><cell></cell><cell>BioM-ALBERTxxlarge+MNLI+SQuAD+BioASQ</cell><cell>#1</cell><cell>0.5175</cell></row><row><cell></cell><cell>BioM-ALBERTxxlarge+SQuAD+BioASQ</cell><cell>#2</cell><cell>0.4927</cell></row><row><cell>List</cell><cell>Ir_sys2</cell><cell>#3</cell><cell>0.4804</cell></row><row><cell></cell><cell>BioM-ELECTRA-large+SQuAD+BioASQ</cell><cell>#7</cell><cell>0.4031</cell></row><row><cell></cell><cell>BioM-ELECTRA-large+MNLI+BioASQ</cell><cell>#8</cell><cell>0.3936</cell></row><row><cell></cell><cell>KU-DMIS-2</cell><cell>#1</cell><cell>0.8246</cell></row><row><cell cols="2">Yes/No BioM-ALBERTxxlarge+SQuAD+BioASQ</cell><cell>#4</cell><cell>0.7564</cell></row><row><cell></cell><cell>BioM-ELECTRA-large+SQuAD+BioASQ</cell><cell>#5</cell><cell>0.6801</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>We would like to acknowledge the support we have from <rs type="funder">Tensorflow Research Cloud (TFRC)</rs> team to grant us access to TPUv3 units.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,112.66,344.14,395.17,10.91;6,112.66,357.69,393.33,10.91;6,112.66,371.24,393.33,10.91;6,112.66,384.79,395.01,10.91;6,112.66,398.34,312.00,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,194.79,357.69,311.20,10.91;6,112.66,371.24,243.74,10.91">Overview of bioasq 2020: The eighth bioasq challenge on large-scale biomedical semantic indexing and question answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Bougiatiotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Rodriguez-Penagos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58219-7_16</idno>
		<ptr target="https://link.springer.com/chapter/10.1007/978-3-030-58219-7_16" />
	</analytic>
	<monogr>
		<title level="m" coord="6,380.16,371.24,125.83,10.91;6,112.66,384.79,279.47,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,411.89,393.33,10.91;6,112.66,425.43,295.16,10.91" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<title level="m" coord="6,334.34,411.89,171.65,10.91;6,112.66,425.43,165.13,10.91">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,438.98,393.53,10.91;6,112.66,452.53,361.04,10.91" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<title level="m" coord="6,407.44,438.98,98.74,10.91;6,112.66,452.53,231.08,10.91">Albert: A lite bert for self-supervised learning of language representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,466.08,393.32,10.91;6,112.26,479.63,393.73,10.91;6,112.66,493.18,394.52,10.91;6,112.66,506.73,397.48,10.91;6,112.66,522.72,68.18,7.90" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,219.86,466.08,286.12,10.91;6,112.26,479.63,153.47,10.91">BioM-transformers: Building large biomedical language models with BERT, ALBERT and ELECTRA</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Alrowili</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Shanker</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.bionlp-1.24</idno>
		<ptr target="https://www.aclweb.org/anthology/2021.bionlp-1.24.doi:10.18653/v1/2021.bionlp-1.24" />
	</analytic>
	<monogr>
		<title level="m" coord="6,289.99,479.63,216.00,10.91;6,112.66,493.18,288.32,10.91">Proceedings of the 20th Workshop on Biomedical Language Processing, Association for Computational Linguistics</title>
		<meeting>the 20th Workshop on Biomedical Language Processing, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="221" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,533.83,393.33,10.91;6,112.66,547.38,365.37,10.91" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="6,390.74,533.83,115.25,10.91;6,112.66,547.38,234.98,10.91">Transferability of natural language inference to biomedical question answering</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00217</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,560.93,393.61,10.91;6,112.66,574.48,393.33,10.91;6,112.66,588.02,394.52,10.91;6,112.28,601.57,395.00,10.91;6,112.66,615.12,347.35,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,368.95,560.93,137.31,10.91;6,112.66,574.48,260.52,10.91">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5446</idno>
		<ptr target="https://www.aclweb.org/anthology/W18-5446.doi:10.18653/v1/W18-5446" />
	</analytic>
	<monogr>
		<title level="m" coord="6,397.85,574.48,108.13,10.91;6,112.66,588.02,394.52,10.91;6,112.28,601.57,191.51,10.91">Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, Association for Computational Linguistics</title>
		<meeting>the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, Association for Computational Linguistics<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,628.67,393.33,10.91;6,112.66,642.22,394.62,10.91;6,112.66,655.77,394.96,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,355.95,628.67,150.04,10.91;6,112.66,642.22,261.24,10.91">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btz682</idno>
		<ptr target="https://doi.org/10.1093/bioinformatics/btz682.doi:10.1093/bioinformatics/btz682" />
	</analytic>
	<monogr>
		<title level="j" coord="6,382.77,642.22,66.92,10.91">Bioinformatics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,86.97,393.33,10.91;7,112.66,100.52,393.33,10.91;7,112.66,114.06,393.32,10.91;7,112.66,127.61,393.33,10.91;7,112.66,141.16,394.03,10.91;7,112.66,154.71,234.20,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,323.15,86.97,182.83,10.91;7,112.66,100.52,186.91,10.91">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://www.aclweb.org/anthology/N19-1423.doi:10.18653/v1/N19-1423" />
	</analytic>
	<monogr>
		<title level="m" coord="7,327.87,100.52,178.11,10.91;7,112.66,114.06,393.32,10.91;7,112.66,127.61,99.97,10.91">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="7,112.66,168.26,393.33,10.91;7,112.66,181.81,393.33,10.91;7,112.66,195.36,395.01,10.91;7,112.66,208.91,17.97,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,349.23,168.26,156.76,10.91;7,112.66,181.81,72.96,10.91">Results of the seventh edition of the bioasq challenge</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Bougiatiotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/2006.09174.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="7,209.43,181.81,296.55,10.91;7,112.66,195.36,103.39,10.91">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,222.46,395.17,10.91;7,112.66,236.01,395.01,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m" coord="7,137.85,236.01,241.29,10.91">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,249.56,393.33,10.91;7,112.66,263.11,371.43,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m" coord="7,419.10,249.56,86.88,10.91;7,112.66,263.11,241.15,10.91">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,276.66,395.17,10.91;7,112.66,290.20,395.01,10.91;7,112.66,306.20,97.35,7.90" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<title level="m" coord="7,459.90,276.66,47.94,10.91;7,112.66,290.20,363.43,10.91">Megatronlm: Training multi-billion parameter language models using model parallelism</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,317.30,393.33,10.91;7,112.66,330.85,393.33,10.91;7,112.66,344.40,393.33,10.91;7,112.66,357.95,395.01,10.91;7,112.66,371.50,288.58,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,287.48,317.30,218.51,10.91;7,112.66,330.85,286.01,10.91">Pretrained language models for biomedical and clinical tasks: Understanding and extending the state-of-the-art</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.clinicalnlp-1.17</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.clinicalnlp-1.17.doi:10.18653/v1/2020.clinicalnlp-1.17" />
	</analytic>
	<monogr>
		<title level="m" coord="7,421.76,330.85,84.22,10.91;7,112.66,344.40,393.33,10.91;7,112.66,357.95,47.51,10.91">Proceedings of the 3rd Clinical Natural Language Processing Workshop, Association for Computational Linguistics</title>
		<meeting>the 3rd Clinical Natural Language Processing Workshop, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,385.05,394.61,10.91;7,112.66,398.60,393.33,10.91;7,112.66,412.15,395.17,10.91;7,112.66,425.70,394.03,10.91;7,112.66,439.25,303.46,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="7,447.74,385.05,59.53,10.91;7,112.66,398.60,191.28,10.91">BioMegatron: Larger biomedical domain language model</title>
		<author>
			<persName coords=""><forename type="first">H.-C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Bakhturina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mani</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.379</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.emnlp-main.379.doi:10.18653/v1/2020.emnlp-main.379" />
	</analytic>
	<monogr>
		<title level="m" coord="7,328.58,398.60,177.40,10.91;7,112.66,412.15,271.20,10.91">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4700" to="4706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,452.79,394.53,10.91;7,112.66,466.34,394.53,10.91;7,112.66,479.89,122.77,10.91" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="7,112.66,466.34,390.01,10.91">Domain-specific language model pretraining for biomedical natural language processing</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.15779</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,493.44,395.17,10.91;7,112.66,506.99,393.33,10.91;7,112.66,520.54,394.52,10.91;7,112.66,534.09,110.27,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="7,148.79,506.99,106.04,10.91">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,277.88,506.99,228.11,10.91;7,112.66,520.54,215.30,10.91">Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,547.64,394.53,10.91;7,112.34,561.19,394.84,10.91;7,112.66,574.74,394.53,10.91;7,112.39,588.29,394.31,10.91;7,112.66,601.84,219.36,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="7,168.72,561.19,125.13,10.91">Generative adversarial nets</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="s" coord="7,277.62,574.74,224.82,10.91">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<date type="published" when="2014">2014</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,615.39,393.53,10.91;7,112.66,628.93,393.33,10.91;7,112.66,642.48,395.17,10.91;7,112.66,656.03,395.00,10.91;7,112.66,669.58,138.14,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="7,245.77,615.39,260.41,10.91;7,112.66,628.93,29.70,10.91">Know what you don&apos;t know: Unanswerable questions for SQuAD</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-2124</idno>
		<ptr target="https://www.aclweb.org/anthology/P18-2124.doi:10.18653/v1/P18-2124" />
	</analytic>
	<monogr>
		<title level="m" coord="7,166.12,628.93,339.86,10.91;7,112.66,642.48,49.38,10.91;7,286.92,642.48,192.00,10.91">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct coords="8,112.66,86.97,394.53,10.91;8,112.66,100.52,393.33,10.91;8,112.66,114.06,276.05,10.91" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Katsimpras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Vandorou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gasco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.14885</idno>
		<title level="m" coord="8,112.66,100.52,393.33,10.91;8,112.66,114.06,145.67,10.91">Overview of bioasq 2021: The ninth bioasq challenge on large-scale biomedical semantic indexing and question answering</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,127.61,394.53,10.91;8,112.66,141.16,394.53,10.91;8,112.66,154.71,393.33,10.91;8,112.66,168.26,393.33,10.91;8,112.66,181.81,394.53,10.91;8,112.66,195.36,395.01,10.91;8,112.66,208.91,197.48,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="8,296.48,154.71,209.50,10.91;8,112.66,168.26,46.35,10.91">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Le</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.emnlp-demos.6.doi:10.18653/v1/2020.emnlp-demos.6" />
	</analytic>
	<monogr>
		<title level="m" coord="8,186.08,168.26,319.91,10.91;8,112.66,181.81,390.37,10.91">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,222.46,393.32,10.91;8,112.66,236.01,393.33,10.91;8,112.66,249.56,360.58,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="8,262.69,222.46,243.30,10.91;8,112.66,236.01,31.51,10.91">Task-to-task transfer learning with parameter-efficient adapter</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,353.52,236.01,152.47,10.91;8,112.66,249.56,86.03,10.91">Natural Language Processing and Chinese Computing</title>
		<editor>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Hong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="391" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,263.11,393.33,10.91;8,112.66,276.66,395.01,10.91" xml:id="b21">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10529</idno>
		<title level="m" coord="8,404.09,263.11,101.89,10.91;8,112.66,276.66,215.78,10.91">SpanBERT: Improving pre-training by representing and predicting spans</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,112.66,290.20,395.17,10.91;8,112.66,303.75,393.61,10.91;8,112.66,317.30,394.53,10.91;8,112.41,330.85,371.40,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="8,166.34,290.20,341.49,10.91;8,112.66,303.75,165.47,10.91">On the effectiveness of small, discriminatively pre-trained language representation models for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">B</forename><surname>Ozyurt</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.sdp-1.12</idno>
		<ptr target="https://aclanthology.org/2020.sdp-1.12.doi:10.18653/v1/2020.sdp-1" />
	</analytic>
	<monogr>
		<title level="m" coord="8,300.28,303.75,205.98,10.91;8,112.66,317.30,289.19,10.91">Proceedings of the First Workshop on Scholarly Document Processing, Association for Computational Linguistics</title>
		<meeting>the First Workshop on Scholarly Document Processing, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="104" to="112" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
