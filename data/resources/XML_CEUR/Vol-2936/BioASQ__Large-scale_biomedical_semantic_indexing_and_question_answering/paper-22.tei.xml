<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.69,84.74,417.25,15.42;1,89.29,106.66,69.36,15.43">A Neural Text Ranking Approach for Automatic MeSH Indexing</title>
				<funder ref="#_rgBsv9j">
					<orgName type="full">National Library of Medicine, National Institutes of Health</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.87,134.97,69.86,11.96"><forename type="first">Alastair</forename><forename type="middle">R</forename><surname>Rae</surname></persName>
							<email>alastair.rae@nih.gov</email>
							<affiliation key="aff0">
								<orgName type="institution">National Library of Medicine</orgName>
								<address>
									<addrLine>8600 Rockville Pike</addrLine>
									<postCode>20894</postCode>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,169.95,134.97,72.19,11.96"><forename type="first">James</forename><forename type="middle">G</forename><surname>Mork</surname></persName>
							<email>jmork@mail.nih.gov</email>
							<affiliation key="aff0">
								<orgName type="institution">National Library of Medicine</orgName>
								<address>
									<addrLine>8600 Rockville Pike</addrLine>
									<postCode>20894</postCode>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,269.59,134.97,114.66,11.96"><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Library of Medicine</orgName>
								<address>
									<addrLine>8600 Rockville Pike</addrLine>
									<postCode>20894</postCode>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.69,84.74,417.25,15.42;1,89.29,106.66,69.36,15.43">A Neural Text Ranking Approach for Automatic MeSH Indexing</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">35A5F65C3F3688736E1CBDB6A3628506</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Automatic MeSH Indexing</term>
					<term>Medical Text Indexing</term>
					<term>Neural Text Ranking</term>
					<term>Transformers</term>
					<term>BERT</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The U.S. National Library of Medicine (NLM) has been indexing the biomedical literature with MeSH terms since the mid-1960s, and in recent years the library has increasingly relied on AI assistance and automation to curate the biomedical literature more efficiently. Since 2002, the NLM has been using natural language processing algorithms to assist indexers by providing MeSH term recommendations, and we are continually working to improve the quality of these recommendations. This work presents a new neural text ranking approach for automatic MeSH indexing. The domain-specific pretrained transformer model, PubMedBERT, was fine-tuned on MEDLINE data and used to rank candidate main headings obtained from a Convolutional Neural Network (CNN). Pointwise, listwise, and multi-stage ranking approaches are demonstrated, and the algorithm performance was evaluated by participating in the BioASQ challenge task 9a on semantic indexing. The neural text ranking approach was found to have very competitive performance in the final batch of the challenge, and the multi-stage ranking method typically boosted the CNN model performance by about 5% points in terms of micro F1-score.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The U.S. National Library of Medicine (NLM) maintains the MEDLINE Â® bibliographic database to help the biomedical research community find the journal articles that they need. To improve the quality of PubMed search results, all MEDLINE articles are indexed with a controlled vocabulary called Medical Subject Headings (MeSH Â® ) 1 .</p><p>MeSH indexing is a time-consuming and highly specialized activity. NLM indexers review the full text of an article and then assign MeSH terms that represent the central concepts as well as every other topic that is discussed to a significant extent. This work focuses on the indexing of main headings, which are also known as MeSH descriptors. There are currently over 29,000 main headings in the 2021 MeSH vocabulary and each main heading describes an important biomedical concept. On average indexers assign about 11 main headings per article.</p><p>Each year close to 1 million articles are indexed for MEDLINE, and the library uses AI assistance and automation to increase the efficiency of the indexing process. Since 2002, indexing assistance has been provided by the Medical Text Indexer (MTI) system <ref type="bibr" coords="1,461.87,586.01,14.14,10.91" target="#b0">[1]</ref>. MTI improves productivity by providing a pick list of recommended MeSH terms that can be quickly selected by indexers.</p><p>Automatic MeSH indexing is a difficult machine learning problem, and the main challenges are the large number of main headings and their highly imbalanced frequency distribution. This work presents a new neural text ranking approach for automatic MeSH indexing. Pointwise, listwise, and multi-stage ranking approaches are demonstrated using a domain-specific pretrained transformer model called PubMedBERT <ref type="bibr" coords="2,298.46,168.26,16.88,10.91" target="#b1">[2]</ref>. To the best of our knowledge this is the first time that text ranking using pretrained transformers has been applied to the automatic MeSH indexing problem. The performance of the new approach was evaluated by participating in the BioASQ challenge task 9a on semantic indexing <ref type="bibr" coords="2,327.07,208.91,13.52,10.91" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In recent years two high-performing approaches for automatic MeSH indexing have emerged: learning-to-rank approaches, and neural network multi-label classification approaches. Examples of learning-to-rank based systems are MTI <ref type="bibr" coords="2,319.55,294.63,16.14,10.91" target="#b0">[1]</ref> and DeepMeSH <ref type="bibr" coords="2,404.37,294.63,16.72,10.91" target="#b3">[4]</ref>, and examples of neural network multi-label classification systems are MeSHProbeNet <ref type="bibr" coords="2,386.21,308.18,16.05,10.91" target="#b4">[5]</ref> and AttentionMeSH <ref type="bibr" coords="2,487.90,308.18,14.82,10.91" target="#b5">[6]</ref>. Recently, You et al. achieved state-of-the-art performance with BERTMeSH <ref type="bibr" coords="2,411.31,321.73,16.72,10.91" target="#b6">[7]</ref>. BERTMeSH is a neural network multi-label classification approach that leverages pretrained transformers <ref type="bibr" coords="2,491.74,335.28,14.24,10.91" target="#b7">[8]</ref> and the article full-text.</p><p>Learning-to-rank <ref type="bibr" coords="2,175.88,362.38,14.19,10.91" target="#b8">[9]</ref> is a methodology that uses supervised machine learning algorithms to solve ranking problems. Typically, it is applied to automatic MeSH indexing by treating the title and abstract as the query, and candidate main headings as the documents to be ranked. Learningto-rank algorithms usually make use of hand-crafted features and rank documents by integrating multiple sources of evidence. For example, MTI uses a learning-to-rank algorithm <ref type="bibr" coords="2,452.26,416.58,19.01,10.91" target="#b9">[10]</ref> to rank candidate main headings from MetaMap <ref type="bibr" coords="2,271.11,430.13,21.21,10.91" target="#b10">[11]</ref>, PubMed Related Citations <ref type="bibr" coords="2,420.46,430.13,17.67,10.91" target="#b11">[12]</ref>, and machine learning algorithms. Sources of evidence include text features such as the fraction of main heading unigrams and bigrams that appear in the title or abstract.</p><p>Learning-to-rank algorithms can be classified as pointwise, pairwise, and listwise approaches depending on the loss function that is used. Pointwise approaches compute a loss for individual query-document pairs. The training task is to predict whether individual candidate documents are relevant to a query. At inference time, the model is run on all query-document pairs, and overall rankings are obtained using the predicted relevance scores. Pairwise approaches compute a loss for a query and a pair of documents. The training task is to predict which document is more relevant to a query. At inference time, for each query, pairwise rankings are obtained for all candidate documents pairs, and then these pairwise rankings are converted into an overall ranking. Finally, listwise approaches compute a loss for a query and all candidate documents. The training task is to predict the correct overall document ranking for a query. Hence, listwise approaches directly solve the ranking problem.</p><p>Recently, neural text ranking using pretrained transformers has proven to be a very effective approach for ad-hoc information retrieval <ref type="bibr" coords="2,279.21,633.36,16.76,10.91" target="#b12">[13]</ref>. On the MS MARCO passage ranking dataset large-scale pretrained transformer models, such as BERT <ref type="bibr" coords="2,340.77,646.91,20.59,10.91" target="#b13">[14]</ref>, have outperformed traditional information retrieval approaches by a considerable margin <ref type="bibr" coords="2,346.32,660.46,18.97,10.91" target="#b14">[15]</ref>. Text ranking using pretrained transformers was first demonstrated by Nogueira and Cho <ref type="bibr" coords="3,342.37,86.97,19.18,10.91" target="#b15">[16]</ref>. They implemented a pointwise approach by training BERT as a relevance classifier on MS MARCO query-passage pairs. Pairwise text ranking using BERT was also demonstrated as part of a multi-stage ranking architecture <ref type="bibr" coords="3,486.30,114.06,17.09,10.91" target="#b16">[17]</ref>. To the best of our knowledge there is no prior work on listwise text ranking using pretrained transformers. For a recent review of text ranking with pretrained transformers the interested reader is referred to "Pretrained Transformers for Text Ranking: BERT and Beyond" by J. Lin et al. <ref type="bibr" coords="3,100.76,168.26,15.29,10.91" target="#b12">[13]</ref>.</p><p>Domain-specific pretraining of transformer models can improve performance on downstream tasks <ref type="bibr" coords="3,110.83,195.36,12.92,10.91" target="#b7">[8,</ref><ref type="bibr" coords="3,126.60,195.36,7.65,10.91" target="#b1">2]</ref>, and BioBERT <ref type="bibr" coords="3,198.39,195.36,16.22,10.91" target="#b7">[8]</ref> is a popular domain-specific version of BERT that has been pretrained on a biomedical corpus. The BioBERT authors started with the original BERT checkpoint and ran additional pretraining steps on a corpus of PubMed abstracts and PubMed Central article full-text. Recently, PubMedBERT was shown to outperform BioBERT on the Biomedical Language Understanding and Reasoning Benchmark (BLURB) <ref type="bibr" coords="3,361.08,249.56,14.76,10.91" target="#b1">[2]</ref>. PubMedBERT was also pretrained on PubMed abstracts and PubMed Central article full-text, however, unlike BioBERT, it was trained from scratch using a domain-specific vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>This section describes our automatic MeSH indexing approaches that were evaluated by participating in the BioASQ challenge task 9a on semantic indexing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Convolutional Neural Network</head><p>The baseline approach was our previously described Convolutional Neural Network (CNN) for automatic MeSH indexing <ref type="bibr" coords="3,221.48,411.62,18.51,10.91" target="#b17">[18]</ref>. It is a neural network multi-label classification approach that takes the article title, abstract, journal, publication year, and indexing year as input. The top ğ‘ results from this model were also used as candidate main headings for the text ranking approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pointwise Text Ranking</head><p>The neural text ranking approaches were implemented using a domain-specific pretrained transformer model called PubMedBERT <ref type="bibr" coords="3,264.02,515.30,16.88,10.91" target="#b1">[2]</ref>. PubMedBERT is a BERT model with a domainspecific vocabulary that has been pretrained from scratch on a biomedical corpus. It was chosen because it was the top performing model in the BLURB benchmark <ref type="bibr" coords="3,391.70,542.39,15.35,10.91" target="#b1">[2]</ref>, and also because its domain-specific vocabulary was expected to encode biomedical text efficiently. More details about the BERT architecture and fine-tuning configurations can be found in the original BERT paper <ref type="bibr" coords="3,112.11,583.04,18.25,10.91" target="#b13">[14]</ref>.</p><p>For the pointwise text ranking approach PubMedBERT was configured as a relevance classifier using the text pair classification configuration. The input sequence was:</p><formula xml:id="formula_0" coords="3,232.66,633.09,273.33,10.91">[[ğ¶ğ¿ğ‘†], ğ‘, [ğ‘†ğ¸ğ‘ƒ ], ğ‘‘, [ğ‘†ğ¸ğ‘ƒ ]],<label>(1)</label></formula><p>where the query, ğ‘, comprises the concatenated tokens of the indexing year, journal name, title, and abstract, and ğ‘‘ comprises the tokens of the candidate main heading.</p><p>[ğ¶ğ¿ğ‘†] and [ğ‘†ğ¸ğ‘ƒ ] are the classification and separator special tokens respectively.</p><p>In the text pair classification configuration, the [ğ¶ğ¿ğ‘†] token is used to represent the input sequence, and the relevance probability was computed by adding a softmax classification head on top of its contextualized embedding (ğ‘‡ [ğ¶ğ¿ğ‘†] ):</p><formula xml:id="formula_1" coords="4,168.26,153.69,337.73,11.95">ğ‘ƒ (ğ‘…ğ‘’ğ‘™ğ‘’ğ‘£ğ‘ğ‘›ğ‘¡ = 1|ğ‘‘ ğ‘– , ğ‘) = ğ‘  ğ‘– â‰œ ğ‘ ğ‘œğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘‡ [ğ¶ğ¿ğ‘†] ğ‘Š + ğ‘) 1 ,<label>(2)</label></formula><p>where ğ‘Š and ğ‘ are the weights and bias of the classification layer respectively, and ğ‘ ğ‘œğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥(â€¢) ğ‘– denotes the ğ‘–-th element of the softmax output.</p><p>The training task was to predict whether a candidate main heading is relevant, given the article title, abstract, and other metadata. For the automatic MeSH indexing task, a main heading is considered relevant if it was indexed. PubMedBERT was fine-tuned on positive and negative article-candidate main heading pairs sampled from the CNN top results using the cross-entropy loss:</p><formula xml:id="formula_2" coords="4,203.64,270.33,302.35,25.61">ğ¿ = - âˆ‘ï¸ ğ‘—âˆˆğ½ğ‘ğ‘œğ‘  ğ‘™ğ‘œğ‘”(ğ‘  ğ‘— ) - âˆ‘ï¸ ğ‘—âˆˆğ½ğ‘›ğ‘’ğ‘” ğ‘™ğ‘œğ‘”(1 -ğ‘  ğ‘— ),<label>(3)</label></formula><p>where ğ½ ğ‘ğ‘œğ‘  is a set of indexes for article-main heading pairs where the main heading was indexed, and ğ½ ğ‘›ğ‘’ğ‘” is a set indexes for article-main heading pairs where the main heading was not indexed. At inference time, the fine-tuned model was run on all candidate main headings from the CNN top results and predicted relevance scores were used to generate a per-article main heading ranking. The final set of predicted main headings for an article was obtained by applying a decision threshold to the ranking scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Listwise Text Ranking</head><p>For the listwise text ranking approach PubMedBERT was configured for text tagging, and the second text input was the shuffled candidate main headings separated by the pipe symbol. The input sequence was therefore:</p><formula xml:id="formula_3" coords="4,194.58,487.06,311.41,11.42">[[ğ¶ğ¿ğ‘†], ğ‘, [ğ‘†ğ¸ğ‘ƒ ], |, ğ‘‘ 1 , |, ğ‘‘ 2 , ..., |, ğ‘‘ ğ‘ , [ğ‘†ğ¸ğ‘ƒ ]].<label>(4)</label></formula><p>The pipe symbols allow the model to distinguish between different candidate main headings, and random shuffling was employed to prevent overfitting to the main heading order. Relevance probabilities were computed by feeding the contextualized embedding of the first token of each main heading to the softmax classification head described in Equation 2. Thus, the listwise approach directly creates a ranking by performing relevance classification on all candidate main headings at once. PubMedBERT was fine-tuned on the top ğ‘ main headings from the CNN model using cross-entropy loss. Again, the final set of main headings was obtained by applying a decision threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Multi-Stage Text Ranking</head><p>The listwise approach is expected to outperform the pointwise approach because it can consider interactions between main headings. However, a problem with the listwise approach is that the length of the input sequence is proportional to the number of candidate main headings, and this limits the recall of the approach because there is a maximum number of candidate main headings that can fit within BERT's maximum sequence length of 512 tokens. The multi-stage text ranking approach attempts to overcome this limitation by first ranking the candidate main headings using the pointwise approach. The pointwise approach can rank any number of main headings, and it is expected to have higher recall@N than the CNN model. For the multi-stage ranking approach it was also found to be beneficial to average the ranking scores of the different stages.</p><p>More formally, starting with the CNN ranking, ğ‘… 0 , the top ğ‘ ğ‘ results were reranked using the pointwise approach to generate a ranking ğ‘… 1 . ğ‘… 0 and ğ‘… 1 scores were then averaged to generate ğ‘… 2 . Next, the top ğ‘ ğ‘™ results in ğ‘… 2 were reranked using the listwise approach to generate ranking ğ‘… 3 . The final ranking was computed by averaging the scores of ğ‘… 2 and ğ‘… 3 . A decision threshold was applied to generate the final main heading predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Multi-Stage Text Ranking with COVID-19 Rules</head><p>During the BioASQ challenge it was noticed that our machine learning approaches were performing poorly on COVID-19 articles, and two specific problems were identified:</p><p>â€¢ The "COVID-19" main heading was always being indexed with unnecessary additional main headings "Pneumonia, Viral", "Coronavirus Infections", and "Pandemics". â€¢ The "SARS-CoV-2" main heading was always being indexed with the unnecessary additional main heading "Betacoronavirus".</p><p>The precision of the multi-stage text ranking approach was improved by removing these unnecessary main headings using manually written COVID-19 rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Hybrid Approach</head><p>MTI First Line Indexing (MTIFL) and MTI Review Filtering (MTIR) are used for selected journals to partially automate the NLM indexing process<ref type="foot" coords="5,300.73,474.94,3.71,7.97" target="#foot_0">2</ref> . For MTIFL journals, MTI provides the initial indexing, and this is later reviewed (and potentially modified) by human indexers. For MTIR journals the process is the same except that human curation is only used for critical elements.</p><p>Empirically, MTI is found to perform very well for semi-automatically indexed journals. In order to achieve the highest possible agreement with human indexing, we therefore implemented a hybrid approach that used MTI First Line Index results for MTIFL journals, Default MTI results for MTIR journals, and multi-stage text ranking results (with COVID-19 rules) for all other journals. Default MTI is configured for balanced precision and recall, and its results were expected to be the most similar to the initial indexing provided for MTIR journals. MTI results are made publicly available for anyone wanting to use them during the challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">BioASQ Task 9a</head><p>The performance of our proposed approaches were evaluated by participating in the large-scale online biomedical semantic indexing task of the 2021 BioASQ challenge (task 9a). The task released 3 batches of 5 test sets, and these contained between 4,000 and 11,000 soon-to-beindexed MEDLINE articles. Participants had a limited time window to submit results, and this was necessary to ensure that indexing predictions were made before indexer annotations became available.</p><p>The NLM team used the challenge to evaluate various different text ranking approaches and configurations. This paper describes our final best-performing approaches, and these are evaluated on the 5 weekly test sets of batch 3. Results for our final approaches were only submitted to the last two test sets of batch 3, and for a more comprehensive performance evaluation, we have independently generated indexing predictions for the week 1-3 test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Dataset</head><p>The dataset was constructed from the MEDLINE/PubMed 2021 annual baseline <ref type="foot" coords="6,451.61,314.78,3.71,7.97" target="#foot_1">3</ref> . Fully and semi-automatically indexed articles ("Automated" or "Curated" indexing method) were excluded as we believe that the indexing of these articles may be biased by MTI's predictions. 20,000 randomly selected articles published in 2020/2021 were reserved for a validation set, and another 40,000 randomly selected articles published in 2020/2021 were reserved for our personal test set. The remaining 10 million articles published after 2006 were used for the training set.</p><p>The presented approaches were evaluated on the BioASQ task 9a batch 3 test sets, and independently generated predictions were evaluated using indexer annotations downloaded from the NLM E-Utilities<ref type="foot" coords="6,202.86,423.17,3.71,7.97" target="#foot_2">4</ref> service on the 28th of June 2021. The final challenge results were calculated using the indexing available on the 21st of May 2021, and to allow for fair comparisons between systems, indexing completed after this date was excluded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Evaluation Metrics</head><p>The primary evaluation metric used by the semantic indexing task is the micro F1-score (ğ‘€ ğ‘–ğ¹ ) and this is defined as the harmonic mean of the micro precision (ğ‘€ ğ‘–ğ‘ƒ ) and the micro recall (ğ‘€ ğ‘–ğ‘…):</p><formula xml:id="formula_4" coords="6,241.04,539.30,264.94,24.43">ğ‘€ ğ‘–ğ¹ = 2 â€¢ ğ‘€ ğ‘–ğ‘ƒ â€¢ ğ‘€ ğ‘–ğ‘… ğ‘€ ğ‘–ğ‘ƒ + ğ‘€ ğ‘–ğ‘… ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_5" coords="6,230.85,577.05,271.28,33.75">ğ‘€ ğ‘–ğ‘ƒ = âˆ‘ï¸€ ğ‘ ğ´ ğ‘–=1 âˆ‘ï¸€ ğ‘ ğ¿ ğ‘—=1 ğ‘¦ ğ‘–ğ‘— â€¢ ğ‘¦ Ë†ğ‘–ğ‘— âˆ‘ï¸€ ğ‘ ğ´ ğ‘–=1 âˆ‘ï¸€ ğ‘ ğ¿ ğ‘—=1 ğ‘¦ Ë†ğ‘–ğ‘— , (<label>6</label></formula><formula xml:id="formula_6" coords="6,502.13,588.47,3.86,10.91">)</formula><formula xml:id="formula_7" coords="6,230.92,616.56,271.21,33.75">ğ‘€ ğ‘–ğ‘… = âˆ‘ï¸€ ğ‘ ğ´ ğ‘–=1 âˆ‘ï¸€ ğ‘ ğ¿ ğ‘—=1 ğ‘¦ ğ‘–ğ‘— â€¢ ğ‘¦ Ë†ğ‘–ğ‘— âˆ‘ï¸€ ğ‘ ğ´ ğ‘–=1 âˆ‘ï¸€ ğ‘ ğ¿ ğ‘—=1 ğ‘¦ ğ‘–ğ‘— . (<label>7</label></formula><formula xml:id="formula_8" coords="6,502.13,627.98,3.86,10.91">)</formula><p>In the above equations ğ‘¦ are the indexer annotations, ğ‘¦ Ë†are the model predictions, ğ‘ ğ´ is the number of articles, and ğ‘ ğ¿ is the number of main headings. Model predictions were made after applying a decision threshold to the predicted scores. There is an optimum decision threshold that results in the highest F1-score, and this threshold was determined by a linear search on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Configuration</head><p>The configuration for the CNN model has previously been described in Rae et al. <ref type="bibr" coords="7,450.62,190.49,15.33,10.91" target="#b17">[18]</ref>, and the model was retrained on the MEDLINE/PubMed 2021 dataset described in this paper.</p><p>The pointwise and listwise ranking models were implemented using the Hugging Face Transformers library (v4.2.2) with a PyTorch (v1.7.1) backend. PubMedBERT pretrained weights were downloaded from the Hugging Face model repository, and the uncased model pretrained on abstracts and full-text was selected ("BiomedNLP-PubMedBERT-base-uncased-abstractfulltext").</p><p>The pointwise model was implemented in Hugging Face Transformers using the BertForSe-quenceClassification class (specifying the number of labels as 2), and the default PubMedBERT configuration was left unchanged. Training was run for approximately 1 epoch on a balanced dataset, and the Adam optimizer was used with L2 weight decay set to 0.01. The learning rate schedule included 10,000 warmup steps, a maximum learning rate of 2e-5, and a linear decay to zero thereafter.</p><p>The listwise model was implemented in Hugging Face Transformers using the BertForTo-kenClassification class, with the number of labels set to 2. All tokens, except for the first token of each main heading, were assigned the masking label of -100. The first token of each main heading was assigned a label of 1 or 0 for indexed and not-indexed main headings respectively. Again, the PubMedBERT configuration was not altered, and the model was trained on the CNN top 50 results for approximately 10 epochs. Other training settings were the same as for the pointwise approach, except that a lower maximum learning rate of 9e-6 was used.</p><p>Both ranking models were trained on the Biowulf cluster<ref type="foot" coords="7,351.05,459.72,3.71,7.97" target="#foot_3">5</ref> using NVIDIA V100x 32GB GPUs. The pointwise and listwise models were trained on 4 and 2 GPUs respectively for approximately 10 days. FP16 training was used and an effective batch size of 128 was achieved using gradient accumulation. Validation set performance of the listwise model had converged after 10 days, however the performance of the pointwise model was still improving.</p><p>For the hybrid approach, MTI results <ref type="foot" coords="7,260.43,527.47,3.71,7.97" target="#foot_4">6</ref> and MTIFL and MTIR journal lists<ref type="foot" coords="7,415.09,527.47,3.71,7.97" target="#foot_5">7</ref> (22nd of September 2020 versions) were downloaded from the NLM website.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4.">Results</head><p>Table <ref type="table" coords="7,115.63,592.10,5.04,10.91" target="#tab_0">1</ref> summarizes the micro F1-score performance of top performing systems in batch 3. For each weekly test set, the table includes the highest micro F1-score achieved by each team, along with the best performing MTI baseline for reference. The table shows that the performance of the neural text ranking approach is very competitive. Our best performing hybrid approach outperformed the MTI baseline by about 5% points, and it has very similar performance to the state-of-the-art dmiip_fdu systems.</p><p>Table <ref type="table" coords="8,126.96,397.55,5.06,10.91" target="#tab_1">2</ref> shows micro F1-score performance of NLM approaches in batch 3. Note that results for the "Multi-stage + COVID-19 rules" approach were not submitted to the challenge because teams were allowed a maximum of 5 systems. Comparing the performance of the multi-stage ranking approach to the CNN model, it can be seen that neural text ranking provided about a 5% point performance boost on average. The table shows that the listwise approach outperformed the pointwise approach and also that multi-stage ranking was beneficial. The COVID-19 rules provided small but consistent performance improvements, and the hybrid approach, which substituted MTI results for semi-automatically indexed journals, was the best performing NLM system in all batch 3 test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Listwise Model Hyperparameter Search</head><p>There is a performance trade-off for the listwise approach: increasing the number of candidate main headings (ğ‘ ) increases the maximum achievable recall, but it also results in more input truncation due to longer input sequence lengths. This section explores this trade-off through a hyperparameter search for the optimum number of candidate main headings for the listwise approach.</p><p>For the study, the listwise model was trained with four different values of ğ‘ between 25 and 50, and input truncation percentages and model performance were measured on the validation set. BioBERT input truncation percentages were also measured for comparison.</p><p>The results of the study are shown in Figure <ref type="figure" coords="8,307.77,664.07,3.81,10.91" target="#fig_0">1</ref>. Figure <ref type="figure" coords="8,352.17,664.07,10.26,10.91" target="#fig_0">1a</ref> shows a significant increase in the percentage of truncated inputs as the number of candidate main headings is increased from 25 to 50. For PubMedBERT, 15% of inputs were truncated for 25 candidate main headings, and this rises to 39% of inputs for 50 candidate main headings. The figure also shows that input truncation percentages were much higher for BioBERT than for PubMedBERT, and this is because BioBERT does not have a domain-specific vocabulary. Despite the relatively high input truncation percentages observed in Figure <ref type="figure" coords="9,447.54,384.73,8.47,10.91" target="#fig_0">1a</ref>, Figure <ref type="figure" coords="9,495.33,384.73,10.66,10.91" target="#fig_0">1b</ref> shows that the listwise model micro F1-score increases with ğ‘ , and the model trained with 50 candidate main headings is shown to have the highest micro F1-score of 0.7020 on the validation set. As expected, the increase in micro F1-score is correlated with the increase in CNN model recall, but for ğ‘ = 50 the strength of this correlation appears to be weakening.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>As expected, the results indicate that the listwise text ranking approach outperforms the pointwise text ranking approach, but to confirm this we would need to train the pointwise model to convergence and also optimize the number of candidate main headings. The pointwise approach considers one article-main heading pair per training example, whereas the listwise approach considers 50 article-main heading pairs per training example, and so it makes sense that training of the pointwise model would converge more slowly.</p><p>This work has presented a hyperparameter search for the optimum number of candidate main headings for the listwise approach and increasing the number of candidate main headings from 25 to 50 was shown to result in a 0.51% point improvement in micro F1-score performance on the validation set. Increasing the number of candidate main headings further may result in additional performance improvements, however; for ğ‘ = 50 there is some evidence that input truncation is starting to limit performance. The study also indicates that PubMedBERT was a good model choice because it was shown to encode biomedical text more efficiently than BioBERT resulting in significantly less input truncation. For 50 candidate main headings the BioBERT tokenizer was shown to truncate about 75% of input sequences, and this would likely have a large negative impact on MeSH indexing performance.</p><p>The poor performance of our machine learning models on COVID-19 articles (before applying the COVID-19 rules) can be explained by inconsistent and out-of-date training data. The problem is that indexing of COVID-19 articles has evolved during the pandemic due to changing indexing rules and also after the addition of COVID-19 specific main headings. This is an interesting example of how sudden data and concept drift have been problematic for machine learning systems during the COVID-19 pandemic.</p><p>Finally, substituting MTI predictions for semi-automatically indexed journals was shown to consistently improve performance. An explanation could be that indexing of MTIFL and MTIR journals is biased by MTI's predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper has presented a new neural text ranking approach for automatic MeSH indexing. PubMedBERT was fine-tuned on MEDLINE data and used to rank candidate main headings obtained from a CNN model. Pointwise, listwise, and multi-stage text ranking approaches were demonstrated, and their performance was evaluated on batch 3 of the BioASQ 2021 semantic indexing task. The neural text ranking approach was shown to have very competitive performance, and the multi-stage text ranking method was found to boost the CNN model micro F1-score performance by about 5% points.</p><p>In the future, we would like to investigate the zero-shot performance of neural text ranking models for automatic MeSH indexing. In particular, it would be interesting to know if they can correctly index a new main heading for a concept that has only been seen during unsupervised pretraining. It would be very useful if the text ranking models are learning the general concept of "indexing relevance" rather than specific indexing rules for each main heading.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="9,89.29,255.47,416.96,8.93;9,89.29,267.48,416.69,8.87;9,89.29,279.43,39.12,8.87;9,89.29,84.19,418.86,151.91"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: a) Percentage of truncated inputs vs. number of candidate main headings (ğ‘ ) for PubMedBERT and BioBERT. b) Listwise model micro F1-score and CNN model recall vs. number of candidate main headings.</figDesc><graphic coords="9,89.29,84.19,418.86,151.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,88.99,90.49,418.30,111.08"><head>Table 1</head><label>1</label><figDesc>Micro F1-score performance of top performing systems in batch 3.</figDesc><table coords="8,95.67,118.53,411.62,83.04"><row><cell>System</cell><cell cols="6">Week 1 Week 2 Week 3 Week 4 Week 5 Average</cell></row><row><cell>NLM System 3 (hybrid approach)</cell><cell>0.7059</cell><cell>0.6973</cell><cell>0.6966</cell><cell>0.6999</cell><cell>0.7075</cell><cell>0.7014</cell></row><row><cell>dmiip_fdu systems</cell><cell>0.7060</cell><cell>0.6976</cell><cell>0.6980</cell><cell>0.6966</cell><cell>0.7013</cell><cell>0.6999</cell></row><row><cell>MTI First Line Index</cell><cell>0.6555</cell><cell>0.6445</cell><cell>0.6541</cell><cell>0.6491</cell><cell>0.6508</cell><cell>0.6508</cell></row><row><cell>pi_dna</cell><cell>0.6443</cell><cell>0.6464</cell><cell>0.6503</cell><cell>0.6466</cell><cell>0.6498</cell><cell>0.6475</cell></row><row><cell>DeepSys2</cell><cell>0.5780</cell><cell>0.5674</cell><cell></cell><cell>0.5651</cell><cell>0.5625</cell><cell>0.5683</cell></row><row><cell>iria-1</cell><cell>0.4895</cell><cell>0.4778</cell><cell>0.4758</cell><cell>0.4818</cell><cell>0.4729</cell><cell>0.4796</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,88.99,221.47,408.85,111.12"><head>Table 2</head><label>2</label><figDesc>Micro F1-score performance of NLM approaches in batch 3.</figDesc><table coords="8,97.43,249.51,400.42,83.08"><row><cell>Approach</cell><cell cols="6">Week 1 Week 2 Week 3 Week 4 Week 5 Average</cell></row><row><cell>Hybrid (NLM System 3)</cell><cell>0.7059</cell><cell>0.6973</cell><cell>0.6966</cell><cell>0.6999</cell><cell>0.7075</cell><cell>0.7014</cell></row><row><cell>Multi-stage + COVID-19 rules</cell><cell>0.7032</cell><cell>0.6971</cell><cell>0.6953</cell><cell>0.6932</cell><cell>0.7011</cell><cell>0.6980</cell></row><row><cell>Multi-stage (NLM System 2)</cell><cell>0.7000</cell><cell>0.6945</cell><cell>0.6931</cell><cell>0.6894</cell><cell>0.6932</cell><cell>0.6940</cell></row><row><cell>Listwise (NLM System 4)</cell><cell>0.6931</cell><cell>0.6888</cell><cell>0.6884</cell><cell>0.6836</cell><cell>0.6876</cell><cell>0.6883</cell></row><row><cell>Pointwise (NLM System 1)</cell><cell>0.6888</cell><cell>0.6831</cell><cell>0.6820</cell><cell>0.6801</cell><cell>0.6799</cell><cell>0.6828</cell></row><row><cell>CNN (NLM CNN)</cell><cell>0.6482</cell><cell>0.6434</cell><cell>0.6424</cell><cell>0.6381</cell><cell>0.6424</cell><cell>0.6429</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="5,108.93,670.93,145.68,8.97"><p>https://ii.nlm.nih.gov/MTI/MTIFL.shtml</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="6,108.93,660.08,254.20,8.97"><p>https://www.nlm.nih.gov/databases/download/pubmed_medline.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="6,108.93,671.04,178.13,8.97"><p>https://www.ncbi.nlm.nih.gov/books/NBK25497/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="7,108.93,649.10,72.28,8.97"><p>https://hpc.nih.gov/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="7,108.93,660.06,108.18,8.97"><p>http://ii.nlm.nih.gov/BioASQ/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5" coords="7,108.93,671.02,145.68,8.97"><p>https://ii.nlm.nih.gov/MTI/MTIFL.shtml</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was supported by the <rs type="programName">Intramural Research Program</rs> of the <rs type="funder">National Library of Medicine, National Institutes of Health</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_rgBsv9j">
					<orgName type="program" subtype="full">Intramural Research Program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,112.66,574.48,393.54,10.91;10,112.66,588.02,245.66,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,218.93,574.48,287.27,10.91;10,112.66,588.02,106.66,10.91">Demner-Fushman, 12 years on -is the NLM medical text indexer still useful and relevant?</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mork</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aronson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,227.54,588.02,83.26,10.91">J. Biomed. Semant</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,601.57,394.53,10.91;10,112.66,615.12,394.53,10.91;10,112.66,628.67,122.77,10.91" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="10,112.66,615.12,390.01,10.91">Domain-specific language model pretraining for biomedical natural language processing</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.15779</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,642.22,394.62,10.91;10,112.66,655.77,393.33,10.91;10,112.66,669.32,336.09,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,409.27,642.22,92.68,10.91;10,112.66,655.77,305.37,10.91">Large-scale biomedical semantic indexing and question answering</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,449.03,655.77,56.96,10.91;10,112.66,669.32,94.24,10.91">Advances in Information Retrieval</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="550" to="556" />
		</imprint>
	</monogr>
	<note>BioASQ at CLEF</note>
</biblStruct>

<biblStruct coords="11,112.66,86.97,393.33,10.91;11,112.66,100.52,395.01,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,383.29,86.97,122.70,10.91;11,112.66,100.52,244.22,10.91">DeepMeSH: deep semantic representation for improving large-scale MeSH indexing</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mamitsuka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,365.23,100.52,64.30,10.91">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="70" to="79" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,114.06,393.53,10.91;11,112.66,127.61,172.72,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,307.12,114.06,199.07,10.91;11,112.66,127.61,66.68,10.91">MeSHProbeNet: a self-attentive probe net for MeSH indexing</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Xun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,187.86,127.61,65.61,10.91">Bioinformatics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,141.16,393.32,10.91;11,112.66,154.71,395.01,10.91;11,112.66,168.26,286.70,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,276.86,141.16,229.12,10.91;11,112.66,154.71,105.41,10.91">AttentionMeSH: simple, effective and interpretable automatic MeSH indexer</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,240.18,154.71,95.31,10.91;11,112.66,168.26,183.04,10.91">Proceedings of the 6th BioASQ Workshop</title>
		<meeting>the 6th BioASQ Workshop<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2018">1 Novemeber 2018. 2018</date>
			<biblScope unit="page" from="47" to="56" />
		</imprint>
	</monogr>
	<note>6th BioASQ Workshop</note>
</biblStruct>

<biblStruct coords="11,112.66,181.81,393.33,10.91;11,112.66,195.36,393.98,10.91;11,112.66,208.91,38.81,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,275.47,181.81,230.52,10.91;11,112.66,195.36,277.88,10.91">BERTMeSH: deep contextual representation learning for large-scale high-performance MeSH indexing with full text</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mamitsuka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,398.35,195.36,65.80,10.91">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="684" to="692" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,222.46,393.33,10.91;11,112.66,236.01,394.53,10.91;11,112.41,249.56,22.69,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,355.95,222.46,150.04,10.91;11,112.66,236.01,251.54,10.91">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,372.87,236.01,64.30,10.91">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,263.11,393.98,10.91;11,112.66,276.66,38.81,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,158.86,263.11,190.80,10.91">Learning to rank for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,359.47,263.11,108.27,10.91">Found. Trends Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="225" to="331" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,290.20,393.33,10.91;11,112.66,303.75,395.01,10.91;11,112.66,317.30,295.76,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,294.35,290.20,211.64,10.91;11,112.66,303.75,87.36,10.91">Using learning-to-rank to enhance NLM medical text indexer results</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Zavorin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mork</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,112.66,317.30,197.33,10.91">Proceedings of the Fourth BioASQ workshop</title>
		<meeting>the Fourth BioASQ workshop<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2016-08-13">12-13 August 2016. 2016</date>
			<biblScope unit="page" from="8" to="15" />
		</imprint>
	</monogr>
	<note>4th BioASQ workshop</note>
</biblStruct>

<biblStruct coords="11,112.66,330.85,393.33,10.91;11,112.66,344.40,250.05,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,237.08,330.85,268.90,10.91;11,112.66,344.40,38.24,10.91">An overview of metamap: historical perspective and recent advances</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Aronson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F.-M</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,159.61,344.40,119.16,10.91">J. Am. Med. Inform. Assoc</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="229" to="236" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,357.95,393.33,10.91;11,112.66,371.50,197.03,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,200.33,357.95,305.66,10.91;11,112.66,371.50,40.44,10.91">PubMed related articles: a probabilistic topic-based model for content similarity</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Wilbur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,161.07,371.50,90.95,10.91">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">423</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,385.05,394.52,10.91;11,112.66,398.60,122.77,10.91" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yates</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.06467</idno>
		<title level="m" coord="11,239.76,385.05,262.41,10.91">Pretrained transformers for text ranking: BERT and beyond</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,412.15,393.33,10.91;11,112.66,425.70,393.33,10.91;11,112.66,439.25,393.32,10.91;11,112.66,452.79,382.85,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,323.15,412.15,182.83,10.91;11,112.66,425.70,186.91,10.91">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,327.87,425.70,178.11,10.91;11,112.66,439.25,393.32,10.91;11,112.66,452.79,102.30,10.91">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="11,112.66,466.34,393.33,10.91;11,112.66,479.89,150.64,10.91" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Campos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.07662</idno>
		<title level="m" coord="11,315.05,466.34,190.93,10.91;11,112.66,479.89,20.95,10.91">Overview of the TREC 2020 deep learning track</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,493.44,357.44,10.91" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04085</idno>
		<title level="m" coord="11,206.07,493.44,132.96,10.91">Passage re-ranking with BERT</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,506.99,395.01,10.91;11,112.66,522.98,97.35,7.90" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.14424</idno>
		<title level="m" coord="11,285.34,506.99,190.07,10.91">Multi-stage document ranking with BERT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,534.09,394.62,10.91;11,112.66,547.64,395.17,10.91;11,112.66,561.19,394.53,10.91;11,112.66,574.74,22.69,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="11,387.46,534.09,119.82,10.91;11,112.66,547.64,205.67,10.91">Automatic mesh indexing: Revisiting the subheading attachment problem</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">O</forename><surname>Pritchard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Mork</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,341.83,547.64,166.01,10.91;11,112.66,561.19,167.83,10.91">AMIA 2020, American Medical Informatics Association Annual Symposium</title>
		<meeting><address><addrLine>Virtual Event, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AMIA</publisher>
			<date type="published" when="2020">November 14-18, 2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
