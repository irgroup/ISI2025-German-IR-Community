<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,390.75,15.42;1,89.29,106.66,342.52,15.42;1,89.29,128.58,374.47,15.43">LASIGE-BioTM at MESINESP2: entity linking with semantic similarity and extreme multi-label classification on Spanish biomedical documents</title>
				<funder ref="#_7QznR9f #_wBCcjy6">
					<orgName type="full">LASIGE Research Unit</orgName>
				</funder>
				<funder ref="#_YmndK38">
					<orgName type="full">Deep Semantic Tagger (DeST)</orgName>
				</funder>
				<funder ref="#_AKnrrv2">
					<orgName type="full">FCT</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,156.89,54.80,11.96"><forename type="first">Pedro</forename><surname>Ruas</surname></persName>
							<email>psruas@fc.ul.pt</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">LASIGE</orgName>
								<orgName type="department" key="dep2">Faculdade de Ci√™ncias</orgName>
								<orgName type="institution">Universidade de Lisboa</orgName>
								<address>
									<postCode>1749-016</postCode>
									<settlement>Lisbon</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,156.74,156.89,96.07,11.96"><forename type="first">Vitor</forename><forename type="middle">D T</forename><surname>Andrade</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">LASIGE</orgName>
								<orgName type="department" key="dep2">Faculdade de Ci√™ncias</orgName>
								<orgName type="institution">Universidade de Lisboa</orgName>
								<address>
									<postCode>1749-016</postCode>
									<settlement>Lisbon</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,283.81,156.89,94.89,11.96"><forename type="first">Francisco</forename><forename type="middle">M</forename><surname>Couto</surname></persName>
							<email>fcouto@di.fc.ul.pt</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">LASIGE</orgName>
								<orgName type="department" key="dep2">Faculdade de Ci√™ncias</orgName>
								<orgName type="institution">Universidade de Lisboa</orgName>
								<address>
									<postCode>1749-016</postCode>
									<settlement>Lisbon</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,390.75,15.42;1,89.29,106.66,342.52,15.42;1,89.29,128.58,374.47,15.43">LASIGE-BioTM at MESINESP2: entity linking with semantic similarity and extreme multi-label classification on Spanish biomedical documents</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">C14154E64CA5A4ABE87BFC14D3599E9B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Named Entity Recognition</term>
					<term>Named Entity Linking</term>
					<term>Extreme Multi-Label Classification</term>
					<term>Multilingual</term>
					<term>Text Mining</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our team, LASIGE_BioTM, participated in the three sub-tracks of MESINESP2: (1) scientific literature, (2) clinical trials, and (3) patents. Our system comprises two modules: entity linking and extreme multilabel classification. The first module uses the entities recognized in text and then applies a graph-based entity linking model to link them to the DeCS vocabulary. In the end, it applies a semantic similaritybased filter to determine the most relevant entities in each document, which are then fed to the second module. The second module consists of an adapted version of the X-Transformer algorithm, and is responsible for associating each document with the top-20 relevant DeCS codes, which can be viewed as an extreme multi-label classification algorithm. The obtained results (micro F1-scores) were 0.2007, 0.0686, and 0.0314 for sub-tracks 1, 2, and 3, respectively. These represent low values when compared to other participants, mainly because of the lack of time our team had available to train the models. All of the used software is available in an open access repository.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Automatic semantic indexing is essential to organise the growing text data that is available, which is particularly critical in scientific domains, including the biomedical one, where most of the findings are available in the text format. We can view this task as an extreme multi-label classification (XMC) problem, in which the goal is to tag a given data point with a subset of relevant labels from an extremely large label list. Therefore, the data points are the text documents to classify, and the label list provided by a knowledge base, such as an ontology. Most of the proposed XMC approaches focus on datasets including Wikipedia articles or on datasets with commercial application (e.g. dynamic search advertising) and less attention is devoted to the biomedical domain. Additionally, multilingual approaches focusing on other languages besides English are also scarce, such is the case of Spanish.</p><p>In this sense, initiatives such as BioASQ <ref type="bibr" coords="1,282.79,580.83,12.95,10.91" target="#b0">[1]</ref> are necessary to stimulate the development of biomedical, multilingual-focused approaches. In particular, the Medical Semantic Indexing In Spanish (MESINESP) task was first introduced in the BioASQ 2020 challenge and the goal was to perform semantic indexing of Spanish health-related documents, like scientific articles, clinical trials, and healthcare project summaries, with terms from the Spanish version of the Descriptores en Ciencias de la Salud (DeCS). The second edition, the MESINESP2 shared-task <ref type="bibr" coords="2,89.29,141.16,12.85,10.91" target="#b1">[2]</ref> was extended and included the following sub-tracks: MESINESP-L -Scientific Literature: Automatic indexing with DeCS terms of Spanish abstracts from two databases, IBECS and LILACS; MESINESP-T -Clinical Trials: Automatic indexing with DeCS terms of Spanish clinical trials from REEC (Registro Espa√±ol de Estudios Cl√≠nicos); MESINESP-P -Patents: Automatic indexing with DeCS terms Spanish patents extracted from Google Patents.</p><p>In the past, named entities have been considered important features that aid the classification of texts. For instance, Gui et al <ref type="bibr" coords="2,238.30,222.46,12.99,10.91" target="#b2">[3]</ref> proposed a hierarchical text classification method that leverages named entities as features, and, according to the conclusions of the referred study, the features are responsible for the improvement of the method's performance. More recently, Anelic and co-workers <ref type="bibr" coords="2,162.73,263.11,12.99,10.91" target="#b3">[4]</ref> have argued that named entities do not improve the performance of text classification, and can even decrease it. However, none of these works attempted to normalise the recognised entities to concepts belonging to structured vocabularies, the approaches only used the surface form of the entities instead of the designations for the associated concepts. Besides, not every entity recognised in a given document has the same importance, i.e., some entities may not be related with the main topic of the document, which can be particularly true in documents containing a large number of different entities. Therefore, we explored the hypothesis that linking the recognised entities to concepts of a structured vocabulary and selecting only the most relevant entities to feed the text classification algorithm improve its performance.</p><p>After participating in the first edition <ref type="bibr" coords="2,268.46,398.60,11.40,10.91" target="#b4">[5]</ref>, this paper describes the participation of our team, LASIGE_BioTM, in the sub-tracks of MESINESP2. We developed a pipeline based on two modules: the first one performs entity linking, by mapping the recognised entities in text to terms of the DeCS vocabulary and then applying a semantic similarity-based filter to obtain the most relevant entities in each document; the second module is based on the X-Transformer algorithm <ref type="bibr" coords="2,89.29,466.34,11.58,10.91" target="#b5">[6]</ref>, and is responsible to classify each document with the most relevant DeCS terms. The software used in the experiments is available on: https://github.com/lasigeBioTM/MESINESP2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related work 1.Entity Linking</head><p>The extraction of entities is carried out through the text mining process. This process can be executed by different approaches such as: rule-based methods, machine learning and deep learning.</p><p>Rule-based methods include a set of terms, regular expressions or sentence constructions defined by experts <ref type="bibr" coords="2,176.99,604.39,11.58,10.91" target="#b6">[7]</ref>. Rule-based methods also include dictionary approaches, in which a given text is matched against a lexicon using string matching <ref type="bibr" coords="2,364.82,617.94,11.43,10.91" target="#b7">[8]</ref>.</p><p>Machine learning methods in text mining are trained on training and validation datasets to make predictions on a test dataset <ref type="bibr" coords="2,258.65,645.03,11.59,10.91" target="#b6">[7]</ref>. Deep learning is a subset of machine learning that consists of artificial neural networks that include multiple hidden layers between input and output. An artificial neural network is composed of nodes, processing units with a similar function to the neurons in the brain. The input for the nodes in text mining applications are word embeddings, which are vector representations of words. According to the way the nodes are organised, deep neural networks can be classified as Recurrent Neural Network (RNN), Convolutional Neural Network (CNN), among others.</p><p>Usually text mining approaches include the tasks of Named Entity Recognition (NER) and Named Entity Linking (NEL). NER corresponds to the recognition of entities mentioned in the text and NEL to the linking of the recognised entities to concepts of a given knowledge base.</p><p>For the NER task, state-of-art approaches usually have a bidirectional long short-term memory -conditional random fields (BiLSTM-CRF) architecture. However, approaches that use pretrained language models have recently emerged and showed promising results. One of the pre-trained language models that has been highlighted in the tasks of text mining is BERT <ref type="bibr" coords="3,491.99,236.01,11.40,10.91" target="#b8">[9]</ref>, which is organized in a multilayer bidirectional transformer encoder. This architecture is based on an attention mechanism and allows the finding of dependencies between input and output <ref type="bibr" coords="3,89.29,276.66,16.36,10.91" target="#b9">[10]</ref>. Several variations of the original BERT model are trained in different scientific corpora, such as BioBERT <ref type="bibr" coords="3,167.98,290.20,17.96,10.91" target="#b10">[11]</ref> which was trained in PubMed and PMC articles and SciBERT <ref type="bibr" coords="3,465.04,290.20,16.29,10.91" target="#b11">[12]</ref>, that was trained in Semantic Scholar articles. After the pre-training, these variations and the original BERT model can also be fine-tuned for NEL tasks <ref type="bibr" coords="3,307.57,317.30,17.14,10.91" target="#b12">[13]</ref>.</p><p>In addition to the pre-trained language models, NEL state-of-the-art approaches in the biomedical domain also include graph-based models. Usually, these build a disambiguation graph composed by candidates for entity mentions and then ranked according to their relevance and coherence in the graph. Models that use the Personalized PageRank algorithm to determine the relevance of the candidates in the graph have been proposed, such as Pershina et al. <ref type="bibr" coords="3,481.82,385.05,16.25,10.91" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.2.">Semantic similarity</head><p>The calculation of the relevance of the candidates in a graph normally requires a similarity measure to compare its nodes, as was proposed by Lamurias et al. <ref type="bibr" coords="3,386.56,447.93,16.27,10.91" target="#b14">[15]</ref>. A semantic similarity measure is a metric to compare the similarity between sets of text based on their implicit and explicit semantics. In the present work, we measured the semantic similarity between each entity and the remaining entities of a given document through Resnik's metric <ref type="bibr" coords="3,432.98,488.57,16.08,10.91" target="#b15">[16]</ref>. This metric is based on the extrinsic information content (IC) of the most informative common ancestor (MICA) of two given concepts <ref type="bibr" coords="3,225.40,515.67,17.91,10.91" target="#b16">[17]</ref> and is defined as:</p><formula xml:id="formula_0" coords="3,212.49,543.50,170.30,10.77">ùëÜùëÜùëÄ ùëüùëíùë†ùëõùëñùëò (ùëí 1 , ùëí 2 ) = ùêºùê∂ ùë†‚Ñéùëéùëüùëíùëë (ùëí 1 , ùëí 2 )</formula><p>Being ùëí 1 and ùëí 2 the entity 1 and the entity 2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.3.">Extreme multi-label classification and biomedical semantic indexing</head><p>Chang et al. <ref type="bibr" coords="3,145.68,612.12,17.91,10.91" target="#b17">[18]</ref> divided the approaches to the XMC task in four categories: one-vs-all, partitioning methods, embedding-based, and deep-learning-based.</p><p>The Parabel algorithm <ref type="bibr" coords="3,199.20,639.22,17.76,10.91" target="#b18">[19]</ref> follows a one-vs-all approach because it learns a separate classifier for each label in the label list. It also applies a tree-based method, since it learns a balanced hierarchy over labels, which helps identifying the most similar labels with respect to a given The current state-of-the-art in XMC consists of approaches that leverage pre-trained deep language models. The first approach of this type was X-BERT (BERT for eXtreme Multi-label Text Classification) <ref type="bibr" coords="4,174.30,334.51,16.11,10.91" target="#b17">[18]</ref>, later renamed to X-Transformer <ref type="bibr" coords="4,339.95,334.51,11.30,10.91" target="#b5">[6]</ref>, which fine-tunes BERT, RoBERTa, and XLNet for the XMC task. The main challenges of applying Transformer to the XMC problem are the extremely large set of possible labels and the label sparsity, which arises from the fact that too few labels are associated with a large number of training instances. The model includes three components: a semantic label indexer, a deep neural matcher, and a ranker. The authors applied the developed algorithm to four datasets, Eurlex-4K, Wiki10-28K, AmazonCat-13K and Wiki-500K, obtaining the following precision@1 values: 86.00%, 85.75 %, 95.17 %, 67.87 %.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.4.">MESINESP1</head><p>With respect to the MESINESP task, six teams have participated in the first edition, including our team, which have developed a pipeline <ref type="bibr" coords="4,283.38,478.69,12.87,10.91" target="#b4">[5]</ref> based on the X-Transformer algorithm <ref type="bibr" coords="4,473.89,478.69,12.87,10.91" target="#b5">[6]</ref> and the MER tool <ref type="bibr" coords="4,151.27,492.24,18.00,10.91" target="#b19">[20]</ref> for the named entity recognition and linking step. The approach with best performance was based on AttentionXML with multilingual-BERT <ref type="bibr" coords="4,380.67,505.78,16.09,10.91" target="#b20">[21]</ref>, which achieved a micro F-measure value of 0.4254, whereas our approach achieved a micro F1-score of 0.2507.</p><p>Besides DeCS and MeSH vocabularies, there are also related works that focus on the classification or coding of clinical content with codes belonging to other vocabularies, in particular the International Classification of Diseases (ICD) terminology <ref type="bibr" coords="4,366.77,559.98,16.43,10.91" target="#b21">[22,</ref><ref type="bibr" coords="4,385.93,559.98,12.55,10.91" target="#b22">23,</ref><ref type="bibr" coords="4,401.20,559.98,12.55,10.91" target="#b23">24,</ref><ref type="bibr" coords="4,416.48,559.98,12.32,10.91" target="#b24">25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Data description</head><p>The target label list consisted of 34,046 codes belonging to the DeCS vocabulary<ref type="foot" coords="5,438.30,130.49,3.71,7.97" target="#foot_0">1</ref> (2020 edition), complemented with additional COVID-related descriptors added by the organisation. Both corpora (JSON files) and the DeCS vocabulary (TSV file) were provided by the organisation and downloaded from the following link: https://zenodo.org/record/4634129#.YHcShxIo9an.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Entity Linking</head><p>Our approach consisted in using the recognised entities from the documents of each subtrack that were provided in the folder "Additional Data". The entities of these files were then further linked to the respective DeCS codes through an entity linking model. This model searches for the ten best candidates of DeCS through string matching and then develops a disambiguation graph with those candidates. The Personalized PageRank algorithm is applied to the disambiguation graph and estimates the coherence of each node, i.e. candidate, to the graph. The coherence is associated with the node degree, meaning that nodes linked to a high number of other candidate nodes are probable candidates for their respective entities compared with more isolated nodes. Besides coherence, the IC of the DeCS code associated with the nodes is used for ranking: nodes associated with DeCS codes with higher IC receive higher ranking scores. IC corresponds to the presence of an entity in a corpus, if an entity is not common in a corpus its IC will be high. The higher the IC of a candidate is, the better ranking that candidate will have in the graphic. After ranking all the candidates, the PPR selects the candidate with better ranking to map each entity. At the end, all entities in a given document are linked to their respective DeCS concepts.</p><p>To explore the guiding hypothesis of this work, we filtered the number of entities to include each document by applying a semantic similarity-based filter, more concretely, by selecting the entities for which there were other similar entities recognised in the same document.</p><p>After this step, the average of the several semantic similarity values obtained for an entity corresponded to the final score of that entity. The entities were then sorted by their score. At the end, we explored two values for the semantic similarity-based filter: 1.0 and 0.25. Considering the filter 0.25, we only included the top 25% entities according to their score, and for the filter 1.0, we included all the entities in the document. This way, we could determine the impact of choosing the most relevant entities in the performance of the classifier algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Extreme Multi-Label Classification</head><p>We approached the sub-tracks as an Extreme Multi-Label Classification (XMC) problem. Our starting point was a pipeline based on the X-Transformer algorithm <ref type="bibr" coords="5,388.10,583.98,12.70,10.91" target="#b5">[6]</ref> that was adapted to the biomedical domain by our group in the context of past competitions, such as BioASQ <ref type="bibr" coords="5,473.78,597.53,12.91,10.91" target="#b4">[5]</ref> and CANTEMIST <ref type="bibr" coords="5,151.65,611.08,16.28,10.91" target="#b25">[26]</ref>. The pipeline was further adapted to the present competition, and includes the following modules: entity linking (subsection 2.2), preprocessing, semantic label indexer, deep neural matcher, and ranker. The main modifications were made in the entity linking and preprocessing modules. The complete description of the entity linking component is available in the previous subsection 2.2.</p><p>The preprocessing module imports the retrieved dataset JSON files (train, dev, and text subsets) and the DeCS TSV file, the JSON files with the output from the entity linking (subsection 2.2), and, for each dataset, it generates several files: 1. vocabulary file ("label_vocab.txt"): it includes the internal numerical identifier for each DeCS term. For example, the term "calcimicina" has the internal numerical identifier "0". 2. label correspondence file ("label_correspondence.txt"): it includes the correspondence between the internal numerical identifiers, and the respective DeCS labels and terms. For example, "0" corresponds to "D000001", which corresponds to "calcimicina". 3. subset files ("subset.txt", "subset_raw_text.txt", "subset_raw_labels.txt"): for each subset (train, dev, and test) it is generated the three aforementioned files. The file "subset.txt" includes the DeCS labels that are associated with the respective documents, separated by commas, the stemmed texts of documents' titles, and the DeCS terms that were extracted in the documents appended to the end of the stemmed titles. The file "subset_raw_text.txt" includes only the stemmed titles, and the file "subset_raw_labels.txt" only the DeCS terms relative to the labels associated with the documents.</p><p>We only considered the titles of the documents based on the results described by Neves et al. <ref type="bibr" coords="6,89.29,351.53,11.53,10.91" target="#b4">[5]</ref>: the performance of the models using titles is similar to that of models using abstracts, so it is more efficient to use titles since they have less text. The limited time that we had to train models also influenced our decision to only use the titles, since the required time is lower. The titles were stemmed using the Snowball Stemmer implementation for Spanish text provided by the NLTK package <ref type="foot" coords="6,185.39,403.98,3.71,7.97" target="#foot_1">2</ref> . As the documents belonging to the test sets were unlabeled, we added the placeholder "0" to each document in the "subset.txt" files. The module was also modified in order to integrate extracted entities independently of the tool employed.</p><p>The X-Transformer algorithm includes three modules: semantic label indexer, deep neural matcher, and ranker. The semantic label indexer first obtain meaningful representations for labels that are based on embeddings of the text descriptions associated with the labels, and on Positive Instance Feature Aggregation (PIFA), which is a type of label embeddings based on the TF-IDF features that are relevant instances for the labels. Then, it applies k-means clustering in order to generate label clusters according to the semantic representations described before. The deep neural matcher performs fine-tuning of BERT to encode an instance embedding, which is then used to find the most relevant clusters for the instance. At the end of this step, only a small subset of clusters are considered for the next step, which is performed by the ranker. The ranker determines the relevance of the labels in the chosen clusters to the instance, which is substantially more efficient than performing the ranking of all the initial labels. For a more complete description of the X-Transformer algorithm please refer to the original publication by Chang et al. <ref type="bibr" coords="6,145.33,608.97,11.43,10.91" target="#b5">[6]</ref>.</p><p>The models developed for the different sub-tracks are shown in Table <ref type="table" coords="6,403.84,622.52,3.66,10.91">2</ref>. We explored the finetuning of different deep neural matchers. The BERT Base Multilingual Cased model was trained on the Wikipedia dumps of the top 104 largest languages in Wikipedia and has the following</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Models used for the three sub-tracks, with the respective target datasets, thresholds (top entities to consider according to their relevance), and deep neural matcher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Target dataset Threshold Deep neural matcher</p><formula xml:id="formula_1" coords="7,95.27,151.49,343.72,45.13">LASIGE_BioTM-1 L 1.0 CANTEMIST LASIGE_BioTM-2 0.25 LASIGE_BioTM-3 T 1.0 BERT Multilingual Base Cased LASIGE_BioTM-4</formula><p>0.25 LASIGE_BioTM-5 P 1.0 BERT Multilingual Base Cased characteristics: 12-layer, 768-hidden, 12-heads, 110M parameters. The X-Transformer algorithm uses the Pytorch implementation from HuggingFace Transformers <ref type="bibr" coords="7,398.70,249.23,16.41,10.91" target="#b26">[27]</ref>. The CANTEMIST model corresponds to the Model 7 described by Ruas et al. <ref type="bibr" coords="7,345.01,262.78,16.21,10.91" target="#b25">[26]</ref>. It is also based on the the BERT Base Multilingual Cased model and was first fine-tuned on 318,658 Spanish biomedical articles from the IBECS, LILACS and PubMed databases, jointly with extracted entities in the context of the participation in the first edition of MESINESP <ref type="bibr" coords="7,312.21,303.43,11.43,10.91" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Training approach</head><p>We explored several training approaches according to the target corpus: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results and discussion</head><p>The results obtained for each sub-track are shown on Table <ref type="table" coords="7,353.97,628.09,3.71,10.91">3</ref>. The official evaluation metric of the competition was the micro F1-score (MiF). Our best models achieved a MiF of 0.2007, 0.0686, and 0.0314 in the sub-tracks L, T, and P, respectively. These results are low when compared to Table <ref type="table" coords="8,116.06,90.49,5.12,8.93">3</ref> Results on test sets for the three sub-tracks. Performance for the baseline models, the best models, and our models are shown according to the metrics: MiF-micro F1-score, MiP-micro precision, MiR-micro recall, MaF-macro F1-score, MaP-macro precision, MaR-macro recall. With respect to the initial hypothesis, the obtained results were mixed. In the sub-track L, the LASIGE_BioTM-1 model, which included all the entities recognised in the documents, obtained slightly better results (0.2007 MiF) compared with LASIGE_BioTM-2 model (0.1886 MiF), which only included 25% of the top relevant entities. However, in the sub-track T, the opposite happened, since LASIGE_BioTM-4 (top 25% entities) obtained marginally better results (0.0686 MiF) than LASIGE_BioTM-3 (0.0679 MiF). Consequently, we cannot confirm the initial hypothesis that feeding only the most relevant entities to the classifier algorithm improves its performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sub</head><p>Assuming that there were no coding errors that may have undermined the results, there are several possible reasons behind the relatively low performance that our models achieved in the three sub-tracks.</p><p>Arguably, the main one is related with the impossibility of carrying out an optimisation of the hyper-parameters of the classifier algorithm, in particular the number of training epochs. Each model was only trained or fine-tuned during one epoch in the respective training dataset, which is not enough to accurately learn relevant features. The limited time we had available made it impossible to extend the training process during more epochs. Additionally, we were not able to train the models in a multi-gpu setting due to unresolved errors, so the duration of each training epoch was approximately two days using a single gpu. Beyond the number of training epochs, the optimization of other hyperparameters such as train_batch_size, eval_batch_size, and learning_rate, would probably lead to a better performance.</p><p>With respect to the sub-track 2 and sub-track 3, the developed models were trained on documents belonging to the L corpus (sub-track 1), and not on documents of the respective subtracks corpora. The text present in scientific literature has different characteristics compared with the text associated with clinical trials and patents, so the models fine-tuned in a certain type of text will necessarily have a worse performance when their evaluation occurs over a different type of text. For sub-track 3, there was no training dataset available, but for sub-track 2 probably it would have been better if we had trained models 3 and 4 over the training dataset of the task and not over the training dataset for sub-track 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>Our approach including an entity linking model and the X-Transformer algorithm obtained a micro F1-score of 0.2007, 0.0686, and 0.0314 in sub-tracks 1, 2, and 3, respectively, which is a low performance compared with the top participants, and even with the baseline approaches.</p><p>In order to improve the performance, we need to perform a careful error-analysis to identify any coding errors that may have undermined the results. Next, we need to spend more time in the training process, more concretely, by training the models during more epochs, to perform hyper-parameter optimisation, to solve the problems associated with multi-gpu training, to explore the use of summarisation tools to feed only the relevant content to the classifier, and to explore less resource-demanding pre-trained models, such as DistilBERT. Besides, we only used the titles of the articles based on previous studies, but in the future we will explore the impact of using more text in the performance of the classification algorithm.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,107.28,378.66,398.70,10.91;7,116.56,392.21,303.94,10.91;7,107.28,407.11,398.70,10.91;7,116.56,420.66,389.42,10.91;7,116.56,434.21,389.43,10.91;7,116.56,447.76,285.12,10.91;7,107.28,462.67,398.70,10.91;7,116.56,476.22,389.42,10.91;7,116.31,489.76,333.96,10.91;7,100.20,515.27,407.46,10.91;7,89.29,528.82,416.69,10.91;7,89.29,542.37,416.95,10.91;7,89.29,555.92,417.89,10.91;7,89.29,571.91,312.56,7.90"><head>‚Ä¢</head><label></label><figDesc>L corpus: Fine-tuning of the model CANTEMIST using the provided training dataset of 249,474 documents and the provided test set with 10,179 documents. ‚Ä¢ T corpus: Training of the model BERT Multilingual Base Cased using the provided training dataset of 249,474 documents from the L corpus and a generated test set built from the 3560 clinical trials of the training set, the 147 clinical trials of the development set, and the 8919 clinical trials of the test set (total of 12,627 documents). ‚Ä¢ P corpus: Training of the model BERT Multilingual Base Cased using the provided training dataset of 249,474 documents from the L corpus and a generated test set built from the 115 patents of the development set and the 68,404 patents from the test set.The training of the deep neural matcher is the limiting step of the algorithm in terms of time. Each model was trained during a single epoch then evaluated on the respective test set. The training and evaluation time was approx. 2 days for each model using a single NVIDIA Tesla P4 GPU. The values for the hyper-parameters are the following: depth = 6, train_batch_size=4, eval_batch_size=4, learning_rate=0.00005, warmup_rate=0.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.89,90.49,418.94,214.29"><head>Table 1</head><label>1</label><figDesc>Number of documents in each corpus. label, i.e. those that are present in the same leaves. It performs sub-sampling of data points by restricting a given label's negative training examples to those examples that are annotated with similar or confusing labels, which decreases training and prediction times from linear to logarithmic. The approach then applies a hierarchical multi-label model, which is a generalisation of the multi-class hierarchical softmax model. Each classifier learns a joint probability distribution over the possible labels that is based on data point features and on the label hierarchy. Parabel was applied to Dynamic Search Advertising, which aims to predict the subset of search engine queries that will lead to a click on a given ad page.</figDesc><table coords="4,155.03,122.05,285.21,50.26"><row><cell>Corpus</cell><cell>Train</cell><cell>Dev</cell><cell>Test</cell></row><row><cell cols="4">Scientific literature (L) 249,474 1,065 10,179 (500 gold standard)</cell></row><row><cell>Clinical Trials (T)</cell><cell>3,560</cell><cell>147</cell><cell>8,919 (250 gold standard)</cell></row><row><cell>Patents (P)</cell><cell>-</cell><cell>115</cell><cell>68,404 (150 gold standard)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,108.93,671.02,112.84,8.97"><p>http://red.bvsalud.org/decs/en/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,108.93,671.04,80.21,8.97"><p>https://www.nltk.org/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported by <rs type="funder">FCT</rs> through funding of <rs type="funder">Deep Semantic Tagger (DeST)</rs> project (ref. <rs type="grantNumber">PTDC/CCI-BIO/28685/2017</rs>) and <rs type="funder">LASIGE Research Unit</rs> (ref. <rs type="grantNumber">UIDB/00408/2020</rs> and ref. <rs type="grantNumber">UIDP/00408/2020</rs>); and <rs type="funder">FCT</rs> through funding of <rs type="grantName">PhD Scholarship</rs>, ref. <rs type="grantNumber">2020.05393</rs>.BD.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_YmndK38">
					<idno type="grant-number">PTDC/CCI-BIO/28685/2017</idno>
				</org>
				<org type="funding" xml:id="_7QznR9f">
					<idno type="grant-number">UIDB/00408/2020</idno>
				</org>
				<org type="funding" xml:id="_wBCcjy6">
					<idno type="grant-number">UIDP/00408/2020</idno>
				</org>
				<org type="funding" xml:id="_AKnrrv2">
					<idno type="grant-number">2020.05393</idno>
					<orgName type="grant-name">PhD Scholarship</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,466.08,394.53,10.91;9,112.66,479.63,395.17,10.91;9,112.66,493.18,220.84,10.91" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="9,112.66,479.63,94.98,10.91;9,237.47,479.63,270.36,10.91;9,112.66,493.18,183.78,10.91">The ninth BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Katsimpras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Vandorou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gasco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">.</forename><surname>Paliouras</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note>Overview of BioASQ</note>
</biblStruct>

<biblStruct coords="9,112.66,506.73,394.53,10.91;9,112.66,520.28,395.00,10.91;9,112.66,533.83,393.33,10.91;9,112.66,547.38,110.68,10.91" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gasco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Estrada-Zavala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R.-T</forename><surname>Murasaki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Primo-Pe√±a</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bojo-Canales</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<title level="m" coord="9,312.32,520.28,195.34,10.91;9,112.66,533.83,355.17,10.91">Overview of BioASQ 2021-MESINESP track. Evaluation of advance hierarchical classification techniques for scientific literature</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>patents and clinical trials</note>
</biblStruct>

<biblStruct coords="9,112.66,560.93,393.32,10.91;9,112.66,574.48,393.33,10.91;9,112.66,588.02,395.01,10.91;9,112.66,601.57,193.04,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,247.56,560.93,258.42,10.91;9,112.66,574.48,65.25,10.91">Hierarchical text classification for news articles based-on named entities</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-35527-1_27</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,412.13,588.02,24.28,10.91">LNAI</title>
		<imprint>
			<biblScope unit="volume">7713</biblScope>
			<biblScope unit="page" from="318" to="329" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,615.12,393.33,10.91;9,112.66,628.67,393.66,10.91;9,112.66,642.22,48.11,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,349.57,615.12,156.42,10.91;9,112.66,628.67,32.48,10.91">Text Classification Based on Named Entities</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Andelic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kondic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Peric</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jocic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kovacevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,167.82,628.67,338.50,10.91">7th International Conference on Information Society and Technology ICIST</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,86.97,393.33,10.91;10,112.66,100.52,394.61,10.91;10,112.31,114.06,167.72,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,281.47,86.97,224.51,10.91;10,112.66,100.52,173.98,10.91">Extreme Multi-Label Classification applied to the Biomedical and Multilingual Panorama</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lamurias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M</forename><surname>Couto</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_67.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="10,310.73,100.52,116.86,10.91">CLEF 2020 Working Notes</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,127.61,393.33,10.91;10,112.66,141.16,395.00,10.91;10,112.66,154.71,308.03,10.91" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="10,356.44,127.61,149.55,10.91;10,112.66,141.16,191.73,10.91">Taming Pretrained Transformers for Extreme Multi-label Text Classification</title>
		<author>
			<persName coords=""><forename type="first">W.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dhillon</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394486.3403368</idno>
		<idno type="arXiv">arXiv:1905.02331v4</idno>
		<ptr target="https://doi.org/10.1145/3394486.3403368.doi:10.1145/3394486.3403368" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,168.26,394.53,10.91;10,112.66,181.81,275.44,10.91" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lamurias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Couto</surname></persName>
		</author>
		<idno type="DOI">10.1016/B978-0-12-809633-8.20409-3</idno>
		<title level="m" coord="10,214.14,168.26,263.48,10.91">Text Mining for Bioinformatics Using Biomedical Literature</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="602" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,195.36,393.33,10.91;10,112.66,208.91,321.86,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,232.03,195.36,273.95,10.91;10,112.66,208.91,131.89,10.91">Mer: a shell script and annotation server for minimal named entity recognition and linking</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M</forename><surname>Couto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lamurias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,252.73,208.91,124.12,10.91">Journal of Cheminformatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">58</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,222.46,393.33,10.91;10,112.66,236.01,363.59,10.91" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="10,353.43,222.46,152.55,10.91;10,112.66,236.01,181.08,10.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,249.56,395.17,10.91;10,112.66,263.11,394.53,10.91;10,112.66,276.66,90.72,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,148.82,263.11,106.21,10.91">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">≈Å</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,278.07,263.11,224.48,10.91">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,290.20,393.33,10.91;10,112.66,303.75,393.98,10.91;10,112.41,317.30,48.96,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,361.64,290.20,144.35,10.91;10,112.66,303.75,268.25,10.91">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,394.29,303.75,66.92,10.91">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,330.85,394.61,10.91;10,112.66,344.40,393.33,10.91;10,112.66,357.95,395.17,10.91;10,112.66,371.50,394.53,10.91;10,112.66,385.05,394.72,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,236.51,330.85,251.42,10.91">SciBERT: A pretrained language model for scientific text</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1371</idno>
		<ptr target="https://www.aclweb.org/anthology/D19-1371.doi:10.18653/v1/D19-1371" />
	</analytic>
	<monogr>
		<title level="m" coord="10,112.66,344.40,393.33,10.91;10,112.66,357.95,395.17,10.91;10,112.66,371.50,229.97,10.91">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3615" to="3620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,398.60,393.71,10.91;10,112.66,412.15,280.60,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,213.23,398.60,252.89,10.91">Bert-based ranking for biomedical entity normalization</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,478.26,398.60,28.11,10.91;10,112.66,412.15,207.72,10.91">AMIA Summits on Translational Science Proceedings</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page">269</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,425.70,395.17,10.91;10,112.66,439.25,393.53,10.91;10,112.66,452.79,395.17,10.91;10,112.66,466.34,394.62,10.91;10,112.66,479.89,338.66,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,280.20,425.70,227.63,10.91;10,112.66,439.25,41.11,10.91">Personalized page rank for named entity disambiguation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pershina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Grishman</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/N15-1026</idno>
		<ptr target="https://www.aclweb.org/anthology/N15-1026.doi:10.3115/v1/N15-1026" />
	</analytic>
	<monogr>
		<title level="m" coord="10,185.54,439.25,320.64,10.91;10,112.66,452.79,395.17,10.91;10,112.66,466.34,183.29,10.91">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="238" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,493.44,395.16,10.91;10,112.66,506.99,394.51,10.91;10,112.66,522.98,104.78,7.90" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,269.67,493.44,238.16,10.91;10,112.66,506.99,152.84,10.91">PPR-SSM: Personalized PageRank and semantic similarity measures for entity linking</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lamurias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ruas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M</forename><surname>Couto</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12859-019-3157-y</idno>
	</analytic>
	<monogr>
		<title level="j" coord="10,275.38,506.99,93.12,10.91">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,534.09,393.60,10.91;10,112.66,547.64,141.20,10.91" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="10,158.70,534.09,314.56,10.91">Using information content to evaluate semantic similarity in a taxonomy</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Resnik</surname></persName>
		</author>
		<idno>arXiv preprint cmp-lg/9511007</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,561.19,393.33,10.91;10,112.66,574.74,140.08,10.91" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="10,214.23,561.19,291.76,10.91;10,112.66,574.74,100.36,10.91">Semantic similarity definition, Encyclopedia of bioinformatics and computational biology</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Couto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lamurias</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,588.29,393.32,10.91;10,112.66,601.84,393.98,10.91;10,112.41,615.39,290.89,10.91" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="10,351.13,588.29,154.85,10.91;10,112.66,601.84,364.89,10.91">X-BERT: eXtreme Multi-label Text Classification with using Bidirectional Encoder Representations from Transformers</title>
		<author>
			<persName coords=""><forename type="first">W.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dhillon</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1905.02331" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,628.93,393.54,10.91;10,112.66,642.48,393.33,10.91;10,112.66,656.03,394.53,10.91;10,112.66,669.58,331.88,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,352.46,628.93,153.74,10.91;10,112.66,642.48,305.05,10.91">Parabel: Partitioned label trees for extreme classification with application to dynamic search advertising</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harsola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<idno type="DOI">10.1145/3178876.3185998</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,440.52,642.48,65.46,10.91;10,112.66,656.03,209.18,10.91">Proceedings of the World Wide Web Conference, WWW 2018</title>
		<meeting>the World Wide Web Conference, WWW 2018<address><addrLine>New York, NY, USA; Lyon, France</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">April 23-27, 2018. 2018</date>
			<biblScope unit="page" from="993" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,86.97,393.33,10.91;11,112.66,100.52,394.62,10.91;11,112.66,114.06,394.51,10.91;11,112.66,130.06,104.78,7.90" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="11,247.25,86.97,258.73,10.91;11,112.66,100.52,170.10,10.91">MER: a shell script and annotation server for minimal named entity recognition and linking</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M</forename><surname>Couto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lamurias</surname></persName>
		</author>
		<idno type="DOI">10.1186/s13321-018-0312-9</idno>
		<ptr target="https://jcheminf.biomedcentral.com/articles/10.1186/s13321-018-0312-9.doi:10.1186/s13321-018-0312-9" />
	</analytic>
	<monogr>
		<title level="j" coord="11,293.38,100.52,127.71,10.91">Journal of Cheminformatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">58</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,141.16,395.17,10.91;11,112.66,154.71,393.33,10.91;11,112.66,168.26,393.32,10.91;11,112.66,181.81,284.74,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="11,383.29,141.16,124.55,10.91;11,112.66,154.71,393.33,10.91;11,112.66,168.26,58.72,10.91">AttentionXML: Label Treebased Attention-Aware Deep Model for High-Performance Extreme Multi-Label Text Classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mamitsuka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01727</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,194.25,168.26,311.74,10.91;11,112.66,181.81,21.62,10.91">33rd Conference on Neural Information Processing Systems (NeurIPS 2019)</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,195.36,394.61,10.91;11,112.66,208.91,393.32,10.91;11,112.33,222.46,350.87,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="11,270.92,195.36,216.37,10.91">A Neural Architecture for Automated ICD Coding</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,112.66,208.91,393.32,10.91;11,112.33,222.46,59.64,10.91">Proceedings ofthe 56th Annual Meeting ofthe Association for Computational Linguistics (Long Papers)</title>
		<meeting>the 56th Annual Meeting ofthe Association for Computational Linguistics (Long Papers)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1066" to="1076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,236.01,393.33,10.91;11,112.66,249.56,260.28,10.91" xml:id="b22">
	<monogr>
		<title level="m" type="main" coord="11,304.27,236.01,201.71,10.91;11,112.66,249.56,37.61,10.91">Towards Automated ICD Coding Using Deep Learning</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04075v3</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="11,112.66,263.11,393.33,10.91;11,112.66,276.66,393.33,10.91;11,112.66,290.20,371.45,10.91" xml:id="b23">
	<monogr>
		<title level="m" type="main" coord="11,329.63,263.11,176.36,10.91;11,112.66,276.66,186.71,10.91">Exploit Multilingual Language Model at Scale for ICD-10 Clinical Text Classification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Silvestri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Gargiulo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ciampi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">De</forename><surname>Pietro</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCC50000.2020.9219640</idno>
		<imprint>
			<date type="published" when="2020-07">2020-July (2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,303.75,395.17,10.91;11,112.66,317.30,394.61,10.91;11,112.66,330.85,239.89,10.91" xml:id="b24">
	<monogr>
		<title level="m" type="main" coord="11,281.40,303.75,226.43,10.91;11,112.66,317.30,338.54,10.91">From Extreme Multi-label to Multi-class: A Hierarchical Approach for Automated ICD-10 Coding Using Phrase-level Attention</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tahmasebi</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="2102.09136" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,344.40,393.61,10.91;11,112.66,357.95,393.33,10.91;11,112.66,371.50,393.32,10.91;11,112.66,385.05,393.86,10.91;11,112.66,398.60,55.83,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="11,324.78,344.40,181.49,10.91;11,112.66,357.95,393.33,10.91;11,112.66,371.50,149.88,10.91">Lasigebiotm at cantemist: Named entity recognition and normalization of tumour morphology entities and clinical coding of Spanish health-related documents</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ruas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">D</forename><surname>Andrade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M</forename><surname>Couto</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2664/cantemist_paper11.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="11,286.36,371.50,219.62,10.91;11,112.66,385.05,96.44,10.91">Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2020)</title>
		<meeting>the Iberian Languages Evaluation Forum (IberLEF 2020)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="422" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,412.15,394.53,10.91;11,112.66,425.70,394.53,10.91;11,112.66,439.25,395.17,10.91;11,112.66,452.79,393.33,10.91;11,112.66,466.34,394.53,10.91;11,112.66,479.89,385.60,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="11,315.63,439.25,192.20,10.91;11,112.66,452.79,72.82,10.91">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.emnlp-demos.6" />
	</analytic>
	<monogr>
		<title level="m" coord="11,207.25,452.79,298.74,10.91;11,112.66,466.34,390.37,10.91">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
