<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,375.42,15.42;1,89.29,106.66,309.33,15.42">Comparing Transformer-based NER approaches for analysing textual medical diagnoses</title>
				<funder ref="#_BXbdrmD">
					<orgName type="full">Apulia Region, Italy</orgName>
				</funder>
				<funder ref="#_b2XhkD2">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,81.18,11.96"><forename type="first">Marco</forename><surname>Polignano</surname></persName>
							<email>marco.polignano@uniba.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bari Aldo Moro</orgName>
								<address>
									<addrLine>Via E. Orabona 4</addrLine>
									<postCode>70125</postCode>
									<settlement>Bari</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,181.69,134.97,88.68,11.96"><forename type="first">Marco</forename><surname>De Gemmis</surname></persName>
							<email>marco.degemmis@uniba.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bari Aldo Moro</orgName>
								<address>
									<addrLine>Via E. Orabona 4</addrLine>
									<postCode>70125</postCode>
									<settlement>Bari</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,297.83,134.97,94.18,11.96"><forename type="first">Giovanni</forename><surname>Semeraro</surname></persName>
							<email>giovanni.semeraro@uniba.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bari Aldo Moro</orgName>
								<address>
									<addrLine>Via E. Orabona 4</addrLine>
									<postCode>70125</postCode>
									<settlement>Bari</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,375.42,15.42;1,89.29,106.66,309.33,15.42">Comparing Transformer-based NER approaches for analysing textual medical diagnoses</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">CF65075523B9578C7B91B0FF06AB4E3B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The automated analysis of medical documents has grown in research interest in recent years as a consequence of the social relevance of the thematic and the difficulties often encountered with short and very specific documents. In particular, this fervent area of research has stimulated the development of several techniques of automatic document classification, question answering, and name entity recognition (NER). Nevertheless, many open issues must be addressed to obtain results that are satisfactory for a field in which the effectiveness of predictions is a fundamental factor in order not to make mistakes that could compromise people's lives. To this end, we focused on the name entity recognition task from medical documents and, in this work, we will discuss the results we obtained by our hybrid approach. In order to take advantage of the most relevant findings in the field of natural language processing, we decided to focus on deep neural network models. We compared several configurations of our model by varying the transformer architecture, such as BERT, RoBERTa and ELECTRA, until we obtained a configuration that we considered the best for our goals. The most promising model was used to participate in the SpRadIE task of the annual CLEF (Conference and Labs of the Evaluation Forum). The obtained results are encouraging and can be of reference for future studies on the topic.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Different definitions of digital health <ref type="bibr" coords="1,255.87,431.02,12.93,10.91" target="#b0">[1]</ref> are provided in the scientific literature, and many of them focus on the use of smart systems that enable the delivery of medical and health services directly to the patients. Those systems are grounded on the use of innovative technologies that not only allow providing innovative functionalities to users such as recommendations, monitoring services, and document archiving, but they also collect an enormous amount of data that needs to be automatically processed to be correctly dispatched to physicians, specialist doctors or the national health network.</p><p>Generally speaking, eHealth approaches could be grouped into two main macro-categories by considering their domain of application:</p><p>• Wellness: all those applications for fitness and to support a correct lifestyle; • Disease &amp; Treatment Management: all those systems for the management of a pathology, supporting all the phases from diagnosis to treatment and monitoring of the same.</p><p>Both these two main kind of eHealth systems,have recently gained a large amount of new young consumers. eHealth systems and applications are quickly increasing, and the market of eHealth related systems and mobile application is expected to constantly grow of the 48.44% each year for the next four years <ref type="foot" coords="2,241.32,125.86,3.71,7.97" target="#foot_0">1</ref> . The amount of user of eHalth platforms, which today is around 825 million worldwide, will cross the billion mark in the next four years. Unfortunately, despite the increasing amount of data in healthcare available, the percentage of those annotated and usable for supervised machine learning systems is still very small. As already stated in <ref type="bibr" coords="2,493.20,168.26,12.78,10.91" target="#b1">[2]</ref> this behavior is more frequent to observe in the medical domain than in others due to some critical limits about the ownership of sensitive data and the deep knowledge need for correctly annotating them. Only a few years ago, in Europe, the regulation regarding the use of sensitive user data (GDPR) <ref type="bibr" coords="2,167.95,222.46,12.70,10.91" target="#b2">[3]</ref> was defined and some guidelines regarding how to properly manage such data in a uniform manner among the member states of the European Union have been provided. Nevertheless, often a simple anonymization of data is not enough and a careful procedure of requesting consents for research purposes is necessary. These limitations make it necessary for artificial intelligence techniques that can make good use of the little data available, inheriting, where possible, knowledge from other similar application domains. Fortunately, in recent years, transfer learning has become a common practice to address various tasks of natural language processing. This opportunity has allowed the growth of several applications <ref type="bibr" coords="2,430.84,317.30,12.85,10.91" target="#b3">[4]</ref> in the field of eHealth including name entity recognition <ref type="bibr" coords="2,282.69,330.85,11.39,10.91" target="#b4">[5,</ref><ref type="bibr" coords="2,296.81,330.85,7.48,10.91" target="#b5">6,</ref><ref type="bibr" coords="2,307.03,330.85,7.59,10.91" target="#b6">7]</ref>, the main topic of interest of this work. In particular, we decided to focus on NER's task regarding medical diagnoses in text format due to the substantial amount of available data provided for the SpRadIE 2021 <ref type="bibr" coords="2,425.26,357.95,11.48,10.91" target="#b7">[8,</ref><ref type="bibr" coords="2,439.50,357.95,9.03,10.91" target="#b8">9]</ref> competition co-located with the Conference and Labs of the Evaluation Forum 2021. In detail the main contribution of this work is about:</p><p>• the definition of an hybrid NER model for analysing textual medical diagnoses, based on a deep learning transformer based models <ref type="bibr" coords="2,305.92,420.85,17.91,10.91" target="#b9">[10]</ref> and conditional random field (CRF) <ref type="bibr" coords="2,485.97,420.85,16.39,10.91" target="#b10">[11]</ref>; • the comparison of different model configurations in order to detect the better performing transformer based model among BERT, BERT-mul, BETO, RoBERTa, XLM-RoBERTa, ELECTRA, ELECTRA-ES, ELECTRA-MED, BioBert, BioClinBERT.</p><p>Following we will present some relevant work useful for better understanding the technologies used in the model described in Chapter 3. In Chapter 4, we will discuss the configurations performed and the results obtained, until we present the one that performed best on the data at hand. We will conclude with our considerations and directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In recent years, natural language processing has gained a lot of attention, quickly becoming one of the most trending research areas. In particular, technologies based on deep learning have made it possible to learn models on a vast amount of data that are highly efficient and versatile in many scenarios of text analysis and classification. The most famous of such transfer learning approaches is BERT <ref type="bibr" coords="2,216.80,638.50,16.09,10.91" target="#b11">[12]</ref>, a transformer model that uses a bidirectional encoding model to generate a text representation that takes into account the context of the use of each term. Its applicability has been demonstrated in several works by adopting it for facing tasks as sentiment analysis <ref type="bibr" coords="3,128.84,114.06,16.55,10.91" target="#b12">[13,</ref><ref type="bibr" coords="3,148.37,114.06,12.59,10.91" target="#b13">14,</ref><ref type="bibr" coords="3,163.95,114.06,12.42,10.91" target="#b14">15]</ref>, question-answering, name-entity recognition, text summarization, and many more <ref type="bibr" coords="3,141.22,127.61,16.08,10.91" target="#b15">[16]</ref>. The use of BERT is not only limited to the English language. Different versions of it, trained on different sets of data, have been presented to make it suitable in scenarios where it needs to work correctly with multilingual text (BERT multilang). Still, it has also been trained on many single-language data in order to correctly work on specific languages different than English, such as Italian (AlBERTo) <ref type="bibr" coords="3,264.49,181.81,17.79,10.91" target="#b16">[17]</ref> and Spanish (BETO) <ref type="bibr" coords="3,376.32,181.81,16.13,10.91" target="#b17">[18]</ref>. Different specializations of BERT have been proposed for a specific domain of use, including the medical one. Indeed, BioBERT <ref type="bibr" coords="3,132.82,208.91,18.02,10.91" target="#b18">[19]</ref> has been trained on biomedical literature, making it suitable for many tasks of text analysis in the biological domain. BioClinicalBERT <ref type="bibr" coords="3,338.78,222.46,17.93,10.91" target="#b19">[20]</ref> starting from BioBert specialized the vocabulary on clinical terms. Taking inspiration from the BERT architecture, other models based on transformer have been realized. Among these, we would like to cite RoBERTa <ref type="bibr" coords="3,486.67,249.56,16.41,10.91" target="#b20">[21]</ref>, a model that evolves BERT with the aim of making the learning phase less demanding from the computational point of view while maintaining performances comparable to those of the original approach. In addition, to ensure its easy use in contexts where more than one language is used in the text, the XLM-RoBERTa <ref type="bibr" coords="3,257.57,303.75,17.78,10.91" target="#b21">[22]</ref> version was created. Similar to RoBERTa, ELECTRA <ref type="bibr" coords="3,89.29,317.30,17.87,10.91" target="#b22">[23]</ref> follows the BERT architecture by replacing its training model. Instead of using a masking approach, it uses another neural network that attempts to trick the model during the training phase by replacing random tokens with fake tokens. This strategy allows ELECTRA to train a more effective final language understanding model.</p><p>The idea of using BERT as the basis for a name entity recognition task in the medical domain is not entirely new. The name entity recognition (NER) task consists of identifying n-grams of a textual content that refers to entities or names relevant for the domain of application. It is common to find in literature NER systems able to detect in text locations, organizations, companies, person names but having enough training data it is possible to train those systems on every entity of interest. The task is a subproblem of information extraction and involves processing structured and unstructured documents <ref type="bibr" coords="3,312.50,466.34,16.08,10.91" target="#b23">[24]</ref>. Since it does not require the annotation of strictly personal and sensitive data, but generic texts are sufficient, this task is widely studied also in the literature even in the medical domain of application. Mao et al. <ref type="bibr" coords="3,422.85,493.44,17.96,10.91" target="#b24">[25]</ref> use BERT and the Conditional Random Field (CRF) models to identify the name of hospitals, medical staff, and territory in medical records. Similarly, Xue wt al. <ref type="bibr" coords="3,327.49,520.54,17.83,10.91" target="#b25">[26]</ref> fine-tuned the BERT model in order to detect entities and relations in Chinese medical documents. Vunikili et al. <ref type="bibr" coords="3,445.31,534.09,18.07,10.91" target="#b26">[27]</ref> explored the use of Bidirectional Encoder Representations from Transformers (BERT) based contextual embeddings trained on general domain Spanish text to extract tumor morphology from clinical reports written in Spanish. In a similar way, its not difficult to find attempts that use the RoBERTa model <ref type="bibr" coords="3,165.77,588.29,16.41,10.91" target="#b27">[28]</ref>, ELECTRA <ref type="bibr" coords="3,238.56,588.29,18.06,10.91" target="#b28">[29]</ref> or BioBERT <ref type="bibr" coords="3,316.23,588.29,16.41,10.91" target="#b29">[30]</ref>. All those work fall in not evaluating the efficacy of different transformer models before choosing one to use in their models. On the contrary, we focus on developing a hybrid model that merges an arbitrary transformer based architecture with a classic CRF layer. In this way, we will not limit ourselves to use with blinded eyes only a single transformer based model. Still, we will evaluate different ones in order to identify the one that best performing in the medical domain and, in particular, on documents written in Spanish as provided us by the organizers of the SpRadIe challenge <ref type="bibr" coords="3,433.03,669.58,11.36,10.91" target="#b7">[8,</ref><ref type="bibr" coords="3,447.11,669.58,7.57,10.91" target="#b8">9]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Transformer based NER Model</head><p>The hybrid model we propose is inspired by a common architecture largely used for dealing with a NER task. In particular, we are looking at the architecture proposed by Huang <ref type="bibr" coords="4,458.90,318.19,17.76,10.91" target="#b30">[31]</ref> that is composed by the concatenation of a Long Short Term Memory (LSTM) model and a Conditional Random Field layer (CRF) <ref type="bibr" coords="4,207.28,345.28,16.34,10.91" target="#b10">[11]</ref>. This approach has been demonstrated to be very effective for dealing with the identification of entities and relations into the text, and consequently, it has been our starting point. We decided to combine a transformer based model with a CRF layer, similarly to the approach discussed in <ref type="bibr" coords="4,260.35,385.93,16.25,10.91" target="#b31">[32]</ref>.</p><p>The architecture of our proposed hybrid model is showed in Fig. <ref type="figure" coords="4,387.80,399.48,3.71,10.91" target="#fig_0">1</ref>. The first step consists of pre-processing the text to make it suitable for the transformer based model. In particular, we split the text into sentences, and we added special tokens at the beginning and end of the sentence. The [CLS] token is used for indicating the beginning of the sentence, [SEP] for indicating its end, and [PAD] tokens are added before the [SEP] for making the input length uniform. For each token, the transformer based model requires input ids, a sequence of integers identifying each input token to its index number in the tokenizer vocabulary provided by the specific pre-trined model. Consequently, the tokens are transformed in their numerical counterpart in order to be properly encoded using token embeddings, sentence embeddings, and positional vectors <ref type="bibr" coords="4,487.29,507.87,16.30,10.91" target="#b32">[33]</ref>. The formatted input of length n is denoted as 𝑤 =&lt; 𝑤 1 , 𝑤 2 , .., 𝑤 𝑛 &gt;, and the corresponding tag sequence is denoted as 𝑡 =&lt; 𝑡 1 , 𝑡 2 , ..., 𝑡 𝑛 &gt;. The tag annotation schema used for the model is BIO. The schema allows to learn a NER model able to identify tokens in text that can represent one of the following tag: (i) the Beginning (B) of the entity; (ii) the word Inside (I) the entity; (iii) words Outside (O) entities <ref type="bibr" coords="4,225.40,575.62,16.25,10.91" target="#b33">[34]</ref>.</p><p>When the data is ready, it is given as input to the chosen transformer based model. Each one of them has its internal architecture, but all of them allow us to obtain from an embedding representation of each token present in the input sentence. Generally speaking, transformer is an architecture for transforming one sequence into another one by using two modules: the encoder and the decoder. Both encoder and decoder are composed of blocks that can be stacked on top of each other multiple times in order to obtain the desired depth of the network. Each block is mainly composed of a multi-head attention layer, followed by a normalization layer and a feed-forward layer. Using only the encoder module, we can obtain a word embedding representation of each token of the input after any possible arbitrary number of encoding blocks. In particular, BERT, in its basic version, is trained on a Transformer network made of only 12 encoding blocks, 768 dimensional states and 12 heads of attention for a total of 110M of parameters trained on BooksCorpus <ref type="bibr" coords="5,252.55,154.71,17.90,10.91" target="#b34">[35]</ref> and Wikipedia English for 1M of steps. The learning phase is performed by scanning the span of text in both directions, from left to right and from right to left, as was already done in Bidirectional LSTM. Moreover, BERT uses a "masked language model": during the training, random terms are masked in order to be predicted by the net. Jointly, the network is also designed to potentially learn the next span of text from the one given in input. These peculiarities allow BERT to be the current state of the art language understanding model.</p><p>By following the logical flow of our hybrid model, we used the embedding generated by the transformer based model to perform a token-level classification task. In particular, an hidden layer has been added on top of the stack of the model in order to obtain a prediction matrix P ∈ R 𝑛×𝑘 (i.e. one prediction vector for each on of the 𝑛 tokens) for the given input sequence. Formally speaking, the classification layer projects each token's encoded representation to the tag space R 𝐻 ↦ → R 𝑘 , where 𝑘 is the number of tags which varies in accordance with the number of classes and the tagging scheme. The CRF layer consequently learns only the transition probability of the output labels, 𝐴 ∈ R 𝐾+2×𝐾+2 where +2 indicates one tag each for start and end marker. The matrix 𝐴 is such that 𝐴 𝑖,𝑗 represents the score of transitioning from tag 𝑖 to tag 𝑗 <ref type="bibr" coords="5,134.16,371.50,16.41,10.91" target="#b35">[36]</ref>. For an input sequence 𝑥 =&lt; 𝑥 1 , ..., 𝑥 𝑛 &gt; and a sequence of tag predictions 𝑦 =&lt; 𝑦 1 , ..., 𝑦 𝑛 &gt;, 𝑦 𝑖 ∈ 1, ..., 𝑘, the score of the sequence is defined as <ref type="bibr" coords="5,404.53,385.05,16.39,10.91" target="#b10">[11]</ref>:</p><formula xml:id="formula_0" coords="5,222.45,406.85,283.53,33.71">𝑆(𝑥, 𝑦) = 𝑛 ∑︁ 𝑖=𝑜 𝐴 𝑦 𝑖 ,𝑦 𝑖+1 + 𝑛 ∑︁ 𝑖=𝑜 P 𝑖,𝑦 𝑖<label>(1)</label></formula><p>The objective function is the maximum likelihood of the probability distribution denoted as:</p><formula xml:id="formula_1" coords="5,199.59,469.76,306.40,37.32">𝑙𝑛( 𝑝(𝑦|𝑥) ) = 𝑆(𝑥, 𝑦) -𝑙𝑛 ⎛ ⎝ ∑︁ 𝑦 ′ ∈𝑦 𝑒 𝑆(𝑥,𝑦 ′ ) ⎞ ⎠<label>(2)</label></formula><p>We use 𝑦 ⋆ to represent the most likely tag sequence of x that will be considered as output of the model:</p><formula xml:id="formula_2" coords="5,244.45,545.81,261.54,21.02">𝑦 ⋆ = arg max 𝑦 ′ ∈𝑦 (𝑆(𝑥, 𝑦 ′ ))<label>(3)</label></formula><p>During the evaluation, the most likely sequence is obtained by the Viterbi algorithm. The output obtained is re-processed to group back sub-tokens obtained due to the tokenization approach used by transformers models <ref type="bibr" coords="5,267.09,601.96,16.36,10.91" target="#b36">[37]</ref>. Only the tag of the first sub-token is considered for the final tag sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental setting and discussion of results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation</head><p>The model architecture has been implemented by using the Python programming language. In particular we used the Pytorch <ref type="bibr" coords="6,227.66,145.80,17.83,10.91" target="#b37">[38]</ref> syntax and the Transformer Huggingface library <ref type="bibr" coords="6,465.53,145.80,16.16,10.91" target="#b38">[39]</ref>. The CRF layer used is the one provided by the library TorchCRF<ref type="foot" coords="6,358.06,157.59,3.71,7.97" target="#foot_1">2</ref> . We set AdamW as optimizer; a modified version of the Adam optimizer that uses a weight decay factor. Consequently we set a weight decay of 0.01, a learning rate of 1e-5, a number of epochs equal to 500, a number of steps equal to 2000, and a batch size of 16. The code has been run on Google Colab environment<ref type="foot" coords="6,488.46,198.24,3.71,7.97" target="#foot_2">3</ref> by using a Tesla T4 Nvidia GPU with 16GB of RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Dataset</head><p>The dataset is provided by the organizers of the SpRadIe 2021 <ref type="bibr" coords="6,353.10,263.27,11.23,10.91" target="#b7">[8,</ref><ref type="bibr" coords="6,366.38,263.27,8.88,10.91" target="#b8">9]</ref> competition. It consists of 513 ultrasonography reports provided by a pediatric hospital in Argentina. Reports are unstructured and have abundance of orthographic and grammatical errors and have been anonymized in order to remove patient IDs, and names and the enrollment numbers of the physicians. Reports were annotated by clinical experts and then revised by linguists. Annotation guidelines and training were provided for both rounds of annotations. Automatic classifiers will be expected to perform well in those cases where human annotators have strong agreement, and worse in cases that are difficult for human annotators to identify consistently. Annotations are provided in brat format. More details are reported in <ref type="bibr" coords="6,281.02,371.66,11.31,10.91" target="#b1">[2]</ref>. The annotated dataset is constituted as follows: training set (175 reports); Same-sample development set (47 reports); Held-out development set (45 reports); Held-out test set (207 reports).</p><p>The following entities are distinguished:</p><p>• Anatomical Entity: entities corresponding to an anatomical part, for example breast (pecho), liver (hígado), right thyroid lobe (lóbulo tiroideo derecho); • Finding: a pathological finding or diagnosis, for example: cyst, cyanosis;</p><p>• Location: it refers to a location in the body. Examples of locations are: walls, cavity, longitudinal, frontal, occipital, cervicodorsolumbosacra, lumbosacral, intracanalar, subcutanea; • Measure: expression indicating a measure; • Type of Measure: expression indicating a type of measure; • Degree: It indicates the degree of a finding or some other property of an entity, for example, "leve", "levemente" (slight), "mínimo" (minimal); • Abbreviation: acronyms or abbreviations to indicate a medical concept; • Negation: hedge cues indicating negation; • Uncertainty: hedge cues indicating a probability (not a certainty) that some finding may be present in a given patient.  • Conditional Temporal: hedge cues indicating that something occurred in the past or may occur in the future.</p><p>In Fig. <ref type="figure" coords="7,135.35,413.98,5.17,10.91" target="#fig_1">2</ref> is reported an example of annotated piece of diagnosis. In Fig. <ref type="figure" coords="7,441.37,413.98,5.17,10.91" target="#fig_2">3</ref> it is showed the distribution of different entities among the training dataset. The entity type Finding is particularly challenging, as it presents great variability in its textual forms. It ranges from a single word to more than 10 words in some cases, and comprising all kinds of phrases. However, this is also the most informative type of entity for the potential users of these annotations. Other challenging phenomena are the regular polysemy observed between Anatomical entities and Locations, and the irregular uses of Abbreviations. Moreover the same token can be labeled with different tags at the same time making the automatic annotation a complex task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Setting and metrics</head><p>In order to train the proposed hybrid NER model, we merged the train set with the Same-sample development set resulting in a single set of 222 diagnoses (i.e. 18428 tokens obtained by using a white space splitting). Consequently we used only the Held-out development set for evaluating the performances of the model on 45 diagnoses (i.e. 5090 tokens). The performances of the models are evaluated by using classic metrics such as Precision, Recall and F1-score. As a consequence of the unbalancing of the dataset, we decided to consider the micro average of results obtained for each tag category.</p><p>We chose ten different transformer models imported through the Transformer HuggingFace library. They are listed in the following:</p><p>• bert-base-uncased (BERT): it is the standard base version of BERT <ref type="bibr" coords="8,411.88,122.18,16.37,10.91">(12-</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Discussion of results</head><p>The results we obtained are reported in Table <ref type="table" coords="8,308.10,493.44,4.14,10.91" target="#tab_1">1</ref>-3. It is possible to observe that the best performing model, by considering F1-score as main metric, is BERT in its base version trained on a multilingual dataset. The model has obtained a micro F1-score of 0.69632 that outperforms the competitors in a range from 0.59% to 17.94%. The most significant difference is obtained by comparing the results with them of the ELECTRA-MED base model. In particular, these results are unexpected low and required further investigation for understanding their causes.</p><p>On the contrary, the model based on BETO is performing particularly well obtaining a results comparable with the one obtained by using BERT multilingual. The differences among the two models are very low, probably because they share the same internal architecture, and both of them are based on a vocabulary able to deal with Spanish terms. If from one end BERT multilanguage is trained on many different languages simultaneously creating some noise on data, on the other end, BETO is trained on a smaller set of data losing then in accuracy. The two issues of the two models are able to produce results somehow very close to each other. If we observe the results obtained for precision (Table <ref type="table" coords="8,334.40,669.58,4.22,10.91" target="#tab_1">1</ref>) and recall (Table <ref type="table" coords="8,422.84,669.58,3.62,10.91" target="#tab_2">2</ref>), we can keep an interesting behavior of the two models. BETO, even if for a small gap, obtains better results for precision than BERT-mul. On the contrary, considering the recall, BERT-mul is the best. Still looking at the precision values, we can observe that, in general, the chosen models show prediction difficulties for the tag classes "Conditional Temporal", "Uncertainly", "Degree" and "Location". In particular, such behavior could derive from the specificity of such classes for the medical field. It is, in fact, evident as for classes of more general character like "Negation", "Type of Measure", "Abbreviation" all the models perform medium-well. The only exception is the class "Anatomical Entity" that even if much specific of the medical domain turns out to be classified correctly in nearly the totality of the models. This could derive from the wide everyday use that is made of terms regarding the parts of the body also in the common language. Looking at the recall, we can see that some classes were easier to find than others, such as "Anatomical Entity", "Negation", "Abbreviation", "Uncertainly". For the first three classes, obtaining good results of precision and recall, we can say that the models are able not only to be precise in the classification but also to identify them correctly in the text. For the class "Uncertainty", however, the models tend to overestimate its presence in the sentences because it has a good recall value and low precision.</p><p>If we want to understand if the training language of the model is an issue for a correct classification of individual tokens, we can observe the differences between monolingual and multilingual models. Generally speaking, we can observe that multilingual and Spanish-trained models are better performing than their counterpart monolingual. This result was expected as a consequence of the language of the dataset. It is obvious that a model able to manage words in the dataset language is more efficient than a model trained on English data also if the SentencePiece tokenizer, typical of transformer-based architectures, is alleviating these differences through the use of sub-tokens. If we focus on domain-specific models (BioCliBERT and BioBERT), we can observe that they perform in line with their non-specialized counterparts. The difference in F1-score among BERT-base, BioCliBERT, and BioBERT is only around 0.01. This suggests that the training on the medical domain is not correctly supporting the task of NER for clinical diagnoses, if, as in this case, the model is pre-trained on a different language of the testing dataset (i.e. English). In order for it to be possible to draw conclusions about the usefulness of a model trained on medical data for the specific NER task, it would be necessary as a future development to train a new model with medical data in the Spanish language.</p><p>By focusing on the differences in transformer architectures, we can affirm that BERT is commonly performing better than others even if XLM-RoBERTa and ELECTRA are, however, obtaining results not very far from the BERT counterpart. Finally, all the models could not correctly classify Conditional Temporal tags, but this problem is almost certainly related with the size of this class in the dataset. Indeed, only 19 instances are available in the training set and only one instance was present in the set used for the analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">SpRadIe submission</head><p>In light of the results, decisions were made to submit to the SpRadIe competition. In particular, the first decision taken concerns the configuration of the hybrid model to use. The obvious choice would have fallen on using BERT-mul as the transformer-based model. On the contrary, XLM- RoBERTa was chosen. This decision is motivated by our feeling that the validation dataset was not large enough for containing a representative subset of all the possible variations in data we could have in validation phase, especially regarding the less numerous classes. Indeed, a model best performing on validation set is not always the best performing on test set, especially when the validation set is very small. Therefore, it was decided to opt for a model with intermediate performance in F1-score, following the idea that a less specialized model could better deal the Our second decision concerned the number of models to be learned. In particular, our study of the dataset revealed that the same token could be annotated with more than one tag. For example, the term "cm" could be both parts of a "Measure" tag and an abbreviation tag. To this end, we decided to create four groups of tags with as little overlap as possible and to create If considering the results we obtained from the task organizers reported in Table <ref type="table" coords="12,464.22,652.16,3.77,10.91" target="#tab_4">4</ref>, we can observe that the results are lower than expected, especially for some specific tag class. In In any case it will be considered future work to investigate the reasons for these poorer than expected performances as soon as the annotated test set is released.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Automated analysis of textual documents in medical settings is still a research challenge that will be a topic of discussion in the coming years. Recently, the amount of medical data available has grown rapidly due to the deployment of numerous applications and systems to support the patient and wellness enthusiast. Nevertheless, the amount of annotated data is still meager due to their privacy issues. Recently, however, attempts have been made to overcome this problem with transfer learning strategies based on large pre-trained models that use deep learning techniques. Among these, those based on transformers have proved to be the most reliable and versatile. In the wake of these scientific innovations, it was decided to address the problem of name entity recognition in the medical field by exploiting these approaches. Specifically, we proposed a hybrid model that combines the power of representation of transformer models with the predictive power of probabilistic conditional random field models. The model was evaluated in several of its combinations, varying the transformer model at the core of its architecture. Specifically, the models BERT, BERT-mul, BETO, RoBERTa, XLM-RoBERTa, ELECTRA, ELECTRA-ES, ELECTRA-MED, BioBert, BioClinBERT were evaluated. The experimentation was performed on data provided by the SpRadIE 2021 <ref type="bibr" coords="13,334.77,459.09,11.48,10.91" target="#b7">[8,</ref><ref type="bibr" coords="13,349.03,459.09,9.03,10.91" target="#b8">9]</ref> competition co-located with the Conference and Labs of the Evaluation Forum 2021. In particular, the training set counted 222 medical diagnoses in the Spanish language appropriately annotated by experts in the field. The preliminary analysis was carried out on a validation set extracted from a data collection different from the training set. Specifically, this portion of data contained 45 medical diagnoses, also in Spanish. The results obtained showed excellent effectiveness of the transformer-based models trained on multilingual or Spanish data. Specifically, BERT-base in its multilingual version and BETO proved to be the most suitable for the task of NER for medical data. The results of this analysis were used to make implementation decisions regarding the system to be submitted to the SpRadIE 2021 competition. Specifically, it was decided to perform a submission using the transformer based XLM-RoBERTa model with the hope of obtaining satisfactory results through the use of a model with average performance in the preliminary analysis phase. This decision was made to avoid basing our decisions solely on a portion of the data that was not expressive enough due to its size. The results obtained proved to be lower than expected, but further study and investigation will be carried out upon release of the annotated test set.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,242.72,163.25,8.93;4,89.29,84.19,416.69,134.01"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Hybrid architecture proposed</figDesc><graphic coords="4,89.29,84.19,416.69,134.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,89.29,138.27,209.72,8.93;7,89.29,165.12,425.02,150.67"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of annotated piece of diagnosis.</figDesc><graphic coords="7,89.29,165.12,425.02,150.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,89.29,340.31,286.22,8.93"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Distribution of tag annotations among the training dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,88.99,90.49,406.73,480.08"><head>Table 1</head><label>1</label><figDesc>Results obtained for the chosen transformer models considering the Precision.</figDesc><table coords="10,99.58,118.42,396.14,452.14"><row><cell></cell><cell>BERT</cell><cell cols="2">BERT-MUL BETO</cell><cell cols="3">RoBERTa XLM-RoBERTa</cell></row><row><cell cols="2">Abbreviation 0.79412</cell><cell>0.73874</cell><cell>0.69919</cell><cell cols="2">0.66071</cell><cell>0.6774</cell></row><row><cell>Anatomical Entity</cell><cell>0.75776</cell><cell>0.77193</cell><cell>0.76630</cell><cell cols="2">0.77844</cell><cell>0.7794</cell></row><row><cell>Conditional Temporal</cell><cell>0.00000</cell><cell>0.00000</cell><cell>0.00000</cell><cell cols="2">0.00000</cell><cell>0.0000</cell></row><row><cell>Degree</cell><cell>0.33333</cell><cell>0.45455</cell><cell>0.55556</cell><cell cols="2">0.50000</cell><cell>0.5294</cell></row><row><cell>Finding</cell><cell>0.50292</cell><cell>0.63725</cell><cell>0.63462</cell><cell cols="2">0.52688</cell><cell>0.5418</cell></row><row><cell>Measure</cell><cell>0.58549</cell><cell>0.71721</cell><cell>0.67932</cell><cell cols="2">0.69515</cell><cell>0.7111</cell></row><row><cell>Negation</cell><cell>0.89394</cell><cell>0.89888</cell><cell>0.86408</cell><cell cols="2">0.86357</cell><cell>0.8701</cell></row><row><cell>Type of measure</cell><cell>0.89394</cell><cell>0.79167</cell><cell>0.70492</cell><cell cols="2">0.80625</cell><cell>0.8421</cell></row><row><cell>Uncertainty</cell><cell>0.50000</cell><cell>0.42857</cell><cell>0.33333</cell><cell cols="2">0.50000</cell><cell>0.5000</cell></row><row><cell>Location</cell><cell>0.46400</cell><cell>0.51250</cell><cell>0.63846</cell><cell cols="2">0.43810</cell><cell>0.4806</cell></row><row><cell>microAvg</cell><cell cols="2">0.62489 0.69462</cell><cell cols="3">0.69645 0.65649</cell><cell>0.6635</cell></row><row><cell></cell><cell>ELECTRA</cell><cell>ELECTRA -ES</cell><cell cols="2">ELECTRA -MED</cell><cell cols="2">BioCliBERT BioBERT</cell></row><row><cell cols="2">Abbreviation 0.66667</cell><cell>0.71585</cell><cell>0.36111</cell><cell></cell><cell>0.75385</cell><cell>0.81818</cell></row><row><cell>Anatomical Entity</cell><cell>0.74859</cell><cell>0.77273</cell><cell>0.64306</cell><cell></cell><cell>0.79655</cell><cell>0.78049</cell></row><row><cell>Conditional Temporal</cell><cell>0.00000</cell><cell>0.00000</cell><cell>0.00000</cell><cell></cell><cell>0.25000</cell><cell>0.00000</cell></row><row><cell>Degree</cell><cell>0.38095</cell><cell>0.57895</cell><cell>0.27778</cell><cell></cell><cell>0.31579</cell><cell>0.50000</cell></row><row><cell>Finding</cell><cell>0.50581</cell><cell>0.57285</cell><cell>0.56757</cell><cell></cell><cell>0.47181</cell><cell>0.56836</cell></row><row><cell>Measure</cell><cell>0.63687</cell><cell>0.73092</cell><cell>0.66818</cell><cell></cell><cell>0.68456</cell><cell>0.60870</cell></row><row><cell>Negation</cell><cell>0.86765</cell><cell>0.91429</cell><cell>0.80723</cell><cell></cell><cell>0.86667</cell><cell>0.89773</cell></row><row><cell>Type of measure</cell><cell>0.93103</cell><cell>0.74627</cell><cell>0.39130</cell><cell></cell><cell>0.85714</cell><cell>0.82927</cell></row><row><cell>Uncertainty</cell><cell>0.50000</cell><cell>0.26667</cell><cell>0.23077</cell><cell></cell><cell>0.42857</cell><cell>0.28571</cell></row><row><cell>Location</cell><cell>0.54082</cell><cell>0.48503</cell><cell>0.44037</cell><cell></cell><cell>0.44231</cell><cell>0.41104</cell></row><row><cell>microAvg</cell><cell>0.63983</cell><cell>0.67321</cell><cell>0.56189</cell><cell></cell><cell>0.63311</cell><cell>0.64984</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,88.99,90.49,416.99,531.82"><head>Table 2</head><label>2</label><figDesc>Results obtained for the chosen transformer models considering the Recall.</figDesc><table coords="11,89.02,118.45,416.97,503.86"><row><cell></cell><cell>BERT</cell><cell cols="2">BERT-mul BETO</cell><cell cols="3">RoBERTa XLM-RoBERTa</cell></row><row><cell cols="2">Abbreviation 0.87097</cell><cell>0.85417</cell><cell>0.81132</cell><cell cols="2">0.86071</cell><cell>0.8630</cell></row><row><cell>Anatomical Entity</cell><cell>0.80795</cell><cell>0.78338</cell><cell>0.81503</cell><cell cols="2">0.80069</cell><cell>0.8179</cell></row><row><cell>Conditional Temporal</cell><cell>0.00000</cell><cell>0.00000</cell><cell>0.00000</cell><cell cols="2">0.00000</cell><cell>0.0000</cell></row><row><cell>Degree</cell><cell>0.37500</cell><cell>0.58824</cell><cell>0.55556</cell><cell cols="2">0.50286</cell><cell>0.5294</cell></row><row><cell>Finding</cell><cell>0.42786</cell><cell>0.53830</cell><cell>0.52800</cell><cell cols="2">0.41294</cell><cell>0.4467</cell></row><row><cell>Measure</cell><cell>0.59162</cell><cell>0.75431</cell><cell>0.68511</cell><cell cols="2">0.59553</cell><cell>0.6038</cell></row><row><cell>Negation</cell><cell>0.85507</cell><cell>0.83333</cell><cell>0.85577</cell><cell cols="2">0.83869</cell><cell>0.8481</cell></row><row><cell>Type of measure</cell><cell>0.85507</cell><cell>0.76000</cell><cell>0.76786</cell><cell cols="2">0.75152</cell><cell>0.7619</cell></row><row><cell>Uncertainty</cell><cell>1.0</cell><cell>0.75000</cell><cell>0.75000</cell><cell cols="2">0.66667</cell><cell>1.0</cell></row><row><cell>Location</cell><cell>0.65169</cell><cell>0.75926</cell><cell>0.72807</cell><cell cols="2">0.64750</cell><cell>0.6526</cell></row><row><cell>microAvg</cell><cell cols="2">0.63077 0.69803</cell><cell cols="3">0.68801 0.63621</cell><cell>0.6404</cell></row><row><cell></cell><cell>ELECTRA</cell><cell>ELECTRA -ES</cell><cell cols="2">ELECTRA -MED</cell><cell cols="2">BioCliBERT BioBERT</cell></row><row><cell cols="2">Abbreviation 0.83871</cell><cell>0.85065</cell><cell>0.75581</cell><cell></cell><cell>0.87500</cell><cell>0.70130</cell></row><row><cell>Anatomical Entity</cell><cell>0.87748</cell><cell>0.72070</cell><cell>0.67964</cell><cell></cell><cell>0.81338</cell><cell>0.78528</cell></row><row><cell>Conditional Temporal</cell><cell>0.00000</cell><cell>0.00000</cell><cell>0.00000</cell><cell></cell><cell>1.0</cell><cell>0.00000</cell></row><row><cell>Degree</cell><cell>0.50000</cell><cell>0.55000</cell><cell>0.29412</cell><cell></cell><cell>0.46154</cell><cell>0.41176</cell></row><row><cell>Finding</cell><cell>0.43284</cell><cell>0.51805</cell><cell>0.43659</cell><cell></cell><cell>0.43923</cell><cell>0.45887</cell></row><row><cell>Measure</cell><cell>0.59686</cell><cell>0.71094</cell><cell>0.66516</cell><cell></cell><cell>0.58286</cell><cell>0.58065</cell></row><row><cell>Negation</cell><cell>0.85507</cell><cell>0.80672</cell><cell>0.72043</cell><cell></cell><cell>0.83871</cell><cell>0.89773</cell></row><row><cell>Type of measure</cell><cell>0.77143</cell><cell>0.64935</cell><cell>0.73469</cell><cell></cell><cell>0.80000</cell><cell>0.77273</cell></row><row><cell>Uncertainty</cell><cell>1.0</cell><cell>0.80000</cell><cell>1.0</cell><cell></cell><cell>1.0</cell><cell>0.66667</cell></row><row><cell>Location</cell><cell>0.59551</cell><cell>0.61364</cell><cell>0.45714</cell><cell></cell><cell>0.58974</cell><cell>0.67000</cell></row><row><cell>microAvg</cell><cell>0.64530</cell><cell>0.65794</cell><cell>0.58129</cell><cell></cell><cell>0.63252</cell><cell>0.62697</cell></row><row><cell cols="7">variability on new unseen data. In particular, we decided to use this strategy as a regularization</cell></row><row><cell cols="7">one, similarly to what is commonly performed with early-stopping for reducing overfitting.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="12,88.91,90.49,418.27,559.04"><head>Table 3</head><label>3</label><figDesc>Results obtained for the chosen transformer models considering the F1-score. Conditional Temporal. In the prediction phase for each tag, we obtained the results of each classifier and chose the one with the highest probability.</figDesc><table coords="12,88.91,118.45,418.27,517.53"><row><cell></cell><cell>BERT</cell><cell cols="2">BERT-mul BETO</cell><cell cols="3">RoBERTa XLM-RoBERTa</cell></row><row><cell cols="2">Abbreviation 0.83077</cell><cell>0.79227</cell><cell>0.75109</cell><cell cols="2">0.74071</cell><cell>0.7590</cell></row><row><cell>Anatomical Entity</cell><cell>0.78205</cell><cell>0.77761</cell><cell>0.78992</cell><cell cols="2">0.77513</cell><cell>0.7982</cell></row><row><cell>Conditional Temporal</cell><cell>0.00000</cell><cell>0.00000</cell><cell>0.00000</cell><cell cols="2">0.00000</cell><cell>0.0000</cell></row><row><cell>Degree</cell><cell>0.35294</cell><cell>0.51282</cell><cell>0.55556</cell><cell cols="2">0.51053</cell><cell>0.5294</cell></row><row><cell>Finding</cell><cell>0.46237</cell><cell>0.58361</cell><cell>0.57642</cell><cell cols="2">0.46865</cell><cell>0.4896</cell></row><row><cell>Measure</cell><cell>0.58854</cell><cell>0.73529</cell><cell>0.68220</cell><cell cols="2">0.62508</cell><cell>0.6531</cell></row><row><cell>Negation</cell><cell>0.87407</cell><cell>0.86486</cell><cell>0.85990</cell><cell cols="2">0.84060</cell><cell>0.8590</cell></row><row><cell>Type of measure</cell><cell>0.87407</cell><cell>0.77551</cell><cell>0.73504</cell><cell cols="2">0.75385</cell><cell>0.8000</cell></row><row><cell>Uncertainty</cell><cell>0.66667</cell><cell>0.54545</cell><cell>0.46154</cell><cell cols="2">0.57143</cell><cell>0.6667</cell></row><row><cell>Location</cell><cell>0.54206</cell><cell>0.61194</cell><cell>0.68033</cell><cell cols="2">0.50979</cell><cell>0.5536</cell></row><row><cell>microAvg</cell><cell cols="2">0.62782 0.69632</cell><cell cols="3">0.69220 0.64619</cell><cell>0.6517</cell></row><row><cell></cell><cell>ELECTRA</cell><cell>ELECTRA -ES</cell><cell cols="2">ELECTRA -MED</cell><cell cols="2">BioCliBERT BioBERT</cell></row><row><cell cols="2">Abbreviation 0.74286</cell><cell>0.77745</cell><cell>0.48872</cell><cell></cell><cell>0.80992</cell><cell>0.75524</cell></row><row><cell>Anatomical Entity</cell><cell>0.80793</cell><cell>0.74581</cell><cell>0.66084</cell><cell></cell><cell>0.80488</cell><cell>0.78287</cell></row><row><cell>Conditional Temporal</cell><cell>0.00000</cell><cell>0.00000</cell><cell>0.00000</cell><cell></cell><cell>0.40000</cell><cell>0.00000</cell></row><row><cell>Degree</cell><cell>0.43243</cell><cell>0.56410</cell><cell>0.28571</cell><cell></cell><cell>0.37500</cell><cell>0.45161</cell></row><row><cell>Finding</cell><cell>0.46649</cell><cell>0.54408</cell><cell>0.49354</cell><cell></cell><cell>0.45494</cell><cell>0.50778</cell></row><row><cell>Measure</cell><cell>0.61622</cell><cell>0.72079</cell><cell>0.66667</cell><cell></cell><cell>0.62963</cell><cell>0.59434</cell></row><row><cell>Negation</cell><cell>0.86131</cell><cell>0.85714</cell><cell>0.76136</cell><cell></cell><cell>0.85246</cell><cell>0.89773</cell></row><row><cell>Type of measure</cell><cell>0.84375</cell><cell>0.69444</cell><cell>0.51064</cell><cell></cell><cell>0.82759</cell><cell>0.80000</cell></row><row><cell>Uncertainty</cell><cell>0.66667</cell><cell>0.40000</cell><cell>0.37500</cell><cell></cell><cell>0.60000</cell><cell>0.40000</cell></row><row><cell>Location</cell><cell>0.56684</cell><cell>0.54181</cell><cell>0.44860</cell><cell></cell><cell>0.50549</cell><cell>0.50951</cell></row><row><cell>microAvg</cell><cell>0.64255</cell><cell>0.66549</cell><cell>0.57143</cell><cell></cell><cell>0.63282</cell><cell>0.63820</cell></row><row><cell cols="7">a classifier for each. In particular, the four groups are composed as follows: (i) Finding; (ii)</cell></row><row><cell cols="7">Anatomical Entity, Measure, Degree; (iii) Location, Negation, Type of Measure; (iv) Abbreviation,</cell></row><row><cell>Uncertainty,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="13,88.99,90.49,418.53,117.47"><head>Table 4</head><label>4</label><figDesc>Results obtained for the SpRadIe submission considering the "exact" matching and the micro F1-score.</figDesc><table coords="13,89.29,117.86,416.98,90.11"><row><cell>Abbr.</cell><cell>Anat. Entity</cell><cell>Cond. Temp</cell><cell cols="5">Degree Finding Location Measure Negation</cell><cell>Type measure</cell><cell>Unc.</cell></row><row><cell>SWAP 0,49</cell><cell>0,52</cell><cell>0,00</cell><cell>0,48</cell><cell>0,44</cell><cell>0,31</cell><cell>0,51</cell><cell>0,76</cell><cell>0,37</cell><cell>0,17</cell></row><row><cell cols="10">particular, "Type of measure" is resulted 116.2% lower than what obtained in the preliminary</cell></row><row><cell cols="10">evaluation. Similar values are obtained also for the classes Uncertainity, Abbreviation and</cell></row><row><cell cols="2">Anatomical Entity.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.93,660.08,396.92,8.97;2,89.29,671.04,100.58,8.97"><p>https://www.mckinsey.com/industries/healthcare-systems-and-services/our-insights/the-era-of-exponentialimprovement-in-healthcare</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,108.93,660.07,128.09,8.97"><p>https://pypi.org/project/TorchCRF/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="6,108.93,671.03,125.02,8.97"><p>https://colab.research.google.com/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7.">Acknowledgment</head><p>This work has been supported by <rs type="funder">Apulia Region, Italy</rs> through the project "<rs type="projectName">Un Assistente Dialogante Intelligente per il Monitoraggio Remoto di Pazienti</rs>" (Grant n. <rs type="grantNumber">10AC8FB6</rs>) in the context of "<rs type="projectName">Research for Innovation -REFIN</rs>".</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_BXbdrmD">
					<idno type="grant-number">10AC8FB6</idno>
					<orgName type="project" subtype="full">Un Assistente Dialogante Intelligente per il Monitoraggio Remoto di Pazienti</orgName>
				</org>
				<org type="funded-project" xml:id="_b2XhkD2">
					<orgName type="project" subtype="full">Research for Innovation -REFIN</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="14,112.66,197.00,393.33,10.91;14,112.66,210.55,236.41,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,274.82,197.00,231.17,10.91;14,112.66,210.55,45.69,10.91">What is ehealth?: a systematic review of published definitions</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Rizo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Enkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,166.43,210.55,108.85,10.91">World Hosp Health Serv</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="32" to="40" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,224.10,393.33,10.91;14,112.66,237.65,255.73,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,335.20,224.10,170.78,10.91;14,112.66,237.65,110.32,10.91">Annotation of entities and relations in spanish radiology reports</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Cotik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Filippo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,249.68,237.65,29.51,10.91">RANLP</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,251.20,393.33,10.91;14,112.66,264.75,330.76,10.91" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="14,162.82,251.20,343.17,10.91;14,112.66,264.75,24.09,10.91">Von dem Bussche, The eu general data protection regulation (gdpr), A Practical Guide</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Voigt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">3152676</biblScope>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
	<note>1st Ed</note>
</biblStruct>

<biblStruct coords="14,112.66,278.30,394.53,10.91;14,112.66,291.85,393.33,10.91;14,112.66,305.40,394.53,10.91;14,112.66,318.95,80.57,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,222.70,291.85,184.75,10.91">Overview of the clef ehealth evaluation lab</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Névéol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ramadier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Spijker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,449.04,291.85,56.95,10.91;14,112.66,305.40,346.66,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,332.50,393.33,10.91;14,112.66,346.05,328.55,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="14,321.30,332.50,184.68,10.91;14,112.66,346.05,157.43,10.91">A hybrid bi-lstm-crf model for knowledge recognition from ehealth documents</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M R</forename><surname>Zavala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Segura-Bedmar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,298.36,346.05,64.18,10.91">TASS@ SEPLN</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="65" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,359.59,394.61,10.91;14,112.66,373.14,393.33,10.91;14,112.66,386.69,393.33,10.91;14,112.66,400.24,353.13,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="14,327.75,359.59,179.52,10.91;14,112.66,373.14,281.62,10.91">Ixa-ner-re at ehealth-kd challenge 2020: Cross-lingual transfer learning for medical relation extraction</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Andrés</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Sainz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Atutxa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">L</forename><surname>De Lacalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,420.70,373.14,85.29,10.91;14,112.66,386.69,393.33,10.91;14,112.66,400.24,261.05,10.91">Proceedings of the Iberian Languages Evaluation Forum co-located with 36th Conference of the Spanish Society for Natural Language Processing, IberLEF@ SEPLN</title>
		<meeting>the Iberian Languages Evaluation Forum co-located with 36th Conference of the Spanish Society for Natural Language Processing, IberLEF@ SEPLN</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,413.79,393.53,10.91;14,112.66,427.34,393.33,10.91;14,112.33,440.89,227.74,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="14,229.56,413.79,74.48,10.91;14,331.10,413.79,175.10,10.91;14,112.66,427.34,120.73,10.91">Text mining and semantic knowledge for automated clinical encoding</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Garcıa-Santa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cetina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,256.43,427.34,249.56,10.91;14,112.33,440.89,197.61,10.91">Working Notes of Conference and Labs of the Evaluation (CLEF) Forum. CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note>Fle at clef ehealth</note>
</biblStruct>

<biblStruct coords="14,112.66,454.44,394.53,10.91;14,112.66,467.99,393.33,10.91;14,112.66,481.54,393.33,10.91;14,112.66,495.09,186.50,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="14,313.06,467.99,192.93,10.91">Overview of the clef eHealth evaluation lab</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Alemany</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Bassani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Brew-Sam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Cotik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Filippo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>González-Sáez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Luque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,152.89,481.54,273.50,10.91">CLEF 2021 -12th Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="14,434.17,481.54,71.82,10.91;14,112.66,495.09,108.44,10.91">Lecture Notes in Computer Science (LNCS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,508.64,395.01,10.91;14,112.66,522.18,393.33,10.91;14,112.66,535.73,393.33,10.91;14,112.66,549.28,249.65,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="14,242.96,522.18,263.03,10.91;14,112.66,535.73,247.81,10.91">Overview of CLEF eHealth Task 1 -SpRadIE: A challenge on information extraction from Spanish Radiology Reports</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Cotik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Alemany</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Filippo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Luque</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vivaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ayach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Carranza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Francesca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dellanzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,383.78,535.73,122.21,10.91;14,112.66,549.28,168.87,10.91">CLEF 2021 Evaluation Labs and Workshop: Online Working Notes</title>
		<imprint>
			<publisher>CEUR-WS</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,562.83,395.17,10.91;14,112.66,576.38,394.53,10.91;14,112.66,589.93,90.72,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="14,148.47,576.38,105.26,10.91">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,276.42,576.38,226.02,10.91">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,603.48,393.54,10.91;14,112.66,617.03,206.20,10.91" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="14,282.41,603.48,223.78,10.91;14,112.66,617.03,174.28,10.91">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,630.58,393.33,10.91;14,112.66,644.13,393.33,10.91;14,112.66,657.68,393.32,10.91;15,112.66,86.97,393.33,10.91;15,112.66,100.52,394.03,10.91;15,112.66,114.06,93.34,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="14,323.15,630.58,182.83,10.91;14,112.66,644.13,186.91,10.91">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/N19-1423" />
	</analytic>
	<monogr>
		<title level="m" coord="14,327.87,644.13,178.11,10.91;14,112.66,657.68,393.32,10.91;15,112.66,86.97,99.97,10.91">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="15,112.66,127.61,226.33,10.91;15,359.42,127.61,146.57,10.91;15,112.66,141.16,231.81,10.91;15,374.74,141.16,132.54,10.91;15,112.66,154.71,367.39,10.91;15,112.41,168.26,255.32,10.91" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Polignano</surname></persName>
		</author>
		<ptr target="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058658278&amp;partnerID=40&amp;md5=9719e0512649279" />
		<title level="m" coord="15,359.42,127.61,146.57,10.91;15,112.66,141.16,227.87,10.91">Overview of the evalita 2018 aspect-based sentiment analysis task (absita)</title>
		<imprint>
			<date type="published" when="2018">2018. a6ed46a8262f050dc</date>
			<biblScope unit="volume">2263</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,181.81,393.82,10.91;15,112.66,195.36,395.17,10.91;15,112.66,208.91,395.01,10.91;15,112.66,222.46,379.09,10.91" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="15,450.57,181.81,55.91,10.91;15,112.66,195.36,395.17,10.91;15,112.66,208.91,31.29,10.91">Overview of the aspect term extraction and aspect-based sentiment analysis task</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>De Mattei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iovine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Miaschi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Polignano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Rambelli</surname></persName>
		</author>
		<ptr target="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097555110&amp;partnerID=40&amp;md5=2122e26ed7367" />
		<imprint>
			<date type="published" when="2020">2020. 2020. e5a7c40642835591903</date>
			<biblScope unit="volume">2765</biblScope>
		</imprint>
	</monogr>
	<note>Ate absita @ evalita. cited By 5</note>
</biblStruct>

<biblStruct coords="15,112.66,236.01,393.33,10.91;15,112.66,249.56,395.01,10.91;15,112.66,263.11,393.67,10.91;15,112.66,276.66,394.51,10.91;15,112.36,290.20,116.91,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="15,345.06,236.01,160.93,10.91;15,112.66,249.56,202.02,10.91">Time of your hate: The challenge of time in hate speech detection on social media</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Florio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Polignano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Patti</surname></persName>
		</author>
		<idno type="DOI">10.3390/APP10124180</idno>
		<ptr target="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087763770&amp;doi=10.3390%2fAPP10124180&amp;partnerID=40&amp;md5=d" />
	</analytic>
	<monogr>
		<title level="j" coord="15,323.59,249.56,76.26,10.91">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2020">2020. 4c6b8ba193ed062299bc19f02</date>
			<pubPlace>Switzerland</pubPlace>
		</imprint>
	</monogr>
	<note>cited By 7</note>
</biblStruct>

<biblStruct coords="15,112.66,303.75,393.61,10.91;15,112.66,317.30,393.58,10.91;15,112.33,330.85,29.19,10.91" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="15,374.83,303.75,131.43,10.91;15,112.66,317.30,247.38,10.91">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,112.66,344.40,393.33,10.91;15,112.66,357.95,393.32,10.91;15,112.66,371.50,395.01,10.91;15,112.41,385.05,18.52,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="15,412.98,344.40,93.00,10.91;15,112.66,357.95,324.30,10.91">Alberto: Italian bert language understanding model for nlp challenging tasks based on tweets</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Polignano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Gemmis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Semeraro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Basile</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,460.15,357.95,45.83,10.91;15,112.66,371.50,251.14,10.91">6th Italian Conference on Computational Linguistics, CLiC-it 2019</title>
		<imprint>
			<publisher>CEUR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2481</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,398.60,393.32,10.91;15,112.66,412.15,266.40,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="15,395.98,398.60,110.00,10.91;15,112.66,412.15,115.82,10.91">Spanish pre-trained bert model and evaluation data</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cañete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chaperon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-H</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,251.30,412.15,97.80,10.91">PML4DC at ICLR 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,425.70,393.33,10.91;15,112.66,439.25,393.98,10.91;15,112.41,452.79,48.96,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="15,361.64,425.70,144.35,10.91;15,112.66,439.25,268.25,10.91">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,394.29,439.25,66.92,10.91">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,466.34,394.53,10.91;15,112.66,479.89,369.79,10.91" xml:id="b19">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Alsentzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-H</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mcdermott</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03323</idno>
		<title level="m" coord="15,112.66,479.89,186.98,10.91">Publicly available clinical bert embeddings</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,112.66,493.44,394.53,10.91;15,112.30,506.99,393.68,10.91;15,112.66,520.54,107.17,10.91" xml:id="b20">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m" coord="15,173.53,506.99,256.77,10.91">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,112.66,534.09,394.53,10.91;15,112.66,547.64,393.33,10.91;15,112.66,561.19,201.68,10.91" xml:id="b21">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02116</idno>
		<title level="m" coord="15,271.58,547.64,234.41,10.91;15,112.66,561.19,19.96,10.91">Unsupervised cross-lingual representation learning at scale</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,112.66,574.74,393.33,10.91;15,112.66,588.29,347.38,10.91" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<title level="m" coord="15,335.14,574.74,170.85,10.91;15,112.66,588.29,165.13,10.91">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,112.66,601.84,393.33,10.91;15,112.48,615.39,307.35,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="15,281.46,601.84,160.12,10.91">Named entity recognition approaches</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">S</forename><surname>Affendey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mamat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,449.04,601.84,56.95,10.91;15,112.48,615.39,228.49,10.91">International Journal of Computer Science and Network Security</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="339" to="344" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,628.93,394.62,10.91;15,112.66,642.48,163.64,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="15,186.80,628.93,292.76,10.91">Hadoken: a bert-crf model for medical document anonymization</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,112.66,642.48,74.83,10.91">IberLEF@ SEPLN</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="720" to="726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,656.03,393.33,10.91;15,112.66,669.58,393.33,10.91;16,112.66,86.97,289.56,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="15,341.84,656.03,164.15,10.91;15,112.66,669.58,190.82,10.91">Fine-tuning bert for joint entity and relation extraction in chinese medical text</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,351.94,669.58,154.05,10.91;16,112.66,86.97,174.79,10.91">IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="892" to="897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,100.52,394.62,10.91;16,112.66,114.06,393.33,10.91;16,112.66,127.61,81.94,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="16,290.93,100.52,194.56,10.91">Clinical ner using spanish bert embeddings</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Vunikili</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Sh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Marica</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,112.66,114.06,311.71,10.91">Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2020)</title>
		<title level="s" coord="16,431.39,114.06,74.60,10.91;16,112.66,127.61,51.81,10.91">CEUR Workshop Proceedings</title>
		<meeting>the Iberian Languages Evaluation Forum (IberLEF 2020)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,141.16,393.32,10.91;16,112.66,154.71,278.66,10.91" xml:id="b27">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">S</forename><surname>Kalyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sangeetha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16146</idno>
		<title level="m" coord="16,232.20,141.16,273.79,10.91;16,112.66,154.71,96.60,10.91">Want to identify, extract and normalize adverse drug reactions in tweets? use roberta</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="16,112.66,168.26,395.17,10.91;16,112.66,181.81,393.33,10.91;16,112.66,195.36,227.27,10.91" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="16,171.03,168.26,336.80,10.91;16,112.66,181.81,198.08,10.91">On the effectiveness of small, discriminatively pre-trained language representation models for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">B</forename><surname>Ozyurt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,334.61,181.81,171.37,10.91;16,112.66,195.36,139.40,10.91">Proceedings of the First Workshop on Scholarly Document Processing</title>
		<meeting>the First Workshop on Scholarly Document Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="104" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,208.91,393.32,10.91;16,112.66,222.46,393.33,10.91;16,112.66,236.01,244.49,10.91" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="16,275.09,208.91,230.89,10.91;16,112.66,222.46,65.49,10.91">Biobert based named entity recognition in electronic medical record</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,203.77,222.46,302.22,10.91;16,112.66,236.01,139.84,10.91">2019 10th International Conference on Information Technology in Medicine and Education (ITME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="49" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,249.56,393.33,10.91;16,112.66,263.11,107.17,10.91" xml:id="b30">
	<monogr>
		<title level="m" type="main" coord="16,218.61,249.56,217.14,10.91">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="16,112.66,276.66,393.33,10.91;16,112.66,290.20,293.05,10.91" xml:id="b31">
	<monogr>
		<title level="m" type="main" coord="16,259.06,276.66,246.93,10.91;16,112.66,290.20,110.86,10.91">Ltp: A new active learning strategy for bert-crf based named entity recognition</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.02524</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="16,112.66,303.75,393.33,10.91;16,112.66,317.30,363.59,10.91" xml:id="b32">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="16,353.43,303.75,152.55,10.91;16,112.66,317.30,181.08,10.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="16,112.66,330.85,394.61,10.91;16,112.66,344.40,393.32,10.91;16,112.33,357.95,147.76,10.91" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="16,199.50,330.85,288.81,10.91">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,112.66,344.40,393.32,10.91;16,112.33,357.95,59.50,10.91">Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009)</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,371.50,393.32,10.91;16,112.66,385.05,393.33,10.91;16,112.66,398.60,395.01,10.91;16,112.41,412.15,28.67,10.91" xml:id="b34">
	<analytic>
		<title level="a" type="main" coord="16,466.25,371.50,39.73,10.91;16,112.66,385.05,393.33,10.91;16,112.66,398.60,23.71,10.91">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,159.51,398.60,302.69,10.91">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,425.70,394.61,10.91;16,112.66,439.25,63.38,10.91" xml:id="b35">
	<monogr>
		<title level="m" type="main" coord="16,265.96,425.70,217.84,10.91">Bidirectional lstm-crf for named entity recognition</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Panchendrarajan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Amaresan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>PACLIC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,452.79,393.33,10.91;16,112.66,466.34,393.33,10.91;16,112.33,479.89,29.19,10.91" xml:id="b36">
	<monogr>
		<title level="m" type="main" coord="16,225.16,452.79,280.83,10.91;16,112.66,466.34,237.64,10.91">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06226</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="16,112.66,493.44,394.53,10.91;16,112.66,506.99,393.33,10.91;16,112.66,520.54,249.34,10.91" xml:id="b37">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01703</idno>
		<title level="m" coord="16,261.26,506.99,244.72,10.91;16,112.66,520.54,67.64,10.91">Pytorch: An imperative style, high-performance deep learning library</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="16,112.66,534.09,394.53,10.91;16,112.66,547.64,395.17,10.91;16,112.66,561.19,212.50,10.91" xml:id="b38">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<title level="m" coord="16,206.90,547.64,300.93,10.91;16,112.66,561.19,30.43,10.91">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
