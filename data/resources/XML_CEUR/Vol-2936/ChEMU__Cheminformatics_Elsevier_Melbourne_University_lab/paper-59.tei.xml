<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.69,84.74,379.24,15.42;1,89.29,106.66,136.26,15.42">A pipelined approach to Anaphora Resolution in Chemical Patents</title>
				<funder>
					<orgName type="full">Dow Chemical</orgName>
				</funder>
				<funder ref="#_zfMJb45 #_twHDrkK #_UNcXJbe #_k7fe7hh">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,53.29,11.96"><forename type="first">Ritam</forename><surname>Dutt</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Avenue</addrLine>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,160.56,134.97,65.33,11.96"><forename type="first">Sopan</forename><surname>Khosla</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Avenue</addrLine>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,262.29,134.97,65.39,11.96"><forename type="first">Carolyn</forename><surname>Ros√©</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Avenue</addrLine>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.69,84.74,379.24,15.42;1,89.29,106.66,136.26,15.42">A pipelined approach to Anaphora Resolution in Chemical Patents</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">08EE55CBD303E2C7CA5F03860C09D007</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information Extraction</term>
					<term>Anaphora Resolution</term>
					<term>Chemical Patents</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present our pipelined approach for the sub-task of anaphora resolution in chemical patents as part of the ChEMU shared task at CLEF, 2021. Our approach consists of independently trained mention extraction and relation classification modules. For the former, we set up a BERT-CRF and leverage the BIO scheme to represent the mentions. We include a post-processing step after mention extraction to correct boundary errors and handle nested mentions. For relation classification, we develop a BERT-based model that captures the context between the two candidate mentions to predict the relation between the two. Our final submission ensembles BERT models pretrained on different types of clinical data and achieves a Strict F1 of 0.785 on the official test set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Chemical patents play a crucial role in disseminating information about the synthesis, properties, and applications of new chemical compounds <ref type="bibr" coords="1,290.62,393.13,15.96,10.91" target="#b0">[1,</ref><ref type="bibr" coords="1,309.30,393.13,7.64,10.91" target="#b1">2]</ref>. The rapid publication pace over the past decade necessitates the need of automated techniques to extract semi-structured knowledge from the patent text <ref type="bibr" coords="1,176.46,420.23,11.23,10.91" target="#b2">[3,</ref><ref type="bibr" coords="1,189.64,420.23,7.49,10.91" target="#b3">4]</ref>, such as components and process conditions corresponding to chemical reactions.</p><p>A key step in understanding how chemical reactions (in patent text) involves identifying anaphoric dependencies between entities mentioned in the reaction <ref type="bibr" coords="1,394.29,460.88,13.12,10.91" target="#b2">[3]</ref>. These dependencies involve co-reference relations where different surface mentions refer to the same chemical entity, or bridging relations where different entities interact amongst themselves in a particular manner. The first instance in Table <ref type="table" coords="1,205.29,501.52,4.97,10.91">1</ref> highlights a co-referent relation between N-methylpyrrolidone and NMP. Likewise, the second instance talks about how the stirring event transforms the mixture. We describe them in detail in ¬ß2.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Instances of text snippets and their corresponding anaphora relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text snippet Relation</head><p>[N-methylpyrrolidone] 1 [(NMP)] 2 was stirred for 1 day over CaH2 and finally distilled off.</p><p>CR(ùê∏ùëõùë° 1 , ùê∏ùëõùë° 2 )</p><p>[The mixture] 1 was stirred at room temperature for 1 day. A 2 molL aqueous solution of hydrochloric acid was added to [the mixture] 2 .</p><p>TR(ùê∏ùëõùë° 1 , ùê∏ùëõùë° 2 )</p><p>[Acetic acid (9.8 ml)] 1 and [water (4.9 ml)] 2 were added to [the solution of Compound (4) (0.815 g, 1.30 mmol)in THF (4.9 ml)] 3 . [The mixture] 4 was stirred for 3 hrs at 50 ùëú ùê∂ and then cooled to 0 ùëú ùê∂. We present a pipe-lined approach to solve anaphoric resolution in chemical patents, comprising of two key phases of Mention Extraction and Relation Classification. We perform ensembling after each of these two phase to reduce spurious corrleations and improve prediction. We also incorporate a post-processing module after extracting mentions to handle boundary issues, discontinuous, and nested spans. We describe our methodology in detail in ¬ß3 and Figure <ref type="figure" coords="2,501.06,381.54,5.17,10.91" target="#fig_0">1</ref> illustrates a pictorial representation of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RA(ùê∏ùëõùë°</head><p>We provide the experimental details in ¬ß4 and present our results in ¬ß5. Our proposed approach achieves a performance of 0.804 F1 score and 0.785 F1 score on the validation and test set respectively for the strict matching paradigm, successfully beating the proposed baseline <ref type="bibr" coords="2,89.29,449.28,11.28,10.91" target="#b2">[3]</ref>. For relaxed or inexact match, our scores are substantially higher by almost a margin of 0.07 F1 scores. We conclude and present future ideas in ¬ß6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task Description</head><p>We focus on the sub-task of anaphora resolution in chemical patents as part of the ChEMU shared task at CLEF, 2021 <ref type="foot" coords="2,209.23,533.25,3.71,7.97" target="#foot_0">1</ref> . The task of anaphora resolution seeks to identify the nature of anaphoric dependencies between mentions/expressions in chemical patents. Prior work <ref type="bibr" coords="2,475.72,548.55,12.70,10.91" target="#b2">[3]</ref> has investigated the following 5 anaphoric dependencies in chemical patents. We present instances of those in Table <ref type="table" coords="2,165.99,575.65,3.74,10.91">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Coreference (CR):</head><p>The relationship between expressions or mentions wherein they refer to the same chemical Mention. 2. Reaction Associated (RA): The relationship between a chemical compound and its immediate sources via a mixing/chemical process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Transformed (TR) :</head><p>The relationship between expressions or mentions, which have undergone physical changes (e.g., pH and temperature) but have the same chemical composition. 4. Work Up (WU): The relationship between chemical compounds that were used for isolating or purifying mentions, and their corresponding outputs. 5. Contained (CN): The association between chemical compounds and the equipment in which they are placed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>We outline the details of a pipelined architecture to anaphora resolution in this section. Our approach consists of two major steps of Mention Extraction and Relation Classification. In the Mention Extraction phase, we identify all possible mentions from the patent text, whereas in the Relation Classification phase, we infer whether a given pair of mentions have any anaphoric dependency between them. We describe the neural architecture we have employed for these two phases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Mention Extraction</head><p>Prior work has demonstrated the success of neural architectures in extracting chemical and bio-medical mentions <ref type="bibr" coords="3,187.00,360.62,11.32,10.91" target="#b1">[2]</ref>, spans of chemical reaction <ref type="bibr" coords="3,323.94,360.62,12.73,10.91" target="#b4">[5]</ref> and the specific roles of mentions in a reaction <ref type="bibr" coords="3,128.33,374.17,11.43,10.91" target="#b3">[4]</ref>.</p><p>Self attention + Feed-forward x 12</p><p>After cooling, the solid was .. In this task, we consider any span of text snippet which was annotated either as an antecedent or an anaphora as a mention. Based on the annotation corpus of <ref type="bibr" coords="3,367.25,662.69,11.28,10.91" target="#b2">[3]</ref>, mentions include quantified chemical compounds (0.51 g of methanol, K2CO3 (300 mg, 2.2 mmol)), proper nouns (DMF, (2,6-dichloro-4-fluorophenyl)hydrazine hydrochloride ), identifiers (5i, 4a), pronouns (it, they,) and noun phrases (the solvent, an autoclave). We note that approximately 3% of mentions in the dataset have dis-continuous spans, and we leverage post-processing techniques to deal with such spans.</p><p>We thus, model the task of mention extraction as a sequence labeling task. For this phase, we encode the longest contiguous span of text that includes these individual, discontinuous spans as the span of the given mention. Based on the recent success of transformer-based modules like <ref type="bibr" coords="4,108.22,195.36,12.79,10.91" target="#b5">[6]</ref> in information extraction <ref type="bibr" coords="4,238.52,195.36,11.32,10.91" target="#b6">[7,</ref><ref type="bibr" coords="4,252.57,195.36,7.46,10.91" target="#b7">8,</ref><ref type="bibr" coords="4,262.76,195.36,7.55,10.91" target="#b8">9]</ref>, we employ a similar approach in our case. We use a transformer-based encoder to encode the mentions and then pass the encoding through a Linear Conditional Random Field (CRF) <ref type="bibr" coords="4,237.99,222.46,16.35,10.91" target="#b9">[10]</ref>. An overview of the Mention Extraction architecture is shown in Figure <ref type="figure" coords="4,164.29,236.01,3.74,10.91" target="#fig_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Relation Classification</head><p>After cooling, the solid was .. We present an overview of the Relation Classification architecture in Figure <ref type="figure" coords="4,444.79,559.79,3.79,10.91" target="#fig_3">3</ref>. For a given pair of mentions, we define the context for a pair of mentions as the sequence of sentences that has the mention pair. We pass the context through a transformer Based encoder, and use mean-pooling over the individual mention tokens to obtain a representation corresponding to that mention. We concatenate the representations of the two mention spans and project it through a linear layer of 6 classes. These correspond to the 5 anaphoric dependencies and the No-Relation class if there is no dependency between the pair of mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Mention Extraction</head><p>For the task of mention extraction, we experiment with several transformer-Based encoder modules, such as BERT <ref type="bibr" coords="5,197.01,145.80,11.56,10.91" target="#b5">[6]</ref>, Clinical BERT (trained on clinical notes) <ref type="bibr" coords="5,399.73,145.80,18.04,10.91" target="#b10">[11]</ref> and Pubmed-BERT (trained on Pubmed abstracts) <ref type="bibr" coords="5,229.80,159.35,16.41,10.91" target="#b11">[12]</ref>. Moreover, since chemical compounds are often several characters long, a single compound could be decomposed into several tokens. To circumvent this tokenization issue, we include the "LONG TOKEN" similar to <ref type="bibr" coords="5,392.38,186.44,11.58,10.91" target="#b1">[2]</ref>, as a special token, to subsume the remaining tokens of a compounds beyond a certain length. For our experiments, the length is kept to 25. We use the BIO (Beginning Inside Outside) scheme to represent the mentions. For example "the residue is heated" will be converted to "B-ENT I-ENT O O".</p><p>We evaluate mention extraction in terms of precision, recall, and F1 score, for both exact (strict) and inexact (relaxed) match similar to <ref type="bibr" coords="5,291.21,254.19,11.39,10.91" target="#b3">[4]</ref>. We use the BRAT evaluation script provided by the organizers to compute the scores. We ran our models using the huggingface transformers library in PyTorch, with a batch size of 8, a learning rate of 1e-5, a dropout of 0.1, Adam optimizer, and patience of 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Post-Processing</head><p>To correct boundary errors and extract nested spans, we further post-process the output from the neural mention extractor using several rule-based sieves. The sieves were chosen after close inspection on the validation data and are described in detail in the subsequent section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Relation Classification</head><p>For the task of relation classification, we experiment with several transformer-Based encoder modules, namely BERT-Base and BERT-large <ref type="bibr" coords="5,293.39,434.94,11.49,10.91" target="#b5">[6]</ref>, Clinical BERT <ref type="bibr" coords="5,377.09,434.94,16.31,10.91" target="#b10">[11]</ref>, Pubmed-BERT <ref type="bibr" coords="5,468.73,434.94,17.98,10.91" target="#b11">[12]</ref> and BioBERT <ref type="bibr" coords="5,132.75,448.49,16.34,10.91" target="#b12">[13]</ref>. Moreover, since we have to check anaphoric dependency between all possible pairs of entities during validation and testing, it is imperative to incorporate negative instances during training. Thus, all pairs of entities which do not have an anaphoric dependency between them are taken as negative instances and assigned the "NO RELATION" label. We also experiment by varying the proportion of negative instances during training. Similar to mention extraction, we use the BRAT evaluation script provided by the organizes. We find the precision, recall, and F1 score for the relation classification task, similar to <ref type="bibr" coords="5,375.64,529.78,11.43,10.91" target="#b2">[3]</ref>.</p><p>We ran our relation classification models using the huggingface transformers library in PyTorch, with a batch size of 16, a learning rate of 3e-5, a dropout of 0.1, Adam optimizer, and patience of 5. We curated negative samples by pairing mentions that were not more than 10 mentions apart from each other in the patent document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ensembling</head><p>We perform ensembling twice, once after the mention extraction phase and once after the relation classification phase. We carry out majority voting over the outputs of five models and consider only those outputs which have been predicted by at least three models. The outputs  <ref type="bibr" coords="6,379.57,326.54,16.17,10.91" target="#b13">[14]</ref>, and has been employed for several tasks <ref type="bibr" coords="6,164.13,340.09,16.43,10.91" target="#b14">[15,</ref><ref type="bibr" coords="6,183.28,340.09,12.55,10.91" target="#b15">16,</ref><ref type="bibr" coords="6,198.56,340.09,7.57,10.91" target="#b8">9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Mention Extraction</head><p>We note the results for mention extraction in Table <ref type="table" coords="6,314.37,419.69,3.66,10.91" target="#tab_1">2</ref>. At the outset, we observe that the models achieve almost 0.98 F1 score for the inexact (relaxed) match. However, they suffer almost a 0.09 drop in F1 score under the exact (strict) match evaluation. A majority of the errors occurs due to nested spans or discontinuous spans (see Example 4 in the post-processing sub-section). Another common issue is the omission or inclusion of tokens at the beginning/end of the span like (see Examples 1,2, and 3 in the post-processing sub-section), which we refer to as "boundary issues". Since the entities are multi-faceted and can range from simple noun-phrases and identifiers to complex chemical quantifiers, it makes the task exceedingly challenging. We also observe that incorporating tokenizers corresponding to bio-medical and clinical domains perform slightly better than the uncased BERT model. The best performance is observed for PubMed-BERT-Long with F1 score of 0.890, as opposed to 0.865 for the uncased BERT-Long model. Inclusion of the "LONG TOKEN" is shown to benefit PubMed-BERT and BERT-uncased but fares worse for the Clinical-BERT model.</p><p>Unsurprisingly, ensembling over these 5 models achieves the highest score in the exact match setting across all three metrics. We use the entities extracted from the ensemble setting as the final ones and post-process them before the relation classification phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>Strict match for the Relation prediction task using the gold mentions and the predicted mentions after our post-processing step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold entities</head><p>Predicted </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Post-Processing</head><p>For post-processing, we pass the outputs of the mention extraction module through several sequential rule-based sieves:</p><p>1. If the extracted mention ends with strings like {' and', ' under', ' or', ' over'}, remove them from the mention. E.g. [Alcohol and] 1 ‚Üí [Alcohol] 1 and 2. If a mention is preceded by an article like {'a', 'the'}, include that article in the mention. 3. If the extracted mention ends with {' with', ' of', ' in'} and there is an adjoining mention after it, combine the two. E.g.</p><p>[ethanol in] 1 [the reaction mixture] 2 ‚Üí [ethanol in the reaction mixture] 1 4. We observed that often the patent documents refer to compounds with an ID. This ID is annotated as a coreferent mention to the actual compound. E.g. In order to extract these coreferents from outputs with such patterns we identify instances that follow the below template. If predicted m2 starts with a word (w1) which follows regex ([0-9]+[a-z]+), and contains a second word (w2) which starts with a {'('}, then combine m2 with m1 excluding w1 which is separated out as the coreferent.</p><p>We find that this post-processing substantially improves the performance (from 0.895 F1 to 0.922 F1) on the official mention extraction Strict metrics (Table <ref type="table" coords="7,347.94,547.86,3.56,10.91" target="#tab_1">2</ref>). Each of our sieves work towards increasing the number of exact matches between gold and system mentions. Furthermore, Sieve 3 and 4 also uncover new spans simultaneously impacting the Relaxed Match scores (from 0.967 F1 to 0.970 F1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Relation Classification</head><p>We present the performance of models for the relation classification task for both the gold and predicted mentions in Table <ref type="table" coords="7,216.27,651.79,3.74,10.91">3</ref>. We observe that pre-trained Pubmed-BERT and BioBERT fares slightly better than the uncased BERT-Base and BERT-Large model, highlighting again the Models that are trained on 5% and 10% of the total negative samples achieve an F1-Score (gold entities) of 0.762 and 0.794 respectively, around 0.15 F1 points below their 100% counterpart. A majority of the misclassification errors takes place when an anaphoric dependency between a pair of mentions is predicted as "NO RELATION" and vice versa, since the negative classes account for around 87% of all labels. The only other instance of misclassification takes place where the RA relation is predicted as WU, since both describe associations between chemical compounds. Unlike <ref type="bibr" coords="8,131.75,388.95,11.27,10.91" target="#b2">[3]</ref>, our pair-wise approach can circumvent the problems of discontinuous and nested spans and hence we include those mentions. Nevertheless, we note how the errors in the mention extraction phase propagate downstream and downgrade the relation classification performance on the predicted entities. This results in an average score of 0.79 F1 for predicted entities (a drop in approximately 0.12 F1 points). While it would be prudent ideally to carry out the two phases in a joint fashion like <ref type="bibr" coords="8,218.55,456.69,12.78,10.91" target="#b2">[3]</ref> (to prevent cascading errors), the crucial post-processing step of fixing boundary issues and extracting additional nested mentions, necessitates the pipelined approach.</p><p>In fact, our architecture beats the transformer baseline that performs joint co-reference and bridging on both the validation and test set by 0.03 F1 score on the exact match metric. The boost for relaxed match is substantially higher, with our model outperforming the baseline by approximately 0.07 F1 score on both validation and test. Moreover, ensembling over the different models boosts the performance further by 0.01 F1 for both the gold and predicted entities.</p><p>We report the performance for the 5 individual anaphoric relations in Table <ref type="table" coords="8,422.29,578.64,3.66,10.91" target="#tab_3">4</ref>. Coreference (CR) relations with their nuanced rules and long-term dependencies have the poorest performance <ref type="bibr" coords="8,89.29,605.73,11.45,10.91" target="#b2">[3]</ref>, whereas the bridging relations being more local and specific in nature, fared considerably better. We acknowledge there is immense scope for improvement and posit how incorporating additional information like events or entity types can help bolster performance. We defer this exploration for future work.</p><p>Our final performance on the validation set was 0.804 F1 and 0.887 F1 for the strict and relaxed match respectively. Likewise, our performance on the test set was 0.785 and 0.872 F1 for strict and relaxed match. We are currently ranked the first in the shared task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Resolving anaphora dependencies in chemical patents plays a key role in understanding the nuances of how chemical reactions are described, and the interactions between participating entities. We describe a pipelined approach to address this challenge using independently trained mention extraction and relation classification modules. Such a design choice facilitates the inclusion of a rule-based post-processing module to handle boundary errors, and discontinuous/nested spans. We achieve a Strict F1 score of 0.785 and a relaxed F1 score of 0.872 on the official test set, significantly outperforming the baseline.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,89.29,588.40,418.22,8.93"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Flowchart outlining our pipelined methodology for anaphora resolution in chemical patents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="1,89.29,627.79,367.14,7.99;1,100.50,638.09,340.69,8.97"><head>CLEF 2021 -</head><label>2021</label><figDesc>Conference and Labs of the Evaluation Forum, September 21-24, 2021, Bucharest, Romania rdutt@cs.cmu.edu (R. Dutt*); sopank@cs.cmu.edu (S. Khosla*); cprose@cs.cmu.edu (C. Ros√©)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,89.29,607.07,416.69,8.93;3,89.29,619.07,223.94,8.87"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Extracting mentions using a transformer-Based encoder and CRF. The highlighted part in the text (in green) correspond to the extracted entities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,89.29,494.16,417.29,8.93;4,89.29,506.17,416.69,8.87;4,89.29,518.12,416.69,8.87;4,89.29,530.08,102.93,8.87"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Relation classification between pairs of mentions. The parts of the text highlighted (in green) refer to to entity mentions and their corresponding embeddings are shaded. Mean pooling over the embeddings yield the final representation of the mention, and pairwise relation classification of the mentions are carried out.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,404.23,433.43,103.71,9.57;7,116.56,446.98,192.42,10.63;7,309.48,446.25,196.50,10.91;7,116.56,459.79,389.42,11.36;7,116.56,474.28,40.00,10.43;7,157.07,473.34,2.35,10.91"><head>[7-fluorobenzofuran- 3 (</head><label>3</label><figDesc>2H)-one] 1 [84c] 2 [(340 mg, 2.2 mmol)] 1 . But since our neural model can only extract contiguous mentions, it outputs [7-fluorobenzofuran-3(2H)-one] 1 [84c (340 mg, 2.2 mmol)] 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,97.10,191.94,391.93,106.96"><head></head><label></label><figDesc>1 ,ùê∏ùëõùë° 4 ), RA(ùê∏ùëõùë° 2 ,ùê∏ùëõùë° 4 ), RA(ùê∏ùëõùë° 3 , ùê∏ùëõùë° 4 ) [The mixture] 1 was extracted with [ethyl acetate] 2 for 3 times. [The combined organic layer] 3 was washed with water and saturated aqueous sodium chloride. WU(ùê∏ùëõùë° 1 ,ùê∏ùëõùë° 3 ), WU(ùê∏ùëõùë° 2 ,ùê∏ùëõùë° 3 )</figDesc><table coords="2,97.10,267.95,389.43,30.95"><row><cell cols="2">[Pyrazinecarboxylic acid (152.8 mg, 1.23 mmol, 1 eq)] 1 and</cell><cell></cell><cell></cell><cell>CN(ùê∏ùëõùë° 1 ,ùê∏ùëõùë° 3 ),</cell></row><row><cell>[H-Phe-OtBu-HCl(253.8 mg, 0.98 mmol, 0.8 eq)] 2 were</cell><cell>charged</cell><cell>into</cell><cell>an</cell><cell>CN(ùê∏ùëõùë° 2 ,ùê∏ùëõùë° 3 )</cell></row><row><cell>[eggplant flask] 3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,88.99,90.49,418.66,246.97"><head>Table 2</head><label>2</label><figDesc>Strict and Relaxed match of the Mention Extraction module using different tokenizers in terms of Precision, Recall, and F1 score.</figDesc><table coords="6,136.65,133.96,324.83,138.21"><row><cell></cell><cell>Strict</cell><cell>Relaxed</cell></row><row><cell>Method</cell><cell>Precision Recall F1</cell><cell>Precision Recall F1</cell></row><row><cell>BERT-Base-Long</cell><cell>0.877 0.853 0.865</cell><cell>0.977 0.949 0.963</cell></row><row><cell>Clinical-BERT</cell><cell>0.897 0.872 0.884</cell><cell>0.981 0.953 0.967</cell></row><row><cell>Pubmed-BERT</cell><cell>0.888 0.860 0.874</cell><cell>0.969 0.939 0.954</cell></row><row><cell>Pubmed-BERT-Long</cell><cell>0.909 0.871 0.890</cell><cell>0.989 0.947 0.967</cell></row><row><cell>Clinical-BERT-Long</cell><cell>0.881 0.863 0.872</cell><cell>0.974 0.955 0.964</cell></row><row><cell>Ensemble</cell><cell>0.915 0.876 0.895</cell><cell>0.988 0.947 0.967</cell></row><row><cell>+ Post-processing</cell><cell>0.943 0.903 0.922</cell><cell>0.991 0.949 0.970</cell></row></table><note coords="6,89.29,299.44,416.69,10.91;6,89.29,312.99,416.69,10.91;6,89.29,326.54,287.55,10.91"><p>correspond to the extracted mention-span for mention extraction, and over pairs of extracted span and their corresponding relation label for relation classification. Ensembling has been proven to reduce spurious correlations and improve performance</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,88.99,90.49,417.00,214.53"><head>Table 4</head><label>4</label><figDesc>Strict and Relaxed match for the complete Anaphora resolution task on the test set.</figDesc><table coords="8,317.50,122.88,140.51,10.07"><row><cell>Strict</cell><cell>Relaxed</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.93,671.01,184.94,8.97"><p>http://chemu.eng.unimelb.edu.au/chemu/overview</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>We thank the anonymous reviewers for their insightful comments. This was funded in part by <rs type="funder">NSF</rs> grants (<rs type="grantNumber">IIS 1917668</rs> and <rs type="grantNumber">IIS 1822831</rs>, <rs type="grantNumber">IIS 1949110</rs>, and <rs type="grantNumber">IIS 1546393</rs>) and funding from <rs type="funder">Dow Chemical</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_zfMJb45">
					<idno type="grant-number">IIS 1917668</idno>
				</org>
				<org type="funding" xml:id="_twHDrkK">
					<idno type="grant-number">IIS 1822831</idno>
				</org>
				<org type="funding" xml:id="_UNcXJbe">
					<idno type="grant-number">IIS 1949110</idno>
				</org>
				<org type="funding" xml:id="_k7fe7hh">
					<idno type="grant-number">IIS 1546393</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,384.79,394.53,10.91;9,112.66,398.34,393.33,10.91;9,112.66,411.89,134.41,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,231.65,398.34,274.34,10.91;9,112.66,411.89,30.61,10.91">Automatic identification of relevant chemical compounds from patents</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Akhondi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schw√∂rer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Toomey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Nau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ilchmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sheehan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Irmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bobach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,151.57,411.89,40.56,10.91">Database</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,425.43,394.53,10.91;9,112.66,438.98,393.32,10.91;9,112.26,452.53,394.93,10.91;9,112.28,466.08,395.00,10.91;9,112.31,479.63,322.09,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,169.79,438.98,336.20,10.91;9,112.26,452.53,79.40,10.91">Improving chemical named entity recognition in patents with contextualized word embeddings</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Akhondi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Druckenbrodt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Verspoor</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-5035</idno>
		<ptr target="https://www.aclweb.org/anthology/W19-5035.doi:10.18653/v1/W19-5035" />
	</analytic>
	<monogr>
		<title level="m" coord="9,223.19,452.53,283.99,10.91;9,112.28,466.08,186.71,10.91">Proceedings of the 18th BioNLP Workshop and Shared Task, Association for Computational Linguistics</title>
		<meeting>the 18th BioNLP Workshop and Shared Task, Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="328" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,493.18,393.71,10.91;9,112.66,506.73,393.33,10.91;9,112.41,520.28,394.87,10.91;9,112.66,533.83,395.01,10.91;9,112.66,547.38,270.55,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,440.64,493.18,65.73,10.91;9,112.66,506.73,286.94,10.91">ChEMU-ref: A corpus for modeling anaphora resolution in the chemical domain</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Druckenbrodt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Akhondi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Verspoor</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2021.eacl-main.116" />
	</analytic>
	<monogr>
		<title level="m" coord="9,423.32,506.73,82.67,10.91;9,112.41,520.28,394.87,10.91;9,112.66,533.83,257.84,10.91">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Association for Computational Linguistics</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1362" to="1375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,560.93,394.53,10.91;9,112.66,574.48,393.33,10.91;9,112.66,588.02,393.33,10.91;9,112.66,601.57,168.28,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,304.83,574.48,201.16,10.91;9,112.66,588.02,201.68,10.91">Chemu: named entity recognition and event extraction of chemical reactions from patents</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yoshikawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Druckenbrodt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hoessel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Akhondi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,337.42,588.02,168.57,10.91;9,112.66,601.57,38.01,10.91">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="572" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,615.12,395.17,10.91;9,112.26,628.67,393.73,10.91;9,112.33,642.22,395.50,10.91;9,112.66,655.77,394.62,10.91;9,112.66,669.32,203.32,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,203.29,628.67,183.21,10.91">Detecting chemical reactions in patents</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yoshikawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Druckenbrodt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Akhondi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Verspoor</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/U19-1014" />
	</analytic>
	<monogr>
		<title level="m" coord="9,418.74,628.67,87.24,10.91;9,112.33,642.22,395.50,10.91;9,112.66,655.77,195.59,10.91">Proceedings of the The 17th Annual Workshop of the Australasian Language Technology Association, Australasian Language Technology Association</title>
		<meeting>the The 17th Annual Workshop of the Australasian Language Technology Association, Australasian Language Technology Association<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="100" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,86.97,393.33,10.91;10,112.66,100.52,393.33,10.91;10,112.66,114.06,393.32,10.91;10,112.66,127.61,357.71,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,353.43,86.97,152.55,10.91;10,112.66,100.52,186.91,10.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,327.87,100.52,178.11,10.91;10,112.66,114.06,393.32,10.91;10,112.66,127.61,102.30,10.91">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="10,112.66,141.16,393.33,10.91;10,112.66,154.71,393.59,10.91;10,112.66,168.26,146.44,10.91" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Newman-Griffis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Dutt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Rose</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00460</idno>
		<title level="m" coord="10,385.28,141.16,120.71,10.91;10,112.66,154.71,356.14,10.91">Improving broad-coverage medical entity linking with semantic type prediction and large-scale datasets</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,181.81,393.33,10.91;10,112.66,195.36,393.33,10.91;10,112.14,208.91,395.05,10.91;10,112.66,222.46,395.01,10.91;10,112.66,236.01,138.14,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,247.95,181.81,258.04,10.91;10,112.66,195.36,251.10,10.91">Biomedical relation extraction with pre-trained language representations and minimal task-specific architecture</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Thillaisundaram</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Togia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5713</idno>
		<ptr target="https://www.aclweb.org/anthology/D19-5713.doi:10.18653/v1/D19-5713" />
	</analytic>
	<monogr>
		<title level="m" coord="10,396.43,195.36,109.56,10.91;10,112.14,208.91,390.73,10.91">Proceedings of The 5th Workshop on BioNLP Open Shared Tasks, Association for Computational Linguistics</title>
		<meeting>The 5th Workshop on BioNLP Open Shared Tasks, Association for Computational Linguistics<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="84" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,249.56,393.33,10.91;10,112.66,263.11,393.32,10.91;10,112.66,276.66,307.21,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,249.44,249.56,256.55,10.91;10,112.66,263.11,135.00,10.91">Melaxtech: A report for clef 2020-chemu task of chemical reaction extraction from patent</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W Y R Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,270.35,263.11,235.63,10.91;10,112.66,276.66,93.98,10.91">Working Notes of CLEF 2020-Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="10,259.81,276.66,129.93,10.91">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2696</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,290.20,393.53,10.91;10,112.66,303.75,393.33,10.91;10,112.66,317.30,247.38,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,275.22,290.20,230.98,10.91;10,112.66,303.75,239.48,10.91">Conditional random fields: Probabilistic models for segmenting and labeling sequence data: proceedings</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,355.07,303.75,150.92,10.91;10,112.66,317.30,75.67,10.91">of the 18th international conf. on machine learning</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,330.85,394.53,10.91;10,112.66,344.40,393.33,10.91;10,112.66,357.95,394.52,10.91;10,112.66,371.50,395.01,10.91;10,112.66,385.05,138.14,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,112.66,344.40,194.37,10.91">Publicly available clinical BERT embeddings</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Alsentzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-H</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jindi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mcdermott</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-1909</idno>
		<ptr target="https://www.aclweb.org/anthology/W19-1909.doi:10.18653/v1/W19-1909" />
	</analytic>
	<monogr>
		<title level="m" coord="10,330.71,344.40,175.27,10.91;10,112.66,357.95,330.65,10.91">Proceedings of the 2nd Clinical Natural Language Processing Workshop, Association for Computational Linguistics</title>
		<meeting>the 2nd Clinical Natural Language Processing Workshop, Association for Computational Linguistics<address><addrLine>Minneapolis, Minnesota, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="72" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,398.60,393.32,10.91;10,112.66,412.15,393.33,10.91;10,112.66,425.70,394.52,10.91;10,112.66,439.25,394.51,10.91;10,112.36,455.24,68.18,7.90" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,213.90,398.60,292.09,10.91;10,112.66,412.15,266.92,10.91">Transfer learning in biomedical natural language processing: An evaluation of BERT and ELMo on ten benchmarking datasets</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-5006</idno>
		<ptr target="https://www.aclweb.org/anthology/W19-5006.doi:10.18653/v1/W19-5006" />
	</analytic>
	<monogr>
		<title level="m" coord="10,402.21,412.15,103.78,10.91;10,112.66,425.70,347.54,10.91">Proceedings of the 18th BioNLP Workshop and Shared Task, Association for Computational Linguistics</title>
		<meeting>the 18th BioNLP Workshop and Shared Task, Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,466.34,395.16,10.91;10,112.66,479.89,394.53,10.91;10,112.66,493.44,134.07,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,366.98,466.34,140.85,10.91;10,112.66,479.89,277.65,10.91">Biobert: A pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,399.65,479.89,66.92,10.91">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
			<pubPlace>Oxford, England</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,506.99,395.17,10.91;10,112.66,520.54,302.91,10.91" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09816</idno>
		<title level="m" coord="10,205.29,506.99,302.54,10.91;10,112.66,520.54,120.81,10.91">Towards understanding ensemble, knowledge distillation and selfdistillation in deep learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,534.09,393.33,10.91;10,112.66,547.64,394.53,10.91;10,112.66,561.19,45.01,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,159.64,534.09,270.31,10.91">Emotionx-ar: Cnn-dcnn autoencoder based emotion classifier</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Khosla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,452.14,534.09,53.85,10.91;10,112.66,547.64,364.74,10.91">Proceedings of the sixth international workshop on natural language processing for social media</title>
		<meeting>the sixth international workshop on natural language processing for social media</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="37" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,574.74,394.62,10.91;10,112.66,588.29,394.62,10.91;10,112.66,601.84,387.33,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,354.18,574.74,153.10,10.91;10,112.66,588.29,375.08,10.91">Ltiatcmu at semeval-2020 task 11: Incorporating multi-level features for multi-granular propaganda span identification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Dutt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tsvetkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,112.66,601.84,289.30,10.91">Proceedings of the Fourteenth Workshop on Semantic Evaluation</title>
		<meeting>the Fourteenth Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1756" to="1763" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
