<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,405.82,15.42;1,89.29,106.66,223.32,15.42">Overview of LiLAS 2021 -Living Labs for Academic Search (Extended Overview)</title>
				<funder ref="#_YWJ4J5s">
					<orgName type="full">DFG</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,69.39,11.96"><forename type="first">Philipp</forename><surname>Schaer</surname></persName>
							<email>philipp.schaer@th-koeln.de</email>
							<affiliation key="aff0">
								<orgName type="institution">TH Köln -University of Applied Sciences</orgName>
								<address>
									<settlement>Cologne</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,171.29,134.97,60.58,11.96"><forename type="first">Timo</forename><surname>Breuer</surname></persName>
							<email>timo.breuer@th-koeln.de</email>
							<affiliation key="aff0">
								<orgName type="institution">TH Köln -University of Applied Sciences</orgName>
								<address>
									<settlement>Cologne</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,244.46,134.97,80.69,11.96"><forename type="first">Leyla</forename><forename type="middle">Jael</forename><surname>Castro</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">ZB MED -Information</orgName>
								<orgName type="institution">Centre for Life Sciences</orgName>
								<address>
									<settlement>Cologne</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,337.75,134.97,74.86,11.96"><forename type="first">Benjamin</forename><surname>Wolff</surname></persName>
							<email>wolff@zbmed.de</email>
							<affiliation key="aff1">
								<orgName type="department">ZB MED -Information</orgName>
								<orgName type="institution">Centre for Life Sciences</orgName>
								<address>
									<settlement>Cologne</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,425.21,134.97,76.74,11.96"><forename type="first">Johann</forename><surname>Schaible</surname></persName>
							<email>johann.schaible@gesis.org</email>
							<affiliation key="aff2">
								<orgName type="institution">GESIS -Leibniz Institute for the Social Sciences</orgName>
								<address>
									<settlement>Cologne</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,113.26,148.92,124.13,11.96"><forename type="first">Narges</forename><surname>Tavakolpoursaleh</surname></persName>
							<email>tavakolpoursaleh@gesis.org</email>
							<affiliation key="aff2">
								<orgName type="institution">GESIS -Leibniz Institute for the Social Sciences</orgName>
								<address>
									<settlement>Cologne</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,405.82,15.42;1,89.29,106.66,223.32,15.42">Overview of LiLAS 2021 -Living Labs for Academic Search (Extended Overview)</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">BA7B1F8A3D08E1F3E0B929A6F7910FF9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Living labs</term>
					<term>evaluation</term>
					<term>academic search</term>
					<term>dataset recommendation</term>
					<term>ad-hoc retrieval</term>
					<term>STELLA framwework</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Living Labs for Academic Search (LiLAS) lab aims to strengthen the concept of user-centric living labs for academic search. The methodological gap between real-world and lab-based evaluation should be bridged by allowing lab participants to evaluate their retrieval approaches in two real-world academic search systems from life sciences and social sciences. This overview paper outlines the two academic search systems LIVIVO and GESIS Search, and their corresponding tasks within LiLAS, which are ad-hoc retrieval and dataset recommendation. The lab is based on a new evaluation infrastructure named STELLA that allows participants to submit results corresponding to their experimental systems in the form of pre-computed runs and Docker containers that can be integrated into production systems and generate experimental results in real-time. Both submission types are interleaved with the results provided by the productive systems allowing for a seamless presentation and evaluation. The evaluation of results and a meta-analysis of the different tasks and submission types complement this overview.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="31" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="32" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The Living Labs for Academic Search (LiLAS) lab aims to strengthen the concept of user-centric living labs for the domain of academic search. By allowing lab participants to evaluate their retrieval approaches in two real-world academic search portals (called sites) from life sciences and social sciences, the methodological gap between real-world and lab-based evaluations is effectively reduced.</p><p>This gap is based on the different opportunities available to researchers in academia and industry. While industry-based research in the field of information retrieval (IR) has the opportunity to conduct experiments in-vivo -thanks to the availability of large systems, with a wide range and correspondingly large user base -these opportunities usually remain closed to academic research. In-vivo here describes the possibility to perform IR experiments integrated into real-world systems and to conduct experiments where the actual interaction with these systems takes place. It should be emphasized here that these are not classic user experiments in which the focus is on the individual interactions of users (e.g., to investigate questions of UI design), but rather aggregated usage data is collected in large quantities in order to generate reliable quantitative research results. The potential of living labs and real-world evaluation techniques has been shown in previous CLEF labs such as NewsREEL <ref type="bibr" coords="2,411.75,286.33,12.99,10.91" target="#b0">[1]</ref> and LL4IR <ref type="bibr" coords="2,477.73,286.33,11.59,10.91" target="#b1">[2]</ref>, or TREC OpenSearch <ref type="bibr" coords="2,176.44,299.88,11.58,10.91" target="#b2">[3]</ref>. In a similar vein, LiLAS is designed around the living lab evaluation concept and introduces different use cases in the broader field of academic search. Academic search solutions, which have to deal with the phenomena around the exponential growing rate <ref type="bibr" coords="2,89.29,340.53,13.00,10.91" target="#b3">[4]</ref> of scientific information and knowledge, tend to fall behind the real-world requirements and demands. The vast amount of scientific information does not only include traditional journal publication, but also a constantly growing amount of pre-prints, research datasets, code, survey data, and many other research objects. This heterogeneity and mass of documents and datasets introduces new challenges to the disciplines of information retrieval, recommender systems, digital libraries, and related fields. Academic search is a conceptional umbrella to subsume all these different disciplines and is well-known through (mostly domain-specific) search systems and portals such as PubMed, arXiv.org, or dblp. While those three are examples of open-science-friendly systems as they allow re-use of metadata, usage data and/or access to fulltext data, other systems such as Google Scholar or ResearchGate. The later offer no access at all to their internal algorithms and data and are therefore representatives of a closed-science (and commercial) mindset.</p><p>Progress in the field of academic search and its corresponding domains is usually evaluated by means of shared tasks that are based on the principles of Cranfield/TREC-style studies <ref type="bibr" coords="2,492.36,516.67,11.48,10.91" target="#b4">[5]</ref>. Typical shared tasks at the Conference and Labs of the Evaluation Forum (CLEF) and the Text Retrieval Conference (TREC) are based on the offline computation of results/runs missing a valuable link to real-world environments <ref type="bibr" coords="2,276.28,557.32,11.52,10.91" target="#b5">[6]</ref>. Most recently the TREC-COVID <ref type="bibr" coords="2,443.08,557.32,12.93,10.91" target="#b6">[7]</ref> evaluation campaign run by NIST attracted a high number of participants and showed the high impact of scientific retrieval tasks in the community. Within TREC-COVID a wide range of systems and retrieval approaches participated and generally showed the massive retrieval performance that recent BERT and other transformer-based machine learning approaches are capable of. However, classic vector-space retrieval was also highly successful using the well-known SMART system <ref type="foot" coords="2,501.96,623.31,3.71,7.97" target="#foot_0">1</ref>and showed the limitations of the test collection-based evaluation approach of TREC-COVID and the general need for innovation in the field of academic search and IR. Meta-evaluation studies of system performances in TREC and CLEF showed a need for innovation in IR evaluation <ref type="bibr" coords="3,482.72,100.52,11.23,10.91" target="#b7">[8,</ref><ref type="bibr" coords="3,496.43,100.52,7.49,10.91" target="#b8">9]</ref>. The field of academic search is no exception to this. The central concern of academic search is finding both relevant and high-quality documents. The question of what constitutes relevance in academic search is multilayered <ref type="bibr" coords="3,245.32,141.16,17.91,10.91" target="#b9">[10]</ref> and an ongoing research area.</p><p>In 2020 we held a first iteration of LiLAS as a so-called workshop lab. This year we provide participants exclusive access to real-world systems, their document base (in our case a very heterogeneous set of research articles and research data including, for instance, surveys), and the actual interactions including the query string and the corresponding click data (see overview on the setup in Figure <ref type="figure" coords="3,173.64,208.91,3.50,10.91" target="#fig_0">1</ref>). To foster different experimental settings we compile a set of head queries and candidate documents to allow pre-computed submissions. Using the STELLA-infrastructure, we allow participants to easily integrate their approaches into the real-world systems using Docker containers and provide the possibility to compare different approaches at the same time.</p><p>This extended lab overview is a longer version of the condensed LNCS lab overview <ref type="bibr" coords="3,466.67,263.11,16.09,10.91" target="#b10">[11]</ref>. It is structured as follows: In Sections 2 and 3 we introduce the two main use cases of LiLAS which are bond to the sites granting us access to their retrieval systems: LIVIVO and GESIS Search. In these two sections the systems, the provided datasets, and task are described. In Section 4 we outline the evaluation setup and STELLA, our living lab evaluation framework, and the two submission types, namely pre-computed runs and Docker container submissions. Section 4 also includes the description of the evaluation metrics used with in the lab and a short overview on the organizational structure of the lab. In Section 5 we introduce the participating groups and approaches. We outline the results of the evaluation rounds in Section 6 and conclude in Section 7. In addition to the condensed LNCS overview we included some more textual details and additional tables and figures in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Ad-hoc Search in LIVIO</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">LIVIVO Literature Search Portal</head><p>LIVIVO<ref type="foot" coords="3,123.64,476.44,3.71,7.97" target="#foot_1">2</ref>  <ref type="bibr" coords="3,130.56,478.19,17.93,10.91" target="#b11">[12]</ref> is a literature search portal developed and supported by ZB MED -Information Centre for Life Sciences.ZB MED is a non-profit organization providing specialized literature in Life Sciences at a national (German) and international level and hosting one of the largest stock of life science literature in Europe. Since 2015, ZB MED supports users including librarians, students, general practitioners and researchers with LIVIVO, a comprehensive and interdisciplinary search portal for Life Sciences.</p><p>LIVIVO integrates various literature resources from medicine, health, environment, agriculture and nutrition, covering a variety of scholarly publication types (e.g., conferences, preprints, peer-review journals). LIVIVO corpus includes about 80 million documents from more than 50 data sources in multiple languages (e.g., English, German, French). To better support its users, LIVIVO offers an end-user interface in English and German, an automatically and semantically enhanced search capability, and a subject-based categorization covering the different areas it supports (e.g., environment, agriculture, nutrition, medicine). Precision of search queries is # Sample head query { "qid": 1001, "qstr": "integrierte AND versorgung", "freq": 12 } # Sample documents { "DBRECORDID": "AGRISFR2016215853", "TITLE": ["Dissection ..."], "AUTHOR": ["Teyssèdre, Simon"], "SOURCE": ["Dissection ..."], "LANGUAGE": ["fra"], "DATABASE": ["AGRIS"] } # Sample candidate list { "qid": 1001, "qstr": "integrierte AND versorgung", "candidates": ["C951899619", "C676171", "848078", "C765841" ... ] }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2:</head><p>Examples for head queries, documents, and candidate lists for the LIVIVO system.</p><p>improved by using descriptors with semantic support; in particular, LIVIVO uses three multilingual vocabularies to this end (Medical Subject Headings MeSH,UMTHES,and AGROVOC.In addition to its search capabilities, LIVIVO also integrates functionality supporting inter-library loans at a national level in Germany. Since 2020, LIVIVO also offers a specialized collection on COVID-19<ref type="foot" coords="4,136.23,368.14,3.71,7.97" target="#foot_2">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">LIVIVO Dataset</head><p>For the LiLAS challenge, we prepared training and test datasets comprising head queries together with 100-document candidate list. In Figure <ref type="figure" coords="4,329.41,432.99,5.14,10.91">2</ref> we include an excerpt of the different elements included in the data. Data was formatted in JSON and presented as JSONL files to facilitate processing. Participating head queries were restricted to keywords-based search and keywords-based search plus AND, OR and NOT operators.</p><p>Head queries were assigned an identifier, namely qid, a query string, qstr and as an additional information the query frequency, freq. For each head query, a candidate list was also provided. Candidate lists include the query identifier as well as corresponding string, together with a list of 100 document identifiers (i.e. the native identifier used in the LIVIVO database).</p><p>In addition to head queries and candidate lists, we also provided a set of documents in LIVIVO corresponding to three of the major bibliographic scholarly databases so participants could create their own indexes. The document set contains metadata for approx. 35 million documents and is provided as a JSONL file. To reduce complexity and keep the data manageable, we decided to provide only the 6 most important data fields (DBRECORDID, TITLE, AUTHOR, SOURCE, LANGUAGE, DATABASE). Additional metadata and fulltext is mostly available from the original database curators. The aformentioned databases correspond to Medline, the National Library of Medicine's (NLM) bibliographic database for life sciences and biomedical information including about 20 million of abstracts; the NLM catalog, providing access to bibliographic data for over 1.4 million journals, books and similar data; and the Agricultural Science and Technology Information (AGRIS) database, a Food and Agriculture Organization of the United Nations initiative compiling information on agricultural research with 8.9 million structured bibliographical records on agricultural science and technology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Task</head><p>Finding the most relevant publications in relation to a head query remains a challenge in scholarly Information Retrieval systems. While most repositories or registries deal mostly with publications in English, LIVIVO, the production system used at LiLAS, supports multilingualism, adding an extra layer of complexity and presenting a challenge to participants.</p><p>The goal of this ad-hoc search task is supporting researchers to find the most relevant literature regarding a head query. Participants were asked to define and implement their ranking approach using as basis a multi-lingual candidate documents list. A good ranking should present users with the most relevant documents on top of the result set. An interesting aspect of this task is the multilingualism as multiple languages can be used to pose a query (e.g. English, German, French); however, regardless of the language used on the query, the retrieval can include documents in other languages as part of the result set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Research Data Recommendations in GESIS-Search</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">GESIS Search Portal</head><p>GESIS Search<ref type="foot" coords="5,149.44,390.67,3.71,7.97" target="#foot_3">4</ref> is a search portal for social science research data and open access publications developed and supported by GESIS -Leibniz Institute for the Social Sciences. GESIS is a member of the Leibniz Association with the purpose to promote social science research. It provides essential and internationally relevant research-based services for the social sciences, and as the largest European infrastructure institute for the social sciences, GESIS offers advice, expertise and services to scientists at all stages of their research projects.</p><p>GESIS Search aims at helping its users find appropriate scholarly information on the broad topic of social sciences <ref type="bibr" coords="5,195.97,487.27,16.41,10.91" target="#b12">[13]</ref>. To this end, it provides different types of information from the social sciences in multiple languages, comprising literature (114.7k publications), research data (84k), questions and variables (13.6k), as well as instruments and tools (440). A well-configured relevance ranking together with a well-defined structure and faceting mechanism allow to address the users' information needs, however, the most interesting aspect is the inclusion of scientific literature with research data. Typically, those types of information are accessible through different portals only, posing the problem of a lack of links between these two types of information. GESIS Search provides such an integrated access to research data as well as to publications. The information items are connected to each other based on links that are either manually created or automatically extracted by services that find data references in full texts. Such linking allows researchers to explore the connections between information items interactively.</p><p># Sample publication document { "id": "csa201419416", "title": "The Changing Value...", "abstract": "This article reviews...", "topic":[ "Children", "Child Mortality", "Values"] } # Sample research dataset document { "id": "DA3433", "title": "Kindheit, Jugend und Erwachsenwerden...", "title_en": "Childhood, Adolencence, and Becoming an Adult...", "abstract": "Die Hauptthemen der Studie...", "abstract_en": "The primary topics of the study...", "topic": ["Familie und Ehe", "Kinder"], "topic_en": ["Family life and marriage", "Children"] } # Sample candidate list { "s_id": "gesis-ssoar-62031", "candidate_docs": { "ZA6752": 0.1856689453125, "ZA6751": 0.183837890625, "ZA6749": 0.181396484375, "ZA6782": 0.1795654296875} } </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">GESIS Search Dataset</head><p>For LiLAS, we focus on all publications and research data comprised by GESIS Search. The publications are mostly in English and German, and are annotated with further textual metadata including title, abstract, topic, persons, and others. Metadata on research data comprises (among others) a title, topics, datatype, abstract, collection method, primary investigators, and contributors in English and/or German.</p><p>The data provided to participants comprises the mentioned metadata on social science literature and research data on social science topics comprised in the GESIS Search. In Figure <ref type="figure" coords="6,486.47,546.78,4.97,10.91" target="#fig_1">3</ref> we include an excerpt of the different elements included in the data. For the dataset recommendation task with pre-computed results (see details in Section 3.3), in addition, the participants were given the set of research data candidates that are recommended for each publication. This candidate set is computed based on context similarity between publications and research data. It is created by applying the TF-IDF score to vectorize the combination of title, abstract, and topics for each document type and computing the cosine similarities between cross-data types. It contains a list of research data for each publication with the highest similarities to the publication among other research data in the corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Task</head><p>Research data is of high importance in scientific research, especially when making progress in experimental investigations. However, finding useful research data can be difficult and cumbersome, even if using dataset search engines, such as Google Dataset Search<ref type="foot" coords="7,458.35,132.88,3.71,7.97" target="#foot_4">5</ref> . Another approach is scanning scientific publication for utilized or mentioned research data; however, this allows to find explicitly stated research data and not other research data relevant to the subject. To alleviate the situation, we aim at evolving the recommendation of appropriate research data beyond explicitly mentioned or cited research data. To this end, we propose to recommend research data based on publications of the user's interest between a scientific publication and possible research data candidates.</p><p>The main task is: given a seed-document, participants are asked to calculate the best fitting research data recommendations with regards to the seed-document. This resembles the use case of providing highly useful recommendations of research data relevant to the publication that the user is currently viewing. For example, the user is interested in the impact of religion on political elections. She finds a publication regarding that topic, which has a set of research data candidates covering the same topic.</p><p>The participants were allowed to submit pre-computed and live runs (see section 4.2 for more details). For submitting the pre-computed run, the participants also received a first candidate list comprising 1k publication each having a list of recommended research data. The task here was to re-rank this candidate list. On the contrary, for submitting the live runs, such a candidate list was not needed, as the recommended candidates needed to be calculated first. To do so, participants are provided metadata on publications as well as on the research data comprised in GESIS Search (see Section 3.1 for more details on the provided data).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">STELLA Infrastructure</head><p>The technical infrastructure and platform was provided by our evaluation service called STELLA (as illustrated in Figure <ref type="figure" coords="7,190.52,485.21,3.50,10.91" target="#fig_2">4</ref>). It complements existing shared task platforms by allowing experimental ranking and recommendation systems to be fully integrated into an evaluation environment, with no interference in the interaction between the users and the system as the whole process is transparent for users. Besides transparency and reproduciblity, one of the STELLA main principles is the integration of experimental systems as micro-services. More specifically, lab participants package their single systems as Docker containers that are bundled in a multicontainer application (MCA). Providers of academic research infrastructures deploy the MCA in their back-end and use the REST-API either to get ranking and recommendations or to post the corresponding user feedback that is mainly used for our evaluations. Intermediate evaluation results are available through a public dashboard service that is hosted on a central server, also part of the STELLA infrastructure. After authentication, participants can register experimental systems at this central instance and access feedback data that can be used to optimize their systems. In the following, each component of the infrastructure is briefly described to give the reader a better idea on how STELLA serves as a proxy for user-oriented experiments with ranking and recommendation systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Micro-services</head><p>As pointed out before, we request our lab participants to package their systems with Docker. For the sake of compatibility, we provide templates for these micro-services to implement minimal REST-based web services. Participants can adapt their systems to these templates as they see it fits as long as the pre-defined REST endpoints deliver technically correct responses. The templates can be retrieved from GitHub <ref type="foot" coords="8,262.29,484.94,3.71,7.97" target="#foot_5">6</ref> that is fundamental to our infrastructure. Not only the templates, but also the participant systems should be hosted in a public Git repository in order to be integrated into the MCA. As soon as the developments are done, the participants register their Git(Hub) URL at the central dashboard service of the infrastructure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Multi-container Application (MCA)</head><p>Once the experimental systems pass technical unit tests and sanity checks for selected queries and target items, they are ready to be deployed and evaluated via user interactions. To reduce the deployments costs for the site providers, the single experimental systems are bundled into an MCA which serves as the entry point to the infrastructure. The MCA handles the query distribution among the experimental systems and also sends user feedback data to the central server at regular intervals. After the REST-API corresponding to the MCA is connected to the search interface, the user traffic can be redirected to the MCA which will actually deliver the experimental results. We then interleave results of single experimental systems with those from the baseline system by using a Team-Draft-Interleaving (TDI) approach. This results in two benefits: 1) we prevent users from subpar retrieval results that also might affect the site's reputation, and 2), as shown before, interleaved results can be used to infer statistically significant results with less user data as compared to conventional A/B tests. The site providers rely on their own logging tools. STELLA expects a minimal set of information required when sending feedback; however, sites are free to add any additional JSON-formatted feedback information and interactions to the data payload, for instance logged clicks on site-specific SERP elements. The underlying source code of the MCA is hosted in a public GitHub repository <ref type="foot" coords="9,489.90,207.15,3.71,7.97" target="#foot_6">7</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Central Server</head><p>The central server instance of the infrastructure fulfills four functionalities: 1) participants, sites and administrators visit the server to register user accounts and systems; 2) a dashboard service provides visual analytics and first insights about the performance of experimental systems; 3) likewise, feedback data in the form of user interactions is stored in a database that can be downloaded for system optimizations and further evaluations; and 4) the server implements an automated update job of the MCA in order to integrate newly submitted systems if suitable.</p><p>Each MCA that is instantiated with legitimate credentials posts the logged user feedback to the central infrastructure server. Even though the infrastructure would allow continuous integration of newly submitted systems, we stuck to the official dates of round 1 and 2 when updating the MCAs at the sites. Due to moderate traffic, we run the central server on a lightweight single core virtual machine with 2GB RAM and 50GB storage capacity <ref type="foot" coords="9,370.92,391.81,3.71,7.97" target="#foot_7">8</ref> . More technical details about the implementations can be found in the public GitHub repository<ref type="foot" coords="9,384.46,405.36,3.71,7.97" target="#foot_8">9</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Submission Types</head><p>Participants can choose between two different submission types for both tasks (i.e. ad-hoc search and dataset recommendation). Similar to previous living labs, Type A are pre-computed runs that contain rankings and recommendations of the most frequent queries and the most frequently viewed document, respectively for reach task. Alternatively, it is possible to integrate the entire experimental system as a micro-service as part of a Type B submission. Both submission types have their own distinct merits as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Type A -Pre-computed Runs</head><p>Even though the primary goal of the STELLA framework is the integration of entire systems as micro-services, we offer the possibility to participate in the experiments by submitting system outputs, i.e. in the form of pre-computed rankings and recommendations. We do so for two reasons. First, the Type A submissions resemble those of previous living labs and serve as the baseline in order to evaluate the feasibility of our new infrastructure design. Second, we hope to lower technical barriers for some participants that want to submit the system outputs only. To make it easier for participants, we follow the familiar TREC run file syntax.</p><p>Depending on the chosen task, for each of the selected top-k queries or target items (identified by &lt;qid&gt;) a ranking or recommendation has to be computed in advance and then uploaded to the dashboard service. The upload process is tightly integrated into the GitHub ecosystem. Once the run file is uploaded, a new repository is automatically created from the previously described micro-template to which the uploaded run is committed. This is made possible thanks to GitHub API and access tokens. The run file itself is loaded as a pandas DataFrame into the running micro-service when the indexing endpoint is called. Upon request, the queries and target items are translated into the corresponding &lt;qid&gt; to filter the DataFrame. Due to manageable sizes of top-k queries and target items, the entire (compressed) run file can be uploaded to the repository and can be kept in memory after it is indexed as a DataFrame. As a technical safety check, we also integrate a dedicated verification tool <ref type="foot" coords="10,402.51,247.80,7.41,7.97" target="#foot_9">10</ref> in combination with GitHub Actions to verify that the uploaded files follow the correct syntax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Type B -Docker Containers</head><p>Running fully-fledged ranking and recommendation systems as micro-services overcomes the restrictions of responses that are limited to top-k queries and target items. Therefore, we offer the possibility to integrate the entire systems as a Docker container into the STELLA infrastructure as part of Type B submissions. As pointed out earlier, participants fork the template of the micro-services and adapt it to their experimental system. While Docker and the implementation of pre-defined REST endpoints are hard requirements, participants have total freedom w.r.t. the implementation and tools they use within their container, i.e., they do not even have to build up on the Python web application that is provided in the template. Solely, the index endpoint and, depending on the chosen task, either the ranking or recommendation endpoint have to deliver technically correct results. For this purpose, we include unit tests in the template repository that can be run in order to verify that the Docker containers can be properly integrated. If these unit tests pass, the participants register the URL of the corresponding Git repository at the dashboard service. Later on, the system URL is added to the build file of the MCA when an update process is invoked. If the MCA is updated at the sites, newly submitted experimental systems are build from the Dockerfiles in the specified repositories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Baseline Systems</head><p>LIVIVO baseline system for ranking is built on Apache Solr and Apache Lucene. The index contains about 80 million documents from more than 50 data sources in multiple languages and about 120 searchable fields ranging from basic data such as Title, Abstract, Authors to more specific such as MeSH-Terms, availability or OCR-Data. For ranking, LIVIVO uses the Lucene default ranker which is a variant of TF-IDF; on top of it, a custom boosting is added. Newer documents as well as search queries occurring in title or author fields are boosted. An exact match of search phrases in title-field results in a very high boosting. Moreover LIVIVO uses a Lucene-based plugin which executes NLP-tasks like stemming, lemmatization, multilingual search; it also makes use of semantic technologies, mainly based on the Medical Subject Headings (MeSH) vocabulary.</p><p>The baseline system for recommendation of research data based on publications in Gesis Search utilizes Pyserini, a Python interface to the IR toolkit built on Lucene designed to support reproducible IR research. The baseline system for recommendation applies the SimpleSearcher of Pyserini that provides the entry point for sparse retrieval BM25 ranking using bag-of-words representations. The Lucene-based index contains abstracts and titles of all research data. The publication identifier (target item of the recommendation) is translated into the publication title, which, in turn, is used to query the index with a BM25 algorithm. Accordingly, the research data recommendations are based on the title and abstracts of the research data and queries made from the publication titles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation Metrics</head><p>Our logging infrastructure allows us to track search sessions and the corresponding interactions made by users. Each session comprises a specific site user, multiple queries (or target items) as wells as the corresponding results and feedback data in the form of user interactions, primarily logged as clicks with timestamps.</p><p>Similar to previous living lab initiatives, we design our user-oriented experiments with interleaved result lists. Given a list with interleaved results and the corresponding clicks of users, we determine Wins, Losses, Ties, and the derived Outcomes for relative comparisons of the experimental and baseline systems <ref type="bibr" coords="11,240.81,367.03,11.28,10.91" target="#b1">[2]</ref>. Following previous living lab experiments, we implement the interleaving method by the Team-Draft-Interleaving algorithm <ref type="bibr" coords="11,388.17,380.58,16.33,10.91" target="#b13">[14]</ref>. More specifically, we refactored exactly the same implementation 11 for the highest degree of comparability.</p><p>Furthermore we follow Gingstad et al. 's proposal of a weighted score based on click events <ref type="bibr" coords="11,89.29,421.23,17.91,10.91" target="#b14">[15]</ref> and define the Reward as</p><formula xml:id="formula_0" coords="11,254.75,445.71,251.24,25.61">Reward = ∑︁ 𝑠𝜖𝑆 𝑤 𝑠 𝑐 𝑠<label>(1)</label></formula><p>where 𝑆 denotes the set of all elements on a search engine result page (SERP) for which clicks are considered, 𝑤 𝑠 denotes the corresponding weight of the SERP element 𝑠 that was clicked, and 𝑐 𝑠 denotes the total number of clicks on the SERP element 𝑠. The Normalized Reward is defined as</p><formula xml:id="formula_1" coords="11,216.24,541.51,289.75,25.41">nReward = Reward exp Reward exp + Reward base<label>(2)</label></formula><p>that is the sum of all weighted clicks on experimental results (Reward exp ) normalized by the total Reward given by Reward exp + Reward base . Note that, only those clicks from the experimental systems where rankings were interleaved with results of the two compared systems are considered. Figure <ref type="figure" coords="11,226.69,614.37,4.99,10.91">5</ref> shows the SERP elements that were logged at LIVIVO and the corresponding weights for our evaluations. We do not implement the Mean Normalized Reward proposed by <ref type="bibr" coords="11,146.65,641.47,50.87,10.91">Gingstad et</ref>  during which the systems as well as the underlying document collections are not modified and we already determine the Normalized Reward over all aggregated clicks of a specific round.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Lab Rounds and Overall Lab Schedule</head><p>The lab was originally split in two separated rounds of 4 weeks each. Due to technical issues for LIVIVO round 1 was four days shorter and round 2 started one week later as planned. To compensate this, we decided to let round 2 last until 24 May 2021, so in total round 2 lasted nearly six instead of four weeks. An overview of the general LiLAS 2021 schedule is given in Table <ref type="table" coords="12,115.06,551.15,3.66,10.91" target="#tab_1">2</ref>. Each participating groups received a set of feedback data after each round; the feedback was also made publicly available on the lab website <ref type="foot" coords="12,316.88,562.95,7.41,7.97" target="#foot_10">12</ref> . Before each round a training phase was offered to allow the participants to build or adapt their systems to the new datasets or click feedback data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Participation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Team lemuren</head><p>Team lemuren participated in both rounds with pre-computed results and dockerized systems for the ad-hoc search task at LIVIVO <ref type="bibr" coords="13,265.38,145.80,16.41,10.91" target="#b15">[16]</ref>. For both rounds, they submitted two different approaches.</p><p>The pre-computed ranking results of lemuren elk are based on built-in functions of Elasticsearch. This system uses a combination between the divergence from randomness model and the Jelinek-Mercer smoothing method for re-ranking candidate documents. The preprocessing pipeline implements stop-word removal, stemming and considers synonyms for medical and COVID19-related terms. The system was tuned only to the results in English.</p><p>save fami is another pre-computed system. It also uses Elasticsearch combined with natural language processing (NLP) modules implemented with the Python package spaCy. Similar to the second submission for the pre-computed round, this dockerized system is build on top of Elasticsearch and spaCy. The indexing pipeline follows a multilingual approach supporting English and German languages. For both languages the system implements full solutions available in spaCy, either by the models en_core_sci_lg (English biomedical texts) or de_core_news_lg (general German texts). The system uses the Google Translator API 13  for language detection and automatic translating of incoming queries (from German to English and vice versa). For indexing and document-retrieval Elasticsearch was used with a custom boosting for MeSH and Chemical-tokens. lemuren elastic only (LEO) is the second dockerized system by this team which, different from LEPREP, relies only on Elasticsearchs built-in tools for indexing documents and processing queries. For indexing documents a custom ingestion pipeline is used to detect the documents language (English or German) and creating the corresponding language fields. Handling of basic acronyms was modeled by using the built-in word-delimiter function. Similar to LEPREP-System, LEO uses Google Translator API for automatic query translation. The system is complemented by a fuzzy match and fuzzy query-expansion to obtain better results for mistyped queries. Like lemuren elk in round one, LEO also uses DFR and LMJelinekMercer to calculate a score and a similarity distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Team tekma</head><p>Team tekma contributed experiments to both rounds. In the first round, they submitted the pre-computed results of the system tekma_s for the ad-hoc search task at LIVIVO <ref type="bibr" coords="13,456.71,534.25,16.18,10.91" target="#b16">[17]</ref>. In the second round, they submitted pre-computed recommendations (covering the entire volume of publications) for the corresponding task at GESIS. Both systems are described below.</p><p>tekma_s used Apache Solr to index the document and used pseudo-relevance feedback to extend the queries for the ad-hoc search task. The system only considers documents in English. The system got few impressions and clicks in comparison to the baseline system. tekma_n participated in the second round producing pre-computed recommendations. They used Apache Solr BM25 ranking function and applied query expansion and data enrichment by adding the metadata translations and re-ranking the retrieved result using user feedback and KNN. To generate the primary recommendations for a publication, they used publication fields as a query to search the indexed dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Team GESIS Research</head><p>In addition to the baseline system, team GESIS Research contributed a fully dockerized system in both rounds <ref type="bibr" coords="14,144.04,287.80,16.09,10.91" target="#b17">[18]</ref>. gesis_rec_pyterrier implements a naive content-based recommendation without any advanced knowledge about user preferences and usage metrics. It uses the metadata available in both entity types, i.e., title, abstract, and topics. They employed the classical tfidfbased weighting model from the PyTerrier frameworkto obtain first-hand experience with the online evaluation. The indexing and query have been made of the combination of words in title, abstract, and research data topics and publications. They decided to submit the same experimental system for both rounds to gain more user feedback for their unique system. Even though only tfidf-based recommendations are implemented at the current state, it offers a good starting point for further experimentation with PyTerrier and the declarative manner of defining retrieval pipelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head><p>Our experimental evaluations are twofold. First, we evaluate overall statistics of both rounds and sites. Second, we evaluate the performance of all participating systems based on the click data logged during the active periods. As mentioned before, the first round ran during four weeks from March 1st, 2021 to March 28th, 2021 and the second round for five weeks from April 17th, 2021 until May 24th, 2021 at LIVIVO and for six weeks from April 12th, 2021 until May 24th, 2021 at GESIS. To foster transparency and reproducibility of the evaluations, we release the corresponding evaluation scripts in an open-source GitHub repository<ref type="foot" coords="14,419.24,547.91,7.41,7.97" target="#foot_11">14</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Overall evaluations of both rounds and sites</head><p>Table <ref type="table" coords="14,116.11,599.39,5.14,10.91" target="#tab_2">3</ref> provides an overview of the traffic logged in both rounds. In sum, substantially more sessions, impressions, and clicks were logged in the second round not only due a longer period but also because more systems contributed as Type B submissions. In the first round, systems deployed at LIVIVO were mostly contributed as Type A submissions, meaning their responses were restricted to pre-selected head queries. LIVIVO started the second round with full systems which delivered results for arbitrary queries and thus more session data was logged. GESIS started both rounds with the majority of systems contributed as type B submissions. In comparison to LIVIVO, more sessions and impressions were logged in the first round, but less recommendations were clicked. Similarly, there are less clicks in the second round in comparison to LIVIVO, which is also reflected by the Click-Through Rate (CTR) that is determined by the ratio between Clicks and Impressions. As mentioned before, GESIS introduced the recommendations of research datasets as a new service, and, presumably, users were not aware of this new feature.</p><p>Figure <ref type="figure" coords="15,130.78,456.09,9.97,10.91" target="#fig_0">15</ref> shows the distributions of sessions and impressions over the entire time span from the beginning of the first round until the end of the second round. Note that, after the end of the first round, we did not log any interactions until the beginning of the second round. In comparison, the sessions and impressions are more uniformly distributed at GESIS. This can be explained by the deployment of type B systems from the early beginning of the first round and systems could provide recommendations for the entire volume of the publications.</p><p>During the first two weeks of the first round, the amount of logged data at LIVIVO is comparatively low due to systems with pre-computed results for pre-selected head queries. After that, the first type B systems was deployed and increasingly more user traffic could be redirected to our infrastructure. Figure <ref type="figure" coords="15,259.89,578.03,4.97,10.91" target="#fig_3">6</ref> illustrates these effects. The cumulative sums of logged sessions, impressions, and clicks rapidly increased after the first Type B system got online in mid-March.</p><p>The logged impressions follow a power-law distribution for both rankings and recommendations as shown in Figure <ref type="figure" coords="15,203.57,632.23,3.81,10.91" target="#fig_4">7</ref>. Most of the impressions can be attributed to a few top-k queries (rankings) or documents (recommendations). Table <ref type="table" coords="15,317.60,645.78,5.03,10.91" target="#tab_3">4</ref> and<ref type="table" coords=""></ref>  lower-casing and removing special characters. As it can be seen from Table <ref type="table" coords="16,433.04,386.55,5.17,10.91" target="#tab_3">4</ref> the COVID-19 pandemic has a clear influence on the query distributions: the most frequent and the fifth most frequent query are "covid19" and "covid", respectively. Three of the ten most frequent queries are definitely German queries ("demenz", "pflege", "schlaganfall"). Others are either domain-specific or can also be interpreted as English queries. In Table <ref type="table" coords="16,332.28,440.74,5.12,10.91" target="#tab_5">6</ref> we report statistics about the queries logged during both rounds at LIVIVO. In both rounds, interaction data was logged for 11,822 unique queries with an average length of 2.9840 terms and each session had 1.9340 queries on average. Nine out of the ten most frequent target items of the recommendations at GESIS are publications with German titles as shown in Table <ref type="table" coords="16,315.52,494.94,3.74,10.91" target="#tab_4">5</ref>.</p><p>Likewise the total number of clicks over queries and documents is extremely thin-tailed (cf. Figure <ref type="figure" coords="16,120.25,522.04,10.10,10.91" target="#fig_9">11</ref> and<ref type="figure" coords="16,152.17,522.04,7.87,10.91" target="#fig_10">12</ref>). A large amount of the clicks at LIVIVO were made for the query "polyvinyl and nasal and packing" and LIVIVO's internal server logs indicate a crawling process here. All other queries received 23 or less clicks. As mentioned before, less clicks were made at GESIS. Three clicks were made at maximum on recommendations for the most frequently clicked documents.</p><p>Similar power-law distributions can be observed for the total number of clicks over documents (rankings) and datasets (recommendations) in Figure <ref type="figure" coords="16,324.05,603.34,10.03,10.91" target="#fig_11">13</ref> and 14, respectively. A few documents and datasets receive most of the clicks. Details about the corresponding items can be found in Table <ref type="table" coords="16,115.79,630.43,10.15,10.91" target="#tab_12">13</ref> and<ref type="table" coords="16,147.81,630.43,8.36,10.91" target="#tab_13">14</ref>.</p><p>Another important aspect to be considered as part of the system evaluations is the position bias inherent in the logged data. Click decisions are biased towards the top ranks of the result lists as shown in Figure <ref type="figure" coords="17,225.36,553.44,3.76,10.91" target="#fig_5">8</ref>. For both use cases, the rankings and recommendations were displayed to users as vertical lists. Note that, GESIS restricted the recommendations to the first six recommended datasets and no pagination over the following recommended items was possible. LIVIVO shows ten results per page to its users, and as it can be seen from the logged data, users rarely click results beyond the fifth page. In addition to "simple" clicks on ranked items, we logged specific SERP elements that were clicked at LIVIVO. Table <ref type="table" coords="17,200.96,634.73,5.10,10.91" target="#tab_4">5</ref> already provided an overview on which elements were logged and Figure <ref type="figure" coords="17,120.45,648.28,5.09,10.91" target="#fig_6">9</ref> shows the CTR of these elements also follows a power-law distribution. The number  of clicks is the highest for the Details button and it is followed by the Title and Fulltext click options. In comparison, the other four logged elements receive substantially less clicks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">System evaluations</head><p>An overview of all systems participating in our experiments is provided in Table <ref type="table" coords="18,465.76,568.66,3.81,10.91" target="#tab_6">7</ref>. In the first round, three type A systems (lemuren_elk, tekmas, save_fami) were submitted and deployed at LIVIVO. They were also deployed in the second round, but did not receive any updates between the two rounds. Since there were no type B submissions in the first round for LIVIVO, we deployed the type B system livivo_rank_pyserini after two weeks in mid-March. It provided results for the entire volume of publications and rankings were based on the BM25 method. It was implemented with Pyserini <ref type="bibr" coords="18,347.91,649.96,18.07,10.91" target="#b18">[19]</ref> and the corresponding default settings <ref type="foot" coords="19,124.59,354.83,7.41,7.97" target="#foot_12">15</ref> . In contrast to the other systems, it was online for the last two weeks of the first round only. In the second round, it was online in the first days until the other type B systems were ready to be deployed since we wanted to distribute the user traffic among the participants' systems only. In the second round, two type B systems lemuren_elastic_only and lemuren_elastic_preprocessing were contributed. Both systems build up on Elasticsearch, whereas they differ by the pre-processing as outlined before. At GESIS, gesis_rec_pyterrier, submitted as type B system, was online in both rounds. In the first round, the only type A submission was gesis_rec_precom that was substituted in the second round by tekma_n.</p><p>Both baseline systems at LIVIVO (livivo_base) and GESIS (gesis_rec_pyserini) were integrated as type B systems, remained unmodified, and could deliver results for every request. Table <ref type="table" coords="19,127.20,492.07,5.11,10.91" target="#tab_7">8</ref> compares the experimental systems' outcomes and the corresponding logged interactions and session data during the first round. Regarding the Outcome measure, none of the experimental systems was able to outperform the baseline systems. Note that the reported Outcomes of the baseline systems result from comparisons against all experimental systems. The systems with pre-computed rankings (type A submissions) received a total number of 32 clicks over a period of four weeks at LIVIVO. Since interaction data was sparse in the first round, we only received enough data for livivo_rank_pyserini to conduct significance tests. The reported p-value results from a Wilcoxon signed-rank test and shows a significant difference between the experimental and baseline system.</p><p>Table <ref type="table" coords="19,125.79,614.02,4.97,10.91" target="#tab_8">9</ref> shows the results of the second round. tekma_n was contributed as type A submission, but results were pre-computed for the entire volume of publications at GESIS. It replaced gesis_rec_precom and achieved a higher CTR compared to the other recommender systems. Likewise, it achieves an Outcome of 0.62, which might be an indicator that it outperforms the baseline recommendations given by gesis_rec_pyserini. Unfortunately, we are not able to conduct any meaningful significance tests due to the sparsity of click data. At LIVIVO, the systems with pre-computed rankings (type A submissions) received a comparable amount of clicks similar to the first round. In sum, all three systems received a total number of 35 clicks over a period of five weeks. Even though, click data is sparse and interpretations have to be made carefully, the relative ranking order of these three systems is preserved in the second round (e.g. in terms of the Outcome, total number of clicks, or CTR).</p><p>In the second round, no experimental system could outperform the baseline system at LIVIVO. Both experimental type B systems lemuren_elastic_only and lemuren_elastic_preprocessing achieve significantly lower Outcome scores as the baseline. However, the second system has substantially lower Outcome and CTR scores. Both systems share a fair amount of the same methodological approach and only differ by the processing of the input text. In this case, the system performance does not seem to benefit from this specific pre-processing step, when interpreting clicks as positive relevance signals. The third type B system at LIVIVO livivo_rank_pyserini did not participate the entire second round, since we took it offline as soon as the other type B systems were available. Despite having participated in comparatively less experiments than in the first round (1260 sessions vs. 243 sessions), the system achieves in both rounds comparable results in terms of Outcome and CTR scores. This circumstance raises the question for how long systems have to be online to deliver reliable performance estimates. Figure <ref type="figure" coords="21,121.14,195.36,10.35,10.91" target="#fig_8">10</ref> provides an overview of how the Outcome score evolves over aggregated sessions for different systems and rounds. As the figures show, after a certain number of sessions, the outcome tends to stabilize. In our future work, we want to investigate how much sessions (or online time) is required to deliver meaningful estimates of system performance in terms of the Outcome and other measures derived from interleaving experiments.</p><p>Previous studies showed that a system is more likely to win if its documents are ranked at higher positions <ref type="bibr" coords="21,161.86,276.66,16.09,10.91" target="#b19">[20]</ref>. As part of our experimental evaluations, we can confirm this circumstance. We also determined the Spearman correlation between an interleaving outcome (1: win, -1: loss, 0: tie) and the highest ranked position of a document contributed by an experimental system. At both sites, we see a weak but significant correlation (LIVIVO: 𝜌 = -0.0883, 𝑝 = 1.3535𝑒 -09; GESIS: 𝜌 = -0.3480, 𝑝 = 4.7422𝑒 -07).</p><p>One shortcoming of the previous measures derived from interleaving experiments is the simplified interpretation of click interactions. As outlined in Section 4, by weighting clicks differently, it is possible to account for the meaning of the corresponding SERP elements. Table <ref type="table" coords="21,89.04,385.05,10.35,10.91" target="#tab_9">10</ref> shows the total number of clicks on SERP elements for each systems and the Normalized Reward (nReward) resulting from the weighting scheme given in Figure <ref type="figure" coords="21,419.93,398.60,3.81,10.91">5</ref>. We compare the total number of clicks of those (interleaving) experiments in which the experimental and baseline systems delivered results. As it can be seen, comparing systems by clicks on different SERP elements, provides a more diverse analysis. For instance, some of the systems achieve higher numbers of clicks (and CTRs) for some SERP elements in direct comparison to the baseline systems. livivo_rank_pyserini, lemuren_elastic_only got more clicks on the Bookmark element than the baseline system, while all systems achieve lower numbers of total clicks.</p><p>None of the systems could outperform the baseline system in terms of the nReward measure, but in comparison to the Outcome scores, there is a more balanced ratio between the nReward scores that also accounts for the meaning of specific clicks. Likewise, it accounts for clicks even if the experimental system did not "win" in the interleaving experiment. In Table <ref type="table" coords="21,441.16,547.64,9.94,10.91" target="#tab_9">10</ref> we compare the total number of clicks over multiple sessions. While the Win, Loss, Tie, and Outcome only measure if there have been more clicks in a single experiment, the nReward also considers those clicks that were made in experiments in which the experimental system did not necessarily win.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>The Living Labs for Academic Search (LiLAS) lab re-introduced the living lab paradigm with a focus on tasks in the domain of academic search. The lab offered the possibility to participate in two different tasks, which were either dedicated to ad-hoc search in the Life Sciences or research data recommendations in the Social Sciences. Participants were provided with datasets and access to the underlying search portals for experimentation. For both tasks, participants could contribute their experimental systems either by pre-computed outputs for selected queries (or target items) or as fully-fledged dockerized systems. In total, we evaluated nine experimental systems out of which seven were contributed by three participating groups. In sum, two groups contributed experiments that cover pre-computed rankings and fully dockerized systems at LIVIVO and pre-computed recommendations at GESIS. The GESIS research team contributed another completely dockerized recommendation system. Our experimental setup is based on interleaving experiments that combine experimental results with those from the corresponding baseline systems at LIVIVO and GESIS. In accordance with the living lab paradigm, our evaluations are based on user interactions, i.e. in the form of click feedback.</p><p>A key component of the underlying infrastructure is the integration of experimental ranking and recommendation systems as micro-services that are implemented with the help of Docker. The LiLAS lab was the first test-bed to use this evaluation service and it exemplified some of the benefits resulting from the new infrastructure design. First of all, completely dockerized systems can overcome the restrictions of results limited to filtered lists of top-k queries or target items. Significantly more data and click interactions can be logged if the experimental systems can deliver results on-the-fly for arbitrary requests of rankings and recommendations. As a consequence, this allows much more data aggregation in a shorter period of time and provides a solid basis for statistical significance tests.</p><p>Furthermore, the deployment effort for site providers and organizers is considerably reduced. Once the systems are properly described with the corresponding Dockerfile, they can be rebuild on purpose, exactly as the participants and developers intended them to be. Likewise, the entire infrastructure service can be migrated with minimal costs due to Docker. However, we  hypothesize that one reason for the low participation might be the technical overhead for those who were not already familiar with Docker. On the other hand, the development efforts pay off. If the systems are properly adapted to the required interface and the source code is available in a public repository, the (IR) research community can rely on these artifacts that make the experiments transparent and reproducible. Thus, we address the reproducibility of these living lab experiments mostly from a technological point of view, in the sense that we can repeat the experiments in the future with reduced efforts, since the participating systems are openly available and should be reconstructible with the help of the corresponding Dockerfiles. Future work should investigate how feasible it is to rely on the Dockerfiles for the long-term preservation. Since experimental systems are rebuilt each time with the help of the Dockerfile, updates of the underlying dependencies might be a threat to the reproducibility. An intuitive solution would be the integration of pre-built Docker images that may allow a longer reproducibility. Apart from the underlying technological aspects, the reproducibility of the actual experimental results has to be investigated. Our experimental setup would allow to answer questions with regard to the reproducibility of the experimental results over time and also across different domains (e.g. Life vs. Social Sciences).</p><p>Most of the evaluation measures are made for interleaving experiments that also depend on the results of the baseline system and not solely on those of an experimental system. We have not investigated yet, if the experimental results follow a transitive relation: if the experimental system A outperforms the baseline system B, denoted as 𝐴 ≻ 𝐵, and the baseline system B outperforms another experimental system C (𝐵 ≻ 𝐶), can we conclude that system A would also outperform system C (𝐴 ≻ 𝐶)? As the evaluations showed, click results are heavily biased towards the first ranks and likewise they are context-dependent, i.e. they depend on the entire result list and single click decisions have to be interpreted in relation to neighboring and previously seen results and further evaluations in these directions would require counterfactual reasoning. Nonetheless, in the second round it was illustrated how our infrastructure service can be used for incremental developments and component-wise analysis of experimental systems. The two experimental systems lemuren_elastic_only and lemuren_elastic_preprocessing follow a similar approach and only differ by the pre-processing component that has been shown not to be of any benefit.</p><p>In addition to established outcome measures of interleaving experiments (Win, Loss, Tie, Outcome), we also account for the meaning of clicks on different SERP elements. In this context, we implement the Reward measure that is the weighted sum of clicks on different elements corresponding to a specific result. Even though most of the experimental systems could not outperform the baseline systems in terms of the overall scores, we see some clear differences between the system performance, which allow us to assess a system's merits more thoroughly, when the evaluations are based on different SERP elements.</p><p>Overall, we consider our lab as a successful advancement to previous living lab experiments. We were able to exemplify the benefits of fully dockerized systems delivering results for arbitrary results on-the-fly. Furthermore, we could confirm several previous findings, for instance the power laws underlying the click distributions. Additionally, we were able to conduct more diverse comparison by differentiating between clicks on different SERP elements and accounting for their meaning. Unfortunately, we could not attract many participants, leaving some aspects not tested, e.g. how many systems/experiments can be run simultaneously considering the limitations of the infrastructure design, hardware requirements, server load and user traffic. Likewise, no experimental ranking system could outperform the baseline system. In the future, it might be helpful to provide participants with open and more transparent baseline systems they can build upon. Some of the pre-computed experimental ranking and recommendations seem to deliver promising results; however, the evaluations need to be interpreted with care due to the sparsity of the available click data. As a way out, we favor continuous evaluations freed from the time limits of rounds, in order to re-frame the introduced living lab service as an ongoing evaluation challenge. The corresponding source code can be retrieved from a public GitHub project 16 and we plan to release the aggregated session data as a curated research dataset.     Physiotherapie bei Parkinson-Syndromen   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,167.43,206.91,8.93"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the live evaluation pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,89.29,395.36,416.69,8.93;6,89.29,407.37,89.11,8.87"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Examples for publication documents, research dataset documents, and candidate lists for the GESIS Search system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,89.29,332.02,201.72,8.93"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Overview of the STELLA infrastructure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="15,89.29,298.14,416.70,8.93;15,89.29,310.14,221.87,8.87"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Cumulative sum of logged session data at LIVIVO before (blue) and after (green) the first fully dockerized system went online in the first round.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="16,89.29,348.95,179.63,8.93"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Impressions vs. Query/Document</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="18,89.29,369.18,184.77,8.93"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Click-through Rate (CTR) vs. Rank</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="19,89.29,318.98,234.78,8.93"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Click distribution on SERP elements at LIVIVO</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="23,127.21,204.75,2.66,7.92;23,145.53,204.75,135.24,7.92;23,182.45,210.48,41.68,7.92;23,122.95,195.17,2.66,7.92;23,120.28,179.91,5.33,7.92;23,117.62,164.66,7.99,7.92;23,117.62,149.40,7.99,7.92;23,117.62,134.15,7.99,7.92;23,117.62,118.89,7.99,7.92;23,117.62,103.64,7.99,7.92;23,108.76,175.19,7.92,10.49;23,108.76,157.52,7.92,16.34;23,108.76,152.15,7.92,4.04;23,108.76,139.35,7.92,11.47;23,108.76,122.67,7.92,15.35;23,108.76,112.86,7.92,8.48;23,159.46,221.86,8.09,7.92;23,187.65,221.86,5.38,7.92;23,213.14,221.86,7.24,7.92;23,240.48,221.86,18.38,7.92;23,281.00,195.17,6.66,7.92;23,281.00,181.66,6.66,7.92;23,281.00,168.16,6.66,7.92;23,281.00,154.65,6.66,7.92;23,281.00,141.15,6.66,7.92;23,281.00,127.64,6.66,7.92;23,281.00,114.13,6.66,7.92;23,281.00,100.63,6.66,7.92;23,286.20,139.70,7.92,19.11;23,128.34,83.45,149.93,9.50;23,319.58,205.16,2.63,7.81;23,338.39,205.16,124.26,7.81;23,374.04,210.80,41.09,7.81;23,315.38,195.71,2.63,7.81;23,310.12,175.30,7.88,7.81;23,307.50,154.90,10.50,7.81;23,307.50,134.49,10.50,7.81;23,307.50,114.08,10.50,7.81;23,307.50,93.67,10.50,7.81;23,298.76,176.02,7.81,10.35;23,298.76,158.59,7.81,16.11;23,298.76,153.30,7.81,3.98;23,298.76,140.67,7.81,11.31;23,298.76,124.23,7.81,15.13;23,298.76,114.56,7.81,8.36;23,351.37,222.03,7.98,7.81;23,379.18,222.03,5.30,7.81;23,404.30,222.03,7.14,7.81;23,431.26,222.03,18.12,7.81;23,471.21,195.71,6.56,7.81;23,471.21,182.39,6.56,7.81;23,471.21,169.08,6.56,7.81;23,471.21,155.76,6.56,7.81;23,471.21,142.44,6.56,7.81;23,471.21,129.13,6.56,7.81;23,471.21,115.81,6.56,7.81;23,471.21,102.50,6.56,7.81;23,476.34,141.02,7.81,18.84;23,320.69,85.55,147.83,9.37;23,127.21,357.44,2.66,7.92;23,148.30,357.44,7.99,7.92;23,172.05,357.44,7.99,7.92;23,195.81,357.44,7.99,7.92;23,219.56,357.44,56.83,7.92;23,182.45,363.17,41.68,7.92;23,122.95,347.86,2.66,7.92;23,120.28,331.09,5.33,7.92;23,117.62,314.32,7.99,7.92;23,117.62,297.55,7.99,7.92;23,117.62,280.78,7.99,7.92;23,117.62,264.01,7.99,7.92;23,117.62,247.24,7.99,7.92;23,108.76,327.88,7.92,10.49;23,108.76,310.21,7.92,16.34;23,108.76,304.84,7.92,4.04;23,108.76,292.04,7.92,11.47;23,108.76,275.36,7.92,15.35;23,108.76,265.55,7.92,8.48;23,159.46,374.55,8.09,7.92;23,187.65,374.55,5.38,7.92;23,213.14,374.55,7.24,7.92;23,240.48,374.55,18.38,7.92;23,281.00,351.91,6.66,7.92;23,281.00,338.74,6.66,7.92;23,281.00,325.57,6.66,7.92;23,281.00,312.41,6.66,7.92;23,281.00,299.24,6.66,7.92;23,281.00,286.07,6.66,7.92;23,281.00,272.90,6.66,7.92;23,281.00,259.73,6.66,7.92;23,281.00,246.57,6.66,7.92;23,286.20,292.39,7.92,19.11;23,117.49,236.14,171.62,9.50;23,314.77,357.03,2.70,8.03;23,344.76,357.03,5.40,8.03;23,374.75,357.03,8.10,8.03;23,406.08,357.03,8.10,8.03;23,437.42,357.03,8.10,8.03;23,370.81,362.83,42.28,8.03;23,310.45,347.30,2.70,8.03;23,307.75,332.83,5.40,8.03;23,307.75,318.36,5.40,8.03;23,307.75,303.89,5.40,8.03;23,307.75,289.42,5.40,8.03;23,307.75,274.95,5.40,8.03;23,307.75,260.48,5.40,8.03;23,307.75,246.01,5.40,8.03;23,298.76,327.04,8.03,10.64;23,298.76,309.12,8.03,16.58;23,298.76,303.67,8.03,4.09;23,298.76,290.68,8.03,11.64;23,298.76,273.77,8.03,15.57;23,298.76,263.82,8.03,8.60;23,347.48,374.38,8.21,8.03;23,376.08,374.38,5.46,8.03;23,401.93,374.38,7.35,8.03;23,429.67,374.38,18.64,8.03;23,470.77,348.45,6.75,8.03;23,470.77,333.60,6.75,8.03;23,470.77,318.76,6.75,8.03;23,470.77,303.92,6.75,8.03;23,470.77,289.08,6.75,8.03;23,470.77,274.24,6.75,8.03;23,470.77,259.40,6.75,8.03;23,470.77,244.56,6.75,8.03;23,476.04,291.04,8.03,19.38;23,304.92,233.98,174.09,9.64;23,123.33,506.29,2.63,7.81;23,144.25,506.29,126.95,7.81;23,177.79,511.94,41.09,7.81;23,119.12,496.84,2.63,7.81;23,113.87,477.67,7.88,7.81;23,113.87,458.50,7.88,7.81;23,113.87,439.33,7.88,7.81;23,113.87,420.16,7.88,7.81;23,111.25,400.99,10.50,7.81;23,102.51,477.15,7.81,10.35;23,102.51,459.72,7.81,16.11;23,102.51,454.43,7.81,3.98;23,102.51,441.80,7.81,11.31;23,102.51,425.36,7.81,15.13;23,102.51,415.69,7.81,8.36;23,155.12,523.16,7.98,7.81;23,182.93,523.16,5.30,7.81;23,208.05,523.16,7.14,7.81;23,235.01,523.16,18.12,7.81;23,274.96,496.84,6.56,7.81;23,274.96,478.29,6.56,7.81;23,274.96,459.75,6.56,7.81;23,274.96,441.20,6.56,7.81;23,274.96,422.65,6.56,7.81;23,274.96,404.10,6.56,7.81;23,280.09,442.15,7.81,18.84;23,112.03,386.69,172.65,9.37;23,318.77,506.73,2.58,7.69;23,340.82,506.73,7.75,7.69;23,364.15,506.73,84.22,7.69;23,372.40,512.29,40.46,7.69;23,314.64,497.43,2.58,7.69;23,309.47,482.40,7.75,7.69;23,309.47,467.36,7.75,7.69;23,309.47,452.33,7.75,7.69;23,309.47,437.29,7.75,7.69;23,306.88,422.26,10.34,7.69;23,306.88,407.23,10.34,7.69;23,298.28,478.04,7.69,10.19;23,298.28,460.89,7.69,15.86;23,298.28,455.68,7.69,3.92;23,298.28,443.25,7.69,11.14;23,298.28,427.06,7.69,14.90;23,298.28,417.54,7.69,8.23;23,350.08,523.34,7.85,7.69;23,377.45,523.34,5.22,7.69;23,402.18,523.34,7.03,7.69;23,428.73,523.34,17.84,7.69;23,468.06,497.43,6.46,7.69;23,468.06,477.77,6.46,7.69;23,468.06,458.10,6.46,7.69;23,468.06,438.44,6.46,7.69;23,468.06,418.77,6.46,7.69;23,468.06,399.11,6.46,7.69;23,473.10,443.59,7.69,18.55;23,295.56,388.98,194.15,9.22"><head></head><label></label><figDesc>Wins, Losses, and Ties + Outcome</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="23,89.29,542.29,416.69,8.93;23,88.99,554.30,416.99,8.87;23,89.29,567.55,162.35,7.22"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10:Dockerized systems that were deployed at LIVIVO in Round 1 and 2 (livivo_base and livivo_rank_pyserini) and Round 2 (lemuren_elastic_only and lemuren_elastic_preprocessing).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="28,89.29,517.68,282.83,8.93"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Number of clicks versus Query at LIVIVO in both rounds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="29,89.29,623.52,294.34,8.93"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Number of clicks versus Document at GESIS in both rounds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="30,89.29,617.45,299.91,8.93"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Number of clicks versus Document at LIVIVO in both rounds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12" coords="31,89.29,617.34,283.17,8.93"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Number of clicks versus Dataset at GESIS in both rounds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="11,200.25,641.47,305.73,10.91"><head>Table 1 :</head><label>1</label><figDesc>al. due to a different evaluation setup. Our lab is organized in rounds SERP Element 𝑤 𝑠 Example illustrating the SERP elements for that clicks were logged at LIVIVO and the corresponding weights 𝑤 𝑠 according to Equation 1.</figDesc><table coords="12,403.74,107.38,80.42,80.60"><row><cell>Bookmark</cell><cell>10</cell></row><row><cell>Order</cell><cell>10</cell></row><row><cell>Fulltext</cell><cell>8</cell></row><row><cell>In Stock</cell><cell>8</cell></row><row><cell>More Links</cell><cell>2</cell></row><row><cell>Title</cell><cell>1</cell></row><row><cell>Details</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="12,88.99,243.68,382.36,163.29"><head>Table 2</head><label>2</label><figDesc>Schedule for the LiLAS lab 2021</figDesc><table coords="12,121.43,273.07,23.15,8.87"><row><cell>Event</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="14,88.99,90.49,355.00,93.78"><head>Table 3</head><label>3</label><figDesc>Number of Sessions, impressions, clicks and click through rate (CTR).</figDesc><table coords="14,148.79,122.10,295.21,62.16"><row><cell cols="2">Evaluation round Site</cell><cell cols="3">Sessions Impressions Clicks</cell><cell>CTR</cell></row><row><cell>Round 1</cell><cell>LIVIVO</cell><cell>2852</cell><cell>4658</cell><cell>2452</cell><cell>0.5264</cell></row><row><cell>Round 1</cell><cell>GESIS</cell><cell>4568</cell><cell>8390</cell><cell>152</cell><cell>0.0181</cell></row><row><cell>Round 2</cell><cell>LIVIVO</cell><cell>12962</cell><cell>25830</cell><cell cols="2">11562 0.4476</cell></row><row><cell>Round 2</cell><cell>GESIS</cell><cell>6576</cell><cell>12068</cell><cell>250</cell><cell>0.0207</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="17,88.99,90.49,416.99,177.47"><head>Table 4</head><label>4</label><figDesc>Top ten queries at LIVIVO during our logging periods of round 1 and 2. Query strings were lower-cased and special characters removed.</figDesc><table coords="17,183.12,134.06,226.54,133.90"><row><cell cols="2">Rank Query string</cell><cell>Impressions</cell></row><row><cell>1</cell><cell>covid19</cell><cell>45</cell></row><row><cell>2</cell><cell>demenz</cell><cell>39</cell></row><row><cell>3</cell><cell>guillian barre syndrome</cell><cell>36</cell></row><row><cell>4</cell><cell>polyvinyl and nasal and packing</cell><cell>31</cell></row><row><cell>5</cell><cell>covid</cell><cell>24</cell></row><row><cell>6</cell><cell>pflege</cell><cell>22</cell></row><row><cell>7</cell><cell>cancer</cell><cell>21</cell></row><row><cell>8</cell><cell>parkinson</cell><cell>18</cell></row><row><cell>9</cell><cell>depression</cell><cell>17</cell></row><row><cell>10</cell><cell>schlaganfall</cell><cell>17</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="17,88.99,290.21,416.99,235.52"><head>Table 5</head><label>5</label><figDesc>Top ten documents for which recommendations were made at GESIS during our logging periods of round 1 and 2</figDesc><table coords="17,96.06,331.56,95.04,8.87"><row><cell>Rank Document title</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="18,88.99,402.33,307.20,76.35"><head>Table 6</head><label>6</label><figDesc>Statistics of the queries at LIVIVO</figDesc><table coords="18,196.59,433.95,199.60,44.73"><row><cell>Number of Unique Queries</cell><cell>11822</cell></row><row><cell>Average Query Length [Terms]</cell><cell>2.9840</cell></row><row><cell cols="2">Average Number of Queries per Session 1.9340</cell></row><row><cell>Average Number of Clicks per Query</cell><cell>0.4547</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="20,88.99,90.49,380.74,177.47"><head>Table 7</head><label>7</label><figDesc>System overview</figDesc><table coords="20,123.05,122.10,346.69,145.85"><row><cell>System name</cell><cell cols="2">Task Type Experimental Round 1 Round 2</cell></row><row><cell>lemuren_elk</cell><cell>1</cell><cell>A</cell></row><row><cell>tekmas</cell><cell>1</cell><cell>A</cell></row><row><cell>save_fami</cell><cell>1</cell><cell>A</cell></row><row><cell>livivo_rank_pyserini</cell><cell>1</cell><cell>B</cell></row><row><cell>lemuren_elastic_only</cell><cell>1</cell><cell>B</cell></row><row><cell>lemuren_elastic_preprocessing</cell><cell>1</cell><cell>B</cell></row><row><cell>livivo_base</cell><cell>1</cell><cell>B</cell></row><row><cell>tekma_n</cell><cell>2</cell><cell>A</cell></row><row><cell>gesis_rec_precom</cell><cell>2</cell><cell>A</cell></row><row><cell>gesis_rec_pyterrier</cell><cell>2</cell><cell>B</cell></row><row><cell>gesis_rec_pyserini</cell><cell>2</cell><cell>B</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="20,88.99,290.21,416.99,199.59"><head>Table 8</head><label>8</label><figDesc>Outcomes of Round 1. Dagger symbols ( †) indicate baseline systems. Significant differences are denoted by an asterisk symbol (*).</figDesc><table coords="20,95.27,331.95,406.01,157.85"><row><cell>System</cell><cell>Win</cell><cell>Loss</cell><cell>Tie</cell><cell>Outcome</cell><cell>Sessions</cell><cell>Impressions</cell><cell>Clicks</cell><cell>CTR</cell></row><row><cell>gesis_rec_pyserini †</cell><cell>36</cell><cell>36</cell><cell>1</cell><cell>0.50</cell><cell>2284</cell><cell>4195</cell><cell>37</cell><cell>0.0088</cell></row><row><cell>gesis_rec_pyterrier</cell><cell>26</cell><cell>28</cell><cell>1</cell><cell>0.48</cell><cell>1968</cell><cell>3675</cell><cell>28</cell><cell>0.0076</cell></row><row><cell>gesis_rec_precom</cell><cell>10</cell><cell>8</cell><cell>0</cell><cell>0.56</cell><cell>316</cell><cell>520</cell><cell>11</cell><cell>0.0212</cell></row><row><cell>livivo_base †</cell><cell>332</cell><cell>234</cell><cell>67</cell><cell>0.59</cell><cell>1426</cell><cell>2329</cell><cell>677</cell><cell>0.2907</cell></row><row><cell>livivo_rank_pyserini</cell><cell>215</cell><cell>302</cell><cell>64</cell><cell cols="2">0.42 * 1260</cell><cell>2135</cell><cell>517</cell><cell>0.2422</cell></row><row><cell>lemuren_elk</cell><cell>4</cell><cell>8</cell><cell>1</cell><cell>0.33</cell><cell>45</cell><cell>55</cell><cell>10</cell><cell>0.1818</cell></row><row><cell>tekmas</cell><cell>6</cell><cell>10</cell><cell>1</cell><cell>0.38</cell><cell>64</cell><cell>77</cell><cell>8</cell><cell>0.1039</cell></row><row><cell>save_fami</cell><cell>9</cell><cell>12</cell><cell>1</cell><cell>0.43</cell><cell>57</cell><cell>62</cell><cell>14</cell><cell>0.2258</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="22,88.99,90.49,416.99,223.50"><head>Table 9</head><label>9</label><figDesc>Outcomes of Round 2. Dagger symbols ( †) indicate baseline systems. Significant differences are denoted by an asterisk symbol (*).</figDesc><table coords="22,95.27,132.22,406.01,181.76"><row><cell>System</cell><cell>Win</cell><cell>Loss</cell><cell>Tie</cell><cell>Outcome</cell><cell>Sessions</cell><cell>Impressions</cell><cell>Clicks</cell><cell>CTR</cell></row><row><cell>gesis_rec_pyserini †</cell><cell>51</cell><cell>68</cell><cell>2</cell><cell>0.43</cell><cell>3288</cell><cell>6034</cell><cell>53</cell><cell>0.0088</cell></row><row><cell>gesis_rec_pyterrier</cell><cell>26</cell><cell>25</cell><cell>1</cell><cell>0.51</cell><cell>1529</cell><cell>2937</cell><cell>27</cell><cell>0.0092</cell></row><row><cell>tekma_n</cell><cell>42</cell><cell>26</cell><cell>1</cell><cell>0.62</cell><cell>1759</cell><cell>3097</cell><cell>45</cell><cell>0.0145</cell></row><row><cell>livivo_base †</cell><cell>2447</cell><cell>1063</cell><cell>372</cell><cell>0.70</cell><cell>6481</cell><cell cols="2">12915 3791</cell><cell>0.2935</cell></row><row><cell>livivo_rank_pyserini</cell><cell>48</cell><cell>71</cell><cell>15</cell><cell>0.40</cell><cell>243</cell><cell>434</cell><cell>112</cell><cell>0.2581</cell></row><row><cell>lemuren_elastic_only</cell><cell>707</cell><cell>1042</cell><cell>218</cell><cell cols="2">0.40 * 3131</cell><cell>6274</cell><cell>1273</cell><cell>0.2029</cell></row><row><cell cols="2">lemuren_elastic_preprocessing 291</cell><cell>1308</cell><cell>135</cell><cell cols="2">0.18 * 2948</cell><cell>6026</cell><cell>570</cell><cell>0.0946</cell></row><row><cell>lemuren_elk</cell><cell>6</cell><cell>13</cell><cell>0</cell><cell>0.32</cell><cell>61</cell><cell>69</cell><cell>10</cell><cell>0.1449</cell></row><row><cell>tekma_s</cell><cell>4</cell><cell>7</cell><cell>1</cell><cell>0.36</cell><cell>36</cell><cell>42</cell><cell>5</cell><cell>0.1190</cell></row><row><cell>save_fami</cell><cell>7</cell><cell>6</cell><cell>3</cell><cell>0.54</cell><cell>62</cell><cell>70</cell><cell>20</cell><cell>0.2857</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="24,88.99,90.49,417.00,296.86"><head>Table 10</head><label>10</label><figDesc>Experimental systems of round 2 and the corresponding number of clicks on SERP elements, total number of clicks, and the Reward score.</figDesc><table coords="24,95.27,131.31,410.56,256.03"><row><cell></cell><cell>Bookmark</cell><cell>Details</cell><cell>Fulltext</cell><cell>In Stock</cell><cell>More Links</cell><cell>Order</cell><cell>Title</cell><cell>Total Clicks</cell><cell>nReward</cell></row><row><cell>livivo_rank_pyserini</cell><cell>182</cell><cell>341</cell><cell>176</cell><cell>55</cell><cell>62</cell><cell>28</cell><cell>263</cell><cell cols="2">1107 0.4367</cell></row><row><cell>livivo_base</cell><cell>180</cell><cell>443</cell><cell>228</cell><cell>154</cell><cell>57</cell><cell>29</cell><cell>329</cell><cell cols="2">1420 0.5633</cell></row><row><cell>lemuren_elastic_only</cell><cell>63</cell><cell>832</cell><cell>481</cell><cell>107</cell><cell>105</cell><cell>54</cell><cell>638</cell><cell cols="2">2280 0.4045</cell></row><row><cell>livivo_base</cell><cell>56</cell><cell cols="2">1066 646</cell><cell>295</cell><cell>129</cell><cell>85</cell><cell>858</cell><cell cols="2">3135 0.5955</cell></row><row><cell cols="2">lemuren_elastic_preprocessing 23</cell><cell>355</cell><cell>257</cell><cell>23</cell><cell>28</cell><cell>21</cell><cell>285</cell><cell>992</cell><cell>0.2143</cell></row><row><cell>livivo_base</cell><cell>69</cell><cell cols="2">1190 762</cell><cell>301</cell><cell>119</cell><cell>82</cell><cell>934</cell><cell cols="2">3457 0.7857</cell></row><row><cell>lemuren_elk</cell><cell>1</cell><cell>13</cell><cell>16</cell><cell>0</cell><cell>2</cell><cell>0</cell><cell>10</cell><cell>42</cell><cell>0.4242</cell></row><row><cell>livivo_base</cell><cell>1</cell><cell>24</cell><cell>7</cell><cell>14</cell><cell>1</cell><cell>0</cell><cell>20</cell><cell>67</cell><cell>0.5758</cell></row><row><cell>tekmas</cell><cell>2</cell><cell>11</cell><cell>2</cell><cell>2</cell><cell>1</cell><cell>0</cell><cell>6</cell><cell>24</cell><cell>0.3430</cell></row><row><cell>livivo_base</cell><cell>0</cell><cell>13</cell><cell>6</cell><cell>7</cell><cell>0</cell><cell>1</cell><cell>9</cell><cell>36</cell><cell>0.6570</cell></row><row><cell>save_fami</cell><cell>11</cell><cell>21</cell><cell>9</cell><cell>3</cell><cell>1</cell><cell>1</cell><cell>16</cell><cell>62</cell><cell>0.5496</cell></row><row><cell>livivo_base</cell><cell>8</cell><cell>13</cell><cell>7</cell><cell>5</cell><cell>2</cell><cell>1</cell><cell>6</cell><cell>42</cell><cell>0.4504</cell></row><row><cell>All experimental systems</cell><cell>282</cell><cell cols="2">1573 941</cell><cell>190</cell><cell>199</cell><cell>104</cell><cell cols="3">1218 4507 0.3485</cell></row><row><cell>livivo_base</cell><cell>314</cell><cell cols="3">2749 1656 776</cell><cell>308</cell><cell>198</cell><cell cols="3">2156 8157 0.6515</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="28,88.99,127.53,416.99,175.25"><head>Table 11</head><label>11</label><figDesc>Top ten queries clicked at LIVIVO during both rounds. Query strings were normalized and special characters removed.</figDesc><table coords="28,196.29,168.88,202.69,133.90"><row><cell cols="2">Rank Query string</cell><cell>Clicks</cell></row><row><cell>1</cell><cell>polyvinyl and nasal and packing</cell><cell>278</cell></row><row><cell>2</cell><cell>nabelschnur</cell><cell>23</cell></row><row><cell>3</cell><cell>rtms and doc</cell><cell>20</cell></row><row><cell>4</cell><cell>vegan</cell><cell>19</cell></row><row><cell>5</cell><cell>fußball subkultur</cell><cell>19</cell></row><row><cell>6</cell><cell>anrede</cell><cell>19</cell></row><row><cell>7</cell><cell>skoliose and schroth</cell><cell>18</cell></row><row><cell>8</cell><cell>clown therapy</cell><cell>17</cell></row><row><cell>9</cell><cell>vegan ernährung kind</cell><cell>17</cell></row><row><cell>10</cell><cell>mammakarzinom</cell><cell>17</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="29,88.99,129.83,411.99,213.93"><head>Table 12</head><label>12</label><figDesc>Top ten documents for which recommendations were clicked at GESIS during both rounds.</figDesc><table coords="29,95.27,161.45,95.04,8.87"><row><cell>Rank Document title</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="30,88.99,135.90,391.86,177.57"><head>Table 13</head><label>13</label><figDesc>Titles of the top ten documents clicked at LIVIVO during both rounds.</figDesc><table coords="30,114.42,167.51,85.37,8.87"><row><cell>Rank Query string</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="31,88.99,136.01,403.01,190.22"><head>Table 14</head><label>14</label><figDesc>Top ten datasets clicked at GESIS during both rounds.</figDesc><table coords="31,100.79,167.62,95.04,8.87"><row><cell>Rank Document title</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.93,671.02,159.97,8.97"><p>https://ir.nist.gov/covidSubmit/archive.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,108.93,671.02,79.37,8.97"><p>https://www.livivo.de</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,108.93,671.04,115.19,8.97"><p>https://www.livivo.de//covid19.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,108.93,671.01,87.17,8.97"><p>https://search.gesis.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="7,108.93,671.04,155.39,8.97"><p>https://datasetsearch.research.google.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="8,108.93,671.03,200.89,8.97"><p>https://github.com/stella-project/stella-micro-template</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="9,108.93,649.12,157.75,8.97"><p>https://github.com/stella-project/stella-app</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="9,108.93,660.08,108.81,8.97"><p>https://lilas.stella-project.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="9,108.93,671.04,167.04,8.97"><p>https://github.com/stella-project/stella-server</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9" coords="10,108.93,671.03,196.36,8.97"><p>https://github.com/stella-project/syntax_checker_CLI</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_10" coords="12,108.93,670.91,179.35,8.97"><p>https://th-koeln.sciebo.de/s/OBm0NLEwz1RYl9N</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_11" coords="14,108.93,671.02,186.20,8.97"><p>https://github.com/stella-project/stella-evaluations</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_12" coords="19,108.93,671.02,197.60,8.97"><p>https://github.com/stella-project/livivo_rank_pyserini</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_13" coords="32,114.98,108.58,36.95,12.03;32,114.98,115.95,36.95,12.03;32,114.98,123.33,36.95,12.03;32,114.98,130.70,36.95,12.03;32,114.98,138.08,36.95,12.03;32,114.98,145.45,36.95,12.03;32,114.98,152.83,36.95,12.03;32,114.98,160.20,36.95,12.03;32,114.98,167.57,36.95,12.03;32,114.98,174.95,36.95,12.03;32,114.98,182.32,36.95,12.03;32,114.98,189.70,36.95,12.03;32,114.98,197.07,36.95,12.03;32,114.98,204.45,36.95,12.03;32,114.98,211.82,36.95,12.03;32,114.98,219.20,36.95,12.03;32,114.98,226.57,36.95,12.03;32,114.98,233.95,36.95,12.03;32,114.98,241.32,36.95,12.03;32,114.98,248.69,36.95,12.03;32,114.98,256.07,36.95,12.03;32,114.98,263.44,36.95,12.03;32,114.98,270.82,36.95,12.03;32,114.98,278.19,36.95,12.03;32,114.98,285.57,36.95,12.03;32,114.98,292.94,36.95,12.03;32,114.98,300.32,36.95,12.03;32,114.98,307.69,36.95,12.03;32,114.98,315.07,36.95,12.03;32,114.98,322.44,36.95,12.03;32,114.98,329.81,36.95,12.03;32,114.98,337.19,36.95,12.03;32,114.98,344.56,36.95,12.03;32,114.98,351.94,36.95,12.03;32,114.98,359.31,36.95,12.03;32,114.98,366.69,36.95,12.03;32,114.98,374.06,36.95,12.03;32,114.98,381.44,36.95,12.03;32,114.98,388.81,36.95,12.03;32,114.98,396.19,36.95,12.03;32,114.98,403.56,36.95,12.03;32,114.98,410.93,36.95,12.03;32,114.98,418.31,36.95,12.03;32,114.98,425.68,36.95,12.03;32,114.98,433.06,36.95,12.03;32,114.98,440.43,36.95,12.03;32,114.98,447.81,36.95,12.03;32,114.98,455.18,36.95,12.03;32,114.98,462.56,36.95,12.03;32,114.98,469.93,36.95,12.03;32,114.98,477.31,36.95,12.03;32,114.98,484.68,36.95,12.03;32,114.98,492.05,36.95,12.03;32,114.98,499.43,36.95,12.03;32,114.98,506.80,36.95,12.03;32,114.98,514.18,36.95,12.03;32,114.98,521.55,36.95,12.03;32,114.98,528.93,36.95,12.03;32,114.98,536.30,36.95,12.03;32,114.98,543.68,36.95,12.03;32,114.98,551.05,36.95,12.03;32,114.98,558.42,36.95,12.03;32,114.98,565.80,36.95,12.03;32,114.98,573.17,36.95,12.03;32,114.98,580.55,36.95,12.03;32,114.98,587.92,36.95,12.03;32,114.98,595.30,36.95,12.03;32,114.98,602.67,36.95,12.03;32,114.98,610.05,36.95,12.03;32,114.98,617.42,36.95,12.03;32,114.98,624.80,36.95,12.03;32,114.98,632.17,36.95,12.03;32,114.98,639.54,36.95,12.03;32,114.98,646.92,36.95,12.03;32,114.98,654.29,36.95,12.03;32,114.98,661.67,36.95,12.03"><p>2021-03-02 2021-03-03 2021-03-04 2021-03-05 2021-03-06 2021-03-07 2021-03-08 2021-03-09 2021-03-10 2021-03-11 2021-03-12 2021-03-13 2021-03-14 2021-03-15 2021-03-16 2021-03-17 2021-03-18 2021-03-19 2021-03-20 2021-03-21 2021-03-22 2021-03-23 2021-03-24 2021-03-25 2021-03-26 2021-03-27 2021-03-28 2021-03-29 2021-03-30 2021-03-31 2021-04-01 2021-04-07 2021-04-08 2021-04-09 2021-04-13 2021-04-14 2021-04-15 2021-04-16 2021-04-17 2021-04-18 2021-04-19 2021-04-20 2021-04-21 2021-04-22 2021-04-23 2021-04-24 2021-04-25 2021-04-26 2021-04-27 2021-04-28 2021-04-29 2021-04-30 2021-05-01 2021-05-02 2021-05-03 2021-05-04 2021-05-05 2021-05-06 2021-05-07 2021-05-08 2021-05-09 2021-05-10 2021-05-11 2021-05-12 2021-05-13 2021-05-14 2021-05-15 2021-05-16 2021-05-17 2021-05-18 2021-05-19 2021-05-20 2021-05-21 2021-05-22 2021-05-23 2021-05-24</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported by <rs type="funder">DFG</rs> (project no. <rs type="grantNumber">407518790</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_YWJ4J5s">
					<idno type="grant-number">407518790</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="25,112.66,587.85,393.32,10.91;25,112.66,601.40,393.33,10.91;25,112.66,614.95,225.52,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="25,352.86,587.85,153.12,10.91;25,112.66,601.40,249.36,10.91">Newsreel multimedia at mediaeval 2018: News recommendation with image and text content</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lommatzsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kille</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hopfgartner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ramming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="25,384.44,601.40,121.55,10.91;25,112.66,614.95,144.06,10.91">Working Notes Proceedings of the MediaEval 2018 Workshop</title>
		<imprint>
			<publisher>CEUR-WS</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,628.50,395.17,10.91;25,112.66,642.04,394.53,10.91;25,102.87,669.46,5.56,5.98;25,108.93,671.02,118.75,8.97;26,112.66,86.97,395.17,10.91;26,112.66,100.52,393.58,10.91;26,112.66,114.06,393.32,10.91;26,112.66,127.61,395.00,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="25,242.41,628.50,265.42,10.91;25,112.66,642.04,92.17,10.91">Overview of the living labs for information retrieval evaluation (LL4IR) CLEF lab</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24027-5_47</idno>
		<ptr target="https://github.com/stella-projectE" />
	</analytic>
	<monogr>
		<title level="m" coord="26,300.97,86.97,206.85,10.91;26,112.66,100.52,393.58,10.91;26,112.66,114.06,17.79,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction -6th International Conference of the CLEF Association, CLEF 2015</title>
		<title level="s" coord="26,438.47,115.08,67.51,9.72;26,112.66,128.63,72.91,9.72">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Sanjuan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><surname>Ferro</surname></persName>
		</editor>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015-09-08">2015. September 8-11, 2015. 9283. 2015</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="484" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,141.16,393.72,10.91;26,112.66,154.71,393.33,10.91;26,112.66,168.26,164.24,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="26,462.21,141.16,44.17,10.91;26,112.66,154.71,152.96,10.91">Overview of the trec 2016 open search track</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dekker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schaer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tavakolpoursaleh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P.-Y</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="26,290.61,154.71,215.38,10.91;26,112.66,168.26,107.39,10.91">Proceedings of the Twenty-Fifth Text REtrieval Conference (TREC 2016)</title>
		<meeting>the Twenty-Fifth Text REtrieval Conference (TREC 2016)</meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,181.81,394.81,10.91" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>De Solla Price</surname></persName>
		</author>
		<title level="m" coord="26,197.24,181.81,56.25,10.91">Little Science</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Columbia University Press</publisher>
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,195.36,393.33,10.91;26,112.66,208.91,397.48,10.91;26,112.36,224.90,140.39,7.90" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="26,457.07,195.36,48.92,10.91;26,112.66,208.91,181.98,10.91">Evaluation infrastructures for academic shared tasks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schaible</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Breuer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tavakolpoursaleh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wolff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schaer</surname></persName>
		</author>
		<idno type="DOI">10.1007/s13222-020-00335-x</idno>
	</analytic>
	<monogr>
		<title level="j" coord="26,302.74,208.91,96.21,10.91">Datenbank-Spektrum</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="29" to="36" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,236.01,394.53,10.91;26,112.66,249.56,395.17,10.91;26,112.66,263.11,395.17,10.91;26,112.66,276.66,395.01,10.91;26,112.66,290.20,395.01,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="26,112.66,249.56,395.17,10.91;26,112.66,263.11,36.77,10.91">Continuous Evaluation of Large-Scale Information Access Systems: A Case for Living Labs</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hopfgartner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lommatzsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kille</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Larson</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-22948-1_21</idno>
	</analytic>
	<monogr>
		<title level="m" coord="26,298.12,263.11,209.72,10.91;26,112.66,276.66,44.59,10.91">Information Retrieval Evaluation in a Changing World</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="511" to="543" />
		</imprint>
	</monogr>
	<note>series Title: The Information Retrieval Series</note>
</biblStruct>

<biblStruct coords="26,112.66,303.75,394.53,10.91;26,112.66,317.30,393.33,10.91;26,112.66,330.85,275.97,10.91" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="26,216.83,317.30,289.16,10.91;26,112.66,330.85,41.60,10.91">TREC-COVID: constructing a pandemic information retrieval test collection</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04474</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,344.40,395.17,10.91;26,112.66,357.95,395.17,10.91;26,112.66,371.50,395.17,10.91;26,112.66,385.05,395.01,10.91;26,112.66,398.60,155.44,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="26,263.10,344.40,244.73,10.91;26,112.66,357.95,347.95,10.91">Critically Examining the &quot;Neural Hype&quot;: Weak Baselines and the Additivity of Effectiveness Gains from Neural Ranking Models</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3331184.3331340</idno>
	</analytic>
	<monogr>
		<title level="m" coord="26,488.38,357.95,19.45,10.91;26,112.66,371.50,395.17,10.91;26,112.66,385.05,179.59,10.91">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval -SIGIR&apos;19</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval -SIGIR&apos;19<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1129" to="1132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,412.15,393.33,10.91;26,112.66,425.70,393.32,10.91;26,112.66,439.25,395.01,10.91;26,112.66,452.79,155.44,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="26,328.39,412.15,177.60,10.91;26,112.66,425.70,96.58,10.91">Improvements that don&apos;t add up: ad-hoc retrieval results since</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">G</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Webber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
		<idno type="DOI">10.1145/1645953.1646031</idno>
	</analytic>
	<monogr>
		<title level="m" coord="26,253.78,425.70,252.20,10.91;26,112.66,439.25,180.89,10.91">Proceeding of the 18th ACM conference on information and knowledge management, CIKM &apos;09</title>
		<meeting>eeding of the 18th ACM conference on information and knowledge management, CIKM &apos;09<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998. 2009</date>
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,466.34,393.33,10.91;26,112.66,479.89,393.33,10.91;26,112.66,493.44,393.33,10.91;26,112.66,506.99,393.33,10.91;26,112.41,520.54,302.51,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="26,213.25,466.34,292.74,10.91;26,112.66,479.89,192.30,10.91">On the connection between citation-based and topical relevance ranking: Results of a pretest using isearch</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Carevic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schaer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="26,331.66,479.89,174.33,10.91;26,112.66,493.44,393.33,10.91;26,112.66,506.99,158.06,10.91">Proceedings of the First Workshop on Bibliometric-enhanced Information Retrieval co-located with 36th European Conference on Information Retrieval (ECIR 2014)</title>
		<title level="s" coord="26,147.03,520.54,167.49,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting>the First Workshop on Bibliometric-enhanced Information Retrieval co-located with 36th European Conference on Information Retrieval (ECIR 2014)<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-13">April 13, 2014. 2014</date>
			<biblScope unit="volume">1143</biblScope>
			<biblScope unit="page" from="37" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,534.09,393.73,10.91;26,112.66,547.64,394.53,10.91;26,112.66,561.19,393.33,10.91;26,112.66,574.74,393.33,10.91;26,112.66,588.29,393.33,10.91;26,112.66,601.84,142.00,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="26,461.99,534.09,44.40,10.91;26,112.66,547.64,200.94,10.91">Overview of lilas 2021 -living labs for academic search</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schaer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Breuer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wolff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schaible</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tavakolpoursaleh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="26,447.41,561.19,58.58,10.91;26,112.66,574.74,393.33,10.91;26,112.66,588.29,253.92,10.91">Proceedings of the Twelfth International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="26,474.16,589.30,31.82,9.72;26,112.66,602.85,112.43,9.72">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Candan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Larsen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting>the Twelfth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="volume">12880</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="26,112.66,615.39,393.33,10.91;26,112.66,628.93,394.04,10.91;26,112.66,642.48,256.69,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="26,347.05,615.39,158.94,10.91;26,112.66,628.93,73.46,10.91">LIVIVO -the Vertical Search Engine for Life Sciences</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Poley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pössel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hagelstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gübitz</surname></persName>
		</author>
		<idno type="DOI">10.1007/s13222-016-0245-2</idno>
		<ptr target="https://doi.org/10.1007/s13222-016-0245-2.doi:10.1007/s13222-016-0245-2" />
	</analytic>
	<monogr>
		<title level="j" coord="26,197.10,628.93,97.74,10.91">Datenbank-Spektrum</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="29" to="34" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,656.03,393.33,10.91;26,112.66,669.58,393.33,10.91;27,112.66,86.97,395.01,10.91;27,112.66,100.52,155.44,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="26,374.51,656.03,131.48,10.91;26,112.66,669.58,221.69,10.91">A digital library for research data and related information in the social sciences</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hienert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Boland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zapilko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mutschke</surname></persName>
		</author>
		<idno type="DOI">10.1109/JCDL.2019.00030</idno>
	</analytic>
	<monogr>
		<title level="m" coord="26,357.29,669.58,148.69,10.91;27,112.66,86.97,139.11,10.91">19th ACM/IEEE Joint Conference on Digital Libraries, JCDL 2019</title>
		<meeting><address><addrLine>Champaign, IL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">June 2-6, 2019, 2019</date>
			<biblScope unit="page" from="148" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,114.06,394.52,10.91;27,112.66,127.61,394.53,10.91;27,112.28,141.16,393.70,10.91;27,112.66,154.71,394.53,10.91;27,112.28,168.26,257.14,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="27,272.92,114.06,230.11,10.91">How does clickthrough data reflect retrieval quality?</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kurup</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<idno type="DOI">10.1145/1458082.1458092</idno>
	</analytic>
	<monogr>
		<title level="m" coord="27,216.83,141.16,289.15,10.91;27,112.66,154.71,163.65,10.91">Proceedings of the 17th ACM Conference on Information and Knowledge Management, CIKM 2008</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Shanahan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Amer-Yahia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Manolescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Evans</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Kolcz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Choi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Chowdhury</surname></persName>
		</editor>
		<meeting>the 17th ACM Conference on Information and Knowledge Management, CIKM 2008<address><addrLine>Napa Valley, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">October 26-30, 2008. 2008</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,181.81,393.32,10.91;27,112.66,195.36,393.59,10.91;27,112.33,208.91,393.65,10.91;27,112.66,222.46,395.01,10.91;27,112.66,236.01,155.44,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="27,275.89,181.81,230.10,10.91;27,112.66,195.36,113.93,10.91">Arxivdigest: A living lab for personalized scientific literature recommendation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gingstad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ø</forename><surname>Jekteberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<idno type="DOI">10.1145/3340531.3417417</idno>
	</analytic>
	<monogr>
		<title level="m" coord="27,141.72,208.91,364.26,10.91;27,112.66,222.46,56.21,10.91">CIKM &apos;20: The 29th ACM International Conference on Information and Knowledge Management</title>
		<editor>
			<persName><forename type="first">M</forename><surname>D'aquin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Dietze</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Hauff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Curry</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Cudré-Mauroux</surname></persName>
		</editor>
		<meeting><address><addrLine>Virtual Event, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">October 19-23, 2020. 2020</date>
			<biblScope unit="page" from="3393" to="3396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,249.56,394.52,10.91;27,112.28,263.11,394.91,10.91;27,112.28,276.66,393.71,10.91;27,112.66,290.20,262.05,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="27,112.28,263.11,285.29,10.91">Ad-hoc retrieval of scientific documents on the livivo search portal</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">H M</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kruff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Thos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Krah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Reiners</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ax</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brech</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gharib</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Pawlas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="27,263.99,276.66,242.00,10.91;27,112.66,290.20,93.98,10.91">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="27,214.65,290.20,129.93,10.91">CEUR Workshop Proceedings</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,303.75,393.33,10.91;27,112.66,317.30,393.33,10.91;27,112.33,330.85,393.65,10.91;27,112.14,344.40,130.59,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="27,215.49,303.75,290.49,10.91;27,112.66,317.30,163.95,10.91">Tekma at clef-2021: Bm-25 based rankings for scientific publication retrieval and data set recommendation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">P M</forename><surname>Munz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="27,141.47,330.85,330.60,10.91">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="27,479.78,330.85,26.20,10.91;27,112.14,344.40,100.46,10.91">CEUR Workshop Proceedings</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,357.95,393.53,10.91;27,112.66,371.50,393.33,10.91;27,112.33,385.05,393.65,10.91;27,112.14,398.60,130.59,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="27,270.71,357.95,235.48,10.91;27,112.66,371.50,163.74,10.91">Pyterrier-based research data recommendations for scientific articles in the social sciences</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tavakolpoursaleh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schaible</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="27,141.47,385.05,330.60,10.91">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="27,479.78,385.05,26.20,10.91;27,112.14,398.60,100.46,10.91">CEUR Workshop Proceedings</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,412.15,393.33,10.91;27,112.66,425.70,393.33,10.91;27,112.66,439.25,198.20,10.91" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="27,358.16,412.15,147.83,10.91;27,112.66,425.70,357.58,10.91">Pyserini: An easy-to-use python toolkit to support replicable IR research with sparse and dense representations</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10073</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,452.79,393.33,10.91;27,112.66,466.34,284.88,10.91;32,317.20,101.20,36.95,12.03;32,317.20,108.58,36.95,12.03;32,317.20,115.95,36.95,12.03;32,317.20,123.33,36.95,12.03;32,317.20,130.70,36.95,12.03;32,317.20,138.08,36.95,12.03;32,317.20,145.45,36.95,12.03;32,317.20,152.83,36.95,12.03;32,317.20,160.20,36.95,12.03;32,317.20,167.57,36.95,12.03;32,317.20,174.95,36.95,12.03;32,317.20,182.32,36.95,12.03;32,317.20,189.70,36.95,12.03;32,317.20,197.07,36.95,12.03;32,317.20,204.45,36.95,12.03;32,317.20,211.82,36.95,12.03;32,317.20,219.20,36.95,12.03;32,317.20,226.57,36.95,12.03;32,317.20,233.95,36.95,12.03;32,317.20,241.32,36.95,12.03;32,317.20,248.69,36.95,12.03;32,317.20,256.07,36.95,12.03;32,317.20,263.44,36.95,12.03;32,317.20,270.82,36.95,12.03;32,317.20,278.19,36.95,12.03;32,317.20,285.57,36.95,12.03;32,317.20,292.94,36.95,12.03;32,317.20,300.32,36.95,12.03;32,317.20,307.69,36.95,12.03;32,317.20,315.07,36.95,12.03;32,317.20,322.44,36.95,12.03;32,317.20,329.81,36.95,12.03;32,317.20,337.19,36.95,12.03;32,317.20,344.56,36.95,12.03;32,317.20,351.94,36.95,12.03;32,317.20,359.31,36.95,12.03;32,317.20,366.69,36.95,12.03;32,317.20,374.06,36.95,12.03;32,317.20,381.44,36.95,12.03;32,317.20,388.81,36.95,12.03;32,317.20,396.19,36.95,12.03;32,317.20,403.56,36.95,12.03;32,317.20,410.93,36.95,12.03;32,317.20,418.31,36.95,12.03;32,317.20,425.68,36.95,12.03;32,317.20,433.06,36.95,12.03;32,317.20,440.43,36.95,12.03;32,317.20,447.81,36.95,12.03;32,317.20,455.18,36.95,12.03;32,317.20,462.56,36.95,12.03;32,317.20,469.93,36.95,12.03;32,317.20,477.31,36.95,12.03;32,317.20,484.68,36.95,12.03;32,317.20,492.05,36.95,12.03;32,317.20,499.43,36.95,12.03;32,317.20,506.80,36.95,12.03;32,317.20,514.18,36.95,12.03;32,317.20,521.55,36.95,12.03;32,317.20,528.93,36.95,12.03;32,317.20,536.30,36.95,12.03;32,317.20,543.68,36.95,12.03;32,317.20,551.05,36.95,12.03;32,317.20,558.42,36.95,12.03;32,317.20,565.80,36.95,12.03;32,317.20,573.17,36.95,12.03;32,317.20,580.55,36.95,12.03;32,317.20,587.92,36.95,12.03;32,317.20,595.30,36.95,12.03;32,317.20,602.67,36.95,12.03;32,317.20,610.05,36.95,12.03;32,317.20,617.42,36.95,12.03;32,317.20,624.80,36.95,12.03;32,317.20,632.17,36.95,12.03;32,317.20,639.54,36.95,12.03;32,317.20,646.92,36.95,12.03;32,317.20,654.29,36.95,12.03;32,317.20,661.67,36.95,12.03;32,353.75,96.86,12.03,4.05;32,374.80,88.77,117.30,12.14;32,500.54,317.03,14.43,33.10;32,500.54,352.55,14.43,10.92;32,500.54,365.90,14.43,45.92;32,500.54,414.25,14.43,2.76;32,500.54,419.44,14.43,42.08;32,486.65,629.17,12.03,27.58;32,477.32,629.17,12.03,38.27;32,114.98,101.20,36.95,12.03" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="27,265.25,452.79,240.74,10.91;27,112.66,466.34,40.57,10.91">Opensearch: Lessons learned from an online evaluation campaign</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jagerman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<idno>15. 2021-03-01 2021-03-02 2021-03-03 2021-03-04 2021-03-05 2021-03-06 2021-03-07 2021-03-08 2021-03-09 2021-03-10 2021-03-11 2021-03-12 2021-03-13 2021-03-14 2021-03-15 2021-03-16 2021-03-17 2021-03-18 2021-03-19 2021-03-20 2021-03-21 2021-03-22 2021-03-23 2021-03-24 2021-03-25 2021-03-26 2021-03-27 2021-03-28 2021-03-29 2021-03-30 2021-03-31 2021-04-01 2021-04-07 2021-04-08 2021-04-09 2021-04-13 2021-04-14 2021-04-15 2021-04-16 2021-04-17 2021-04-18 2021-04-19 2021-04-20 2021-04-21 2021-04-22 2021-04-23 2021-04-24 2021-04-25 2021-04-26 2021-04-27 2021-04-28 2021-04-29 2021-04-30 2021-05-01 2021-05-02 2021-05-03 2021-05-04 2021-05-05 2021-05-06 2021-05-07 2021-05-08 2021-05-09 2021-05-10 2021-05-11 2021-05-12 2021-05-13 2021-05-14 2021-05-15 2021-05-16 2021-05-17 2021-05-18 2021-05-19 2021-05-20 2021</idno>
	</analytic>
	<monogr>
		<title level="m" coord="32,500.54,365.90,14.43,45.92;32,500.54,414.25,14.43,2.76;32,500.54,419.44,14.43,42.08;32,486.65,629.17,12.03,27.58;32,477.32,629.17,12.03,38.27;32,114.98,101.20,14.78,12.03">Impressions -livivo_base Sessions Impressions 2021</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="3" to="4" />
		</imprint>
	</monogr>
	<note>Sessions vs</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
