<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.69,84.74,377.26,15.42;1,89.29,106.66,167.35,15.42">Ad-hoc Retrieval of Scientific Documents on the LIVIVO Search Portal</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.87,134.97,45.44,11.96"><forename type="first">Anh</forename><surname>Huy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">TH K√∂ln -University of Applied Sciences</orgName>
								<address>
									<settlement>Cologne</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,137.30,134.97,69.12,11.96"><forename type="first">Matthias</forename><surname>Tran</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">TH K√∂ln -University of Applied Sciences</orgName>
								<address>
									<settlement>Cologne</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,219.41,134.97,69.02,11.96"><forename type="first">Andreas</forename><surname>Kruff</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">TH K√∂ln -University of Applied Sciences</orgName>
								<address>
									<settlement>Cologne</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,301.40,134.97,60.03,11.96"><forename type="first">Joshua</forename><surname>Thos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">TH K√∂ln -University of Applied Sciences</orgName>
								<address>
									<settlement>Cologne</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,374.41,134.97,81.06,11.96"><forename type="first">Constantin</forename><surname>Krah</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">TH K√∂ln -University of Applied Sciences</orgName>
								<address>
									<settlement>Cologne</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,148.92,81.34,11.96"><forename type="first">Michelle</forename><surname>Reiners</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">TH K√∂ln -University of Applied Sciences</orgName>
								<address>
									<settlement>Cologne</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,183.61,148.92,49.49,11.96"><forename type="first">Fabian</forename><surname>Ax</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">TH K√∂ln -University of Applied Sciences</orgName>
								<address>
									<settlement>Cologne</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,246.09,148.92,62.10,11.96"><forename type="first">Saskia</forename><surname>Brech</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">TH K√∂ln -University of Applied Sciences</orgName>
								<address>
									<settlement>Cologne</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,321.16,148.92,69.59,11.96"><forename type="first">Sascha</forename><surname>Gharib</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">TH K√∂ln -University of Applied Sciences</orgName>
								<address>
									<settlement>Cologne</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,422.08,148.92,70.95,11.96"><forename type="first">Verena</forename><surname>Pawlas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">TH K√∂ln -University of Applied Sciences</orgName>
								<address>
									<settlement>Cologne</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.69,84.74,377.26,15.42;1,89.29,106.66,167.35,15.42">Ad-hoc Retrieval of Scientific Documents on the LIVIVO Search Portal</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">3E3E3E3710C9BF59CC38701F4D5AA3C9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Elasticsearch</term>
					<term>Information Retrieval</term>
					<term>Preprocessing</term>
					<term>Tokenization</term>
					<term>Multi-Language</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper documents the participation in LiLAS @ CLEF2021 concerning the ad-hoc retrieval of scientific documents for LIVIVO and illustrates several approaches on tackling the problem of multi-language search on the scientific corpus found on LIVIVO using Elasticsearch and the STELLA framework with two different search systems: Ingestion Pipelines to offer multi-language indexing with half the indexing space and an elaborate preprocessing process in tandem with language libraries such as Google Translate and SpaCy. The results show, however, that while relevant results appear to be of higher quality in comparison to the base system found on LIVIVO, the general search and relevance performance of these two systems turn out to be demonstrably worse than the base system in terms of click metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In LiLAS <ref type="bibr" coords="1,131.04,393.53,12.79,10.91" target="#b0">[1]</ref> one of the two tasks involved developing an ad-hoc retrieval system for scientific documents for the multi-source life science search portal LIVIVO over two separate rounds, where tackling the problems and complexity of supporting multi-language search queries in an effective manner quickly became an important aspect. Therefore, the aim of this paper lies in implementing and testing multiple measures ranging from multi-fields, ingestion processes and involving the use of language libraries like SpaCy <ref type="bibr" coords="1,311.65,461.28,12.79,10.91" target="#b1">[2]</ref> and Google Translate <ref type="bibr" coords="1,424.58,461.28,12.79,10.91" target="#b2">[3]</ref> in an elaborate preprocessing measure in order to improve the search system with respect to retrieving relevant queries for both English and German language. For more details on the different tasks and the setup of the lab, please see the official lab overview <ref type="bibr" coords="1,319.36,501.92,11.43,10.91" target="#b3">[4]</ref>.</p><p>With the help of the STELLA Infrastructure for Living Labs <ref type="bibr" coords="1,365.01,515.47,11.35,10.91" target="#b4">[5]</ref>, two different lines of search systems by two groups were developed over the two rounds and implemented into the working environment of the LIVIVO platform. While one line of systems(lemuren_elk and LEO) is completely relying on the built-in functions of Elasticsearch <ref type="bibr" coords="1,334.40,556.12,11.28,10.91" target="#b5">[6]</ref>, the other line of systems(save_fami and LEPREP) outsources the preprocessing of the documents to a Python application with the aim to find out which approach performs better. The Team-Draft-Interleaving (TDI) <ref type="bibr" coords="1,458.00,583.22,12.68,10.91" target="#b6">[7]</ref> process allows us to deliver search results from both the experimental systems and the base system to a user, allowing us to directly compare the performance of the two systems with the current existing base system. In this task, the retrieval approaches are evaluated in two separate rounds, where the first preliminary round enables us to first test out the first approaches with pre-computed runs in an easy and fast to implement manner and then act upon the received feedback to create an improved, fully dockerized version for the following second round. As such, the paper will describe the implemented retrieval approaches in the first round and then the two subsequent improved and dockerized implementations in the subsequent second round: lemuren_elastic_only (LEO) and lemuren_elastic_preprocessing (LEPREP) (See Table <ref type="table" coords="2,347.17,423.19,5.07,10.91" target="#tab_0">1</ref> and Figure <ref type="figure" coords="2,405.19,423.19,3.57,10.91">1</ref>). Consequently, the paper as a whole is structured as follows: In Section <ref type="bibr" coords="2,417.01,436.74,11.66,10.91" target="#b1">[2]</ref>, we give a short overview of the many tools and resources utilized in this paper. In Section <ref type="bibr" coords="2,417.73,450.29,11.26,10.91" target="#b2">[3]</ref>, the implementation of the first round of this task is discussed. In Section <ref type="bibr" coords="2,352.25,463.84,13.10,10.91" target="#b3">[4]</ref> the frameworks, pipelines and implementation of our two dockerized final systems, LEO and LEPREP, are explained in closer detail. In Section <ref type="bibr" coords="2,166.43,490.94,11.36,10.91" target="#b4">[5]</ref>, the performance of the systems from both the first and second round are analysed, evaluated and then compared with the existing base system on LIVIVO according to self-defined metrics. Finally, in Section <ref type="bibr" coords="2,269.24,518.03,11.66,10.91" target="#b5">[6]</ref>, we discuss our results in a short conclusion and provide pointers for possible future improvements and research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Tools &amp; Resources</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">LIVIVO Data set</head><p>The basis of this research paper deals with the LIVIVO search portal, which contains a huge data set of more than 58 million references to various life science literature. It contains a various amount of different literature, crossing a multi-language domain between German, French, English and other languages <ref type="bibr" coords="2,221.43,651.83,11.59,10.91" target="#b7">[8]</ref>. For this project, a document data subset of over 30 million documents from various databases from LIVIVO were provided, which includes metadata such as titles, abstracts and controlled vocabulary such as MESH and CHEM tokens, of which only a partial subset was indexed on our own systems for the first round, because the given headqueries and the corresponding candidates were also generated on the reduced document corpus. For the ad-hoc retrieval system in round 2 the complete corpus was indexed so that individual user queries could be applied on the full corpus. While the document set is predominantly English, various other document languages such as French or German can be found within the data set. This document data set is complimented with a candidate list with various most common queries used by users of LIVIVO, alongside the specific query frequency and a search result list of candidates ordered by relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">spaCy</head><p>spaCy is an open-source library for the Python language and is used for natural language processing (NLP). The library supports over 64 languages and includes NLP-features such as tokenization, part-of-speech tagging (POS), lemmatization or named entity recognition (NER), which were used for the preprocessing approach to recognize specific tokens, and offers a plenty of prebuilt statistical models for different languages and purposes for usage such as SciSpacy, which is used for specifically scientific language <ref type="bibr" coords="3,310.71,312.83,11.58,10.91" target="#b8">[9]</ref>. In our project the library is used in the save_fami and the LEPREP system for preprocessing the documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Elasticsearch</head><p>Elasticsearch is an open source search and analytics engine for textual, numeric and geospatial data in structured and unstructured form. In this project Elasticsearch was used to create a search system and index of the provided LIVIVO data set. Using Elasticsearch, the technical foundation to preprocess data was built, also it was used to build indexes and enable subsequent search queries <ref type="bibr" coords="3,150.90,430.31,12.97,10.91" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Docker</head><p>Docker is an application platform primarily used for distributing applications and services, the so-called deployment. It enables to deploy the different search systems in isolation without dependency on any other system in an easy and scalable manner. This means that they are significantly more efficient in terms of system resources than "hypervisors" for example. Instead of virtualizing the whole set of hardware, separate containers are run on a singular Linux instances. The biggest advantage is that less resources are needed on private systems and can run processes faster than on a "pure" virtual machine while significantly simplifying deployment in conjunction with the STELLA framework. Docker was used to to implement the search system with individual Elasticsearch and Kibana instances <ref type="bibr" coords="3,347.12,588.43,17.59,10.91" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">STELLA</head><p>As a multi-container-application, STELLA provides a user-focused evaluation approach that allows users to deploy and evaluate search and recommendation systems that they wish to implement in a live environment <ref type="bibr" coords="3,234.39,665.25,11.28,10.91" target="#b4">[5]</ref>. For this purpose, STELLA accepts the submission of either pre-computed runs or full docker images that follow the pre-defined REST-API, enabling full reproducibility of the system. In practice, following the integration of an experimental search system as a docker-image into STELLA, its indexing-endpoint is called when the system in question is called for the first time, where it builds the index with the provided data from LIVIVO. Then for the whole userbase using LIVIVO, their search query gets sent to the ranking-endpoint of both the docker system and the base system, which then return a list of relevant document-ids formed from a mix of results from both the implemented experimental docker system and the base system. Following this, the STELLA app will log user-feedback and activity data such as clicks, downloads, dwell times but also other relevant performance indicators such as wins, losses, the number of sessions and impressions and CTR metrics between the experimental search systems and the on-production base system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach &amp; Implementation: First Round</head><p>The first prototypes of the search systems(save_fami and lemuren_elk) involve a simple reranking of the search results of a given a set of the most common headqueries on LIVIVO, called a pre-computed run, which then transfer this specific re-ranking onto the live system on LIVIVO for just these headqueries. A user querying these specific headqueries on LIVIVO would then retrieve these altered search results. This approach however, would not yield enough raw click data for any kind of adequate analysis, which we solve with the docker container approach mentioned in Section <ref type="bibr" coords="4,186.76,362.38,12.84,10.91" target="#b3">[4]</ref> for the second round.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">SYSTEM 1: lemuren_elk</head><p>The lemuren_elk system<ref type="foot" coords="4,208.37,410.35,3.71,7.97" target="#foot_0">1</ref> is fully based on built-in functions from Elasticsearch, and the index settings were based on a previous Elasticsearch configuration from the TREC-COVID Challenge <ref type="bibr" coords="4,137.38,439.20,16.35,10.91" target="#b10">[11]</ref>, a challenge to study methods on how quickly a reliable information retrieval system specialized on COVID-relevant documents can be built <ref type="bibr" coords="4,365.30,452.75,16.09,10.91" target="#b10">[11]</ref>. For long text fields like the abstract field the DFR similarity model <ref type="bibr" coords="4,264.11,466.30,20.19,10.91" target="#b11">[12]</ref>was determined as a good metric in this challenge, while for the title, the LMJ similarity performed best in comparison to a fully standard base system using BM25 similarity. Based on this observation, these two similarity models were applied on different fields. These exact settings were also used in the second round and will be described in detail later on. Furthermore, like the keyword field, multi-fields were applied for the title field, which was an attempt to implement an exact-match for the system by using the raw title as one keyword. Furthermore the synonym list adapted for common terms and medical terms especially in regard to COVID specific terms were re-used. In general, when compared to the systems implemented in the second round, this system is not language specific, which means that the synonyms, the acronyms, the stopword removal and the stemming procedure were specialized for English language only.</p><p>For the query settings, a different approach in comparison to the TREC-COVID settings was implemented, since the head queries that were retrieved from the supplied head queries list had a significant amount of boolean operators. As such, instead of the built-in "multi_match" function, the "query_string" operator was implemented, because it allows the parsing of multiple boolean operators inside one query. In general three query requests for every query were created. First the query was applied to the three title multi-fields, then it was applied on the abstract and after that it was applied on the "MESH" field. As such, this version already explicitly reads and parses queries with boolean operator logic in mind and applies boosting to the various used fields as described in the later sections.</p><p>To roughly estimate a baseline quality, this result list was then compared with a fixed candidate list from the live base system for the query-list and then complemented with any missing candidate for every query with a fixed minimal relevance score of 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">SYSTEM 2: save_fami</head><p>The save_fami<ref type="foot" coords="5,159.33,243.33,3.71,7.97" target="#foot_1">2</ref> system is characterized by realizing the text preprocessing in Python with usage of spaCy. The choice to use spaCy is mostly based on the fact that great results were already achieved in the TREC-COVID Challenge in a previously finished group project using the CORD-19 data set <ref type="bibr" coords="5,191.05,285.73,16.41,10.91" target="#b10">[11]</ref>. With the help of the subject-specific models, subject terms such as the MESH terms should be better processed. In order to prepare the content of the index properly for queries, it is necessary to analyze and convert the content from the metadata into proper tokens that can be used for an accurate search.</p><p>For the implementation, Elasticsearch (Elastic) as the search engine combined with a Python application was used. Elastic was used for the indexing process and the actual retrieval application. With Python, the remaining tasks, such as creating an indexing pipeline or configuring the search, were realized.</p><p>Finally, the save_fami system appended missing candidates to the results list. However, this system ranked the remaining candidates in descending order according to their position in the candidate list. A so-called pseudo-score was created, which started at the value 1 and was decreased by the value 0.001 after each candidate.</p><p>Further details and improvements to this system can be found in the description of its continuation system LEPREP in Section [4.2].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Approach &amp; Implementation: Second Round</head><p>In the second round, we implemented our systems mentioned in the first round as a fully dockerized version implemented in the live LIVIVO search engine environment and improved them further. The docker container approach allows our experimental systems to dynamically work with all kind of queries sent to LIVIVO instead of only handling the searches for a predefined set of headqueries, enabling us to collect click data on a much bigger scale than we did in the first round (see: Section <ref type="bibr" coords="6,226.24,179.03,11.09,10.91" target="#b4">[5]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">SYSTEM 3: lemuren_elastic_only (LEO)</head><p>The lemuren_elastic_only (LEO) system<ref type="foot" coords="6,267.62,227.00,3.71,7.97" target="#foot_2">3</ref> implemented for the second round is characterized by the fact that mainly only Elasticsearch built-in function were used to create the index and handle the querying process. It is a continuation of the previously mentioned lemuren_elk system (see: 3.1 ).</p><p>In general, it is a straight forward approach, where different languages are indexed in different fields of the same title multi-fields and different language-dependent analyzers for the title and abstract fields are used to tokenize them appropriately. In addition, two different similarity modules are used for longer and shorter texts found in the title and abstracts fields specifically (See Appendix Code-Block A.1).</p><p>An overview of LEO's general pipeline can be seen in Figure <ref type="figure" coords="6,372.47,350.70,3.74,10.91" target="#fig_0">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Multi-field Indexing</head><p>The title-field in the index is separated into four different multi-fields in order to enable a more specific searching of the titles of each documents, each with their own specific language-specific analyzer:</p><p>‚Ä¢ TITLE_EN: Titles processed by English-specific language analyzers ‚Ä¢ TITLE_GER: Titles processed by German-specific language analyzers ‚Ä¢ TITLE.analysis: Titles processed by scientific language analyzers ‚Ä¢ TITLE.keyword: The whole title as a single keyword While TITLE_EN and TITLE_GER follow a language-specific logic defined by an implemented ingestion pipeline, TITLE.analysis contains added acronyms and TITLE.keywords contains the whole title of document as one singular token in order to allow exact-match keyword search for document titles(See Appendix Code-Block A.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Ingestion Pipeline</head><p>In order to apply language specific analyzers on the title field while avoiding the drawback of indexing all titles as separate German and English indexes, an ingestion pipelines was used to create new fields, based on specified conditions, given the content of the original title metadata(See Appendix Code-Block A.2). This allows us the utility of language-specific indexes while retaining the same space size of a regular index.</p><p>In this specific use case, the assigned language field of each document was parsed and used to roughly determine the document's language and subsequently either create a title field for German language (TITLE_GER) or English language (TITLE_EN). This process allows an analysis of the language and to tokenize the title in this language-specific manner without blowing up the index size by indiscriminately creating a German and English index for all documents <ref type="bibr" coords="7,140.68,449.85,16.25,10.91" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Acronyms</head><p>Concerning the handling of acronyms, the built-in "word_delimiter" function was used. It allows the system to split words into subwords and performs optional transformations on subword groups before indexing.</p><p>The "catenate_all" option catenates all subwords parts into one token, like "wi-fi-4000" -&gt; "wifi4000", if set on true. The generate_word_parts parameter was set to false, so that words with case transition (e.g. "PowerShot" -&gt; "Power" "Shot") will not get split on that transition. The generate_number_part parameter was also set to false, to prevent number sequences not get split at special characters (e.g. "500-42" -&gt; "500" "42"). At last the preserve_original parameter was set on true. With that parameter on set true, the original word will also be retained (e.g. "500-42" -&gt; "500-42" "500" "42"). The split_on_numerics parameter was not changed, defaulting true. This parameter will split a word, if a number appears in it (e.g. "j2se" -&gt; "j" "2" "se"). Finally, the stem_english_parameter was not changed which defaulting to true. This setting removes trailings from subwords (e.g. "O'Neil's" -&gt; "O", "Neil").</p><p>The main motivation behind this implementation was to cover multiple options for acronyms so that semantically identical terms such as "USA" and "U.S.A" would both be recognized as the same term and thus would be matched be matched for the same document, preventing a bad user experience due to small spelling differences <ref type="bibr" coords="8,307.44,127.61,16.25,10.91" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4.">Querying</head><p>The querying approach assumed that most of the users would form their queries in either Enlish or German. Under this assumption, with the help of the Google Translator API, each query gets translated in both an English or German version for the purpose of achieving language independent results. Furthermore, any boolean "UND" and "ODER" operators were translated into proper "AND" and "OR" operators, as Elasticsearch's query_string operator only supports english boolean operators within its logic, since the system received many queries using german boolean operators in the first few days of round two, prompting a system update enabling this feature after the fourth day of experiment.</p><p>As the query gets sent into Elasticsearch, it is processed further with various analyzers in the multi-fields to automatically delete any language-specific stop words, tokenize, stem and apply additional asciifolding to prevent encoding problems.</p><p>The query logic in Elasticsearch is implemented specifically in a three-way process, where the English, German and original query are matched against the index in their specific aforementioned multifields:</p><p>‚Ä¢ query_tokenized_eng for TITLE_EN ‚Ä¢ query_tokenized_ger for TITLE_GER ‚Ä¢ query_raw for TITLE.analysis In detail, for every query language (original, English, German), the system searches in every available multi-field in the title and only account for the score retrieved from the best match. While the abstract was also taken into account, a boost on the language-specific title fields by a factor of 2 was applied. Also a match for the original raw query with the TITLE.keywords fields and a boosting factor of 10 on the condition of a match of 75% was applied in order to implement an exact match search where an exact match with a title will always be ranked first in the resulting search list. To fulfill the need of the user without forcing him to match the title of the document exactly we used a parameter called "minimum-should-match", an built-in function from Elasticsearch, that allows a deviation of the exact match by a certain amount.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.5.">Fuzzyness</head><p>In order to further prevent poor search results for mistyped queries, the LEO system is complemented by a fuzzymatch and a fuzzy query-expansion. The parameters resulted from experiments with the "fuzzy_match_expansions" parameter, which limits the maximum amount of terms to which the query expands for fuzzy matching.</p><p>Additionally the "fuzzy_prefix_length" was set to the default of 0 which defines the number of beginning characters left unchanged for fuzzy matching. The parameter "fuzziness" was set to "AUTO" to dynamically define the maximum edit distance allowed for matching. At last "fuzzy_transpositions" is to true so that edits for fuzzy matching include transpositions of two adjacent characters (ùëéùëè ‚Üí ùëèùëé) <ref type="bibr" coords="9,225.08,114.06,16.25,10.91" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.6.">Similarity Models</head><p>Concerning the Ranking Models, two different similarity modules and scoring methods have been implemented:</p><p>‚Ä¢ DFR: Divergence from randomness.</p><p>‚Ä¢ LMJelinekMercer: A Language model based on the Jelinek-Mercer smoothing method. These ranking models were quickly tested and evaluated for their search performance in the title and abstract fields respectively in a previous TREC-Covid Challenge, in order to determine the most optimal one for similar scientific corpus specified for documents relevant to COVID-19 <ref type="bibr" coords="9,102.52,265.28,16.41,10.91" target="#b10">[11]</ref>. Both data sets from the TREC-Covid Challenge and LIVIVO are based on scientific publications in the medical field and thus it was assumed that the approaches from the TREC-Covid Challenge are also generally applicable to for this project. Under these assumption, two different similarity modules were used for the fields; LMJelinekMercer for the title multi-fields and the DFR algorithm for the longer texts found in the abstracts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.7.">Analyzer</head><p>For this implementation, four different analyzers were applied to process different kinds of given data and the incoming queries.</p><p>German Analyzer This analyzer processes German words. It sets all words to lowercase and normalized them, and then stems the words and removes any German stopwords and is applied for the German title field TITLE_GER. Additionally, it uses the German normalizer in order to normalize special characters like "√ü", "√§", "√∂", "√º", "ae", "oe", "ue" and replaces these by more easily parsed letters such as "ss", "a", "o", "u", "a", "o", "u". "Ue" is only replaced if it is not followed by a q <ref type="bibr" coords="9,160.27,477.28,16.25,10.91" target="#b15">[16]</ref>.</p><p>English Analyzer This analyzer is basically the same as the German analyzer with the difference that it removes English stopwords, uses an English stemmer and that it does not normalize the words, because there is no need to do it for English language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Analyzer</head><p>The query analyzer has a few more parameter than the English or German analyzer. It processes the incoming queries and sets them to lowercase, compares the query to a given keyword list so that these specific words wont be separated from each other like "Vitamin D" will not be seperated into "Vitamin" + "D". This analyzer also removes any English stopwords and includes an asciifolding filter. This filter converts tokens like "√°" into "a".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LIVIVO Analyzer</head><p>This analyzer has the same parameters as the query analyzer, but it also has a filter for acronyms. With this filter acronyms like "U.S.A" will be taken as one token and not as single digits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">SYSTEM 4: lemuren_elastic_preprocessing (LEPREP)</head><p>The lemuren_elastic_preprocessing (LEPREP) system<ref type="foot" coords="10,335.52,105.78,3.71,7.97" target="#foot_3">4</ref> implemented for the second round completely takes over the preprocessing from the save_fami system from the first round, utilizing spaCy to realize text preprocessing and creating a custom indexing pipeline(see: 3.2 ).</p><p>The main aspect of improvement over the iteration in the first round lies in the correct implementation of boolean queries as they were removed in the first round due to the translation logic, significant performance improvements through the use of parallelization with the use of the pandarallel module <ref type="bibr" coords="10,189.61,188.83,20.53,10.91" target="#b16">[17]</ref> and the dockerization enabling us to integrate the LEPREP search system into the live LIVIVO system for significantly more click data.</p><p>An overview of the general pipeline of the system can be seen in Figure <ref type="figure" coords="10,422.31,215.93,3.74,10.91" target="#fig_1">3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Tokenization</head><p>The indexing pipeline was built with Python and the spaCy module. For this purpose a reference that introduced the approaches of different ranking methods in Elastic was quite helpful <ref type="bibr" coords="10,487.28,528.12,16.31,10.91" target="#b17">[18]</ref>. Irrelevant characters and stop words -even words that are considered stop words in the specific scientific context -are removed. The remaining words are put into their base form by using lemmatization.</p><p>By means of a comprehensive NLP model of SpaCy with the designation en_core_sci_lg, which is suitable for the treated scientific topics, the subject-related tokens MESH and CHEM were generated which are of primary relevance for the further course <ref type="bibr" coords="10,389.41,609.42,16.09,10.91" target="#b18">[19]</ref>. Additionally, the NLP model de_core_news_lg containing a preprocessing pipeline optimized for German language was used for processing German vocabulary <ref type="bibr" coords="10,284.51,636.52,16.09,10.91" target="#b19">[20]</ref>. But since Elastic itself also uses a tokenizer, it was necessary to create a new token only with the use of each blank character <ref type="bibr" coords="10,440.96,650.07,16.25,10.91" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Language Handling</head><p>Googletrans is a free and publicly available Python library that calls language detection and translation methods using the Google Translate Ajax API. Due to changes in the Google Translation API, the original module "googletrans 3.0.0" <ref type="bibr" coords="11,348.44,134.63,18.07,10.91" target="#b21">[22]</ref> may no longer work. For this reason the alternative "google-trans-new 1.1.9" <ref type="bibr" coords="11,309.79,148.18,13.00,10.91" target="#b2">[3]</ref> was implemented, which has fixed the problems of the previous module.</p><p>Since the documents in the corpus are available in different languages, different language queries were used. Due to the fact that most of the queries are formulated in German or English and the majority of the documents are available in German or English, the queries were translated into the languages mentioned above. However, some queries contain technical terms, which is why the translation generated rather less meaningful search terms in some places. Therefore, not only the English and German queries, but also the original queries were sent.</p><p>It is also important to note that Google has restrictions and usage limits <ref type="bibr" coords="11,427.27,256.58,16.77,10.91" target="#b22">[23]</ref>. The request limit for the Google Translate API is limited to 600 queries per minute, i.e. 300 queries with 2 translations each. Therefore, in the first round a sleeptimer which provided a pause of 1.5 seconds after each topic was installed. Since the second round worked without the candidate, the sleeptimer was replaced with a try/except statement.</p><p>The three different queries were processed with the same tokenizers, which was also used for the documents (see: Tokenization 4.2.1). In Elastic, the Boolean Query "should" was used to ensure that one of the three tokenized queries is used at a time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Elastic Settings after Preprocessing</head><p>A boost on the with SciSpacy processed MESH and CHEM tokens by the factor 1.7 and KEY-WORDS by the factor 0.75 was applied, because it was assumed to be logical that these keywords have a high significance in a scientific database. For the ranking method, we majorly used the DFR similarity model as explained in Section [4.1.6].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4.">Improving Performance with Parallelization</head><p>The performance of the preprocessing pipeline has been a great issue throughout the development. The first version used for the precomputed first run took 40 minutes to process the 100k documents. While this time was acceptable for the first run, it would have taken multiple weeks to run on the entire data set for the second run.</p><p>After trying different ways to address this problem, the pandarallel module emerged as the best approach. It is a helper module for pandas and is capable of spreading the normal .apply() method to multiple threads instead of just one. Therefore the processing time is roughly divided by the amount of available threads. A relatively powerful machine with a 16 Core 32-thread processor and 128 GB of RAM was available for testing purposes, of which 90 GB could be used freely. The LIVIVO Machine in comparison had 1 TB RAM and 72 Cores. Using pandarallel on this machine meant a reduction of computing time by a factor of 32: the time needed per 100k documents dropped to just 1 minute and 15 seconds and the performance went up from 0.217 GB/h to 7 GB/h. This enabled us to process the entire data set in a more reasonable time <ref type="bibr" coords="11,484.12,653.32,17.87,10.91" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments &amp; Evaluation</head><p>With the help of logged feedback data containing information about received queries, clicks and further details beyond the clicks, an analysis and comparison of the experimental systems and the on-production base system can be made. For the evaluation, an inspection of the received feedback from the first round and second round while inspecting general metrics that indicate the general performance of the experimental search systems LEO and LEPREP is performed, with a closer look on user behaviour after a click. Following that, the success rate of the experimental systems and the base system concerning the different categories of queries such as English queries, German queries or exact matches were compared in order to measure the success of the additional multi-language components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">First Round: save_fami &amp; lemuren_elk</head><p>Unfortunately, the approach to re-rank the head queries of LIVIVO by only providing a precomputed run did not work out as well as expected, as only a very small subset of users in the first round explicitly used the pre-defined headqueries and as such, only minimal click data was retrieved and made subsequent analysis negligible (see Table <ref type="table" coords="12,379.31,310.05,3.54,10.91" target="#tab_1">2</ref>). This, however, gave more urgency to implement the next systems in round two as fully functional dockerized systems in order to capture a bigger amount of feedback. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Second Round</head><p>Thanks the docker approach utilized in the second round, the submitted systems were integrated into the live LIVIVO system and thus were able to process any kind of query rather than just pre-defined headqueries. As such, the systems were able to gather substantially more click data in comparison to the first round, making further analysis possible.</p><p>In the framework, a search engine is considered improved, if it manages to achieve an overall higher win-ratio than the opposing base system. However, the different metrics are weighted differently, as orders and bookmarks are worth more than just clicks by themselves.</p><p>As such, in order to properly evaluate the search engines, we need to introduce the basic metrics that determine the performance of a search engine in the LIVIVO environment <ref type="bibr" coords="12,478.60,623.66,11.43,10.91" target="#b6">[7]</ref>.</p><p>At first glance, both submitted systems were outperformed by the base system in terms of wins and the win outcome percentage, with both systems having substantially less amount of clicks compared to the base system (see Table <ref type="table" coords="12,295.00,664.31,3.57,10.91" target="#tab_2">3</ref>). Upon further inspection of the user behaviour after a click has been received, users seemed more interested in the results retrieved by the submitted LEO system than the results from the base system. This can be seen when inspecting the segmentation of click behaviour.</p><p>Although the LEO system has received 288 less clicks than the base system, the ratio between the bookmark and order metrics in comparison to the raw number of clicks received is substantially higher for the LEO system than the base system. This is also reflected in the "Click Conversion Rate", where the ratio between clicks and subsequent clicks on the document's detail field was calculated.</p><p>Overall, this inspection could be interpreted as a strong positive sign that the user had a high interest in the shown result, rather than just clicking the document's title and leaving. For this aspect and metric alone, the LEO system was able to outperform the base system (see Table <ref type="table" coords="13,496.87,346.87,3.56,10.91" target="#tab_3">4</ref>). Additionally, some other metrics to determine the user experience of the submitted systems were calculated. The "Clicks per Query" metric signifies the amount of click per retrieved result list where at least on result was clicked. On one hand, a low "Clicks per Query" metric could be interpreted as a user already being satisfied with the first clicked result, having no further need to inspect the rest of the result list. Under this assumption, the LEPREP system was able to outperform the other system based on this metric, followed by the LEO system. On the other hand, the user ideally wants to investigate multiple relevant results rather than just one, under which the LEPREP system performed the worst and the base system the best. The metrics for the "Mean Reciprocal Rank" are very distorted and hard to interpret because of the high "Abandonment Rate" of all systems, meaning most of the sent result for both the submitted systems and the base system did not lead to any clicks, leaving room for a lot of potential future improvements (see Table <ref type="table" coords="14,275.18,100.52,3.57,10.91" target="#tab_4">5</ref>). At first, the long and the short German and English queries are compared between the submitted systems and the base system, filtered by length. The short queries are defined by a length shorter than 20 characters and the long queries are defined by the length between 20 to 50 characters, while exact match searches on the title of a document longer than 50 characters should not be a part of this analysis. For the LEO system, the document titles that were filtered for their usage for the "Keyword &amp; Query" (see: 5.2.2) analysis were also excluded from the long queries. Inspecting the metrics of both submitted systems shows, that they performed generally better for English queries in comparison to German queries(see Table <ref type="table" coords="14,379.12,669.58,3.65,10.91" target="#tab_5">6</ref>). This can be observed for both longer and shorter queries. Looking at the performance of the LEO system, it is notable that long queries for both German and English queries perform substantially better than for short queries, which is interesting when these results are compared with the following results from the section "Keyword &amp; Queries"(see: 5.2.2 ). For long queries ranging between 20 and 50 characters, the results are pretty lower than the base system's performance with a performance difference of 5.8 % for the English and 15.16 % for the German queries. For the LEPREP system however, the gap between the long and short queries is pretty small and varies for the language, so a general hypothesis cannot be made (see <ref type="bibr" coords="15,404.98,355.96,32.96,10.91">Table 7)</ref>. Unfortunately for both submitted systems, approximately 40 % of the received queries were in English, which leads to a poor overall performance if you do not differentiate in terms of the language. For a scientific database like LIVIVO, MeSH terms are important keywords to consider. MeSH (Medical Subject Headings) present a hierarchical thesaurus that is controlled and edited by the National Library of Medicine <ref type="bibr" coords="15,221.35,601.84,16.25,10.91" target="#b23">[24]</ref>. For the MeSH term analysis, if a query contains at least one known MeSH term, the queries were then filtered with the scispacy module and the "en_core_sci_lg", which contains 785.000 scientific words and 600.000 word vectors via the "EntityLinker" function <ref type="bibr" coords="15,408.92,642.48,17.62,10.91" target="#b18">[19]</ref>. For this analysis the queries were also separated between German and English queries. At first it is worth mentioning that almost every query contained at least one or more MeSH terms, so only a few queries ended up being excluded, but increasing the minimum required amount of MeSH terms would have led to the problem that terms like "clown therapy", which are not MeSH terms but might present a medical term, would have been filtered out. Since MeSH terms are always in English, both the German and the English queries are parsed by the biomedical pipeline model "en_core_sci_lg". Once again it can be observed that both submitted systems perform better with English Queries than with German Queries (see Table <ref type="table" coords="16,354.07,154.71,3.57,10.91" target="#tab_7">8</ref>). While the LEO system just used the title and abstract field for querying, the LEPREP system utilized scientific MeSH Term tokins in the "Mesh_tokens" field with an additional boosting of 1.7 for its query process. However the LEPREP system was outperformed by both the base system and the LEO system. It might be that similar to the "clown therapy" problem, very general MeSH terms were present in the MeSH term fields, leading to a high intersection of many documents from the corpus with a variety of actually semantically different "therapies". This is also a reason why the criteria of minimum MeSH terms for the tested queries were kept at one, because the phenomenon and source of this discrepancy in the two systems should be detected and avoided (see Table <ref type="table" coords="16,232.80,276.66,3.57,10.91" target="#tab_7">8</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 9</head><p>Queries with logical operators. The amount of won queries per system is shown: E.g. the base system during won 576 queries while the experimental LEO system won 376 queries. None applies when no system received clicks and tie applies when the base and experimental system received the same amount of clicks. Looking at the win/loss rate of the experimental systems compared to the base system, it is apparent that the LEO system performs almost 3 times better than the LEPREP system in terms of queries with boolean operators (see Table <ref type="table" coords="16,282.60,479.34,3.50,10.91">9</ref>). Seeing how the second system with an elaborate preprocessing and tokenizing process with an overall focus using on scientific language such as MeSH Terms and CHEM tokens to determine matches lead to worse performance an almost aspects in comparison to the first system and even more to the base system, it can be concluded that this custom implementation of this process does not work in terms of satisfying a user's information need in a scientific environment like LIVIVO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Keyword &amp; Queries</head><p>For the analysis of our approach in handling titles as keywords within queries, the possibly affected keywords needed to be extracted from the rest of the queries 4.1.1. Several patterns were used to filter "normal" queries from the keyword queries. The most effective one was to filter all queries by a certain string length, although this might exclude certain short titles. In the end the queries containing titles as keyword were identified and reduced to a total of 285 queries which were then again inspected manually. This reduced the amount of queries to a final product of 238 queries. For this particular set, the win and loss rate was calculated for the LEO system in comparison to the base system. It turned out that 126 of these queries got results from both systems. While the LEO system won 40 of these queries, the base system outperformed this system with 75 wins, so the win ratio of 34.78% turns out to be worse than the overall performance comparison of these two systems. Furthermore 11 ties were calculated for this set of keyword queries. As such, the approach with a "minimum_should_match" of 75% did not pay off as expected. Another issue might have been that the score was calculated in a should statement with four different queries, the scores were accumulated, making it trigger on a semantically false match. However, the significant boosting of ten for the LEO system should effect the score enough to fulfill the set goal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">Fuzzy Matches</head><p>As described earlier in the paper one approach for the submitted LEO system was to extend the queries with fuzzy matching in order to improve query results containing typos from the user input (See 4.1.5). During the evaluation period, it turned out that it was hard to evaluate the performance of this approach for the LEO system, since most names and medical terms were detected as typos from the used pyspellchecker. After some filtering, 454 terms in German and English were left, which had to be filtered manually, resulting in 50 terms with some kind of typos. With just a small set of 50 terms, it is not possible to form an adequate analysis about the performance of the fuzzy match. However when calculating the outcomes for wins and losses, it turns out that, because of the high abandonment rate of the two systems, just six of the 50 queries actually collected a click at all. Five of these queries were won by the base system, which means that the submitted LEO system had an win outcome of 16.6%. Although the amount of usable test data is very small, it raises the question whether the "fuzzy match" built-in function of Elasticsearch actually worsened the search performance for the correctly spelled queries as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion &amp; Future Research</head><p>Overall, the individual approaches towards improving query search for multi-language domains did not manage to win over the existing base system on LIVIVO when considering the base metrics such as wins and loses. The more sophisticated approach in handling the preprocessing and tokenization of the documents primarily externally outside of the in-built Elasticsearch functions with SpaCy and an elevated importance on controlled vocabulary such as MESH and CHEM tokens even worsened the performance of the search engine considerably.</p><p>Furthermore, it became very apparent in the experiments section that the more approaches were implemented on the index side of Elasticsearch and the more custom made the tokenization process became, the worse the overall resulting system became. This could be a case of oversaturation of tokens in the LEPREP system, as the inclusion of MeSH and CHEM tokens towards its index might have worsened the search results in terms of apparent relevancy, as a lot of documents contain many rather unspecific MeSH terms such as "disorder", thus polluting the search result list. Therefore, it might be beneficial to rely on in-built Elastic functions for the index side and lessen the associated boosting of MeSH Terms, while looking into implementing more elaborate ways to handle the multi-language problem more on the query side.</p><p>For additional further research, other approaches could be inspected closer such as: Implementing a German compound word list to improve the handling of compound words that can be commonly found in the German language, further expanding the existing ingestion pipeline with more conditional fields in order to finetune search depending on the query and its language received and expanding the query process of the experimental systems with a more comprehensive synonym word list adapted to the knowledge needs of the LIVIVO platform.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,89.29,570.20,416.69,8.93;6,89.29,582.21,25.97,8.87;6,89.29,373.41,416.70,178.25"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: LEO overview detailing the Pipeline from Document to Ingestion Pipeline to Index and the Query</figDesc><graphic coords="6,89.29,373.41,416.70,178.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="10,89.29,438.12,416.69,8.93;10,89.29,450.13,77.27,8.87;10,89.29,238.01,416.70,181.57"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: LEPREP Overview detailing the Pipeline between the raw Data Set, the Index Creation and the Query Process.</figDesc><graphic coords="10,89.29,238.01,416.70,181.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,88.99,90.49,331.73,222.73"><head>Table 1</head><label>1</label><figDesc>List of submitted Systems for each Round in CLEF2021</figDesc><table coords="2,186.51,122.10,234.22,38.26"><row><cell>Round 1</cell><cell>Round 2</cell></row><row><cell>lemuren_elk</cell><cell>lemuren_elastic_only (LEO)</cell></row><row><cell>save_fami</cell><cell>lemuren_elastic_preprocessing (LEPREP)</cell></row></table><note coords="2,89.29,304.29,245.23,8.93"><p>Figure 1: Overview of the provided Systems for each round</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="12,88.99,366.15,410.56,94.18"><head>Table 2</head><label>2</label><figDesc>Results Round 1 with pre-computed Runs # of Days active # of Queries Clicks Impressions Win Lose Outcome%</figDesc><table coords="12,95.73,415.20,379.84,45.13"><row><cell>Base</cell><cell>18</cell><cell>41</cell><cell>14</cell><cell>66</cell><cell>8</cell><cell>4</cell><cell>66.7</cell></row><row><cell>lemuren_elk</cell><cell>18</cell><cell>41</cell><cell>9</cell><cell>41</cell><cell>4</cell><cell>8</cell><cell>33.3</cell></row><row><cell>Base</cell><cell>23</cell><cell>52</cell><cell>14</cell><cell>54</cell><cell>12</cell><cell>10</cell><cell>54.55</cell></row><row><cell>save_fami</cell><cell>23</cell><cell>52</cell><cell>15</cell><cell>54</cell><cell>10</cell><cell>12</cell><cell>45.45</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="13,88.99,90.49,383.21,94.18"><head>Table 3</head><label>3</label><figDesc>General Metrics including Run Time, Number of Queries, Impressions and Win/Lose Ratio # of Days active # of Queries Impressions Win Lose Outcome%</figDesc><table coords="13,123.07,139.53,325.15,45.13"><row><cell>Base</cell><cell>24</cell><cell>4083</cell><cell>4083</cell><cell>669</cell><cell>444 60.01</cell></row><row><cell>LEO</cell><cell>24</cell><cell>4083</cell><cell>3985</cell><cell>444</cell><cell>669 39.9</cell></row><row><cell>Base</cell><cell>20</cell><cell>3763</cell><cell>3763</cell><cell>791</cell><cell>172 82.14</cell></row><row><cell>LEPREP</cell><cell>20</cell><cell>3763</cell><cell>3674</cell><cell>172</cell><cell>791 17.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="13,88.99,375.87,375.32,118.09"><head>Table 4</head><label>4</label><figDesc>Segmentation of Click Behaviour</figDesc><table coords="13,132.63,407.49,331.68,86.47"><row><cell></cell><cell cols="4">Clicks Details Fulltext Title Book-</cell><cell cols="2">Order Click Con-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>marks</cell><cell></cell><cell>version</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Rate</cell></row><row><cell>Base</cell><cell>1075</cell><cell>656</cell><cell>386</cell><cell>539 27</cell><cell>38</cell><cell>0.61</cell></row><row><cell>LEO</cell><cell>787</cell><cell>509</cell><cell>274</cell><cell>400 31</cell><cell>30</cell><cell>0.64</cell></row><row><cell>Base</cell><cell>1164</cell><cell>692</cell><cell>438</cell><cell>542 47</cell><cell>44</cell><cell>0.59</cell></row><row><cell>LEPREP</cell><cell>346</cell><cell>201</cell><cell>159</cell><cell>162 9</cell><cell>11</cell><cell>0.58</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="14,88.98,129.26,417.00,239.23"><head>Table 5</head><label>5</label><figDesc></figDesc><table coords="14,88.98,141.27,417.00,227.22"><row><cell>Additional Metrics</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Clicks per Query Mean Reciprocal Rank Abandonment Rate</cell></row><row><cell>Base</cell><cell>1.27</cell><cell>0.16</cell><cell>0.79</cell></row><row><cell>LEO</cell><cell>1.23</cell><cell>0.12</cell><cell>0.84</cell></row><row><cell>Base</cell><cell>1.3</cell><cell>0.18</cell><cell>0.76</cell></row><row><cell>LEPREP</cell><cell>1.17</cell><cell>0.055</cell><cell>0.92</cell></row><row><cell>5.2.1. Query Analysis</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">For further analysis, it is also important to inspect how the respective search systems performed</cell></row><row><cell cols="2">for different kind of search queries:</cell><cell></cell><cell></cell></row><row><cell cols="2">‚Ä¢ [Long and short Queries]</cell><cell></cell><cell></cell></row><row><cell cols="2">‚Ä¢ [German and English Queries]</cell><cell></cell><cell></cell></row><row><cell cols="2">‚Ä¢ [Queries with boolean Operators]</cell><cell></cell><cell></cell></row><row><cell cols="2">‚Ä¢ [Queries with MeSH terms]</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="14,88.99,492.35,339.73,145.19"><head>Table 6</head><label>6</label><figDesc>Metrics for long Queries</figDesc><table coords="14,166.55,523.97,262.18,113.57"><row><cell></cell><cell cols="5">Query Language Wins Lose Ties Outcome%</cell></row><row><cell>Base</cell><cell>English</cell><cell>100</cell><cell>89</cell><cell>35</cell><cell>52.9</cell></row><row><cell>LEO</cell><cell>English</cell><cell>89</cell><cell>100</cell><cell>35</cell><cell>47.1</cell></row><row><cell>Base</cell><cell>German</cell><cell>224</cell><cell>165</cell><cell>46</cell><cell>57.58</cell></row><row><cell>LEO</cell><cell>German</cell><cell>165</cell><cell>224</cell><cell>46</cell><cell>42.42</cell></row><row><cell>Base</cell><cell>English</cell><cell>127</cell><cell>37</cell><cell>14</cell><cell>78.44</cell></row><row><cell>LEPREP</cell><cell>English</cell><cell>37</cell><cell>127</cell><cell>14</cell><cell>22.56</cell></row><row><cell>Base</cell><cell>German</cell><cell>261</cell><cell>43</cell><cell>22</cell><cell>85.86</cell></row><row><cell>LEPREP</cell><cell>German</cell><cell>43</cell><cell>261</cell><cell>22</cell><cell>14.14</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="15,88.99,90.49,325.50,144.27"><head>Table 7</head><label>7</label><figDesc>Metrics for short Queries</figDesc><table coords="15,180.78,121.19,233.71,113.57"><row><cell></cell><cell cols="5">Language Wins Lose Ties Outcome%</cell></row><row><cell>Base</cell><cell>English</cell><cell>87</cell><cell>56</cell><cell>19</cell><cell>60.85</cell></row><row><cell>LEO</cell><cell>English</cell><cell>56</cell><cell>87</cell><cell>19</cell><cell>39.16</cell></row><row><cell>Base</cell><cell>German</cell><cell>120</cell><cell>60</cell><cell>22</cell><cell>66.67</cell></row><row><cell>LEO</cell><cell>German</cell><cell>60</cell><cell>120</cell><cell>22</cell><cell>33.33</cell></row><row><cell>Base</cell><cell>English</cell><cell>85</cell><cell>27</cell><cell>17</cell><cell>76.89</cell></row><row><cell>LEPREP</cell><cell>English</cell><cell>27</cell><cell>85</cell><cell>17</cell><cell>24.11</cell></row><row><cell>Base</cell><cell>German</cell><cell>151</cell><cell>28</cell><cell>21</cell><cell>84.36</cell></row><row><cell>LEPREP</cell><cell>German</cell><cell>28</cell><cell>151</cell><cell>21</cell><cell>15.64</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="15,88.99,411.89,339.73,144.27"><head>Table 8</head><label>8</label><figDesc>Metrics for Queries with MeSH Terms</figDesc><table coords="15,166.55,442.59,262.18,113.57"><row><cell></cell><cell cols="5">Query Language Wins Lose Ties Outcome%</cell></row><row><cell>Base</cell><cell>English</cell><cell>181</cell><cell>140</cell><cell>52</cell><cell>56.39</cell></row><row><cell>LEO</cell><cell>English</cell><cell>140</cell><cell>181</cell><cell>52</cell><cell>43.61</cell></row><row><cell>Base</cell><cell>German</cell><cell>328</cell><cell>213</cell><cell>66</cell><cell>60.63</cell></row><row><cell>LEO</cell><cell>German</cell><cell>213</cell><cell>328</cell><cell>66</cell><cell>39.37</cell></row><row><cell>Base</cell><cell>English</cell><cell>208</cell><cell>63</cell><cell>30</cell><cell>76.75</cell></row><row><cell>LEPREP</cell><cell>English</cell><cell>63</cell><cell>208</cell><cell>30</cell><cell>23.25</cell></row><row><cell>Base</cell><cell>German</cell><cell>398</cell><cell>71</cell><cell>42</cell><cell>84.76</cell></row><row><cell>LEPREP</cell><cell>German</cell><cell>71</cell><cell>396</cell><cell>42</cell><cell>15.24</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,108.93,671.02,238.43,8.97"><p>https://github.com/AH-Tran/DIS17.1-Suchmaschinentechnologie</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,108.93,671.02,147.16,8.97"><p>https://github.com/dis22-lilas/save_fami</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="6,108.93,670.93,188.92,8.97"><p>https://github.com/dis22-lilas/lemuren_elastic_only</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="10,108.93,671.04,223.60,8.97"><p>https://github.com/dis22-lilas/elastic_lemuren_preprocessing</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Thanks to the developers of STELLA [25] and to the developers of LIVIVO [8] for the cooperation and development with their respective platforms.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head><p>A.1. Multi-fields "TITLE": { "type": "text", "fields": { "analysis": { "type": "text", "analyzer": "general_analyzer", "similarity": "LMJelinekMercer_short" }, "en": { "type": "text", "analyzer": "english_analyzer", "similarity": "LMJelinekMercer_short" }, "ger": { "type": "text", "analyzer": "german_analyzer", "similarity": "LMJelinekMercer_short" }, "keyword": { "type": "keyword" } }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Ingestion Pipeline</head><p>"processors": [ { "set": { "if": "ctx.LANGUAGE == 'eng'", "field": "TITLE_EN", "value": "{{TITLE}}" } }, { "set": { "if": "ctx.LANGUAGE == 'ger'", "field": "TITLE_GER", "value": "{{TITLE}}" } } ]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Acronym handling</head><p>"filter": { "acronym": { "type": "word_delimiter", "catenate_all": true, "generate_word_parts": false, "generate_number_parts": false, "preserve_original": true, "split_on_case_change": true, "split_on_numerics": true, "stem_english_possessive": true }</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="18,112.66,312.61,393.33,10.91;18,112.66,326.16,394.53,10.91;18,112.66,339.71,393.33,10.91;18,112.66,353.26,394.53,10.91;18,112.39,366.81,394.89,10.91;18,112.31,380.36,374.62,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="18,269.60,312.61,236.39,10.91;18,112.66,326.16,126.10,10.91">Living lab evaluation for life and social sciences search platforms -lilas at CLEF 2021</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schaer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schaible</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J G</forename><surname>Castro</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-72240-1_77</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-72240-1_77.doi:10.1007/978-3-030-72240-1\_77" />
	</analytic>
	<monogr>
		<title level="m" coord="18,202.85,339.71,303.14,10.91;18,112.66,353.26,105.71,10.91;18,414.33,353.26,89.84,10.91">Advances in Information Retrieval -43rd European Conference on IR Research, ECIR 2021</title>
		<title level="s" coord="18,185.14,367.82,144.92,9.72">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Moens</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Mothe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Perego</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Potthast</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</editor>
		<meeting><address><addrLine>Virtual Event</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021-04-01">March 28 -April 1, 2021. 2021</date>
			<biblScope unit="volume">12657</biblScope>
			<biblScope unit="page" from="657" to="664" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II</note>
</biblStruct>

<biblStruct coords="18,112.66,393.91,394.61,10.91;18,112.41,407.46,51.67,10.91" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="18,144.69,393.91,165.51,10.91">German spacy models documentation</title>
		<author>
			<persName coords=""><surname>Spacy</surname></persName>
		</author>
		<ptr target="https://spacy.io/" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>19.05.2021</note>
</biblStruct>

<biblStruct coords="18,112.66,421.01,394.52,10.91;18,112.66,434.55,117.92,10.91" xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Lushan</surname></persName>
		</author>
		<idno>1.1.9</idno>
		<ptr target="https://pypi.org/project/google-trans-new/" />
		<imprint>
			<date type="published" when="2020-05">2020. 05.2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,448.10,393.73,10.91;18,112.66,461.65,394.53,10.91;18,112.66,475.20,393.33,10.91;18,112.66,488.75,393.33,10.91;18,112.66,502.30,393.32,10.91;18,112.66,515.85,142.00,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="18,461.99,448.10,44.40,10.91;18,112.66,461.65,200.94,10.91">Overview of lilas 2021 -living labs for academic search</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schaer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Breuer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wolff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schaible</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tavakolpoursaleh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,447.41,475.20,58.58,10.91;18,112.66,488.75,393.33,10.91;18,112.66,502.30,254.11,10.91">Proceedings of the Twelfth International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="18,474.41,503.31,31.57,9.72;18,112.66,516.86,112.43,9.72">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Candan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Larsen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting>the Twelfth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="volume">12880</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="18,112.66,529.40,394.61,10.91;18,112.66,542.95,51.67,10.91" xml:id="b4">
	<monogr>
		<ptr target="https://stella-project.org/" />
		<title level="m" coord="18,112.66,529.40,160.28,10.91">Stella -infrastructures for living labs</title>
		<imprint>
			<date type="published" when="2021-05-21">2021. 21.05.2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,556.50,394.62,10.91;18,112.66,570.05,300.17,10.91" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="18,147.93,556.50,303.63,10.91">Elasticsearch: Die offizielle engine f√ºr verteilte suche und analytics</title>
		<author>
			<persName coords=""><surname>Elastic</surname></persName>
		</author>
		<ptr target="https://www.elastic.co/de/elasticsearch/" />
		<imprint>
			<date type="published" when="2021-05-27">2021. 27.05.2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,583.60,395.17,10.91;18,112.66,597.15,394.53,10.91;18,112.66,610.69,395.17,10.91;18,112.66,624.24,395.17,10.91;18,112.66,637.79,395.01,10.91;18,112.66,651.34,276.09,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="18,280.21,583.60,227.62,10.91;18,112.66,597.15,15.26,10.91">How does clickthrough data reflect retrieval quality?</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kurup</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<idno type="DOI">10.1145/1458082.1458092</idno>
		<ptr target="https://doi.org/10.1145/1458082.1458092.doi:10.1145/1458082.1458092" />
	</analytic>
	<monogr>
		<title level="m" coord="18,256.84,610.69,251.00,10.91;18,112.66,624.24,216.41,10.91">Proceedings of the 17th ACM Conference on Information and Knowledge Management, CIKM 2008</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Shanahan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Amer-Yahia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Manolescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Evans</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Kolcz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Choi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Chowdhury</surname></persName>
		</editor>
		<meeting>the 17th ACM Conference on Information and Knowledge Management, CIKM 2008<address><addrLine>Napa Valley, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">October 26-30, 2008. 2008</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
	<note>16.06.2021</note>
</biblStruct>

<biblStruct coords="19,112.66,86.97,393.33,10.91;19,112.66,100.52,95.61,10.91" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="19,151.00,86.97,177.15,10.91">Livivo -the search portal for life sciences</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Med</surname></persName>
		</author>
		<ptr target="https://www.livivo.de/" />
		<imprint>
			<date type="published" when="2021-05-28">2021. 28.05.2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,114.06,394.62,10.91;19,112.41,127.61,51.67,10.91" xml:id="b8">
	<monogr>
		<ptr target="https://spacy.io/usage/facts-figures" />
		<title level="m" coord="19,112.66,114.06,102.20,10.91">spaCy, Facts &amp; figures</title>
		<imprint>
			<date type="published" when="2021-05">2021. 19.05.2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,141.16,394.62,10.91;19,112.41,154.71,51.67,10.91" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="19,160.60,141.16,77.32,10.91">Docker @ elastic</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Docker</surname></persName>
		</author>
		<ptr target="https://www.docker.elastic.co/" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>19.05.2021</note>
</biblStruct>

<biblStruct coords="19,112.66,168.26,394.04,10.91;19,112.66,181.81,229.79,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><surname>Kaggle</surname></persName>
		</author>
		<ptr target="https://ir.nist.gov/covidSubmit/index.html" />
		<title level="m" coord="19,147.80,168.26,223.22,10.91">Covid-19 open research dataset challenge (cord-19)</title>
		<imprint>
			<date type="published" when="2020-05-20">2020. 20.05.2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,195.36,393.33,10.91;19,112.66,208.91,395.01,10.91;19,112.66,222.46,340.22,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="19,259.31,195.36,246.68,10.91;19,112.66,208.91,198.68,10.91">Probabilistic models of information retrieval based on measuring the divergence from randomness</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
		<idno type="DOI">10.1145/582415.582416</idno>
		<ptr target="https://doi.org/10.1145/582415.582416.doi:10.1145/582415.582416" />
	</analytic>
	<monogr>
		<title level="j" coord="19,322.26,208.91,98.58,10.91">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="357" to="389" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,236.01,395.00,10.91;19,112.66,249.56,395.17,10.91;19,112.66,263.11,85.95,10.91" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="19,162.76,236.01,314.91,10.91">Designing an optimal multi-language search engine with elasticsearch</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bugara</surname></persName>
		</author>
		<ptr target="https://codarium.substack.com/p/designing-an-optimal-multi-language" />
		<imprint>
			<date type="published" when="2020-05-20">2020. 20.05.2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,276.66,394.04,10.91;19,112.66,290.20,325.57,10.91" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="19,154.41,276.66,170.90,10.91">Analysis -anatomy of an analyzer</title>
		<author>
			<persName coords=""><surname>Fossies</surname></persName>
		</author>
		<ptr target="https://fossies.org/linux/elasticsearch/docs/reference/analysis.asciidoc" />
		<imprint>
			<date type="published" when="2021-05-28">2021. 28.05.2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,303.75,394.04,10.91;19,112.66,317.30,315.61,10.91" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="19,148.92,303.75,86.36,10.91">Query string query</title>
		<author>
			<persName coords=""><surname>Elastic</surname></persName>
		</author>
		<ptr target="https://www.elastic.co/guide/en/elasticsearch/reference/current/release-notes-7.13.0.html" />
		<imprint>
			<date type="published" when="2021-05-28">2021. 28.05.2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,330.85,394.62,10.91;19,112.31,344.40,377.14,10.91;19,112.66,357.95,269.09,10.91" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="19,220.79,330.85,175.99,10.91">Class german normalization filter</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Foundation</surname></persName>
		</author>
		<ptr target="https://lucene.apache.org/core/8_8_0/analyzers-common/org/apache/lucene/analysis/de/GermanNormalizationFilter.html" />
		<imprint>
			<date type="published" when="2021-05-28">2021. 28.05.2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,371.50,394.61,10.91;19,112.41,385.05,51.67,10.91" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Nalepa</surname></persName>
		</author>
		<ptr target="https://github.com/nalepae/pandarallel" />
		<title level="m" coord="19,169.95,371.50,46.74,10.91">pandarallel</title>
		<imprint>
			<date type="published" when="2021-05">2021. 19.05.2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,398.60,394.04,10.91;19,112.66,412.15,393.76,10.91" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="19,160.01,398.60,247.69,10.91">Natural Language Processing With spaCy in Python</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Singh</surname></persName>
		</author>
		<ptr target="https://realpython.com/natural-language-processing-spacy-python/" />
		<imprint>
			<date type="published" when="2021-05">2021. 19.05.2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,425.70,394.93,10.91" xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Scispacy</forename></persName>
		</author>
		<ptr target="https://allenai.github.io/scispacy/" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>19.05.2021</note>
</biblStruct>

<biblStruct coords="19,112.66,439.25,393.32,10.91;19,112.66,452.79,95.61,10.91" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="19,144.13,439.25,162.64,10.91">German spacy models documentation</title>
		<author>
			<persName coords=""><surname>Spacy</surname></persName>
		</author>
		<ptr target="https://spacy.io/models/de/" />
		<imprint>
			<date type="published" when="2021-05">2021. 19.05.2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,466.34,394.04,10.91;19,112.66,479.89,362.60,10.91" xml:id="b20">
	<monogr>
		<author>
			<persName coords=""><surname>Elastic</surname></persName>
		</author>
		<ptr target="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-whitespace-tokenizer.html" />
		<title level="m" coord="19,147.02,466.34,96.65,10.91">Whitespace Tokenizer</title>
		<imprint>
			<date type="published" when="2021-05">2021. 19.05.2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,493.44,394.62,10.91;19,112.41,506.99,51.67,10.91" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="19,146.88,493.44,73.44,10.91">googletrans 3.0.0</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<ptr target="https://pypi.org/project/googletrans/" />
		<imprint>
			<date type="published" when="2020">2020. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,520.54,393.33,10.91;19,112.66,534.09,95.61,10.91" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">Llc</forename></persName>
		</author>
		<ptr target="https://cloud.google.com/translate/quotas" />
		<title level="m" coord="19,152.95,520.54,79.36,10.91">Quotas and limits</title>
		<imprint>
			<date type="published" when="2021-05">2021. 19.05.2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,547.64,395.01,10.91;19,112.66,561.19,241.59,10.91" xml:id="b23">
	<monogr>
		<title level="m" type="main" coord="19,193.21,547.64,162.71,10.91">Welcome to medical subject headings</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">L</forename><surname>Medicine</surname></persName>
		</author>
		<ptr target="https://www.nlm.nih.gov/mesh/meshhome.html" />
		<imprint>
			<date type="published" when="2021-05-28">2021. 28.05.2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,574.74,394.53,10.91;19,112.66,588.29,394.41,10.91;19,112.66,601.84,395.00,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="19,201.17,574.74,301.37,10.91">A living lab architecture for reproducible shared task experimentation</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Breuer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schaer</surname></persName>
		</author>
		<idno type="DOI">10.5283/epub.44953</idno>
	</analytic>
	<monogr>
		<title level="m" coord="19,126.95,588.29,380.12,10.91;19,112.66,602.85,37.90,9.72">Information between Data and Knowledge, volume 74 of Schriften zur Informationswissenschaft</title>
		<meeting><address><addrLine>Gl√ºckstadt</addrLine></address></meeting>
		<imprint>
			<publisher>Werner H√ºlsbusch</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="348" to="362" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
