<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,118.83,84.74,357.61,15.42;1,244.71,107.08,90.08,11.96">Overview of Touch√© 2021: Argument Retrieval Extended Version*</title>
				<funder ref="#_TNR9g47 #_AB8tuWF">
					<orgName type="full">DFG</orgName>
				</funder>
				<funder ref="#_bxXmgRH">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.87,132.98,112.73,11.96"><forename type="first">Alexander</forename><surname>Bondarenko</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Martin-Luther-Universit√§t Halle-Wittenberg</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,214.24,132.98,73.02,11.96"><forename type="first">Lukas</forename><surname>Gienapp</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Leipzig University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,299.91,132.98,55.47,11.96"><forename type="first">Maik</forename><surname>Fr√∂be</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Martin-Luther-Universit√§t Halle-Wittenberg</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,368.03,132.98,80.81,11.96"><forename type="first">Meriem</forename><surname>Beloucif</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Universit√§t Hamburg</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,88.95,146.93,67.51,11.96"><forename type="first">Yamen</forename><surname>Ajjour</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Martin-Luther-Universit√§t Halle-Wittenberg</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,169.10,146.93,107.29,11.96"><forename type="first">Alexander</forename><surname>Panchenko</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Skolkovo Institute of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,289.04,146.93,72.98,11.96"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Universit√§t Hamburg</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,374.66,146.93,58.99,11.96"><forename type="first">Benno</forename><surname>Stein</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Bauhaus-Universit√§t Weimar</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,160.88,104.05,11.96"><forename type="first">Henning</forename><surname>Wachsmuth</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">University</orgName>
								<address>
									<settlement>Paderborn</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,205.98,160.88,76.81,11.96"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Leipzig University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,313.79,160.88,77.83,11.96"><forename type="first">Matthias</forename><surname>Hagen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Martin-Luther-Universit√§t Halle-Wittenberg</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,118.83,84.74,357.61,15.42;1,244.71,107.08,90.08,11.96">Overview of Touch√© 2021: Argument Retrieval Extended Version*</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">13C9B16240BA882C243AB84E96B0D16F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Argument retrieval</term>
					<term>Controversial questions</term>
					<term>Comparative questions</term>
					<term>Shared task</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper is a report on the second year of the Touch√© shared task on argument retrieval held at CLEF 2021. With the goal to provide a collaborative platform for researchers, we organized two tasks:</p><p>(1) supporting individuals in finding arguments on controversial topics of social importance and (2) supporting individuals with arguments in personal everyday comparison situations.</p><p>Unlike in the first year, several of the 27 teams participating in the second year managed to submit approaches that improved upon argumentation-agnostic baselines for the two tasks. Most of the teams made use of last year's Touch√© data for parameter optimization and fine-tuning their best configurations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Informed decision making and opinion formation are natural routine tasks. Generally, both of these tasks often involve weighing two or more options. Any choice to be made may be based on personal prior knowledge and experience, but they may also often require searching and processing new knowledge. With the ubiquitous access to various kinds of information on the web-from facts over opinions and anecdotes to arguments-everybody has the chance to acquire knowledge for decision making or opinion formation on almost any topic. However, large amounts of easily accessible information imply challenges such as the need to assess their relevance to the specific topic of interest and to estimate how well an implied stance is justified; no matter whether it is about topics of social importance or "just" about personal decisions. In the simplest form, such a justification might be a collection of basic facts and opinions.</p><p>1. Argument retrieval for controversial questions from a focused collection of debates to support opinion formation on topics of social importance.</p><p>2. Argument retrieval for comparative questions from a generic web crawl to support informed decision making.</p><p>Approaches to these two tasks that take argumentative quality into account besides topical relevance will help search engines to deliver more accurate argumentative results. Additionally, they will also be an important part of open-domain conversational agents that "discuss" controversial societal topics with humans-as showcased by IBM's Project Debater <ref type="bibr" coords="2,459.80,551.59,11.36,10.91" target="#b5">[6,</ref><ref type="bibr" coords="2,473.88,551.59,7.47,10.91" target="#b6">7,</ref><ref type="bibr" coords="2,484.08,551.59,3.79,10.91" target="#b7">8</ref>]. <ref type="foot" coords="2,495.44,549.83,3.71,7.97" target="#foot_2">3</ref>The teams that participated in the second year of Touch√© were able to use the topics and relevance judgments from the first year to develop their approaches. Many trained and optimized learning-based rankers as part of their retrieval pipelines and employed a large variety of preprocessing methods (e.g., stemming, duplicate removal, query expansion), argument quality features, or comparative features (e.g., credibility, part-of-speech tags). Overall, different to the first Touch√© lab, the majority of the submitted approaches improved over the argumentationagnostic DirichletLM and BM25F-based baselines. In this paper, we review the participants' approaches in depth and cover all runs in the evaluation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Previous Work</head><p>Queries in argument retrieval often are phrases that describe a controversial topic, questions that ask to compare two options, or even complete arguments themselves <ref type="bibr" coords="3,424.99,199.79,11.57,10.91" target="#b8">[9]</ref>. In the Touch√© lab, we address the first two types in two different shared tasks. Here, we briefly summarize the related work on argument retrieval and on retrieval in comparative scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Argument Retrieval</head><p>Argument retrieval aims for delivering arguments to support users in making a decision or to help persuading an audience of a specific point of view. An argument is usually modeled as a conclusion with supporting or attacking premises <ref type="bibr" coords="3,315.62,303.71,11.52,10.91" target="#b1">[2]</ref>. While a conclusion is a statement that can be accepted or rejected, a premise is a more grounded statement (e.g., a statistical evidence).</p><p>The development of an argument search engine is faced with challenges that range from mining arguments from unstructured text to assessing their relevance and quality <ref type="bibr" coords="3,443.13,344.36,11.27,10.91" target="#b1">[2]</ref>. Argument retrieval follows several paradigms that start from different sources and perform argument mining and retrieval tasks in different orders <ref type="bibr" coords="3,293.88,371.46,16.36,10.91" target="#b9">[10]</ref>. Wachsmuth et al. <ref type="bibr" coords="3,398.45,371.46,11.45,10.91" target="#b1">[2]</ref>, for instance, extract arguments offline using heuristics that are tailored to online debate portals. Their argument search engine args.me uses BM25F to rank the indexed arguments while giving conclusions more weight than premises. Also Levy et al. <ref type="bibr" coords="3,291.86,412.11,17.91,10.91" target="#b10">[11]</ref> use distant supervision to mine arguments offline for a set of topics from Wikipedia before ranking them. Following a different paradigm, Stab et al. <ref type="bibr" coords="3,133.11,439.20,12.84,10.91" target="#b2">[3]</ref> retrieve documents from the Common Crawl<ref type="foot" coords="3,342.94,437.45,3.71,7.97" target="#foot_3">4</ref> in an online fashion (no prior offline argument mining) and use a topic-dependent neural network to extract arguments from the retrieved documents at query time. With the two Touch√© tasks, we address the paradigms of Wachsmuth et al. <ref type="bibr" coords="3,168.76,479.85,12.84,10.91" target="#b1">[2]</ref> (Task 1) and Stab et al. <ref type="bibr" coords="3,287.18,479.85,12.84,10.91" target="#b2">[3]</ref> (Task 2), respectively.</p><p>Argument retrieval should rank arguments according to their topical relevance but also to their quality. What makes a good argument has been studied since the time of Aristotle <ref type="bibr" coords="3,444.36,506.95,16.09,10.91" target="#b11">[12]</ref>. Recently, Wachsmuth et al. <ref type="bibr" coords="3,169.96,520.50,17.91,10.91" target="#b12">[13]</ref> categorized the different aspects of argument quality into a taxonomy that covers three dimensions: logic, rhetoric, and dialectic. Logic concerns the local structure of an argument, i.e, the conclusion and the premises and their relations. Rhetoric covers the effectiveness of the argument in persuading an audience with its conclusion. Dialectic addresses the relations of an argument to other arguments on the topic. For example, an argument that has many attacking premises might be rather vulnerable in a debate. The relevance of an argument to a query's topic is categorized by Wachsmuth et al. <ref type="bibr" coords="3,326.97,601.80,17.91,10.91" target="#b12">[13]</ref> under dialectic quality.</p><p>Researchers assess argument relevance by measuring an argument's similarity to a query's topic or incorporating its support/attack relations to other arguments. Potthast et al. <ref type="bibr" coords="3,460.24,628.89,17.91,10.91" target="#b13">[14]</ref> evaluate four standard retrieval models at ranking arguments with regard to the quality dimensions of relevance, logic, rhetoric, and dialectic. One of the main findings is that DirichletLM is better at ranking arguments than BM25, DPH, and TF-IDF. Gienapp et al. <ref type="bibr" coords="4,411.87,100.52,17.91,10.91" target="#b14">[15]</ref> extend this work by proposing a pairwise strategy that reduces the costs of crowdsourcing argument retrieval annotations in a pairwise fashion by 93% (i.e., annotating only a small subset of argument pairs).</p><p>Wachsmuth et al. <ref type="bibr" coords="4,183.99,141.16,17.91,10.91" target="#b15">[16]</ref> create a graph of arguments by connecting two arguments when one uses the other's conclusion as a premise. Later on, they exploit this structure to rank the arguments in the graph using PageRank scores <ref type="bibr" coords="4,292.57,168.26,16.09,10.91" target="#b16">[17]</ref>. This method is shown to outperform several baselines that only consider the content of the argument and its local structure (conclusion and premises). Dumani et al. <ref type="bibr" coords="4,198.00,195.36,17.91,10.91" target="#b17">[18]</ref> introduce a probabilistic framework that operates on semantically similar claims and premises. The framework utilizes support/attack relations between clusters of premises and claims and between clusters of claims and a query. It is found to outperform BM25 in ranking arguments. Later, Dumani and Schenkel <ref type="bibr" coords="4,340.80,236.01,17.91,10.91" target="#b18">[19]</ref> also proposed an extension of the framework to include the quality of a premise as a probability by using the fraction of premises that are worse with regard to the three quality dimensions of cogency, reasonableness, and effectiveness. Using a pairwise quality estimator trained on the Dagstuhl-15512 ArgQuality Corpus <ref type="bibr" coords="4,123.56,290.20,16.09,10.91" target="#b19">[20]</ref>, their probabilistic framework with the argument quality component outperformed the one without it on the 50 Task 1 topics of Touch√© 2020.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Retrieval for Comparisons</head><p>Comparative information needs in web search have first been addressed by basic interfaces where two to-be-compared products are entered separately in a left and a right search box <ref type="bibr" coords="4,471.72,367.03,16.55,10.91" target="#b20">[21,</ref><ref type="bibr" coords="4,491.11,367.03,12.42,10.91" target="#b21">22]</ref>. Comparative sentences are then identified and mined from product reviews in favor or against one or the other to-be-compared option using opinion mining approaches <ref type="bibr" coords="4,413.95,394.13,16.30,10.91" target="#b22">[23,</ref><ref type="bibr" coords="4,432.89,394.13,12.50,10.91" target="#b23">24,</ref><ref type="bibr" coords="4,448.03,394.13,12.23,10.91" target="#b24">25]</ref>. Recently, the identification of the comparison preference (the "winning" option) in comparative sentences has been tackled in a more open domain (not just product reviews) by applying feature-based and neural classifiers <ref type="bibr" coords="4,184.12,434.78,16.31,10.91" target="#b25">[26,</ref><ref type="bibr" coords="4,203.16,434.78,12.24,10.91" target="#b26">27]</ref>. Such preference classification forms the basis of the comparative argumentation machine CAM <ref type="bibr" coords="4,229.25,448.32,13.00,10.91" target="#b4">[5]</ref> that takes two comparison objects and some comparison aspect(s) as input, retrieves comparative sentences in favor of one or the other object using BM25, and then classifies the sentences' preferences for a final merged result table presentation. A proper argument ranking, however, is still missing in CAM. Chekalina et al. <ref type="bibr" coords="4,423.18,488.97,17.91,10.91" target="#b27">[28]</ref> later extended the system to accept comparative questions as input and to return a natural language answer to the user. A comparative question is parsed by identifying the comparison objects, aspect(s), and predicate. The system's answer is either generated directly based on Transformers <ref type="bibr" coords="4,462.07,529.62,17.98,10.91" target="#b28">[29]</ref> or by retrieval from an index of comparative sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Lab Overview and Statistics</head><p>The second edition of the Touch√© lab received 36 registrations (compared to 28 registrations in the first year), with a majority coming from Germany and Italy, but also from the Americas, Europe, Africa, and Asia (16 from Germany, 10 from Italy, 2 from the United States and Mexico, and 1 each from Canada, India, the Netherlands, Nigeria, the Russian Federation, and Tunisia). Aligned with the lab's fencing-related title, the participants were asked to select a real or fictional swordsman character (e.g., Zorro) as their team name upon registration.</p><p>We received result submissions from 27 of the 36 registered teams (up from 17 active teams in the first year). As in the previous edition of Touch√©, we paid attention to foster the reproducibility of the developed approaches by using the TIRA platform <ref type="bibr" coords="5,368.46,114.06,18.07,10.91" target="#b29">[30]</ref> that allows easy software submission and automatic evaluation. Upon registration, each team received an invitation to TIRA to deploy actual software implementations of their approaches. TIRA is an integrated cloud-based evaluation-as-a-service research architecture on which participants can install their software within a dedicated virtual machine. By default, the virtual machines operate the server version of Ubuntu 20.04 with one CPU (Intel Xeon E5-2620), 4 GB of RAM, and 16 GB HDD, but we adjusted the resources to the participants' requirements when needed (e.g., one team asked for 30 GB of RAM, 3 CPUs, and 30 GB of HDD). The participants had full administrative access to their virtual machines. Still, we pre-installed the latest versions of reasonable standard software (e.g., Docker and Python) to simplify the deployment of the approaches.</p><p>Using TIRA, the teams could create result submissions via a click in the web UI that then initiated the following pipeline: the respective virtual machine is shut down, disconnected from the internet, and powered on again in a sandbox mode, mounting the test datasets for the respective Touch√© tasks, and running a team's deployed approach. The interruption of the internet connection ensures that the participants' software works without external web services that may disappear or become incompatible-possible causes of reproducibility issuesbut it also means that downloading additional external code or models during the execution was not possible. We offered our support when this connection interruption caused problems during the deployment, for instance, with spaCy that tries to download models if they are not already available on the machine, or with PyTerrier that, in its default configuration, checks for online updates. To simplify participation of teams that do not want to develop a fully-fledged retrieval pipeline on their end, we enabled two exceptions from the interruption of the internet connection for all participants: the APIs of args.me and ChatNoir were available even in the sandbox mode to allow accessing a baseline system for each of the tasks. The virtual machines that the participants used for their submissions will be archived such that the respective systems can be re-evaluated or applied to new datasets as long as the APIs of ChatNoir and args.me remain available-which are both maintained by us.</p><p>When a software submission in TIRA really was not possible for some reason, the participants could also simply submit plain run files with their result rankings-an option chosen by 5 of the 27 participating teams. Per task, we allowed each team to submit up to 5 runs whose output must follow the standard TREC-style format. <ref type="foot" coords="5,285.04,518.79,3.71,7.97" target="#foot_4">5</ref> We checked the validity of all submitted run files and of the run files produced via TIRA, asking participants to resubmit their files or to rerun their software in case of validity issues-again, also offering our support in case of problems. All 27 active teams managed to submit at least one valid run. The total of 88 valid runs more than doubles the 41 valid runs from the first year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Task 1: Argument Retrieval for Controversial Questions</head><p>The goal of the Touch√© 2021 lab's first task was to advance technologies that support individuals in forming opinions on socially important controversial topics such as: "Should hate speech </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Description</head><p>Given the increasing amount of online hate speech, a user questions the necessity and legitimacy of taking legislative action to punish or inhibit hate speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Narrative</head><p>Highly relevant arguments include those that take a stance in favor of or opposed to stronger legislation and penalization of hate speech and that offer valid reasons for either stance. Relevant arguments talk about the prevalence and impact of hate speech, but may not mention legal aspects. Irrelevant arguments are the ones that are concerned with offensive language that is not directed towards a group or individuals on the basis of their membership in the group.</p><p>be penalized more?". For such topics, the task was to retrieve relevant and high-quality argumentative texts from the args.me corpus <ref type="bibr" coords="6,271.74,297.29,16.26,10.91" target="#b9">[10]</ref>, a focused crawl of online debate portals. In this scenario, relevant arguments should help users to form an opinion on the topic and to find arguments that are potentially useful in debates or discussions. The results of last year's Task 1 participants indicated that improving upon the "classic" argument-agnostic DirichletLM retrieval model is challenging, but, at the same time, the results of this baseline still left some room for potential improvements. Also, the detection of the degree of argumentativeness and the assessment of the quality of an argument were not "solved" in the first year, but identified as potentially interesting contributions of submissions to the task's second edition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Task Definition</head><p>Given a controversial topic formulated as a question, approaches to Task 1 needed to retrieve relevant and high-quality arguments from the args.me corpus, which covers a wide range of timely controversial topics. To enable approaches that leverage training and fine-tuning, the topics and relevance judgments from the 2020 edition of Task 1 were provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Data Description</head><p>Topics. We formulated 50 new search questions on controversial topics. Table <ref type="table" coords="6,468.40,545.79,5.17,10.91" target="#tab_0">1</ref> shows an example consisting of a title (i.e., a question on a controversial topic), a description that summarizes the particular information need and search scenario, and a narrative describing what makes a retrieved result relevant (meant as a guideline for human assessors). We carefully selected the topics by clustering the debate titles in the args.me corpus, formulating questions for a balanced mix of frequent and niche topics-manually ensuring that at least some relevant arguments are contained in the args.me corpus for each topic. Document Collection. The document collection for Task 1 was the args.me corpus <ref type="bibr" coords="7,486.06,86.97,16.56,10.91" target="#b9">[10]</ref>; freely available for download <ref type="foot" coords="7,221.65,98.76,3.71,7.97" target="#foot_5">6</ref> and also accessible via the args.me API. <ref type="foot" coords="7,407.15,98.76,3.71,7.97" target="#foot_6">7</ref> The corpus contains about 400,000 structured arguments crawled from several debate portals (debatewise.org, idebate.org, debatepedia.org, and debate.org), each with a conclusion (claim) and one or more supporting or attacking premises (reasons).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Judgment Process</head><p>The teams' result rankings should be formatted in the "standard" TREC format where document IDs are sorted by descending relevance score for each search topic. Prior to creating the assessment pools, we ran a near-duplicate detection for all submitted runs using the CopyCat framework <ref type="bibr" coords="7,140.24,231.54,16.11,10.91" target="#b30">[31]</ref>, since near-duplicates might impact evaluation results <ref type="bibr" coords="7,399.78,231.54,16.32,10.91" target="#b31">[32,</ref><ref type="bibr" coords="7,418.83,231.54,12.24,10.91" target="#b32">33]</ref>. The framework found only 1.1% of the arguments in the top-5 results to be near-duplicates (mostly due to debate portal users reusing their arguments in multiple debate threads). We created duplicate-free versions of each result list by removing the documents for which a higher-ranked document is a near-duplicate; in such cases, the next ranked non-near-duplicate then just moved up the ranked list. The top-5 results of the original and the deduplicated runs then formed the judgment pool-created with TrecTools <ref type="bibr" coords="7,224.50,312.83,19.04,10.91" target="#b33">[34]</ref>-resulting in 3,711 unique documents that were manually assessed with respect to their relevance and their argumentative quality.</p><p>For the assessment, we used the Doccano tool <ref type="bibr" coords="7,305.79,339.93,17.86,10.91" target="#b34">[35]</ref> and followed previously suggested annotation guidelines <ref type="bibr" coords="7,167.17,353.48,16.49,10.91" target="#b14">[15,</ref><ref type="bibr" coords="7,186.37,353.48,12.37,10.91" target="#b13">14]</ref>. Our eight graduate and undergraduate student volunteers (all with a computer science background) assessed each argument's relevance to the given topic with four labels (0: not relevant, 1: relevant, 2: highly relevant, or -2: spam) and the argument's rhetorical quality <ref type="bibr" coords="7,170.48,394.13,18.04,10.91" target="#b19">[20]</ref> with three labels (0: low quality, 1: sufficient quality, and 2: high quality). To calibrate the annotators' interpretations of the guidelines (i.e., the topics including the narratives and instructions on argument quality), we conducted an initial kappa test in which each annotator had to label the same 15 arguments from 3 topics (5 arguments from each topic). The observed Fleiss' ùúÖ values of 0.50 for argument relevance (moderate agreement) and of 0.39 for argument quality (fair agreement) are similar to previous studies <ref type="bibr" coords="7,394.36,461.87,16.39,10.91" target="#b14">[15,</ref><ref type="bibr" coords="7,413.48,461.87,12.53,10.91" target="#b35">36,</ref><ref type="bibr" coords="7,428.74,461.87,12.30,10.91" target="#b19">20]</ref>. However, we still had a follow-up discussion with all the annotators to clarify potential misinterpretations. Afterwards, each annotator independently judged the results for disjoint subsets of the topics (i.e., each topic was judged by one annotator only).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Submitted Approaches and Results</head><p>Twenty-one participating teams submitted at least one valid run to Task 1. The submissions partly continued the trend of Touch√© 2020 <ref type="bibr" coords="7,279.23,565.80,17.91,10.91" target="#b36">[37]</ref> by deploying "classical" retrieval models, however, with an increased focus on machine learning models (especially for query expansion and for assessing argument quality). Overall, we observed two kinds of contributions: (1) Reproducing and fine-tuning approaches from the previous year by increasing their robustness, and (2) developing new, mostly neural approaches for argument retrieval by fine-tuning pre-trained models for the domain-specific search task at hand. Like in the first year, combining "classical" retrieval models with various query expansion methods and domain-specific re-ranking features remained a frequent choice of approaches to Task 1. Not really surprising-given last year's baseline results-DirichletLM was employed most often as the initial retrieval model, followed by BM25. For query expansion, most participating teams continued to leverage WordNet <ref type="bibr" coords="8,261.27,141.16,16.29,10.91" target="#b37">[38]</ref>. However, transformer-based approaches received increased attention, such as query hallucination, which was successfully used by Akiki and Potthast <ref type="bibr" coords="8,128.51,168.26,17.91,10.91" target="#b38">[39]</ref> in the previous Touch√© lab. Similarly, utilizing deep semantic phrase embeddings to calculate the semantic similarity between a query and possible result documents gained widespread adoption. Moreover, many approaches tried to use some form of argument quality estimation as one of their features for ranking or re-ranking. This year's approaches benefited from the judgments released for Touch√© in 2020. Many teams used them for general parameter optimization but also to evaluate intermediate results of their approaches and to fine-tune or select the best configurations. For instance, comparing different kinds of pre-processing methods based on the available judgments from last year received much attention (e.g., stopword lists, stemming algorithms, or duplicate removal).</p><p>The results of the runs with the best nDCG@5 scores per participating team are reported in Table <ref type="table" coords="8,128.11,303.75,5.13,10.91">2</ref> (cf. Appendix A for evaluation results of all submitted runs). Below, we review the participants' approaches submitted to Task 1, ordered alphabetically by team name <ref type="foot" coords="8,458.31,315.55,3.71,7.97" target="#foot_7">8</ref>Asterix <ref type="bibr" coords="8,134.39,330.85,17.81,10.91" target="#b39">[40]</ref> preprocesses the args.me corpus by removing duplicate documents and filtering out documents that are too short. The resulting dataset is indexed using BM25. Then a linear regression model on the Webis-ArgQuality-20 argument quality dataset <ref type="bibr" coords="8,441.92,357.95,17.99,10.91" target="#b14">[15]</ref> is trained, predicting a given argument's overall quality. At retrieval time, the topic query is expanded using WordNet-based query expansion, 1,000 documents are retrieved using the BM25 index, and then re-ranked using a weighted combination of the normalized predicted quality score and the normalized BM25 score. They optimize the weighting against nDCG@5 using the relevance judgments from Touch√© 2020. A total of five runs were submitted.</p><p>Athos uses a DirichletLM retrieval model with a ùúá value of 2,000 and indexes the fields of an argument (conclusion and premise) separately. Both fields get preprocessed by lower-casing and removing stop words, urls, and emails. The ranking scores for both fields are then weighted as follows: 0.1 for conclusion and 0.9 for premise. A single run was submitted.</p><p>Blade uses a DirichletLM retrieval model in one run, and two variations of a BM25-based retrieval in two further runs. Unfortunately, no further details have been provided.</p><p>Batman <ref type="bibr" coords="8,136.45,520.54,17.76,10.91" target="#b40">[41]</ref> sets out to quantify the contributions of various steps of a retrieval pipeline, using argument retrieval as their proving ground. A finite search space is defined and effectiveness is systematically measured as more modules are added to the retrieval pipeline. Using relevance judgments from Touch√© 2020, the best combination of similarity function and tokenizer is determined, and then, gradually, different modules are added, valuate, and frozen, such as different stop word lists, different stemmers, and different filtering approaches. This amounts to a comprehensive grid search in hyperparameter space that allowed for choosing better-working components over worse ones for the retrieval pipeline, and provided for a good comparative overview of them. A total of three runs were submitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Results for Task 1: Argument Retrieval for Controversial Questions. The left part (a) shows the evaluation results of a team's best run according to the results' relevance, while the right part (b) shows the best runs according to the results' quality. An asterisk ( ‚ãÜ ) indicates that the runs with the best relevance and the best quality differ for a team. The baseline DirichletLM ranking is shown in bold.</p><p>(a) Best relevance score per team Deadpool applies a query expansion technique with a DirichletLM model (ùúá=4000). Both the conclusion and the premise of an argument are indexed, with 0.1 and 0.9 weights, respectively. The query expansion technique relies on the top-5 arguments to derive terms that associated with the query term. To quantify the co-occurrence of a term in an argument with the query terms, its conditional probability to that of the query terms are calculated and smoothed by the term's inverse document frequency. The conditional probability of a term given a query term is calculated using the count of arguments that contain both terms, divided by the count of arguments that contains the query term. A single run was submitted.</p><p>Dread Pirate Roberts <ref type="bibr" coords="9,194.85,599.60,18.07,10.91" target="#b41">[42]</ref> uses four classes of approaches to retrieve relevant arguments from the args.me corpus for a query on a controversial topic. Therefore, Roberts contrasts two "traditional" approaches with two novel approaches. The traditional approaches involve one run that uses a Dirichlet-smoothed language-model with low-quality arguments removed by argument clustering with the Universal Sentence Encoder model <ref type="bibr" coords="9,406.57,653.79,16.41,10.91" target="#b42">[43]</ref>, and two featurebased learning to rank approaches with LambdaMART <ref type="bibr" coords="9,338.13,667.34,16.33,10.91" target="#b43">[44]</ref>. The learning to rank models are trained on the relevance labels of Task 1 of Touch√© 2020 and differ in the used features. With 31 features belonging to 5 different feature classes as starting point, Roberts runs a greedy feature-selection identifying a subset of 4 and 9 features with best nDCG scores in a five-fold cross-validation setup. Afterwards, both feature sets are used on all relevance labels of Task 1 of Touch√© 2020 to train dedicated LambdaMART models that re-rank the top-100 results of the DirichletLM retrieval, producing 2 LambdaMART runs. Roberts further submits one run that re-ranks the top-100 results of the DirichletLM retrieval with a question-answering model. The idea behind this run is to phrase the task to retrieve relevant arguments for a controversial query as deciding whether an argument "answers" the controversial query. Therefore, the question-answering retrieval model coming with the Universal Sentence Encoder scores the top-100 argument for a query whether the argument "answers" the query or not, sorting the arguments by descending question-answering score. The fifth run submitted by Dread Pirate Roberts uses transformer-based query expansion where the query is expanded with keywords generated with RoBERTa <ref type="bibr" coords="10,206.78,263.11,16.42,10.91" target="#b44">[45]</ref>. Therefore, Dread Pirate Roberts embedded the controversial query into a pattern letting RoBERTa predict tokens, expanding the query with the top-10 tokens and their RoBERTa score as a weighted query submitted to the DirichletLM retrieval model. A total of five runs were submitted.</p><p>Elrond focuses on implementing a document analyzing pipeline to be used together with a DirichletLM-based retrieval. They rely on the Krovetz stemming algorithm and remove stop words using a custom stop list. They also compute part-of-speech tags and remove tokens from documents by filtering out certain tags. Documents are further enriched using WordNet-based synonyms. A total of four runs were submitted.</p><p>Gandalf indexes for each argument only the conclusion and uses BM25 as a retrieval model in a single-run submission.</p><p>Goemon Ishikawa <ref type="bibr" coords="10,180.37,412.15,17.80,10.91" target="#b45">[46]</ref> explores different configurations of a standard Lucene-based retrieval pipeline, varying the similarity function (BM25, DirichletLM), tokenizers (Lucene, OpenNLP), stop word lists (Lucene, Atire, Terrier, Smart), and lemmatizers (OpenNLP). Additionally, they test query expansion with synonyms from WordNet. Thirteen such configurations were evaluated on topics from Touch√© 2020 with respect to average precision, precision@10, nDCG, and nDCG@5. In an analysis of variance, they observe overall high variances for all evaluation measures and configurations, and that DirichletLM-based configurations perform significantly better, however, the effect of different tokenizers, stop word lists, or lemmatizers could not be assessed conclusively. A manual analysis by the authors on two topics suggests that expanding the query with synonyms can possibly drift the query. Using five DirichletLM models, two of which expand the query, and non apply lemmatization, a total of five runs were submitted.</p><p>Heimdall <ref type="bibr" coords="10,143.12,561.19,17.93,10.91" target="#b46">[47]</ref> aims at including both topical relevance and argument quality while ranking arguments. As a basic retrieval model, DirichletLM is used. The basic retrieval model is considered to give a mere textual relevance. To assess the topical relevance of an argument, arguments are embedded using the Universal Sentence Encoder and then clustered using kmeans with ùëò = 300. Arguments are then represented using their cluster centroids and the topical relevance of an argument is calculated using the cosine similarity of the query to the centroid. Argument quality is assessed using a support vector regression model that is trained on the Webis-ArgQuality-20 corpus. The regression model achieves a mean squared error of 0.19. Before assessing the quality of arguments, an argumentativeness classifier is used to filter input instances that are not arguments. The support vector machine classifier is also trained on the same dataset and achieves an F1-score of 0.88. A total of five runs were submitted.</p><p>Hua Mulan <ref type="bibr" coords="11,152.32,114.06,18.03,10.91" target="#b47">[48]</ref> proposes to expand documents from the args.me corpus prior to indexing, evaluating how different expansion methods affect the argument retrieval for controversial topics. Three expansion approaches are presented: the first uses a transformer-based query prediction to generate queries based on the premises and conclusions as input, which are then added to the documents. The second is also transformer-based and generates ("hallucinates") arguments using GPT-2 based on the conclusions. The third approach uses TF-IDF to determine the top-10 keywords and expands the premises using synonyms from the WordNet database. For evaluation, all corpora were indexed and retrieved using Elasticsearch and the DirichletLM similarity. The altered args.me corpus with expansions is made available as dataset. A total of three runs were submitted.</p><p>Jean-Pierre Polnareff <ref type="bibr" coords="11,194.95,249.56,18.07,10.91" target="#b48">[49]</ref> combines differently weighted versions of the BM25 and Dirich-letLM retrieval model with a WordNet-based query expansion, and a re-ranking component that incorporates sentiment analysis to explore whether boosting arguments with high sentiment scores or boosting neutral arguments leads to better results. The authors provide an ablative evaluation study for each of these three components, motivating their parameter choice at each step. Furthermore, different text pre-processing steps were reviewed in-depth, evaluating the effect of the choice of stop word list and stemming algorithm on the final result. A single run was submitted.</p><p>Little Foot applies a query expansion technique over an Okapi BM25 model. The team indexes three fields for each argument: conclusion, premise, and context. Preprocessing the three fields includes lemmatization and removing stop words. The query expansion technique expands nouns, adjectives, and adverbs in the query with synonyms from WordNet. When multiple meanings exist for a word (known as "synset" in WordNet jargon), the approach uses the Lesk algorithm <ref type="bibr" coords="11,134.77,425.70,17.76,10.91" target="#b49">[50]</ref> to disambiguate the meaning of the word based on the context. A single run was submitted.</p><p>Luke Skywalker indexes for each argument its premise, conclusion, and context. As a retrieval model they implemented their own tf ‚Ä¢idf model in a single-run submission.</p><p>Macbeth <ref type="bibr" coords="11,139.82,479.89,17.76,10.91" target="#b50">[51]</ref> describes an approach that utilizes fine-tuned SBERT sentence embeddings <ref type="bibr" coords="11,488.23,479.89,17.76,10.91" target="#b51">[52]</ref> in conjunction with different retrieval strategies. First, further pre-training of the RoBERTa model on the args.me corpus with annotated relevance labels is carried out. They then obtain sentence embeddings of all documents in the args.me corpus with SBERT based on the pretrained model. Weakly supervised data-augmentation is used to fine-tune the bi-encoder further, based on labels inferred using a cross-encoder architecture. Three retrieval strategies are then applied: (1) approximate nearest-neighbor vector retrieval on the inferred document embeddings, (2) BM25, and (3) a mixture of both. An initial retrieved pool of candidate documents is reranked by direct query/document comparison using a cross-encoder architecture. The authors experiment with different pipeline configurations. A total of five runs were submitted.</p><p>Palpatine, befittingly, submitted one of the worst-performing of all runs, without providing any explanation whatsoever.</p><p>Pippin Took <ref type="bibr" coords="11,154.20,642.48,18.02,10.91" target="#b52">[53]</ref> first preprocesses documents with the Krovetz Stemmer <ref type="bibr" coords="11,429.84,642.48,16.36,10.91" target="#b53">[54]</ref>, and remove stop words using a custom stop word list curated from various libraries. After parameter-tuning Lucene's implementation of DirichletLM using the Touch√© 2020 relevance labels, they then experiment with two different retrieval pipelines: (1) query expansion with WordNet, and (2) phrase search with term trigrams, which follows the idea that arguments containing parts of the query as phrases will be part of an effective argumentative ranking. Therefore, the arguments are indexed as term trigrams, and each query is split into term trigrams to retrieve arguments with DirichletLM. However, preliminary experiments suggested that argument retrieval with term trigrams substantially decreases the nDCG@5. Hence, Took omits phrase search and submits three runs with DirichletLM only, and two runs with DirichletLM and query expansion, varying the parameter ùúá of DirichletLM, for a total of five runs.</p><p>Robin Hood relies on the RM3 implementation from the Pyserini toolkit <ref type="bibr" coords="12,411.66,195.36,17.76,10.91" target="#b54">[55]</ref> to perform query expansion. For retrieval, they embed both the premise and the conclusion of each argument into two separate vector spaces using the Universal Sentence Encoder, ranking arguments based on the cosine similarity between embedded query and document. The two embeddings are incorporated with different weights. They further take document length into account, deducting up to 15% of an arguments score if its length lies outside of one standard deviation of the mean across the whole corpus. They submit one baseline run using the DirichletLM retrieval model, one with RM3 query expansion applied on top of that, one using only cosine similarity on phrase embeddings, and one using RM3 in conjunction with phrase embeddings for retrieval, for a total of four runs.</p><p>Shanks <ref type="bibr" coords="12,134.95,330.85,18.07,10.91" target="#b55">[56]</ref> indexes discussion titles in addition to the premises and conclusions in the args.me corpus. They construct a custom stop word list based on the Smart and Lucene lists, as well as frequent terms within the document collection. They then use a Boolean model with the individual terms of the query to apply boosts to the indexed documents. Each matched term between query and discussion titles, conclusions, and premises in the corpus, as well as all identified WordNet synonyms of query terms receive a boosting factor. Both BM25 and DirichletLM are then used to retrieve relevant documents, with boosting applied. Additionally, a proximity search for all term pairs within the query can be performed and boosted individually. A total of five runs were submitted.</p><p>Skeletor <ref type="bibr" coords="12,137.01,452.79,17.76,10.91" target="#b56">[57]</ref> submits five runs using three different approaches: (1) BM25 retrieval, (2) ranking arguments based on their semantic similarity to the query, and (3) using pseudo relevance feedback in combination with the semantic similarity of passages. Unanimously, the arguments' premise is used for ranking. The BM25 approach uses Pyserini with the BM25 parameters ùëò 1 and ùëè fine-tuned with grid search on the relevance judgments from Touch√© 2020. The two semantic similarity runs use the model msmarco-distilbert-base-v3 provided by Sentence Transformers <ref type="bibr" coords="12,149.98,534.09,16.08,10.91" target="#b51">[52]</ref>, which was fine-tuned for question-answering on MS MARCO <ref type="bibr" coords="12,438.95,534.09,16.09,10.91" target="#b57">[58]</ref>. Therefore, arguments are split by sentence into passages of approximately 200 words, using the maximum cosine similarity of all passages in the argument to the encoded query as retrieval score. The submitted runs differ as follows: Run 1 ranks documents solely by their semantic similarity to the query using approximate nearest neighbor search; Runs 2 and 3 interpolate the semantic similarity score with the tuned BM25 scores; Runs 4 and 5 use the top-3 arguments retrieved by the interpolation of BM25 with the semantic similarity score as pseudo relevance feedback: for each passage from the relevance feedback, the 50 most similar passages are identified with an approximate nearest neighbor search on all encoded passages of the corpus. The probabilities that a passage is highly similar to a passage in the pseudo relevance feedback are determined with manifold approximation and summed as the argument's score. In Run 4 all arguments in the corpus are ranked with this score, and in Run 5 only the top-10 results of the interpolation of BM25 with the semantic similarity are re-ranked.</p><p>The baseline run of Swordsman encompasses two separate approaches: the Elasticsearch implementation of query likelihood with Dirichlet-smoothed language models (DirichletLM <ref type="bibr" coords="13,483.81,127.61,15.58,10.91" target="#b58">[59]</ref>), as well as the args.me API.</p><p>The Yeagerists <ref type="bibr" coords="13,165.68,154.71,17.94,10.91" target="#b59">[60]</ref> describe an approach that integrates two components: query expansion and argument quality regression. Query expansion is performed using a pretrained BERT model which is prompted to substitute certain masked words (adjectives, nouns, and past participles) in the topics. Argument quality regression is performed by training a BERT as a regression model on Webis-ArgQuality-20. The regression model is trained in a 8:1:1 split using mean squared error (MSE) as a loss function, and achieves an MSE of 0.728 on the test split. At retrieval time, for each topic, ten queries are generated using the lexical substitution algorithm and then forwarded to a DirichletLM retrieval model to produce a relevance score. The top-100 arguments are then passed to the regression model to predict their quality score. The relevance score and quality score are normalized and averaged with a weighting variable ùõº that controls the contribution of the quality score to the averaged score. The team tests different ùõº-values using the relevance labels from Touch√© 2020 to motivate parameter choices for their submitted runs. A total of five runs were submitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Task 2: Argument Retrieval for Comparative Questions</head><p>The goal of the Touch√© 2021 lab's second task was to support individuals making informed decisions in "everyday" or personal comparison situations-in its simplest form for questions such as "Is X or Y better for Z?". Decision making in such situations benefits from finding balanced justifications for choosing one or the other option, for instance, via an overview of relevant and high-quality pro/con arguments.</p><p>Similar to Task 1, the results of last year's Task 2 participants indicated that improving upon an argument-agnostic BM25F baseline is challenging. Promising proposed approaches tried to re-rank based on features capturing "comparativeness" or "argumentativeness. "</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Task Definition</head><p>Given a comparative question, an approach to Task 2 needed to retrieve documents from the general web crawl ClueWeb12<ref type="foot" coords="13,224.41,532.29,3.71,7.97" target="#foot_8">9</ref> that help to come to an informed decision on the comparison. Ideally, the retrieved documents should be argumentative with convincing arguments for or against one or the other option. To identify arguments in web documents, the participants were not restricted to any system; they could use own technology or any existing argument taggers such as MARGOT <ref type="bibr" coords="13,208.53,588.25,16.35,10.91" target="#b60">[61]</ref>. To lower the entry barriers for participants new to argument mining, we offered support for using the neural argument tagger TARGER <ref type="bibr" coords="13,425.91,601.80,11.49,10.91" target="#b3">[4]</ref>, hosted on our own servers and accessible via an API.<ref type="foot" coords="13,260.62,613.59,7.41,7.97" target="#foot_9">10</ref>  Description A soon-to-be high-school graduate finds themself at a crossroad in their live. Based on their interests, majoring in philosophy or in psychology are the potential options and the graduate is searching for information about the differences and similarities, as well as advantages and disadvantages of majoring in either of them (e.g., with respect to career opportunities or gained skills).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Narrative</head><p>Relevant documents will overview one of the two majors in terms of career prospects or developed new skills, or they will provide a list of reasons to major in one or the other. Highly relevant documents will compare the two majors side-by-side and help to decide which should be preferred in what context. Not relevant are study program and university advertisements or general descriptions of the disciplines that do not mention benefits, advantages, or pros/cons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Data Description</head><p>Topics. For the second edition of Task 2, we manually selected 50 new comparative questions from the MS MARCO dataset <ref type="bibr" coords="14,217.64,378.63,17.75,10.91" target="#b57">[58]</ref> (questions from Bing's search logs) and the Quora dataset <ref type="bibr" coords="14,488.23,378.63,17.76,10.91" target="#b61">[62]</ref> (questions asked on the Quora question answering website). We ensured to have questions on diverse topics, for example, asking about electronics, cuisine, house appliances, life choices, etc. Table <ref type="table" coords="14,115.17,419.28,4.97,10.91" target="#tab_2">3</ref> shows an example topic for Task 2 that consists of a title (i.e., a comparative question), a description of the possible search context and situation, and a narrative describing what makes a retrieved result relevant (meant as a guideline for human assessors). In the topic selection, we ensured that relevant documents for each topic were actually contained in the ClueWeb12 (i.e., avoiding questions on comparison options not known at the ClueWeb12 crawling time in 2012).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document Collection.</head><p>The document collection was formed by the ClueWeb12 dataset that contains 733 million English web pages <ref type="bibr" coords="14,275.16,515.78,105.77,10.91">(27.3 TB uncompressed)</ref>, crawled by the Language Technologies Institute at Carnegie Mellon University between February and May 2012. For participants of Task 2 who could not index the ClueWeb12 at their site, we provided access to the indexed corpus through the BM25F-based search engine ChatNoir <ref type="bibr" coords="14,402.95,556.43,17.91,10.91" target="#b62">[63]</ref> via its API.<ref type="foot" coords="14,472.19,554.68,7.41,7.97" target="#foot_10">11</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Judgment Process</head><p>Using the CopyCat framework <ref type="bibr" coords="14,233.89,606.16,16.42,10.91" target="#b30">[31]</ref>, we found that, on average, 11.6% of the documents in the top-5 results of a run were near-duplicates-a non-negligible redundancy that might have negatively impacted the reliability and validity of our evaluation since rankings containing multiple relevant duplicates tend to overestimate the actual retrieval effectiveness <ref type="bibr" coords="14,471.00,646.81,16.55,10.91" target="#b31">[32,</ref><ref type="bibr" coords="14,491.11,646.81,12.42,10.91" target="#b32">33]</ref>.</p><p>Following the strategy used in Task 1, we pooled the top-5 documents from the original and the deduplicated runs, resulting in 2,076 unique documents that needed to be judged. Our eight volunteer annotators (same as for Task 1) labeled a document for its topical relevance (three labels; 0: not relevant, 1: relevant, and 2: highly relevant) and for whether rhetorically well-written arguments <ref type="bibr" coords="15,199.24,141.16,18.07,10.91" target="#b19">[20]</ref> were contained (three labels; 0: low quality or no arguments in the document, 1: sufficient quality, and 2: high quality). Similar to Task 1, our eight volunteer assessors went through an initial kappa test on 15 documents from 3 topics (5 documents per topic). As in case of Task 1, the observed Fleiss' ùúÖ values of 0.46 for relevance (moderate agreement) and of 0.22 for quality (fair agreement) are similar to previous studies <ref type="bibr" coords="15,456.68,195.36,16.48,10.91" target="#b14">[15,</ref><ref type="bibr" coords="15,475.89,195.36,12.57,10.91" target="#b35">36,</ref><ref type="bibr" coords="15,491.19,195.36,12.36,10.91" target="#b19">20]</ref>. Again, however, we had a follow-up discussion with all the annotators to clarify some potential misinterpretations. Afterwards, each annotator independently judged the results for disjoint subsets of the topics (i.e., each topic was judged by one annotator only).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Submitted Approaches and Results</head><p>For Task 2, six teams submitted approaches that all used ChatNoir for an initial document retrieval, either by submitting the original topic titles as queries, or by applying query preprocessing (e.g., lemmatization and POS-tagging) and query expansion techniques (e.g., synonyms from WordNet <ref type="bibr" coords="15,157.54,326.38,16.36,10.91" target="#b37">[38]</ref>, or generated with word2vec <ref type="bibr" coords="15,310.41,326.38,18.03,10.91" target="#b63">[64]</ref> or sense2vec embeddings <ref type="bibr" coords="15,448.16,326.38,15.84,10.91" target="#b64">[65]</ref>). On the retrieved ChatNoir results, most teams then applied a document "preprocessing" (e.g., removing HTML markup) before re-ranking with feature-based or neural classifiers trained on last year's judgments with, for instance, argumentativeness, credibility, or comparativeness scores as features. The teams predicted document relevance labels by using a random forest classifier, XGBoost <ref type="bibr" coords="15,130.90,394.13,16.17,10.91" target="#b65">[66]</ref>, LightGBM <ref type="bibr" coords="15,203.09,394.13,16.18,10.91" target="#b66">[67]</ref>, or a fine-tuned BERT <ref type="bibr" coords="15,322.31,394.13,16.18,10.91" target="#b28">[29]</ref>. The results of the runs with the best nDCG@5 scores per participating team are reported in Table <ref type="table" coords="15,352.51,407.68,4.97,10.91" target="#tab_4">4</ref> (cf. Appendix A for the evaluation results of all submitted runs). Below, we give an overview of the approaches submitted to Task 2, ordered alphabetically by team name. 12  Jack Sparrow <ref type="bibr" coords="15,160.83,448.32,17.76,10.91" target="#b67">[68]</ref> lemmatizes the question queries in a preprocessing step, creates expansion terms by detecting "comparison" terms in the questions (e.g., nouns or comparative adjectives/adverbs as identified by spaCy's POS tagger 13 ), and identifies synonyms of these terms from WordNet synsets <ref type="bibr" coords="15,191.61,488.97,16.22,10.91" target="#b37">[38]</ref>, from word2vec <ref type="bibr" coords="15,284.34,488.97,16.22,10.91" target="#b63">[64]</ref>, and sense2vec embeddings <ref type="bibr" coords="15,429.24,488.97,16.22,10.91" target="#b64">[65]</ref>. The top-100 ChatNoir results returned for the preprocessed and expanded questions are then re-ranked by a support vector regression trained on the Touch√© 2020 topics and judgments to predict relevance scores for the documents using combinations of the following normalized features:</p><p>(1) argumentative score (sum of argumentativeness probabilities returned by TARGER for each token inside premises and claims), (2) (pseudo) trustworthiness score (0-10-valued PageRank scores obtained from Open PageRank) 14 , (3) relevance labels predicted by a BERT-based classifier fine-tuned on the Touch√© 2020 topics and judgments, and (4) the ChatNoir relevance score. Different runs of Sparrow use different combinations of query preprocessing and expansion, and different feature combinations for the support vector regression; the most effective run 12 One team participated in Task 2 with a valid run, but did not submit a notebook describing their approach. Their methodology is summarized in short here, after consulting with the team members. 13   uses query lemmatization and expansion while the regression is trained on the BERT relevance predictions, combined with the ChatNoir relevance scores. A total of four runs were submitted. Katana <ref type="bibr" coords="16,134.69,338.98,17.99,10.91" target="#b68">[69]</ref> re-ranks the top-100 ChatNoir results (original questions as queries) but using different feature-based and neural classifiers/rankers to predict the final relevance labels: (1) an XGBoost <ref type="bibr" coords="16,132.42,366.07,18.07,10.91" target="#b65">[66]</ref> approach (overall relevance-wise most effective run), (2) a LightGBM <ref type="bibr" coords="16,472.05,366.07,18.07,10.91" target="#b66">[67]</ref> approach (team Katana's quality-wise best run), (3) Random Forests <ref type="bibr" coords="16,386.75,379.62,16.37,10.91" target="#b69">[70]</ref>, and (4) a BERT-based ranker from OpenNIR <ref type="bibr" coords="16,191.00,393.17,16.35,10.91" target="#b70">[71]</ref>. The feature-based approaches are trained on the topics and judgments from Touch√© 2020, employing a range of relevance features (e.g., ChatNoir relevance score) and "comparativness" features (e.g., number of identified comparison objects, aspects, or predicates <ref type="bibr" coords="16,150.82,433.82,15.89,10.91" target="#b27">[28]</ref>). The BERT-based ranker is trained on the ANTIQUE question-answering dataset <ref type="bibr" coords="16,122.57,447.37,17.76,10.91" target="#b71">[72]</ref> (34,000 text passages with relevance annotations for 2,600 open-domain non-factoid questions). A total of six runs were submitted (we evaluated all of them since the overall judgment load was feasible).</p><p>Mercutio <ref type="bibr" coords="16,141.24,488.02,17.78,10.91" target="#b72">[73]</ref> expands the original question queries with synonyms obtained from word2vec embeddings <ref type="bibr" coords="16,145.57,501.57,17.91,10.91" target="#b63">[64]</ref> (Mercutio's best run uses embeddings pre-trained on the Gigaword corpus <ref type="foot" coords="16,495.48,499.81,7.41,7.97" target="#foot_11">15</ref> ) or nouns found in GPT-2 <ref type="bibr" coords="16,205.77,515.11,18.06,10.91" target="#b73">[74]</ref> extensions when prompted with the question. The respective top-100 ChatNoir results are then re-ranked based on a linear combination of several scores (e.g., term-frequency counts, ratio of premises and claims in documents as identified by TARGER, etc.). The weights of the individual scores are optimized in a grid search on the Touch√© 2020 topics and judgments. A total of three runs were submitted.</p><p>Prince Caspian re-ranks the top-40 ChatNoir results returned for the questions without stop words. The re-ranking uses the results' main content (extracted with the BoilerPy3 library; <ref type="foot" coords="16,498.07,594.66,7.41,7.97" target="#foot_12">16</ref>topic title terms in the extracted main content masked with a "MASK" token) and a logistic regression classifier (features: tf ‚Ä¢idf -weighted 1-to 4-grams; training on the Touch√© 2020 topics and judgments) that predicts the probability of a result being relevant (final ranking by descending probability). A single run was submitted.</p><p>The baseline run of Puss in Boots simply uses the results that ChatNoir <ref type="bibr" coords="17,419.58,100.52,18.01,10.91" target="#b62">[63]</ref> returns for the original question query. ChatNoir is an Elasticsearch-based search engine for the ClueWeb12 (and several other web corpora) that employs BM25F ranking (fields: document title, keywords, main content, and the full document) and SpamRank scores <ref type="bibr" coords="17,357.07,141.16,16.25,10.91" target="#b74">[75]</ref>.</p><p>Rayla <ref type="bibr" coords="17,128.04,154.71,18.05,10.91" target="#b75">[76]</ref> uses two query processing/expansion techniques: (1) removing stop words and punctuation, and then lemmatizing the remaining tokens with spaCy, and (2) expanding comparative adjectives/adverbs (POS-tagged with spaCy) with a maximum of five synonyms and antonyms. The final re-ranking is created by linearly combining different scores such as a ChatNoir's relevance score, PageRank, and SpamRank (both also returned by ChatNoir), an argument support score (ratio of argumentative sentences (premises and claims) in documents found with a custom DistilBERT-based <ref type="bibr" coords="17,264.93,236.01,17.93,10.91" target="#b76">[77]</ref> classifier), and a similarity score (averaged cosine similarity between the original query and every argumentative sentence in the document represented by Sentence-BERT embeddings <ref type="bibr" coords="17,259.20,263.11,15.53,10.91" target="#b51">[52]</ref>). The weights of the individual scores are optimized in a grid search on the Touch√© 2020 topics and judgments. A total of four runs were submitted.</p><p>Thor <ref type="bibr" coords="17,124.09,290.20,17.81,10.91" target="#b77">[78]</ref> removes, as query preprocessing, any punctuation from the topic titles. They then locally create an Elasticsearch BM25F index of the top-110 ChatNoir results (fields: original and lemmatized document title, document body extracted using the BoilerPy3 library, and premises and claims as identified by TARGER in the body) with the BM25 parameters optimized by a grid search on the Touch√© 2020 judgments (ùëè = 0.68 and ùëò 1 = 1.2). The local index is then queried with the lemmatized topic title expanded by WordNet synonyms <ref type="bibr" coords="17,409.08,357.95,16.09,10.91" target="#b37">[38]</ref>. A single run was submitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Summary and Outlook</head><p>From the 36 teams that registered for the Touch√© 2021 lab, 27 actively participated by submitting at least one valid run to one of the two shared tasks: (1) argument retrieval for controversial questions, and (2) argument retrieval for comparative questions. Most of the participating teams used the judgments from the first lab's edition to train feature-based or neural approaches that predict argument quality or that re-rank some initial retrieval result set. Overall, many more approaches could improve upon the argumentation-agnostic baselines (DirichletLM for Task 1 and BM25F for Task 2) than in the first year, indicating that progress was achieved. For a potential third year of the Touch√© lab, we currently plan to focus on retrieving the most relevant/argumentative text passages and on detecting the pro/con stance of the returned results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 6</head><p>Quality results of all runs submitted to Task 1: Argument Retrieval for Controversial Questions. Reported are the mean nDCG@5 and the 95% confidence intervals. The two baseline rankings of the args.me search engine and DirichletLM are shown in bold. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,88.99,90.49,304.00,57.99"><head>Table 1</head><label>1</label><figDesc>Example topic for Task 1: Argument Retrieval for Controversial Questions.</figDesc><table coords="6,89.88,123.35,236.47,25.13"><row><cell>Number</cell><cell>89</cell></row><row><cell>Title</cell><cell>Should hate speech be penalized more?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="14,88.99,90.49,301.85,57.99"><head>Table 3</head><label>3</label><figDesc>Example topic for Task 2: Argument Retrieval for Comparative Questions.</figDesc><table coords="14,89.88,123.35,258.20,25.13"><row><cell>Number</cell><cell>88</cell></row><row><cell>Title</cell><cell>Should I major in philosophy or psychology?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="16,88.99,90.49,418.66,194.68"><head>Table 4</head><label>4</label><figDesc>Results for Task 2: Argument Retrieval for Comparative Questions. The left part (a) shows the evaluation results of a team's best run according to the results' relevance, while the right part (b) shows the best runs according to the results' quality. An asterisk ( ‚ãÜ ) indicates that the runs with the best relevance and the best quality differ for a team. The baseline ChatNoir ranking is shown in bold.</figDesc><table coords="16,89.29,154.04,416.69,131.12"><row><cell cols="2">(a) Best relevance score per team</cell><cell></cell><cell cols="2">(b) Best quality score per team</cell><cell></cell></row><row><cell>Team</cell><cell cols="2">nDCG@5</cell><cell>Team</cell><cell cols="2">nDCG@5</cell></row><row><cell></cell><cell>Relevance</cell><cell>Quality</cell><cell></cell><cell>Quality</cell><cell>Relevance</cell></row><row><cell>Katana ‚ãÜ</cell><cell>0.489</cell><cell>0.675</cell><cell>Rayla ‚ãÜ</cell><cell>0.688</cell><cell>0.466</cell></row><row><cell>Thor</cell><cell>0.478</cell><cell>0.680</cell><cell>Katana ‚ãÜ</cell><cell>0.684</cell><cell>0.460</cell></row><row><cell>Rayla ‚ãÜ</cell><cell>0.473</cell><cell>0.670</cell><cell>Thor</cell><cell>0.680</cell><cell>0.478</cell></row><row><cell>Jack Sparrow</cell><cell>0.467</cell><cell>0.664</cell><cell>Jack Sparrow</cell><cell>0.664</cell><cell>0.467</cell></row><row><cell>Mercutio</cell><cell>0.441</cell><cell>0.651</cell><cell>Mercutio</cell><cell>0.651</cell><cell>0.441</cell></row><row><cell>Puss in Boots</cell><cell>0.422</cell><cell>0.636</cell><cell>Puss in Boots</cell><cell>0.636</cell><cell>0.422</cell></row><row><cell>Prince Caspian</cell><cell>0.244</cell><cell>0.548</cell><cell>Prince Caspian</cell><cell>0.548</cell><cell>0.244</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.93,627.20,397.06,8.97;2,89.29,638.16,417.28,8.97;2,89.29,649.12,97.87,8.97"><p>The name of the lab is inspired by the usage of the term 'touch√©' as an exclamation "used to admit that someone has made a good point against you in an argument or discussion. " [https://dictionary.cambridge.org/ dictionary/english/touche]</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,108.93,660.08,87.61,8.97"><p>https://touche.webis.de/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,108.93,671.04,254.02,8.97"><p>https://www.research.ibm.com/artificial-intelligence/project-debater/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,108.93,671.02,89.60,8.97"><p>http://commoncrawl.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="5,108.93,671.03,303.28,8.97"><p>The expected format was described at the lab's web page [https://touche.webis.de].</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="7,108.93,660.07,157.93,8.97"><p>https://webis.de/data.html#args-me-corpus</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="7,108.93,671.03,122.39,8.97"><p>https://www.args.me/api-en.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="8,108.93,662.64,398.44,8.97;8,89.02,673.60,416.96,8.97;8,89.29,684.56,167.96,8.97"><p>Nine teams participated in Task 1 with valid runs, but did not submit a notebook describing their approach. Their methodology is summarized in short here, after consulting with the respective team members. Blade and Palpatine did not provide further information.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="13,108.93,673.63,132.13,8.97"><p>https://lemurproject.org/clueweb12/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9" coords="13,108.93,684.58,153.21,8.97"><p>https://demo.webis.de/targer-api/apidocs/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10" coords="14,108.93,671.03,108.05,8.97"><p>https://www.chatnoir.eu/doc/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_11" coords="16,108.93,660.08,156.10,8.97"><p>https://catalog.ldc.upenn.edu/LDC2011T07</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_12" coords="16,108.93,671.04,126.00,8.97"><p>https://pypi.org/project/boilerpy3/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We are very grateful to the CLEF 2021 organizers and the Touch√© participants, who allowed this lab to happen. We also want to thank <rs type="person">Jan Heinrich Reimer</rs> for setting up Doccano, <rs type="person">Christopher Akiki</rs> for providing the baseline DirichletLM implementation Swordsman, our volunteer annotators who helped to create the relevance and argument quality assessments, and our reviewers for their valuable feedback on the participants' notebooks.</p><p>This work was partially supported by the <rs type="funder">DFG</rs> through the project "<rs type="projectName">ACQuA: Answering Comparative Questions with Arguments"</rs> (grants <rs type="grantNumber">BI 1544/7-1</rs> and <rs type="grantNumber">HA 5851/2-1</rs>) as part of the priority program "<rs type="projectName">RATIO: Robust Argumentation Machines"</rs> (<rs type="grantNumber">SPP 1999</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_TNR9g47">
					<idno type="grant-number">BI 1544/7-1</idno>
					<orgName type="project" subtype="full">ACQuA: Answering Comparative Questions with Arguments&quot;</orgName>
				</org>
				<org type="funded-project" xml:id="_AB8tuWF">
					<idno type="grant-number">HA 5851/2-1</idno>
					<orgName type="project" subtype="full">RATIO: Robust Argumentation Machines&quot;</orgName>
				</org>
				<org type="funding" xml:id="_bxXmgRH">
					<idno type="grant-number">SPP 1999</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Full Evaluation Results of Touch√© 2021: Argument Retrieval Table 5</head><p>Relevance results of all runs submitted to Task 1: Argument Retrieval for Controversial Questions. Reported are the mean nDCG@5 and the 95% confidence intervals. The two baseline rankings of the args.me search engine and DirichletLM are shown in bold. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 7</head><p>Relevance results of all runs submitted to Task 2: Argument Retrieval for Comparative Questions. Reported are the mean nDCG@5 and the 95% confidence intervals; ChatNoir baseline in bold. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 8</head><p>Quality results of all runs submitted to Task 2: Argument Retrieval for Comparative Questions. Reported are the mean nDCG@5 and the 95% confidence intervals; ChatNoir baseline in bold. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="18,112.66,172.69,394.53,10.91;18,112.66,186.24,313.98,10.91;18,445.87,186.24,60.12,10.91;18,112.33,199.79,393.65,10.91;18,112.66,213.34,394.53,10.91;18,112.66,226.89,65.44,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="18,445.87,186.24,60.12,10.91;18,112.33,199.79,147.26,10.91">Overview of Touch√© 2021: Argument Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gienapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fr√∂be</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beloucif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,282.66,199.79,223.33,10.91;18,112.66,213.34,137.06,10.91">Proceedings of the 12th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="18,356.29,214.35,146.74,9.72">Lecture Notes in Computer Science</title>
		<meeting>the 12th International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="volume">12880</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,240.44,394.53,10.91;18,112.48,253.99,393.50,10.91;18,112.66,267.54,393.54,10.91;18,112.66,281.08,388.67,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="18,213.90,253.99,215.01,10.91">Building an Argument Search Engine for the Web</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">A</forename><surname>Khatib</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Puschmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dorsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Morari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w17-5106</idno>
		<ptr target="https://doi.org/10.18653/v1/w17-5106" />
	</analytic>
	<monogr>
		<title level="m" coord="18,452.73,253.99,53.25,10.91;18,112.66,267.54,317.78,10.91">Proceedings of the 4th Workshop on Argument Mining (ArgMining@EMNLP 2017)</title>
		<meeting>the 4th Workshop on Argument Mining (ArgMining@EMNLP 2017)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="49" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,294.63,394.53,10.91;18,112.28,308.18,393.70,10.91;18,112.66,321.73,393.33,10.91;18,112.66,335.28,395.01,10.91;18,112.66,348.83,190.28,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="18,112.28,308.18,302.65,10.91">ArgumenText: Searching for Arguments in Heterogeneous Sources</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Stab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Daxenberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Stahlhut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schiller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tauchmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Eger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-5005</idno>
		<ptr target="https://doi.org/10.18653/v1/n18-5005" />
	</analytic>
	<monogr>
		<title level="m" coord="18,438.91,308.18,67.07,10.91;18,112.66,321.73,393.33,10.91;18,112.66,335.28,133.79,10.91">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT 2018)</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT 2018)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="21" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,362.38,394.53,10.91;18,112.28,375.93,395.55,10.91;18,112.66,389.48,395.17,10.91;18,112.66,403.03,394.62,10.91;18,112.66,416.58,202.01,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="18,193.21,375.93,258.09,10.91">TARGER: Neural Argument Mining at Your Fingertips</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chernodub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Oliynyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Heidenreich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P19-3031" />
	</analytic>
	<monogr>
		<title level="m" coord="18,488.39,375.93,19.44,10.91;18,112.66,389.48,395.17,10.91;18,112.66,403.03,69.72,10.91">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL 2019)</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics (ACL 2019)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="195" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,430.13,395.17,10.91;18,112.66,443.67,393.33,10.91;18,112.66,457.22,393.53,10.91;18,112.66,470.77,393.23,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="18,490.39,430.13,17.44,10.91;18,112.66,443.67,281.10,10.91">Answering Comparative Questions: Better than Ten-Blue-Links?</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schildw√§chter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zenker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<idno type="DOI">10.1145/3295750.3298916</idno>
		<ptr target="https://doi.org/10.1145/3295750.3298916" />
	</analytic>
	<monogr>
		<title level="m" coord="18,420.67,443.67,85.31,10.91;18,112.66,457.22,320.73,10.91">Proceedings of the Conference on Human Information Interaction and Retrieval (CHIIR 2019)</title>
		<meeting>the Conference on Human Information Interaction and Retrieval (CHIIR 2019)</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="361" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,484.32,393.33,10.91;18,112.66,497.87,393.33,10.91;18,112.28,511.42,393.70,10.91;18,112.66,524.97,395.00,10.91;18,112.66,538.52,59.21,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="18,416.50,484.32,89.49,10.91;18,112.66,497.87,262.46,10.91">From Arguments to Key Points: Towards Automatic Argument Summarization</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bar-Haim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Eden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kantor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lahav</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Slonim</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.371</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.acl-main.371" />
	</analytic>
	<monogr>
		<title level="m" coord="18,399.14,497.87,106.84,10.91;18,112.28,511.42,335.84,10.91">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020)</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4029" to="4039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,552.07,394.53,10.91;18,112.28,565.62,393.71,10.91;18,112.66,579.17,393.33,10.91;18,112.28,592.72,393.70,10.91;18,112.66,606.27,326.87,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="18,291.95,565.62,214.03,10.91;18,112.66,579.17,182.48,10.91">From Surrogacy to Adoption; From Bitcoin to Cryptocurrency: Debate Topic Expansion</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bar-Haim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Krieger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Toledo-Ronen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Edelstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bilu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Halfon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Menczel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Aharonov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Slonim</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1094</idno>
		<ptr target="https://doi.org/10.18653/v1/p19-1094" />
	</analytic>
	<monogr>
		<title level="m" coord="18,318.65,579.17,187.33,10.91;18,112.28,592.72,245.36,10.91">Proceedings of the 57th Conference of the Association for Computational Linguistics (ACL 2019)</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics (ACL 2019)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="977" to="990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,619.81,393.33,10.91;18,112.66,633.36,393.33,10.91;18,112.66,646.91,394.52,10.91;18,112.66,660.46,356.60,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="18,482.00,619.81,23.99,10.91;18,112.66,633.36,226.62,10.91">Word Emphasis Prediction for Expressive Text to Speech</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Mass</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mordechay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hoory</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">S</forename><surname>Shalom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Konopnicki</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2018-1159</idno>
		<ptr target="https://doi.org/10.21437/Interspeech.2018-1159" />
	</analytic>
	<monogr>
		<title level="m" coord="18,362.94,633.36,143.04,10.91;18,112.66,646.91,390.15,10.91">Proceedings of the 19th Annual Conference of the International Speech Communication Association (Interspeech 2018)</title>
		<meeting>the 19th Annual Conference of the International Speech Communication Association (Interspeech 2018)</meeting>
		<imprint>
			<publisher>ISCA</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2868" to="2872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,86.97,393.53,10.91;19,112.33,100.52,393.86,10.91;19,112.66,114.06,394.52,10.91;19,112.66,127.61,288.10,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="19,265.28,86.97,240.91,10.91;19,112.33,100.52,76.66,10.91">Retrieval of the Best Counterargument without Prior Topic Knowledge</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P18-1023/" />
	</analytic>
	<monogr>
		<title level="m" coord="19,216.68,100.52,289.51,10.91;19,112.66,114.06,169.62,10.91">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL 2018)</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (ACL 2018)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="241" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,141.16,395.17,10.91;19,112.66,154.71,393.33,10.91;19,112.66,168.26,394.51,10.91;19,112.66,184.25,123.08,7.90" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="19,444.09,141.16,63.75,10.91;19,112.66,154.71,215.23,10.91">Data Acquisition for Argument Search: The args.me Corpus</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-30179-8_4</idno>
	</analytic>
	<monogr>
		<title level="m" coord="19,355.28,154.71,150.71,10.91;19,112.66,168.26,205.61,10.91">Proceedings of the 42nd German Conference on Artificial Intelligence (KI 2019)</title>
		<meeting>the 42nd German Conference on Artificial Intelligence (KI 2019)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="48" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,195.36,395.17,10.91;19,112.66,208.91,393.33,10.91;19,112.66,222.46,393.33,10.91;19,112.66,236.01,378.35,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="19,356.56,195.36,151.28,10.91;19,112.66,208.91,197.56,10.91">Towards an Argumentative Content Search Engine using Weak Supervision</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Bogin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gretz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Aharonov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Slonim</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/C18-1176/" />
	</analytic>
	<monogr>
		<title level="m" coord="19,335.79,208.91,170.19,10.91;19,112.66,222.46,251.20,10.91">Proceedings of the 27th International Conference on Computational Linguistics (COLING 2018)</title>
		<meeting>the 27th International Conference on Computational Linguistics (COLING 2018)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2066" to="2081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,249.56,393.33,10.91;19,112.66,263.11,100.43,10.91" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="19,231.84,249.56,193.92,10.91">On Rhetoric: A Theory of Civic Discourse</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">A</forename><surname>Aristotle</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kennedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,276.66,395.17,10.91;19,112.66,290.20,393.33,10.91;19,112.66,303.75,393.53,10.91;19,112.66,317.30,395.00,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="19,480.73,276.66,27.11,10.91;19,112.66,290.20,228.42,10.91">Argumentation Quality Assessment: Theory vs. Practice</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Naderi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Habernal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hirst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-2039</idno>
		<ptr target="https://doi.org/10.18653/v1/P17-2039" />
	</analytic>
	<monogr>
		<title level="m" coord="19,364.03,290.20,141.96,10.91;19,112.66,303.75,316.33,10.91">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="250" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,330.85,394.53,10.91;19,112.66,344.40,393.33,10.91;19,112.41,357.95,393.57,10.91;19,112.33,371.50,394.94,10.91;19,112.31,385.05,154.13,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="19,165.91,344.40,228.10,10.91">Argument Search: Assessing Argument Relevance</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gienapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Euchner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Heilenk√∂tter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Weidmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3331184.3331327</idno>
		<ptr target="https://doi.org/10.1145/3331184.3331327" />
	</analytic>
	<monogr>
		<title level="m" coord="19,420.74,344.40,85.25,10.91;19,112.41,357.95,393.57,10.91;19,112.33,371.50,54.93,10.91">Proceedings of the 42nd International Conference on Research and Development in Information Retrieval (SIGIR 2019)</title>
		<meeting>the 42nd International Conference on Research and Development in Information Retrieval (SIGIR 2019)</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1117" to="1120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,398.60,393.33,10.91;19,112.66,412.15,393.33,10.91;19,112.66,425.70,395.01,10.91;19,112.66,439.25,269.19,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="19,312.62,398.60,193.37,10.91;19,112.66,412.15,30.65,10.91">Efficient Pairwise Annotation of Argument Quality</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gienapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.511/" />
	</analytic>
	<monogr>
		<title level="m" coord="19,165.64,412.15,340.35,10.91;19,112.66,425.70,100.07,10.91">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020)</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5772" to="5781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,452.79,393.32,10.91;19,112.66,466.34,393.33,10.91;19,112.66,479.89,395.01,10.91;19,112.66,493.44,189.24,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="19,273.51,452.79,155.23,10.91">PageRank&quot; for Argument Relevance</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/e17-1105</idno>
		<ptr target="https://doi.org/10.18653/v1/e17-1105" />
	</analytic>
	<monogr>
		<title level="m" coord="19,452.02,452.79,53.96,10.91;19,112.66,466.34,393.33,10.91;19,112.66,479.89,104.56,10.91">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2017)</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2017)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1117" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,506.99,393.33,10.91;19,112.66,520.54,395.01,10.91;19,112.66,534.09,103.17,10.91" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="19,311.38,506.99,194.60,10.91;19,112.66,520.54,73.05,10.91">The PageRank Citation Ranking: Bringing Order to the Web</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<idno>1999-66</idno>
		<ptr target="http://ilpubs.stanford.edu:8090/422/" />
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>Stanford InfoLab</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="19,112.66,547.64,393.33,10.91;19,112.28,561.19,393.71,10.91;19,112.66,574.74,394.52,10.91;19,112.66,588.29,354.84,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="19,291.11,547.64,214.88,10.91;19,112.28,561.19,216.88,10.91">A Framework for Argument Retrieval -Ranking Argument Clusters by Frequency and Specificity</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dumani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schenkel</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-45439-5_29</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-45439-5_29" />
	</analytic>
	<monogr>
		<title level="m" coord="19,351.93,561.19,154.05,10.91;19,112.66,574.74,167.16,10.91">Proceedings of the 42nd European Conference on IR Research (ECIR 2020)</title>
		<title level="s" coord="19,358.67,575.75,144.37,9.72">Lecture Notes in Computer Science</title>
		<meeting>the 42nd European Conference on IR Research (ECIR 2020)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12035</biblScope>
			<biblScope unit="page" from="431" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,601.84,393.33,10.91;19,112.66,615.39,393.32,10.91;19,112.66,628.93,395.01,10.91;19,112.41,642.48,124.35,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="19,225.80,601.84,170.28,10.91">Quality Aware Ranking of Arguments</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dumani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schenkel</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-45439-5_29</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-45439-5_29" />
	</analytic>
	<monogr>
		<title level="m" coord="19,421.40,601.84,84.59,10.91;19,112.66,615.39,393.32,10.91;19,112.66,628.93,21.75,10.91">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management (CIKM 2020)</title>
		<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management (CIKM 2020)</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="335" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,656.03,394.53,10.91;19,112.66,669.58,393.33,10.91;20,112.66,86.97,393.33,10.91;20,112.66,100.52,391.27,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="19,112.66,669.58,316.45,10.91">Computational Argumentation Quality Assessment in Natural Language</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Naderi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bilu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Thijm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hirst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/E17-1017" />
	</analytic>
	<monogr>
		<title level="m" coord="19,452.57,669.58,53.42,10.91;20,112.66,86.97,393.33,10.91;20,112.66,100.52,104.83,10.91">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2017)</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2017)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="176" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,114.06,393.33,10.91;20,112.14,127.61,393.85,10.91;20,112.33,141.16,394.94,10.91;20,112.31,154.71,143.99,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="20,221.79,114.06,284.20,10.91;20,112.14,127.61,47.90,10.91">A Comparative Web Browser (CWB) for Browsing and Comparing Web Pages</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nadamoto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Tanaka</surname></persName>
		</author>
		<idno type="DOI">10.1145/775152.775254</idno>
		<ptr target="https://doi.org/10.1145/775152.775254" />
	</analytic>
	<monogr>
		<title level="m" coord="20,191.33,127.61,314.66,10.91;20,112.33,141.16,61.44,10.91">Proceedings of the 12th International World Wide Web Conference (WWW 2003)</title>
		<meeting>the 12th International World Wide Web Conference (WWW 2003)</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="727" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,168.26,394.53,10.91;20,112.66,181.81,394.53,10.91;20,112.28,195.36,394.41,10.91;20,112.41,208.91,75.82,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="20,314.95,168.26,187.21,10.91">CWS: A Comparative Web Search System</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1145/1135777.1135846</idno>
		<ptr target="https://doi.org/10.1145/1135777.1135846" />
	</analytic>
	<monogr>
		<title level="m" coord="20,127.31,181.81,375.54,10.91">Proceedings of the 15th International Conference on World Wide Web (WWW 2006)</title>
		<meeting>the 15th International Conference on World Wide Web (WWW 2006)</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="467" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,222.46,393.33,10.91;20,112.66,236.01,393.33,10.91;20,112.66,249.56,394.62,10.91;20,112.66,263.11,179.39,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="20,188.47,222.46,239.77,10.91">Identifying Comparative Sentences in Text Documents</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1145/1148170.1148215</idno>
		<ptr target="https://doi.org/10.1145/1148170.1148215" />
	</analytic>
	<monogr>
		<title level="m" coord="20,452.09,222.46,53.90,10.91;20,112.66,236.01,393.33,10.91;20,112.66,249.56,98.07,10.91">Proceedings of the 29th Annual International Conference on Research and Development in Information Retrieval (SIGIR 2006)</title>
		<meeting>the 29th Annual International Conference on Research and Development in Information Retrieval (SIGIR 2006)</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="244" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,276.66,393.33,10.91;20,112.66,290.20,393.32,10.91;20,112.66,303.75,394.62,10.91;20,112.66,317.30,252.71,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="20,191.57,276.66,205.21,10.91">Mining Comparative Sentences and Relations</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://www.aaai.org/Library/AAAI/2006/aaai06-209.php" />
	</analytic>
	<monogr>
		<title level="m" coord="20,421.34,276.66,84.65,10.91;20,112.66,290.20,393.32,10.91;20,112.66,303.75,215.47,10.91">Proceedings of the 21st National Conference on Artificial Intelligence and the 18th Innovative Applications of Artificial Intelligence Conference (AAAI 2006)</title>
		<meeting>the 21st National Conference on Artificial Intelligence and the 18th Innovative Applications of Artificial Intelligence Conference (AAAI 2006)</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1331" to="1336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,330.85,393.33,10.91;20,112.66,344.40,394.53,10.91;20,112.66,357.95,395.01,10.91;20,112.66,371.50,256.59,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="20,206.45,330.85,207.09,10.91">A Corpus of Comparisons in Product Reviews</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kessler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kuhn</surname></persName>
		</author>
		<ptr target="http://www.lrec-conf.org/proceedings/lrec2014/summaries/1001.html" />
	</analytic>
	<monogr>
		<title level="m" coord="20,438.76,330.85,67.23,10.91;20,112.66,344.40,394.53,10.91;20,112.66,357.95,218.50,10.91">Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC 2014), European Language Resources Association (ELRA)</title>
		<meeting>the 9th International Conference on Language Resources and Evaluation (LREC 2014), European Language Resources Association (ELRA)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2242" to="2248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,385.05,395.17,10.91;20,112.66,398.60,395.17,10.91;20,112.66,412.15,394.62,10.91;20,112.66,425.70,167.84,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="20,405.53,385.05,102.30,10.91;20,112.66,398.60,63.27,10.91">Categorizing Comparative Sentences</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Franzek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w19-4516</idno>
		<ptr target="https://doi.org/10.18653/v1/w19-4516" />
	</analytic>
	<monogr>
		<title level="m" coord="20,204.69,398.60,303.14,10.91;20,112.66,412.15,70.88,10.91">Proceedings of the 6th Workshop on Argument Mining (ArgMining@ACL 2019)</title>
		<meeting>the 6th Workshop on Argument Mining (ArgMining@ACL 2019)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,439.25,393.33,10.91;20,112.28,452.79,393.70,10.91;20,112.28,466.34,395.55,10.91;20,112.66,479.89,394.03,10.91;20,112.66,493.44,134.12,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="20,295.41,439.25,210.57,10.91;20,112.28,452.79,267.87,10.91">Entity-Aware Dependency-Based Deep Graph Attention Network for Comparative Preference Classification</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.512/" />
	</analytic>
	<monogr>
		<title level="m" coord="20,402.62,452.79,103.36,10.91;20,112.28,466.34,395.55,10.91;20,112.66,479.89,154.72,10.91">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020), Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020), Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5782" to="5788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,506.99,393.33,10.91;20,112.66,520.54,393.33,10.91;20,112.66,534.09,393.53,10.91;20,112.66,547.64,394.53,10.91;20,112.28,561.19,395.39,10.91;20,112.66,574.74,158.75,10.91" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="20,476.85,506.99,29.14,10.91;20,112.66,520.54,393.33,10.91;20,112.66,534.09,90.68,10.91">Which is Better for Deep Learning: Python or MATLAB? Answering Comparative Questions in Natural Language</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chekalina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beloucif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2021.eacl-demos.36/" />
	</analytic>
	<monogr>
		<title level="m" coord="20,228.37,534.09,277.82,10.91;20,112.66,547.64,390.15,10.91">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations (EACL 2021)</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations (EACL 2021)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="302" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,588.29,393.33,10.91;20,112.33,601.84,393.65,10.91;20,112.66,615.39,393.32,10.91;20,112.66,628.93,394.52,10.91;20,112.66,642.48,283.72,10.91" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="20,316.84,588.29,189.14,10.91;20,112.33,601.84,192.59,10.91">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
		<ptr target="https://doi.org/10.18653/v1/n19-1423" />
	</analytic>
	<monogr>
		<title level="m" coord="20,330.38,601.84,175.60,10.91;20,112.66,615.39,393.32,10.91;20,112.66,628.93,195.45,10.91">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2019)</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2019)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,656.03,394.61,10.91;20,112.66,669.58,393.32,10.91;21,112.66,86.97,394.61,10.91;21,112.66,100.52,199.04,10.91" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="20,319.39,656.03,187.88,10.91;20,112.66,669.58,393.32,10.91;21,112.66,86.97,33.11,10.91">TIRA Integrated Research Architecture, in: Information Retrieval Evaluation in a Changing World -Lessons Learned from 20 Years of CLEF</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-22948-1_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-22948-1_5" />
	</analytic>
	<monogr>
		<title level="s" coord="21,214.46,87.98,137.15,9.72">The Information Retrieval Series</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="123" to="160" />
			<date type="published" when="2019">2019</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,114.06,394.61,10.91;21,112.66,127.61,393.33,10.91;21,112.66,141.16,393.33,10.91;21,112.66,154.71,395.01,10.91;21,112.66,168.26,148.21,10.91" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="21,465.65,114.06,41.62,10.91;21,112.66,127.61,318.55,10.91">CopyCat: Near-Duplicates within and between the ClueWeb and the Common Crawl</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fr√∂be</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gienapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>V√∂lske</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3404835.3463246</idno>
		<ptr target="https://dl.acm.org/doi/10.1145/3404835.3463246" />
	</analytic>
	<monogr>
		<title level="m" coord="21,452.94,127.61,53.05,10.91;21,112.66,141.16,393.33,10.91;21,112.66,154.71,96.31,10.91">Proceedings of the 44th International ACM Conference on Research and Development in Information Retrieval (SIGIR 2021)</title>
		<meeting>the 44th International ACM Conference on Research and Development in Information Retrieval (SIGIR 2021)</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,181.81,395.17,10.91;21,112.66,195.36,393.33,10.91;21,112.66,208.91,393.53,10.91;21,112.66,222.46,395.01,10.91" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="21,380.34,181.81,127.50,10.91;21,112.66,195.36,132.02,10.91">Sampling Bias Due to Near-Duplicates in Learning to Rank</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fr√∂be</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Reimer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3397271.3401212</idno>
		<ptr target="https://doi.org/10.1145/3397271.3401212" />
	</analytic>
	<monogr>
		<title level="m" coord="21,267.13,195.36,238.86,10.91;21,112.66,208.91,315.27,10.91">Proceedings of the 43rd International ACM Conference on Research and Development in Information Retrieval (SIGIR 2020)</title>
		<meeting>the 43rd International ACM Conference on Research and Development in Information Retrieval (SIGIR 2020)</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1997" to="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,236.01,395.17,10.91;21,112.66,249.56,393.33,10.91;21,112.66,263.11,394.52,10.91;21,112.66,276.66,303.02,10.91" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="21,324.44,236.01,183.39,10.91;21,112.66,249.56,213.26,10.91">The Effect of Content-Equivalent Near-Duplicates on the Evaluation of Search Engines</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fr√∂be</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-45442-5_2</idno>
	</analytic>
	<monogr>
		<title level="m" coord="21,350.18,249.56,155.80,10.91;21,112.66,263.11,167.16,10.91">Proceedings of the 42nd European Conference on IR Research (ECIR 2020)</title>
		<title level="s" coord="21,358.67,264.12,144.37,9.72">Lecture Notes in Computer Science</title>
		<meeting>the 42nd European Conference on IR Research (ECIR 2020)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12036</biblScope>
			<biblScope unit="page" from="12" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,290.20,393.54,10.91;21,112.66,303.75,393.33,10.91;21,112.66,317.30,393.33,10.91;21,112.33,330.85,394.94,10.91;21,112.31,344.40,154.13,10.91" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="21,290.07,290.20,216.13,10.91;21,112.66,303.75,304.51,10.91">TrecTools: an Open-source Python Library for Information Retrieval Practitioners Involved in TREC-like Campaigns</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R M</forename><surname>Palotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Scells</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
		<idno type="DOI">10.1145/3331184.3331399</idno>
		<ptr target="https://doi.org/10.1145/3331184.3331399" />
	</analytic>
	<monogr>
		<title level="m" coord="21,440.87,303.75,65.11,10.91;21,112.66,317.30,393.33,10.91;21,112.33,330.85,54.93,10.91">Proceedings of the 42nd International Conference on Research and Development in Information Retrieval (SIGIR 2019)</title>
		<meeting>the 42nd International Conference on Research and Development in Information Retrieval (SIGIR 2019)</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1325" to="1328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,357.95,393.33,10.91;21,112.33,371.50,393.65,10.91;21,112.66,385.05,165.16,10.91" xml:id="b34">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Nakayama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kubo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Taniguchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="https://github.com/doccano/doccano" />
		<title level="m" coord="21,383.41,357.95,122.58,10.91;21,112.33,371.50,66.91,10.91">Doccano: Text Annotation Tool for Human</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,398.60,395.17,10.91;21,112.66,412.15,393.33,10.91;21,112.66,425.70,393.53,10.91;21,112.66,439.25,203.57,10.91" xml:id="b35">
	<analytic>
		<title level="a" type="main" coord="21,480.73,398.60,27.11,10.91;21,112.66,412.15,228.42,10.91">Argumentation Quality Assessment: Theory vs. Practice</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Naderi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Habernal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hirst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,364.03,412.15,141.96,10.91;21,112.66,425.70,316.28,10.91">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="250" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,452.79,394.53,10.91;21,112.66,466.34,313.98,10.91;21,445.87,466.34,60.12,10.91;21,112.33,479.89,393.65,10.91;21,112.66,493.44,395.01,10.91" xml:id="b36">
	<analytic>
		<title level="a" type="main" coord="21,445.87,466.34,60.12,10.91;21,112.33,479.89,143.49,10.91">Overview of Touch√© 2020: Argument Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fr√∂be</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beloucif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gienapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/" />
	</analytic>
	<monogr>
		<title level="m" coord="21,278.52,479.89,227.46,10.91;21,112.66,493.44,17.86,10.91">Working Notes Papers of the CLEF 2020 Evaluation Labs</title>
		<title level="s" coord="21,204.58,494.46,121.01,9.72">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2696</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,506.99,345.22,10.91" xml:id="b37">
	<monogr>
		<title level="m" type="main" coord="21,170.66,506.99,182.49,10.91">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Bradford Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,520.54,393.33,10.91;21,112.66,534.09,395.01,10.91;21,112.66,547.64,63.56,10.91" xml:id="b38">
	<analytic>
		<title level="a" type="main" coord="21,216.33,520.54,224.13,10.91">Exploring Argument Retrieval with Transformers</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/" />
	</analytic>
	<monogr>
		<title level="m" coord="21,466.22,520.54,39.77,10.91;21,112.66,534.09,209.22,10.91">Working Notes Papers of the CLEF 2020 Evaluation Labs</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2696</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,561.19,393.33,10.91;21,112.66,574.74,395.17,10.91;21,112.66,588.29,393.33,10.91;21,112.66,601.84,90.78,10.91" xml:id="b39">
	<analytic>
		<title level="a" type="main" coord="21,290.44,561.19,215.55,10.91;21,112.66,574.74,224.26,10.91">A Search Engine System for Touch√© Argument Retrieval task to answer Controversial Questions</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Raimondi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Alessio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Levorato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,365.66,574.74,142.17,10.91;21,112.66,588.29,203.15,10.91">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="21,323.88,588.29,182.11,10.91;21,112.66,601.84,38.13,10.91">CEUR Workshop Proceedings, CLEF and CEUR-WS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,615.39,393.33,10.91;21,112.66,628.93,393.33,10.91;21,112.66,642.48,196.08,10.91" xml:id="b40">
	<analytic>
		<title level="a" type="main" coord="21,277.84,615.39,164.44,10.91">Step approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Raimondi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Alessio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Levorato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,467.81,615.39,38.18,10.91;21,112.66,628.93,307.49,10.91">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="21,428.74,628.93,77.25,10.91;21,112.66,642.48,143.42,10.91">CEUR Workshop Proceedings, CLEF and CEUR-WS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,656.03,393.33,10.91;21,112.66,669.58,394.53,10.91;22,112.66,86.97,274.20,10.91" xml:id="b41">
	<analytic>
		<title level="a" type="main" coord="21,311.97,656.03,194.02,10.91;21,112.66,669.58,37.82,10.91">Learning to Rank Arguments with Feature Selection</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fr√∂be</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,172.33,669.58,329.69,10.91">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="22,112.66,86.97,221.54,10.91">CEUR Workshop Proceedings, CLEF and CEUR-WS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,100.52,395.17,10.91;22,112.66,114.06,394.53,10.91;22,112.66,127.61,393.49,10.91" xml:id="b42">
	<monogr>
		<title level="m" type="main" coord="22,377.61,114.06,124.59,10.91">Universal Sentence Encoder</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kurzweil</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1803.11175" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,141.16,393.57,10.91;22,112.33,154.71,42.07,10.91" xml:id="b43">
	<analytic>
		<title level="a" type="main" coord="22,169.89,141.16,275.38,10.91">From RankNet to LambdaRank to LambdaMART: An Overview</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,454.20,141.16,39.32,10.91">Learning</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">81</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,168.26,395.17,10.91;22,112.66,181.81,393.33,10.91;22,112.33,195.36,296.49,10.91" xml:id="b44">
	<monogr>
		<title level="m" type="main" coord="22,139.38,181.81,265.56,10.91">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1907.11692" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,208.91,393.53,10.91;22,112.66,222.46,393.33,10.91;22,112.66,236.01,394.53,10.91;22,112.66,249.56,136.83,10.91" xml:id="b45">
	<analytic>
		<title level="a" type="main" coord="22,329.64,208.91,176.54,10.91;22,112.66,222.46,107.92,10.91">Touch√© Task1: Argument Retrieval for Controversial Questions</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Carnelos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Menotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Porro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Prando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,425.21,222.46,80.77,10.91;22,112.66,236.01,254.08,10.91">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="22,374.65,236.01,132.54,10.91;22,112.66,249.56,84.18,10.91">CEUR Workshop Proceedings, CLEF and CEUR-WS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Resolution provided by Team Goemon</note>
</biblStruct>

<biblStruct coords="22,112.66,263.11,393.33,10.91;22,112.66,276.66,393.33,10.91;22,112.66,290.20,196.08,10.91" xml:id="b46">
	<analytic>
		<title level="a" type="main" coord="22,171.03,263.11,268.51,10.91">Quality-aware Argument Retrieval with Topical Clustering</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gienapp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,466.22,263.11,39.77,10.91;22,112.66,276.66,307.49,10.91">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="22,428.74,276.66,77.25,10.91;22,112.66,290.20,143.42,10.91">CEUR Workshop Proceedings, CLEF and CEUR-WS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,303.75,393.33,10.91;22,112.66,317.30,394.53,10.91;22,112.66,330.85,274.20,10.91" xml:id="b47">
	<analytic>
		<title level="a" type="main" coord="22,304.04,303.75,201.95,10.91;22,112.66,317.30,37.25,10.91">Exploring Document Expansion for Argument Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mailach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Eysoldt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kleine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,171.78,317.30,330.24,10.91">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="22,112.66,330.85,221.54,10.91">CEUR Workshop Proceedings, CLEF and CEUR-WS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,344.40,393.53,10.91;22,112.28,357.95,393.70,10.91;22,112.66,371.50,308.58,10.91" xml:id="b48">
	<analytic>
		<title level="a" type="main" coord="22,350.69,344.40,155.49,10.91;22,112.28,357.95,73.89,10.91">Development of an IR System for Argument Search</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Alecci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">B</forename><surname>Amd Luca Martinelli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Ziroldo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,207.68,357.95,298.30,10.91;22,112.66,371.50,26.38,10.91">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="22,147.04,371.50,221.55,10.91">CEUR Workshop Proceedings, CLEF and CEUR-WS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,385.05,393.33,10.91;22,112.66,398.60,393.33,10.91;22,112.28,412.15,394.91,10.91;22,112.66,425.70,395.01,10.91;22,112.66,439.25,143.58,10.91" xml:id="b49">
	<analytic>
		<title level="a" type="main" coord="22,154.62,385.05,351.37,10.91;22,112.66,398.60,181.91,10.91">Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lesk</surname></persName>
		</author>
		<idno type="DOI">10.1145/318723.318728</idno>
		<ptr target="https://doi.org/10.1145/318723.318728.doi:10.1145/318723.318728" />
	</analytic>
	<monogr>
		<title level="m" coord="22,401.75,398.60,104.24,10.91;22,112.28,412.15,348.07,10.91">Proceedings of the 5th Annual International Conference on Systems Documentation, SIGDOC 1986</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Debuys</surname></persName>
		</editor>
		<meeting>the 5th Annual International Conference on Systems Documentation, SIGDOC 1986<address><addrLine>Toronto, Ontario, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1986">1986. 1986</date>
			<biblScope unit="page" from="24" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,452.79,393.33,10.91;22,112.66,466.34,395.17,10.91;22,112.66,479.89,393.33,10.91;22,112.66,493.44,90.78,10.91" xml:id="b50">
	<analytic>
		<title level="a" type="main" coord="22,285.35,452.79,220.63,10.91;22,112.66,466.34,221.18,10.91">Exploring Argument Retrieval for Controversial Questions Using Retrieve and Re-rank Pipelines</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Koniaev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schaefer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,364.05,466.34,143.79,10.91;22,112.66,479.89,203.15,10.91">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="22,323.88,479.89,182.11,10.91;22,112.66,493.44,38.13,10.91">CEUR Workshop Proceedings, CLEF and CEUR-WS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,506.99,395.17,10.91;22,112.66,520.54,393.33,10.91;22,112.66,534.09,393.33,10.91;22,112.66,547.64,395.01,10.91;22,112.66,561.19,243.70,10.91" xml:id="b51">
	<analytic>
		<title level="a" type="main" coord="22,231.26,506.99,276.57,10.91;22,112.66,520.54,41.53,10.91">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1410</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1410" />
	</analytic>
	<monogr>
		<title level="m" coord="22,183.50,520.54,322.49,10.91;22,112.66,534.09,393.33,10.91;22,112.66,547.64,151.86,10.91">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP 2019)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP 2019)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3980" to="3990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,574.74,393.33,10.91;22,112.66,588.29,393.33,10.91;22,112.66,601.84,196.08,10.91" xml:id="b52">
	<analytic>
		<title level="a" type="main" coord="22,269.16,574.74,176.81,10.91">Exploring Approaches for Touch√© Task 1</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">D</forename><surname>Togni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Frasson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zanatta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,467.81,574.74,38.18,10.91;22,112.66,588.29,307.49,10.91">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="22,428.74,588.29,77.25,10.91;22,112.66,601.84,143.42,10.91">CEUR Workshop Proceedings, CLEF and CEUR-WS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,615.39,393.33,10.91;22,112.28,628.93,393.70,10.91;22,112.33,642.48,395.34,10.91;22,112.66,656.03,120.57,10.91" xml:id="b53">
	<analytic>
		<title level="a" type="main" coord="22,167.79,615.39,204.92,10.91">Viewing Morphology as an Inference Process</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Krovetz</surname></persName>
		</author>
		<idno type="DOI">10.1145/160688.160718</idno>
		<ptr target="https://doi.org/10.1145/160688.160718" />
	</analytic>
	<monogr>
		<title level="m" coord="22,398.16,615.39,107.82,10.91;22,112.28,628.93,393.70,10.91;22,112.33,642.48,53.46,10.91">Proceedings of the 16th Annual International Conference on Research and Development in Information Retrieval (SIGIR 1993)</title>
		<meeting>the 16th Annual International Conference on Research and Development in Information Retrieval (SIGIR 1993)</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="191" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,669.58,393.33,10.91;23,112.33,86.97,393.66,10.91;23,112.66,100.52,269.59,10.91" xml:id="b54">
	<monogr>
		<title level="m" type="main" coord="22,355.76,669.58,150.23,10.91;23,112.33,86.97,359.89,10.91">Pyserini: An Easy-to-Use Python Toolkit to Support Replicable IR Research with Sparse and Dense Representations</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<idno>CoRR abs/2102.10073</idno>
		<ptr target="https://arxiv.org/abs/2102.10073" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,114.06,394.53,10.91;23,112.66,127.61,393.32,10.91;23,112.14,141.16,244.73,10.91" xml:id="b55">
	<analytic>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Berno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cassetta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Codogno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Vicentini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Piva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Touch√©</forename><surname>Shanks</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Homework</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,127.43,127.61,343.43,10.91">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="23,478.92,127.61,27.06,10.91;23,112.14,141.16,192.08,10.91">CEUR Workshop Proceedings, CLEF and CEUR-WS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,154.71,393.33,10.91;23,112.66,168.26,393.33,10.91;23,112.66,181.81,196.08,10.91" xml:id="b56">
	<analytic>
		<title level="a" type="main" coord="23,270.97,154.71,170.54,10.91">Argument Retrieval and Visualization</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,466.22,154.71,39.77,10.91;23,112.66,168.26,307.49,10.91">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="23,428.74,168.26,77.25,10.91;23,112.66,181.81,143.42,10.91">CEUR Workshop Proceedings, CLEF and CEUR-WS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,195.36,394.61,10.91;23,112.28,208.91,393.70,10.91;23,112.66,222.46,393.32,10.91;23,112.66,236.01,393.33,10.91;23,112.33,249.56,394.95,10.91;23,112.31,263.11,235.39,10.91" xml:id="b57">
	<analytic>
		<title level="a" type="main" coord="23,112.28,208.91,296.40,10.91">A Human Generated MAchine Reading COmprehension Dataset</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Marco</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="23,437.99,208.91,67.99,10.91;23,112.66,222.46,393.32,10.91;23,112.66,236.01,393.33,10.91;23,112.33,249.56,49.52,10.91">Proceedings of the Workshop on Cognitive Computation: Integrating Neural and Symbolic Approaches Co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016)</title>
		<title level="s" coord="23,239.08,249.56,167.39,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting>the Workshop on Cognitive Computation: Integrating Neural and Symbolic Approaches Co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1773</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,276.66,393.32,10.91;23,112.66,290.20,393.33,10.91;23,112.66,303.75,393.33,10.91;23,112.66,317.30,394.53,10.91;23,112.66,330.85,280.63,10.91" xml:id="b58">
	<analytic>
		<title level="a" type="main" coord="23,212.89,276.66,293.09,10.91;23,112.66,290.20,111.43,10.91">A study of smoothing methods for language models applied to ad hoc information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<idno type="DOI">10.1145/383952.384019</idno>
	</analytic>
	<monogr>
		<title level="m" coord="23,480.14,290.20,25.85,10.91;23,112.66,303.75,393.33,10.91;23,112.66,317.30,183.68,10.91">SIGIR 2001: Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Harper</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Kraft</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Zobel</surname></persName>
		</editor>
		<meeting><address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">September 9-13, 2001. 2001</date>
			<biblScope unit="page" from="334" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,344.40,393.33,10.91;23,112.66,357.95,393.33,10.91;23,112.66,371.50,359.27,10.91" xml:id="b59">
	<analytic>
		<title level="a" type="main" coord="23,272.54,344.40,233.45,10.91;23,112.66,357.95,104.26,10.91">Exploring BERT Synonyms and Quality Prediction for Argument Retrieval</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Moroldo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Valente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,240.54,357.95,265.44,10.91;23,112.66,371.50,77.06,10.91">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="23,197.73,371.50,221.54,10.91">CEUR Workshop Proceedings, CLEF and CEUR-WS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,385.05,395.01,10.91;23,112.28,398.60,321.98,10.91" xml:id="b60">
	<analytic>
		<title level="a" type="main" coord="23,208.16,385.05,235.78,10.91">MARGOT: A Web Server for Argumentation Mining</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lippi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Torroni</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2016.08.050</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2016.08.050" />
	</analytic>
	<monogr>
		<title level="j" coord="23,453.29,385.05,54.38,10.91;23,112.28,398.60,24.19,10.91">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="292" to="303" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,412.15,395.17,10.91;23,112.66,425.70,349.80,10.91" xml:id="b61">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dandekar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Csernai</surname></persName>
		</author>
		<ptr target="https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs" />
		<title level="m" coord="23,261.73,412.15,197.46,10.91">First Quora Dataset Release: Question Pairs</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,439.25,393.33,10.91;23,112.66,452.79,393.33,10.91;23,112.66,466.34,394.53,10.91;23,112.66,479.89,286.67,10.91" xml:id="b62">
	<analytic>
		<title level="a" type="main" coord="23,326.25,439.25,179.74,10.91;23,112.66,452.79,144.14,10.91">Elastic ChatNoir: Search Engine for the ClueWeb and the Common Crawl</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-76941-7_83</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-76941-7_83" />
	</analytic>
	<monogr>
		<title level="m" coord="23,279.85,452.79,226.13,10.91;23,112.66,466.34,93.00,10.91">Proceedings of the 40th European Conference on IR Research (ECIR 2018)</title>
		<title level="s" coord="23,288.13,467.36,146.73,9.72">Lecture Notes in Computer Science</title>
		<meeting>the 40th European Conference on IR Research (ECIR 2018)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">10772</biblScope>
			<biblScope unit="page" from="820" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,493.44,393.33,10.91;23,112.30,506.99,395.53,10.91;23,112.66,520.54,266.73,10.91" xml:id="b63">
	<analytic>
		<title level="a" type="main" coord="23,296.39,493.44,209.60,10.91;23,112.30,506.99,53.74,10.91">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1301.3781" />
	</analytic>
	<monogr>
		<title level="m" coord="23,188.88,506.99,318.95,10.91;23,112.66,520.54,74.84,10.91">Proceedings of the 1st International Conference on Learning Representations (ICLR 2013)</title>
		<meeting>the 1st International Conference on Learning Representations (ICLR 2013)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,534.09,393.33,10.91;23,112.66,547.64,394.62,10.91;23,112.31,561.19,218.89,10.91" xml:id="b64">
	<monogr>
		<title level="m" type="main" coord="23,247.24,534.09,258.74,10.91;23,112.66,547.64,203.21,10.91">sense2vec -A Fast and Accurate Method for Word Sense Disambiguation in Neural Word Embeddings</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Trask</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Michalak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1511.06388" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,574.74,393.33,10.91;23,112.66,588.29,394.52,10.91;23,112.28,601.84,394.41,10.91;23,112.66,615.39,75.82,10.91" xml:id="b65">
	<analytic>
		<title level="a" type="main" coord="23,210.01,574.74,189.52,10.91">XGBoost: A Scalable Tree Boosting System</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939785</idno>
		<ptr target="https://doi.org/10.1145/2939672.2939785" />
	</analytic>
	<monogr>
		<title level="m" coord="23,423.10,574.74,82.88,10.91;23,112.66,588.29,389.61,10.91">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,628.93,393.60,10.91;23,112.66,642.48,393.33,10.91;23,112.66,656.03,340.97,10.91" xml:id="b66">
	<analytic>
		<title level="a" type="main" coord="23,411.67,628.93,94.59,10.91;23,112.66,642.48,183.68,10.91">LightGBM: A Highly Efficient Gradient Boosting Decision Tree</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,319.45,642.48,186.54,10.91;23,112.66,656.03,176.09,10.91">Proceedings of the Annual Conference on Neural Information Processing Systems</title>
		<meeting>the Annual Conference on Neural Information Processing Systems<address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,669.58,393.33,10.91;24,112.66,86.97,393.33,10.91;24,112.66,100.52,359.27,10.91" xml:id="b67">
	<analytic>
		<title level="a" type="main" coord="23,240.16,669.58,265.83,10.91;24,112.66,86.97,96.41,10.91">Argument Retrieval for Comparative Questions Based on Independent Features</title>
		<author>
			<persName coords=""><forename type="first">J.-N</forename><surname>Weder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K H</forename><surname>Luu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,235.74,86.97,270.25,10.91;24,112.66,100.52,77.06,10.91">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="24,197.73,100.52,221.54,10.91">CEUR Workshop Proceedings, CLEF and CEUR-WS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,114.06,393.33,10.91;24,112.66,127.61,393.33,10.91;24,112.66,141.16,387.80,10.91" xml:id="b68">
	<analytic>
		<title level="a" type="main" coord="24,238.95,114.06,267.04,10.91;24,112.66,127.61,143.60,10.91">Retrieving Comparative Arguments using Ensemble Methods and Neural Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chekalina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,278.77,127.61,227.21,10.91;24,112.66,141.16,105.59,10.91">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="24,226.26,141.16,221.55,10.91">CEUR Workshop Proceedings, CLEF and CEUR-WS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,154.71,394.62,10.91;24,112.41,168.26,68.35,10.91" xml:id="b69">
	<analytic>
		<title level="a" type="main" coord="24,166.88,154.71,68.41,10.91">Random Forests</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1010933404324</idno>
		<ptr target="https://doi.org/10.1023/A:1010933404324" />
	</analytic>
	<monogr>
		<title level="j" coord="24,243.40,154.71,56.97,10.91">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,181.81,393.33,10.91;24,112.66,195.36,394.53,10.91;24,112.28,208.91,394.41,10.91;24,112.66,222.46,75.82,10.91" xml:id="b70">
	<analytic>
		<title level="a" type="main" coord="24,181.27,181.81,247.66,10.91">OpenNIR: A Complete Neural Ad-Hoc Ranking Pipeline</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Macavaney</surname></persName>
		</author>
		<idno type="DOI">10.1145/3336191.3371864</idno>
		<ptr target="https://doi.org/10.1145/3336191.3371864" />
	</analytic>
	<monogr>
		<title level="m" coord="24,451.81,181.81,54.17,10.91;24,112.66,195.36,390.29,10.91">Proceedings of the 13th ACM International Conference on Web Search and Data Mining (WSDM 2020)</title>
		<meeting>the 13th ACM International Conference on Web Search and Data Mining (WSDM 2020)</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="845" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,236.01,393.33,10.91;24,112.28,249.56,393.70,10.91;24,112.33,263.11,395.33,10.91;24,112.66,276.66,228.79,10.91" xml:id="b71">
	<analytic>
		<title level="a" type="main" coord="24,348.40,236.01,157.59,10.91;24,112.28,249.56,98.39,10.91">ANTIQUE: A Non-factoid Question Answering Benchmark</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-45442-5_21</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-45442-5_21" />
	</analytic>
	<monogr>
		<title level="m" coord="24,234.33,249.56,271.65,10.91;24,112.33,263.11,48.62,10.91">Proceedings of the 42nd European Conference on IR Research (ECIR 2020)</title>
		<title level="s" coord="24,238.97,264.12,143.51,9.72">Lecture Notes in Computer Science</title>
		<meeting>the 42nd European Conference on IR Research (ECIR 2020)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12036</biblScope>
			<biblScope unit="page" from="166" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,290.20,395.17,10.91;24,112.66,303.75,394.52,10.91;24,112.66,317.30,393.32,10.91;24,112.14,330.85,244.73,10.91" xml:id="b72">
	<analytic>
		<title level="a" type="main" coord="24,351.61,290.20,156.21,10.91;24,112.66,303.75,389.88,10.91">Touch√© Task 2: Comparative Argument Retrieval. A Document-based Search Engine for Answering Comparative Questions</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Helmrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Streitmatter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Heykeroth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,127.43,317.30,343.43,10.91">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="24,478.92,317.30,27.06,10.91;24,112.14,330.85,192.08,10.91">CEUR Workshop Proceedings, CLEF and CEUR-WS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,344.40,393.33,10.91;24,112.66,357.95,258.64,10.91" xml:id="b73">
	<analytic>
		<title level="a" type="main" coord="24,407.45,344.40,98.53,10.91;24,112.66,357.95,145.68,10.91">Language Models are Unsupervised Multitask Learners</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="24,266.84,357.95,56.94,10.91">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,371.50,393.32,10.91;24,112.66,385.05,395.00,10.91;24,112.41,398.60,108.52,10.91" xml:id="b74">
	<analytic>
		<title level="a" type="main" coord="24,322.52,371.50,183.46,10.91;24,112.66,385.05,155.55,10.91">Efficient and Effective Spam Filtering and Re-ranking for Large Web Datasets</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10791-011-9162-z</idno>
		<ptr target="https://doi.org/10.1007/s10791-011-9162-z" />
	</analytic>
	<monogr>
		<title level="j" coord="24,276.65,385.05,38.50,10.91">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="441" to="465" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,412.15,395.17,10.91;24,112.66,425.70,395.17,10.91;24,112.66,439.25,393.33,10.91;24,112.66,452.79,90.78,10.91" xml:id="b75">
	<analytic>
		<title level="a" type="main" coord="24,383.67,412.15,124.16,10.91;24,112.66,425.70,238.39,10.91">DistilBERT-based Argumentation Retrieval for Answering Comparative Questions</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Alhamzeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bouhaouel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Egyed-Zsigmond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mitroviƒá</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,374.15,425.70,133.68,10.91;24,112.66,439.25,203.15,10.91">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="24,323.88,439.25,182.11,10.91;24,112.66,452.79,38.13,10.91">CEUR Workshop Proceedings, CLEF and CEUR-WS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,466.34,394.52,10.91;24,112.66,479.89,395.01,10.91;24,112.66,493.44,127.84,10.91" xml:id="b76">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1910.01108" />
		<title level="m" coord="24,293.71,466.34,213.47,10.91;24,112.66,479.89,121.16,10.91">DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,506.99,393.33,10.91;24,112.66,520.54,393.33,10.91;24,112.66,534.09,308.58,10.91" xml:id="b77">
	<analytic>
		<title level="a" type="main" coord="24,237.24,506.99,70.28,10.91;24,337.72,506.99,168.27,10.91;24,112.66,520.54,43.19,10.91">Argument Retrieval for Comparative Questions</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shirshakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Wattar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,183.05,520.54,322.94,10.91;24,112.66,534.09,26.38,10.91">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="24,147.04,534.09,221.55,10.91">CEUR Workshop Proceedings, CLEF and CEUR-WS</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note>Thor at Touch√©</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
