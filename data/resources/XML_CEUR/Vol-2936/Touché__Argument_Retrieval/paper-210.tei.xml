<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,95.14,84.74,404.99,15.42;1,212.37,106.66,170.54,15.42;1,137.11,129.00,324.97,11.96">Document retrieval task on controversial topic with Re-Ranking approach Notebook for the Touché Lab on Argument Retrieval at CLEF 2021</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,154.90,79.86,11.96"><forename type="first">Andrea</forename><surname>Cassetta</surname></persName>
							<email>andrea.cassetta@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,182.13,154.90,61.17,11.96"><forename type="first">Alberto</forename><surname>Piva</surname></persName>
							<email>alberto.piva.8@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,274.64,154.90,79.67,11.96"><forename type="first">Enrico</forename><surname>Vicentini</surname></persName>
							<email>enrico.vicentini.1@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Computer Engineering&quot;</orgName>
								<orgName type="department" key="dep2">Department of Information Engineering</orgName>
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,95.14,84.74,404.99,15.42;1,212.37,106.66,170.54,15.42;1,137.11,129.00,324.97,11.96">Document retrieval task on controversial topic with Re-Ranking approach Notebook for the Touché Lab on Argument Retrieval at CLEF 2021</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">E62A5AFC49EF4897DC88318D67078BEA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Argument Retrieval CLEF 2021 Touché Task 1</term>
					<term>WordNet synonyms</term>
					<term>Re-ranking</term>
					<term>BM25</term>
					<term>DirichletLM</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper is the report of the work done for Argument Retrieval CLEF 2021 Touché Task 1 by Shanks team (based in Italy and precisely the members are University's of Padua students). Argument Retrieval CLEF 2021 Touché Task 1 focuses on the problem of retrieving relevant arguments for a given controversial topic, from a focused crawl of online debate portals. After some tests Shanks group has decided to parse the input documents taking only the title, premises and conclusion of the arguments (as well as the stance necessary to understand the arguments' author point of view). After the indexing part of the documents the work is concentrate on how the retrieving and the raking are done. After some tests, we discover that the better results are obtained using a WordNet [1] based query expansion approach and a re-ranking process with two different similarity functions. This report describes in details how the documents parsing work and how the indexing and searching part are developed. The unexpected update of the qrels file did not allow us to re-run all the tests. In the end, however, we also reported the results of the runs obtained from parameter tuning on the new qrels.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this report, we describe the project developed for the participation by the Shanks group to the CLEF 2021 Touché Task 1. The task focuses on the problem of retrieving relevant arguments for a given controversial topic, from a focused crawl of online debate portals.</p><p>Our goal is to develop a Java based information retrieval system that finds and ranks the relevant documents from the args.me corpus dataset composed by over 380.000 arguments crawled from 5 different debate forums <ref type="bibr" coords="1,266.40,511.09,12.84,10.91" target="#b1">[2]</ref> for 50 topics (query). The retrieved results need to be relevant for each input topic the system has to elaborate. This paper is structured as follow: Section 3 is about the solutions that we've taken in consideration to build the retrieval system, Section 4 describes the whole workflow of the program; Section 5 explains our experimental setup including the software, tools and methods used; Section 6 discusses results; and finally, Section 9 draws some conclusions and outlooks for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>To create our search engine we build upon some source code created by Professor Nicola Ferro to show as some toy examples and changed as described in the subsequent sections. We have also read the overview of CLEF 2020 on the Touchè task <ref type="bibr" coords="2,337.15,199.55,12.57,10.91" target="#b2">[3]</ref>.</p><p>After some research and as suggested by Professor Ferro, we discover a different way of re-ranking and merging the results. In practice the paper Combination of Multiple Searches by Fox and Shaw <ref type="bibr" coords="2,155.04,240.20,11.43,10.91" target="#b3">[4]</ref>. Shaw, described an interesting method to increase the performance of our system by combining the similarity values from different output runs, using Boolean retrieval methods. Into the paper is also described how they have done the indexing (and analyzing) part, but we decide to overtake that part because the stating dataset is different, and we have already analyzed our dataset to reach better index possible. For the purpose to understand how the results merging has been done is not useful to analyze also how the query are written and so we decide only to relate that the P-norm queries are written using a complex boolean expression using AND and OR operators. When all the runs are done, the second part of the experiment consists in combining the output runs (obviously obtained from the same collection of data) to reach the best result. Different way of combining them are, for example, taking the top N documents retrieved for each run or modify the value of N for each run, based on the eleven point average precision for that run. In TREC-2, their experiments concentrated on methods of combining runs based on the similarity values of a document to each query for each of the runs. After some tests, the best choice is to weight each of the separate runs equally and not favor any individual run or method, but sometimes some runs has to be weighted more or less, depending on their performance. This method of merging the runs help the retrieval system to make a trade-off between the runs' errors. During the tests have been considered six different way to combine the runs:</p><p>• CombMIN: it is used to minimize the probability that a non-relevant document would be highly ranked;</p><p>• CombMAX: it is used to minimize the number of relevant documents being poorly ranked;</p><p>• CombMED: it is used to take the median similarity value (to solve the previous methods' problem) instead of taking a value from only one run;</p><p>• CombSUM: it is used to take the sum of the set of the similarity values;</p><p>• CombANZ: it is used to take the average of the non-zero similarity values, so it ignores the runs that fail to retrieve a relevant document;</p><p>• CombMNZ: it is used to consider the higher weights to documents retrieved by multiple retrieval methods.</p><p>We have to point out that the first two method have a specific objective but they do not care about the possible problems that they can generate on the other retrieved documents. During the tests CombMIN has worse performance than all the single runs, on contrary CombANZ and CombMNZ methods have better performance than the individual runs, it is possible maybe because they produce the same ranked sequence for all the documents retrieved by all five individual runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Initial Attempts</head><p>Before going into the details of our final solution, it is useful to describe previous approaches we took into account to solve the problem and why we chose not to explore them further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Parsing Documents</head><p>Multiple parsers have been developed to parse the documents from the provided collection. The most trivial parser, called 𝑃1, extracts the sourceText and discussionTitle elements from the corpus documents. The second parser 𝑃2 extracts the elements related to the conclusion, the premise, the discussion title, and all the text from the sourceText field in between the premise and the conclusion. The third parser 𝑃0, which at the end we decided to use for our more advanced experiments, extracts only the discussion title, the premise and the conclusion for each document. Table <ref type="table" coords="3,189.24,357.37,5.07,10.91" target="#tab_0">1</ref> shows how the index statistics are affected by each of these parser. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Query Expansion</head><p>When we have developed the software to create the index, we have deeply thought about how we could have used the resulting tokens from the analysis of the topic query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">OpenAI GPT-2</head><p>In the attempt of expanding the queries we came across the OpenAI GPT-2 model, a Machine Learning algorithm that generates synthetic text samples from an arbitrary input.</p><p>The idea was to use this powerful algorithm to make a query expansion, giving as input the topic title to generate a more complete phrase with hopefully new words that could help the searching part. Unfortunately the output of GPT-2 is not always what we expect. For example if we give it as input the tokenized query title, that could be only made of 2 words, the output is a not very useful dialog for our task. Another problem is the structure of the query, in fact since they are all questions, the GPT-2 algorithm generates an answer for them which still is not what we were interested in. The problem persists also if we remove the question mark at the end of the phrase. In fact, the queries still have a question structure. For these reasons, we have decided to set aside this kind of approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Randomly Weighted Synonyms</head><p>An approach initially devised for query expansion, but which we later decided not to explore further, was to generate multiple queries for the same topic, each with randomly generated synonym boost values. For each query, the rankings of 1000 documents were then generated, and finally all the rankings were merged into one. The first performances obtained by this method did not encourage us to proceed with the development because there were many possible paths to follow from that point and the search time increased considerably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Minimum body length</head><p>During the process of documents exploration, useful to detect which field are needed and present in each collection, we notice that in some documents the sourceText field were constituted by useless text without a single relevant information about its topic. In order to avoid this kind of documents ending up in the inverted file, we have tried to include, during the indexing process, a check on the length of the ParsedDocument's body. If this field, the one that we have considered as the union of the conclusion and premise fields, is made up of less than a certain number of token, recurrent aspect that the parsing phase highlights for such instance, we avoid to consider them in the indexing phase. After doing some tests with different values for the "min body length" (5, 10, 15), we have compared the results of this kind of solution with the results obtained without using it and we have discovered that, taking as example "min body length" equal to 10, the number of retrieved document switches from 48781 to 48764 and the number of relevant document switches from 1263 to 1257 (the other evaluation measures are no so affected by this change). Considering this result we have decided not to use this kind of document pre-processing to avoid the discarding of some document that could be considered relevant in the qrels file (and so for an user) even if they don't seem like it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Re-ranking with discussion ID</head><p>One of our primary goals is to improve the final ranking of the documents. With this assumption we tried to improve performance by using re-ranking.</p><p>As a first analysis we observed that, in the documents of the dataset, the posts related to the same discussion had the same first part of the document ID. Based on the assumption that only posts from some discussion are relevant to a query, we tried to index those posts as a single document. We finally obtained an index with a discussion-based clustering of documents. In the searching phase, we firstly searched the query in the normal index, retrieving the classic ranking of single documents. Secondly we searched the same query in the second index of discussion clusters, thus obtaining a ranking of discussions. Finally the scores of documents in the first ranking were increased based on the rank of their respective discussion in the second ranking. Unfortunately this approach did not provide the desired results because it assumes that all posts related to a discussion have the same relevance to the searched topic. In fact, it was found that some posts do not contain any useful information to argue the searched topic but are part of a discussion that is really relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">OpenNLP attempt</head><p>Exploring new solutions, we have even tried to implement a version of the program that uses the OpenNLP Machine Learning toolkit in order to see which advantages a tokenization able to distinguish location, personal nouns and so on could provide for the solution. Following this path we have encountered an error which requires significant changes in the workflow of what we had done up to that point. For this reason we have decided not to keep going on with this branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head><p>The goal of this task is to retrieve relevant arguments from online debate portals, given a query on a controversial topic. We have checked the dataset and we have noticed that it is composed by five JSON files. We have also read some documents and we have noticed that the main structure is the same for each one but with some different fields. To use the documents with Lucene we had to parse the documents. To do so, we have used the Jackson library and we have implemented our parser 𝑃0 that takes the premises, conclusions, the document title and the stance attribute (pro or cons) of the documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Indexing</head><p>We have built four different parts starting from the Lucene default ones: ArgsParser, Shanks-Analyzer, DirectoryIndexer and the Searcher. There is indeed a ShanksTouche class which has the main method and allows us to setup parameters for indexing and analyzing parts. After converting the documents into something that Lucene can work on, we focused ourselves on the development of how the indexer is created, in particular on how the analyzer module works. Into the tokenization phase we have used the StandardTokenizer, the LowerCaseFilter, the EnglishPossessiveFilter and the stop-words StopFilter.</p><p>The arguments of the collection are stored in the index using four fields: ID, Title, Body, and Stance (pro or con).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Custom Stop-List</head><p>As you can see in Figure <ref type="figure" coords="5,202.19,606.01,5.07,10.91" target="#fig_3">4</ref> we have compared the baseline with the default stop-list and with our custom stop-list; after that comparison we have decided to use a custom stop-list to better achieve the project goal. Our stop-list contains 1362 words that are derived from the merging of other stop-lists (e.g. smart and lucene) typically used. Our custom stop-list reduces memory usage by approximately 38% and indexing time by almost 20%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Searching</head><p>In the searching phase of our program we focus our attention in finding strategies to improve the general quality of the results: experimenting with different approaches like query expansion based on WordNet synonyms or defining queries to score differently the fields of the documents. The approach we have chosen to perform involves the use of BooleanQuery. In this way it is possible to assign specific weights to every term of the query (boosts).</p><p>Finally, we decided to use and test both BM25Similarity and LMDirichletSimilarity similarities and then their combination with a MultiSimilarity to get the document scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Re-Ranking</head><p>During various experiments we have noticed that some measures of similarity and set of parameters favored the precision at the expense of recall and vice versa. With the purpose of combining advantages of both cases, we opted for a re-reranking method which exploits different similarities and query parameters. Our implementation is made of two steps. In the first one we use a query able to obtain a higher recall value when searching the index for relevant documents, whose parameters and similarity are decided based on empirical trials. In the following step we use a second query with better performance in terms of 𝑛𝑑𝑐𝑔 to re-evaluate the returned documents and so re-ranking them according to the new score. We call maxRecall the first query and maxNdcg the second one. According to our implementation, this approach turns out to be effective on the 2020 topic set, but at the expense of the time spent in the search phase, which increases quite substantially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Setup</head><p>Our work is based on the following experimental setups:</p><p>• Repository: https://bitbucket.org/upd-dei-stud-prj/seupd2021-goldr;</p><p>• During the develop and the experimentation we have used our own computer and in the end we have run our code using Tira;</p><p>• Evaluation measure: BM25, LMDirichlet and a "MULTI" where both were combined;</p><p>• Apache Maven, Lucene;</p><p>• Java JDK version 11;</p><p>• Version control system git.</p><formula xml:id="formula_0" coords="6,107.28,603.83,89.42,10.91">• 𝑡𝑟𝑒𝑐_𝑒𝑣𝑎𝑙 tool [5]</formula><p>The collection is a set of 387,740 arguments crawled from debatewise.org, idebate.org, debatepedia.org, debate.org and 48 arguments from Canadian parliament discussions. We used the 50 topics from Touché 2020 Task 1 <ref type="bibr" coords="6,252.10,653.45,12.84,10.91" target="#b5">[6]</ref> of the contest to train and refine our search engine. Furthermore we developed the source code collaborating through the BitBucket platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results and Discussion</head><p>In this section we provide graphical and numerical results about the experiments we conducted during the development of the project. We also discuss these results to derive some useful insights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Our Baseline</head><p>In order to be able to track performance progress during development, we created our own baseline. For each of the three parsers 𝑃0, 𝑃1, 𝑃2 we produced a run using a simple analyzer that uses a StandardTokenizer, LowerCaseFilter and a StopFilter where the stop-list used is the standard one offered by Lucene. The similarity used is BM25. Figure <ref type="figure" coords="7,388.98,228.75,5.07,10.91" target="#fig_1">1</ref> compares the three baselines. From these results we decided to develop our approach based on the 𝑃0 parser. Figures <ref type="figure" coords="7,89.29,255.85,5.07,10.91">2</ref> and<ref type="figure" coords="7,118.22,255.85,5.07,10.91" target="#fig_2">3</ref> show how 𝑃0 compares to the other parser evaluating performance per topic. The choice of 𝑃0 was motivated not only by the statistics in Table <ref type="table" coords="7,365.84,269.40,5.07,10.91" target="#tab_0">1</ref> but also by its overall efficacy as shown in the following figures. Figure <ref type="figure" coords="7,280.23,282.95,5.07,10.91" target="#fig_1">1</ref> shows how differently the three approaches can behave. 𝑃0, that extracts only the discussion title, the premise and the conclusion for each document (with its stance), highlights better performance with respect to the other two retrieving more relevant document across the entire run. Obviously we chose to discard 𝑃1 because his performances were inferior to the other two as shown in Figure <ref type="figure" coords="7,381.81,337.15,3.74,10.91">2</ref>. To better understand the behavior of 𝑃2 versus to 𝑃0, Figure <ref type="figure" coords="7,250.21,350.70,5.07,10.91" target="#fig_2">3</ref>     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1.">Baseline with the custom stop-list</head><p>The results obtained by our custom stop list, compared to the one offered by Lucene are comparable. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Parameters Tuning</head><p>Our ultimate goal is to maximize the average value of 𝑛𝐷𝐺𝐶@5 across all topics. To find the optimal parameter values, we performed extensive iterative tests, trying all combinations of parameters with values belonging to discrete intervals we defined. The same experiments were conducted using three different similarities : BM25Similarity (BM25), LMDirichletSimilarity (LMD), MultiSimilarity (MULTI). The MultiSimilarity we have used, combines BM25Similarity and LMDirichletSimilarity. The method we developed is governed by five parameters, when using re-ranking they become ten. Out of those ten, five parameters concern the query search in the index which aims to maximize the overall recall, the other five affect the re-ranking process and are chosen to maximize 𝑛𝐷𝐺𝐶@5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Optimal parameters</head><p>The optimal set of parameters we found for the 𝑚𝑎𝑥𝑅𝑒𝑐𝑎𝑙𝑙 query and for the 𝑚𝑎𝑥𝑛𝐷𝐶𝐺 query are available in Tables <ref type="table" coords="9,187.68,497.96,5.07,10.91">2</ref> and<ref type="table" coords="9,213.16,497.96,3.74,10.91">3</ref>. The best similarity measure to maximize 𝑛𝐷𝐺𝐶@5, according to our empirical tests is LMD. To maximize recall, on the other hand, the best measure is MULTI. As it can be seen in Table <ref type="table" coords="9,202.39,525.06,3.74,10.91">2</ref>, the best value of tBoost for maxnDCG is 0. This allowed us to come to the conclusion that considering the title of the discussion (which is the same for all posts in it) can be misleading with respect to the relevance of the content of each post that is part of it. The title, however, is very useful to obtain higher recall. This intuition is what pushed us to abandon the method of re-ranking based on discussion ID, which is described in 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>The optimal parameters obtained by training on the 2020 topics (topic description not considered).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head><p>Similarity tBoost sBoost pBoost pDist maxRecall MULTI 3,50 0,15 1,75 12 maxnDCG LMD 0,00 0,05 0,75 15</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>The optimal parameters obtained by training on the 2020 topics (considering the topic description).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head><p>Similarity tBoost sBoost pBoost pDist maxRecall MULTI 3,50 0,15 1,75 12 maxnDCG LMD 0,30 0,05 1,75 15</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">maxnDCG and maxRecall</head><p>In this section we want to compare the two queries maxnDCG and maxRecall with the optimal parameter values established by the tests. From the graph in Figure <ref type="figure" coords="10,391.75,207.30,5.07,10.91" target="#fig_4">5</ref> we can see that the precision is significantly higher for maxnDCG, while maxRecall has a better recall on the whole ranking. From these data we believe to have obtained the desired result from the two queries.</p><p>In Figures <ref type="figure" coords="10,137.17,247.95,7.47,10.91">6,</ref><ref type="figure" coords="10,147.79,247.95,3.74,10.91">7</ref>, 8 we compare the Average Precision per topic for the three test runs. We can notice how there is not much difference between maxRecall and P0. The same is not true for maxnDCG, the latter proves to be better than P0 in almost all topics. This scenario is further confirmed by Figure <ref type="figure" coords="10,183.07,288.60,3.74,10.91" target="#fig_5">8</ref>, which shows the dominance of 𝐴𝑃 𝑚𝑎𝑥𝑛𝐷𝐶𝐺 over 𝐴𝑃 𝑚𝑎𝑥𝑅𝑒𝑐𝑎𝑙𝑙 . The performance can be further compared with the numerical results reported in Table <ref type="table" coords="10,445.72,302.15,3.74,10.91" target="#tab_4">4</ref>. From these data we can realize the actual trade-off between the two approaches. The advantage in terms of recall for maxRecall is significant compared to maxnDCG. The same advantage applies in the opposite way for precision andnDCG.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Re-Ranking Results</head><p>Here we compare the Re-Ranking approach described in 4.3, with the previous ones. Figure <ref type="figure" coords="11,500.91,642.48,5.07,10.91" target="#fig_6">9</ref> shows how re-ranking based on maxNdcg (in red), greatly improves the interpolated precision of maxRecall (in green). It is also possible to note that it allows for better performance with respect to maxNdcg alone (in orange). Figure <ref type="figure" coords="12,295.21,86.97,10.15,10.91" target="#fig_1">10</ref> shows the Average Precision value obtained by re-ranking for each topic. From it, it is possible to identify the most problematic topics, for which the method developed is not very effective. In particular, the most critical topics are: {2, 8, 22, 40, 44}. From Figure <ref type="figure" coords="12,222.32,127.61,10.15,10.91" target="#fig_1">11</ref> we can see that the Re-Ranking method improves on almost every topic w.r.t the baseline. In Figure <ref type="figure" coords="12,263.61,141.16,10.15,10.91" target="#fig_1">12</ref> it can be seen that the Re-Ranking method improves on maxNdcg on many topics, except for topic 10 and 20. Figure <ref type="figure" coords="12,365.85,154.71,8.36,10.91" target="#fig_7">13</ref>, as predictable, clearly shows the better performance achieved by Re-Ranking w.r.t. maxRecall, only the first topic is penalized by the re-ranking process.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.">RUN Submission</head><p>The five run we decided to submit are the following:</p><p>• run-1 : Re-Ranking approach.</p><p>• run-2 : like run-1 but proximity searches areonly with pairs of subsequent tokens.</p><p>• run-3 : maxnDCG query with LMDirichletSimilarity</p><p>• run-4 : maxnDCG query with MultiSimilarity</p><p>• run-5 : maxRecall query with MultiSimilarity Table <ref type="table" coords="14,126.99,358.46,5.07,10.91">5</ref> shows the numerical results we obtained for each run. Out of all, the first run is the best one overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 5</head><p>Numerical statistics from the trec-evaluations of the 5 run on the 2020 topic set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RUN</head><p>num_rel_ret map P_5 recall_1000 nDCG nDCG@5 shanks-run- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NEW EXPERIMENTAL RESULTS AFTER A NEW CORRECTED VERSION OF THE QRELS WAS RELEASED</head><p>All the previous results are based on an incorrect version of the .qrels file for the 2020 topics. Since we only knew about the new corrected version when the deadline was approaching, we could not recreate all the graphs and comparisons in 6. However, we managed to find the new optimal parameter sets and repeat the five run. The new data is provided in Tables <ref type="table" coords="14,258.21,641.94,5.07,10.91">6</ref> and<ref type="table" coords="14,285.15,641.94,3.74,10.91">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 6</head><p>The optimal parameters obtained by training on the 2020 topics (WITH CORRECTED QRELS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head><p>Similarity tBoost sBoost pBoost pDist maxRecall MULTI 0,3 0,2 0,75 12 maxnDCG LMD 0,15 0,05 0,75 17</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 7</head><p>Numerical statistics from the 5 run on the 2020 topic set (WITH CORRECTED QRELS). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RUN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Statistical Analysis</head><p>Here we analyze our models with some important statistics to evaluate them in a deeper way via hypothesis testing. We used ANOVA, tStudent test and through boxplots. All the analyses focused on the mean of two key metrics, average precision and nDCG@5. We analyzed all 5 different retrieval models, the ones described in the section 6.6.</p><p>A boxplot gives a visual representation about location, symmetry of the data, dispersion and presence of outliers (points that escape the construction of the boxplot). It can be appreciate that run1,run3 have almost an identical structure: the median, interquartile range(IRQ) and whiskers length. On the other hand, run4 and run5 exhibit less performance in this metric because those runs were tuned for maximize recall or nDCG@5, the bottom whiskers in fact are closer to zero meaning that for some topics the system did not retrieve enough relevant documents. All the runs have outliers, the points above the whiskers, represented by circles, are topics which perform better than the others, or worse if they are below the boxplot. Run1 and run3 are skewed to the right and show less variance compared to other systems.</p><p>Looking at the boxlot of nDCG@5 reveals the same behavior seen before, run1 and run3 produce higher scores compared to other runs and they seem identical in performances. Run3 has better results w.r.t run4 so we can conclude that using different similarities change dramatically the results. Run2 shows less IRQ among others, in particular compared to run1 that share the same architecture with the only difference in the proximity parameter. We can say that run1 is able to score higher score exploiting the more flexible proximity parameter.</p><p>Further analysis with anova and tTest will help to understand the possible similarities or not between the systems. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Hypothesis testing</head><p>The first tool that we utilize is ANOVA (Analysis of Variance) a statistical test of whether or not the means of several groups are equal. H 0 , the null hypothesis that all the means are equal, is tested against the possibility to reject or don't reject it. As we can see in Figure <ref type="figure" coords="17,452.55,388.27,10.15,10.91" target="#fig_1">16</ref> there are multiple factors to be taken into account to perform a correct analysis of the F-statistic. The system sum of squares, SS_system and the SS_error are divided by their degree of freedom to obtain the mean squares(MS). The F-statistic (F) is equal to the ratio of MS_System an MS_Error. Having a p-value = 0.1849 we cannot reject H 0 . To do that we should have had a value lower than 𝛼. The significance level 𝛼 is set at 0.05.</p><p>We could conclude that our systems are statistically similar in mean average precision but performing the ANOVA2 test the situation is reversed. As it can be seen from Figure <ref type="figure" coords="18,233.86,425.70,10.15,10.91" target="#fig_5">18</ref> and Figure <ref type="figure" coords="18,297.25,425.70,8.36,10.91" target="#fig_6">19</ref>, the null hypothesis can be rejected because the runs have statistically significant differences. The topic effect, that can be read in cell <ref type="bibr" coords="18,485.67,439.25,12.19,10.91" target="#b2">[3,</ref><ref type="bibr" coords="18,497.86,439.25,8.13,10.91" target="#b1">2]</ref> of Figure <ref type="figure" coords="18,132.28,452.79,8.36,10.91" target="#fig_1">17</ref>, is able to express much more variance, that means that this variability is greater than the one of the systems as we can expect by topics.</p><p>Run 3 vs run 4: as we pointed out previously, the use of LMDirichlet similarity improve the results. For this pair the null hypothesis can be rejected. As we will see for the nDCG@5 analysis run 4 and run 5 are different from the others, and the p-value of their statistic suggest us that the runs are equivalent in mean. Instead, for the others (run 1, run 3, run 2) we cannot reject H 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unfortunately we have not been able to simplify the reading of the images. Here is a sort of conversion between the numbers on the axes and the real ones. 1 -2 -3 -4 -5 (image numbers) -&gt; run1 -run3 -run2 -run5 -run4 (true values). This is applicable to Figure 18, 19, 21, 23</head><p>Moving the study to nDCG@5, in ANOVA table, Figure <ref type="figure" coords="18,358.03,628.93,8.36,10.91">20</ref>, the p-value is lower than the confidence level so we can reject the null hypothesis and claim that there is at least one mean between the various runs that differs significantly from the others. This table doesn't tell us which systems are different on average, it just tells us that there is at least one.</p><p>Tukey Honestly Significant Difference (HSD) test, as we did in AP case, answer to that point creating confidence intervals for all pairwise differences between the systems we want to compare, while controlling the family error rate. Otherwise, the probability of a type I(one) error would be magnified.</p><p>Run4 and run5, Figure <ref type="figure" coords="19,198.36,141.16,8.36,10.91">22</ref>, are statistically different from the others, while for them we cannot reject the null hypothesis. It seems that supporting the MaxRecall or MaxnDCG query approach does not yield performance benefits except through different similary as in case of run3 or rerank(run1 and run2). Accordingly to what the boxplot chart suggested for run3 and run4 we can reject H 0 , the p-value is below 𝛼 = 0.05. The different similarity approach is visible.</p><p>We can conclude that our Re-ranking approach is much more significant than a standard technique even if run3 through this analysis it returns comparable results. In the future this behavior can be investigated carefully. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Failure analysis</head><p>Looking at the results obtained from the trec_eval program execution with our runs as a parameter we can see the performance of our information retrieval systems. In particular we are now focusing on finding and understanding for which topic the systems fail in achieving good performance. Therefore we have decided to apply this kind of evaluation to our best run (shanks-run 1). Looking at it, and using the map field as reference parameter for the following analysis, we have discovered that the top (map) performance come from the topics 42, 43 and 1 and the worst from 22, 12 and 44. Searching for the reason why, we understand that the main weakness of the process is the lacking of an argument quality evaluation phase and of a more consistent lexical analysis process.</p><p>As an example we take and compare the topics 1, 12 and 44.</p><p>Topic 1: Should teachers get tenure? Narrative: highly relevant arguments make a clear statement about tenure for teachers in schools or universities. Relevant arguments consider tenure more generally, not specifically for teachers, or, instead of talking about tenure, consider the situation of teachers' financial independence.</p><p>The process of stopwording applied to the topics lead to obtain the parsed query "teachers tenure", that even without the whole phrase construction explain very well what we are searching for. The document about this topic are well retrieved by our system, in fact if we look for example at the first 5 position (the most relevant ones for a browser) we can see from the comparison with the qrels that all of them are considered as "highly relevant".</p><p>Topic 12: Should birth control pills be available over the counter? Narrative: highly relevant arguments argue for or against the availability of birth control pills without prescription. Relevant arguments argue with regard to birth control pills and their side effects only. Arguments only arguing for or against birth control are irrelevant.</p><p>The process of stopwording applied to the topics lead to obtain the parsed query "birth control pills available". This seems a quite explicative phrase but in the retrieval phase the system has trouble in finding the proper relevant results. The critical issue with this query is the distinction between high and low quality argument. The lacking of an effective argument quality process leads the program in failing the document quality evaluation. Due to this fact the system is unable to put in the right ranking position the appropriate document. We can notice this aspect even looking at the other topic fields (not only the map), in fact if we look for example at the growing of the recall parameter we can notice that is mainly focused in the tail of the process (when many documents are retrieved).</p><p>Topic 44: Should election day be a national holiday? Narrative: highly relevant arguments explain why making election day a holiday is a good idea, or why not. Relevant arguments mention the fact or its remedy as one of the problems that elections have.</p><p>The process of stopwording applied to the topics lead to obtain the parsed query "election national holiday". The results obtained from this kind of search are quite bad in fact the system retrieves as relevant some discussion like "Potato day should be a national holiday" or "Star Wars day should be a national holiday". These mismatches came from the fact that the system does not recognize "election day" as unique mandatory query term and so document about similar topic that differ only for some word are wrongly retrieved.</p><p>To overcome these kind of issues it could be useful to equip the system with an argument quality evaluator and a sort of word pattern recognizer that catches the words which must not be separate and a way to identify structure like "subject, predicate, object" that assign higher weights to the subject and marks it as mandatory word in the documents to be retrieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusions and Future Work</head><p>At the end of this experiment we have discovered that the changes we made, lead to obtain fairly good improvement in respect to the starting baseline [Table <ref type="table" coords="22,367.24,124.83,4.48,10.91" target="#tab_4">4</ref>] of our information retrieval system. All the statistics have undergone a significant increase, as an example we can see the improvement as follows: MAP +39.3%, P5 +68.8%, nDCG@5 +91.4%.</p><p>Comparing our results with the last year ones we have noticed that they follows their trend for the nDCG@5 parameter, but looking at the applied strategies they seem to be fairly different. Our results are in line with those of last year.</p><p>The whole process we followed highlighted a lot of possible expansions that with more time could be implemented and explored. As an example it could be interesting the application of some sort of location word detection, personal nouns recognition and compound words discernment and an improvement or even the addition of a new ranking phase that allows the insertion of some kind of argument quality analysis. The possibility of implementing a different approach based on some kind of machine learning model remains open. As we stated in the initial attempt section 3 we dropped the idea of using GPT-2 because it returned us unsatisfying results, so in the future we can go more into details of this modern tool and improve in this way the performance of our solution.</p><p>Another possible approach that can be exploited derives from the paper <ref type="bibr" coords="22,421.91,328.07,11.43,10.91" target="#b3">[4]</ref>. It can be interesting because it takes different runs in input and combines them to reach the best result.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,258.35,350.70,247.64,10.91;7,89.29,364.24,416.69,10.91;7,89.29,378.97,13.95,9.51"><head></head><label></label><figDesc>compare the per topic average precision. Approximately 10% of the topic, in particular topic 3, 23, 34 ad 43 to 46 tend to give nicer results with parser 𝑃2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,89.29,634.44,327.08,8.93;7,183.01,399.05,226.77,222.84"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Plot of the Interpolated precision against recall of the three baselines.</figDesc><graphic coords="7,183.01,399.05,226.77,222.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,89.29,407.89,262.32,10.24"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Per topic Average Precision Difference = 𝐴𝑃 𝑃0 -𝐴𝑃 𝑃2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,89.29,257.89,416.69,8.93;9,89.29,269.90,169.13,8.87;9,211.35,84.19,170.08,167.13"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Plot of the Interpolated precision against recall of the two P0 baselines obtained by the Lucene stop-list and the custom stop-list.</figDesc><graphic coords="9,211.35,84.19,170.08,167.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="10,89.29,601.89,409.41,8.96;10,183.01,366.50,226.77,222.84"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Plot of the Interpolated precision against recall for the maxnDCG query versus maxRecall.</figDesc><graphic coords="10,183.01,366.50,226.77,222.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="11,89.29,586.27,317.79,10.52"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Per topic Average Precision Difference = 𝐴𝑃 𝑚𝑎𝑥𝑛𝐷𝐶𝐺 -𝐴𝑃 𝑚𝑎𝑥𝑅𝑒𝑐𝑎𝑙𝑙 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="12,89.29,434.92,416.69,8.96;12,89.29,446.87,45.71,8.96;12,183.01,205.52,226.77,222.84"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Plot of the Interpolated precision against recall comparing baseline, maxRecall, maxnDCG, Re-Ranking</figDesc><graphic coords="12,183.01,205.52,226.77,222.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="13,89.29,588.90,325.52,10.52"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Per topic Average Precision Difference = 𝐴𝑃 𝑟𝑒-𝑟𝑎𝑛𝑘𝑖𝑛𝑔 -𝐴𝑃 𝑚𝑎𝑥𝑅𝑒𝑐𝑎𝑙𝑙 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="16,89.29,304.73,219.20,8.93;16,166.87,84.19,259.06,207.98"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Boxplots of the 5 runs of Aveage Precision</figDesc><graphic coords="16,166.87,84.19,259.06,207.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="17,89.29,304.73,190.30,8.93;17,166.87,84.19,259.06,207.98"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Boxplots of the 5 runs of nDCG@5</figDesc><graphic coords="17,166.87,84.19,259.06,207.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="17,89.29,597.67,107.61,8.93;17,136.69,506.82,319.41,78.30"><head>Figure 16 :Figure 19 :</head><label>1619</label><figDesc>Figure 16: Anova1 results</figDesc><graphic coords="17,136.69,506.82,319.41,78.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="19,89.29,347.75,155.79,8.93;19,125.29,259.72,342.21,75.48"><head>Figure 20 : 5 Figure 23 :Figure 24 :</head><label>2052324</label><figDesc>Figure 20: nDCG@5 -Anova1 results</figDesc><graphic coords="19,125.29,259.72,342.21,75.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="20,170.39,84.19,252.00,211.68"><head></head><label></label><figDesc></figDesc><graphic coords="20,170.39,84.19,252.00,211.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,89.29,386.81,416.70,96.37"><head>Table 1</head><label>1</label><figDesc>Statistics</figDesc><table coords="3,145.00,438.36,305.28,44.82"><row><cell cols="5">Parser Term Count Storage (MB) Time (seconds) Time ratio</cell></row><row><cell>P0</cell><cell>1078017</cell><cell>195</cell><cell>99</cell><cell>1,00</cell></row><row><cell>P1</cell><cell>1196289</cell><cell>1745</cell><cell>507</cell><cell>5,12</cell></row><row><cell>P2</cell><cell>1153517</cell><cell>706</cell><cell>249</cell><cell>2,52</cell></row></table><note coords="3,129.37,398.81,376.62,8.87;3,89.29,410.77,416.70,8.87;3,89.29,422.72,110.70,8.87"><p>regarding three indexes generated using the three implemented document parser. The analyzer is always the same. Time Ratio is obtained by dividing the time taken by each parser by the time taken by the fastest parser.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,89.17,84.06,399.03,296.97"><head></head><label></label><figDesc>Per topic Average Precision Difference = 𝐴𝑃 𝑃0 -𝐴𝑃 𝑃1 .</figDesc><table coords="8,89.17,84.06,399.03,296.97"><row><cell></cell><cell>0.15</cell></row><row><cell>Average Precision difference</cell><cell>0.05 0.00 0.05 0.10</cell></row><row><cell></cell><cell>0.10</cell></row><row><cell></cell><cell>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 Topic</cell><cell>43 44 45 46 47 48 49 50</cell></row><row><cell cols="2">2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 Topic Figure 2: 1 0.075 0.100 0.050 0.025 0.000 0.025 0.050 0.075 Average Precision difference</cell><cell>44 45 46 47 48 49 50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,89.17,84.06,399.03,475.33"><head></head><label></label><figDesc>Per topic Average Precision Difference = 𝐴𝑃 𝑚𝑎𝑥𝑅𝑒𝑐𝑎𝑙𝑙 -𝐴𝑃 𝑃0 . Per topic Average Precision Difference = 𝐴𝑃 𝑚𝑎𝑥𝑛𝐷𝐶𝐺 -𝐴𝑃 𝑃0 .</figDesc><table coords="11,89.17,84.06,399.03,475.33"><row><cell></cell><cell>0.100</cell></row><row><cell>Average Precision difference</cell><cell>0.050 0.025 0.000 0.025 0.050 0.075</cell></row><row><cell></cell><cell>0.075</cell></row><row><cell></cell><cell>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 Topic</cell></row><row><cell cols="2">2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 Topic 50 Figure 6: 1 0.05 0.25 0.00 0.05 0.10 0.15 0.20 Average Precision difference</cell></row><row><cell cols="2">2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 Topic 50 Figure 7: 1 0.10 0.15 0.05 0.00 0.05 0.10 Average Precision difference</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="12,89.17,486.90,398.74,151.45"><head></head><label></label><figDesc>Per topic Average Precision for the Re-Ranking approach. Per topic Average Precision Difference = 𝐴𝑃 𝑟𝑒-𝑟𝑎𝑛𝑘𝑖𝑛𝑔 -𝐴𝑃 𝑃0 . Per topic Average Precision Difference = 𝐴𝑃 𝑟𝑒-𝑟𝑎𝑛𝑘𝑖𝑛𝑔 -𝐴𝑃 𝑚𝑎𝑥𝑛𝐷𝐶𝐺 .</figDesc><table coords="12,89.17,486.90,398.74,151.45"><row><cell></cell><cell>0.25</cell></row><row><cell>Average Precision difference</cell><cell>0.00 0.05 0.10 0.15 0.20</cell></row><row><cell></cell><cell>0.05</cell></row><row><cell cols="2">1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 Topic 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 50 Topic Figure 11: 1 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Average Precision 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 Topic 0.015 0.010 0.005 0.000 0.005 0.010 0.015 Average Precision difference Figure 10: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 Topic 50 0.10 0.15 Figure 12: 1 0.05 0.00 0.05 0.10 Average Precision difference</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="14,89.29,90.49,416.69,90.17"><head>Table 4</head><label>4</label><figDesc>Some numerical results for comparing performance of Re-Ranking, maxnDCG, maxRecall and the baseline.</figDesc><table coords="14,97.19,127.74,400.91,52.91"><row><cell>RUN</cell><cell>num_rel_ret map</cell><cell>P_5</cell><cell cols="2">recall_1000 nDCG nDCG@5</cell></row><row><cell>P0-RERANK</cell><cell cols="2">1263 0.1750 0.6490</cell><cell>0.6631 0.4979</cell><cell>0.5495</cell></row><row><cell>P0-LMD-MAX_nDCG@5</cell><cell cols="2">1103 0.1717 0.6490</cell><cell>0.5803 0.4764</cell><cell>0.5495</cell></row><row><cell>P0-MULTI-MAX_RECALL_1000</cell><cell cols="2">1263 0.1276 0.4082</cell><cell>0.6631 0.4283</cell><cell>0.3117</cell></row><row><cell>P0-BM25-BASELINE</cell><cell cols="2">1250 0.1256 0.3796</cell><cell>0.6563 0.4216</cell><cell>0.2871</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="23,107.59,111.28,345.11,10.91" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="23,165.59,111.28,182.39,10.91">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Bradford Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,107.59,124.83,398.40,10.91;23,107.59,138.38,352.69,10.91" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.3734893</idno>
		<ptr target="https://doi.org/10.5281/zenodo.3734893.doi:10.5281/zenodo.3734893" />
		<title level="m" coord="23,410.43,124.83,65.28,10.91">args.me corpus</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,107.59,151.93,398.40,10.91;23,107.59,165.48,398.40,10.91;23,107.59,179.03,146.65,10.91" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="23,336.01,165.48,169.97,10.91;23,107.59,179.03,38.01,10.91">Overview of Touché 2020: Argument Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fröbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beloucif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gienapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<idno>CEUR-WS 2696</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,107.59,192.57,398.40,10.91;23,107.59,206.12,398.40,10.91;23,107.59,219.67,325.78,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="23,196.43,192.57,151.42,10.91">Combination of Multiple Searches</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,466.77,192.57,39.22,10.91;23,107.59,206.12,178.93,10.91">The Second Text REtrieval Conference (TREC-2)</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting><address><addrLine>Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Special Publication</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="243" to="252" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST)</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="23,107.59,233.22,398.40,10.91;23,107.59,246.77,398.40,10.91;23,107.59,260.32,398.40,10.91;23,107.59,273.87,146.29,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="23,287.30,233.22,172.53,10.91">Overview of the TREC 2009 Web Track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,255.60,246.77,250.39,10.91;23,107.59,260.32,54.17,10.91">The Eighteenth Text REtrieval Conference Proceedings (TREC 2009)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<meeting><address><addrLine>Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Special Publication</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="500" to="278" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST)</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="23,107.59,287.42,398.40,10.91;23,107.59,300.97,398.40,10.91;23,107.59,314.52,398.40,10.91;23,107.59,328.07,398.40,10.91;23,107.59,341.62,129.98,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="23,336.01,300.97,169.97,10.91;23,107.59,314.52,38.01,10.91">Overview of Touché 2020: Argument Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fröbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beloucif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gienapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/" />
	</analytic>
	<monogr>
		<title level="m" coord="23,405.87,314.52,100.11,10.91;23,107.59,328.07,146.03,10.91">Working Notes Papers of the CLEF 2020 Evaluation Labs</title>
		<title level="s" coord="23,330.26,329.08,121.71,9.72">CEUR Workshop Proceedings</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2696</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
