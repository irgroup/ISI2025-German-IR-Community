<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.78,84.74,322.27,15.42">TU_DBS in the ARQMath Lab 2021, CLEF</title>
				<funder ref="#_AvyK8CJ">
					<orgName type="full">DFG</orgName>
				</funder>
				<funder>
					<orgName type="full">Center for Information Services and HPC (ZIH)</orgName>
				</funder>
				<funder>
					<orgName type="full">Cluster of Excellence &quot;Physics of Life&quot; of TU Dresden</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.87,116.63,59.11,5.42"><forename type="first">Anja</forename><surname>Reusch</surname></persName>
							<email>anja.reusch@tu-dresden.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Database Systems Group</orgName>
								<orgName type="institution">Technische Universität Dresden</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,159.40,116.63,58.50,5.42"><forename type="first">Maik</forename><surname>Thiele</surname></persName>
							<email>maik.thiele@tu-dresden.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Database Systems Group</orgName>
								<orgName type="institution">Technische Universität Dresden</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,245.34,116.63,84.55,5.42"><forename type="first">Wolfgang</forename><surname>Lehner</surname></persName>
							<email>wolfgang.lehner@tu-dresden.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Database Systems Group</orgName>
								<orgName type="institution">Technische Universität Dresden</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.78,84.74,322.27,15.42">TU_DBS in the ARQMath Lab 2021, CLEF</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">7EB669C4EF1676F6F478EF7265A9D4AD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Mathematical Language Processing</term>
					<term>Information Retrieval</term>
					<term>BERT-based Models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mathematical Information Retrieval (MIR) deals with the task of finding relevant documents that contain text and mathematical formulas. Therefore, retrieval systems should not only be able to process natural language, but also mathematical and scientific notation to retrieve documents. The goal of this work is to review the participation of our team in the ARQMath 2021 Lab where two different approaches based on ALBERT and ColBERT were applied to a Question Answer Retrieval task and a Formula Similarity task. The ALBERT-based classification approach received competitive results for the first task. We found that by pre-training on data separated in chunks of text and formulas, the model performed better on formula data. This way of pre-training could also be beneficial for the Formula Search task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the rising number of scientific publications and mathematics-aware online communities available Mathematical Information Retrieval has become more important since many of these documents and posts not only use natural language, but also mathematical notation to communicate. Only interpreting natural language is not sufficient for retrieval in such documents anymore since the usage of mathematical notation is crucial to understand the information conveyed by the author. Hence, in order to search or retrieve information from these platforms, a retrieval system needs to understand the notation of mathematical expressions.</p><p>The ARQMath Labs 2020 <ref type="bibr" coords="1,209.27,455.77,12.68,4.94" target="#b0">[1]</ref> and 2021 <ref type="bibr" coords="1,264.00,455.77,12.69,4.94" target="#b1">[2]</ref> have two related aims: Task 1 deals with the retrieval of relevant answers given a question from the Mathematics StackExchange Community. This task involves understanding the problem of the question poster in terms of natural language in combination with mathematical notation in form of L A T E X, Symbol Layout Trees (SLTs) or Operator Trees (OPTs). For Task 2 on the other hand, the participants were required to develop a system that returns relevant formulas given a query formula.</p><p>For Natural Language based Information Retrieval systems based on large pre-trained language models such as BERT <ref type="bibr" coords="1,218.26,550.61,12.96,4.94" target="#b2">[3]</ref> have been found to be effective and out-performed previous, traditional IR systems that were based on string matching methods <ref type="bibr" coords="1,398.89,564.16,11.58,4.94" target="#b3">[4]</ref>. In a previous work, we showed that our approach of using an ALBERT-based classifier as a similarity measure is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">ARQMath 2021 Lab</head><p>The aim of ARQMath Lab 2021 <ref type="bibr" coords="2,231.88,490.58,12.99,4.94" target="#b1">[2]</ref> is to accelerate the research in mathematical Information Retrieval and includes two related, but different tasks: Task 1 involves the retrieval of relevant answer posts for a question asked on the Mathematics StackExchange, which is a platform where users post questions to be answered by the community. The questions should be related to mathematics topics at any level <ref type="foot" coords="2,238.62,542.14,3.71,3.61" target="#foot_0">1</ref> . Users have the possibility to add mathematical formulas to their post to clarify their questions. These formulas are written in L A T E X notation. Task 2 is built on top of the same data, but with a different goal in mind: Participants are expected to retrieve relevant formulas given a query formula in context of its post. This task is related to the formula browsing task of NTCIR-12 <ref type="bibr" coords="2,213.58,598.97,11.42,4.94" target="#b7">[8]</ref>. The participating teams submitted for each topic a ranked list of 1.000 documents retrieved by their systems, which were scored by Normalized Discounted Cumulative Gain, but with unjudged documents removed before assessment (nDCG'). The graded relevance scale used for scoring ranged from 0 (not relevant) to 3 (highly relevant). Two additional measures, mAP' and P@10, were also reported using binarized relevance judgments (0 and 1: not relevant, 2 and 3: relevant). The relevance assessment was performed by pooling after the teams submitted their results.</p><p>ARQMath 2021 provides data from the Mathematics StackExchange including question and answer posts from 2010 to 2018. In total, the collection contains 1 M questions and 1.4 M answers. Furthermore, users may use mathematical formulas to clarify their posts. These formulas written in L A T E X notation were extracted and parsed into Symbol Layout Trees and Operator Trees. Each formula got assigned a formula id and a visual ids. Formulas sharing the same visual appearance received the same visual id. Apart from this corpus of posts and formulas that are available for training and evaluating models, also a test set of queries is released by the organizers of ARQMath. The query topics of 2020 and 2021 contain 99 and 100 topics, repectively, which are question posts including title, text and tags. In the 2020 test set 77 queries were evaluated for Task 1 and 45 formula queries for Task 2, while the evaluation of Task 1 in 2021 included 71 queries and 58 for Task 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related Work</head><p>Bidirectional Encoder Representations from Transformers (BERT) is an architecture based on the encoder of a Transformer model which was designed for language modelling <ref type="bibr" coords="3,456.93,324.99,11.56,4.94" target="#b2">[3]</ref>. Due to the success of this and other pre-trained, Transformer-based language models, BERT has been a basis in many systems for Natural Language Understanding (NLU) tasks and applications in Information Retrieval. Hence, there exist several advanced versions such as RoBERTa <ref type="bibr" coords="3,480.59,365.64,12.99,4.94" target="#b8">[9]</ref> or ALBERT <ref type="bibr" coords="3,130.39,379.19,17.91,4.94" target="#b9">[10]</ref> with the goal to optimize BERT's performance.</p><p>The influence of in-domain pre-training has been analyzed by Gururangan et al. <ref type="bibr" coords="3,466.89,392.74,16.31,4.94" target="#b10">[11]</ref> who found that this is especially valuable when the domain vocabulary has a low overlap with the pre-training data. As a consequence, various models for different domains have been developed, such as BioBERT <ref type="bibr" coords="3,216.74,433.39,16.18,4.94" target="#b11">[12]</ref>, ClinicalBERT <ref type="bibr" coords="3,301.56,433.39,16.38,4.94" target="#b12">[13,</ref><ref type="bibr" coords="3,320.65,433.39,13.99,4.94" target="#b13">14]</ref> or SciBERT <ref type="bibr" coords="3,390.26,433.39,17.84,4.94" target="#b14">[15]</ref> for scientific domains or CuBERT <ref type="bibr" coords="3,140.35,446.94,17.76,4.94" target="#b15">[16]</ref> and CodeBERT <ref type="bibr" coords="3,227.97,446.94,16.09,4.94" target="#b16">[17]</ref>. A notable difference between these models is that BioBERT, ClinicalBERT and CodeBERT use the original vocabulary that their base model was pre-trained on while SciBERT and CuBERT trained their own domain specific vocabulary. However, each of these models could demonstrate its improvements compared to the original models without domain specific pre-training.</p><p>BERT-based models for mathematical domains have also been studied with the most recent example being MathBERT <ref type="bibr" coords="3,211.90,528.23,16.41,4.94" target="#b17">[18]</ref>. In addition, during the last ARQMath Lab 2020, two teams submitted systems based on BERT and RoBERTa <ref type="bibr" coords="3,319.13,541.78,16.56,4.94" target="#b18">[19,</ref><ref type="bibr" coords="3,339.41,541.78,12.42,4.94" target="#b19">20]</ref>. Both teams used the models to generate post embeddings for a given question and all answers. Their similarity is calculated by comparing the vectors using cosine similarity.</p><p>Shortly after BERT outperformed previous approaches in various NLU tasks, it was also successfully applied to Information Retrieval. The model by Nogueira et al. classified its input consisting of a query and one document for their relevance resulting in a score that can be used to rank multiple documents <ref type="bibr" coords="3,236.59,623.08,11.32,4.94" target="#b3">[4]</ref>. This approach achieved state-of-the-art performance, but was much slower and computationally expensive than previous systems, because one forward pass through the entire deep neural network was necessary to score one query-document pair. Nevertheless, this approach has also been proven to be effective for the multi-modal retrieval of  <ref type="figure" coords="4,315.82,280.47,3.81,4.79" target="#fig_1">2</ref>) and fine-tuning procedure (details in Figure <ref type="figure" coords="4,501.35,280.47,4.63,4.79" target="#fig_2">3</ref> and<ref type="figure" coords="4,107.22,292.43,3.27,4.79" target="#fig_3">4</ref>).</p><p>source code <ref type="bibr" coords="4,143.99,331.14,17.85,4.94" target="#b16">[17]</ref> and was also applied to Mathematical Question Answering using an ALBERT model trained and evaluated on the ARQMath Lab 2020 test set <ref type="bibr" coords="4,384.11,344.69,11.59,4.94" target="#b4">[5]</ref>. The evaluation results were also broken down to the categories determining which part of the question influenced answering it the most. The model showed the best performance when answering the question depended on the written text. But for questions relying on formulas the results were worse than systems based on non-neural methods. Therefore, the modeling capability of formulas needs to be improved to also be able to capture their semantics in a better way.</p><p>Due to the fact that the ranking model by Nogueira et al. came with a steep increase in computational cost, recent research focused on improving the evaluation time without neglecting its performance gains. Despite there is more than one model dealing with this challenge, we will focus in this work on the approach by Khattab et al. called ColBERT <ref type="bibr" coords="4,391.04,466.64,11.30,4.94" target="#b6">[7]</ref>. ColBERT uses a BERT model to separately encode a query and a document and then apply a novel late interaction mechanism to calculate the similarity. This way they achieved competitive results when reranking on the popular MSMARCO data set <ref type="bibr" coords="4,287.73,507.28,17.93,4.94" target="#b20">[21]</ref> with a latency of 61 ms compared to 107,000 ms using the BERT-based approach by Nogueira et al.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Model Architecture</head><p>BERT-based models have proven to be effective in Natural Language Understanding and Information Retrieval tasks. Their strength was also shown in scenarios where not only natural language plays an important role, such as Code Retrieval or Mathematical Language Processing as in this lab <ref type="bibr" coords="4,151.48,620.11,16.55,4.94" target="#b16">[17,</ref><ref type="bibr" coords="4,171.37,620.11,7.52,4.94" target="#b4">5,</ref><ref type="bibr" coords="4,182.23,620.11,12.42,4.94" target="#b17">18]</ref>. Building on top of these achievements, we apply two deep neural models based on the popular BERT in our submission: ALBERT and ColBERT. ALBERT is an even more recent model based on BERT, which is optimized by factorization of the embeddings and parameter sharing between layers. The general idea of our first approach is to employ the ALBERT model to determine the similarity score between two snippets, for Task 1 a question and an answer and for Task 2 two formulas with context. This is achieved by fine-tuning a classifier on top of the pre-trained ALBERT model which predicts how well the two snippets match. We apply ALBERT for this approach, because its optimizations result in less training parameters and therefore a lower memory consumption and accelerated training speed compared to BERT. The second method that we apply for Task 1 uses a BERT model as a basis of ColBERT. The query and each document are passed through BERT separately in order to encode their respective content. This way an offline computation of the representations of each document is possible beforehand. The late interaction mechanism in form of the L2 distance is applied to aggregate and compare the contextualized embeddings. Finally, the documents are ranked by this computed L2 distance.</p><p>The success of BERT and BERT-based models is attributed to its transformer architecture <ref type="bibr" coords="5,488.23,239.27,17.76,4.94" target="#b21">[22]</ref> and also to the self-supervised pre-training on large amounts of data. In this work, we will focus on the latter aspect and pre-train models on different data highlighting its influence. The overall process of our approach is depicted in Figure <ref type="figure" coords="5,292.20,279.92,3.74,4.94" target="#fig_0">1</ref>. We will present details about the pre-training and fine-tuning in the next sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Task 1</head><p>The goal of Task 1 is the retrieval of an answer post from 2010 -2018 to questions that were posted in 2019. The ARQMath Lab 2021 added a second set of 100 questions asked in 2020. The optimal answers retrieved by the participants are expected to answer the complete question on its own. This relevance of each question was assessed by reviewers during a pooling process. In the following sections we will present our two approaches for this mathematical question answering task. We will first explain the models we used, then how we processed the data corpus for pre-training and fine-tuning. Finally, we give details on our experiments, the results and a comparison to other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Pre-Training</head><p>As mentioned previously, BERT and also ALBERT rely on pre-training on rather simple tasks. BERT is pre-trained using two objectives to obtain general understanding of language: the masked language model (MLM) and the next sentence prediction (NSP).</p><p>Pre-training is performed on a sentence-level granularity. Each sentence 𝑆 is split into tokens:</p><formula xml:id="formula_0" coords="5,89.29,548.33,81.92,10.69">𝑆 = 𝑤 1 𝑤 2 • • • 𝑤 𝑁 .</formula><p>Before inputting the sentence into the model, each token 𝑤 𝑖 is embedded using a sum of three different embeddings, the word embedding 𝑡 𝑖 encoding the semantic of the token, the position embedding 𝑝 𝑖 denoting its position within the sentence, and the segment embedding 𝑠 𝑖 in order to discern between the first and the second segment when the model is presented e.g., two sentences as for the NSP task. The segment embeddings will also help our model to differentiate between the query and document as the two segments later. All three embeddings are added up to form the input embedding 𝐸 𝑖 for each token: In order to obtain a representation of the entire input, we prepend the sentence 𝑆 with a classification token 𝑤 𝑆 = ⟨𝐶𝐿𝑆⟩. It is embedded in the same way as the other tokens and will be used for the NSP task and also for fine-tuning tasks that rely on a representation of the input such as classification.</p><formula xml:id="formula_1" coords="5,257.46,654.13,80.36,10.63">𝐸 𝑖 = 𝑡 𝑖 + 𝑝 𝑖 + 𝑠 𝑖 .</formula><p>The first pre-training task is the masked language model meaning tokens from the input sentence are randomly replaced by a ⟨𝑀 𝐴𝑆𝐾⟩ token, a different token or is not changed at all. After embedding the input, it is feed into the BERT model, consisting of 12 layers of transformer encoder blocks, resulting in a contextualized output embedding 𝑈 𝑖 for each input token:</p><formula xml:id="formula_2" coords="6,194.84,559.12,205.59,10.69">𝐶𝑈 1 𝑈 2 • • • 𝑈 𝑁 = BERT(𝐸 𝐶𝐿𝑆 𝐸 1 𝐸 2 • • • 𝐸 𝑁 ),</formula><p>where 𝐸 𝐶𝐿𝑆 and 𝐶 are the input and output embeddings of the ⟨𝐶𝐿𝑆⟩ token. Afterwards, a simple classifier is applied in order to predict the original word from the input:</p><formula xml:id="formula_3" coords="6,196.09,619.70,203.09,10.68">𝑃 (𝑤 𝑗 |𝑆) = softmax(𝑈 𝑖 • 𝑊 𝑀 𝐿𝑀 + 𝑏 𝑀 𝐿𝑀 ) 𝑗 ,</formula><p>where 𝑤 𝑗 is the 𝑗-th word from the vocabulary. This determines the probability that the 𝑖-th input word was 𝑤 𝑗 given the input sentence 𝑆. The weight matrix 𝑊 𝑀 𝐿𝑀 and its bias 𝑏 𝑀 𝐿𝑀 are only used for this pre-training task and are not reused afterwards.</p><p>The next sentence prediction objective predicts whether the sentence given to the model as the first segment 𝑆 𝐴 appears in a text before the sentences given to the model as the second segment 𝑆 𝐵 (label 1) or whether the second sentence is a random sentence from another document (label 0). This task is performed as a binary classification using the output embedding 𝐶 as its input:</p><formula xml:id="formula_4" coords="7,188.96,155.44,217.36,10.69">𝑝(𝑙𝑎𝑏𝑒𝑙 = 𝑖|𝑆) = softmax(𝐶 • 𝑊 𝑁 𝑆𝑃 + 𝑏 𝑁 𝑆𝑃 ) 𝑖 ,</formula><p>where the matrix 𝑊 𝑁 𝑆𝑃 and the bias 𝑏 𝑁 𝑆𝑃 are only used for the NSP and are not used otherwise later.</p><p>ALBERT also makes use of the MLM objective, but it has been found that NSP, predicting whether the second sentence in the input is swapped with a sentence from another document from the corpus, is a relatively challenging task and was changed to the sentence order prediction (SOP). Here, the model is asked to determine what the correct order of two presented sentences is. Hence, the model is presented with two sentences and performs their classification in the same way as BERT's NSP. Therefore, the formulas for NSP as introduced above apply as well.</p><p>The pre-training process of BERT and ALBERT is depicted together in Figure <ref type="figure" coords="7,450.46,286.39,3.78,4.94" target="#fig_1">2</ref>. Note, that BERT applies a classification on the output embedding 𝐶 for the NSP objective, while ALBERT does the same for the SOP objective. Both models use the MLM objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-Training Data</head><p>Before pre-training we applied the official tool provided by ARQMath to read the posts, wrapped formulas in $ and removed other html markup, yielding a list of paragraphs for each post. BERT and ALBERT models rely on sentence separated data during pre-processing for the NSP and SOP tasks. Two different strategies were tested: <ref type="bibr" coords="7,302.51,403.47,11.48,4.94" target="#b0">(1)</ref> split the text into sentences, (2) split it into chunks of text and formulas. The SOP task is designed to work with sentences. Hence, ( <ref type="formula" coords="7,488.02,417.02,3.94,4.94">1</ref>) is usually used in various NLP tasks. On the other hand, our goal was to increase the model's understanding of formulas. Therefore, strategy (2) splits a paragraph first into sentences, but also when a sentences contains a formula (with more than three LaTeX tokens to avoid splitting at e.g., definitions of symbols). In case the remaining text is too short (less than ten characters), it is concatenated to the formula before, separated by a $ sign. Before inputting the data into the models, tokenizing, creating the pre-training data for each task, i.e., masking tokens and assembling pairs of sentences, and further pre-processing was performed by the pre-processing scripts provided in the official BERT and ALBERT repositories<ref type="foot" coords="7,430.67,522.78,3.71,3.61" target="#foot_1">2</ref> . For the models that started from official checkpoints, we used the released sentencepiece vocabulary <ref type="bibr" coords="7,487.15,538.96,16.41,4.94" target="#b22">[23]</ref>. For the models that started from scratch, we trained our own sentencepiece model using the parameters recommended in the ALBERT repository which had a vocabulary overlap of 32.1% compared to the released sentencepiece vocabulary for ALBERT. Sentencepiece tokenizes the input into subwords using byte-pair-encoding <ref type="bibr" coords="7,298.03,593.16,18.69,4.94" target="#b23">[24]</ref>, e.g., the sentence 'how can i evaluate $ \sum_{n=1}ˆ\infty \frac{2n}{3ˆ{n+1}} $?' would be tokenized into '</p><formula xml:id="formula_5" coords="7,89.29,606.71,417.30,18.49">how can i evaluate $ \ sum _ { n = 1 } ˆ\ in ##ft ##y \ fra ##c { 2 ##n } { 3 ˆ{ n + 1 } } $?</formula><p>' by the BERT tokenizer, where single tokens are separated by spaces. Input sequences whose length after tokenization exceeded the maximum number of input tokens where truncated to the maximum length. In case two segments together exceeding the maximum length during e.g., NSP or fine-tuning, token by token was deleted from the longest sequence until the sum of the number of both segments equaled the maximum length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">ALBERT Model</head><p>In order to predict whether an answer</p><formula xml:id="formula_6" coords="8,89.29,395.96,416.70,37.78">𝐴 = 𝐴 1 𝐴 2 • • • 𝐴 𝑀 is relevant to a question 𝑄 = 𝑄 1 𝑄 2 • • • 𝑄 𝑁 a classifier is trained on top of the pre-trained ALBERT model as depicted in Figure 3. The input string ⟨𝐶𝐿𝑆⟩𝑄 1 𝑄 2 • • • 𝑄 𝑁 ⟨𝑆𝐸𝑃 ⟩𝐴 1 𝐴 2 • • • 𝐴 𝑀 ,</formula><p>with ⟨𝐶𝐿𝑆⟩ being the classification token and ⟨𝑆𝐸𝑃 ⟩ the separation token, is presented to the model:</p><formula xml:id="formula_7" coords="8,179.56,459.07,236.16,10.69">𝐶𝑈 1 𝑈 2 • • • 𝑈 𝑁 = ALBERT(𝐸 𝐶𝐿𝑆 𝐸 1 𝐸 2 • • • 𝐸 𝑁 +𝑀 ),</formula><p>where 𝐸 𝑖 and 𝐸 𝐶𝐿𝑆 are the input embeddings for each input token and the ⟨𝐶𝐿𝑆⟩ token, respectively, calculated as explained in Section 5.1. After the forward pass through the model, the output vector of the ⟨𝐶𝐿𝑆⟩ token 𝐶 is given into a classification layer:</p><formula xml:id="formula_8" coords="8,178.12,531.10,239.03,10.77">𝑝(𝑙𝑎𝑏𝑒𝑙 = 𝑖|𝑄, 𝐴) = softmax(𝐶 • 𝑊 𝑇 𝑎𝑠𝑘1 + 𝑏 𝑇 𝑎𝑠𝑘1 ) 𝑖 ,</formula><p>where the label 1 stands for a matching or correct answer for the query and label 0 otherwise. During evaluation, the resulting probability of the classification layer for label 1, is the assigned similarity score 𝑠 for the answer 𝐴 to a question 𝑄 and is then used to rank the answers in the corpus:</p><p>𝑠(𝑄, 𝐴) = 𝑝(𝑙𝑎𝑏𝑒𝑙 = 1|𝑄, 𝐴).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-Tuning Data</head><p>In order to fine-tune the ALBERT models for Task 1, we paired each question with one correct answer and one incorrect answer. The correct answer was randomly chosen from the answers of the question. Each question in the corpus comes along with tags, i.e., categories indicating the topic of a question such as sequences-and-series or limits. As an incorrect answer for each question, we picked a random answer from one question sharing at least one tag with the original question by chance. Following this procedure, we yielded 1.9M examples, of which 90% were used as training data for the fine-tuning task. We presented the model the entire text of the questions and answers using the structure introduced in the previous section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">ColBERT Model</head><p>Our second approach was to train ColBERT on top of a pre-trained BERT model. In each training step, the model is presented the query 𝑄 and two answers: one being a relevant answer 𝐴, the second being an answer 𝐵 that should be regarded as non-relevant by the model. All three strings, 𝑄, 𝐴 and 𝐵 are prepended with a token denoting the string as either question (query), ⟨𝑄⟩ or answer (document) ⟨𝐷⟩, and are passed through the BERT model individually to create contextualized embeddings for each post:</p><formula xml:id="formula_9" coords="9,179.74,297.42,235.81,35.19">𝐶 𝑄 𝑄𝑈 1 𝑈 2 • • • 𝑈 𝑁 = BERT(𝐸 𝐶𝐿𝑆 𝐸 𝑄 𝐸 1 𝐸 2 • • • 𝐸 𝑁 ), 𝐶 𝐷 𝐷𝑉 1 𝑉 2 • • • 𝑉 𝑀 = BERT(𝐸 𝐶𝐿𝑆 𝐸 𝐷 𝐹 1 𝐹 2 • • • 𝐹 𝑀 ),</formula><p>where 𝐸 𝑖 , 𝐹 𝑖 , 𝐸 𝐶𝐿𝑆 , 𝐸 𝑄 , and 𝐸 𝐷 are the input embeddings for each input token, the ⟨𝐶𝐿𝑆⟩ token, ⟨𝑄⟩ token and the ⟨𝐷⟩ token, respectively, calculated as explained in Section 5.1. Using the late interaction mechanism as specified by Khattab et al. <ref type="bibr" coords="9,357.62,371.59,12.77,4.94" target="#b6">[7]</ref> a relevance or similarity score is calculated for each question-answer pair and optimized by applying softmax cross-entropy loss over the scores:</p><formula xml:id="formula_10" coords="9,216.53,405.82,162.22,33.71">𝑠(𝑄, 𝐴) = 𝑁 ∑︁ 𝑖=1 max 𝑗∈{1,...,𝑀 } 𝑈 𝑖 𝑉 𝑇 𝑗 .</formula><p>More implementation specific detail can be found in work by Khattab et al. <ref type="bibr" coords="9,425.66,449.28,11.43,4.94" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-Tuning Data</head><p>We use the same procedure to generate training data for the ColBERT-based models, but with the difference that we used up to 𝑁 𝑎𝑛𝑠𝑤𝑒𝑟𝑠 = 10 correct and incorrect answers in case a question had that many submitted answers. If less answers were present, the minimum of correct and incorrect answers was used such that the number of correct and incorrect answers matched. We paired each correct answer with all incorrect answers, generating at most 10 × 10 = 100 samples for each question. We experimented with 𝑁 𝑎𝑛𝑠𝑤𝑒𝑟𝑠 = 1 and 𝑁 𝑎𝑛𝑠𝑤𝑒𝑟𝑠 = 5, but we achieved best results with 𝑁 𝑎𝑛𝑠𝑤𝑒𝑟𝑠 = 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Evaluation Data</head><p>During evaluation we exploited the tag information from the queries in order to rank only the answers that shared at least one tag with the query question. In this way, we saved large amounts of computation time for the ALBERT-based models. Each question and the answers were pre-processed and paired in the same way as during fine-tuning. For ColBERT, we generated an index based on all answers whose question had at least one tag that was associated with at least one query question.</p><p>For each query the organizers of the Lab annotated whether answering the question mostly depends on its text, its formulas or both. We used these categories for the interpretation of our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Experiments</head><p>We tested various scenarios for training ALBERT of which we report six in this work: The models Base 750k, Base Combined and Base 250k are initialized from the official weights of the ALBERT base model released by the ALBERT authors and were further pre-trained on ARQMath data using strategy (1), i.e., sentence split text (see Section 5.1). The data pre-processed with strategy (2), i.e., data split into text and L A T E X, was mixed with the aforementioned data to pre-train Base Combined. The weights of Scratch 1M, Scratch Separated and Scratch 2M were initialized randomly. Scratch 1M and Scratch 2M used the sentence split data (1) while Scratch Separated was only pre-trained on the separated data of strategy (2). All six models followed the recommendations for hyperparameter configuration during pre-training, with 12M parameters, using the LAMP optimizer <ref type="bibr" coords="10,261.07,464.02,16.09,4.94" target="#b24">[25]</ref>, 3,125 warm up steps, maximum sequence length of 512 and a vocabulary size of 30,000. Furthermore, we used a batch size of 32 and a learning rate of 0.0005. The models were trained for different numbers of steps: Base 750k was trained for 750k steps while the training of Base 250k was already stopped after 250k steps. Scratch 1M and Scratch Separated pre-trained for 1M steps. This amount was doubled for Scratch 2M. Finally, Base Combined could only be trained for 135k steps before the final submission of the results. A summary of the different combination of pre-training data and number of steps for each model can be found in Table <ref type="table" coords="10,236.90,558.86,3.66,4.94" target="#tab_0">1</ref>. After pre-training, each classification model was fine-tuned for 125k steps using a batch size of 32, a learning rate of 2e-5 and 200 warm-up steps. Both pre-training and fine-tuning was performed using the code published in the official ALBERT repository. We submitted results of four ALBERT-based models to the ARQMath 2021 Lab and evaluated Base 250k and Scratch 2M using the official evaluation tools.</p><p>ColBERT can be seen as an extension of BERT whose performance depends on its pre-training <ref type="bibr" coords="10,89.29,640.15,11.28,4.94" target="#b6">[7]</ref>. Therefore, we apply three differently pre-trained models as the basis for ColBERT: ColBERT uses the weights of the original BERT-base, ColSciBERT uses SciBERT <ref type="bibr" coords="10,402.61,653.70,16.09,4.94" target="#b14">[15]</ref>, which was trained on a large corpus of scientific publications from multiple domains and finally, we pre-trained our own BERT model for ColARQBERT. The last model was initialized using the original BERT weights and was then further pre-trained on the sentence split data (1) described earlier. The hyperparameters recommended by the BERT authors in their repository were used to pre-train this model: The learning rate was set to 2e-05, one batch contained 16 samples and the models were trained for 500k steps. In contrast to the recommendations we set the maximum length of the input to 512, because we did not start to train the model from scratch, where the initial sequence length was set to 128, but rather further trained the already fully pre-trained model on additional data. The training of all three ColBERT models made use of the same hyperparameter configuration. We optimized the L2 similarity between 128-dimensional vectors with a batch size of 128 for 75k steps. Other parameters kept their default values. Punctuation tokens were masked, but we also experimented with models that did not mask them, but we could not see a significant difference in the results. We also started to incorporate ALBERT as a base model for ColBERT, but did not yet find a configuration for a successful training. The pre-training of ColARQBERT was performed using the code published by the BERT authors, while the ColBERT repository was slightly adapted to support different checkpoints than BERT base in order to train the other models. Finally, ColSciBERT model was submitted to the competition, while ColBERT and ColARQBERT were evaluated later using the official evaluation guide.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Evaluation</head><p>The results of our ALBERT and ColBERT-based models are shown in Table <ref type="table" coords="11,435.50,356.74,5.17,4.94" target="#tab_1">2</ref> together with additional experiments that were not submitted and results of other models from the ARQMath 2021 Lab for comparison. We report the scores of the 2020 test set and the new 2021 test set. In addition, we break down the nDCG' score results of 2020 by the categories on which part answering the question depends. These categories are either text, formula or both in combination and were annotated by the organizers of the lab. The scores for each category are reported in Table <ref type="table" coords="11,168.50,438.04,3.74,4.94">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-Training Adjustments</head><p>In general, our results can be seen as competitive. Regarding nDCG', all ALBERT-based models could outperform the baseline systems in both years. On the 2020's test set, one team with three systems received the highest scores for mAP', while our ALBERT-based models are all in the range of the top four teams. In 2021, our best model ranks second among all teams regarding mAP'. Our results for p'@10 are not as high as the best baseline, but there was not a single system from any of the teams that could beat the baseline results for p'@10. Comparing to the other participants, our system receives the highest score for p'@10 in 2021.</p><p>The reason why our Precision is relatively high, but the nDCG' is lower compared to the other teams that received higher scores could be that our systems do not rank all answers for each topic due to the too time consuming evaluation. Possibly, our results would have been better if all answers would have been scored for their relevance.</p><p>We will now take a deeper look at the differences between the models we trained. When comparing Base 750k and Base 250k, the overall score is slightly increased by the longer pretraining. In Table <ref type="table" coords="11,165.94,663.51,4.97,4.94">3</ref> we see that with longer pre-training the model learned a better understanding of text and formulas on their own, but for category 'both' the results decreased. On the other hand, pre-training for too many steps shows effects of over-fitting as the scores start to decrease again as we see in the difference between Scratch 1M and Scratch 2M. The comparison of Scratch 1M and Scratch Separated shows that the separation of text and mathematical formulas leads to better nDCG' scores for queries dependent on formulas and text separately, but the performance degrades on question-answers pairs that depend on both, which is expected since the model was not pre-trained on data that involved both in one example. Base Combined has a much lower nDCG' value for the formula category in comparison to the other models. This can be explained by the fact that it was pre-trained for a much lower number of steps. The same effect is visible when viewing Base 750k and Base 250k. Therefore, we hypothesize that a pre-training of 750k or even more steps could even outperform Base 750k and Scratch Separated in all three categories.</p><p>BERT-base models generally benefit from a long pre-training on a large corpus. In our experiments, we could not observe this behavior. We experimented with models trained for 2M steps on data from 41 StackExchange communities supporting L A T E X, but the results are worse than the ones presented in Table <ref type="table" coords="12,237.04,672.76,3.74,4.94" target="#tab_1">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ColBERT</head><p>ColSciBERT is the fifth model we submitted for the 2020 ARQMath Task 1 and it was trained using ColBERT. As can be seen from the results table, its performance is not optimal hinting at a substantial problem during training or evaluation. This could be caused by using SciBERT as the basis for ColBERT. Two other models that were not officially submitted to the Lab received higher scores, but are still not on par with our other ALBERT-based approaches regarding all three metrics. This confirms the hypothesis that SciBERT is not suitable in this scenario.</p><p>Nevertheless, with ColBERT the time required to evaluate all 100 topics of 2020 took around six minutes using two NVIDIA GTX 1080 while evaluating one query using our ALBERT-based classification approach took between ten minutes and one hour on one NVIDIA V100. Therefore, further research in this direction is worthwhile for speeding up the evaluation while receiving competitive scores at the same time. Future work here should further analyze the performance and determine the best training scenario for a ColBERT-based system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Task 2</head><p>Task 2 deals with the retrieval of relevant formulas given a query formula together with the post in which it appeared. As for Task 1 we will start from a pre-trained ALBERT model, which was already introduced in Section 5.1. In the following section, we will therefore only highlight how the fine-tuning and data processing was performed and present the results of the application of an ALBERT-based model for the task of formula similarity search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Fine-Tuning Model</head><p>For formula similarity search our approach slightly differs from the one presented in Section 5.2 for Task 1. Instead of presenting the model the two formulas as the query and answer to classify whether they are relevant to each other, we add the question in which the first formula appears as additional context since the same formula and especially its variables can have different interpretations depending on its context, e.g., 𝑃 (𝑋) can be a probability of a random variable or a polynomial depending on its context. Each query formula was concatenated with its question forming the first part of the input 𝑄 1 𝑄 2 • • • 𝑄 𝑁 , separated by $. The formula that should be assessed for its similarity to the first formula makes up the second part of the input 𝐴 1 𝐴 2 • • • 𝐴 𝑀 . Analogously to Task 1, the classification token ⟨𝐶𝐿𝑆⟩ is added at the beginning of the input and both parts are separated by the separation token ⟨𝑆𝐸𝑃 ⟩. The output of the classifier is the similarity score that is optimized during training and used for the ranking of candidates during evaluation. The process of fine-tuning ALBERT for Task 2 is depicted in Figure <ref type="figure" coords="14,120.36,225.72,3.74,4.94" target="#fig_3">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-Tuning Data</head><p>Fine-tuning was performed on formulas in context with the post in which they appeared in order to provide the model with information on how the formula was used by the author. First, we removed formulas that contained less than three L A T E X token and filtered the ARQMath corpus for posts that had at least one formula remaining. For each post in the corpus one formula was chosen either by chance (denoted as random) or the longest formula was used as the query formula (denoted as longest). Formulas from the title of the post were preferred when the title contained any formulas, because the title can be seen as a summary of the post that should include the formula with the most meaning to the post. We chose this procedure because we faced the problem that the posts contained many formulas (on average 9.41 formulas per post) not all of which were directly relevant to the post or the given answers, such as, definitions of variables or examples.</p><p>For each query formula we determined positive and negative examples. The positive examples, i.e., the ones that should be classified as relevant formulas by the model, originate in the answers given a post. This assumes that the formulas in the answers are relevant to the formulas in its question post. We chose the negative examples from answer posts where their questions had at least one common tag with the query post. For each query we used a maximum of five positive and negative examples, where the number of positive examples and negative examples were equal. Hence, when a question had 𝑁 𝑓 𝑜𝑟𝑚𝑢𝑙𝑎𝑠 with 𝑁 𝑓 𝑜𝑟𝑚𝑢𝑙𝑎𝑠 &lt; 5 formulas in its answers or we found only 𝑁 𝑓 𝑜𝑟𝑚𝑢𝑙𝑎𝑠 , 𝑁 𝑓 𝑜𝑟𝑚𝑢𝑙𝑎𝑠 &lt; 5 formulas in other posts with the same tags, then only 𝑁 𝑓 𝑜𝑟𝑚𝑢𝑙𝑎𝑠 formulas for positive and negative examples would be used. In total, we yielded 5,812,412 question-answer formula pairs of which 90% were used as training data. The training data was presented to the model in the way that was described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Evaluation Data</head><p>Due to time and hardware constraints, it was not possible to evaluate our models on the entire collection of formulas. Therefore, we limited the corpus to visually distinct formulas with more than three L A T E X tokens which appeared in questions and answers that shared either one or all tags with the query post, denoted by 'one' or 'all' in the results table, respectively. The ARQMath collection provided each formula with its visual id and the post id in which it appeared. We determined the tags of the posts from the post's corpus and aggregated for each visual id the tags of all posts in which formulas with this visual id appeared. For each visual id that remained in the corpus for a given query we provided the model with the query formula, its posts and the first formula that was associated with this visual id. The post id that was reported as our Task 2 result was the post id corresponding to the formula in the model input, i.e., the post id of the first formula for each visual id.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Experiments</head><p>In total, we trained two classifiers on different fine-tuning data and evaluated each of them in two settings as described above. These four configurations can be found in Table <ref type="table" coords="15,472.93,396.34,3.81,4.94" target="#tab_2">4</ref>. Both models are based on the pre-trained ALBERT-base model that was used for the Task 1 Model Base 750k. The fine-tuning hyperparameters are the same as for the models of Task 1: we used a learning rate of 2e-5, batch size of 32 and 200 steps for warm-up. Both classifiers were trained for 125k steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Evaluation</head><p>The results of our experiments for Task 2 can be seen in Table <ref type="table" coords="15,371.20,500.26,3.78,4.94" target="#tab_2">4</ref>. Our best model on the 2020 topics is fine-tuned on the random formulas as queries. It is evaluated on all distinct question and answer formulas that shared at least one tag with the query post. In general, the two models trained on data with a random query formula showed better results than the two models always using the longest formula. Including all formulas that had at least one similar tag increases the search space and therefore the results for the retrieved formulas from this search space are better. This suggests that the performance could be even more increased if all formulas from the entire corpus were used for the classification. This was not done in this work due to our hardware constraints leading to long evaluation times. Generally speaking, our ALBERT-based model is a promising approach, but the comparison to other participants of the lab demonstrates that it is not on par with state-of-the-art models or the baseline system. Hence, future work should explore better methods of representing formulas using ALBERT. In this work, only L A T E X-based representations have been used, but ARQMath also provides tree-based data for each formula. One possible improvement could be the prediction of relationships between nodes in these trees as it was done in a similar way for Programming Language Understanding using data flow graphs <ref type="bibr" coords="16,303.54,117.33,16.09,4.94" target="#b25">[26]</ref>. Furthermore, as seen in the evaluation of Task 1 in Section 5.6, pre-training on data that separated text and formulas improved the scores on formula dependent questions pointing to a better understanding of mathematical formulas compared to models trained on non-split data. Therefore, these pre-trained models could also be beneficial as basis for Task 2 fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>Mathematical Information Retrieval deals with the retrieval of documents from a corpus, which are relevant to a query, where documents and queries may include both, natural language and mathematical formulas. Two instances for such an objective are Task 1 and Task 2 of the ARQMath Lab, whose goal is to either retrieve answers given a question or formula retrieval using a formula in its context. Since this challenge includes not only text written in English, but also formulas, approaches from Natural Language Processing and Information Retrieval have to be adapted in order to be able to interpret also the semantics of mathematical formulas. This has also been demonstrated in our previous work, where we showed that ALBERT has to be further pre-trained on relevant data in order to better handle formulas in MIR tasks. In this work we further analyzed this claim and showed that our previous results for Task 1 could even be improved by a longer pre-training on the data provided by ARQMath. Furthermore, we showed that separating large chunks of natural language text and L A T E X notation in one sentence increased the model's performance on formula-only and text-only dependent questions, respectively. The second contribution of this work was to explore the application of ColBERT in order to accelerate the evaluation of queries, because our classification-based approach is too time-consuming. Thereby, we trained and evaluated a ColBERT model and showed that further improvements are necessary before this approach can reach state-of-the-art performance. We also applied our ALBERT-based approach to the formula retrieval objective of Task 2 and showed that there is still work necessary in order to improve the model's understanding of formula similarity. In conclusion, we showed promising approaches for Mathematical Answer Retrieval and Formula Similarity Search by applying differently pre-trained and fine-tuned ALBERT models and one ColBERT model. In order to improve the modeling capabilities of mathematical formulas, we recommended strategies involving several pre-training methods that include syntactical features of formula that we have not yet taken into account. To facilitate research based on our work, we release the code for data pre-processing and the training of the models in the project's repository <ref type="foot" coords="16,242.10,566.25,3.71,3.61" target="#foot_2">3</ref> . The source code for training the ColBERT-based models was forked from the official ColBERT repository and slightly adjusted <ref type="foot" coords="16,399.64,579.79,3.71,3.61" target="#foot_3">4</ref> .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,278.65,416.70,8.93;4,89.29,292.43,27.75,4.79;4,89.29,84.19,416.68,175.75"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of our pre-training (details in Figure2) and fine-tuning procedure (details in Figure3 and 4).</figDesc><graphic coords="4,89.29,84.19,416.68,175.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,89.29,391.13,416.69,8.93;6,89.29,404.91,311.99,4.79;6,89.29,84.19,416.68,288.40"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: BERT's and ALBERT's pre-training process, 0.9 symbolizes the NSP or SOP score for the two sentences, the red word 'values' is the predicted word for the masked token.</figDesc><graphic coords="6,89.29,84.19,416.68,288.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,89.29,282.49,192.45,8.93;8,108.88,84.19,375.01,185.73"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Architecture of Task 1's Fine-Tuning.</figDesc><graphic coords="8,108.88,84.19,375.01,185.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="13,89.29,282.49,192.45,8.93;13,108.88,84.19,375.01,185.73"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Architecture of Task 2's Fine-Tuning.</figDesc><graphic coords="13,108.88,84.19,375.01,185.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="10,88.99,90.49,307.97,115.39"><head>Table 1</head><label>1</label><figDesc>Overview of Pre-Training Configurations of ALBERT models</figDesc><table coords="10,198.32,123.88,198.64,82.00"><row><cell>Model</cell><cell cols="2">Pre-Training Data Steps</cell></row><row><cell>Base 750k</cell><cell>(1) sentence split</cell><cell>750k</cell></row><row><cell>Base 250k</cell><cell>(1) sentence split</cell><cell>250k</cell></row><row><cell>Base Combined</cell><cell>(1)+(2) combined</cell><cell>135k</cell></row><row><cell>Scratch 1M</cell><cell>(1) sentence split</cell><cell>1M</cell></row><row><cell>Scratch 2M</cell><cell>(1) sentence split</cell><cell>2M</cell></row><row><cell cols="2">Scratch Separated (2) separated</cell><cell>1M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="12,88.99,90.49,415.22,349.07"><head>Table 2</head><label>2</label><figDesc>Results of Task 1</figDesc><table coords="12,329.47,121.65,132.55,4.79"><row><cell>2020</cell><cell>2021</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="15,88.99,90.49,401.95,142.56"><head>Table 4</head><label>4</label><figDesc>Results of Task 2</figDesc><table coords="15,104.34,121.65,386.60,111.39"><row><cell cols="3">Fine-Tuning Eval Official</cell><cell></cell><cell>2020</cell><cell></cell><cell>2021</cell><cell></cell></row><row><cell>Data</cell><cell cols="2">Tags Identifier</cell><cell cols="5">nDCG' mAP' p'@10 nDCG' mAP' p'@10</cell></row><row><cell>random</cell><cell>one</cell><cell>TU_DBS_A3</cell><cell>0.426</cell><cell>0.298 0.386</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>longest</cell><cell>one</cell><cell>TU_DBS_A1</cell><cell>0.396</cell><cell>0.271 0.391</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>random</cell><cell>all</cell><cell>TU_DBS_A2</cell><cell>0.157</cell><cell>0.085 0.122</cell><cell>0.154</cell><cell cols="2">0.071 0.217</cell></row><row><cell>longest</cell><cell>all</cell><cell>TU_DBS_P</cell><cell>0.152</cell><cell>0.080 0.122</cell><cell>0.153</cell><cell cols="2">0.069 0.216</cell></row><row><cell>Best 2020</cell><cell></cell><cell>DPRL-ltrall</cell><cell>0.738</cell><cell>0.525 0.542</cell><cell>0.445</cell><cell cols="2">0.216 0.333</cell></row><row><cell>Best 2021</cell><cell></cell><cell cols="2">Approach0-P300 0.507</cell><cell>0.342 0.441</cell><cell>0.555</cell><cell cols="2">0.361 0.488</cell></row><row><cell>Baseline</cell><cell></cell><cell>TangentS_Res</cell><cell>0.691</cell><cell>0.446 0.453</cell><cell>0.492</cell><cell cols="2">0.272 0.419</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.93,673.72,117.99,4.06"><p>https://math.stackexchange.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="7,108.93,673.71,314.94,4.06"><p>https://github.com/google-research/bert, https://github.com/google-research/ALBERT</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="16,108.93,662.63,181.77,4.06"><p>https://github.com/AnReu/ALBERT-for-Math-AR</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="16,108.93,673.59,183.18,4.06"><p>https://github.com/AnReu/ColBERT-for-Formulas</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported by the <rs type="funder">DFG</rs> under <rs type="programName">Germany's Excellence Strategy</rs>, Grant No. <rs type="grantNumber">EXC-2068-390729961</rs>, <rs type="funder">Cluster of Excellence "Physics of Life" of TU Dresden</rs>. Furthermore, the authors are grateful for the GWK support for funding this project by providing computing time through the <rs type="funder">Center for Information Services and HPC (ZIH)</rs> <rs type="institution">at TU Dresden</rs>. We would also like to thank the reviewers for their helpful comments and recommendations.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_AvyK8CJ">
					<idno type="grant-number">EXC-2068-390729961</idno>
					<orgName type="program" subtype="full">Germany&apos;s Excellence Strategy</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="17,112.66,227.36,394.61,4.94;17,112.66,240.91,394.52,4.94;17,112.66,254.46,80.57,4.94" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="17,315.08,227.36,192.19,4.94;17,112.66,240.91,119.62,4.94">Finding old answers to new math questions: the arqmath lab at clef 2020</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,255.18,240.91,205.52,4.94">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="564" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,268.01,395.17,4.94;17,112.66,281.56,144.82,4.94" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="17,311.06,268.01,196.77,4.94;17,112.66,281.56,50.74,4.94">Advancing math-aware search: The arqmath-2 lab at clef</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="631" to="638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,295.11,393.33,4.94;17,112.66,308.66,363.59,4.94" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="17,353.43,295.11,152.55,4.94;17,112.66,308.66,181.08,4.94">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="17,112.66,322.21,395.01,4.94" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04085</idno>
		<title level="m" coord="17,205.30,322.21,124.33,4.94">Passage re-ranking with bert</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="17,112.66,335.76,393.33,4.94;17,112.66,349.31,393.33,4.94;17,112.66,362.86,306.00,4.94" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="17,265.60,335.76,240.38,4.94;17,112.66,349.31,72.38,4.94">An albert-based similarity measure for mathematical answer retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Reusch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Thiele</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Lehner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,209.80,349.31,296.19,4.94;17,112.66,362.86,229.90,4.94">Proceedings of the 44rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="17,112.66,376.41,393.53,4.94;17,112.66,389.95,393.33,4.94;17,112.66,403.50,327.57,4.94" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="17,335.61,376.41,170.58,4.94;17,112.66,389.95,79.80,4.94">Cedr: Contextualized embeddings for document ranking</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,215.66,389.95,290.33,4.94;17,112.66,403.50,229.90,4.94">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1101" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,417.05,393.33,4.94;17,112.66,430.60,393.33,4.94;17,112.66,444.15,321.41,4.94" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="17,219.71,417.05,286.28,4.94;17,112.66,430.60,105.76,4.94">Colbert: Efficient and effective passage search via contextualized late interaction over bert</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,239.84,430.60,266.15,4.94;17,112.66,444.15,244.04,4.94">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,457.70,393.61,4.94;17,112.66,471.25,122.89,4.94" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="17,413.82,457.70,92.45,4.94;17,112.66,471.25,36.27,4.94">Ntcir-12 mathir task overview</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aizawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kohlhase</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Topic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Davila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>NTCIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,484.80,394.53,4.94;17,112.30,498.35,393.68,4.94;17,112.66,511.90,107.17,4.94" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m" coord="17,173.53,498.35,256.77,4.94">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="17,112.66,525.45,393.53,4.94;17,112.66,539.00,393.33,4.94;17,112.33,552.55,29.19,4.94" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<title level="m" coord="17,408.01,525.45,98.18,4.94;17,112.66,539.00,237.41,4.94">Albert: A lite bert for self-supervised learning of language representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="17,112.66,566.09,394.53,4.94;17,112.66,579.64,393.33,4.94;17,112.66,593.19,395.01,4.94;17,112.66,606.74,48.96,4.94" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="17,112.66,579.64,313.47,4.94">Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Marasović</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,450.78,579.64,55.20,4.94;17,112.66,593.19,347.99,4.94">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8342" to="8360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,620.29,393.33,4.94;17,112.66,633.84,393.98,4.94;17,112.41,647.39,48.96,4.94" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="17,361.64,620.29,144.35,4.94;17,112.66,633.84,268.25,4.94">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,394.29,633.84,66.92,4.94">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,90.23,395.00,4.94;18,112.66,103.78,379.73,4.94" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="18,170.40,103.78,186.98,4.94">Publicly available clinical bert embeddings</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Alsentzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-H</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Redmond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">B</forename><surname>Mcdermott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="18,366.39,103.78,58.19,4.94">NAACL HLT</title>
		<imprint>
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page">72</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,117.33,393.33,4.94;18,112.66,130.88,272.20,4.94" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Altosaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05342</idno>
		<title level="m" coord="18,276.78,117.33,229.21,4.94;18,112.66,130.88,89.80,4.94">Clinicalbert: Modeling clinical notes and predicting hospital readmission</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="18,112.66,144.43,394.62,4.94;18,112.66,157.97,393.33,4.94;18,112.66,171.52,395.17,4.94;18,112.66,185.07,132.19,4.94" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="18,239.01,144.43,248.29,4.94">Scibert: A pretrained language model for scientific text</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,112.66,157.97,393.33,4.94;18,112.66,171.52,395.17,4.94;18,112.66,185.07,33.90,4.94">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3606" to="3611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,198.62,393.33,4.94;18,112.66,212.17,394.53,4.94;18,112.66,225.72,90.72,4.94" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="18,342.15,198.62,163.84,4.94;18,112.66,212.17,117.15,4.94">Learning and evaluating contextual embedding of source code</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Maniatis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,255.79,212.17,213.13,4.94">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5110" to="5121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,239.27,394.53,4.94;18,112.66,252.82,393.33,4.94;18,112.66,266.37,107.17,4.94" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="18,112.66,252.82,319.87,4.94">Codebert: A pre-trained model for programming and natural languages</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08155</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="18,112.66,279.92,393.33,4.94;18,112.66,293.47,244.72,4.94" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.00377</idno>
		<title level="m" coord="18,258.59,279.92,247.40,4.94;18,112.66,293.47,62.22,4.94">Mathbert: A pre-trained model for mathematical formula understanding</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="18,112.66,307.02,393.33,4.94;18,112.66,320.56,355.89,4.94" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="18,248.08,307.02,257.91,4.94;18,112.66,320.56,75.31,4.94">Psu at clef-2020 arqmath track: Unsupervised re-ranking using pretraining</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rohatgi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,210.91,320.56,129.93,4.94">CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,334.11,393.32,4.94;18,112.66,347.66,179.52,4.94" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="18,308.30,334.11,101.72,4.94">Three is better than one</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Novotnỳ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sojka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Štefánik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lupták</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,432.32,334.11,73.66,4.94;18,112.66,347.66,51.81,4.94">CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,361.21,393.71,4.94;18,112.66,374.76,368.97,4.94" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="18,449.54,361.21,56.83,4.94;18,112.66,374.76,258.71,4.94">Ms marco: A human generated machine reading comprehension dataset</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,394.17,374.76,57.15,4.94">CoCo@ NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,388.31,395.17,4.94;18,112.66,401.86,393.32,4.94;18,112.33,415.41,78.48,4.94" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="18,484.04,388.31,23.79,4.94;18,112.66,401.86,142.26,4.94">Polosukhin, Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="18,263.51,401.86,229.78,4.94">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,428.96,393.33,4.94;18,112.66,442.51,393.33,4.94;18,112.33,456.06,29.19,4.94" xml:id="b22">
	<monogr>
		<title level="m" type="main" coord="18,225.16,428.96,280.83,4.94;18,112.66,442.51,237.64,4.94">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06226</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="18,112.66,469.61,393.33,4.94;18,112.66,483.16,393.32,4.94;18,112.66,496.70,394.52,4.94;18,112.66,510.25,397.48,4.94;18,112.36,522.98,103.79,7.90" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="18,262.37,469.61,243.61,4.94;18,112.66,483.16,20.72,4.94">Neural machine translation of rare words with subword units</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1162</idno>
		<ptr target="https://www.aclweb.org/anthology/P16-1162.doi:10.18653/v1/P16-1162" />
	</analytic>
	<monogr>
		<title level="m" coord="18,156.17,483.16,349.81,4.94;18,112.66,496.70,49.24,4.94">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="18,112.66,537.35,394.52,4.94;18,112.66,550.90,394.61,4.94;18,112.66,564.45,270.52,4.94" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="18,167.25,550.90,319.67,4.94">Large batch optimization for deep learning: Training bert in 76 minutes</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,112.66,564.45,240.50,4.94">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,578.00,394.52,4.94;18,112.66,591.55,394.53,4.94;18,112.66,605.10,393.33,4.94;18,112.66,618.65,393.33,4.94;18,112.66,632.20,60.88,4.94" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="18,112.66,605.10,304.80,4.94">Graphcode{bert}: Pre-training code representations with data flow</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Svyatkovskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tufano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">K</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clement</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Sundaresan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=jLoC4ez43PZ" />
	</analytic>
	<monogr>
		<title level="m" coord="18,446.71,605.10,59.28,4.94;18,112.66,618.65,182.30,4.94">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
