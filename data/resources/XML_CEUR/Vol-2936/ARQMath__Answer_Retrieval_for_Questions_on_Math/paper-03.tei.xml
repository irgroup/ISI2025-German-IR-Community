<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,416.70,15.42">BERT-Based Embedding Model for Formula Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,113.06,69.72,11.96"><forename type="first">Pankaj</forename><surname>Dadure</surname></persName>
							<email>krdadure@gmail.com</email>
						</author>
						<author>
							<persName coords="1,170.57,113.06,69.16,11.96"><forename type="first">Partha</forename><surname>Pakray</surname></persName>
							<email>parthapakray@gmail.com</email>
						</author>
						<author>
							<persName coords="1,267.18,113.06,108.96,11.96"><forename type="first">Sivaji</forename><surname>Bandyopadhyay</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Engineering</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">National Institute of Technology Silchar Assam</orgName>
								<address>
									<postCode>788010</postCode>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,416.70,15.42">BERT-Based Embedding Model for Formula Retrieval</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">66CEA38AF72B6320DD374840AC450BC5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Formula Retrieval</term>
					<term>Mathematical Information Retrieval</term>
					<term>BERT</term>
					<term>Math Stack Exchange</term>
					<term>Formula Embedding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The web is a rich repository of mathematical information, the task of finding relevant information in such collection is a laborious one. However, numerous efforts have been made to develop web-accessible mathematical and scientific search systems in the recent past. In this paper, we have presented the BERT-based formula embedding model to facilitated formula retrieval in ARQMath2 tasks. To depict the performance of the pre-trained model for mathematical language processing tasks, the proposed BERT-based model is trained on the math exchange corpus of the ARQMath. It takes the L A T E X formulas as input and produced the fixed dimensional embeddings for the same. For similarity measure, the cosine similarity has been used. The obtained results have shown that the proposed approach provides better fits than existing embedding approaches and infers the meaningful semantic relationships between equations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The web is a rich repository of mathematical information, including Wikipedia, Arxiv, a growing number of digital libraries, and research publications in science &amp; engineering. In such a collection, the formula is the most common form of mathematical information. Meanwhile, formulae are highly structured and represented in predefined layout structures such as L A T E X and MathML. The ongoing development of such information encouraged the implementation of advanced tools and techniques to handle and analyze mathematical formulas effectively. The automatic retrieval of mathematical formulas is significantly beneficial for understanding scientific documents. As a result, the conventional information retrieval (IR) system treated the formulae as a text, however, it is unable to capture the structural and semantic meaning of the formulae. To fill this gap, the mathematical information retrieval (MIR) system comes under the limelight and has drawn increasing attention from researchers. The prime task of a mathematical information retrieval system is to retrieve the scientific document/formulae relevant to the queried formula <ref type="bibr" coords="1,226.50,575.05,11.28,10.91" target="#b0">[1]</ref>. In MIR, the sense of the term 'relevant' is a subjective matter <ref type="bibr" coords="1,89.29,588.60,12.69,10.91" target="#b1">[2]</ref> and is defined in two ways: the first one finds the relevance of queried formula based on their structural similarities, and the second one finds the relevance of queried formula based semantic similarities (formulae hold the same meaning but holds the different representation). In addition to these, it's considered not only the exact matching of queried formula but also those formulae that are partially matched with the queried formula (sub-formula and parent-formula). In the retrieval of a mathematical formula, the effective representation of the formulas is the prime aspect, leading to the preserve the connections between the mathematical symbols and creating the navigator for exploring the scientific documents.</p><p>For many applications like information retrieval, document clustering, or classification, the process of capturing semantics is a probabilistic way. The first step is common to incorporate words or documents into a vector space. Neural network approaches like word embedding models <ref type="bibr" coords="2,123.63,208.91,12.81,10.91" target="#b2">[3]</ref> acts as state-of-the-art models for representing individual words with semantically fixed-length vectors and provides excellent results compared to the tf-idf or count vectorizer. Word embedding makes it possible to successfully apply deep learning to natural language processing applications such as machine translation, text summarization, and question answering. Recently, several neural network-based retrieval approaches have been introduced, and some of them have made significant improvements in their performance <ref type="bibr" coords="2,372.42,276.66,11.32,10.91" target="#b3">[4]</ref>. However, in mathematical language tasks, there have been only a few similar problems.</p><p>This paper has implemented the formula embedding approach, which encodes the formula into the embedded vector. For encoding the formula, we have used pre-trained bidirectional encoder representations from transformers model. The proposed embedding model takes the L A T E X formula as input and produces an output as a fixed dimensional embedding representation. Furthermore, the embeddings of the formulae &amp; the queried formula are compared, and cosine similarity is estimated. The performance of the proposed approach has been tested using a math stack exchange corpus of ARQMath 2020, and obtained results have shown a remarkable contribution in the task of formula retrieval.</p><p>The paper is structured as follows: Section 2 describes the prior work related to MIR domain. Section 3 gives a detailed account of the dataset. Section 4 provides detailed description about the system architecture. Section 5 &amp; 6 describes the experimental setup and results. Section 7 concludes with summary and directions of further research and developments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Mathematical information retrieval is described as the low-hanging fruit of mathematical knowledge management, and people who come from the retrieval community have addressed it in several papers. In mathematical information retrieval, formulae and text both are important factors to achieve the state-of-the-art results and satisfy the user's need based on the formula-based query or text-based query or both (formula+text). For example, the variable typing approach <ref type="bibr" coords="2,89.29,579.17,13.00,10.91" target="#b4">[5]</ref> has assigned the meaning to variables based on the text contained in the same sentence. Types are multi-word phrases which normally used to indicate mathematical terminologies such as objects (e.g., "set"), algebraic structures (e.g., "monoid"), and instantiable notions (e.g., "cardinality of a set"). To evaluate the contribution of variable typing approach, two baseline system i.e.nearest type &amp; the SVM proposed by <ref type="bibr" coords="2,298.42,633.36,12.68,10.91" target="#b5">[6]</ref>[7] and three newly proposed approaches i.e. extended version of SVM baseline, convolutional neural network, and bidirectional LSTM <ref type="bibr" coords="2,480.61,646.91,12.69,10.91" target="#b7">[8]</ref> <ref type="bibr" coords="2,493.30,646.91,12.69,10.91" target="#b8">[9]</ref> have been employed. Among these approaches, the bidirectional LSTM achieved remarkable results. The signature-based hash indexing approach <ref type="bibr" coords="3,333.56,86.97,18.07,10.91" target="#b9">[10]</ref> appears to be a more appropriate alternative to text-based models. In this approach, mathematical formulae have been extracted from scientific documents and transformed to structure encoded strings (SES). These strings served as the input for the hash-based indexing scheme, which aimed to converts these SES into a bit vector/signatures. Finally, these bit vectors have been compared with the user-entered query, and relevant results have been retrieved.In the formula-based engine, the vector-based approach shown remarkable performance. For instance, a Binary Vector Transformation of Math Formula (BVTMF) <ref type="bibr" coords="3,173.14,181.81,17.91,10.91" target="#b10">[11]</ref> has attained Presentation MathML formulae from documents and constructs the fairly large-sized binary vectors where '0's represents absence and '1's represents the presence of a particular entity in the formula. The generated formula vector has representative of the information content of the corresponding formula. Moreover, for indexing and searching textual contents, the system relies on Apache Lucene<ref type="foot" coords="3,325.67,234.25,3.71,7.97" target="#foot_0">1</ref> . Text and math search results retrieved have been re-ranked to prioritize the results containing text and math components of the user query. Motivated from promising performances of the LSTM for sequence-to-sequence tasks, an LSTM based Formula Entailment (LFE) approach <ref type="bibr" coords="3,304.93,276.66,17.78,10.91" target="#b11">[12]</ref> has successfully identified the entailment between the formula-based user query and formulae contained in the scientific documents. The LFE approach has been trained and validated using a symbol level Math Formula Entailment (MENTAIL) dataset.</p><p>At ARQMath-2020 <ref type="bibr" coords="3,185.99,330.85,17.94,10.91" target="#b12">[13]</ref> formula search task, our earlier system titled "variable size formula embedding approach" <ref type="bibr" coords="3,191.08,344.40,18.06,10.91" target="#b13">[14]</ref> transformed the formula (Presentation MathML Format) into the variable size vector where each bit of vector represents their occurrence and corresponds to their position in Bit Position Information Table (BPIT). At ARQMath-2020 <ref type="bibr" coords="3,422.25,371.50,16.27,10.91" target="#b12">[13]</ref>, the DPRL has one of the well-performed research teams from Pattern Recognition Lab of Rochester Institute of Technology, which introduced the Tangent-CFT system <ref type="bibr" coords="3,351.63,398.60,16.26,10.91" target="#b14">[15]</ref>. The Tangent-CFT system has used both the Symbol Layout Tree (SLT) and Operator Tree (OT) representations of formula to consider both the appearance and the syntax of formulas. Tangent+CFT has the extension of the Tangent-CFT embedding model in which each formula has two vector representations: Formula Vector: Vector representation obtained by Tangent-CFT system where vector size is 300. Text Vector: Vector representation obtained by considering the formula as a word and trained the fastText model on the surrounding words of the formula. The vector size of 100 is the fastText default value. Moreover, team MIRMU <ref type="bibr" coords="3,264.54,493.44,18.01,10.91" target="#b15">[16]</ref> has designed the two systems named Soft Cosine Measure (SCM) and Formula2Vec. The SCM system combines TF-IDF with unsupervised word embeddings to produce interpretable representations of math documents and math formulae that enable fast information retrieval. In Formula2Vec, documents and formulae are represented by document and formula embeddings produced by training the Doc2Vec DBOW model on text and math data.</p><p>The mathematical formulae are diverse in nature in terms of syntax and semantic. To integrate this feature in a mathematical information retrieval system, HFS (Hesitation Fuzzy Sets) and BERT (Bidirectional Encoder Representations from Transformer) have examined the mathematical formula and calculates the membership degree of symbolic multi-attributes. With the extraction of the text of the formula, BERT has been used to calculate the context similarity. Then, the documents have been ranked according to the similarity of context &amp; formula, and the final retrieval result has been obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Corpus Description</head><p>The ARQMath-2021 task <ref type="bibr" coords="4,202.31,272.19,17.95,10.91" target="#b12">[13]</ref> provided the formula-based corpus, which is collected from the knowledge-sharing platform, i.e., Math Stack Exchange (MSE). The provided dataset comprised the formulas extracted from the question, answer, and comment posts. In the dataset, the formulas are represented in three different formats, i.e., L A T E X, Presentation MathML, and Content MathML format. The number of formulas comprised in L A T E X, Presentation MathML, and Content MathML formats is 28320920, 26075012, and 25366913 of size 1.5 GB, 11.5 GB, and 10.9 GB respectively. Each format has five distinct attributes, namely formula_id, post_id, thread_id, type, and formula. The metadata about the formula dataset is shown in Table <ref type="table" coords="4,482.83,367.04,3.74,10.91" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">System Architecture</head><p>In ARQMath-2021, we have submitted a total of four runs, one for the topics provided in the ARQMath-2020 and the remaining three-run for the topics provided in the ARQMath-2021.</p><p>The run submitted for the ARQMath-2020 topics is based on a newly designed BERT-based formula embedding approach. Moreover, the first run of the ARQMath2-2021 has been obtained from our earlier system <ref type="bibr" coords="4,197.11,479.86,17.95,10.91" target="#b13">[14]</ref> that we have designed in ARQMath-2020. In addition to this, the second and third runs of the ARQMath2-2021 have been obtained from the BERT-based formula embedding approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Our Earlier System</head><p>We have designed the variable-size vector based approach in ARQMath-2020, which is motivated from the existing Bit Position Information Table (BPIT) <ref type="bibr" coords="4,338.58,570.23,17.94,10.91" target="#b16">[17]</ref> and Term-Document matrix <ref type="bibr" coords="4,487.32,570.23,16.28,10.91" target="#b17">[18]</ref>. The prime objective of this system is to transform the formula (Presentation MathML format) into the variable size vector. Each weight of the vector represents the occurrence count of a particular entity in a formula and corresponds to entity position in BPIT <ref type="bibr" coords="4,416.99,610.88,16.32,10.91" target="#b16">[17]</ref>. The process of formula to variable-size vector transformation is shown in Figure <ref type="figure" coords="4,373.03,624.43,3.66,10.91" target="#fig_0">1</ref>. In the vector transformation process, entities in a formula are categorized into three categories based on the tags. The entities tagged by &lt;mi&gt; comes under the first category, entities tagged by &lt;mo&gt; comes under the second category, and the third category holds the essential MathML tags, which contribute to After a successful formula transformation into the variable size vector, the indexer module indexed the formula vector into an index. Each index stored the three different fields, namely embedded formula vector, formula id, and post id from which the formula is originated. Hereafter, the searcher module compared the query formula vector with all the indexed formula vectors and computed the similarity for each indexed formula. For similarity calculation, the system compared each bit of query vector with the bits of formula vector, and those formulas contained the maximum number of similar bits that formula have maximum similarity score. Based on the maximum similarity score, the system retrieves the top-k formulas with respect to the user-entered query formula.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Proposed System in ARQMath2</head><p>Word embedding is one of the most common text vocabulary representations. It captures the meaning of words, semantic &amp; syntactic correlation, and similarity within the words. Word embeddings describe the word in low dimensional vector form, and to obtain this, an appropriate composition function is required. The composition function is a mathematical framework that combines multiple words into a single vector. The prior research works have witnessed that the embeddings of longer size input strings or sentences achieved excellent performance in the Semantic Textual Similarity (STS) <ref type="bibr" coords="6,260.90,374.68,16.39,10.91" target="#b18">[19]</ref>. Motivated from this, we have performed formula embedding, which encodes the formulas to embedding vectors. For encoding the formula, we have used bidirectional encoder representations from transformers (BERT) model. The workflow of the BERT-based formula embedding model is shown in Figure <ref type="figure" coords="6,379.19,415.33,3.74,10.91" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Preprocessing</head><p>Formulas in L A T E X like 𝐴<ref type="foot" coords="6,199.89,463.43,4.23,6.99" target="#foot_1">2</ref> + 𝐵 2 and 𝑎 2 + 𝑏 2 have same syntactic and semantic meaning but when not converted to the lower case those two are represented as two different formulas in the vector space model 2 . To handle this, we have converted all formulas into the lower case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Formula Embedding</head><p>After preprocessing of the formulas, the preprocessed formulas are fed into the BERT model. The BERT model takes the preprocessed L A T E X formula as input and produces an output as a fixed dimensional embedding vector. Furthermore, the embedding vectors of the formulae and embedding vector of the queried formula is compared and computes the cosine similarity <ref type="bibr" coords="6,89.29,595.28,16.99,10.91" target="#b19">[20]</ref> <ref type="bibr" coords="6,106.28,595.28,16.99,10.91" target="#b20">[21]</ref>. Based on the highest similarity score, the top-K formulas have been retrieved. BERT: Bidirectional Encoder Representations from Transformers: BERT <ref type="bibr" coords="6,440.14,608.83,17.76,10.91" target="#b21">[22]</ref> is a contextindependent word representational model based on the masked language model and pre-trained using bidirectional transformers <ref type="bibr" coords="6,241.90,635.93,16.42,10.91" target="#b22">[23]</ref>. BERT uses a masked language model that predicts randomly masked words in a sequence and hence can be used for learning bidirectional representations. Also, it obtains state-of-the-art performance on most NLP tasks while requiring minimal task-specific architectural modification. As the BERT integrates the information from bidirectional representations, we believed that such bidirectional representations are crucial in mathematical information retrieval as complex relationships between mathematical terms often exist in scientific documents. Primarily, the BERT model pretrained using English Wikipedia and BooksCorpus and proposing new pre-training objectives: the Masked Language Model (MLM) and Next-sentence Prediction (NSP). The MLM task randomly replaces 15% of the input words into "masked" tokens and predicts them; 80% and 10% of the masked tokens are replaced by [MASK] and random tokens, respectively, while 10% remains unchanged. The BERT model enables contextualized formula embedding by learning deep bidirectional representations through the MLM task. The proposed formula embedding module is mapping formulas into vectors to enable computers to understand mathematical language. The NSP task determines whether two formulas are mathematically associated. Through this module, BERT learned to understand the relationship among formulas and mapped their similarities. Several BERT models are available depending on their size, including BERT-Base (12 layers, 768 hidden size, 12 attention heads, and 110 million parameters) and BERT-Large (24 layers, 1024 hidden size, 16 attention heads, and 340 million parameters). In this work, we have used the BERT-Base model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental setups</head><p>The experimental platform is a standalone Ubuntu 18.04 desktop to validate our claim. The configuration of the experimental environment is demonstrated in Table <ref type="table" coords="7,415.85,586.22,3.76,10.91" target="#tab_1">2</ref>. At the time of the experiment, we have carefully validated our approach and avoided any kind of noise. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Results</head><p>The experimental results of the BERT-based formula embedding approach revealed several characteristics of mathematical formulae, which indicated that the natural language embedding models are potentially useful for the formula embedding task. The results value for the topics released in ARQMath and ARQMath2 are shown in Table <ref type="table" coords="8,347.88,262.66,3.75,10.91" target="#tab_2">3</ref>. The obtained results have shown that the pre-trained embedding model can handle the mathematical representation and able to preserve their syntactic meaning. In ARQMath-2021, we have submitted a total of four runs, one for the topics provided in the ARQMath-2020 and the remaining three-runs for the topics provided in the ARQMath-2021. The run submitted for the ARQMath-2020 topics is based on the BERT-based formula embedding approach, which has been trained only on the 20 million L A T E X formula. Moreover, the first run of the ARQMath2-2021 has been obtained from our earlier system <ref type="bibr" coords="8,124.07,357.51,18.07,10.91" target="#b13">[14]</ref> that we have designed in ARQMath-2020. This system has used the formula in Presentation MathML format, which is almost 26075012. In addition to this, the second and third runs of the ARQMath2 have been obtained from the BERT-based formula embedding approach. The second run has been trained only on the 10 million L A T E X formula, and the third run has been trained only on the 20 million L A T E X formula. The reasons behind these formula selections are inaccessible computing power and unavailability of training time.</p><p>The aim of the test queries is to verify the properties of math-aware search engine like retrieval of sub-formula, parent-formula, similar formula and nearly-similar formula which are briefly explained as follows:</p><p>• Sub-formula is a part of a formula. For example, the obtained results of query B.48, which are depicted in Table <ref type="table" coords="8,211.05,504.78,5.02,10.91">4</ref> where BERT-based formula embedding model effectively handles the retrieval of sub-formula. • Parent formula is a formula that holds the existence of the queried formula. For example, the obtained results of query B.30 shown the evaluation of the parent formula search for queried formula 𝑎 3 + 𝑏 3 + 𝑐 3 -3𝑎𝑏𝑐. • The nearly-similar formula is a formula that has a similar meaning to the formula in the dataset. For example, the obtained results of the queried formula B.12 depict the retrieval accuracy of the nearly-similar search. • A similar formula is a formula which is semantically similar or considerably similar to a queried formula, such as retrieved results of query B.60.</p><p>Table <ref type="table" coords="8,127.45,656.03,5.16,10.91">4</ref> shows the sample test queries with their relevant formulas present in MSE corpus. The relevant formulas are ordered by their similarities to the queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4 Retrieved Search Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query ID</head><p>Query Formula Retrieved formula B.12 </p><formula xml:id="formula_0" coords="9,103.95,119.45,347.24,125.08">(1 + 𝑖 √ 3) 1/2 (2 + √ 2) 1/3 (1 - √ 3𝑖) 1/2 (1 + 𝑖 √ 3)/2 (-1 + √ 3𝑖)/2 (-1/2 + √ 3𝑖/2) B.30 𝑎 3 + 𝑏 3 + 𝑐 3 -3𝑎𝑏𝑐 𝑎 3 + 𝑏 3 + 𝑐 3 -3𝑎𝑏𝑐 𝑎 3 + 𝑏 3 + 𝑐 3 -3𝑎𝑏𝑐? 𝑛 = 𝑎 3 + 𝑏 3 + 𝑐 3 -3𝑎𝑏𝑐 3|(𝑎 3 + 𝑏 3 + 𝑐 3 -3𝑎𝑏𝑐) 𝑝 = 𝑎 3 + 𝑏 3 + 𝑐 3 -3𝑎𝑏𝑐 B.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions and Future Scope</head><p>Mathematical information retrieval has been researched consciously in recent years, and there have been many productive results. In this paper, we have reported the contribution of our BERT-based formula embedding model in ARQMath2-2021. The proposed BERT-based model trained on the math exchange corpus of the ARQMath where it takes the L A T E X formulas as input and produced the fixed dimensional embeddings for the same. For similarity computation, cosine similarity has been used. The experimental results support the hypothesis that a pre-trained model for natural language processing tasks positively responds to mathematical language processing tasks. The obtained results have shown that the proposed model has better retrieval accuracy than our earlier system and shows a remarkable contribution.</p><p>In future studies, we will integrate the textual information with a formula to achieve better retrieval accuracy for semantically similar formulas. Moreover, we aim to test other recently developed embedding models like Sentence Deep Bidirectional Transformers (SBERT) and Deep Contextualized Word Representations (ELMo), which are computationally more expensive but have the better embedding formation ability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,89.29,323.52,182.27,8.93;5,183.01,84.09,226.34,226.87"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Formula to Vector Transformation</figDesc><graphic coords="5,183.01,84.09,226.34,226.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,89.29,323.53,273.25,8.93;6,140.48,84.19,311.83,226.77"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Workflow Diagram of the Proposed BERT-Based System</figDesc><graphic coords="6,140.48,84.19,311.83,226.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.99,90.49,362.33,98.73"><head>Table 1</head><label>1</label><figDesc>Math Stack Exchange Corpus Description</figDesc><table coords="4,143.95,118.53,307.38,70.69"><row><cell>Corpus</cell><cell>Math Stack Exchange ARQMath-2020</cell></row><row><cell>Type</cell><cell>Formula</cell></row><row><cell>Formats</cell><cell>L A T E X, Presentation MathML, and Content MathML</cell></row><row><cell>Size</cell><cell>1.5 GB, 11.5 GB and 10.9 GB</cell></row><row><cell>No. of Formulas</cell><cell>28320920, 26075012 and 25366913</cell></row><row><cell>No. of Test Queries</cell><cell>45 (ARQMath) and 60 (ARQMath2)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,88.99,90.49,349.00,168.86"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="7,89.29,102.49,348.70,156.86"><row><cell>Experimental Environment</cell><cell></cell></row><row><cell>Name</cell><cell>Features</cell></row><row><cell>CPU</cell><cell>Intel(R) Xeon(R) W-2155 CPU @ 3.30GHz</cell></row><row><cell>Number of CPU</cell><cell>20</cell></row><row><cell>L1d cache</cell><cell>32K</cell></row><row><cell>L1i cache</cell><cell>32K</cell></row><row><cell>L2 cache</cell><cell>1024K</cell></row><row><cell>L3 cache</cell><cell>14080K</cell></row><row><cell>RAM</cell><cell>64 GB</cell></row><row><cell>Operating System</cell><cell>Ubuntu 18.04 LTS</cell></row><row><cell>HDD</cell><cell>2 TB</cell></row><row><cell>Programming Language</cell><cell>Python</cell></row><row><cell>Version of the language</cell><cell>3.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,88.99,90.49,560.64,82.95"><head>Table 3</head><label>3</label><figDesc></figDesc><table coords="8,89.29,102.49,560.34,70.95"><row><cell>Evaluation Results</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Query</cell><cell>Approach</cell><cell cols="4">Data nDCG´MAP´P@10Á</cell></row><row><cell>RQMath-2020</cell><cell>Formula Embedding</cell><cell>Math</cell><cell>0.233</cell><cell>0.140</cell><cell>0.271</cell></row><row><cell></cell><cell cols="2">Variable-Size Vector Based Approach Math</cell><cell>0.091</cell><cell>0.032</cell><cell>0.151</cell></row><row><cell>ARQMath2-2021</cell><cell>Formula Embedding_A</cell><cell>Math</cell><cell>0.114</cell><cell>0.039</cell><cell>0.152</cell></row><row><cell></cell><cell>Formula Embedding_P</cell><cell>Math</cell><cell>0.161</cell><cell>0.059</cell><cell>0.197</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,103.95,221.92,395.05,141.06"><head></head><label></label><figDesc>48 (𝑥 + 𝑦) 𝑘 ≥ 𝑥 𝑘 + 𝑦 𝑘 𝑥 𝑘 ≥ 𝑦 𝑘 (𝑥 𝑘 + 𝑦 𝑘 ) &lt; (𝑥 + 𝑦) 𝑘 (𝑥 + 𝑦) 𝑘 ⩾ 0 (𝑥 + 𝑦) 𝑘 ⩾ 𝑥 + 𝑘𝑦 (𝑥 + 𝑦) 𝑛 ≥ 𝑥 + 𝑛𝑦 B.60 lim 𝑛→∞ 𝑎 𝑛 lim sup 𝑛→∞ 𝐴 𝑛 𝑙𝑖𝑚 𝑛→∞ 𝑎 𝑛 = 𝐿 lim sup 𝑛→∞ 𝑎 𝑛 𝑙𝑖𝑚 𝑛→∞ 𝑎 𝑛 lim 𝑛→∞ 𝑎 𝑛 = 0 B.80 ∅, {1}, {2}, {1, 2}, {3}, {1, 3}, {2, 3}, {1, 2, 3}, {4}, . . . ∅, {1}, {2}, {1, 2}, {3}, {1, 3}, {2, 3}, {1, 2, 3}, {4}, . . . ∅, {1}, {2}, {3}, {1, 2}, {1, 3}, {2, 3}, {1, 2, 3} ∅, {1}, {2}, {3}, {1, 2}, {1, 3}, {2, 3}, {1, 2, 3}. {{1}, {2}, {3}, {1, 2}, {1, 3}, {3, 2}, {1, 2, 3}, ∅} ∅, {1}, {2}, {3}, {1, 2}, {1, 3}, {2, 3}, 𝑆</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,108.93,671.02,91.41,8.97"><p>http://lucene.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,108.93,671.01,246.51,8.97"><p>https://thehelloworldprogram.com/python/python-string-methods/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors would like to express gratitude to the <rs type="institution">Department of Computer Science and Engineering and Center for Natural Language Processing, National Institute of Technology Silchar, India</rs> for providing infrastructural facilities and support.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="10,112.66,145.59,393.33,10.91;10,112.66,159.14,164.91,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,206.11,145.59,220.92,10.91">A survey on retrieval of mathematical knowledge</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Guidi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">S</forename><surname>Coen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,436.14,145.59,69.85,10.91;10,112.66,159.14,80.98,10.91">Mathematics in Computer Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="409" to="427" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,172.69,393.33,10.91;10,112.66,186.24,296.19,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,208.50,172.69,166.12,10.91">An approach to math-similarity search</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Youssef</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,396.62,172.69,109.37,10.91;10,112.66,186.24,165.17,10.91">International Conference on Intelligent Computer Mathematics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="404" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,199.79,393.32,10.91;10,112.66,213.34,105.08,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,241.24,199.79,184.50,10.91">How to generate a good word embedding</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,434.90,199.79,71.09,10.91;10,112.66,213.34,36.36,10.91">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="5" to="14" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,226.89,394.61,10.91;10,112.66,240.44,393.33,10.91;10,112.66,253.99,133.24,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,256.36,226.89,232.31,10.91">A deep relevance matching model for ad-hoc retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,112.66,240.44,393.33,10.91;10,112.66,253.99,54.62,10.91">Proceedings of the 25th ACM international on conference on information and knowledge management</title>
		<meeting>the 25th ACM international on conference on information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,267.54,393.33,10.91;10,112.66,281.08,393.33,10.91;10,112.66,294.63,394.53,10.91;10,112.30,308.18,191.80,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,298.04,267.54,207.95,10.91;10,112.66,281.08,93.21,10.91">Variable typing: Assigning meaning to variables in mathematical text</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Stathopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Teufel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,232.42,281.08,273.56,10.91;10,112.66,294.63,389.93,10.91">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="303" to="312" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="10,112.66,321.73,393.33,10.91;10,112.66,335.28,393.32,10.91;10,112.48,348.83,85.04,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,394.86,321.73,111.13,10.91;10,112.66,335.28,197.44,10.91">Extracting definitions of mathematical expressions in scientific papers</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">Y</forename><surname>Kristianto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-Q</forename><surname>Nghiem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Matsubayashi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,333.06,335.28,172.92,10.91;10,112.48,348.83,17.62,10.91">Proc. of the 26th Annual Conference of JSAI</title>
		<meeting>of the 26th Annual Conference of JSAI</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,362.38,393.61,10.91;10,112.66,375.93,393.33,10.91;10,112.66,389.48,390.27,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,288.46,362.38,217.81,10.91;10,112.66,375.93,285.10,10.91">Exploiting textual descriptions and dependency graph for searching mathematical expressions in scientific papers</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">Y</forename><surname>Kristianto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Topić</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,420.60,375.93,85.38,10.91;10,112.66,389.48,276.05,10.91">Ninth International Conference on Digital Information Management (ICDIM 2014)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="110" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,403.03,393.32,10.91;10,112.66,416.58,308.72,10.91" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01360</idno>
		<title level="m" coord="10,413.78,403.03,92.20,10.91;10,112.66,416.58,126.53,10.91">Neural architectures for named entity recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,430.13,393.33,10.91;10,112.66,443.67,211.84,10.91" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Crichton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Pyysalo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04361</idno>
		<title level="m" coord="10,271.07,430.13,234.92,10.91;10,112.66,443.67,29.25,10.91">Attending to characters in neural sequence labeling models</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,457.22,394.53,10.91;10,112.28,470.77,378.03,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,188.83,457.22,313.56,10.91">Mathematical document retrieval system based on signature hashing</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,112.28,470.77,309.32,10.91">Aptikom Journal on Computer Science and Information Technologies</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="45" to="56" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,484.32,393.53,10.91;10,112.66,497.87,393.98,10.91;10,112.41,511.42,48.96,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,275.45,484.32,230.73,10.91;10,112.66,497.87,159.51,10.91">Binary vector transformation of math formula for mathematical information retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pakray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,283.43,497.87,178.39,10.91">Journal of Intelligent &amp; Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="4685" to="4695" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,524.97,394.61,10.91;10,112.66,538.52,393.32,10.91;10,112.66,552.07,185.47,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,243.68,524.97,244.08,10.91">Lstm neural network based math information retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pakray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,135.64,538.52,370.34,10.91;10,112.66,552.07,90.70,10.91">Second International Conference on Advanced Computational and Communication Paradigms (ICACCP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,565.62,393.33,10.91;10,112.66,579.17,393.33,10.91;10,112.66,592.72,319.84,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,333.89,565.62,172.09,10.91;10,112.66,579.17,163.51,10.91">Overview of arqmath 2020: Clef lab on answer retrieval for questions on math</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,297.91,579.17,208.08,10.91;10,112.66,592.72,188.80,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="169" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,606.27,393.33,10.91;10,112.66,619.81,393.33,10.91;10,112.66,633.36,107.76,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,288.81,606.27,217.17,10.91;10,112.66,619.81,98.05,10.91">An analysis of variable-size vector based approach for formula searching</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dadure</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pakray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bandyopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,239.53,619.81,266.45,10.91;10,112.66,633.36,77.06,10.91">Working Notes of CLEF 2020-Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,646.91,393.33,10.91;10,112.66,660.46,323.20,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,271.28,646.91,175.61,10.91">Dprl systems in the clef 2020 arqmath lab</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,467.81,646.91,38.18,10.91;10,112.66,660.46,292.51,10.91">Working Notes of CLEF 2020-Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,86.97,393.33,10.91;11,112.66,100.52,393.33,10.91;11,112.66,114.06,207.15,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,314.63,86.97,191.35,10.91;11,112.66,100.52,127.61,10.91">Three is better than one: Ensembling math information retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Novotnỳ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sojka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Štefánik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lupták</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,248.87,100.52,257.11,10.91;11,112.66,114.06,79.94,10.91">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,127.61,393.33,10.91;11,112.66,141.16,237.50,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,267.04,127.61,238.95,10.91;11,112.66,141.16,35.82,10.91">A formula embedding approach to math information retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pakray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,156.38,141.16,109.84,10.91">Computación y Sistemas</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="819" to="833" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,154.71,393.33,10.91;11,112.28,168.26,160.61,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="11,276.51,154.71,140.08,10.91">Term-document representation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Anandarajan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Nolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,443.51,154.71,62.48,10.91;11,112.28,168.26,40.23,10.91">Practical Text Analytics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="61" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,181.81,393.33,10.91;11,112.66,195.36,201.83,10.91;11,332.47,195.36,174.72,10.91;11,112.66,208.91,80.57,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="11,322.93,181.81,183.06,10.91;11,112.66,195.36,75.74,10.91">Semeval-2012 task 6: A pilot on semantic textual similarity</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,213.94,195.36,100.55,10.91;11,332.47,195.36,169.88,10.91">First Joint Conference Lexical and Computational Semantics</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="385" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,222.46,393.33,10.91;11,112.66,236.01,385.70,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="11,280.28,222.46,111.77,10.91">Semantic cosine similarity</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rahutomo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kitasuka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aritsugi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,413.42,222.46,92.57,10.91;11,112.66,236.01,288.25,10.91">The 7th International Student Conference on Advanced Science and Technology ICAST</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,249.56,395.17,10.91;11,112.66,263.11,393.32,10.91;11,112.66,276.66,189.82,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="11,293.93,249.56,213.90,10.91;11,112.66,263.11,165.99,10.91">An empirical analysis on retrieval of math information from the scientific documents</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dadure</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pakray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bandyopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,302.61,263.11,203.37,10.91;11,112.66,276.66,101.67,10.91">International Conference on Communication and Intelligent Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="301" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,290.20,393.33,10.91;11,112.66,303.75,363.59,10.91" xml:id="b21">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="11,353.43,290.20,152.55,10.91;11,112.66,303.75,181.08,10.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,317.30,395.17,10.91;11,112.66,330.85,326.92,10.91" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m" coord="11,149.36,330.85,107.76,10.91">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
