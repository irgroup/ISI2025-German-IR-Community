<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.69,84.74,358.70,15.42;1,88.69,106.66,404.48,15.42;1,89.29,128.58,284.45,15.43">Approach Zero and Anserini at the CLEF-2021 ARQMath Track: Applying Substructure Search and BM25 on Operator Tree Path Tokens</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.72,156.89,54.12,11.96"><forename type="first">Wei</forename><surname>Zhong</surname></persName>
							<email>w32zhong@uwaterloo.ca</email>
						</author>
						<author>
							<persName coords="1,155.48,156.89,64.68,11.96"><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName coords="1,232.80,156.89,27.69,11.96"><forename type="first">Ji</forename><surname>Xin</surname></persName>
							<email>ji.xin@uwaterloo.ca</email>
						</author>
						<author>
							<persName coords="1,273.14,156.89,78.09,11.96"><forename type="first">Richard</forename><surname>Zanibbi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rochester Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,382.23,156.89,51.16,11.96"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
							<email>jimmylin@uwaterloo.ca</email>
						</author>
						<author>
							<persName coords="1,92.44,182.88,63.71,7.99"><forename type="first">David</forename><forename type="middle">R</forename><surname>Cheriton</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.69,84.74,358.70,15.42;1,88.69,106.66,404.48,15.42;1,89.29,128.58,284.45,15.43">Approach Zero and Anserini at the CLEF-2021 ARQMath Track: Applying Substructure Search and BM25 on Operator Tree Path Tokens</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">52A0640B7B4ACCD515B094D72029C2AB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Math Information Retrieval</term>
					<term>Math-aware search</term>
					<term>Math formula search</term>
					<term>Community Question Answering (CQA)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper reports on substructure-aware math search system Approach Zero that is applied to our submission for ARQMath lab at CLEF 2021. We have participated in both Task 1 (math ARQ) and Task 2 (formula retrieval) this year. In addition to substructure retrieval, we have added a traditional full-text search pass based on the Anserini toolkit <ref type="bibr" coords="1,285.31,268.92,9.39,8.97" target="#b0">[1]</ref>. We use the same path features extracted from Operator Tree (OPT) to index and retrieve math formulas in Anserini, and we interpolate Anserini results with structural results from Approach Zero. Automatic and table-based keyword expansion methods for math formulas have also been explored. Additionally, we report preliminary results from using previous years' labels and applying learning to rank for our first-stage search results. In this lab, we obtain the most effective search results in Task 2 (formula retrieval) among submissions from 7 participants including the baseline system. Our experiments have also shown a great improvement over the baseline result we produced from previous year.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The domain of Mathematics Information Retrieval (MIR) has been actively studied over recent years. It covers information retrieval in documents where math language presents. Traditional IR technologies have not addressed the special problems in MIR, including retrieval for structured math formulas or expressions where commutativity and symbol substitution may occur. MIR may also require understanding of math knowledge such as concept-to-formula association, higher level math semantics, and equivalent formula transformations, in order to create effective systems for math-aware text retrieval.</p><p>The ARQMath Lab <ref type="bibr" coords="1,191.70,552.91,11.49,10.91" target="#b1">[2,</ref><ref type="bibr" coords="1,207.00,552.91,9.03,10.91" target="#b2">3]</ref> addresses the problems of MIR under a Community Question Answering (CQA) setting. It utilizes user-generated data from the Math StackExchange (MSE) website as collection, and task topics are extracted from real-world math Q&amp;A threads. The corpus contains over 1 million math-related questions and about 1. <ref type="bibr" coords="1,383.02,593.55,4.11,10.91" target="#b3">4</ref>  over 17 million math formulas or notations. The data collection covers MSE threads from 2010 to 2018, and task topics are selected from MSE questions of 2019 (for ARQMath-2020 <ref type="bibr" coords="2,448.90,230.30,11.90,10.91" target="#b1">[2]</ref>) and 2020 (for ARQMath-2021 <ref type="bibr" coords="2,178.99,243.85,11.03,10.91" target="#b2">[3]</ref>). A main task (CQA Task, or Task 1) and a secondary formula retrieval task (Task 2) are included in this lab. Participants are able to leverage math notations together with its (text) context to retrieve relevant post answers. For Task 1, complete answers are available for applying full-text retrieval, but participants are also allowed to utilize structured formulas in the documents. On the other hand, formula retrieval in Task 2 is about identifying similar formulas in the document that is related to the topic question. The formula retrieval task specifies query formula with its question post, and optionally, participant could use contextual information around the topic formula in the question post. Both tasks ask participants to return up to five runs (one primary run and four alternative runs) that contain relevant post answers to the given question topic. Relevance judgement will be received for primary runs and selected results of alternative runs from the submission pool. Official evaluation metrics include NDCG' <ref type="bibr" coords="2,158.62,392.89,11.28,10.91" target="#b3">[4]</ref>, MAP', and P'@10, where MAP' and P'@10 use H+M binarization (hits with relevance score ≥ 2 are considered as relevant, and relevance levels are collapsed into binary). NDCG', MAP', and P'@K are identical to their corresponding standard measurements except that unjudged hits are removed before metric computation. Relevance is scored on a graded scale, from 0 (irrelevant) to 3 (highly relevant). We submitted 5 runs for both tasks. Our system for this ARQMath lab is based on a structureaware search system Approach Zero <ref type="bibr" coords="2,259.19,474.19,11.48,10.91" target="#b4">[5,</ref><ref type="bibr" coords="2,273.75,474.19,9.03,10.91" target="#b5">6]</ref> and a full-text retrieval system Anserini <ref type="bibr" coords="2,471.64,474.19,11.58,10.91" target="#b0">[1]</ref>. We adapted a two-pass search architecture for most of our submitted runs. In the Approach Zero pass, a substructure matching approach is taken to assess formula similarity where the largest common subexpression from formulas is obtained and we use the maximum matched subtree to compute their structure similarity. Symbol similarity is further calculated with the awareness of symbol substitutions in math formulas. The similarity metric used by Approach Zero is easily interpretable and it may serve better the needs of identifying highly structured mathematical formulas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>million answers, including</head><p>As illustrated by Mansouri et al. <ref type="bibr" coords="2,248.28,582.58,12.84,10.91" target="#b6">[7]</ref> in an example query result (see Table <ref type="table" coords="2,435.46,582.58,3.65,10.91" target="#tab_0">1</ref>), substructure matching and variable name substitution are desired for identifying highly relevant math formulas. Usually, this can be more easily achieved using tree-based substructure search than using full-text search. However, searching math formulas also requires more "fuzzy" match or high-level semantics. In this case, embedding formulas or matching bag-of-word tokens using traditional text retrieval methods (but with careful feature selection) are shown to be effective as well <ref type="bibr" coords="2,165.46,663.88,11.49,10.91" target="#b6">[7,</ref><ref type="bibr" coords="2,180.10,663.88,7.65,10.91" target="#b7">8]</ref>. For example, Tangent-CFT system is able to find document formula 𝑂(𝜆 𝛿 (𝑛) log 𝑛) (not shown in the Table <ref type="table" coords="3,278.82,87.58,4.25,10.91" target="#tab_0">1</ref>) for the example query by considering semantic information, but this formula is hard to be identified by substructure search engine because it shares little common structure feature with the query Operator Tree (OPT).</p><p>In our submission this year, we try to compensate strict substructure matching by introducing a separate pass that performs simple token matching on Operator Tree path tokens. Specifically, we include a full-text search engine Anserini to boost the results of Approach Zero. In the Anserini pass, we use feature tokens extracted from a formula as terms and directly apply full-text retrieval by treating those tokens as normal text terms. The difference between our Anserini pass and other existing bag-of-word math retrieval systems is that we use leaf-root path prefixes from formula Operator Tree representation (See Figure <ref type="figure" coords="3,395.74,210.73,3.55,10.91" target="#fig_0">1</ref>). This representation is the same representation we use to carry structural information for formulas in Approach Zero, but the latter additionally performs substructure matching and variable name substitution in math formulas.</p><p>We further try to improve our system recall by applying query expansion on both text and math query keywords. We investigate RM3 <ref type="bibr" coords="3,279.69,278.47,12.68,10.91" target="#b8">[9]</ref> (for both text and math keywords) and a method using lookup table to extract math keywords from formulas. In addition, we report the result from using previous years' label and applying learning-to-rank methods.</p><p>Our main objectives of experiments for this lab are as follows.</p><p>• Evaluate the effectiveness of treating OPT leaf-root path features as query/index terms.</p><p>• Try different ways to combine results from structure search and traditional bag-of-word matching paradigm. Evaluate the effectiveness of query expansion involving math formulas. • Apply learning-to-rank methods to the ARQMath dataset from previous years' labels and post meta data, and determine its usefulness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Formula Retrieval</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Representation</head><p>In this lab, we adapt an enhanced version of Operator Tree (OPT) representation, trying to improve math formula retrieval recall. As illustrated by an example formula topic in Figure <ref type="figure" coords="3,499.66,500.04,3.76,10.91" target="#fig_0">1</ref>, this representation contains more nodes than typical OPT in the following ways:</p><p>• Always placing an add node on top of a term, this allows matching a path from a single term to another path from a multi-term expression, e.g., 𝑎 and 𝑎 + 𝑏 . • Having an additional sign node (i.e., + and -) on top of each term. They are tokenized into the same token such that it can match another math term even with different signs. It changes the path fingerprint (see Section 2.2) so that we have the information to penalize those paths of different signs. • For any variable, it will place a subsup node (optionally an additional base node) on top of the variable node, even if it comes without a subscript/supscript. This helps to increase recall for cases when subscripted and non-subscripted variables are both commonly used to denote the same math entity, e.g., 𝑘𝑥 and 𝑐 1 𝑥 . Notice that this rule is not applied to constants, as they are not usually subscripted in math notations. When being indexed, a formula OPT will be broken down into linear leaf-root paths, so that they can be treated as normal "terms" to be used in inverted index. Different leafroot paths may end up being the same path tokens after applying tokenization, e.g., path U/base/subsup/+/add/equal and n/base/subsup/+/add/equal will result in the same token path VAR/BASE/SUBSUP/SIGN/ADD/EQUAL since both U and n are tokenized to variable token VAR (a capitalized name indicates it is tokenized). The purpose of tokenizing every node in the path is to improve recall, such that we can find identical equations with different symbol set, as it is frequently the case in math notations.</p><p>In addition to leaf-root path tokens, we also index the prefixes of those path tokens, this is necessary to identify a subexpression from a formula in the document. For example, if we need to find 𝑈 𝑛 = 𝑛 2 + 𝑛 by only querying its subexpression 𝑛 2 + 𝑛 , then all the possible leaf-root path prefixes should also be indexed. To alleviate the cost, one may optionally prune prefix paths which always occur together. For example, the */base path will always follow a */base/subsup path (asterisk denotes any prefix), thus we can remove the former path to reduce index size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Path Symbol Weight</head><p>Tokenization on path boosts recall for formulas, however, we still need original path information to break ties when tokenized paths have matched, e.g., 𝑎 &lt; 0 and 𝑎 ≤ 0 . To address this issue, in this task, we apply a 3-level similarity weight for path-wise matching. More specifically, we use the original operator symbols along the token path to generate a hash value for each path by computing the Fowler-Noll-Vo hash <ref type="bibr" coords="4,261.69,588.03,17.75,10.91" target="#b9">[10]</ref> from the leaf node up to 4 nodes above, and we call this hash value the fingerprint of a path. The fingerprint captures a local symbolic appearance for operators on a path, it can be used to differentiate formulas of the same structure but with different math operator(s).</p><p>Upon scoring the match between a pair of paths, we compare against their leaf node symbol as well as fingerprint value, we will assign the highest path-match weight if both values agree between two paths, a medium weight if leaf symbols match but not the fingerprints, and a lower path-match score if otherwise. A weighted sum of matched paths represents the symbol similarity in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Structure-Based Scoring</head><p>The Approach Zero formula search system takes a tree-based matching approach, and specialized query optimization is applied to match formula substructures during the very first stage of retrieval <ref type="bibr" coords="5,130.72,177.34,11.48,10.91" target="#b5">[6,</ref><ref type="bibr" coords="5,145.48,177.34,7.65,10.91" target="#b4">5]</ref>. The benefit of substructure matching is that the formula similarity score is well-defined and can be interpreted easily. In our case, the structure similarity is previously defined as the number of paths in the maximum matched common subtree <ref type="bibr" coords="5,434.04,204.44,11.59,10.91" target="#b4">[5]</ref>. In this task, we acknowledge the different contribution from paths and apply an IDF weight to a matched formula, defined by the sum of each individual path IDFs:</p><formula xml:id="formula_0" coords="5,228.29,254.25,273.84,34.92">IDF(𝑇 ^𝑞𝑓 ,𝑑 𝑓 ) = ∑︁ 𝑝∈𝑇 ^𝑞𝑓 ,𝑑 𝑓 log 𝑁 𝑝 𝑑𝑓 𝑝 (<label>1</label></formula><formula xml:id="formula_1" coords="5,502.13,260.96,3.86,10.91">)</formula><p>where 𝑝 is the path in the largest common subtree 𝑇 ^𝑞𝑓 ,𝑑 𝑓 of query and document formulas, 𝑑𝑓 𝑝 is the document frequency of path 𝑝, and 𝑁 𝑝 is the total number of paths in the collection. We also incorporate symbol similarity score 𝑆 𝑠𝑦𝑚 (𝑞 𝑓 , 𝑑 𝑓 ) to further differentiate formulas with identical structure but different symbols. This score is only computed in the second stage when structure similarity score is computed and possible to make the hit into top K results. Specifically, we penalize symbol similarity by the length of document formula 𝐿 𝑑 𝑓 :</p><formula xml:id="formula_2" coords="5,145.73,396.60,360.26,30.92">SF(𝑞 𝑓 , 𝑑 𝑓 ) = 1 1 + (1 -𝑆 𝑠𝑦𝑚 (𝑞 𝑓 , 𝑑 𝑓 )) 2 (︃ (1 -𝜂) + 𝜂 1 log(1 + 𝐿 𝑑 𝑓 ) )︃<label>(2)</label></formula><p>where the penalty is determined by parameter 𝜂.</p><p>Given structure similarity and symbol similarity, we adapt the following formula to compute overall similarity for a math formula match:</p><formula xml:id="formula_3" coords="5,194.40,490.33,311.59,14.55">Similarity(𝑞 𝑓 , 𝑑 𝑓 ) = SF(𝑞 𝑓 , 𝑑 𝑓 ) • IDF(𝑇 ^𝑞𝑓 ,𝑑 𝑓 )<label>(3)</label></formula><p>whereas for normal text terms in query, we compute their scores using BM25+ scoring schema <ref type="bibr" coords="5,491.85,516.86,16.09,10.91" target="#b10">[11]</ref>.</p><p>The final score for this pass is then accumulated from math and text keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Text-Based Scoring</head><p>On the other hand, we also add a separate pass in parallel to score document formulas by "bag of paths" (without applying substructure matching). The path set for text-based scoring includes two copies of a path, one with original leaf symbol and another with tokenized leaf symbol, however, both types of paths apply tokenization to operator nodes (See Figure <ref type="figure" coords="5,430.09,620.78,4.97,10.91" target="#fig_2">2</ref> for an example).</p><p>Including original leaf symbol will award exact operand matches, and the fully tokenized leaf paths are included to boost recall and enable us to match expressions with different operand symbols.  Our text-based pass is based on Anserini <ref type="bibr" coords="6,281.35,340.40,11.31,10.91" target="#b0">[1]</ref>, and we apply accurate Lucene BM25 <ref type="bibr" coords="6,462.36,340.40,17.79,10.91" target="#b11">[12]</ref> (with lossless document length) for scoring both text and formula paths, specifically</p><formula xml:id="formula_4" coords="6,156.08,376.66,349.91,30.09">∑︁ 𝑡∈𝑞 log (︂ 1 + 𝑁 -𝑑𝑓 𝑡 + 0.5 𝑑𝑓 𝑡 + 0.5 )︂ • 𝑡𝑓 𝑡,𝑑 𝑡𝑓 𝑡,𝑑 + 𝑘 1 (1 -𝑏 + 𝑏(𝐿 𝑑 /𝐿 𝑎𝑣𝑔 ))<label>(4)</label></formula><p>where 𝑘 1 and 𝑏 are parameters, and 𝑁 , 𝑑𝑓 𝑡 , 𝑡𝑓 𝑡,𝑑 , 𝐿 𝑑 , 𝐿 𝑎𝑣𝑔 refer to the total number of documents, the document frequency of the term 𝑡, the term frequency of term 𝑡 in the document 𝑑, the length of document 𝑑, and the average document length respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Math-Aware Retrieval</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Query Expansion</head><p>In CQA Task, we need to return full-text answer posts as search results. In addition to simply merging results from formula and text retrieval independently, we have identified a few techniques to help map information from one type to another:</p><p>• To make use of information in formulas, we map tokens in L A T E X to text terms so that formula-centered document posts can also be found by querying text keywords. • To utilize the context information in answer posts, we explore query expansion (covering both math and text) based on pseudo relevance feedback to add potentially relevant keywords based on both math and text context.</p><p>In the following sections, we explored two query expansion methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Math Keyword Expansion</head><p>For the purpose of mapping tokens in L A T E X to text, we designed some manual rules to convert a set of L A T E X math-mode commands to text terms. For example, we will expand text term "sine" to the query if a \sin command occurs in formula L A T E X markup. Furthermore, Greek-letter commands in L A T E X are also translated into plain text, e.g., \alpha will be mapped to term "alpha". A specialized L A T E X lexer from our PyA0 package <ref type="bibr" coords="7,339.20,161.73,17.76,10.91" target="#b12">[13]</ref> is used to scan and extract tokens from markup. The list of math keyword expansion mappings we have used in this task is enumerated in Appendix D.</p><p>In order to find more formulas by querying math tokens, we not only expand keywords in query, but also apply math keyword expansion to all document formulas for CQA Task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">RM3 Query Expansion for Mixed Types of Keywords</head><p>In addition to math keyword expansion, we apply RM3 <ref type="bibr" coords="7,335.03,265.26,17.84,10.91" target="#b13">[14]</ref> to expand larger range of possibly relevant terms or formulas from initially retrieved documents. Based on relevance model <ref type="bibr" coords="7,491.82,278.81,11.52,10.91" target="#b8">[9]</ref>, RM3 optimizes the expansion by closing the difference between document language model 𝑄(𝑤|𝐷) and query relevance model 𝑃 (𝑤|𝑅), where random variable 𝑤 represents generated word, 𝐷 and 𝑅 are document and relevant document respectively.</p><p>The objective is then reflected as negative KL divergence After estimating query relevance model from Eq. 6, we further perform an interpolation (using even ratio 𝛼 = 0.5) with maximum likelihood estimate of existing query keywords in order to improve model stability. We use top query keywords from our estimate of 𝑃 (𝑤|𝑅) to query them again in a cascade way. Our parameters for RM3 include: The number of top-𝐾 retrieved results used for estimation, and the number of top query keywords to be selected to query in the second round.</p><formula xml:id="formula_5" coords="7,206.87,356.37,299.12,25.61">-𝐾𝐿(𝑃 |𝑄) ∝ ∑︁ 𝑤∈𝑉 𝑃 (𝑤|𝑅) log 𝑄(𝑤|𝐷)<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning to Rank ARQMath Answers</head><p>We make the assumption that most answer posts are relevant to its linked question post, thus we pair all answer posts with their question posts in the index. To eliminate the consequence from retrieving low-quality answers (i.e., answers irrelevant to its linked question), we apply learning to rank techniques using features such as the number of upvotes for an answer.</p><p>Two learning to rank methods have been explored, i.e., linear regression and LambdaMART <ref type="bibr" coords="8,492.27,161.73,16.09,10.91" target="#b14">[15]</ref>. LambdaMART works by minimizing the cross entropy between pair-wise odds ratio of perfect and actual results, and it is efficient and can be regarded as list-wise learning-to-rank method because it only requires to sample adjacent pairs. Furthermore, it can accumulate the "𝜆" for each document before updating parameters. 𝜆 serves as a nice symmetric connection for the cross entropy w.r.t model parameters. By default, LambdaMART is commonly set up to optimize NDCG measures by multiplying measurement gain directly to pair-wise 𝜆 𝑖,𝑗 <ref type="bibr" coords="8,431.23,243.03,17.91,10.91" target="#b15">[16]</ref> where</p><formula xml:id="formula_6" coords="8,235.31,266.48,270.67,24.83">𝜆 𝑖𝑗 = - 𝑘 1 + 𝑒 𝑘•𝑜 𝑖𝑗 |Δ 𝑁 𝐷𝐶𝐺 |<label>(7)</label></formula><p>𝑘 is a parameter that determines the shape of distribution for probability 𝑝 𝑖𝑗 that a document 𝑖 is ranked higher than 𝑗, and 𝑜 𝑖𝑗 is the likelihood ratio of 𝑝 𝑖𝑗 .</p><p>The following factors are considered to rerank answer posts:</p><p>• Votes (the upvote number): As it is presumably a direct indicator to reflect answer post relevance to its question. • Similarity: The first-phase score for each ranked result (which may be interpolated result from two separate passes, see discussion in Section 2.3 and 2.4). • Tags: The number of tags matched between topic question and linked question of document. In ARQMath lab, each question has been potentially attached some "tags" to indicate the question scope in math terms. Tags are manually labeled by MSE users with a bar on reputation, and they can be a good abstraction for a Q&amp;A thread.</p><p>These features are similar compared to the features proposed by Yin Ki NG et al. <ref type="bibr" coords="8,460.63,471.21,16.41,10.91" target="#b16">[17]</ref>, however, they do not have assessed data available at that time, and they have to mock relevance assessments using indirect indicators (e.g., thread being marked as duplicate by users). Our experiments will be based on direct relevance judgement which is more reliable, accurate and less complicated. Furthermore, we also explore another learning-to-rank method using LambdaMART.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>The official results of our system compared to systems with best results are summarized in Table <ref type="table" coords="8,116.40,611.13,5.17,10.91">9</ref> and 10. System(s) noted "L" in the table are systems applying learning to rank using existing labels (for ARQMath-1, they are trained separately from 45 test topics in Task 2). Systems noted "M" use keywords manually extracted from topic post for querying, <ref type="foot" coords="8,457.70,636.47,3.71,7.97" target="#foot_0">1</ref> while the model execution is still automatic. The same subscripted M letter indicates the same set of topics. Notice that system TU_DBS team uses only text information (noted as "T") for retrieval in Task 1. In addition, although Task 2 asks to submit at most 5 results for each visually unique formula, our index with limited number of visually unique formulas are not available in time, thus our official runs for Task 2 may contain extra results per unique formula (those are marked "F") and it may affect the comparison with other systems (although the official evaluation will remove those extra results, those are holding replaces in returned search results).</p><p>In our runs, a base letter (such as "P", "A" etc.) indicates the set of parameters we have applied in Approach Zero system. Table <ref type="table" coords="9,231.87,195.36,4.97,10.91">8</ref> in Appendix shows the detailed parameters for different base settings. Math path weight is the weight associated to path in matched common subtree, it is used to adjust the contribution importance comparatively to text keyword match; formula 𝜂 shown in Eq. 2 is the penalty applied to over-length formulas; and BM25+ is the scoring used for normal text search in in Approach Zero pass.</p><p>Additionally, we append a number to base letter in our run names to indicate the way it combines results with text-based system Anserini, details can be found in Section 4.5 and 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Task-1 Submission</head><p>In CQA Task, we adapt Lancaster stemmer, an aggressive stemmer that is able to canonicalize more math keywords, e.g. summation will be stemmed to sum whereas other stemmers such as Porter and Snowball will only convert it to summat.</p><p>Our Task-1 results are not quite competitive, however, we observe that text-only retrieval system from TU_DBS team can achieve better results than ours in Task 1. This implies that text retrieval alone in Task 1 plays a crucial role in effectiveness contribution, and a potential gain is anticipated when text retrieval and the way it combines with math search can be further improved in our case.</p><p>In our post experiments, we generate reranked runs for Task 1 by applying Linear Regression and LambdaMART (trees, depth = 7, 5) directly on Approach Zero pass (see Section 4.7), which is trained on all Task-1 judgements from previous year. After applying learning to rank, our post-experiment result is on par with most effective systems in terms of P@10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Task-2 Submission</head><p>In Task 2, two runs C30 and B30 using different base parameters achieve the same scores numerically, they are collapsed into one row in the table. We have also generated similar 5 runs for Task 2 again (having an asterisk on their run names) but with up to 5 results for each visually unique formula. We have further corrected an oversight in our Anserini pass which affects tree path token generation. It turns out our results can be further improved.</p><p>Without using any existing label for training in our official submission, we obtain the best effectiveness across all metrics in Formula Retrieval Task of this year data (ARQMath-2), and according to P'@10 metric, we are able to achieve the highest precision at the very top results in ARQMath-2 by returning results from Approach Zero alone (see run B * ). We attribute this advantage to our structure-aware matching applied to the very first stage of retrieval process. Top-precision systems such as Tangent-S <ref type="bibr" coords="9,281.02,660.64,18.07,10.91" target="#b17">[18]</ref> introduced alignment phase to find matched substructures, and Tangent-CFTED performs tree edit distance calculation in reranking stage. These structure matching methods are too expensive to be applied in the first stage of retrieval.</p><p>Apart of the above results, we have conducted a variety of experiments trying to achieve the objectives listed in Section 1, although we select some best performed runs for submission, we still have made the following attempts in this paper to address those objectives:</p><p>• Explore traditional IR architecture and bag-of-words tokens using Anserini without applying substructure matching. And evaluate these two system using the same set of features extracted from OPT. • Combine text-based approach with tree-based approach using score interpolation as well as search results concatenation, and try a more deeper integration to translate one type of token to another using RM3 and math keyword expansion. • Apply learning-to-rank with ground truth labels from previous year using linear regression and LambdaMART, and evaluate their effectiveness.</p><p>All of our experiments in the following sections will be using previous-year topics for evaluation, since judgement data of this year is not available at the time we write this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Employed Machine Configuration</head><p>Our experiments run on a server machine with the following hardware configuration: Intel(R) Xeon(R) CPU E5-2699 @ 2.20GHz (88 Cores), 1 TB DIMM Synchronous 2400 MHz memory and running on a HDD partitioned with ZFS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Runtimes and Efficiency</head><p>For measuring runtime statistics, both systems are querying tokens extracted from the same representation in a similar way (i.e., by extracting prefix leaf-root paths from Operator Tree representation). Our index contains over 23 million formulas, over 17 millions of which are structured formulas (not single-letter math notations). The statistics of our path tokens per topic is (143.5, 102.5, 110, 400) in (avg, std, med, max). Table <ref type="table" coords="10,128.40,484.76,5.17,10.91">2</ref> reports the query execution time separately from two passes. Compared to our previously published results <ref type="bibr" coords="10,217.65,498.31,11.47,10.91" target="#b4">[5]</ref>, the system we have used in this task has on-disk math index also compressed, that technical improvement has improved system efficiency. However, our query execution times are unable to match Anserini which only performs matching at the token level without aligning substructures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Text and Math Interpolation</head><p>In our substructure search, an uniform weight is associated with each path for scoring, we first investigate how this weight affects overall retrieval effectiveness. We fix the BM25+ parameters (𝑏, 𝑘 1 ) in Anserini pass to (0.75, 1.2), (0.75, 1.5) and (0.75, 2), and change math path weight from 1 to 3.5 with a pace of 0.5. An evaluation on CQA Task is conducted here since this task requires trade-off between text terms and math formulas.</p><p>As seen in Figure <ref type="figure" coords="10,179.54,656.03,3.71,10.91" target="#fig_3">3</ref>, measures from different BM25 parameters follow the similar trend with respect to math path weight. As path weight goes larger, NDCG' degrades consistently, this</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Query execution times (in milliseconds) for our submission in two separated passes (numbers are generated using single thread and the distribution is averaged over four independent runs).  aligns with MathDowsers runs <ref type="bibr" coords="11,229.63,405.87,17.94,10.91" target="#b16">[17]</ref> as they observe best performance when "formula weight" is almost minimal (𝛼 ≈ 0.1). however, the other measures reach higher points when math path is weighted more than text terms, but they tend to be unstable. We believe this is because MAP' and BPref changes are very minor in this evaluation, they will have greater chance to flutter. Also, NDCG' is shown generally more reliable <ref type="bibr" coords="11,297.71,460.07,12.86,10.91" target="#b3">[4]</ref> among other measures used for incomplete assessment data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head><p>Then we have investigated combining bag-of-words tokens from Anserini, and we choose fixing BM25 parameters to (0.4, 0.9) in Anserini pass. We first adapt 𝛼 = 0.5 linear interpolation ratio after normalizing scores to [0, 1], and then merge results from Approach Zero and Anserini in the second stage. The interpolation is expressed by</p><formula xml:id="formula_7" coords="11,211.75,551.28,294.24,11.36">final score = 𝛼 • 𝑆 𝑎𝑝𝑝 + (1 -𝛼) • 𝑆 𝑎𝑛𝑠<label>(8)</label></formula><p>where 𝑆 𝑎𝑝𝑝 and 𝑆 𝑎𝑛𝑠 are scores generated by Approach Zero (fixing math path weight to 1.5) and Anserini respectively. Three cases to combine with Anserini are examined, including using text terms only, using math paths only and using both text terms and maths (different type of tokens are treated the same in Anserini pass, all use BM25 scoring without substructure matching ability). For comparison, we also list the results from each individual system as well.</p><p>As shown in Figure <ref type="figure" coords="11,189.20,656.03,3.72,10.91" target="#fig_3">3</ref>, Anserini generally improves results if combined with Approach Zero. A boost of score from text-only Anserini is expected as Anserini alone achieves better results in text search, and given most of the keywords from query and results are text terms, combining Anserini can be beneficial. We notice that the path-only Anserini run also boosts scores, and we believe this is because paths tokens used in Anserini adds recall, whereas Approach Zero using substructure matching is good at adding precision, which are complementary to each other. However, text-only retrieval from Anserini contribute the most to structure-aware search in Approach Zero.</p><p>We are also interested at the combination effect in Task 2. Under our assumption that structure-aware search tends to produce good precision at the top, and path tokens are helpful to search recall, we have designed two possible ways to merge our math retrieval results from Approach Zero and Anserini: (1) Keep top-K results from Approach Zero using structure search, and the rest of results from top-1000 are concatenated from Anserini pass. (2) Uniformly apply score interpolation in Eq. 8 but with different ratios this time. Our official submissions are also named by above two conditions, i.e., a base run letter following the method we use to merge results. For example, A55 interpolates base run A with Anserini results using a ratio of 0.55, and P300 uses top-300 results from base run P and concatenates results from Anserini for the lower ranks.</p><p>Figure <ref type="figure" coords="12,132.59,651.43,5.17,10.91">4</ref> shows the evaluation summary for combining results from Approach Zero and Anserini using two different methods. Approach Zero uses the same base configuration "P", we vary the interpolation ratio and 𝐾 to see how effectiveness is affected. We observe that the weighted merge of results with a ratio around 0.3 or 0.6 achieves higher NDCG' and MAP' in general. And concatenation of search results is shown slightly better in effectiveness in this case, the almost even concatenation achieves optimal NDCG' and MAP' scores. These have indicated the contribution from either system is essential to achieve good results for Task 2, and they are very complementary when they contribute evenly in general. On the other hand, the concatenation results have justified our assumption that the top results (i.e., top 400 in this case) from Approach Zero are very effective comparatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Text and Math Expansion</head><p>We use our best base runs (A, B and P) to test the effectiveness of math keyword expansion (as described in Section 3.1.1). The experiment is conducted for Task 1 only, and we are only expanding query keywords in Approach Zero pass. Math keyword expansion is applied both in index and in query to boost formula recall.</p><p>As only a small portion of math keywords can be expanded by our manual rules, and content containing formulas is just a partial of the collection, we do not observe a large advancement in effectiveness. However, the gain in NDCG' is consistent. And because the NDCG' measure is shown to be more stable than other measures here (see Table <ref type="table" coords="13,356.64,464.89,3.50,10.91" target="#tab_3">4</ref>), we still think the effect of math expansion in Task 1 is beneficial. Nevertheless, the rules used in math keywords expansion have to be designed manually, and it may ignore other alternative synonyms and equivalent terms for math tokens.</p><p>We have noticed that the naive math keyword expansion applies uniform weight to keywords after expansion, this has important downside in contrast to query expansion methods such as RM3 <ref type="bibr" coords="13,112.82,546.18,17.97,10.91" target="#b13">[14]</ref> which will adjust boost weight to new query according to the relevance model. For example, many formula topics contain "greater or equal to" sign ≥ , however, they are not always relevant to e.g., inequality, so expanding such terms using uniform weights is going to hurt effectiveness. On the other hand, RM3 will assign smaller weight in this case, because the term "inequality" is unlikely to co-occur with ≥ .</p><p>In the following experiment, we also explore using RM3 for expanding query keywords. We simply treat math markup as terms so it can integrate into RM3 naively. RM3 has two parameters in our implementation, i.e., the number of keywords 𝑄 in the query after expansion, and the number of top documents 𝐾 to be sampled for relevance model. We use (𝑄, 𝐾) to uniquely determine a RM3 run.  Two base runs are used in this experiment, P and C. As shown in Table <ref type="table" coords="14,434.62,482.14,3.81,10.91" target="#tab_4">5</ref>, there is good improvement from RM3 in base run C, and it has a greater improvement compared to the gain from using math keyword expansion. However, this improvement is not consistent, as in run P, RM3 is actually harmful if not combined with math keyword expansion. Overall, benefit from query expansion is not notable, and our experiment shows the introducing of RM3 can be also harmful on some initial settings. But because math keyword expansion helps consistently across both experiments, we choose to apply it to all of our submissions for Task 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Learning to Rank</head><p>Inspired by the fact that previous year judgement is available for the first time in this lab, we want to study the effectiveness of reranking from utilizing these data. However, we do not have learning-to-rank results tuned correctly at the time we submit our runs, so these are completed as post-experiment runs.</p><p>Our experiment investigated two methods, simple linear regression and LambdaMART. We have our base run B as baseline and rerank its results with these two models. Experiment is conducted on Task 1 (because our features are mostly indicators for document-level similarity) with 39,124 relevance samples from previous year judgement pool, we split the data into 8 folds and validate model effectiveness by reporting averaged measures across each test data. We use the number of upvotes 𝑉 , the number of tag matches 𝑇 , and the ranking score 𝑆 produced from Approach Zero as feature set.</p><p>As shown in Table <ref type="table" coords="15,182.54,168.26,3.66,10.91" target="#tab_5">6</ref>, simple linear regression can achieve similar performance gain compared to LambdaMART model, presumably because the limited available data we have in ARQMath-1. The averaged coefficients for our resulting 8-fold linear regression model after training is 𝑉, 𝑇, 𝑆 ≈ [0.002, 0.109, 0.007]. Compared to the feature selection by Yin Ki NG et al. <ref type="bibr" coords="15,470.10,208.91,16.21,10.91" target="#b16">[17]</ref>, we do not include user-wise metadata such as user reputation and their history upvotes. However, similar to their findings, our experiment echos that tag matches is a very important feature for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Query Analysis and Case Study</head><p>To understand what is causing effectiveness changes in different methods, and to compare our math retrieval with the state-of-the-art system, we have gone through a query-by-query case analysis to understand results in different methods.</p><p>We plot the NDCG' scores per query for both Task 1 and Task 2 in Appendix E and F. Figure <ref type="figure" coords="15,501.01,339.93,4.97,10.91">5</ref> compares Task 1 scores from different methods: A base run configuration (P), base run with math keyword expansion (P-mexp), base run with RM3 = (20, 10), the same base run results merged with Anserini system using math tokens (P-50-ans0409desc), and the same base run results merged with Anserini system using text tokens (P50-ans0409title). Both merged results have a merge ratio 𝛼 = 0.5 and they use BM25 parameters (0.4, 0.9). Figure <ref type="figure" coords="15,402.31,407.68,5.17,10.91">6</ref> consists of per-query results for the formula retrieval task, and here we also compare the results to Tangent-S system. Run P30-ans7515, P50-ans7515, and P300-ans7515 are different ways to merge results with Anserini, they all use BM25 parameters (0.75, 0.15).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.1.">The Effect of Different Methods</head><p>First, combination with Anserini almost uniformly improves effectiveness, either by using text tokens or math tokens in a bag-of-word model. In a few cases, the improvement from combining bag-of-word math tokens is profound, e.g., for topic A.19 𝑝 4 -1 , A.68 𝑎 𝑛 + 1 and A.93 𝑑𝑒𝑡(𝑥𝐼 -𝐴𝐵) = 𝑑𝑒𝑡(𝑥𝐼 -𝐵𝐴) , when formulas should be matched entirely. However, in cases like A.40</p><formula xml:id="formula_8" coords="15,162.40,564.13,296.13,24.43">𝑎 1 𝑥 1 + 𝑎 2 𝑥 2 + 𝑎 3 𝑥 3 + ... + 𝑎 𝑛 𝑥 𝑛 = or A.83 1, 1 + 1 2 , 1 + 1 2 + 1<label>3</label></formula><p>, ... , math bag-of-word tokens tend to suffer because these formulas require evaluating partial matches more structurally in order to assess similarities. Second, adding only text tokens alone can greatly improve results, because many formula keywords are hurt by either malformed or irregular formula markups. For example, A.32 𝐸𝑚𝑝𝑡𝑦(𝑥) ⇐⇒ ... uses text without surrounding \text, and A.55 has Unicode encoding in the markup and our parser could not handle. Other formula keywords do not produce similar formulas in search results and may need to rely on text keywords as they are more informative, notably A.80 and A.90. Similarly, less informative math formula keywords in the topic generally benefit from query expansion. For example, in topic A.99, formula keyword 𝑓 : R → R adds expansion terms "rational number" which capture the semantic meaning of this math expression even it is hard to find many such structures in the indexed documents.</p><p>Math keyword expansion has boosted a few queries notably, but it can also hurt results such as in A.26, where it expands "fraction" keyword to the query because it contains a fraction in an integral ∫︁ ∞ 0 sin 𝑥 𝑥 𝑑𝑥 , which is obviously more about "integral" than "fraction". This confirms our assumption that weights assignment to expanded query keywords is essential in order to keep math keyword expansion generally beneficial. On the other hand, RM3 has mostly mild increase/decrease on baseline, and the overall improvement is minor.</p><p>In Table <ref type="table" coords="16,137.03,253.99,3.66,10.91" target="#tab_5">6</ref>, we are comparing to one of the most effective systems in Task 2, i.e., Tangent-S <ref type="bibr" coords="16,487.55,253.99,16.09,10.91" target="#b17">[18]</ref>. However, we do notice there are queries we could not generate any result, mostly because our semantic parser is unable to handle some formulas. For example, topic B.11 has the following formula in the original topic where parentheses would not pair correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>∫︁ ∫︁</head><formula xml:id="formula_9" coords="16,203.45,320.30,201.70,33.16">𝑉 𝑓 (𝑥, 𝑦)𝑑𝑥 𝑑𝑦 = ∫︁ ∫︁ 𝑄 𝑓 (Φ(𝑢, 𝑣) ⃒ ⃒ ⃒ ⃒ ⃒ 𝜕Φ 𝜕𝑢 × 𝜕Φ 𝜕𝑣 ⃒ ⃒ ⃒ ⃒ ⃒</formula><p>Tangent-S on the other hand, uses both Symbol Layout Tree and Operator Tree to represent formulas, and if they fail to parse OPT, they can work from Symbol Layout Tree as fallback, the latter only captures topology of the nodes in a formula, and in that case, parentheses can be unpaired. This exposes one of our crucial weakness in searching formulas, i.e., we are heavily relying on well-defined parser rules to handle user-created data, and a failure in parsing would end up zero recall in our system.</p><p>Nevertheless, we have successfully demonstrated some advantages, for example, Table <ref type="table" coords="16,483.45,449.57,5.02,10.91" target="#tab_6">7</ref> is a comparison of results from our system and Tangent-S. We are able to identify commutative operands and rank highly relevant hit to the top. However, our result at rank 3 is not relevant because the exponential power in the query is a fraction, while our returned result does not have fraction as power, even if the number of operands matched in that case is large. In this particular query, our NDCG' score is not competitive to Tangent-S, because after top results, Tangent-S is also able to return partially relevant (relevance level = 1) formulas such as 1 2</p><p>(1 + 𝑖 √ 3) at a lower rank (not shown in the table), while our results at similar positions may match more operands at tree leaf, but they can be less relevant results due to missing key symbolic variable 𝑖 , e.g., (1 + 𝑥) 1/2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.">Strength and Weakness</head><p>In terms of effectiveness, our system is able to retrieve formulas with math structure awareness.</p><p>Our system is very effective in formula search, our structure search is able to be applied to the very first stage of retrieval and produce highly effective results without reranking. However, as indicated by Task-1 results, our method to handle text and math tokens together is not ideal. On the other hand, in Task 2, some of our results are screwed by failure to parse some math markups, our OPT parser is less robust to handle user created math content than SLT parser, because OPT requires higher level of semantic construction to be resolved (e.g., to pair parentheses vertical bars in a math expression).</p><p>So far, our experimental results using learning-to-rank methods does not understand a finegrind level of math semantics, we could incorporate more features in lower level to further exploit these methods. Also, we have not applied embedding to formula retrieval yet. As demonstrated by other recent systems <ref type="bibr" coords="17,261.96,411.71,11.36,10.91" target="#b6">[7,</ref><ref type="bibr" coords="17,276.05,411.71,12.55,10.91" target="#b18">19,</ref><ref type="bibr" coords="17,291.34,411.71,12.32,10.91" target="#b19">20]</ref>, embedding applies less strict matching than substructure search and it can often greatly improve effectiveness.</p><p>Finally, although our formula search results are effective and the structure search pass employs a dedicated dynamic pruning optimization <ref type="bibr" coords="17,287.21,452.36,13.00,10.91" target="#b4">[5]</ref> for querying math formulas, we are still not reaching the level of efficiency of text-based retrieval search system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this paper, we have investigated different ways to combine our previous system Approach Zero based on substructure retrieval and another full-text retrieval system Anserini using bag-of-word tokens. We have evaluated and compared the effect of merging results by different tokens (i.e., text-only, math-only and mixed types), and by different methods (i.e., concatenation and linear interpolation). We demonstrate the usefulness of combining linear tokens into structure-aware math information retrieval using OPT prefix paths.</p><p>We also try using query expansion techniques to assist CQA task, we reported our preliminary evaluation results for math-aware search by applying RM3 model and a new math keyword expansion idea. We have also investigated using a few CQA task features to train and rerank search results, utilizing a small scale of labeled data. Our submissions to formula retrieval task of this year have achieved the best effectiveness over all official metrics. In the future, we need to add a more tolerant and efficient parser so that we can parse user created data more robustly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,251.00,309.16,9.65;4,398.46,248.93,3.97,6.12;4,405.14,251.00,90.39,9.14"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Operator Tree representation for topic formula keyword 𝑈 𝑛 = 𝑛 2 + 𝑛 (i.e., Topic B.285)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,89.29,278.67,416.70,9.15;6,89.29,290.90,417.24,8.87;6,89.29,302.85,161.44,8.87"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Up to bottom: Tree representation for formula keyword 𝑛 × 𝑛 (Topic B.201) in OPT and its decomposed paths to be used as query terms in text-based search system (symbols having a _normal_ prefix indicates they have regular font).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="11,89.29,205.06,251.28,8.93"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Math Path Weight Effect for Task 1 on 2020 Topics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,88.99,90.49,417.24,99.56"><head>Table 1</head><label>1</label><figDesc>Example formula retrieval result comparison between Tangent-CFT and Approach Zero for query 𝑂(𝑚𝑛 log 𝑚)<ref type="bibr" coords="2,153.85,116.68,11.83,8.87" target="#b6">[7]</ref> </figDesc><table coords="2,216.81,139.84,161.65,50.21"><row><cell cols="3">Rank Tangent-CFT Approach Zero</cell></row><row><cell>1</cell><cell>𝑂(𝑚𝑛 log 𝑚)</cell><cell>𝑂(𝑚𝑛 log 𝑚)</cell></row><row><cell>2</cell><cell>𝑂(𝑚 log 𝑛)</cell><cell>𝑂(𝑛𝑘 log 𝑘)</cell></row><row><cell>3</cell><cell>𝑂(𝑛 log 𝑚)</cell><cell>𝑂(𝐾𝑁 log 𝑁 )</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="12,88.99,90.49,416.99,314.85"><head>Table 3</head><label>3</label><figDesc>Results using different type of tokens from Approach Zero, Anserini and even interpolation from both systems. Results from interpolation and concatenation using different parameters. Left to right: simple linear interpolation with different ratios. And concatenation of ranked hits from substructure-matching system Approach Zero at the top K, and hits from non-structural system Anserini for the lower-ranked positions.</figDesc><table coords="12,89.29,133.41,410.31,271.93"><row><cell cols="3">System</cell><cell cols="12">Text NDCG' MAP' BPref NDCG' MAP' BPref NDCG' MAP' BPref Path Text+Path</cell></row><row><cell cols="3">Approach Zero</cell><cell></cell><cell>0.228</cell><cell cols="2">0.088 0.093</cell><cell></cell><cell>0.270</cell><cell cols="2">0.148 0.161</cell><cell></cell><cell>0.326</cell><cell cols="2">0.156 0.168</cell></row><row><cell cols="3">Anserini</cell><cell></cell><cell>0.231</cell><cell cols="2">0.094 0.098</cell><cell></cell><cell>0.220</cell><cell cols="2">0.087 0.095</cell><cell></cell><cell>0.247</cell><cell cols="2">0.097 0.105</cell></row><row><cell cols="3">Combined Ans (𝛼 = 0.5)</cell><cell cols="2">0.363</cell><cell cols="2">0.171 0.179</cell><cell></cell><cell>0.339</cell><cell cols="2">0.157 0.161</cell><cell></cell><cell>0.345</cell><cell cols="2">0.160 0.168</cell></row><row><cell>0.51 0.52 0.53 0.54 Figure 4: 0.2 NDCG' 0.50 MAP'</cell><cell>0.4</cell><cell cols="2">0.6 Ratio</cell><cell>0.8</cell><cell>1.0 NDCG' 0.35 0.36 0.37 0.38</cell><cell>MAP'</cell><cell>NDCG'</cell><cell>0.530 0.535 0.540 0.545 0.525</cell><cell>200 MAP'</cell><cell>400</cell><cell>K</cell><cell>600</cell><cell>800 NDCG' 0.364 0.366 0.368 0.370 0.372 0.374</cell><cell>MAP'</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="13,88.99,90.49,417.17,105.74"><head>Table 4</head><label>4</label><figDesc>Math keyword expansion evaluation. Summary of effectiveness for baseline runs and results after applying math keyword expansion.</figDesc><table coords="13,162.90,133.98,269.47,62.24"><row><cell>Base Run</cell><cell cols="4">Baseline NDCG' MAP' BPref NDCG' MAP' BPref Expansion Applied</cell></row><row><cell>A</cell><cell>0.320</cell><cell>0.164 0.173</cell><cell>0.323</cell><cell>0.159 0.168</cell></row><row><cell>B</cell><cell>0.311</cell><cell>0.163 0.170</cell><cell>0.317</cell><cell>0.160 0.169</cell></row><row><cell>P</cell><cell>0.325</cell><cell>0.160 0.168</cell><cell>0.328</cell><cell>0.160 0.171</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="14,88.99,90.49,416.99,171.47"><head>Table 5</head><label>5</label><figDesc>Effectiveness results from different base runs with and without RM3 being applied. Some of the runs have math keyword expansion (Math Exp.) enabled.</figDesc><table coords="14,173.98,133.13,247.31,128.83"><row><cell>Base Run</cell><cell cols="4">Settings RM3 Parameters Math Exp. NDCG' MAP' BPref Results</cell></row><row><cell></cell><cell>Not Applied</cell><cell></cell><cell>0.325</cell><cell>0.160 0.168</cell></row><row><cell></cell><cell>(15,10)</cell><cell></cell><cell>0.323</cell><cell>0.153 0.158</cell></row><row><cell>P</cell><cell>(20,10)</cell><cell></cell><cell>0.322</cell><cell>0.153 0.159</cell></row><row><cell></cell><cell>Not Applied</cell><cell>✓</cell><cell>0.328</cell><cell>0.160 0.171</cell></row><row><cell></cell><cell>(20,10)</cell><cell>✓</cell><cell>0.329</cell><cell>0.154 0.158</cell></row><row><cell></cell><cell>Not Applied</cell><cell></cell><cell>0.313</cell><cell>0.163 0.171</cell></row><row><cell></cell><cell>(15,10)</cell><cell></cell><cell>0.323</cell><cell>0.160 0.166</cell></row><row><cell>C</cell><cell>(20,10)</cell><cell></cell><cell>0.325</cell><cell>0.161 0.171</cell></row><row><cell></cell><cell>Not Applied</cell><cell>✓</cell><cell>0.319</cell><cell>0.163 0.170</cell></row><row><cell></cell><cell>(20,10)</cell><cell>✓</cell><cell>0.335</cell><cell>0.161 0.168</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="14,88.93,285.64,417.05,172.54"><head>Table 6</head><label>6</label><figDesc>Eight-fold averaged test results using learning-to-rank methods compared to results from baseline B without reranking.</figDesc><table coords="14,166.36,325.20,262.55,132.97"><row><cell>Method</cell><cell>Parameters</cell><cell cols="2">Measures NDCG' MAP' BPref</cell></row><row><cell>Baseline</cell><cell>(base run B)</cell><cell>0.314</cell><cell>0.148 0.160</cell></row><row><cell>Linear Regression</cell><cell></cell><cell>0.324</cell><cell>0.156 0.163</cell></row><row><cell></cell><cell>trees=5, depth=3</cell><cell>0.303</cell><cell>0.132 0.144</cell></row><row><cell></cell><cell>trees=5, depth=5</cell><cell>0.320</cell><cell>0.155 0.167</cell></row><row><cell></cell><cell>trees=7, depth=3</cell><cell>0.302</cell><cell>0.131 0.142</cell></row><row><cell>LambdaMART</cell><cell>trees=7, depth=5</cell><cell>0.324</cell><cell>0.156 0.164</cell></row><row><cell></cell><cell>trees=10, depth=5</cell><cell>0.323</cell><cell>0.153 0.158</cell></row><row><cell cols="2">trees=10, depth=10</cell><cell>0.318</cell><cell>0.153 0.166</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="17,88.99,90.49,416.99,184.45"><head>Table 7</head><label>7</label><figDesc>Query case comparison for top-5 results generated from topic B.12 (1 + 𝑖 √3) 1/2 in Task 2 (judged only).</figDesc><table coords="17,131.37,142.20,332.54,132.73"><row><cell>Rank</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Approach Zero</cell><cell>Tangent-S</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Results</cell><cell></cell><cell>Relev.</cell><cell>Results</cell><cell>Relev.</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell cols="2">(1 +</cell><cell cols="2">√</cell><cell cols="2">3𝑖) 1/2</cell><cell></cell><cell>3</cell><cell>𝑧 = (4 + 4</cell><cell>√</cell><cell>3𝑖) 1/2</cell><cell>2</cell></row><row><cell>2</cell><cell>(4 + 4</cell><cell cols="2">√</cell><cell cols="6">3𝑖) 1/2 = 2(1 +</cell><cell cols="2">√</cell><cell>3𝑖) 1/2</cell><cell>2</cell><cell>4(1 +</cell><cell>√</cell><cell>3𝑖) 1/2</cell><cell>2</cell></row><row><cell>3</cell><cell cols="2">(1 + 𝑖</cell><cell cols="2">√</cell><cell cols="6">3) 8 + (1 -𝑖 √</cell><cell>3) 8 =</cell><cell>0</cell><cell>(1 +</cell><cell>√</cell><cell>3𝑖) 1/2</cell><cell>3</cell></row><row><cell>4 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(1 + 𝑖 (1 + 𝑖</cell><cell cols="2">√ √</cell><cell>3) 8 3) 3</cell><cell></cell><cell>0 1</cell><cell>𝑧 = 𝑧 = (2+2𝑖) 3 (2 + 2𝑖) 3 (1 + 𝑖 √ 3) 4 (1+𝑖 √ 3) 4</cell><cell>0 0</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="8,108.93,660.08,397.64,8.97;8,89.29,671.04,75.51,8.97"><p>The complete set of our manual queries can be found here: https://github.com/approach0/pya0/tree/arqmath2/ topics-and-qrels (for</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2021" xml:id="foot_1" coords="8,185.97,671.04,239.48,8.97"><p>topics, we use the "refined" version as indicated by our file name)</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>We are interested to introduce data driven models that target math retrieval more specially. Additionally, more features can be explored to achieve greater effectiveness boost by learning from existing labels.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Approach Zero Parameter Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Official Results and Post Experiments (Task 1)</head><p>Table <ref type="table" coords="20,116.06,372.92,5.12,8.93">9</ref> Official results from different submissions across years (Task 1). Our system used as baseline in last year did not contribute to the judgment pools (indicated by dagger).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Math Keyword Expansion Mappings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Per-Query NDCG' Results (Task 1)</head><p>Figure <ref type="figure" coords="23,125.64,638.35,4.46,7.64">5</ref>: Individual query results compared to different methods using previous year topics in Task 1.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="18,112.66,186.24,393.33,10.91;18,112.66,199.79,393.32,10.91;18,112.66,213.34,284.93,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="18,224.70,186.24,281.29,10.91;18,112.66,199.79,35.87,10.91">Anserini: Enabling the use of lucene for information retrieval research</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,171.68,199.79,334.30,10.91;18,112.66,213.34,187.26,10.91">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1253" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,226.89,393.33,10.91;18,112.66,240.44,393.58,10.91;18,112.28,253.99,193.97,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="18,342.75,226.89,163.24,10.91;18,112.66,240.44,192.10,10.91">Overview of arqmath 2020: Clef lab on answer retrieval for questions on math</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,332.95,240.44,173.30,10.91;18,112.28,253.99,82.00,10.91">International Conference of the CLEF Association (CLEF</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="169" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,267.54,393.33,10.91;18,112.66,281.08,393.33,10.91;18,112.66,294.63,162.99,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="18,334.57,267.54,104.17,10.91;18,474.08,267.54,31.91,10.91;18,112.66,281.08,221.53,10.91">Second clef lab on answer retrieval for questions on math</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,362.77,281.08,143.22,10.91;18,112.66,294.63,108.91,10.91">International Conference of the CLEF Association (CLEF</title>
		<imprint>
			<date type="published" when="2021">2021. 2021. 2021</date>
		</imprint>
	</monogr>
	<note>Overview of arqmath-2</note>
</biblStruct>

<biblStruct coords="18,112.66,308.18,395.17,10.91;18,112.66,321.73,309.91,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="18,199.80,308.18,308.04,10.91;18,112.66,321.73,121.28,10.91">On information retrieval metrics designed for evaluation with incomplete relevance assessments</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="18,242.56,321.73,96.07,10.91">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="447" to="470" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,335.28,393.61,10.91;18,112.66,348.83,394.52,10.91;18,112.66,362.38,80.57,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="18,345.21,335.28,161.06,10.91;18,112.66,348.83,119.72,10.91">Accelerating substructure similarity search for formula retrieval</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rohatgi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,254.75,348.83,205.88,10.91">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="714" to="727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,375.93,393.33,10.91;18,112.66,389.48,378.87,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="18,215.31,375.93,290.68,10.91;18,112.66,389.48,75.84,10.91">Structural similarity search for formulas using leaf-root paths in operator subtrees</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,211.43,389.48,207.71,10.91">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,403.03,393.33,10.91;18,112.66,416.58,393.33,10.91;18,112.66,430.13,347.36,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="18,431.64,403.03,74.34,10.91;18,112.66,416.58,203.19,10.91">Tangent-cft: An embedding model for mathematical formulas</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rohatgi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,341.27,416.58,164.72,10.91;18,112.66,430.13,269.98,10.91">Proceedings of the 2019 ACM SIGIR International Conference on Theory of Information Retrieval</title>
		<meeting>the 2019 ACM SIGIR International Conference on Theory of Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,443.67,394.53,10.91;18,112.66,457.22,392.68,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="18,256.41,443.67,246.56,10.91">Choosing math features for bm25 ranking with tangent-l</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">W</forename><surname>Tompa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,127.29,457.22,284.55,10.91">Proceedings of the ACM Symposium on Document Engineering</title>
		<meeting>the ACM Symposium on Document Engineering</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,470.77,394.53,10.91;18,112.39,484.32,250.80,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="18,235.69,470.77,153.21,10.91">Relevance-based language models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,417.69,470.77,84.11,10.91">ACM SIGIR Forum</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="260" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,497.87,394.53,10.91;18,112.41,511.42,22.69,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">V</forename><surname>Glenn Fowler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">C</forename><surname>Noll</surname></persName>
		</author>
		<ptr target="www.isthe.com/chongo/tech/comp/fnv" />
		<title level="m" coord="18,242.07,497.87,85.11,10.91">Fowler/noll/vo hash</title>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,524.97,393.33,10.91;18,112.66,538.52,395.01,10.91;18,112.41,552.07,23.60,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="18,181.89,524.97,213.47,10.91">Lower-bounding term frequency normalization</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,421.05,524.97,84.93,10.91;18,112.66,538.52,348.72,10.91">Proceedings of the 20th ACM international conference on Information and knowledge management</title>
		<meeting>the 20th ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="7" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,565.62,395.17,10.91;18,112.66,579.17,393.32,10.91;18,112.66,592.72,158.14,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="18,338.12,565.62,169.71,10.91;18,112.66,579.17,202.79,10.91">Which bm25 do you mean? a largescale reproducibility study of scoring variants</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Kamphuis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Boytsov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,338.36,579.17,167.62,10.91;18,112.66,592.72,38.01,10.91">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="28" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,606.27,393.33,10.91;18,112.66,619.81,393.33,10.91;18,112.66,633.36,158.76,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="18,188.36,606.27,242.66,10.91">Pya0: A python toolkit for accessible math-aware search</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,452.94,606.27,53.05,10.91;18,112.66,619.81,393.33,10.91;18,112.66,633.36,129.02,10.91">Proceedings of the 44th Annual International ACM Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<meeting>the 44th Annual International ACM Conference on Research and Development in Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,646.91,394.53,10.91;19,112.66,86.97,393.33,10.91;19,112.66,100.52,76.18,10.91" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="19,112.66,86.97,162.19,10.91">Umass at trec 2004: Novelty and hard</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Abdul-Jaleel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Larkey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Computer Science Department Faculty Publication Series</publisher>
			<biblScope unit="page">189</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,114.06,395.17,10.91;19,112.66,127.61,394.03,10.91;19,112.66,141.16,241.07,10.91" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="19,296.50,114.06,179.72,10.91">Ranking, Boosting, and Model Adaptation</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">M</forename><surname>Svore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<idno>MSR-TR-2008-109</idno>
		<ptr target="https://www.microsoft.com/en-us/research/publication/ranking-boosting-and-model-adaptation/" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="19,112.66,154.71,395.17,10.91;19,112.66,168.26,393.33,10.91;19,112.66,181.81,179.05,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="19,278.94,154.71,166.51,10.91">On the local optimality of lambdarank</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Donmez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">M</forename><surname>Svore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,468.91,154.71,38.92,10.91;19,112.66,168.26,393.33,10.91;19,112.66,181.81,91.77,10.91">Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 32nd international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="460" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,195.36,393.33,10.91;19,112.66,208.91,393.33,10.91;19,112.66,222.46,295.83,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="19,466.99,195.36,39.00,10.91;19,112.66,208.91,146.68,10.91">Dowsing for math answers with tangent-l</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kassaie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Labahn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Marzouk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">W</forename><surname>Tompa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,285.09,208.91,220.90,10.91;19,112.66,222.46,265.94,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages (Working Notes)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,236.01,393.33,10.91;19,112.66,249.56,393.33,10.91;19,112.28,263.11,395.39,10.91;19,112.41,276.66,38.81,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="19,322.33,236.01,183.66,10.91;19,112.66,249.56,199.14,10.91">Multi-stage math formula search: Using appearance-based similarity metrics at scale</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Davila</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">W</forename><surname>Tompa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,336.18,249.56,169.80,10.91;19,112.28,263.11,349.47,10.91">Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="145" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,290.20,393.33,10.91;19,112.66,303.75,244.72,10.91" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.00377</idno>
		<title level="m" coord="19,258.59,290.20,247.40,10.91;19,112.66,303.75,62.22,10.91">Mathbert: A pre-trained model for mathematical formula understanding</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="19,112.66,317.30,394.52,10.91;19,112.66,330.85,358.43,10.91" xml:id="b19">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Baraniuk</surname></persName>
		</author>
		<ptr target="https://people.umass.edu/˜andrewlan/papers/preprint-forte.pdf" />
		<title level="m" coord="19,247.81,317.30,254.31,10.91">Mathematical formula representation via tree embeddings</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
