<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,369.48,15.42;1,89.29,106.66,291.77,15.42;1,89.29,128.58,313.30,15.43">DPRL Systems in the CLEF 2021 ARQMath Lab: Sentence-BERT for Answer Retrieval, Learning-to-Rank for Formula Retrieval</title>
				<funder ref="#_vS2kyVJ">
					<orgName type="full">National Science Foundation (USA)</orgName>
				</funder>
				<funder ref="#_DfRFTTK">
					<orgName type="full">Alfred P. Sloan Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,160.45,90.05,5.43"><forename type="first">Behrooz</forename><surname>Mansouri</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rochester Institute of Technology</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,191.98,160.45,83.20,5.43"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
							<email>oard@umd.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,306.17,160.45,78.09,5.43"><forename type="first">Richard</forename><surname>Zanibbi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rochester Institute of Technology</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,369.48,15.42;1,89.29,106.66,291.77,15.42;1,89.29,128.58,313.30,15.43">DPRL Systems in the CLEF 2021 ARQMath Lab: Sentence-BERT for Answer Retrieval, Learning-to-Rank for Formula Retrieval</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">DE0591C9B9BC2ED83885521F60C14FDF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Community Question Answering (CQA)</term>
					<term>Mathematical Information Retrieval (MIR)</term>
					<term>Math-aware search</term>
					<term>Math formula search</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the Document and Pattern Recognition Lab from the Rochester Institute of Technology in the CLEF 2021 ARQMath lab. There are two tasks defined for ARQMath: (1) Question Answering, and (2) Formula Retrieval. Three systems were submitted for Task 1, all of which used two-stage retrieval models. First, a set of questions within the test collection that were similar to the query were found using a Sentence-BERT model that had first been trained on Quora Question Pairs and then fine tuned using duplicate question links found within the ARQMath test collection. Then in the second stage, answers given to those questions (identified using links within the collection) were ranked by one of three different similarity scores. For Task 2, five runs were submitted: one using only formula embedding, another using formula embedding followed by re-ranking based on tree-edit distance, the third run using Tangent-S, and the remaining two being alternative ways of reranking Tangent-S results using learning-to-rank techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The ARQMath-2 lab at CLEF 2021 has the same two main tasks <ref type="bibr" coords="1,366.81,459.52,12.68,4.95" target="#b0">[1]</ref> as did the ARQMath-1 lab at CLEF 2020 <ref type="bibr" coords="1,137.81,473.07,11.28,4.95" target="#b1">[2]</ref>. In Task 1 the participants are given a new mathematical question (i.e., a question containing at least one mathematical formula) that had been posted in 2021, and are asked to return a set of relevant answers that had posted between 2010 and 2018. The other task in ARQMath is Formula Retrieval (Task 2), which takes a formula as the query, in that case the system's goal is to find a set of formula instances that are relevant to that query.</p><p>The Document and Pattern Recognition Lab (DPRL) from the Rochester Institute of Technology (RIT, USA) participated in both tasks. For Task 1, the design of our models is motivated by one aspect of user behavior that is fairly common in Math Stack Exchange (and other Community Question Answering forums). 1 When a new question is posted, the Math Stack Exchange moderators (and experienced users with high reputation scores 2 ) can mark a question as a duplicate, referring the asker to the similar question(s) that had previously been posted to Math Stack Exchange, where they can find relevant answers. Following a similar process, in all of our runs we first find similar questions (using some fully automatic technique) and then we rank only the answers given to those questions. Our systems differ in how the first and second stages of that process are implemented.</p><p>To find similar questions, we fine-tune a Sentence-BERT model <ref type="bibr" coords="2,387.00,321.22,11.51,4.95" target="#b2">[3]</ref>, using both related and duplicate questions on Math Stack Exchange. For ranking the answers, we use three different scoring functions: <ref type="bibr" coords="2,171.39,348.32,11.42,4.95" target="#b0">(1)</ref> the Math Stack Exchange answer scores that are available as metadata in the test collection, (2) A computed score from a Sentence-BERT model trained on the ARQMath-1 relevance judgments that estimates the similarity between the Question and Answer pair (QASim), and (3) a combination of scores from those two approaches.</p><p>For Task 2, we modify our previous models Tangent-CFT <ref type="bibr" coords="2,344.73,402.52,12.69,4.95" target="#b3">[4]</ref> and Tangent-CFTED <ref type="bibr" coords="2,450.46,402.52,11.27,4.95" target="#b4">[5]</ref>. Tangent-CFT is an n-gram embedding model built on a linearized tree representation of formulae and Tangent-CFTED re-ranks the results from Tangent-CFT using tree-edit distance. Our two new runs for ARQMath, use learning-to-rank framework for mathematical formulae <ref type="bibr" coords="2,457.25,443.16,12.99,4.95" target="#b5">[6]</ref> trained on ARQMath-1 topics. One run is trained only on the 29 training queries from ARQMath-1, whereas the other was trained on all 77 ARQMath-1 formula queries. The Tangent-S <ref type="bibr" coords="2,460.18,470.26,12.69,4.95" target="#b6">[7]</ref> system is our last (baseline) run. All our runs use only formula matching to find relevant formulae, with no use of the text surounding those formulae in the question from which the query was extracted or in the test collection post from which a potentially relevant formula instance was extracted. Our models make use of both the Operator Tree (OPT) and Symbol Layout Tree (SLT) representations of formulae, one encoding the syntax and the other appearance of a formula. Figure <ref type="figure" coords="2,121.30,551.56,5.17,4.95" target="#fig_0">1</ref> shows the OPT and SLT representations for the formula ğ‘ 2 + ğ‘ 2 = ğ‘ 2 . In the OPT representation, the edge labels for non-commutative operators indicate argument position. In the SLT representation, the edge labels show the spatial relationship between the formula elements. For instance, the edge label 'a' shows that the number '2' is located above the variable 'a', while the edge label 'n' shows operator '+' is located next after 'a'.</p><p>In this paper, we first describe our runs in Task 2, then discuss our proposed models for Task 1, and finish with conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Tuples created on the formula ğ‘ 2 + ğ‘ 2 = ğ‘ 2 (from Figure <ref type="figure" coords="3,320.12,102.49,3.41,8.87" target="#fig_0">1</ref>. Each tuple has four elements: (parent, child, path, path-from-root <ref type="bibr" coords="3,170.10,114.40,20.90,8.96">[PFR]</ref>). V!, N!, O!, and U! are node types (operators like '+' have no SLT node type), 'eob' end-of-baseline, and '-' an empty path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SLT Tuples</head><p>OPT Tuples ( Parent, Child, Path, PFR) ( Parent, Child, Path, PFR)</p><formula xml:id="formula_0" coords="3,139.71,176.50,308.85,53.15">(V! a, N! 2, a, -) (U! eq, U! plus, 0, -) (N!2, eob, n, a) (U! plus, O! SUP, 0, 0) (V! a, +, n, -) (O! SUP, V!a, 0, 00) . . . . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task 2: Formula Retrieval</head><p>In Task 2, one formula inside each topic from Task 1 is selected and the participants are asked to return a set of relevant formula instances from the questions and answers in the collection.</p><p>As described in the ARQMath 2021 overview <ref type="bibr" coords="3,293.29,311.16,11.50,4.95" target="#b7">[8]</ref>, to decide the relevance degree of a formula, the context in which the topic and retrieved formula appear is important. In all our models, we focus only on the structural matching of the formula, and text is ignored. All our models make use of both OPT and SLT representations. Different approaches such as <ref type="bibr" coords="3,402.82,351.80,11.23,4.95" target="#b3">[4,</ref><ref type="bibr" coords="3,416.69,351.80,7.42,4.95" target="#b6">7,</ref><ref type="bibr" coords="3,426.76,351.80,8.88,4.95" target="#b8">9]</ref> have found this beneficial for system effectiveness. Next, we describe our five runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Tangent-S</head><p>Tangent-S<ref type="foot" coords="3,133.82,412.45,3.71,3.62" target="#foot_0">3</ref>  <ref type="bibr" coords="3,140.76,415.08,12.72,4.95" target="#b6">[7]</ref> was reported by the organizers as the Task-2 baseline system in both ARQMath 2020 and 2021, and we also use components or scores from Tangent-S in all of our runs. In this system first a set of candidates are retrieved based on tuple similarity. Using depth-first traversals, tuples with 4 elements are created as (parent, child, path, path-from-root [PFR]). The parent and child are the node values in the form of Type!Value, where type can take values such as Variable (V) or Number(N) and value shows the variable name or the numeric value. Path shows the set edge labels visited connecting parent to child. Path-from-root shows edges labels visited when moving from the root node to the parent node. Table <ref type="table" coords="3,387.98,509.92,5.10,4.95">1</ref> shows the tuples created from SLT and OPT representations of the formula ğ‘ 2 + ğ‘ 2 = ğ‘ 2 from Figure <ref type="figure" coords="3,423.14,523.47,3.66,4.95" target="#fig_0">1</ref>. As the first node is the root of the tree, the path-from-root is empty. The second tuple is showing that the node with type Number and value '2' has no children and we have reached end of baseline (eob). The default edge label for the eob is 'n'. After the tuples are generated, the harmonic mean of recall and precision for matched tuples is used for ranking.</p><p>To re-rank the candidates, three similarity scores are considered. The Maximum Subtree Similarity (MSS) is computed from the largest connected match between query and candidate formulae obtained using a greedy algorithm, evaluating pairwise alignments between trees using unified node values. Two other similarity features used in this system are query node matching after alignment, either with or without -Type unification. The SLT and OPT results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formula</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head><p>Tangent-S Extracted Tuples  After the tuples are encoded using the Tangent-CFT model, vector representations of a query formula are obtained. These vectors are compared with the vector representation of formulae in the collection by using cosine similarity, and top-1000 most similar formulae are returned. Finally, retrieval results from different representations are combined using modified Reciprocal Rank Fusion (RRF) to get the final retrieval result.</p><p>are next combined via linear regression over alignment measures from each representation to produce final similarity scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Tangent-CFT2</head><p>The Tangent-CFT model <ref type="bibr" coords="4,197.02,327.65,12.68,4.95" target="#b3">[4]</ref> was the first embedding model introduced for mathematical formula to use both SLT and OPT representations. In addition to the full representations (which include both the type and the value of a node), this model also employed unification on the SLT to produce a representation called SLT-Type. In the SLT-Type tree representation, only the type of each node was represented; the corresponding values were ignored. For each of the three representations, the model would then convert the tree representation to a vector, and then add the 3 vectors to get the final representation of a formula. Our the process thus has the following steps (refer to <ref type="bibr" coords="4,153.33,422.50,12.84,4.95" target="#b3">[4]</ref> for further details):</p><p>â€¢ Tuple Extraction: Presentation MathML and Content MathML representations (from LaTeXML<ref type="foot" coords="4,161.01,458.92,3.71,3.62" target="#foot_1">4</ref> ) are used as a basis from which to generate internal SLT and OPT formula representations. Built using depth-first traversals with the Tangent-S system, these internal tree representations are strings consisting of a sequence of tuples, as described above for the Tangent-S system. The only difference for Tangent-CFT is that the pathfrom-root element produced by Tangent-S is ignored. â€¢ Tuple Encoding: The tuples are then tokenized and enumerated. The tokenization is based on type and value. For example the tuple (V!a, N!2, a), which represents ğ‘ 2 , will be tokenized to {'V', 'a', 'N', '2', 'a' }.  treated as a text sentence. The final vector representation for a formula is thus obtained by averaging the fastText representations for its individual tuples.</p><p>While the overall pipeline for Tangent-CFT2 is the same as for Tangent-CFT, two modifications were made to the architecture:</p><p>â€¢ Formula Representations. In Tangent-CFT2 we add another representation to the previous model by considering the OPT-Type. This representation is similar to SLT-Type, but for the operator tree. â€¢ Combining the Results. After ARQMath 2020, we modified the result combination part of the Tangent-CFT system. In the previous system, the 3 vectors from each representation were added to get the final vector representation of a formula. In Tangent-CFT2, with each of the four representations of SLT and OPT (full and -Type), first, the top-k results are retrieved. This is done by computing the cosine similarity between the query vector representation and the vector representation of formulae in the collection. Then the four results are combined using modified Reciprocal Rank Fusion <ref type="bibr" coords="5,404.39,432.68,17.77,4.95" target="#b11">[12]</ref> with the following formula:</p><formula xml:id="formula_1" coords="5,223.43,453.01,282.56,29.64">ğ‘…ğ‘…ğ¹ ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’(ğ‘“ âˆˆ ğ¹ ) = âˆ‘ï¸ ğ‘šâˆˆğ‘€ ğ‘  ğ‘š (ğ‘“ ) ğ‘˜ + ğ‘Ÿ ğ‘š (ğ‘“ ) (1)</formula><p>where ğ‘“ is a set of formulae to be ranked, ğ‘€ is a set of models, and ğ‘  ğ‘š and ğ‘Ÿ ğ‘š are the scores and the rank, respectively, of the retrieved formula by model ğ‘š. The top-1000 results from each representation are computed as the cosine similarity between the vectors.</p><p>Figure <ref type="figure" coords="5,120.36,556.05,5.07,4.95" target="#fig_1">2</ref> shows the overall architecture of Tangent-CFT2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Tangent-CFT2TED</head><p>The Tangent-CFTED model was introduced in ARQMath 2020 <ref type="bibr" coords="5,368.22,605.78,11.43,4.95" target="#b4">[5]</ref>. This model first retrieves a set of candidate formulae using the Tangent-CFT model. It then re-ranks them using Tree-Edit Distance (TED), similarly to Kamali and Tompa <ref type="bibr" coords="5,291.21,632.87,20.05,4.95" target="#b12">[13]</ref>. Figure <ref type="figure" coords="5,350.01,632.87,4.98,4.95" target="#fig_2">3</ref> shows an overview of this system. In this section, we first review the Tangent-CFTED model and then describe the changes in the new model.</p><p>Tangent-CFTED uses tree-edit distance to compare the similarity between two formulae using their tree representations. Tree-edit distance is the minimum cost of converting one tree to the other using a set of edit operations. In this work, we consider three edit operations: insertion, deletion, and substitution. Each operation has a unique weight. These weights are similar in both versions, learned on NTCIR-12 <ref type="bibr" coords="6,296.45,144.41,17.87,4.95" target="#b13">[14]</ref> topics. To calculate tree-edit distance, only the node values are used; the edge labels are ignored. The similarity measure is defined as inverse tree-edit distance as follows:</p><formula xml:id="formula_2" coords="6,220.53,185.98,285.45,25.50">ğ‘ ğ‘–ğ‘š(ğ‘‡ 1 , ğ‘‡ 2 ) = 1 ğ‘‡ ğ¸ğ·(ğ‘‡ 1 , ğ‘‡ 2 ) + 1 .<label>(2)</label></formula><p>In our model, we use both SLT and OPT representations and re-rank the candidates with tree-edit distance. The results are then combined using RRF with equation 1. For more details refer to <ref type="bibr" coords="6,124.85,250.66,11.43,4.95" target="#b3">[4]</ref>. There are two modifications made in the new version:</p><p>â€¢ Selecting the candidates. The candidate formulae are selected using Tangent-CFT2.</p><p>â€¢ Combining the Results. To combine the re-ranked results from the SLT and OPT representations, we again use RRF (equation 1). In the previous version, the results were combined linearly with weights learned on the NTCIR-12 <ref type="bibr" coords="6,375.20,312.45,17.91,4.95" target="#b13">[14]</ref> dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Learning-to-Rank</head><p>Broadly speaking, three main approaches have been used for the formula retrieval task: full-tree matching, sub-tree matching, and embedding models. In our learning-to-rank framework, we make use of all these approaches, using results from instances of each approach to create features for the SVM-rank <ref type="bibr" coords="6,209.21,402.49,17.95,4.95" target="#b14">[15]</ref> system for supervised learning to rank. We trained SVM-rank two ways, once with the 29 ARQMath-1 training queries (LtR29), and the other time with all 77 queries from the ARQMath-1 collection (LtRall). The penalty for misclassification during training (C) is 0.01, and the tolerance for termination criterion (epsilon) is 0.001. We computed the following kinds of similarity measures as features:</p><p>â€¢ Tuple matching scores â€¢ Maximum Sub-tree Similarity (MSS)</p><p>â€¢ Node Matching scores â€¢ Unweighted tree edit distance scores â€¢ Weighted tree edit distance scores â€¢ Cosine similarity from Tangent-CFT model All features other than MSS were calculated on both OPT and SLT representations, with and without unification. The MSS features were only calculated for the unified SLT-Type and OPT-Type representations. The first three kinds of similarity features are from the Tangent-S model, which uses sub-tree matching. The tree edit distance features were calculated with the Tangent-CFTED system. For the weighted tree edit distance scores, we use the same weights that Tangent-CFTED uses for retrieval.<ref type="foot" coords="6,262.69,639.74,3.71,3.62" target="#foot_2">5</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Results for Submitted Runs</head><p>Tables <ref type="table" coords="7,119.33,253.91,4.97,4.95" target="#tab_2">2</ref> and<ref type="table" coords="7,145.55,253.91,4.97,4.95" target="#tab_3">3</ref> show the DPRL runs results on Task 2, both for our official submitted runs (Table <ref type="table" coords="7,89.29,267.46,3.55,4.95" target="#tab_2">2</ref>), and for runs obtained after correcting errors in how our systems were run (Table <ref type="table" coords="7,463.88,267.46,3.55,4.95" target="#tab_3">3</ref>). In this section we summarize results for our submitted runs, shown in Table <ref type="table" coords="7,399.83,281.01,3.74,4.95" target="#tab_2">2</ref>.</p><p>ARQMath-1 Results. For the ARQMath-1 topics, our learning-to-rank framework has better effectiveness compared to the Tangent-S system, in particular for P â€² @10 which is higher for all of our runs. Interestingly, our learning-to-rank framework shows similar effectiveness when trained on all training and test topics (LtRall) versus trained on only training topics (LtR29). Using a one-way analysis of variance (ANOVA) test with a .05 significance level, the differences between our runs were not significant in terms of P â€² @10 and MAP â€² . However, with nDCG â€² , the differences between Tangent-CFT2 and both our learning-to-rank models are significant (using posthoc Tukey HSD Test, ğ‘ &lt; 0.05).</p><p>Tangent-CFT2, Tangent-CFT2TED, and Tangent-S take different approaches for formula retrieval using embedding, full-tree, and sub-tree matching. Our learning-to-rank model uses SVM-rank to linearly combine similarity measures from each of these models, in order to overcome their individual limitations.</p><p>In our next analysis, we compare three runs from the Tangent family on the ARQMath-1 topics, and show how learning-to-rank can help. Tangent-CFT2, being an n-gram embedding model, focuses on retrieving formulae that share common n-grams with the input query. This can be beneficial for formulae such as âˆ‘ï¸€ ğ‘ ğ‘›=0 ğ‘›ğ‘¥ ğ‘› , which are small, and perhaps users are sensitive to the variables or numbers they use in their formula. Both Tangent-S and Tangent-CFT2TED return non-relevant formulae such as âˆ‘ï¸€ ğ‘ ğ‘›=0 (-1) ğ‘› ğ‘¥ ğ‘› in their top-10 results, which have SLTs and OPTs that are similar to the query, but are not relevant. For this query, the P â€² @10 for Tangent-CFT2 was 0.9, 0.4 for Tangent-CFT2TED, and 0.6 for Tangent-S. Tangent-CFT2TED uses tree-edit distance as a full-tree matching score.</p><p>Looking at full trees provides better results for formulae such as:</p><formula xml:id="formula_3" coords="7,175.52,601.08,244.24,9.57">âˆ…, {1}, {2}, {1, 2}, {3}, {1, 3}, {2, 3}, {1, 2, 3}, {4}, . . .</formula><p>where partial matches are unlikely to provide useful information. The P â€² @10 values for this query for Tangent [-CFT2TED,-CFT2,-S] are 0.6, 0.4 and 0.3, respectively. Finally, Tangent-S is a system using sub-tree matching, and for complex formulae such as:</p><formula xml:id="formula_4" coords="8,190.12,109.24,215.04,33.16">âˆ«ï¸ âˆ«ï¸ ğ‘‰ ğ‘“ (ğ‘¥, ğ‘¦)ğ‘‘ğ‘¥ ğ‘‘ğ‘¦ = âˆ«ï¸ âˆ«ï¸ ğ‘„ ğ‘“ (Î¦(ğ‘¢, ğ‘£) âƒ’ âƒ’ âƒ’ âƒ’ âƒ’ ğœ•Î¦ ğœ•ğ‘¢ Ã— ğœ•Î¦ ğœ•ğ‘£ âƒ’ âƒ’ âƒ’ âƒ’ âƒ’</formula><p>finding sub-matches can also be useful, returning highly relevant formulae such as:</p><formula xml:id="formula_5" coords="8,181.95,176.10,228.79,33.49">âˆ«ï¸ âˆ«ï¸ ğ·ğ‘¥,ğ‘¦ ğ‘“ (ğ‘¥, ğ‘¦)ğ‘‘ğ‘¥ğ‘‘ğ‘¦ = âˆ«ï¸ âˆ«ï¸ ğ·ğ‘¢,ğ‘£ ğ‘“ (ğ‘‡ (ğ‘¢, ğ‘£)) |ğ½(ğ‘¢, ğ‘£)|ğ‘‘ğ‘¢ğ‘‘ğ‘£,</formula><p>that the other two models did not return in their top-1000 results. Tangent-S has P â€² @5 of 0.5 for this formula, whereas this value is 0.3 for both Tangent-CFT2 and Tangent-CFT2TED.</p><p>From these examples, we can see that each of these models have their strengths and limitations. With our learning-to-rank model, we re-rank Tangent-S results using similarity scores from multiple retrieval models. For example, for the query lcm(ğ‘› 1 , ğ‘› 2 ) = ğ‘› 1 ğ‘› 2 gcd(ğ‘› 1 ,ğ‘› 2 ) Tangent-S ranks non-relevant formulae such as ğ¿ = lcm(ğ‘› 1 , ğ‘› 2 , . . . , ğ‘› ğ‘˜ ), that share sub-trees with the query in its top-10 results. Using our proposed learning-to-rank model, relevant formulae such as lcm(ğ‘, ğ‘) = ğ‘â€¢ğ‘ gcd(ğ‘,ğ‘) are pushed to the top-10 results. As can be seen, the first formula can be converted to the second with a pair of substitutions (ğ‘ for ğ‘› 1 , ğ‘ for ğ‘› 2 ) and removing the multiplication dot in the second formula.</p><p>The learning to rank models use only formula tree similarity features. However, there are formulae that need more features and processing. For instance, for the query:</p><formula xml:id="formula_6" coords="8,234.71,399.26,125.86,9.57">ğ¸ğ‘šğ‘ğ‘¡ğ‘¦(ğ‘¥) â‡â‡’ Ì¸ âˆƒğ‘¦(ğ‘¦ âˆˆ ğ‘¥)</formula><p>there are relevant formulae such as âŠ¢ âˆƒğ‘¥âˆ€ğ‘¦(ğ‘¦ / âˆˆ ğ‘¥) that may not necessarily share SLT or OPT structure with the query. Perhaps using canonicalization methods <ref type="bibr" coords="8,375.01,439.84,17.76,4.95" target="#b15">[16]</ref> can improve effectiveness for these queries to convert them to one unified format. While we focused on structural similarity features, there are still formulae for which the effectiveness is low. Textual features are another missing part of our current model. There are queries such as ğ‘‘ğ‘“ ğ‘‘ğ‘¥ = ğ‘“ (ğ‘¥ + 1), appearing in a question related to differential equations, for which returning a structurally similar formula such as ğ‘‘ğ‘¦ ğ‘‘ğ‘¥ = ğ‘“ (ğ‘¥) is considered non-relevant due to its appearance in a different context (a post on another topic). We consider exploring these features in our future work.</p><p>ARQMath-2 Results. The results for ARQMath-2 for our systems and the baseline are substantially lower than for ARQMath-1, with the baseline outperforming all of our models in nDCG â€² and MAP â€² . This was due in part to changes in relevance assessment for Task 2 (see the ARQMath-2 working notes overview paper for details <ref type="bibr" coords="8,323.55,575.33,15.53,4.95" target="#b16">[17]</ref>), and to errors made while computing our runs, described in the next Section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Corrected Unofficial ARQMath-2 Post-Hoc Runs</head><p>After ARQMath 2021, we determined that we had executed our embedding models incorrectly for ARQMath-2 topics, which impacted all of our official Task 2 runs. Tangent-CFT2 uses embeddings, Tangent-CFT2TED re-ranks Tangent-CFT2 results, and our learning to rank models use cosine similarities over embedding vectors as features. We fixed the issue and recalculated the effectiveness measures shown in Table <ref type="table" coords="9,280.81,258.84,3.74,4.95" target="#tab_3">3</ref>.</p><p>After correction, the learning-to-rank models improve the Tangent-S (baseline) results similarly for both ARQMath-1 and ARQMath-2 topics (roughly 5-6% for nDCG â€² , 7-8% for MAP â€² , and 9-12% for P â€² @10). For both topic sets, training our learning to rank models using all ARQMath-1 topics versus just the 29 ARQMath-1 training topics produces similar results. We choose to re-rank Tangent-S results for our learning to rank models, as Tangent-S had the best performance for Task 2 systems at ARQMath-1. However, for ARQMath-2, this is not the case. Re-ranking results from another system may have provided better re-ranked results. Our strongest results were obtained by the Tangent-CFT2TED system, which had better effectiveness than Tangent-CFT2. Tangent-CFT2TED reranks Tangent-CFT2 results using SLT and OPT tree edit distances with RRF (see above).</p><p>Even after correction, all of our runs have lower nDCG â€² and MAP â€² measures on ARQMath-2 topics than on ARQMath-1 topics. However, our highest ğ‘ƒ â€² @10 is nearly identical, increasing just slightly (by 0.3%) for Tangent-CFT2TED for ARQMath-2 topics vs. ARQMath-1 topics. Note that our proposed models are all based on formula similarity and the context is ignored: there are several queries where our models retrieve formulas with identical or nearly identical SLT/OPT representations, but they are assessed as low relevance or not relevant due to differences in the posts where query and retrieved formulas appear (e.g., different data types and ranges for variables). Some rather small changes can cause formulas to be deemed not-relevant, as illustrated in Table <ref type="table" coords="9,175.61,516.28,3.74,4.95">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Task 1: Answer Retrieval</head><p>In Task 1, the goal is to find relevant answers to a mathematical question. The topics are selected among questions posted on Math Stack Exchange in 2020. The answers in the collection are posted from 2010 to 2018.</p><p>We had 3 runs for Task 1, all following a two-step retrieval model. First, similar questions are retrieved. Then, all answers given to similar questions are ranked using 3 different scoring functions. In all our runs, the mathematical formulae are represented using their original L A T E X strings. Finally, the answers are sorted in descending order by their vote scores. We explain our two-step retrieval model next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4</head><p>ARQMath-2 Task 2 queries where formulae with similar SLT/OPT representations are assessed as having low relevance, or being not relevant. Retrieved formulas are from the Tangent-CFT2TED system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head><p>Retrieved Formula Relevance ğ›¼ 1 + ğ›¼ 2 + ğ›¼ 3 = 3</p><formula xml:id="formula_7" coords="10,181.06,146.06,233.16,105.80">ğ›¼ 1 + ğ›¼ 2 + ğ›¼ 3 = 1 Not Relevant ğ‘¥ ğ‘› + ğ‘¦ ğ‘› + ğ‘§ ğ‘› ğ‘¥ ğ‘› + ğ‘¦ ğ‘› + ğ‘§ ğ‘› Low cos ğœ‹ 5 -cos 2ğœ‹ 5 = 1 2 cos ğœ‹ 5 cos 2ğœ‹ 5 = 1 4 Not Relevant ğ´ = â‹ƒï¸€ âˆ ğ‘›=1 ğ´ ğ‘› ğ´ = â‹ƒï¸€ +âˆ ğ‘›=1 ğ´ ğ‘› Low ğ‘…(ğ‘› 1 , ..., ğ‘› ğ‘ ) ğ‘…(ğ‘¢ 1 , ..., ğ‘¢ ğ‘› ) Not Relevant</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">STEP 1: Finding Similar Questions</head><p>Math Stack Exchange provides links to related and duplicate questions. The related questions have a similar topic, but they are not exactly the same question. The duplicate questions are tagged by the Math Stack Exchange moderators of users with high reputation scores. A duplicate question is a newly posted question that has been asked before on Math Stack Exchange.</p><p>In our retrieval models, to first identify similar questions to a topic, we used the Sentence-BERT Cross-Encoders with the pre-trained model on the Quora question pairs dataset. <ref type="foot" coords="10,480.98,366.97,3.71,3.62" target="#foot_3">6</ref> The model was trained on over 500,000 sentences with over 400,000 pairwise annotations indicating whether two questions are a duplicate or not. Using this model, we did two-step fine-tuning. First, we trained the model on both duplicate and related questions. Then, another fine-tuning was done, using only the duplicate questions. For our training, we used the posts provided in the ARQMath collection (from 2010 to 2018). In the first fine-tuning, 358,306 pairs, and in the second, 57,670 pairs were used. In both cases, half of the pairs were positive samples and the other half were the negative ones, chosen randomly from the collection.</p><p>To train both models, we used multi-task learning, considering two loss functions: constrastive <ref type="bibr" coords="10,89.29,491.54,18.07,4.95" target="#b17">[18]</ref> and multiple negatives ranking loss <ref type="bibr" coords="10,276.42,491.54,16.41,4.95" target="#b18">[19]</ref>. The constrastive loss function minimizes the distance between positive pairs and maximizes it for negative ones, making it suitable for classification tasks. The multiple negatives ranking loss function, however, considers only positive pairs and minimizes the distance between positive pairs out of a large set of possible candidates. We set the batch size to 64 and number of training epochs to 20. The maximum sequence size was set to 128.</p><p>Figure <ref type="figure" coords="10,131.01,572.84,4.07,4.95" target="#fig_3">4</ref>(a) shows the Cross-Encoder model trained for finding similar questions. In the first fine-tuning, a question title and body are concatenated. In the second fine-tuning, however, we considered the same process for training, with three different inputs:</p><p>â€¢ Using the question title, with a maximum sequence length of 128 tokens.</p><p>â€¢ Using the first 128 tokens of question body.</p><p>â€¢ Using the last 128 tokens of question body. To find a similar question, we use the three models to separately retrieve the top-1000 most similar questions. The retrieved results are then combined using RRF as shown in Eq. 1. We call this similarity score Question-Question Similarity (QQSim).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">STEP 2: Finding Related Answers</head><p>After similar questions are found for a topic, all the answers given to them are compiled and ranked based on the question and answer similarity. In each run, we used a different similarity function as follows:</p><p>1. Math Stack Exchange score [MathSE]. Each post on Math Stack Exchange (MathSE) has a score given by the users. The score is the difference between the positive and negative votes given to that post. In our first run, we simply consider this score as an indicator of answer relevance. We used MinMax normalization to map answer scores to values between 0 to 1. Therefore, our final relevance score between a question and a candidate answer is calculated as:</p><formula xml:id="formula_8" coords="11,175.85,511.21,330.14,10.69">ğ‘…ğ‘’ğ‘™ğ‘’ğ‘£ğ‘ğ‘›ğ‘ğ‘’(ğ‘„ ğ‘‡ , ğ´) = ğ‘„ğ‘„ğ‘†ğ‘–ğ‘š(ğ‘„ ğ‘‡ , ğ‘„ ğ´ ) â€¢ ğ‘€ ğ‘ğ‘¡â„ğ‘†ğ¸ ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ (ğ´)<label>(3)</label></formula><p>where the ğ‘„ ğ‘‡ is the question topic and ğ´ is the candidate answer and ğ‘„ ğ´ is the question to which answer ğ´ was given. 2. Two-step Hierarchical Sentence-BERT [QASim]. ARQMath-1 results showed not all the answers with a high score on Math Stack Exchange are relevant. An example is shown in Table <ref type="table" coords="11,187.32,591.80,3.74,4.95" target="#tab_4">5</ref>. Therefore, in our second run, we train Sentence-BERT Cross-encoder fine-tuned on question and answer pairs from ARQMath-1 topics and their assessed hits. Our pretrained model is Tiny-BERT with 6 layers trained on the "MS Marco Passage Reranking" <ref type="bibr" coords="11,116.56,645.73,17.93,4.95" target="#b19">[20]</ref> task. The inputs are triplets of (Question, Answer, Relevance), where the relevance is a number between 0 and 1. In ARQMath-1 evaluation <ref type="bibr" coords="11,362.20,659.28,16.09,4.95" target="#b20">[21]</ref>, high and medium relevance degrees were considered as relevant for precision-based measures. In our training, we ARQMath Topic Title Finding the last two digits of 9 9 9 . . . 9 (nine 9s) Relevant Question Title The last two digits of 9 9 9 Answer At this point, it would seem to me the easiest thing to do is just do 99 mod 100 by hand. The computation should only take a few minutes.</p><p>In particular, you can compute 93 and then cube that. consider a relevance score of 1 for answers assessed with high or medium, 0.5 for low, and 0 for non-relevant. As for the cross-encoder for Question-Question similarity, we use the L A T E X representation for formulae. The input question is the concatenation of the question title and body. We use a batch size of 64, with 20 epochs and a maximum sequence length of 128. We keep our loss functions similar to our Question-Question model, using multi-task learning by constrastive and multiple negatives ranking loss functions. After the training, the cross-encoder outputs the similarity of question and answer, called QASim as shown in Figure <ref type="figure" coords="12,310.20,454.37,3.97,4.95" target="#fig_3">4</ref>(b). In this run, after similar questions are found, the model predicts the similarity of question and answer pair, QASim. Our final ranking score considers two similarity scores; between the question and answer, and also the similarity between the topic question and the question to which the answer is given (calculated in step 1). The ranking function is:</p><formula xml:id="formula_9" coords="12,178.73,529.09,327.26,10.68">ğ‘…ğ‘’ğ‘™ğ‘’ğ‘£ğ‘ğ‘›ğ‘ğ‘’(ğ‘„ ğ‘‡ , ğ´) = ğ‘„ğ‘„ğ‘†ğ‘–ğ‘š(ğ‘„ ğ‘‡ , ğ‘„ ğ´ ) â€¢ ğ‘„ğ´ğ‘†ğ‘–ğ‘š(ğ‘„ ğ‘‡ , ğ´)<label>(4)</label></formula><p>where ğ‘„ ğ´ is the question to which answer ğ´ was given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Combined model [RRF].</head><p>In our last run, we combine the similarity scores obtained from the two previous runs using RRF as given in equation 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Results</head><p>Table <ref type="table" coords="12,116.82,632.19,5.17,4.95" target="#tab_5">6</ref> shows our run results on ARQMath topics for Task 1. Using a one-way analysis of variance (ANOVA) test with a .05 significance level, none of our runs on ARQMath-1 and ARQMath-2 topics were significantly different in any of the effectiveness measures. The top-1000 results returned by all three runs differ only in their rank ordering. This is due to the effect  The summation of numbers in one circle shows is the average P â€² @10 for that system. Intersections between pairs of system are the average number of relevant hits shared by those two systems not in the intersection for all three systems. Numbers associated with only one circle/system are the average number of hits relevant hits found only by each individual system.</p><p>of question-question similarity in all our runs on the final similarity score. However, looking at intersections of the top-10 annotated results for each run in Figure <ref type="figure" coords="13,386.67,323.20,5.04,4.95" target="#fig_5">5</ref> (averaged over 77 topics), our first and second runs, which use different scoring functions for the candidate answers, are able to find a set of relevant answers the the other run cannot find. When combining the results, these relevant answers are left out. For example, when considering all relevance degrees, on average there are 2.1 relevant answers retrieved by our first run (that considers only the Math Stack Exchange score), which are not included in the combined results (run RRF). Therefore, for future work, we might consider a different strategy for combining the results. ARQMath provides different types of topics. Topics are categorized based on their difficulty into hard, easy and medium. Another grouping is based on whether the topic is dependent on the text, formula or both. The last category divides the questions based on their subject into concept, computation and proof. We separate topics based on these categories and calculated P â€² @10 for each group. The results are shown in Table <ref type="table" coords="13,338.44,472.25,3.81,4.95" target="#tab_6">7</ref>. As shown in this table, re-ranking results with the Cross-Encoder trained on ARQMath-1 topics can improve effectiveness for text-dependent questions. The same effect can be seen for topics related to concepts. In this category, 40% of topics are text-dependent and the other are dependent on both formula and text. In contrast to concept-related questions, 50% of questions related to computation are formula-dependent, causing low effectiveness for our models.</p><p>For ARQMath-2 topics, we see a drop in all effectiveness measures, including for the baseline. The increase in the ratio of topics that depend upon both text and formula may partly explain this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>This paper describes the DPRL runs for the ARQMath lab at CLEF 2021. Five runs were submitted for the Formula Retrieval task. For ARQMath-2, our initial formula retrieval runs were computed incorrectly. In our corrected runs, the Tangent-CFT2TED model did better <ref type="bibr" coords="13,412.85,666.36,16.09,4.95" target="#b16">[17]</ref>. Our learning-to- rank models consistently improved results, but re-ranking the baseline Tangent-S runs produced substantially weaker results for ARQMath-2 than ARQMath-1 topics in terms of nDCG â€² and MAP â€² metrics, perhaps because the initial Tangent-S results were weaker for ARQMath-2. Our models retrieved formulae with similar or identical Symbol Layout Tree and Operator Tree representations for some queries. However, when formulae appeared in a different context than the formula query, they could be assessed as being of low relevance, or not relevant at all. For the Answer Retrieval task, three runs were submitted. In our runs, first, a set of similar questions are found for a topic, and then answers given to them are ranked by criteria that are specific to each run. To find similar questions we used Sentence-BERT Cross-encoder fined-tuned on the ARQMath Math Stack Exchange collection. For ranking candidate answers, we used three approaches: (1) using Math Stack Exchange answer scores, (2) using similarity of question and answer with Cross-encoder model trained on ARQMath-1 assessment, and (3) using Reciprocal Rank Fusion to combine the two previous scores. The final relevance score is calculated as the multiplication of question-question similarity and question-answer similarity score. These runs were competitive (obtaining the second-highest nDCG â€² for submitted runs <ref type="bibr" coords="14,89.29,456.34,15.71,4.95" target="#b16">[17]</ref>), even though we treated formulas represented in L A T E X as text.</p><p>For future work, in the Formula Retrieval task, we aim to consider other similarity features such as spatial features <ref type="bibr" coords="14,197.39,483.44,16.42,4.95" target="#b21">[22]</ref>, to improve our learning-to-rank results. Also, text features are ignored in our current formula retrieval models, and we aim to include them. For the Answer Retrieval task, our work is in an early stage, and there are several possible directions. First, our models are mostly trained with default parameters from the Sentence-BERT model and we plan to do a grid search on our models' parameters. Second, we represent formulae as L A T E X strings, and would like to use formula structure features within our model. One possible approach is replacing formulae by their concept name where available, similar to identifying synonymns in text search. For example, the formula ğ‘ 2 + ğ‘ 2 = ğ‘ 2 could be replaced with "Pythagorean Theorem".</p><p>We have found ARQMath-2 to be an excellent opportunity to further develop our ideas, and we look forward to ARQMath-3!</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,212.40,84.31,9.15;2,173.60,210.83,3.97,6.12;2,180.28,212.40,14.24,8.74;2,194.52,210.83,3.97,6.12;2,201.76,212.40,14.83,8.74;2,216.58,210.83,3.97,6.12;2,223.54,212.67,250.90,8.87"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Formula ğ‘ 2 + ğ‘ 2 = ğ‘ 2 represented as (a) Operator Tree and (b) Symbol Layout Tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.29,163.75,416.70,8.93;4,89.29,175.75,418.36,8.87;4,89.29,187.71,416.70,8.87;4,89.29,199.66,418.23,8.87;4,89.29,211.62,416.94,8.87;4,89.29,223.57,179.83,8.87"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Overview of the Tangent-CFT2 Retrieval Process. The tuples for different representations are extracted using Tangent-S. After the tuples are encoded using the Tangent-CFT model, vector representations of a query formula are obtained. These vectors are compared with the vector representation of formulae in the collection by using cosine similarity, and top-1000 most similar formulae are returned. Finally, retrieval results from different representations are combined using modified Reciprocal Rank Fusion (RRF) to get the final retrieval result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,89.29,169.50,416.69,8.93;5,88.93,181.50,417.05,8.87;5,89.29,193.46,343.15,8.87"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Overview of the Tangent-CFT2TED Retrieval Process. The candidate formulae are selected with the Tangent-CFT model. Then, using tree-edit distance, the formulae are re-ranked using SLT and OPT representations. The results are combined using Reciprocal Rank Fusion (RRF).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="11,89.29,248.47,416.70,8.93;11,89.29,260.48,250.66,8.87"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Sentence-BERT Cross-Encoder for identifying similar questions (a) and similarity of question and answer (b). The classifier gives a probability of relevance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="13,183.25,124.19,8.87,6.35;13,183.25,107.81,8.87,6.35;13,166.36,134.84,8.87,6.35;13,203.30,134.84,8.94,6.35;13,214.27,106.17,8.87,6.35;13,151.41,106.27,8.87,6.35;13,183.25,160.74,8.87,6.35;13,310.71,124.19,8.87,6.35;13,310.71,107.81,8.87,6.35;13,293.82,134.84,8.87,6.35;13,330.77,134.84,8.94,6.35;13,341.84,106.17,8.87,6.35;13,278.98,106.27,8.87,6.35;13,310.71,160.74,8.87,6.35;13,438.29,122.65,8.87,6.35;13,438.29,106.27,8.87,6.35;13,421.40,133.30,8.87,6.35;13,458.34,133.30,8.94,6.35;13,469.31,104.64,8.87,6.35;13,406.44,104.74,8.87,6.35;13,438.29,159.21,8.87,6.35;13,130.52,194.63,115.60,6.35;13,266.28,194.22,100.66,6.35;13,412.18,194.73,62.56,6.35"><head></head><label></label><figDesc>score: High, Medium and Low Relevance score: High and Medium Relevance score: High</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="13,89.29,209.03,418.23,8.93;13,89.02,221.03,416.96,8.87;13,89.29,232.99,274.33,8.87;13,363.63,231.14,2.30,6.12;13,366.42,232.99,139.57,8.87;13,89.29,244.89,416.69,8.96;13,89.29,256.90,416.70,8.87;13,89.29,268.85,271.95,8.87"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Venn diagram for average relevant answers retrieved by DPRL runs, on top-10 assessed hits. The dashed circle shows RRF, the dotted circle QASim and the circle with straight line indicates MathSE run.The summation of numbers in one circle shows is the average P â€² @10 for that system. Intersections between pairs of system are the average number of relevant hits shared by those two systems not in the intersection for all three systems. Numbers associated with only one circle/system are the average number of hits relevant hits found only by each individual system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,107.28,570.33,398.71,75.02"><head>â€¢ Training Embedding Models with fastText: Mathematical</head><label></label><figDesc>formulae are diverse, with relatively few training examples being available for any particular formula. For this reason, Mansouri et al chose to apply<ref type="bibr" coords="4,279.47,599.75,17.75,4.95" target="#b9">[10]</ref> the multi-scale fastText<ref type="bibr" coords="4,403.22,599.75,17.76,4.95" target="#b10">[11]</ref> n-gram embedding model get vector representations for each tuple. As the name suggests, fastText was originally designed for text. To apply it to math, each token is treated as it it were a text character, every whole tuple is treated as a text word, and the generated set of tuples is</figDesc><table coords="5,97.77,95.03,399.28,43.97"><row><cell></cell><cell></cell><cell></cell><cell>SLT Retrieval Result</cell><cell></cell><cell></cell></row><row><cell>Formula Query</cell><cell>Tangent-CFT</cell><cell>Formulas Candidate</cell><cell>Tangent-CFTED</cell><cell>RRF</cell><cell>Retrieval Result</cell></row><row><cell></cell><cell></cell><cell></cell><cell>OPT Retrieval Result</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,88.99,90.49,418.53,113.37"><head>Table 2</head><label>2</label><figDesc>DPRL runs for Task 2 on ARQMath-1 (45) and ARQMath-2 (58) topics. Tangent-S is the baseline system.</figDesc><table coords="7,107.15,123.20,378.78,80.66"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Evaluation Measures</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ARQMath-1</cell><cell></cell><cell cols="2">ARQMath-2</cell><cell></cell></row><row><cell></cell><cell>Data</cell><cell>Primary</cell><cell>nDCG â€²</cell><cell>MAP â€²</cell><cell>P â€² @10</cell><cell>nDCG â€²</cell><cell>MAP â€²</cell><cell>P â€² @10</cell></row><row><cell>Tangent-S</cell><cell>Math</cell><cell>âœ“</cell><cell>0.691</cell><cell>0.446</cell><cell>0.453</cell><cell>0.492</cell><cell>0.272</cell><cell>0.419</cell></row><row><cell>LtR29</cell><cell>Math</cell><cell></cell><cell>0.736</cell><cell>0.522</cell><cell>0.520</cell><cell>0.454</cell><cell>0.221</cell><cell>0.317</cell></row><row><cell>LtRall</cell><cell>Math</cell><cell>âœ“</cell><cell>0.738</cell><cell>0.525</cell><cell>0.542</cell><cell>0.445</cell><cell>0.216</cell><cell>0.333</cell></row><row><cell cols="2">Tangent-CFT2TED Math</cell><cell></cell><cell>0.648</cell><cell>0.480</cell><cell>0.502</cell><cell>0.410</cell><cell>0.253</cell><cell>0.464</cell></row><row><cell>Tangent-CFT2</cell><cell>Math</cell><cell></cell><cell>0.607</cell><cell>0.437</cell><cell>0.480</cell><cell>0.338</cell><cell>0.188</cell><cell>0.297</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,88.99,90.49,416.99,125.33"><head>Table 3 DPRL</head><label>3</label><figDesc>Corrected Runs for Task 2 on ARQMath-1 (45) and ARQMath-2 (58) topics. Tangent-S is the baseline system. Runs for ARQMath-2 are corrected (*).</figDesc><table coords="9,107.15,135.15,378.78,80.66"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Evaluation Measures</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ARQMath-1</cell><cell></cell><cell cols="2">ARQMath-2*</cell><cell></cell></row><row><cell></cell><cell>Data</cell><cell>Primary</cell><cell>nDCG â€²</cell><cell>MAP â€²</cell><cell>P â€² @10</cell><cell>nDCG â€²</cell><cell>MAP â€²</cell><cell>P â€² @10</cell></row><row><cell>Tangent-S</cell><cell>Math</cell><cell>âœ“</cell><cell>0.691</cell><cell>0.446</cell><cell>0.453</cell><cell>0.492</cell><cell>0.272</cell><cell>0.419</cell></row><row><cell cols="2">Tangent-CFT2TED Math</cell><cell></cell><cell>0.648</cell><cell>0.480</cell><cell>0.502</cell><cell>0.580</cell><cell>0.381</cell><cell>0.545</cell></row><row><cell>Tangent-CFT2</cell><cell>Math</cell><cell></cell><cell>0.607</cell><cell>0.437</cell><cell>0.480</cell><cell>0.565</cell><cell>0.364</cell><cell>0.516</cell></row><row><cell>LtRall</cell><cell>Math</cell><cell>âœ“</cell><cell>0.738</cell><cell>0.525</cell><cell>0.542</cell><cell>0.548</cell><cell>0.342</cell><cell>0.539</cell></row><row><cell>LtR29</cell><cell>Math</cell><cell></cell><cell>0.736</cell><cell>0.522</cell><cell>0.520</cell><cell>0.548</cell><cell>0.333</cell><cell>0.517</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="12,102.62,90.49,391.39,32.83"><head>Table 5</head><label>5</label><figDesc>An example of accepted answer for a question similar to ARQMath topic, assessed as Nonrelevant.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="12,88.99,218.04,416.99,113.06"><head>Table 6</head><label>6</label><figDesc>DPRL runs for Task 1 on ARQMath-1 (77) and ARQMath-2 (71) topics. The "linked MathSE posts" is a baseline system.</figDesc><table coords="12,106.98,262.55,381.31,68.55"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Evaluation Measures</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ARQMath-1</cell><cell></cell><cell cols="2">ARQMath-2</cell><cell></cell></row><row><cell></cell><cell>Data</cell><cell>Primary</cell><cell>nDCG â€²</cell><cell>MAP â€²</cell><cell>P â€² @10</cell><cell>nDCG â€²</cell><cell>MAP â€²</cell><cell>P â€² @10</cell></row><row><cell>Linked MathSE posts</cell><cell>n/a</cell><cell>âœ“</cell><cell>0.303</cell><cell>0.210</cell><cell>0.418</cell><cell>0.203</cell><cell>0.120</cell><cell>(0.282)</cell></row><row><cell>QASim</cell><cell>Both</cell><cell></cell><cell>0.417</cell><cell>0.234</cell><cell>0.369</cell><cell>0.388</cell><cell>0.147</cell><cell>0.193</cell></row><row><cell>RRF</cell><cell>Both</cell><cell>âœ“</cell><cell>0.422</cell><cell>0.247</cell><cell>0.386</cell><cell>0.347</cell><cell>0.101</cell><cell>0.132</cell></row><row><cell>MathSE</cell><cell>Both</cell><cell></cell><cell>0.409</cell><cell>0.232</cell><cell>0.322</cell><cell>0.323</cell><cell>0.083</cell><cell>0.078</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="14,88.98,90.49,417.00,133.53"><head>Table 7 P</head><label>7</label><figDesc>â€² @10 values for DPRL runs one different categories of questions in Task 1. There are 77 topics in ARQMath-1 and 71 in ARQMath-2 Task 1.</figDesc><table coords="14,106.31,133.46,382.66,90.56"><row><cell></cell><cell></cell><cell>Difficulty</cell><cell></cell><cell cols="2">Dependency</cell><cell></cell><cell></cell><cell>Subject</cell><cell></cell></row><row><cell></cell><cell cols="9">Hard Medium Easy Formula Text Both Computation Concept Proof</cell></row><row><cell>Topic Count (ARQMath-1):</cell><cell>24</cell><cell>21</cell><cell>32</cell><cell>32</cell><cell>13</cell><cell>32</cell><cell>26</cell><cell>10</cell><cell>41</cell></row><row><cell>QASim</cell><cell>0.342</cell><cell>0.376</cell><cell>0.384</cell><cell>0.288</cell><cell cols="2">0.638 0.341</cell><cell>0.296</cell><cell>0.520</cell><cell>0.378</cell></row><row><cell>RRF</cell><cell>0.383</cell><cell>0.371</cell><cell>0.397</cell><cell>0.316</cell><cell cols="2">0.638 0.353</cell><cell>0.296</cell><cell>0.540</cell><cell>0.405</cell></row><row><cell>MathSE</cell><cell>0.346</cell><cell>0.314</cell><cell>0.309</cell><cell>0.284</cell><cell cols="2">0.438 0.313</cell><cell>0.246</cell><cell>0.370</cell><cell>0.359</cell></row><row><cell>Topic Count (ARQMath-2):</cell><cell>19</cell><cell>20</cell><cell>32</cell><cell>21</cell><cell>10</cell><cell>40</cell><cell>25</cell><cell>19</cell><cell>27</cell></row><row><cell>QASim</cell><cell>0.184</cell><cell>0.115</cell><cell>0.247</cell><cell>0.300</cell><cell cols="2">0.130 0.153</cell><cell>0.204</cell><cell>0.116</cell><cell>0.237</cell></row><row><cell>RRF</cell><cell>0.105</cell><cell>0.065</cell><cell>0.191</cell><cell>0.181</cell><cell cols="2">0.140 0.105</cell><cell>0.132</cell><cell>0.100</cell><cell>0.156</cell></row><row><cell>MathSE</cell><cell>0.053</cell><cell>0.050</cell><cell>0.109</cell><cell>0.076</cell><cell cols="2">0.120 0.068</cell><cell>0.056</cell><cell>0.084</cell><cell>0.093</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="3,108.93,673.70,176.58,4.07"><p>https://github.com/MattLangsenkamp/tangent-s</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="4,108.93,673.68,115.87,4.07"><p>https://dlmf.nist.gov/LaTeXML/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="6,108.93,662.75,398.57,4.07;6,89.29,673.71,336.17,4.07"><p>Note that for learning to rank we used the original Tangent-CFT and Tangent-CFTED models, not the Tangent-CFT2 and Tangent-CFT2TED versions that we used for single-system submissions this year.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3" coords="10,108.93,673.71,270.26,4.07"><p>https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank <rs type="person">Matt Langsenkamp</rs> for providing the Tangent-S code on GitHub. This material is based upon work supported by the <rs type="funder">Alfred P. Sloan Foundation</rs> under Grant No. <rs type="grantNumber">G-2017-9827</rs> and the <rs type="funder">National Science Foundation (USA)</rs> under Grant No. <rs type="grantNumber">IIS-1717997</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_DfRFTTK">
					<idno type="grant-number">G-2017-9827</idno>
				</org>
				<org type="funding" xml:id="_vS2kyVJ">
					<idno type="grant-number">IIS-1717997</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="15,112.66,200.26,393.33,4.95;15,112.28,213.80,394.91,4.95;15,112.66,227.35,22.69,4.95" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="15,336.40,200.26,169.59,4.95;15,112.28,213.80,104.77,4.95">Advancing Math-Aware Search: The ARQMath-2 lab at CLEF</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,259.15,213.80,202.26,4.95">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,240.90,394.61,4.95;15,112.66,254.45,394.53,4.95;15,112.66,268.00,65.44,4.95" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="15,315.08,240.90,192.19,4.95;15,112.66,254.45,122.39,4.95">Finding old answers to new math questions: the ARQMath lab at CLEF</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,286.99,254.45,215.89,4.95">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,281.55,395.17,4.95;15,112.66,295.10,393.33,4.95;15,112.66,308.65,395.17,4.95;15,112.66,322.20,393.33,4.95;15,112.66,335.75,76.23,4.95" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="15,231.98,281.55,275.86,4.95;15,112.66,295.10,39.90,4.95">Sentence-BERT: Sentence embeddings using Siamese BERT-Networks</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,336.97,295.10,169.02,4.95;15,112.66,308.65,395.17,4.95;15,112.66,322.20,245.13,4.95">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Inui</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Ng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</editor>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,349.30,393.33,4.95;15,112.66,362.85,393.33,4.95;15,112.66,376.39,299.62,4.95" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="15,425.90,349.30,80.08,4.95;15,112.66,362.85,203.19,4.95">Tangent-CFT: An embedding model for mathematical formulas</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rohatgi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,341.27,362.85,164.72,4.95;15,112.66,376.39,269.98,4.95">Proceedings of the 2019 ACM SIGIR International Conference on Theory of Information Retrieval</title>
		<meeting>the 2019 ACM SIGIR International Conference on Theory of Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,389.94,394.61,4.95;15,112.14,403.49,364.90,4.95" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="15,280.46,389.94,207.29,4.95">DPRL systems in the CLEF 2020 ARQMath lab</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,112.14,403.49,334.20,4.95">Working Notes of CLEF 2020-Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,417.04,395.17,4.95;15,112.66,430.59,393.32,4.95;15,112.66,444.14,135.48,4.95" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="15,277.41,417.04,188.31,4.95">Learning to rank for mathematical formula</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,488.87,417.04,18.96,4.95;15,112.66,430.59,393.32,4.95;15,112.66,444.14,105.84,4.95">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,457.69,393.33,4.95;15,112.66,471.24,393.33,4.95;15,112.66,484.79,259.54,4.95" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="15,209.02,457.69,296.97,4.95;15,112.66,471.24,66.50,4.95">Layout and semantics: Combining representations for mathematical formula search</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Davila</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,206.75,471.24,299.24,4.95;15,112.66,484.79,229.90,4.95">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,498.34,393.32,4.95;15,112.66,511.89,393.33,4.95;15,112.66,525.44,367.47,4.95" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="15,330.36,498.34,109.68,4.95;15,474.74,498.34,31.24,4.95;15,112.66,511.89,228.45,4.95">Second CLEF lab on answer retrieval for questions on math</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,364.82,511.89,141.17,4.95;15,112.66,525.44,263.46,4.95">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note>Overview of ARQMath-2</note>
</biblStruct>

<biblStruct coords="15,112.66,538.98,394.53,4.95;15,112.66,552.53,73.62,4.95" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="15,277.57,538.98,222.06,4.95">Mcat math retrieval system for ntcir-12 mathir task</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">Y</forename><surname>Kristianto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Topic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aizawa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>NTCIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,566.08,394.52,4.95;15,112.66,579.63,341.46,4.95" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="15,278.14,566.08,224.45,4.95">Characterizing searches for mathematical concepts</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,150.31,579.63,246.99,4.95">ACM/IEEE Joint Conference on Digital Libraries (JCDL)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,593.18,393.33,4.95;15,112.66,606.73,375.98,4.95" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="15,331.70,593.18,174.29,4.95;15,112.66,606.73,50.99,4.95">Enriching word vectors with subword information</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,172.21,606.73,276.71,4.95">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,620.28,393.32,4.95;15,112.66,633.83,393.33,4.95;15,112.66,647.38,355.80,4.95" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="15,299.93,620.28,206.05,4.95;15,112.66,633.83,169.31,4.95">Reciprocal rank fusion outperforms condorcet and individual rank learning methods</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Buettcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,307.29,633.83,198.70,4.95;15,112.66,647.38,326.16,4.95">Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,660.93,395.17,4.95;16,112.66,90.22,393.33,4.95;16,112.66,103.77,123.88,4.95" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="15,222.65,660.93,221.93,4.95">Retrieving documents with mathematical content</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">W</forename><surname>Tompa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,467.90,660.93,39.92,4.95;16,112.66,90.22,393.33,4.95;16,112.66,103.77,94.24,4.95">Proceed-ings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>eed-ings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,117.32,393.61,4.95;16,112.66,130.87,122.89,4.95" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="16,413.82,117.32,92.45,4.95;16,112.66,130.87,36.27,4.95">Ntcir-12 mathir task overview</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aizawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kohlhase</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Topic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Davila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>NTCIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,144.41,393.33,4.95;16,112.66,157.96,324.95,4.95" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="16,167.87,144.41,150.45,4.95">Training linear SVMs in linear time</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,339.85,144.41,166.14,4.95;16,112.66,157.96,294.64,4.95">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,168.99,393.33,9.57;16,112.66,185.06,394.53,4.95;16,112.66,198.61,22.69,4.95" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="16,230.09,168.99,275.90,9.57;16,112.66,185.06,111.44,4.95">Extended formula normalization for ğœ€-retrieval and sharing of mathematical knowledge</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Normann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kohlhase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,249.89,185.06,208.93,4.95">Towards Mechanized Mathematical Assistants</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,212.16,393.33,4.95;16,112.66,225.71,395.17,4.95;16,112.66,239.26,394.52,4.95;16,112.66,252.81,89.77,4.95" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="16,336.89,212.16,105.35,4.95;16,468.44,212.16,37.55,4.95;16,112.66,225.71,216.64,4.95">):second clef lab on answer retrieval for questionson math</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,457.89,225.71,49.94,4.95;16,112.66,239.26,394.52,4.95;16,112.66,252.81,23.31,4.95">Proc. International Conference of the Cross-Language Evaluation Forum for European Languages, CEUR</title>
		<meeting>International Conference of the Cross-Language Evaluation Forum for European Languages, CEUR</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note>Overview of arqmath-2. working notes version</note>
</biblStruct>

<biblStruct coords="16,112.66,266.36,393.33,4.95;16,112.66,279.91,393.53,4.95;16,112.30,293.46,288.58,4.95" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="16,271.55,266.36,234.44,4.95;16,112.66,279.91,131.75,4.95">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,289.52,279.91,216.67,4.95;16,112.30,293.46,185.80,4.95">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,307.00,394.53,4.95;16,112.66,320.55,393.33,4.95;16,112.66,334.10,107.17,4.95" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-H</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>LukÃ¡cs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Miklos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kurzweil</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00652</idno>
		<title level="m" coord="16,168.85,320.55,267.34,4.95">Efficient natural language response suggestion for smart reply</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="16,112.66,347.65,394.61,4.95;16,112.28,361.20,379.28,4.95" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="16,112.28,361.20,269.02,4.95">A human generated machine reading comprehension dataset</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Marco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,404.10,361.20,57.15,4.95">CoCo@ NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,374.75,393.33,4.95;16,112.66,388.30,393.33,4.95;16,112.66,401.85,261.96,4.95" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="16,325.95,374.75,100.07,4.95;16,453.62,374.75,52.36,4.95;16,112.66,388.30,163.51,4.95">CLEF lab on answer retrieval for questions on math</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,297.91,388.30,208.08,4.95;16,112.66,401.85,188.80,4.95">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note>Overview of ARQMath</note>
</biblStruct>

<biblStruct coords="16,112.66,415.40,393.33,4.95;16,112.66,428.95,304.42,4.95" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="16,165.94,415.40,181.54,4.95">Spatial vs. Graph-Based Formula Retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Avenoso</surname></persName>
		</author>
		<ptr target="https://scholarworks.rit.edu/theses/10784" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
		<respStmt>
			<orgName>Rochester Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
