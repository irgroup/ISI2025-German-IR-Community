<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.02,75.44,450.96,17.04;1,72.02,96.20,82.85,17.04">BirdCLEF 2021: building a birdcall segmentation model based on weak labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.02,132.22,93.58,10.80"><forename type="first">Maxim</forename><forename type="middle">V</forename><surname>Shugaev</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Automation, Inc</orgName>
								<address>
									<addrLine>15400 Calhoun Drive, Suite 190</addrLine>
									<postCode>20855</postCode>
									<settlement>Rockville</settlement>
									<region>Maryland</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">CLEF</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,184.73,132.22,79.62,10.80"><forename type="first">Naoya</forename><surname>Tanahashi</surname></persName>
							<email>na.tanahashi@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Research and Development Group</orgName>
								<orgName type="institution">Hitachi, Ltd</orgName>
								<address>
									<addrLine>1-280, Higashi-koigakubo, 3 ; 2443 Fillmore St. #380-8802</addrLine>
									<postCode>185-8601, 94115</postCode>
									<settlement>Kokubunji, San Francisco</settlement>
									<region>Tokyo, CA</region>
									<country>Japan, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,283.25,132.22,70.99,10.80"><forename type="first">Philip</forename><surname>Dhingra</surname></persName>
							<email>philipkd@gmail.com</email>
						</author>
						<author>
							<persName coords="1,367.27,132.22,59.63,10.80"><forename type="first">Urvish</forename><surname>Patel</surname></persName>
							<email>urvishp80@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="department">Pirimid Fintech</orgName>
								<address>
									<addrLine>B-503 Mondeal Heights, SG Hwy, near Wide Angle Cinema</addrLine>
									<postCode>380015</postCode>
									<settlement>Ahmedabad</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.02,75.44,450.96,17.04;1,72.02,96.20,82.85,17.04">BirdCLEF 2021: building a birdcall segmentation model based on weak labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D7E934B2FDA59B23D86FFD37575AB9E6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bird identification</term>
					<term>BirdCLEF 2021</term>
					<term>Deep Learning</term>
					<term>Convolutional Neural Network</term>
					<term>multihead self-attention</term>
					<term>audio segmentation</term>
					<term>weak labels</term>
					<term>domain shift</term>
					<term>log-sum-exp aggregation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep-learning-based approach is applied to identify bird species in soundscapes. Two distinct model concepts targeted at audio classification and segmentation are considered. An audio segmentation model incorporating global multi-head self-attention to account for the interaction between different parts of audio is proposed, and peculiarities of building a segmentation model based on weak labels are discussed. To overcome the key challenges of BirdCLEF 2021, label weakness and domain mismatch, we developed a multistep training procedure with generation of segmentation pseudo labels and intelligent sampling of train audio, performed hand annotation of a part of the data, and used the noise from external data to mitigate the domain mismatch and improve model performance on soundscapes containing a substantial level of background noise. Our solution has reached 0.6605 F1 score at the private leader board and achieved top-18 among 816 teams and 1001 competitors of BirdCLEF 2021.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Each of us has heard bird songs in the everyday life. They are not only beautiful and sometimes inspiring but also may uncover the effect of human activity on nature. Changes in bird population and their behavior may indicate more global changes in the entire ecosystem, resulted, for example, by environmental pollution or global warming. To track these changes the ornithology community is collecting many hundreds of hours of audio recording every day. However, since there are more than 10,000 bird species, the task of bird classification based on their calls becomes almost impossible for the public and very difficult even for experts. Recent progress in the field of deep learning has revolutionized such areas as computer vision, natural language processing, and even generation and classification of audio. To harness these novel advancements for the improvement of an automatic bird classification system, Cornell Lab of Ornithology has hosted BirdCLEF 2021 competition at Kaggle <ref type="bibr" coords="1,72.02,585.48,11.79,9.94" target="#b0">[1]</ref>, the world's largest data science competition platform. The overview of this challenge is provided in refs. <ref type="bibr" coords="1,105.36,598.08,12.02,9.94" target="#b1">[2,</ref><ref type="bibr" coords="1,120.14,598.08,7.92,9.94" target="#b2">3]</ref>.</p><p>The objective of BirdCLEF 2021 is identification of bird calls in soundscapes collected in several places around the world. The model performance is evaluated base on row-wise micro averaged F1 score computed for 80 10-minute-long soundscapes split as 35/65 ratio for public and private leader board (LB). The provided data includes the following <ref type="bibr" coords="1,310.67,648.72,12.00,9.94" target="#b3">[4]</ref>:</p><p>(1) train short audio is a collection of short records of individual birds (62874 audios) uploaded to xenocanto.org website <ref type="bibr" coords="2,174.96,87.28,12.97,9.94" target="#b4">[5]</ref> (these files have been down-sampled to 32 kHz to match test set audio and converted to ogg format). The provided metadata contains a variety of information including primary and secondary labels: the main bird present in the recording and birds present in the background, respectively. The total number of considered classes is 397.</p><p>(2) train soundscapes are a collection of 20 soundscapes similar to ones used for model performance evaluation. The labels are provided for each 5-second chunk and may contain several bird songs simultaneously as well as nocall parts.</p><p>In the 2020 Cornell Birdcall Identification challenge <ref type="bibr" coords="2,328.47,175.84,12.83,9.94" target="#b5">[6]</ref> the best performance has been achieved for a model based on a convolutional neural network (CNN) applied to audio files converted into melscale spectrograms. Therefore, our approach is based on this method, and we considered two distinct model concepts targeted at audio classification and segmentation. Since the first of them is widely applied for bird classification, in our manuscript we focus on discussion of the segmentation approach, we proposed working on BirdCLEF 2021, and only briefly touch the key ideas used for building the classification model. The main challenges that arose in this competition include (1) label weakness and noise and (2) domain mismatch between train and test data. These challenges are addressed with the use of multistep training procedure including generation of pseudo labels (PL) for segmentation, sampling based on PL, performing domain mitigation as a separate step, and use of noise from external datasets as well as a smart selection of training audio chunks and manual labeling the part of the data. As the result, among 816 teams and 1001 competitors in this challenge, our team took 18th place by achieving a row-wise micro averaged F1 score of 0.6605 at private LB. In this working note, we introduce our solutions and findings from the competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Data preprocessing</head><p>The audio files, before use as an input, are converted into mel-scale spectrograms with 128 frequency channels. We considered two Fast Fourier Transform (FFT) window sizes nfft of 1024 and 2048. In our work, the larger value of nfft is observed to mitigate the domain mismatch between train and test data. This effect may result from a decrease in the signal from short noisy sounds due to averaging them within a wider FFT window. Meanwhile, since the duration of the bird call is typically much larger than nfft divided by the sampling rate, the birdcall signal preserves the amplitude. Despite we only considered two values of nfft, and the effect of nfft on the mitigation of the domain mismatch requires an additional study, in the final models we used nfft = 2048 because of the better performance. The models are trained on 5-second chunks, which corresponds to 128√ó313 and 128√ó512 mel-scale spectrograms image size for classification and segmentation models, respectively, because of the use of different stride size. The training is performed on short audio, while the train soundscapes are used as a cross-validation (CV) dataset as well as a source of audio segments with noise.</p><p>For augmentation we used MixUp modified to include labels from both audios with values of 1 and a large value of alpha of 4-5 to ensure mixing the spectrograms in approximately the same portion. Since sounds overlay each other rather than overlap, MixUp based augmentation is natural for audio data and provides a considerable performance boost. To mitigate the domain mismatch between train and test data, white and pink noises are added during training. Beyond that, noise extracted from train soundscapes and records from the Rain Forest dataset <ref type="bibr" coords="2,321.47,630.24,12.99,9.94" target="#b6">[7]</ref> is added during training of segmentation models, which provided important insights about the peculiarities of soundscape data as well as enforced the model to distinguish actual birdcalls from sounds similar to birdcalls (for example from frogs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Classification models: mitigation of the effect of weak labels</head><p>In BirdCLEF 2020 we used two distinct approaches: audio classification and segmentation. In this section, we explain the key details of the first of them. After the conversion of the input data into melscale spectrograms, it is passed to a CNN <ref type="bibr" coords="2,270.65,751.22,12.92,9.94" target="#b7">[8]</ref> for bird call classification. As a backbone we used ImageNet pre-trained ResNeSt50 network <ref type="bibr" coords="3,264.17,74.66,12.00,9.94" target="#b8">[9,</ref><ref type="bibr" coords="3,279.77,74.66,13.13,9.94" target="#b9">10]</ref>. Below we explain 3 main ideas used to build the model.</p><p>(1) Smart crop selection. The provided data has various lengths, and before passing it to a model, the data should be aligned to a particular length. We have chosen 5-second segments because it corresponds to the chunk length required for model evaluation. The simplest way to generate a 5-second chunk is a random crop of an input clip. However, as pointed out above, one of the challenges related to the provided data is label weakness: labels are assigned to entire clips. Therefore, after the selection of a random chunk, it may end up containing only a part where the bird is not calling, and the global label assigned to the chunk is not correct anymore. To improve the cropping algorithm, we came up with a simple hypothesis: before uploading audio to the xenocanto website, authors delete parts of audio when there is no song at the beginning or at the end of the clip. This hypothesis was confirmed by the Exploratory Data Analysis (EDA) that we conducted during this competition. Therefore, during training, we assumed that the first 5 seconds and the last 5 seconds of the audio almost certainly contain a birdcall. Specifically, instead of random cropping in 20% cases we selected a chunk from the beginning or the end of the file. In addition, we randomly shift the selected parts by 0-2 seconds to avoid using the same 5-second chunks. We have chosen this strategy because selection of only the first and the last 5-second chunks does not provide sufficient diversity.</p><p>(2) Use only short clips (length 60 seconds or less). The method of smart crop selection, described above, is not perfect, and there is a possibility of selection of a chunk with nocall. To decrease this probability, we have shortened the length of the audio used: for longer audio longer nocall parts may be present. Selection of 60-second clips or shorter helps to choose audio chunks with actual birdcalls. However, it has an obvious disadvantage of reducing the total number of training clips and the diversity of the data. In our case, the decrease of the number of clips from 62874 to 45423 did not hurt training, and more proper selection of 5-second chunks overcame this negative effect resulting in an increase in the model performance in comparison to our baseline.</p><p>(3) Hand labeling. We manually labeled time ranges of nocall and "human voice" in a part of short train audio clips. Labeling individual birdcalls is very time-consuming. Therefore, instead of labeling each birdcall we only marked regions with sufficiently long nocall parts, as depicted in Figure <ref type="figure" coords="3,494.74,416.25,8.71,9.94" target="#fig_0">1a</ref>. To avoid mixing the regions with primary and secondary labels and ensure that parts that are not marked as nocall actually include calls from primary birds, we only considered audio that does not have secondary labels and has a length of 16 seconds or less. These audios are unlikely to contain more than one bird. During labeling we realized that some of the audio files contain digitized human voices, as illustrated in Figure <ref type="figure" coords="3,164.18,479.49,9.20,9.94" target="#fig_0">1b</ref>. It appeared that a particular author included the voice when edited the data. And we went through all corresponding audio and marked parts with a human voice.  </p><formula xml:id="formula_0" coords="3,130.10,711.46,335.25,24.48">whiwre1/XC518092.ogg 1 0 [[25,1000]] ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶</formula><p>Our hand-labeled data is available at ref. <ref type="bibr" coords="4,254.10,74.66,16.88,9.94" target="#b10">[11]</ref>, and below we explain how to use it. An example of our annotation is listed in Table <ref type="table" coords="4,193.97,87.28,4.14,9.94" target="#tab_0">1</ref>. The row_id column contains the primary label and filename concatenated by a slash, while the human_voice and nocall columns indicate whether the data contains human voice and nocall, respectively. In the provided example we can see that for the data with the row_id of whiwre1/XC518092.ogg, the second element of its t_range is set to 1000, well beyond the length of the audio. This indicates that the human voice is present until the end of the audio.</p><p>The above procedures are targeted at mitigating the effect of weak labels by ensuring that a selected chunk contains the actual primary label rather than nocall. The data preparation in our training procedure can be described as the following. When the audio data is loaded, first we check whether the data contains human voice based on the row_id and cut the audio based on t_range to exclude the voice part. Next, we apply Smart Crop Selection (1) step and generate a 5-second chunk. If the loaded audio is indicated to have nocall parts, we check whether the selected chunk is included in any listed nocall intervals, and if it is the case, we reset the original label to nocall. We keep nocall parts to teach the model to deal with nocall chunks present in the evaluation set. Despite improvement in the model performance, the above-described procedure has the disadvantage of reducing the size of training dataset. In addition, the procedure of hand annotation of bird calls is very time-consuming. Therefore, in parallel, we have developed a multistep training procedure, based on label self-distillation, and applied it to train a birdcall segmentation model (Section 2.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Audio segmentation models</head><p>Similar to common audio classification methods <ref type="bibr" coords="4,315.55,359.47,11.70,9.94" target="#b7">[8]</ref>, in our approach the generated mel-scale spectrograms are passed through a convolutional neural network extracting features. However, in contrast to these methods, where the feature extractor is followed by pooling and generation of a prediction, we built a head performing a segmentation task and utilizing global all-versus-all attention to account for similarity and interactions of different parts of the produced audio sequence. Our model is schematically illustrated in Figure <ref type="figure" coords="4,232.61,422.73,4.14,9.94" target="#fig_1">2</ref>. In our study, we use ResNeXt50 backbone pre-trained in a semisupervised manner on one billion images <ref type="bibr" coords="4,264.84,435.45,17.03,9.94" target="#b11">[12]</ref>, which provides both a high level of computational efficiency and accuracy. For training at 5-second audio segments, the generated feature map has dimensions of 2048√ó4√ó16 (number of channels, frequency/temporal dimensions of feature maps). We compress the feature map with 4√ó1 convolution to 512√ó1√ó16 sequence representing a variation of audio feature over time with a resolution of approximately 0.3 seconds. This sequence is passed to 2 sequential multi-head self-attention blocks (8 heads) with skip connections followed by 1√ó1 convolution to generate 16√ónum classes sequence of predictions. In the initial model development performed by Dr. Shugaev, one of the authors of this manuscript, within the Cornell Birdcall Identification challenge <ref type="bibr" coords="4,502.16,523.89,16.81,9.94" target="#b12">[13]</ref>, it was observed that adding batch normalization, a nonlinearity, and a fully connected part to multihead self-attention blocks inhibits convergence on the competition data <ref type="bibr" coords="4,408.55,549.21,16.92,9.94" target="#b13">[14]</ref>. Therefore, they are excluded from our model. To perform training on labels assigned to a clip, the predicted sequence, corresponding to segmentation of the clip, should be collapsed. We use log-sum-exp (LSE) based aggregation. Following ref. <ref type="bibr" coords="4,90.47,738.24,16.91,9.94" target="#b14">[15]</ref>, we introduced a temperature of 5 to LSE. In addition, since bird calls appear only within a short time frame, we enforced a soft minimum of the predicted sequence (LSE with the temperature of -1) for all predictions, even ones having positive labels. This component is added to the total loss with a weight of 0.3. An additional important thing is using sufficiently short audio segments during training. Long segments help to overcome label weakness because the probability of finding a bird within a segment is increasing with its length. However, we realized that a model trained on long segments prefers to learn easy sounds ignoring the rest: it is often enough to find an easy example in a long sequence to assign the corresponding label. To enhance the discriminative ability of the model we used 5-second chunks during training, while inference is performed on longer, 10 min clips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Multistep training</head><p>To addressed the key challenges related to the data (label weakness and domain mismatch) we proposed 3 step train-segment-shift approach outlined below.</p><p>(1) We begin with training a segmentation model based on the original labels assigned to entire clips. The main difficulty is that clip labels are not equal to chunk labels because clips may have no bird calls in particular parts or bird calls from other species. So direct training of a model on clip labels assigned to chunks cannot produce a sufficiently accurate model but gives PL for the next step. As an objective, we use focal loss applied to global predictions generated with LSE pooling with temperatures 5 and -1, as described above. (2) Next, we run a similar setup as at the previous step but perform sampling of clips based on PL segmentation making sure that the primary label bird call is included in each selected audio segment. In addition, we apply segmentation loss to regions having high confidence pseudo labels (both positive and negative cases), while masking regions where confidence is not sufficient in segmentation loss evaluation. The total loss is composed of equal contributions of global and segmentation loss components. The segmentation component guides the model and improves the quality of produced segmentation labels, as illustrated in Figure <ref type="figure" coords="5,271.97,693.60,8.72,9.94" target="#fig_2">3a</ref>. Figure <ref type="figure" coords="5,321.19,693.60,9.20,9.94" target="#fig_2">3b</ref>, meanwhile, provides an example of melscale spectrogram with high and low confidence PL of the primary class marked by red and orange bars, respectively. At step (2) we were able to build a model reaching ~0.86 F1 CV based on out-of-fold predictions on short train audio <ref type="bibr" coords="5,212.46,731.52,12.80,9.94" target="#b3">[4]</ref> evaluated by taking the maximum values for predictions within the entire length of a file. However, when we tried to apply this model directly to provided train soundscapes (which are similar to test data), we reached only 0.67 CV (on soundscapes), which indicates the domain mismatch between short audio and soundscapes. The PL produced at this step are shared at ref. <ref type="bibr" coords="6,131.87,87.28,16.89,9.94" target="#b15">[16]</ref>.</p><p>(3) To accommodate for the domain mismatch, we need one additional step in building a model. When we plot the spectrograms for train soundscapes, we realized a presence of a substantial fraction of noise and background sounds, which likely degrade the model performance on the soundscapes. As discussed in the next section in more details, at this step the original signal from short train audio clips is mixed with pink noise, noise extracted from train soundscapes (using proper CV split to avoid the use of parts with extracted noise for model evaluation), and sounds from the Rain Forest challenge dataset <ref type="bibr" coords="6,106.56,175.84,11.69,9.94" target="#b6">[7]</ref>. This approach boosted the single model performance to 0.7598/0.6486 at public/private challenge LB. We would like to highlight the importance of step (2) and accurate segmentation PL for step <ref type="bibr" coords="6,92.17,201.16,11.68,9.94" target="#b2">(3)</ref>. Without noise the model is able to localize the areas with corresponding bird calls. Meanwhile, if extensive noise is added, the model needs hints on localization of birdcalls and ignoring the rest. Segmentation PL help to guide training under conditions of strong noise and background sounds added to the train data.</p><p>It should be noted that the use of a label self-distillation-based approach <ref type="bibr" coords="6,408.18,251.83,18.32,9.94" target="#b16">[17]</ref> to correct label errors and handle weak labels (which is at the core of the proposed multistep training method) is not a new idea but rather a common procedure to a specific set of tasks. For example, Born-Again Network <ref type="bibr" coords="6,504.94,277.15,18.19,9.94" target="#b17">[18]</ref> based approach has already been used in earlier BirdCLEF competitions <ref type="bibr" coords="6,396.81,289.75,18.45,9.94" target="#b18">[19]</ref> to mitigate the effect of weak labels.</p><p>Additional training details: training at steps (1) and ( <ref type="formula" coords="6,322.91,315.07,4.26,9.94">2</ref>) is performed for 64 epochs with nftt=1024, Ranger-Larese-Lookahead optimizer <ref type="bibr" coords="6,237.22,327.67,16.88,9.94" target="#b19">[20]</ref>. Step (3) is performed for 28-64 epochs with the best model selection with nfft=1024 and nfft=2048. We apply a cosine annealing schedule without warmup with the initial learning rate of 10 -3 for the backbone and 10 -2 for the head (at step (3) we reduced the maximum learning rates twice). Focal loss is used in all experiments, but the total objective function includes contributions from global soft maximum and soft minimum as well as segmentation loss based on PL, as described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Postprocessing</head><p>We have used 3 following postprocessing methods to improve the predictions of our models.</p><p>(1) Global correction. The test data is recorded at the same location within 10 minutes. Therefore, we assumed that if a bird call is detected with high confidence in one of the chunks, it is likely to have the same species call in other parts of the record. To account for this effect, we multiply the model predictions for a particular class by 1.3 if the maximum predicted value over the entire length of the record for this class is exceeding the threshold of 0.5. The values 1.3 and 0.5 are determined based on the maximization of the validation score.</p><p>(2) Power average. The predictions of individual models are combined according to the following</p><formula xml:id="formula_1" coords="6,72.02,555.17,114.19,15.12">equation: ùëù = ‚àö‚àë ùëù ùëñ 2 ùëÅ ‚ÅÑ ùëÅ</formula><p>, where N is the number of models, ùëù ùëñ is the prediction of i-th model, and p is the final prediction.</p><p>(3) Sliding window. Birdcalls may be located at chunk boundaries, which results in difficulties with assigning it to a specific chunk, both in provided audio annotation and in model predictions, as well as boundary effects if the inference is performed on 5-second chunks. To mitigate these effects, in addition to the original sequence of chunks we considered chunks shifted by 2.5-second and averaged our predictions according to the following rule: ùëù(ùë°) = 0.5 ‚Ä¢ ùëù ÃÉ(ùë°) + 0.25‚Ä¢ ùëù ùë† ÃÉ(ùë° -2.5) + 0.25‚Ä¢ ùëù ùë† ÃÉ(ùë° + 2.5), where ùëù ÃÉ(ùë°) is the chunk of the original sequence corresponding to the moment of time t, and ùëù ùë† ÃÉ are predictions for a shifted sequence. For example, a postprocessed prediction for (5,10) second chunk is generated as an average of the model prediction on (5,10) chunk with a weight of 0.5 and (2.5,7.5) and (7.5,12.5) chunks with weights of 0.25. This postprocessing naturally works during the aggregation step in segmentation models but requires a generation of two sets of predictions for classification models.</p><p>In our study, all 3 postprocessing methods are applied to classification model predictions. While only sliding windows postprocessing is appeared to be effective for segmentation models because global attention is already performing global correction of predictions. We also did not consider the information from metadata about recording location and time, which could help to filter out specific species and slightly boost the model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results and discussion</head><p>We started our experiments by exploring the performance of a simple classification-based approach. The first interesting observation we want to point out is the importance of secondary labels provided with the metadata. Including the labels to our objective with the value of 0.995 has boosted the performance of a model at public LB from 0.59 to 0.67. A "secondary label" means that other birds can be heard in the background at a lower volume. There is no distinction, however, between a bird that is in the foreground or the background, and, therefore, both of them should be included. However, one of the key difficulties related to training data is the presence of long segments without primary bird calls. For secondary labels this difficulty becomes even more pronounced since secondary birdcalls often appear only in several places of the audio files. Moreover, in our analysis we noticed that sometimes secondary birdcalls are not assigned or assigned but not present in the corresponding files. Therefore, we proposed two strategies outlined below: use of only short audio with additional hand annotation, where the impact of secondary birdcalls is minimum, and perform multistep training procedure with an iterative generation of PL.</p><p>The use of hand-annotated labels and chunk selection strategies described in Section 2.2 improved the performance of our classification models to 0.7316 and 0.7157 for CV and public LB, respectively. This result is produced based on the power average of predictions of models trained on 5 CV splits of the original short train clips with approximately the same distribution of records for each class (stratified 5-fold split). To improve model performance, we considered sliding window and global correction postprocessing (Section 2.5). The first one has boosted our CV to 0.7393 and public LB score to 0.7161, while global correction has improved CV to 0.7531 and public LB score to 0.7188. So, both of the considered postprocessing techniques increase the prediction accuracy, and their combination results in the final CV of 0.7611 and public LB score of 0.7479 (Table <ref type="table" coords="7,342.19,410.71,4.00,9.94" target="#tab_1">2</ref>).</p><p>As an alternative, we performed training using the train-segment-shift concept, proposed in the previous section. The initial attempt of training a model at step (3) with white noise only resulted in quite a low CV of 0.7064 in comparison with our classification model, Table <ref type="table" coords="7,423.70,448.65,4.14,9.94" target="#tab_1">2</ref>. To boost the model performance and reduce the domain gap between train and evaluation data, we mixed training data with noise extracted from train soundscapes (parts labeled as nocall), which resulted in the CV of 0.7778. This value is substantially higher than the CV of the classification model, but the obtained public LB score is lower, 0.7231. Since the private LB performance of the model was not available to us during experiments (which appeared to agree with CV), we assumed the presence of implicit leaks appearing due to a similarity between the provided train soundscapes.</p><p>We attribute the observed public LB mismatch between our classification models, not using train soundscape noise, and segmentation models to a lower value of nftt = 1024 used in the second case. While this parameter is not playing a substantial role for training on clean data, it appeared to be important for noisy data, and a larger value of nftt = 2048 mitigates the domain mismatch. In addition, to eliminate possible implicit leaks resulted from the use of train soundscapes we started using data provided with the Rain Forest challenge <ref type="bibr" coords="7,248.60,600.48,11.76,9.94" target="#b6">[7]</ref>. This data contains multiple background sounds and sounds similar to bird calls from nonbird species, i.e., frogs. While this data also includes several bird species, we assumed that they are geographically different from the ones considered in the BirdCLEF 2021. This additional data has helped the model to learn how to ignore sounds similar to bird calls as well as bird calls from unknown species. The effect of both of these modifications has boosted the performance of our segmentation model to 0.7790 and 0.7598 for local CV and public LB, respectively (Table <ref type="table" coords="7,502.66,663.72,3.96,9.94" target="#tab_1">2</ref>).</p><p>The inference for segmentation models is performed on full-length, 10 min, clips, which enables global all-versus-all self-attention to account for the interaction between different parts of the audio file and find similar patterns. Inference of our segmentation model on separated 5-second chunks gives only 0.7499 F1 CV, while inference on full-length audio increases this value to 0.7654. Sliding window postprocessing further boosts the value to 0.7790 CV. This observation suggests that looking into the entire audio clip is important to make a decision about a specific part. We also verified that the performance of the model is not changed significantly between recording locations. The CV evaluated on COR location is 0.8073 at the fraction of nocall of 0.699. Meanwhile, CV for SSW location, after reweighting the model score to the same fraction of nocall as in COR soundscapes, has 0.7935 F1. The combination of our final segmentation and final classification models with weights of 0.7 and 0.3, respectively, resulted in 0.8004 CV and 0.7735 and 0.6605 public and private LB scores. This significant improvement is resulted from the inherently different nature of the considered approaches. However, our initial experiments have demonstrated that a combination of nine models having a public LB performance of 0.67 produces a prediction scored at 0.70 F1. Therefore, we can expect that incorporation of additional classification and segmentation models (trained with a different split and slight variation of the training procedure) in the ensemble will easily result in a further performance boost. The predictions of the segmentation model for the first two minutes of 10534_SSW train soundscape are depicted in Figure <ref type="figure" coords="9,170.78,87.28,10.45,9.94" target="#fig_3">4a</ref> for the top-10 predicted classes. The model is quite confident in the detection of nocall. However, sometimes the model may be confused with a prediction of particular cases. For example, a swaspa call with some noticeable probability may correspond to rewbla. Also, we noticed a number of places where the model predicts a birdcall with sufficient confidence, but there are no corresponding ground truth labels. Listening to the audio, we could recognize the presence of birdcalls at 18, 36, 43, 66, 73, 98, 111, and 120 seconds, as suggested by the model, but since we are not experts in bird classification, we cannot make a conclusion about the correctness of the model predictions. Consideration of audio from a different location, 11254_COR, in Figure <ref type="figure" coords="9,417.82,175.84,11.04,9.94" target="#fig_3">4b</ref> does not show any peculiarities in the model performance. The model predicts annotated birdcalls as well as a number of birdcalls at 6, 8, 85 seconds that could be recognized by listening to the audio but do not have an annotation. We understand that human labeling of soundscapes is a very challenging task given a variety of bird species as well as the complexity of vocalizations for specific species. However, the observation from Figure <ref type="figure" coords="9,127.46,239.08,5.52,9.94" target="#fig_3">4</ref> may suggest that some parts of the provided soundscapes with bird calls were labeled by expects as nocall because of insufficient confidence or significant background noise. To exclude missing annotation and improve the accuracy of the ground truth labels, iterative AI (artificial intelligence) assisted labeling may be utilized. In this case, a human expert compares his/her labels with AI-generated predictions and decides on labels correction.</p><p>Finally, we would like to discuss possible ways to improve our approach. Since our efforts on the development of the classification and segmentation models were performed in parallel, step <ref type="bibr" coords="9,473.47,315.07,12.78,9.94" target="#b0">(1)</ref> training is done with random chunk selection. Incorporation of a more elaborate procedure, used for building a classification model, would improve the quality of the model. Next, in our study use of both noise from train soundscapes and noise from the Rain Forest dataset resulted in a noticeable improvement in model performance. However, we did not have a chance to perform an additional study targeted at finetuning a model trained with Rain Forest data with noise extracted from train soundscapes. Since the latter one includes the information about artifacts specific to devices used for soundscape data acquisition, such finetuning would likely further improve the performance. Regarding the segmentation model, we used short, 5-second, chunks to improve the discriminative ability of the model. Meanwhile, longer chunks could help to teach the model to consider the interaction between different parts of the audio. Another option would be consideration of the head from PANNs models <ref type="bibr" coords="9,351.76,441.57,18.35,9.94" target="#b20">[21]</ref> and comparison it with LSE-based aggregation. Finally, the performance of our model at the evaluation set could be improved with more careful threshold selection, performed with bootstrapping and accounting for the variation of the fraction of nocall chunks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Summary</head><p>This report describes a deep-learning-based approach for identification of birdcalls in soundscapes. Convolutional neural networks are applied to mel-scale spectrograms to perform audio classification and segmentation. To overcome the main challenges of BirdCLEF 2021, label weakness and domain mismatch between train and test data, we have proposed a multistep train-segment-shift training procedure producing segmentation PL first and then performing mitigation of the domain mismatch with using noisy data but cleaned labels. As a source of noise, we considered train soundscapes and Rain Forest dataset <ref type="bibr" coords="9,159.32,613.20,11.77,9.94" target="#b6">[7]</ref>, containing background noise and sounds from nonbird species, i.e., frogs. Use of the noise has improved the model performance on soundscapes. In addition, we experimented with an intelligent selection of audio chunks during training and hand labeling of a part of the data to build a classification model. One of the interesting observations in our work is the effect of nftt on the model robustness to domain mismatch, and a larger value of nftt = 2048 improves model performance on soundscapes with background noise. Our findings have helped us to build a model that achieved 0.6605 F1 score at the private leader board, which corresponds to the top-18 among 816 teams and 1001 competitors of BirdCLEF 2021.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,72.02,628.90,446.16,11.04;3,72.00,514.06,451.33,112.55"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of spectrograms with hand labels for nocall (a) and human voice (b) time ranges.</figDesc><graphic coords="3,72.00,514.06,451.33,112.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,72.02,686.98,303.41,11.04;4,77.75,571.80,438.95,112.95"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Schematic illustration of the proposed model architecture.</figDesc><graphic coords="4,77.75,571.80,438.95,112.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,72.02,540.07,451.24,11.04;5,72.02,553.51,451.09,11.04;5,72.02,566.95,451.19,11.04;5,72.02,580.27,451.16,11.04;5,72.02,593.74,451.10,11.04;5,127.25,306.82,339.80,230.29"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) illustration of segmentation predictions for top-5 classes on XC165287 file for models trained at step-1 and step-2 setups (each spike corresponds to a birdcall). Blue color represents primary label prediction, and orange corresponds to one of the secondary labels listed in the provided metadata. (b) An example of a 5-second mel-scale spectrogram extracted from XC449522 file with high and low confidence predictions for a step-2 model outlined by red and orange bars, respectively.</figDesc><graphic coords="5,127.25,306.82,339.80,230.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,72.02,731.02,451.18,11.04;8,72.02,744.46,451.12,11.04;8,72.02,757.92,101.64,11.04;8,84.30,376.20,426.40,352.58"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of the top 10 model predictions on the first two minutes of 10534_SSW (a) and 11254_COR (b) train soundscapes. The ground truth labels are listed on the bottom. The dashed line outlines the threshold.</figDesc><graphic coords="8,84.30,376.20,426.40,352.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,134.06,667.42,324.35,41.64"><head>Table 1 :</head><label>1</label><figDesc>Example of hand annotation</figDesc><table coords="3,134.06,683.14,324.35,25.92"><row><cell>row_id</cell><cell>human_voice</cell><cell>nocall</cell><cell>t_range</cell></row><row><cell>bcnher/XC545676.ogg</cell><cell>0</cell><cell>1</cell><cell>[[1.5,7]]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,72.02,111.50,441.92,149.07"><head>Table 2 :</head><label>2</label><figDesc>results of key experiments</figDesc><table coords="8,80.78,127.10,433.16,133.47"><row><cell>Setup</cell><cell>local CV</cell><cell cols="2">public LB private LB</cell></row><row><cell>Initial classification model with secondary labels</cell><cell>0.7138</cell><cell>0.6767</cell><cell>0.5874</cell></row><row><cell>Classification model with hand label annotation + PP 2</cell><cell>0.7316</cell><cell>0.7157</cell><cell>0.6036</cell></row><row><cell>Classification model with hand label annotation + PP 2,3</cell><cell>0.7393</cell><cell>0.7161</cell><cell>0.6023</cell></row><row><cell>Classification model with hand label annotation + PP 1,2</cell><cell>0.7531</cell><cell>0.7188</cell><cell>0.6058</cell></row><row><cell>Classification model with hand label annotation + all PP</cell><cell>0.7611</cell><cell>0.7479</cell><cell>0.6188</cell></row><row><cell>Segmentation model step (3): nfft=1024, no noise</cell><cell>0.7064</cell><cell>-</cell><cell>-</cell></row><row><cell>Segmentation model step (3): nfft=1024, train noise</cell><cell>0.7778</cell><cell>0.7231</cell><cell>0.6401</cell></row><row><cell>Segmentation model step (3): nfft=2048, rain forest noise</cell><cell>0.7790</cell><cell>0.7598</cell><cell>0.6486</cell></row><row><cell>Segmentation + classification model</cell><cell>0.8004</cell><cell>0.7735</cell><cell>0.6605</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,110.30,758.90,382.71,9.94" xml:id="b0">
	<monogr>
		<ptr target="https://www.kaggle.com/c/birdclef-2021/" />
		<title level="m" coord="9,110.30,758.90,165.87,9.94">BirdClef 2021-Birdcall Identification</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.30,74.66,412.76,9.94;10,110.30,87.28,412.88,9.94;10,110.30,100.00,372.80,9.94" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,180.17,74.66,342.89,9.94;10,110.30,87.28,271.66,9.94">Overview of LifeCLEF 2021: a System-oriented Evaluation of Automated Species Identification and Species Distribution Prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,392.68,87.28,130.49,9.94;10,110.30,100.00,281.95,9.94">Proceedings of the Twelfth International Conference of the CLEF Association (CLEF 2021)</title>
		<meeting>the Twelfth International Conference of the CLEF Association (CLEF 2021)<address><addrLine>lifeclef</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.30,112.60,412.88,9.94;10,110.30,125.32,412.86,9.94;10,110.30,137.92,397.40,9.94" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,110.30,125.32,364.59,9.94">Overview of BirdCLEF 2021: Bird call identification in soundscape recordings</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Velling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqu√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,484.06,125.32,39.10,9.94;10,110.30,137.92,302.86,9.94">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>birdclef</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.30,150.41,412.96,10.05;10,110.30,163.24,43.52,9.94" xml:id="b3">
	<monogr>
		<ptr target="https://www.kaggle.com/c/birdclef-2021/data" />
		<title level="m" coord="10,110.30,150.41,212.53,10.05">BirdClef 2021 -Birdcall Identification data</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.30,175.84,402.11,9.94" xml:id="b4">
	<monogr>
		<ptr target="https://www.xeno-canto.org/" />
		<title level="m" coord="10,110.30,175.84,240.24,9.94">Xeno-Canto Sharing bird sound from around the world</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.30,188.56,412.96,9.94;10,110.30,201.16,53.19,9.94" xml:id="b5">
	<monogr>
		<ptr target="https://www.kaggle.com/c/birdsong-recognition/" />
		<title level="m" coord="10,110.30,188.56,200.51,9.94">Cornell Birdcall Identification challenge</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.30,213.76,412.96,9.94;10,110.30,226.48,125.23,9.94" xml:id="b6">
	<monogr>
		<ptr target="https://www.kaggle.com/c/rfcx-species-audio-detection/data" />
		<title level="m" coord="10,110.30,213.76,235.72,9.94">Rainforest Connection Species Audio Detection data</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.30,239.08,413.00,9.94;10,110.30,251.83,187.11,9.94" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="10,255.53,239.08,262.96,9.94">Automatic tagging using deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Fazekas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00298v1</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,110.30,264.43,412.81,9.94;10,110.30,277.15,413.00,9.94" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m" coord="10,187.25,277.15,153.11,9.94">ResNeSt: Split-Attention Networks</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,110.30,289.75,412.84,9.94;10,110.30,302.35,154.11,9.94" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<title level="m" coord="10,181.49,289.75,97.89,9.94">Pytorch image model</title>
		<imprint>
			<date type="published" when="2021-06-02">June 2, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.30,315.37,290.78,10.80;10,110.30,328.87,396.68,9.94" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tanahashi</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/naoism/birdclef-2021-handlabel-data" />
		<title level="m" coord="10,174.26,316.03,195.01,9.94">BirdCLEF 2021 Handlabel_data (Version 1)</title>
		<imprint>
			<date type="published" when="2021-06-09">June 9, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.30,341.47,412.86,9.94;10,110.30,354.19,334.64,9.94" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Zeki Yalniz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>J√©gou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00546v1</idno>
		<title level="m" coord="10,390.79,341.47,132.37,9.94;10,110.30,354.19,140.81,9.94">Billion-scale semi-supervised learning for image classification</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,110.30,366.79,412.88,9.94;10,110.30,379.39,412.86,9.94;10,110.30,392.11,35.88,9.94" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">V</forename><surname>Shugaev</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/c/birdsong-recognition/discussion/183258" />
		<title level="m" coord="10,182.33,366.79,267.69,9.94">Cornell Birdcall Identification discussion 39th place solution</title>
		<imprint>
			<date type="published" when="2021-06-09">June 9, 2021</date>
		</imprint>
	</monogr>
	<note>top1 at public</note>
</biblStruct>

<biblStruct coords="10,110.30,404.71,412.96,9.94;10,110.30,417.45,199.31,9.94" xml:id="b13">
	<monogr>
		<ptr target="https://www.kaggle.com/c/birdsong-recognition/data" />
		<title level="m" coord="10,110.30,404.71,211.15,9.94">Cornell Birdcall Identification Challenge data</title>
		<imprint>
			<date type="published" when="2021-06-28">June 28, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.30,430.05,412.74,9.94;10,110.30,442.65,218.93,9.94" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="10,244.73,430.05,278.31,9.94;10,110.30,442.65,40.53,9.94">From Image-level to Pixel-level Labeling with Convolutional Networks</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.6228</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,110.30,455.37,412.88,9.94;10,110.30,467.97,122.23,9.94" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Shugaev</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/iafoss/birdclef-2021-oof" />
		<title level="m" coord="10,170.30,455.37,93.79,9.94">BirdCLEF 2021 OOF</title>
		<imprint>
			<date type="published" when="2021-06-28">June 28, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.30,480.69,412.86,9.94;10,110.30,493.29,148.35,9.94" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="10,239.69,480.69,248.14,9.94">Self-Distillation as Instance-Specific Label Smoothing</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05065</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,110.30,505.89,412.52,9.94;10,110.30,518.61,221.69,9.94" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04770</idno>
		<title level="m" coord="10,434.11,505.89,88.71,9.94;10,110.30,518.61,40.48,9.94">Born Again Neural Networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,110.30,531.21,412.83,9.94;10,110.30,543.93,379.35,9.94" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schl√ºter</surname></persName>
		</author>
		<ptr target="http://www.ofai.at/~jan.schlueter/pubs/2018_birdclef.pdf" />
		<title level="m" coord="10,168.01,531.21,319.59,9.94">Bird Identification from Timestamped, Geotagged Audio Recordings</title>
		<imprint>
			<date type="published" when="2021-06-28">June 28, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.30,556.53,412.88,9.94;10,110.30,569.13,24.84,9.94" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="10,170.66,556.53,41.63,9.94">Over</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grankin</surname></persName>
		</author>
		<ptr target="https://github.com/mgrankin/over9000" />
		<imprint>
			<date type="published" when="2021">9000. June 9, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.30,581.88,412.87,9.94;10,110.30,594.48,412.77,9.94;10,110.30,607.20,119.07,9.94" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="10,427.78,581.88,95.39,9.94;10,110.30,594.48,330.75,9.94">PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10211v5</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
