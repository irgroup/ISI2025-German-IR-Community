<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,371.94,15.42;1,89.29,106.66,359.75,15.42;1,89.29,128.58,93.45,15.43">Birdcall Identification Using CNN and Gradient Boosting Decision Trees with Weak and Noisy Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,89.29,156.89,82.75,11.96"><forename type="first">Naoki</forename><surname>Murakami</surname></persName>
							<email>murakami-naoki167@g.ecc.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Science and Technology</orgName>
								<orgName type="institution">University of Tokyo</orgName>
								<address>
									<addrLine>7-3-1, Hongo, Bunkyo-ku</addrLine>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,184.69,156.89,74.00,11.96"><forename type="first">Hajime</forename><surname>Tanaka</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Program of Medicine</orgName>
								<orgName type="department" key="dep2">School of Medicine</orgName>
								<orgName type="institution">Hiroshima University</orgName>
								<address>
									<addrLine>1-2-3 Kasumi, Minami-ku</addrLine>
									<postCode>734-8551</postCode>
									<settlement>Hiroshima</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,289.69,156.89,98.52,11.96"><forename type="first">Masataka</forename><surname>Nishimori</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">MNES</orgName>
								<address>
									<addrLine>1-2-27, Honmachi, Shinonome, Minami-ku</addrLine>
									<settlement>Hiroshima City, Hiroshima</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,371.94,15.42;1,89.29,106.66,359.75,15.42;1,89.29,128.58,93.45,15.43">Birdcall Identification Using CNN and Gradient Boosting Decision Trees with Weak and Noisy Supervision</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">074FC28D24261E8174790CC44F34DF2C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bird Species Recognition</term>
					<term>BirdCLEF2021</term>
					<term>Machine Learning</term>
					<term>CNN</term>
					<term>Gradient Boosting Decision Trees</term>
					<term>Weak Label</term>
					<term>Noisy Label</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a winning solution to the BirdCLEF2021 challenge, in which we detected bird calls from a sound source and classified the type of birds successfully. Our method consists of a multi-stage pipeline using CNN and Gradient Boosting Decision Trees. Using a unique labeling scheme, our models can learn efficiently from data with noisy and weak labels. By transforming the sound recognition task into a time-series tabular data classification task, the model of our method can learn various information such as the preceding and following sounds, regional characteristics, and seasonality. In addition, it can learn with fewer computational resources. The final submission of our method achieved a row-wise micro averaged F1 score of 0.7620 for the public test set and 0.6932 for the private test set. Our method is versatile enough to be applied to tasks on other time-series data that are labeled only for the whole. This reduces the cost of annotations required for training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The ecology of birds, including their habitats, has a huge relationship with the quality of their environment. By identifying which areas birds appear in, it is possible to track current biodiversity trends. However, to keep tracking them on a large scale is not easy to do because of the burden on observers <ref type="bibr" coords="1,211.61,511.84,11.43,10.91" target="#b0">[1]</ref>.</p><p>BirdCLEF2021 <ref type="bibr" coords="1,164.76,525.39,11.28,10.91" target="#b1">[2]</ref>, one of the LifeCLEF challenges <ref type="bibr" coords="1,314.65,525.39,13.02,10.91" target="#b2">[3]</ref>, was a contest to identify bird calls from recordings of soundscapes by training a model mainly using bird sound sources. The better the model it was, the more accurately it would be able to detect bird calls and classify the type of birds from large amounts of audio.</p><p>One of the difficulties of this contest was that the domains of the training data and the test data were different. Most of the training data, we call train_short_audio, consisted of recordings and metadata of 397 different bird calls uploaded by users to https://www.xeno-canto.org/. The peculiarities of the dataset posed the following difficulties:</p><p>‚Ä¢ Due to incomplete labeling, it was unclear which of the labeled birds was singing within each short segment. ‚Ä¢ The label given itself might have been wrong.</p><p>‚Ä¢ The measure of whether a bird was singing or not was unclear.</p><p>Other data provided by BirdCLEF2021 were train_soundscapes and test_soundscapes. These data contained soundscape recordings from four different recording locations. The major difference between train_short_audio and train/test_soundscapes were their recording environments and labeling method. Speaking of the labeling method, train_short_audio was labeled for the entire sound source, but train/test_soundscapes were annotated every 5 seconds. Participants in this contest needed to accurately predict which of 397 different birds were singing and which were not in a 5-second segment. Participants also had to predict nocall if none of the birds were singing. However, a simple sound recognition model could not fully handle this challenge because not only one segment but also the segments before and after it might have been meaningful.</p><p>There are many different ways to perform sound recognition with deep learning. Some use raw, continuous data of audio sources as is <ref type="bibr" coords="2,284.65,340.19,11.55,10.91" target="#b3">[4]</ref>, while other approaches transform audio data into images and then use Convolutional Neural Networks (CNN). For example, the audio can be transformed into a spectrogram using the Fourier transforms or other techniques and then trained with CNN <ref type="bibr" coords="2,172.03,380.83,11.43,10.91" target="#b5">[5]</ref>.</p><p>However, in this contest, using only audio data was not the best learning method. It was difficult to make a decision based on the sound alone since it included many factors such as dialect, degree of mastery of the sound, and similar birds. It was possible to classify with high accuracy by taking into account regional and seasonal characteristics, environmental sounds, and segments before and after the audio comprehensively.</p><p>One of the most promising methods for learning such meta-information and time-series information as tabular data is Gradient Boosting Decision Trees (GBDT) <ref type="bibr" coords="2,409.52,475.68,11.33,10.91" target="#b6">[6]</ref>. It is characterized by its ability to learn quickly and predict something with high accuracy, and there are efficient implementations such as XGBoost <ref type="bibr" coords="2,244.53,502.78,11.43,10.91" target="#b7">[7]</ref>, Lightgbm <ref type="bibr" coords="2,308.16,502.78,11.43,10.91" target="#b8">[8]</ref>, and CatBoost <ref type="bibr" coords="2,388.67,502.78,11.43,10.91" target="#b9">[9]</ref>.</p><p>In this paper, we present a three-step method that achieved highly accurate prediction by learning from sound and metadata. The models of the first stage were used for training of the second and third stages, and the models of the second and third stages were used for inference (Figure <ref type="figure" coords="2,123.80,556.97,3.63,10.91" target="#fig_0">1</ref>). Our method has a low learning cost and is versatile enough to be applied to other tasks on time-series data that are labeled only for the whole.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods</head><p>We describe the details of our pipeline consisting of three stages. First, Section 2.1 gives details about the dataset to be used and presents what data will be used at which stage. In Section 2.2, we explain the details of the nocall detector in the first stage. The predictions of this model will be used during the training of the second and third stages. Section 2.3 describes the second stage of the second and third stages, so it was not used for inference. In the second stage, CNN was used to predict the probability of occurrence for 397 bird species. In the third stage, GBDT was used to predict whether the candidate extracted in the second stage was the correct answer.</p><p>model for multi-label classification with CNN using only audio converted to images. Section 2.4 describes the third stage model, which is trained with Gradient Boosting Decision Trees using metadata and predictions from the second stage. In Section 2.5, we describe a threshold optimization using ternary search and a method that mixes nocall and bird labels in the final prediction to improve the score while expressing ambiguity, which we call nocall injection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Data Sets</head><p>This section describes the data given in BirdCLEF2021 and the external data sets we used.</p><p>Training Data. BirdCLEF2021 provided train_short_audio and train_soundscapes as training and validation data. train_short_audio contained the sounds of 397 bird species that had been uploaded by xeno-canto users. These files were re-sampled to 32 kHz to match the audio in the test set. The label was given only in audio units. While the primary label of the bird that appeared primarily in the audio was given for all audio, the secondary label of other birds that also appeared in the audio might have been missing. It was also possible that the labels themselves were wrong because the data was uploaded by xeno-canto users. It also contained meta-information about the source, such as the latitude and longitude of the recording, and the date. train_soundscapes was a set of 20 10-minute files annotated every 5 seconds. This data set was in the same format as the test data. Each file was recorded in either COR (Costa Rica) or SSW (Sapsucker Woods, Ithaca, New York, USA). The location and date of the recording were given.</p><p>Test Data. test_soundscapes had a total of 80 files consisting of 10 minutes of audio sampled at 32 kHz. Each file was recorded in either COR (Costa Rica), COL (Colombia), SNE (Sierra Nevada, California, USA), or SSW (Sapsucker Woods, Ithaca, New York, USA). The location and date of recording of these files were known. Participants were asked to predict what birds were singing in segments separated by 5 seconds for this audio data. This data was used in the contest to evaluate the results. External Data. Since further improvement in accuracy could be achieved by using data other than that provided by BirdCLEF2021, we used two additional external data sources. The first was freefield1010, which was annotated again in the Bird Audio Detection challenge <ref type="bibr" coords="4,440.19,249.63,16.12,10.91" target="#b10">[10]</ref>, instead of the originally published annotation of 7,690 10-second audio samples <ref type="bibr" coords="4,404.31,263.18,16.39,10.91" target="#b11">[11]</ref>. The data without birdsong was divided into 5,755 segments, while the data with birdsong was divided into 1,935 segments, and every 10 seconds of audio was given a label. The second was the validation data in BirdCLEF2020 <ref type="bibr" coords="4,167.14,303.83,16.28,10.91" target="#b12">[12]</ref>. There was a total of 2 hours of audio data, but we split it into 5-second segments and used only the segments where no bird calls appeared.</p><p>Pre-processing. Table <ref type="table" coords="4,193.88,344.48,4.97,10.91" target="#tab_0">1</ref> shows the stages in which the data was used. All datasets were divided into segments of 5 or 7 seconds. Using as long a segment as possible increases the likelihood that the birdcall will be included in the segment and will match the label added to the entire audio. Therefore, we decided to use 7-second segments when training the first and second stage models. For the BirdCLEF2020 validation data, we used segments divided into 5-second segments, since the data was labeled every 5 seconds. The training data was converted to mel spectrograms in advance. Although this had the disadvantage that data augmentation was not possible for the original audio data, it was superior in that it reduced the cost of converting the audio to a mel spectrogram during training time, and allowed faster training. We used 128 mel-scaled frequencies in the range of 0Hz to 16000Hz. When converting the audio to a mel spectrogram, we set the length of the FFT window to 3200 and the hop length to 800. For example, a 7-second segment would be converted to an image with a resolution of 128x281 (frequencies √ó time).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">First Stage</head><p>In this stage, we created nocall detectors using ResNeXt50 <ref type="bibr" coords="4,350.01,556.70,16.22,10.91" target="#b13">[13]</ref>, which is a kind of CNN-based model. In our pipeline, the nocall detector was used in two ways:</p><p>‚Ä¢ To weight the labels of segments that were predicted to have no birds singing during the second stage of training. ‚Ä¢ To create labels for training during the third stage of training.</p><p>As mentioned in 2.1, the data given by BirdCLEF2021 was not sufficient, so we used freefield1010 <ref type="bibr" coords="4,89.29,656.03,18.07,10.91" target="#b11">[11]</ref> to create the nocall detector. The sound source was given every 10 seconds, but only the sound source for 7 seconds from the start of the audio was used. This was because the train_short_audio used for learning in the second and third stages were separated every 7 seconds. Since the data set was converted to images beforehand, it became difficult to use the data augmentation used in normal sound recognition, such as adding noise. However, we used the usual augmentations for images, such as flip, normalization, decreasing JPEG compression of an image, to improve the generalization performance.</p><p>The model was trained using the ADAM optimizer and a Cosine Annealing Warm Restarts scheduler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Second Stage</head><p>We detected and classified the birds that appeared in the segment. Multi-label classification was performed on the transformed mel spectrogram using CNN-based ResNeSt50 <ref type="bibr" coords="5,435.22,231.54,16.25,10.91" target="#b14">[14]</ref>.</p><p>Data. For the training data, we used train_short_audio separated by 7 seconds. In the case of long audio, only the first 10 segments were used. For some models, we also used freefield1010 and the validation data from BirdCLEF2020. For these data, only the nocall data was used, and all 397 dimensions of the target were set to 0. For the validation data from BirdCLEF2020, since it was separated every 5 seconds, we padded the mel spectrogram with zeros before and after (in random ratios) during training so that it could be converted to a 128x281 image. Data augmentation was difficult to apply to raw sound because the dataset had been converted to images beforehand. Instead, the mixup <ref type="bibr" coords="5,259.27,353.48,17.76,10.91" target="#b15">[15]</ref> was used as data augmentation. This would improve the generalization performance and also allowed us to better recognize that multiple birds might be singing at the same time.</p><p>Labels. We used the nocall detector created in the first stage to modify the labels for the data used for training in this stage. Specifically, each segment was multiplied by the output value of the first stage (the closer to 0, the fewer birds were singing, and the closer to 1, the more likely multiple birds were singing), and the weight of the segment with no birds singing was set to be smaller. In addition, for datasets with secondary labels, we used a value of 0.6 times the predicted value of the nocall detector as the target. This was because birds with secondary labels were considered to occur less frequently than birds with primary labels, and therefore might not have appeared throughout the audio.</p><p>The models were trained using the ADAM optimizer and a Cosine Annealing LR scheduler. The batch size was 100, and label smoothing <ref type="bibr" coords="5,291.04,543.17,17.99,10.91" target="#b16">[16]</ref> was performed to adjust the target value to a maximum of 0.995 and a minimum of 0.0025. To avoid overfitting on noises specific to the equipment used to record the sound, some models were trained and cross-validated using the data grouped by the authors so that it did not appear commonly in the training and validation data. In the next stage, we used the model from the epoch with the highest row-wise F1 score on train_soundscapes. Details of the evaluation method are found in Section 3.1.</p><p>As the data set had weak labels, it was not enough to fix the labels using the first stage models. In the second stage, we still had the following issues:</p><p>‚Ä¢ It was unknown which bird was singing in that segment (primary or secondary labels).</p><p>‚Ä¢ It was only known with probability whether a bird was singing in that segment.</p><p>Since we needed a mechanism to determine which bird was singing with high accuracy while reducing noise to some extent, we used the third stage to sort the prediction results of the second stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Third Stage</head><p>In this stage, based on the results of the second stage, we extracted the top N (we used 5) bird species that were likely to be singing in each segment, and combined information other than singing (date, latitude, longitude, and whether the bird was singing in the previous or following segment). LightGBM was trained to perform binary classification. The closer the output was to 0, the less likely it was that the target bird was singing in the target section, and the closer it was to 1, the more likely it was. Several new features were generated by transforming the metadata.</p><p>To solve the problem of noisy labels in the second stage, we created a problem in which the correct answer was the intersection of the following "Correct answer set" and the "Candidate set. " In this way, we converted the task to one in which the correct labels could be automatically assigned with high accuracy while taking advantage of the characteristics of the second stage model using image features.</p><p>‚Ä¢ Correct answer set: Primary labels and secondary labels for segments judged to be singing, nocall for segments judged not to be singing. ‚Ä¢ Candidate set: Top N values of the probability of each bird singing as predicted by the second stage model from the mel spectrogram (ùëÅ = 5 was adopted).</p><p>Taking advantage of the fact that both candidates and correct answers rarely had anything in common, this method allowed us to automatically annotate with high accuracy.</p><p>By converting the data into tabular data for each segment divided into several-second intervals, we could make predictions that took into account various information such as time-series, seasonality, and regionality in the segment. Details of the features are found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Post-processing</head><p>After the completion of binary classification in the third stage, we determined the optimal threshold value by ternary search, and performed nocall injection to improve the score while leaving ambiguity by mixing nocall and bird labels in the final prediction. Threshold Optimization. A threshold was set for the predicted probability of binary classification in the third stage. Candidates with probability values higher than the threshold were included in the final output, and if none of the candidates were included in the segment, we called it nocall . Assuming that the score was a convex function of the threshold, the threshold was set to the highest score by ternary search, and since the optimal threshold range was multiplied by 2/3 each time the score was calculated, it was possible to determine the optimal threshold after several dozen score calculations. The optimal threshold was calculated using train_soundscapes, and the same value was used when inferring test_soundscapes.</p><p>Nocall Injection. After determining the threshold, we decided whether to add nocalls to the segments that did not become nocalls. Specifically, for the birds that were decided to be included in the final output by setting the threshold, nocall was added to the rows with low total predicted probability. This had the following two advantages:</p><p>‚Ä¢ It was able to unambiguously predict what was not highly predictable in the third stage.</p><p>‚Ä¢ Due to the nature of the row-wise micro averaged F1 score, mixing nocall and bird labels might increase the score.</p><p>As an implementation, we thought it would be convex here as well and used ternary search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Evaluation Metrics</head><p>The BirdCLEF2021 contest dealed with the task of identifying all singing birds in segments of 5 seconds each. Submissions were evaluated based on their row-wise micro averaged F1 score. The F1 score is calculated as follows:</p><formula xml:id="formula_0" coords="7,185.47,351.77,223.14,26.72">F1 = 2 recall -1 + precision -1 = tp tp + 1/2(fp + fn)</formula><p>Until the end of the contest, only the score calculated using 35% of the data (public score) was shown, while the remaining 65% of the data (private score) was hidden. The final ranking was determined using the private score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>First Stage.</head><p>The training of one model took about 90 seconds per epoch, using a Tesla P100 for the GPU. Models were trained for 10 epochs, and the one with the best score was selected. Cross-validation was performed by dividing the data into five parts with equally distributed labels, and the average percentage of correct answers was 0.89857. authors, we split the data into 5 parts and assigned 0-4 ids. When the data was not split by author, the data was also split into 5 parts and given the id of 0'-4'. For each model, the threshold was determined by ternary search, and the epoch with the best F1 score during validation in train_soundscapes was used in the final submission. More details about the condition settings when training the model and the number of epochs and scores are shown in Table <ref type="table" coords="8,177.62,323.36,3.78,10.91" target="#tab_1">2</ref>. The F1 score, calculated as the simple average of the predictions of the models from M1 to M10, was 0.788.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Second</head><p>We trained the models on Tesla P100 or V100 GPUs; for ResNeSt50, it took 6-7 minutes per epoch on P100. Although we did not include ResNeSt26 in the final submission for the contest, ResNeSt26 performed better than ResNeSt50 in the second stage and required less training time (4-5 minutes per epoch) on P100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Third Stage.</head><p>For training, we used the inference results of M4 in the second stage and metadata for the validation data of train_short_audio. When extracting candidates, we selected the top 5 candidates with the highest prediction probability from the inference results for 397 bird species in the second stage. For inference, we used the simple average of the 10 models in Table <ref type="table" coords="8,489.01,458.85,5.16,10.91" target="#tab_1">2</ref> to improve accuracy.</p><p>It took about 15 minutes to complete the submission, including learning the tabular data. When the threshold was optimized, the CV score was 0.833286 and after the nocall injection it was 0.837063. The row-wise micro averaged F1 score for the final submission achieved 0.7620 for the public test set and 0.6932 for the private test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Discussion</head><p>The uniqueness of our work was that we used the intersection for the labels for entire audio files and the segment-level candidates, as the new segment-level labels to train the tabular data in the third stage. This had the following advantages:</p><p>‚Ä¢ Dealing with weak and noisy supervision: It could be used to learn accurately from data that was only labeled in audio units or contained errors.</p><p>‚Ä¢ Taking into account time-series information: Image classification task could be converted to a time-series tabular data classification task, improving accuracy. ‚Ä¢ Taking into account metadata: The accuracy was improved by being able to incorporate a variety of information such as before/after information, regional characteristics, and seasonality. ‚Ä¢ Reducing computational cost: Our solution was fast, which saved time and computational resources in learning and inference.</p><p>Although our method performed well in the contest, there was still room for improvement. For example, it could learn effectively even with noisy and weak labels, but the accuracy could have been further improved by using more data that was accurately annotated.</p><p>For the first stage, in addition to the addition of other data, further performance improvement could have been achieved by changing the CNN model and ensembles.</p><p>For the second stage, due to time constraints, we were not able to incorporate another highly accurate model that we tried, such as ResNeSt26. In addition, only ResNeSt50 was used, and other models were not used in the ensemble.</p><p>For the third stage, the second stage models used for training and inference were different. That is, there was only one model used to infer the probability values for train_short_audio and 10 models used to infer the probability values for train_soundscapes. The fact that these models did not match might cause differences between the training data and inferred data in the third stage, so unifying them could have further improved the accuracy. When extracting candidates, the top five candidates with the highest prediction probability for each segment were extracted, which did not take into account the fact that the ease of appearance of birds differed for each segment. It might have been possible to extract a large number of candidates for segments where many birds were likely to appear, and a smaller number for segments where not so many birds were likely to appear. There was also room for improvement in feature engineering. For example, since there were interactions (co-occurrences) between birds, it might be possible to add new features that included relevant information such as birds that tended to sing together or birds that coexisted.</p><p>Since our method could learn to include information from the previous and following segments, as well as meta-information such as region and time, further improvement could be expected depending on how the features were created.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>We have presented a method for audio recognition consisting of a multi-stage pipeline. The first stage is a binary classification of whether a bird is singing or not for label correction. The second stage is to predict which bird is singing from the sound source. In the third stage, we select the candidates extracted in the second stage.</p><p>By building a multi-stage pipeline and devising a semi-automatic labeling scheme, it is possible to learn effectively even from weak and noisy teacher data. Our method is superior in terms of computational cost, yet can learn accurately from data in different domains that are labeled only per audio. Our semi-automatic labeling method in the third stage makes it possible to label segments of audio by simply annotating the entire audio. Therefore, assuming that our method is used, the cost of annotation can be reduced. It is a great deal of effort for a person to annotate each segment in detail, and the annotation itself is prone to have errors. Therefore, it is a great advantage to reduce the effort required for annotation. In addition, our method may be able to detect preliminary sounds of the bird itself (e.g., the sound of rustling in the bushes), even if the bird itself is not singing. This is difficult for humans to determine, so if we can identify them as candidates, they may be useful for future research.</p><p>Our method is also versatile enough to be applied to other tasks on time-series and series data labeled to the whole. For example:</p><p>‚Ä¢ Identifying a lesion slice from multiple slices in MRI.</p><p>‚Ä¢ Identifying which scenes were good from video ratings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,203.57,416.69,8.93;3,89.29,215.58,416.70,8.87;3,89.29,227.53,416.70,8.87;3,88.93,239.49,313.38,8.87;3,89.29,84.19,416.68,100.00"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Inference Pipeline of the proposed method. The model of the first stage was used for trainingof the second and third stages, so it was not used for inference. In the second stage, CNN was used to predict the probability of occurrence for 397 bird species. In the third stage, GBDT was used to predict whether the candidate extracted in the second stage was the correct answer.</figDesc><graphic coords="3,89.29,84.19,416.68,100.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,126.75,533.54,329.21,10.91;7,107.28,555.37,47.36,10.91;7,107.28,570.08,317.82,10.91;7,107.28,584.80,292.87,10.91;7,107.28,599.51,236.03,10.91;7,107.28,614.22,400.55,10.91;7,116.56,627.77,143.26,10.91;7,107.28,642.48,398.70,10.91"><head></head><label></label><figDesc>Stage. Ten ResNeSt50s were trained, varying on the following conditions: ‚Ä¢ mixup ùõº ‚Ä¢ Whether to cross-validate the nocall detector (No: Hold-out, Yes: CV) ‚Ä¢ Whether to use BirdCLEF2020 validation data in training or not ‚Ä¢ Whether or not to use freefield1010 during training ‚Ä¢ Whether to group authors so that they do not appear in different folds. (No: StratifiedK-Fold, Yes: StratifiedGroupKFold) ‚Ä¢ The fold id of train_short_audio used for verification. If the data were split by grouped</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.99,90.49,418.66,105.74"><head>Table 1</head><label>1</label><figDesc>Data sets used in each stage. BirdCLEF2021 provided train_short_audio and train_soundscapes as training and validation data. We used freefield1010 and validation data of BirdCLEF2020 as external data.</figDesc><table coords="4,121.67,134.06,351.94,62.16"><row><cell>Data Set</cell><cell>First Stage</cell><cell cols="2">Second Stage Third Stage</cell></row><row><cell>train_short_audio</cell><cell>no</cell><cell>train</cell><cell>train</cell></row><row><cell>train_soundscapes</cell><cell>no</cell><cell>validation</cell><cell>validation</cell></row><row><cell>freefield1010</cell><cell>train and validation</cell><cell>train</cell><cell>no</cell></row><row><cell>validation data of BirdCLEF2020</cell><cell>no</cell><cell>train</cell><cell>no</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,88.99,90.49,418.09,165.51"><head>Table 2</head><label>2</label><figDesc>Second Stage Models. Scores and main properties of models used for submitted runs. ResNeSt50 was used as the backbone for all models. The F1 score was calculated for train_soundscapes. The F1 score, calculated as the simple average of the predictions of the models from M1 to M10, was 0.788.</figDesc><table coords="8,96.02,146.01,403.24,109.99"><row><cell>Model ID</cell><cell>M1</cell><cell>M2</cell><cell>M3</cell><cell>M4</cell><cell>M5</cell><cell>M6</cell><cell>M7</cell><cell>M8</cell><cell>M9</cell><cell>M10</cell></row><row><cell>mixup ùõº</cell><cell>0.5</cell><cell>5.0</cell><cell>5.0</cell><cell>5.0</cell><cell>5.0</cell><cell>5.0</cell><cell cols="4">5.0 Yes</cell></row><row><cell>nocall detector cv</cell><cell>No</cell><cell>No</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>grouped by author</cell><cell>No</cell><cell>No</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>validation fold id</cell><cell>0'</cell><cell>0'</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>0</cell><cell>0</cell><cell>1</cell></row><row><cell>epochs</cell><cell>27</cell><cell>13</cell><cell>33</cell><cell>34</cell><cell>34</cell><cell>20</cell><cell>34</cell><cell>78</cell><cell>84</cell><cell>27</cell></row><row><cell>F1</cell><cell cols="10">0.741 0.750 0.755 0.737 0.750 0.755 0.737 0.751 0.757 0.754</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to express our sincere gratitude to many people for their support in conducting this research. First of all, we would like to thank The <rs type="institution">Cornell Lab of Ornithology</rs> for organizing the BirdCLEF2021 Kaggle competition. Without them, we would not have been able to start this research. Secondly, we would also like to thank the other competitors who provided us with many suggestions during the competition. Finally, we would like to express our gratitude to <rs type="person">Ms. Sue Goldberg</rs> who works for the <rs type="institution">University of Cincinnati Foundation</rs>, <rs type="person">Professor Hiroshi Imai</rs> at <rs type="affiliation">Graduate School of Information Science and Technology, University of Tokyo</rs>, and <rs type="person">Dr. Tetsuya Tanimoto</rs> at <rs type="affiliation">Medical Governance Research Institute in Tokyo, Japan</rs>, for proofreading this paper.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Features used in third stage</head><p>‚Ä¢ "year": The year when the audio was recorded.</p><p>‚Ä¢ "month": The month in which the audio was recorded.</p><p>‚Ä¢ "sum_prob": The sum of the probability values of the 397 dimensions for the segment.</p><p>‚Ä¢ "mean_prob": Mean of the 397 dimensional probability values for the segment.</p><p>‚Ä¢ "max_prob": Maximum 397-dimensional probability values for the segment.</p><p>‚Ä¢ "prev_prob" -"prev6_prob": Probability values for the candidate birds in the previous N segments. ‚Ä¢ "prob": Probability value for the candidate bird in the target segment.</p><p>‚Ä¢ "next_prob" -"next6_prob": Probability values of the candidate bird in the next N segments.</p><p>‚Ä¢ "rank": The probability that the bird of interest has the highest probability value out of 397 in the segment. ‚Ä¢ "latitude": The latitude of the location where the audio was recorded.</p><p>‚Ä¢ "longitude": The longitude of the location where the audio was recorded.</p><p>‚Ä¢ "bird_id": Which of the 397 birds is the target.</p><p>‚Ä¢ "seconds": The number of seconds in the segment.</p><p>‚Ä¢ "num_appear": The number of files where the bird is the primary label in train_short_audio ‚Ä¢ "site_num_appear": How many times the bird in train_short_audio appeared in the region ‚Ä¢ "site_appear_ratio": "site_num_appear" divided by "num_appear" ‚Ä¢ "prob_diff": Difference between prob and average of prob values for 3 segments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Online Resources</head><p>The source code is available via GitHub: https://github.com/namakemono/kaggle-birdclef-2021.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,112.66,439.25,393.32,10.91;10,112.66,452.79,395.01,10.91;10,112.26,466.34,382.53,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,287.89,439.25,218.09,10.91;10,112.66,452.79,139.31,10.91">Surveying tropical birds is much harder than you think: a primer of best practices</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">D</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Lees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Blake</surname></persName>
		</author>
		<idno type="DOI">10.1111/btp.12608</idno>
		<ptr target="https://doi.org/10.1111/btp.12608" />
	</analytic>
	<monogr>
		<title level="j" coord="10,260.03,452.79,45.69,10.91">Biotropica</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="846" to="849" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,479.89,394.53,10.91;10,112.66,493.44,393.33,10.91;10,112.66,506.99,328.66,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,112.66,493.44,98.33,10.91;10,238.81,493.44,206.97,10.91">Bird call identification in soundscape recordings</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqu√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,467.81,493.44,38.18,10.91;10,112.66,506.99,297.96,10.91">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note>Overview of BirdCLEF</note>
</biblStruct>

<biblStruct coords="10,112.66,520.54,393.33,10.91;10,112.66,534.09,394.52,10.91;10,112.66,547.64,393.32,10.91;10,112.66,561.19,393.33,10.91;10,112.66,574.74,360.25,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,247.88,547.64,258.10,10.91;10,112.66,561.19,305.78,10.91">Overview of LifeCLEF 2021: a system-oriented evaluation of automated species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Casta√±eda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqu√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dorso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,441.19,561.19,64.80,10.91;10,112.66,574.74,330.51,10.91">Proceedings of the Twelfth International Conference of the CLEF Association (CLEF 2021)</title>
		<meeting>the Twelfth International Conference of the CLEF Association (CLEF 2021)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,588.29,179.76,10.91;10,314.76,588.29,193.07,10.91;10,112.66,601.84,134.08,10.91;10,263.87,601.84,243.32,10.91;10,112.66,615.39,394.53,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,314.76,588.29,193.07,10.91;10,112.66,601.84,129.55,10.91">Soundnet: Learning sound representations from unlabeled video</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="10,199.37,615.39,246.13,10.91">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,628.93,394.04,10.91;10,112.41,642.48,209.11,10.91" xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Curran Associates</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Inc</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2016/file/7dcd340d84f762eba80aa538b0c527f7-Paper.pdf" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,656.03,394.52,10.91;10,112.66,669.58,393.33,10.91;11,112.66,86.97,395.00,10.91;11,112.66,100.52,186.11,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,168.39,669.58,224.26,10.91">librosa: Audio and Music Signal Analysis in Python</title>
		<author>
			<persName coords=""><forename type="first">Brian</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matt</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Mcvicar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oriol</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Nieto</surname></persName>
		</author>
		<idno type="DOI">10.25080/Majora-7b98e3ed-003</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,183.33,86.97,240.32,10.91">Proceedings of the 14th Python in Science Conference</title>
		<editor>
			<persName><forename type="first">Kathryn</forename><surname>Huff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</editor>
		<meeting>the 14th Python in Science Conference</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,114.06,393.33,10.91;11,112.28,127.61,395.38,10.91;11,112.66,141.16,149.51,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,189.18,114.06,282.35,10.91">Greedy function approximation: A gradient boosting machine</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<idno type="DOI">10.1214/aos/1013203451</idno>
		<ptr target="https://doi.org/10.1214/aos/1013203451.doi:10.1214/aos/1013203451" />
	</analytic>
	<monogr>
		<title level="j" coord="11,488.38,114.06,17.60,10.91;11,112.28,127.61,87.32,10.91">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,154.71,393.33,10.91;11,112.66,168.26,394.52,10.91;11,112.66,181.81,395.01,10.91;11,112.66,195.36,362.24,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,212.44,154.71,184.59,10.91">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939785</idno>
		<ptr target="https://doi.org/10.1145/2939672.2939785.doi:10.1145/2939672.2939785" />
	</analytic>
	<monogr>
		<title level="m" coord="11,421.52,154.71,84.47,10.91;11,112.66,168.26,394.52,10.91;11,112.66,181.81,37.28,10.91">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,208.91,393.61,10.91;11,112.66,222.46,394.53,10.91;11,112.66,236.01,393.32,10.91;11,112.66,249.56,394.03,10.91;11,112.66,263.11,278.57,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,418.47,208.91,87.80,10.91;11,112.66,222.46,170.61,10.91">Lightgbm: A highly efficient gradient boosting decision tree</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="s" coord="11,314.47,236.01,191.52,10.91;11,112.66,249.56,34.49,10.91">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,276.66,393.33,10.91;11,112.66,290.20,394.53,10.91;11,112.66,303.75,394.53,10.91;11,112.39,317.30,394.31,10.91;11,112.66,330.85,229.18,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,419.05,276.66,86.94,10.91;11,112.66,290.20,149.73,10.91">Catboost: unbiased boosting with categorical features</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Prokhorenkova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gusev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vorobev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">V</forename><surname>Dorogush</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gulin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2018/file/14491b756b3a51daac41c24863285549-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="s" coord="11,268.99,303.75,233.31,10.91">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2018">2018</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,344.40,393.33,10.91;11,112.66,357.95,393.61,10.91;11,112.66,371.50,394.04,10.91;11,112.66,385.05,385.18,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,368.79,344.40,137.20,10.91;11,112.66,357.95,298.02,10.91">Automatic acoustic detection of birds through deep learning: The first bird audio detection challenge</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Stowell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Pamu≈Ça</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Stylianou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<idno type="DOI">10.1111/2041-210X.13103</idno>
		<ptr target="https://doi.org/10.1111/2041-210X.13103" />
	</analytic>
	<monogr>
		<title level="j" coord="11,418.99,357.95,87.28,10.91;11,112.66,371.50,63.95,10.91">Methods in Ecology and Evolution</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="368" to="380" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,398.60,394.61,10.91;11,112.66,412.15,278.39,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,225.73,398.60,281.54,10.91;11,112.66,412.15,55.03,10.91">An open dataset for research on audio field recording archives: freefield1010</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Stowell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,175.84,412.15,183.28,10.91">Journal of the Audio Engineering Society</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,425.70,394.52,10.91;11,112.66,439.25,394.52,10.91;11,112.66,452.79,393.53,10.91;11,112.66,466.34,389.43,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,112.66,439.25,99.67,10.91;11,240.82,439.25,261.41,10.91">Bird Sound Recognition in Complex Acoustic Environments</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Clapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">A</forename><surname>Hopping</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqu√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<ptr target="https://hal.inria.fr/hal-02989101" />
	</analytic>
	<monogr>
		<title level="m" coord="11,126.81,452.79,379.38,10.91;11,112.66,466.34,90.47,10.91">CLEF 2020 -11th International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note>Overview of BirdCLEF</note>
</biblStruct>

<biblStruct coords="11,112.66,479.89,393.32,10.91;11,112.66,493.44,393.33,10.91;11,112.33,506.99,275.01,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,301.73,479.89,204.25,10.91;11,112.66,493.44,69.89,10.91">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.634</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,205.86,493.44,300.13,10.91;11,112.33,506.99,30.22,10.91">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,520.54,394.53,10.91;11,112.66,534.09,356.38,10.91" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m" coord="11,140.30,534.09,146.12,10.91">Resnest: Split-attention networks</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,547.64,393.60,10.91;11,112.66,561.19,394.62,10.91;11,112.66,574.74,201.92,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,363.95,547.64,142.32,10.91;11,112.66,561.19,58.23,10.91">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1Ddp1-Rb" />
	</analytic>
	<monogr>
		<title level="m" coord="11,200.33,561.19,249.04,10.91">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,588.29,394.52,10.91;11,112.66,601.84,393.33,10.91;11,112.66,615.39,394.62,10.91;11,112.31,628.93,390.54,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,280.90,588.29,150.82,10.91">When does label smoothing help?</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/file/f1748d6b0fd9d439f71450117eba2725-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="s" coord="11,420.63,601.84,85.36,10.91;11,112.66,615.39,144.34,10.91">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alch√©-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
