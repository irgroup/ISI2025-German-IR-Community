<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,377.67,15.42;1,89.29,106.66,377.17,15.42;1,89.29,128.58,95.08,15.43;1,89.29,150.91,253.17,11.96">EfficientNets and Vision Transformers for Snake Species Identification Using Image and Location Information FHDO Biomedical Computer Science Group (BCSG)</title>
				<funder>
					<orgName type="full">University of Applied Sciences and Arts Dortmund, Dortmund, Germany</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,176.82,62.76,11.96"><forename type="first">Louise</forename><surname>Bloch</surname></persName>
							<email>louise.bloch@fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science (FHDO)</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<addrLine>Emil-Figge-StraÃŸe 42</addrLine>
									<postCode>44227</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Medical Informatics, Biometry and Epidemiology (IMIBE)</orgName>
								<orgName type="institution">University Hospital Essen</orgName>
								<address>
									<settlement>Essen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,169.98,176.82,111.78,11.96"><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
							<email>christoph.friedrich@fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science (FHDO)</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<addrLine>Emil-Figge-StraÃŸe 42</addrLine>
									<postCode>44227</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Medical Informatics, Biometry and Epidemiology (IMIBE)</orgName>
								<orgName type="institution">University Hospital Essen</orgName>
								<address>
									<settlement>Essen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,377.67,15.42;1,89.29,106.66,377.17,15.42;1,89.29,128.58,95.08,15.43;1,89.29,150.91,253.17,11.96">EfficientNets and Vision Transformers for Snake Species Identification Using Image and Location Information FHDO Biomedical Computer Science Group (BCSG)</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">1F101F765408E81834EAA900E653B8A5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Snake species identification</term>
					<term>EfficientNets</term>
					<term>Vision Transformer</term>
					<term>Image classification</term>
					<term>Metadata inclusion snakeantivenoms/database/</term>
					<term>[Last accessed:</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The automatic classification of snake species based on non-standardized photographs is important to improve the care of patients suffering from snake bites. The SnakeCLEF 2021 challenge, provides a large database containing images and their recording location of 772 snake species to overcome this problem. This paper describes the participation of the FHDO Biomedical Computer Science Group (BCSG) in this challenge. In the experiments, deep learning-based EfficientNets and Vision Transformer (ViT) models were trained. In a subsequent step, the prior probabilities of the location information were multiplied with the model predictions. An ensemble of both deep learning models achieved the best results, which was a macro averaging ğ¹1-score across countries of 82.88 % for the independent test set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper presents the participation of University of Applied Sciences and Arts Dortmund (FHDO) Biomedical Computer Science Group (BCSG) at the Conference of Labs of the Evaluation Forum (CLEF) 2021 1 SnakeCLEF challenge 2 for snake species identification <ref type="bibr" coords="1,423.27,472.43,11.38,10.91" target="#b0">[1]</ref>. This challenge is part of the LifeCLEF 2021 <ref type="bibr" coords="1,214.72,485.98,12.78,10.91" target="#b1">[2]</ref> research platform that focuses on the automated identification of species <ref type="bibr" coords="1,135.33,499.53,11.43,10.91" target="#b2">[3]</ref>. The LifeCLEF platform consists of four data-driven challenges.</p><p>The mortality of snakebites is between 81,000 and 138,000 people per year <ref type="bibr" coords="1,443.22,513.08,11.58,10.91" target="#b3">[4]</ref>. Annually, 400,000 victims of snakebites suffering from incurable physical and psychological disabilities <ref type="bibr" coords="1,492.63,526.63,11.27,10.91" target="#b3">[4]</ref>. It was expected that the mortality of snakebites can be reduced by identifying the snake species and thus recently administer the right antivenom <ref type="bibr" coords="1,308.37,553.73,11.31,10.91" target="#b4">[5]</ref>. Additionally, snake species identification Further studies performed deep learning-based classification tasks. For example, one approach <ref type="bibr" coords="3,121.15,100.52,17.76,10.91" target="#b12">[13]</ref> compared the accuracies of three different classification networks namely VGG16 <ref type="bibr" coords="3,490.03,100.52,16.09,10.91" target="#b20">[19]</ref>, DenseNet161 <ref type="bibr" coords="3,152.13,114.06,18.06,10.91" target="#b21">[20]</ref> and MobileNetV2 <ref type="bibr" coords="3,255.76,114.06,16.41,10.91" target="#b22">[21]</ref>. The dataset contained 3,050 images containing 28 species. The GrabCut <ref type="bibr" coords="3,187.11,127.61,17.88,10.91" target="#b24">[22]</ref> algorithm was applied as a preprocessing step to extract the snakes from the image background. After 50 training epochs, an accuracy of 72 % was reached for the test dataset and the DenseNet161 architecture.</p><p>Another approach <ref type="bibr" coords="3,186.14,168.26,18.06,10.91" target="#b13">[14]</ref> trained a deep Siamese network <ref type="bibr" coords="3,355.39,168.26,18.07,10.91" target="#b25">[23]</ref> for one-shot learning <ref type="bibr" coords="3,475.88,168.26,18.07,10.91" target="#b26">[24]</ref> to classify between 84 snake species based on the World Health Organization (WHO) venomous snake database 5 . The dataset contained 200 images and three to 16 images per class.</p><p>Although there are more than 3,700 snake species worldwide <ref type="bibr" coords="3,366.93,208.91,16.09,10.91" target="#b27">[25]</ref>, and more than 600 of them were medically relevant <ref type="bibr" coords="3,197.32,222.46,16.18,10.91" target="#b27">[25]</ref>, most recently ML approaches were trained on a small number of snake species.</p><p>The SnakeCLEF challenge <ref type="bibr" coords="3,215.53,249.56,11.23,10.91" target="#b0">[1,</ref><ref type="bibr" coords="3,229.11,249.56,13.95,10.91" target="#b27">25]</ref> provided a large dataset containing images of more than 700 snake species to overcome this problem. Different deep learning approaches were successfully submitted in previous rounds of this challenge. The winning approach <ref type="bibr" coords="3,401.20,276.66,17.79,10.91" target="#b28">[26]</ref> of SnakeCLEF 2020 <ref type="bibr" coords="3,89.29,290.20,17.97,10.91" target="#b29">[27]</ref> used a ResNet architecture pre-trained on ImageNet-21k and reached a macro-averaging F1-score of 62.54 %. The FHDO BCSG <ref type="bibr" coords="3,258.90,303.75,12.83,10.91" target="#b6">[7]</ref> combined OD and image classification using a Mask Region-Based Convolutional Neural Network (Mask R-CNN) <ref type="bibr" coords="3,356.13,317.30,17.76,10.91" target="#b30">[28]</ref> instance detection framework and an EfficientNet-B4 <ref type="bibr" coords="3,197.86,330.85,18.07,10.91" target="#b31">[29]</ref> classification model. This method reached a macro-averaging F1-score of 40.35 %. In post-competition submissions, the score could be optimized to 59.4 %.</p><p>This research expands the ML workflow developed from FHDO BCSG <ref type="bibr" coords="3,406.60,357.95,12.69,10.91" target="#b6">[7]</ref> in SnakeCLEF 2020. In particular, the workflow was extended to use Vision Transformer (ViT) <ref type="bibr" coords="3,420.68,371.50,17.93,10.91" target="#b33">[30]</ref> models and an ensemble of ViTs and EfficientNets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset</head><p>The training dataset of the SnakeCLEF 2021 and AICrowd Snake Species Identification Challenge round 5 consists of 386,006 photographs of 772 snake species. Those photographs were taken in 188 countries. The photographs originated from three different data sources, two online biodiversity platforms, namely iNaturalist 6 (n=277,025 images; 71.77 %), and Herpmapper 7 (n=58,351 images; 15.12 %) and another source containing noisy data downloaded from Flickr 8 (n=50,630 images; 13.12 %). The entire dataset was split into a training set containing 347,405 photographs (90.00 %), and a validation set containing 38,601 subjects (10.00 %). The class distribution of the united training and validation set is visualized in Figure <ref type="figure" coords="3,416.20,538.52,3.66,10.91" target="#fig_0">1</ref>. It can be seen that the dataset was highly imbalanced. Both subsets followed similar class distributions and each class was represented in both datasets. The test dataset consists of 23,673 images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Metadata</head><p>In addition to the photographs, metadata that provides information about the continent and country of the image location were available. Most images (n=246,482; 63.85 %) were recorded in the United States of America. For 50,879 (13.18 %) images, no country information was provided and 51,061 images (13.23 %), had no continent information. Those images were marked with the "unknown" flag.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methods</head><p>The ML workflow used to learn the differences between snake species is visualized in Figure <ref type="figure" coords="4,500.24,472.69,3.71,10.91" target="#fig_1">2</ref>. This ML workflow was implemented in a modular way, thus, during the challenge different combinations of workflow parts and their interactions were investigated. The entire workflow was implemented using the programming language Python v3.6.9 <ref type="bibr" coords="4,384.20,513.34,16.25,10.91" target="#b34">[31]</ref>.</p><p>The preprocessing stage starts with optional filtering of the dataset. Afterwards, optional OD was implemented, which was trained to detect single snakes in the photographs. Due to time constraints, no models using the OD could be submitted during the challenge. The OD stage was followed by an image preprocessing stage which produced images of uniform, quadratic size. Afterwards, data augmentation was used to make the subsequent deep learning models more robust, for example, against rotation, scaling, and noise. EfficientNets and ViTs were trained to distinguish between the snake species. Finally, optional multiplication of the models' prediction probabilities and the a priori probability distribution of the snake species given the location was implemented. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset Filtering</head><p>An analysis of the dataset with AntiDupl<ref type="foot" coords="5,272.32,387.56,3.71,7.97" target="#foot_2">9</ref> revealed 29 "Image not found" images, which were the result of download problems.</p><p>Another problem that has been found are out-of-class images appearing in the Flickr dataset. These images contain no snakes but for example, ice-hockey players, churches, other animals, persons, and mangas. To identify them for exclusion from the training set, a standard ImageNet classifier with 1,000 classes and based on a ResNet50 <ref type="bibr" coords="5,320.16,457.06,17.75,10.91" target="#b18">[18]</ref> architecture has been used. Therefore, a positive list of 46 snake and reptile classes (e.g., garter_snake, sidewinder, . . . ) that are part of the ImageNet Large Scale Visual Recognition Challenge 2012 (ILSRVC2012) <ref type="bibr" coords="5,452.32,484.16,18.06,10.91" target="#b35">[32]</ref> dataset has been used. With this classifier, 6,110 out-of-class images (10.47 %) have been identified as out-of-class images in the Flickr dataset. The excluded images were assigned to 384 species. The filtered dataset contained images of all 772 snake species. The least frequent species after the filtering were "bolyeria multocarinata" and "echinanthera melanostigma", both containing one image. The out-of-class images were manually checked and a large proportion of the identified images visualize no snakes. Due to reasons of time limitations during the competition, the out-of-class images were not formally validated. The effects of the reduced dataset have been tested and compared to the unfiltered dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Object Detection</head><p>The aim of using OD as a preprocessing step to improve image classification was, to focus the subsequently implemented classification models on the object that should be classified. The idea of using OD for the identification of snake species in photographs before executing the image classification was inspired by the winning team <ref type="bibr" coords="6,336.15,148.18,18.00,10.91" target="#b36">[33]</ref> of round 2 of the AICrowd Snake Species Identification Challenge <ref type="bibr" coords="6,238.67,161.73,16.42,10.91" target="#b27">[25]</ref>. The OD used in this paper was already implemented and described in the contribution of the FHDO BCSG in the SnakeCLEF 2020 challenge <ref type="bibr" coords="6,480.30,175.28,11.46,10.91" target="#b6">[7]</ref>. A Mask R-CNN <ref type="bibr" coords="6,150.61,188.83,17.84,10.91" target="#b30">[28]</ref> model was trained to identify snakes in non-standardized photographs. The Mask R-CNN implements instance segmentation, which is a combination of OD and semantic segmentation. Thus, in each image, bounding boxes are identified and classified for each object of interest and each pixel of those bounding boxes was segmented into a range of given classes. Mask R-CNN is an extension of the OD method Faster R-CNN. The Mask R-CNN architecture consists of two phases, in the first phase, identically to Faster R-CNN, a Region Proposal Network (RPN) was implemented to identify candidate bounding boxes followed by a non-maximum suppression <ref type="bibr" coords="6,146.85,283.68,18.06,10.91" target="#b37">[34]</ref> to focus on the most promising candidates. The second stage first used a Region Of Interest (ROI) Align Network on the remaining candidate bounding boxes followed by a parallel implementation of Fully Connected Networks (FCN) to identify the object class and the offset of the bounding boxes as well as a Convolutional Neural Network (CNN) for the semantic segmentation task.</p><p>In this research, the Mask R-CNN was only used as an OD framework to differentiate between snakes and background and did not use the whole instance segmentation pipeline. Due to this reason, it may be an adequate alternative to use Faster R-CNN instead of the Mask R-CNN. The comparison of both models in the TensorFlow OD Application Programming Interface (API) <ref type="bibr" coords="6,488.22,392.07,17.76,10.91" target="#b38">[35]</ref> for the Microsoft Common Objects in Context (COCO) dataset <ref type="bibr" coords="6,362.43,405.62,17.76,10.91" target="#b39">[36]</ref> shows an increased mAP for the Mask R-CNN model. ResNet-50 was used as the backbone network and was pre-trained on the ImageNet-1k dataset. Based on this model, the OD training process was split into two phases. In the first warm-up phase, the newly added layers were trained for 20 epochs. Afterwards, the weights of the entire model were fine-tuned for another 30 epochs.</p><p>An adaption<ref type="foot" coords="6,155.92,471.61,7.41,7.97" target="#foot_3">10</ref> to Tensorflow 2.1.0 of the implementation of the Mask R-CNN model implemented by Abdulla<ref type="foot" coords="6,174.30,485.16,7.41,7.97" target="#foot_4">11</ref> has been used to implement the Mask R-CNN. The OD process included no data augmentation. A threshold of 0.3 was used for the minimum detection confidence. Stochastic Gradient Descent (SDG) was used to optimize the model weights, with a momentum value of 0.9, a weight decay of 10 -4 and a mini-batch size of 8.</p><p>The annotated snake images were available from the winning solution of round 2 <ref type="bibr" coords="6,460.03,541.11,17.79,10.91" target="#b36">[33]</ref> of the AICrowd Snake Species Identification Challenge. This dataset contained annotated bounding boxes for 1,426 snake images which was a subset of round 2 <ref type="bibr" coords="6,353.71,568.21,17.76,10.91" target="#b36">[33]</ref> of the AICrowd Snake Species Identification Challenge.</p><p>The risk of using OD as a preprocessing step for species identification was that important background information was excluded from the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Image Pre-processing</head><p>Deep learning models expect the input of squared images. However, the ROIs which were extracted from the OD procedure do not have predefined dimensions. Thus, the ROIs need to be transformed to the image dimensions of the deep learning model. If all ROI dimensions were larger than the image dimensions of the deep learning model, the ROIs were resized. Images that were smaller than the image dimensions of the deep learning model, were not resized. The aspect ratio of the ROIs was not modified during the image preprocessing. Thus, occurring borders were padded using a specified color. In this approach, the image borders, which result from non-squared OD results were padded with the mean color of the truncated image parts, to find a color that matched the image best <ref type="bibr" coords="7,271.20,215.93,16.25,10.91" target="#b40">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Data Augmentation</head><p>Data augmentation has been used to increase the training image dataset by adding slightly modified copies from existing training images to the training dataset. This method helps to reduce overfitting in deep learning models <ref type="bibr" coords="7,275.87,292.75,16.09,10.91" target="#b41">[38]</ref>. For the subsequently used classification models, different data augmentation procedures were implemented.</p><p>For all EfficientNet models, the data augmentation pipeline includes random cropping of the images from a size of 430 Ã— 430 pixels to 380 Ã— 380 pixels, random rotation in the range of Â±40 âˆ˜ , a width shift, height shift, random shearing and zooming each with a factor of 0.2 as well as a random horizontal flipping. During the data augmentation procedure, missing image positions were padded with the color of the nearest pixel neighbour. Additionally, the Lanczos interpolation <ref type="bibr" coords="7,150.19,387.60,17.91,10.91" target="#b42">[39]</ref> was used.</p><p>For the ViT models, the augmentation pipeline included random cropping from an image size of 250 Ã— 250 to 224 Ã— 224, and a random horizontal as well as vertical flip each with a probability of 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Classification</head><p>Two different model types, as well as an ensemble, were trained to detect snake species. Effi-cientNets were already used in the SnakeCLEF 2020 challenge from the FHDO BCSG <ref type="bibr" coords="7,477.62,491.52,11.58,10.91" target="#b6">[7]</ref>. In comparison to those models, ViT models were used to compare their results and to examine if an ensemble of both models can improve the classification results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1.">EfficientNets</head><p>EfficientNets <ref type="bibr" coords="7,150.90,567.95,18.07,10.91" target="#b31">[29]</ref> are an optimized CNN based model family. The base architecture of Effi-cientNets was developed by a CNN architecture search, which was optimized for accuracy and Floating Point Operations Per Second (FLOPS). The main building blocks of the resulting EfficientNets are Mobile Inverted Bottleneck Convolutional (MBConv) layers. The base model is successively scaled up using a uniform balance between model depth, model width and image resolution. The developed model architecture achieved state of the art performances on the ImageNet classification task while being smaller and faster than many of the compared models <ref type="bibr" coords="7,123.74,662.79,16.25,10.91" target="#b31">[29]</ref>.</p><p>In this work, EfficientNet-B4 models were trained to differentiate between snake species. All images were scaled to an image size of 380 Ã— 380 pixels. The model weights were initialized by a model which was pre-trained using noisy student <ref type="bibr" coords="8,317.30,114.06,16.13,10.91" target="#b43">[40]</ref>. This model was extended by adding a flatten layer, a dense layer with 1,000 neurons using a Swish <ref type="bibr" coords="8,365.41,127.61,18.07,10.91" target="#b44">[41]</ref> activation function and an output dense layer with 772 neurons using the softmax activation function. The entire model contained 276,495,588 parameters.</p><p>A warm-up phase of three epochs was used to train the weights of the newly added layers and the Batch Normalization layers. Afterwards, the weights of all layers were optimized for a larger number of epochs. All EfficientNets were optimized by using an Adam optimizer <ref type="bibr" coords="8,487.91,195.36,18.07,10.91" target="#b45">[42]</ref> with the parameters ğ›½ 1 = 0.9, ğ›½ 2 = 0.999 and ğœ– = 10 -7 . In both phases, a mini-batch size of 11 was used. The warm-up phase used a learning rate of 10 -4 and afterwards, the learning rate was decreased to 10 -6 .</p><p>Figure <ref type="figure" coords="8,132.39,249.56,5.17,10.91" target="#fig_0">1</ref> shows, that the training dataset is imbalanced across snake species. During the SnakeCLEF 2020 challenge, the FHDO BCSG tested different oversampling techniques for snake identification using EfficientNets <ref type="bibr" coords="8,238.55,276.66,11.45,10.91" target="#b6">[7]</ref>. In this research, the class weight oversampling function which performed best in the SnakeCLEF 2020 and is described in Equation 1 was implemented. The oversampling rate increases less for classes with very small frequencies in comparison to a linear oversampling function. ğ¹ (ğ‘) denoted the frequency of class ğ‘.</p><formula xml:id="formula_0" coords="8,231.85,339.75,274.13,32.55">ğ‘¤(ğ‘) = 1 - 1 âˆšï¸ max ğ¹ (ğ‘) ğ¹ (ğ‘) + 0.5<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2.">Vision Transformers</head><p>ViTs <ref type="bibr" coords="8,112.84,409.60,18.07,10.91" target="#b33">[30]</ref> are an alternative to classical CNN deep learning-based image classifiers. Instead of using convolutional operations which focus on local parts of the image, ViTs consider the whole image in parallel. ViT were based on the self-attention principle <ref type="bibr" coords="8,433.57,436.70,16.42,10.91" target="#b46">[43]</ref>, which was previously applied to Natural Language Processing (NLP). The model design of ViTs follows the Transformers which have been introduced in NLP <ref type="bibr" coords="8,338.78,463.80,16.41,10.91" target="#b46">[43]</ref>. In ViTs, the input image is first disassembled into a set number of patches. Additionally, the position of the patch is encoded by a positional encoder. Multiple transformer encoders, which each mainly consists of Multiheaded Self-Attention (MSA) layers and a Multi-Layer Perceptron (MLP) were used to encode the image patches. Another MLP is trained to learn the overall classification from the image encoding. Two different ViT models were trained in this research. First, a ViT Base model architecture with an image size of 224 Ã— 224 pixels, a patch size of 16 Ã— 16 pixels which was pre-trained on the ImageNet21k dataset was used. This model was trained using a mini-batch size of 70, a learning rate of 10 -5 an Adam optimizer (ğ›½ 1 = 0.9, ğ›½ 2 = 0.999, ğœ– = 10 -8 ) and cross-entropy loss. The ViT Base model contained 86,392,324 parameters.</p><p>During the challenge, another ViT model was trained. The architecture of this model was a ViT Large model with an image size of 224 Ã— 224 pixels and a patch size of 16 Ã— 16 pixels. This model was also initialized using the weights achieved for the ImageNet21k dataset. During the training of this model, a mini-batch size of 18 was used and a learning rate of 10 -5 . This model was also trained using an Adam optimizer (ğ›½ 1 = 0.9, ğ›½ 2 = 0.999, ğœ– = 10 -8 ) and cross-entropy loss. The ViT Large model contained 304,092,932 parameters.</p><p>The pre-trained ViT models were loaded from the Python library timm v0.4.8 <ref type="bibr" coords="9,431.94,86.97,17.76,10.91" target="#b47">[44]</ref> and PyTorch v1.8.0+cu101 <ref type="bibr" coords="9,148.87,100.52,17.91,10.91" target="#b48">[45]</ref> was used to train the model.</p><p>No class weight oversampling was used for all ViT models in our experiments. Preliminary experimental tests on the training dataset showed decreased classification results using the class weight function used for the EfficientNets with the described ViT models and given hyperparameters. However, due to reasons of time limitations during the challenge, no complex hyperparameter optimization was applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3.">Ensemble Model</head><p>In addition, a simple ensemble model was added in the experiments. This model ensembles the results of a ViT Large model and an EfficientNet-B4 model, by calculating the mean softmax-scaled probability predictions of both models. It was expected that the ensemble model outperforms the results of the individual models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Adding Location Information</head><p>The last part of the ML workflow was to optionally add location information to probability predictions of the classification models. For this reason, the prior probabilities were estimated by the relative frequency distribution of the snake species at a location in the training and validation dataset. Two different strategies were applied to combine the location and the image information. In the first strategy, the prior probabilities of the image location were multiplied with the prediction probabilities of the image. The second strategy was similar to the first one, except that the prior probabilities were binarized with a cut-off value of 0. Thus, in the binarized strategy, prior probabilities with non-zero values were transformed to one, whereas prior probabilities with a value of zero were kept unchanged. Both strategies primarily use the country in this step, as it contains more information than the continent. The continent information was only added for images with unknown country information. For images with missing country and continent information, the prior probability of the "unknown" class was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>The following sections describe the classification results of the ML workflow and different ablation studies executed to the workflow modules for the validation and the test dataset.   ) and the validation dataset which was filtered for out-of-class images (ğ¹ ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘-ğ‘“</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Preliminary Experiments on the Validation Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>). The EfficientNet-B4 classifier was abbreviated as B4, the ViT Base model was abbreviated as ViT-B and the ViT Large model was abbreviated as ViT-L. The ensemble model was an ensemble of model S1 and model S5. All models were trained without the OD and image preprocessing steps. Models, which were exclusively trained on the training dataset are denoted with a "t", models, which were trained on the training and validation dataset are denoted as "t+v". The best results are highlighted in bold. by a single number, a "C_" was added as a prefix for all models which used a multiplication of the location information and a "B_" was added as a prefix for the binary country encoding. Model S4 was trained on the training and validation set and thus overestimated the ğ¹ 1 -score for the validation dataset. The idea of this model was, to improve the results on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID</head><p>Figure <ref type="figure" coords="10,131.75,554.50,5.16,10.91">3</ref> compares the ğ¹ ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>-scores of the ablation study executed for different classifiers. The validation results of model S5 and model S6 showed improved performances for the ViT Large model in comparison to the ViT Base model, although, the ViT Large model was only trained for 13 epochs, whereas the ViT Base model was trained for 50 epochs. Model S1, which was an EfficientNet-B4 model trained for 123 epochs achieved better classification results for the validation dataset than the ViT Large model. However, the overall comparison of these models is quite complex due to differences in the training pipelines. The best results for both, the original and the filtered validation dataset, were achieved for the ensemble model S7. Ensemble model B_S7 which used the binary encoding of the location information achieved an ğ¹ ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="63.20">% and an ğ¹ ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘-ğ‘“</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>-score of 63.76 %. Thus, the ensemble model outperformed the results of the individual models.</p><p>Figure <ref type="figure" coords="11,130.97,533.38,5.02,10.91">4</ref> compares the ğ¹ ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>-scores for the different metadata inclusion strategies. It can be seen, that all models showed increased classification results if the location information were multiplied with the model predictions. The binary encoding of the location information reached an additional increase of the performances for models S1, S2, S3, S4, S5 and S7.</p><p>Figure <ref type="figure" coords="11,132.28,587.57,5.17,10.91">5</ref> compares the ğ¹ ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘-ğ‘“ 1 -scores achieved using the dataset filtering strategy. This comparison did not show a positive effect for the dataset filtering strategy in these experiments neither for the ğ¹ ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘ 1 -score nor the ğ¹ ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘-ğ‘“ 1 -score. One reason for this effect might be the smaller number of different images for rare classes. Another problem in the experiments was that none of the EfficientNet models saturated for the validation set, thus it was hard to do a fair comparison between the models trained on the entire and the filtered dataset. Therefore, the effect of the dataset filtering was also investigated in the post-competition experiments  described in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Challenge Submissions and Results</head><p>Table <ref type="table" coords="12,116.54,338.74,5.17,10.91">2</ref> summarizes the results of the classification models for the official test dataset. Each team was able to submit up to ten submissions to the organizing team. Thus, some models in Table <ref type="table" coords="12,114.87,365.83,4.97,10.91" target="#tab_0">1</ref> were not validated for the test set. The primary metric for the SnakeCLEF 2021 challenge was the macro averaging ğ¹ 1 -score across countries for the independent test set (ğ¹ ğ‘¡ğ‘’ğ‘ ğ‘¡ 1 ğ¶ ). In addition, this table summarizes the macro-averaging ğ¹ 1 -scores (ğ¹ ğ‘¡ğ‘’ğ‘ ğ‘¡ 1 ) and the accuracy for the test set (ğ´ğ¶ğ¶ ğ‘¡ğ‘’ğ‘ ğ‘¡ ). Similar to Table <ref type="table" coords="12,247.14,406.48,3.70,10.91" target="#tab_0">1</ref>, the experiments included the use of deep learning-based classifiers (introduced in Sec. 4.5), metadata encoding strategies (introduced in Sec. 4.6), a filtering strategy (introduced in Sec. 4.1) and the used training dataset (t: only training dataset, t+v: unified training and validation dataset). Due to time constraints, none of the submissions used the described OD during the challenge. Thus, the effect of the OD was investigated in the post-competition experiments described in Section 5.3. The submission IDs correspond to the IDs of Table <ref type="table" coords="12,145.59,487.78,5.07,10.91" target="#tab_0">1</ref> with the name of the team "FHDO-BCSG-" as a prefix.</p><p>Similar to the validation results, the comparison of model FHDO-BCSG-B_S6 and model FHDO-BCSG-B_S5 showed that the ViT Large model architecture outperformed the ViT Base architecture, although the ViT Base model was trained using a larger number of epochs. The ViT Large model achieved an ğ¹ ğ‘¡ğ‘’ğ‘ ğ‘¡ 1 ğ¶ -score of 77.39 %. Additionally, model FHDO-BCSG-B_S1, which was the best performing EfficientNet-B4 model, reached better results than model FHDO-BCSG-B_S5, which was the ViT Large model. Model FHDO-BCSG-B_S1 reached an ğ¹ ğ‘¡ğ‘’ğ‘ ğ‘¡ 1 ğ¶ -score of 78.33 %. Both models were trained with different data augmentation pipelines and learning parameters, which make it hard to do a fair model comparison. The comparison of models FHDO-BCSG-B_S1, FHDO-BCSG-B_S5 and FHDO-BCSG-B_S7 showed that the ensemble model, which was a combination of model S5 and model S1 outperformed the individual models.</p><p>The comparison of models FHDO-BCSG-C_S7 and FHDO-BCSG-B_S7, showed that using binary location information achieved slightly better results than using the raw prior probabilities for the ensemble model. The results of Model FHDO-BCSG-C_S7, which multiplied the raw</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Classification results achieved for the official test dataset, including macro-averaging ğ¹ 1 -scores (ğ¹ ğ‘¡ğ‘’ğ‘ ğ‘¡ 1 ), macro averaging ğ¹ 1 -scores across countries (ğ¹ ğ‘¡ğ‘’ğ‘ ğ‘¡ 1 ğ¶ ) and classification accuracy (ğ´ğ¶ğ¶ ğ‘¡ğ‘’ğ‘ ğ‘¡ ). The EfficientNet-B4 classifier was abbreviated as B4, the ViT Base model was abbreviated as ViT-B and the ViT Large model was abbreviated as ViT-L. All models were trained without the OD and image preprocessing steps. Models, which were exclusively trained on the training dataset are denoted with a "t", models, which were trained on the training and validation dataset are denoted as "t+v". The ensemble model was an ensemble of model S1 and model S5. The best results are highlighted in bold. b This result was affected by a mistake during the softmax normalization in the initial submission. Post-competition investigations corrected this score to a value of 79.89 %. c This result was affected by a mistake during the softmax normalization in the initial submission. Post-competition investigations corrected this score to a value of 88.74 %.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Submission</head><p>prior probabilities of the location to the model predictions achieved worse results in all metrics in comparison to model FHDO-BCSG-B_S7, which used a binarized version of the location information. Due to a mistake during the softmax normalization in the initial submission, the differences observed by using both location information methods for the ViT Large model showed stronger differences. The post-competition resubmission of this model with corrected softmax normalization led to similar results of both models.</p><p>Model FHDO-BCSG-B_S3, which was trained on the filtered version of the training dataset, achieved slightly worse ğ¹ ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘ 1 and ğ¹ ğ‘¡ğ‘’ğ‘ ğ‘¡ 1 -scores but a better ğ¹ ğ‘¡ğ‘’ğ‘ ğ‘¡ 1 ğ¶ -score in comparison to model FHDO-BCSG-B_S2, which was trained on the unfiltered dataset. Model FHDO-BCSG-B_S3 also reached the overall best ğ´ğ¶ğ¶ ğ‘¡ğ‘’ğ‘ ğ‘¡ -score of 91.17 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Post-Competition Experiments</head><p>As previously mentioned, due to reasons of time limitations during the competition, no fair comparison between a model that used OD and one that did not use OD was performed during the competition. The same applied to using the dataset filtering strategy. Therefore, some post-competition experiments were executed leading to the results presented in Table <ref type="table" coords="14,481.26,204.44,5.17,10.91" target="#tab_4">3</ref> and Table <ref type="table" coords="14,115.76,217.99,3.73,10.91" target="#tab_5">4</ref>. Additionally, the implementation of the EfficientNet models showed some drawbacks. Mainly, the implementation limited the mini-batch size to a small number, which leads to a high number of required epochs before the model saturated. Therefore, the EfficientNet training pipeline was reimplemented after the competition deadline. This reimplementation was similar to the implementation of the ViT training pipeline and thus the pre-trained EfficientNet models were loaded from the Python library timm v0.4.8 <ref type="bibr" coords="14,307.02,285.73,17.84,10.91" target="#b47">[44]</ref> and PyTorch v1.8.0+cu101 <ref type="bibr" coords="14,445.15,285.73,17.84,10.91" target="#b48">[45]</ref> was used to train the models.</p><p>Following the implementation described in Section 4.5.1, EfficientNet-B4 models were used for all experiments. It should be mentioned that the reimplemented training pipeline differs from the pipeline described in Section 4.5.1. All models were pre-trained on the ImageNet dataset. Additionally, a mini-batch size of 25, a learning rate of 10 -4 an Adam optimizer (ğ›½ 1 = 0.9, ğ›½ 2 = 0.999, ğœ– = 10 -8 ) and cross-entropy loss were used to train the models. No additional dense layer was introduced before the classification layer. Thus, the models contain 18,932,812 parameters. Similar to the ViT implementation, no class weights were used to train the models. A scheduler was implemented which reduced the learning rate by a factor of 0.1 if the classification accuracies did not improve for five epochs. Early stopping was implemented if the loss function showed no improvement for more than eight epochs. The maximum number of epochs chosen was 100. A mixed-precision strategy was implemented to speed up the training process and increase the mini-batch size.</p><p>Similar to the ViT models, the augmentation pipeline included random cropping from an image size of 418 Ã— 418 to 380 Ã— 380, and a random horizontal as well as vertical flip each with a probability of 0.5.</p><p>The experimental results for the validation dataset are summarized in Table <ref type="table" coords="14,432.81,516.07,3.66,10.91" target="#tab_4">3</ref>. In comparison to the models submitted during the challenge, the results showed that the number of epochs to train the EfficientNet model was reduced from more than 100 to less than 10 epochs using the reimplemented pipeline. Nevertheless, the achieved results outperformed the challenge results.</p><p>The effect of two pipeline modules was investigated. First, the effect of the dataset filtering strategy was examined by training two models, model S8 was trained on the entire training dataset and model S9 used the filtered training set. As can be seen in Figure <ref type="figure" coords="14,427.07,597.37,3.72,10.91">6</ref>, the comparison of both models showed improved results for model S9. Additionally, the overall best ğ¹ ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘   The second experiment investigated the effect of the OD module and was visualized in Figure <ref type="figure" coords="15,501.26,491.04,4.97,10.91">7</ref> using barplots. Therefore, the ğ¹ 1 -scores achieved by model S9 were compared to those reached for model S10, trained using the OD module described in Section 4.2. It can be noted that the results of model S10 reached worse results than model S9. Thus, for this model architecture, the OD module harmed the classification results. It was assumed that this reduction of the classification results might lead from the preprocessing pipeline used after the OD. For this reason, model S11 was trained using the OD module but no further image preprocessing. It can be noted that the results for model S11 outperformed the results of model S10, and reached similar results as model S9. However, the OD showed no improvement for the validation dataset in comparison to using the unprocessed images.</p><p>All post-competition models with the binary location encoding were evaluated for the test set. The achieved results are summarized in Table <ref type="table" coords="15,308.71,640.08,3.66,10.91" target="#tab_5">4</ref>. Model B_S8 was the EfficientNet-B4 model that was trained on the entire training dataset. This model reached an ğ¹ ğ‘¡ğ‘’ğ‘ ğ‘¡ 1 ğ¶ -score of 65.70 %. Model B_S9, which was trained for the filtered dataset, outperformed this model by reaching an The comparison between model B_S9, B_S10 and B_S11 investigated the effect of the OD module. Model B_S10 used both, the OD and image preprocessing pipelines and performed worse than model B_S9. However, model B_S11 that used only the OD module outperformed both models with an ğ¹ ğ‘¡ğ‘’ğ‘ ğ‘¡ 1 ğ¶ -score of 78.44 %.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Overall, snake species identification is a challenging task, dealing with highly imbalanced class distributions, high intra-class variance and small inter-class variance.</p><p>The experiments investigated in this research, show that both, EfficientNets and ViTs can be successfully trained to distinguish between snake species. The best results were achieved by combining both model types using an ensemble model. ViT models which used the large architecture achieved better classification results than the ViT Base architecture. The multiplication of location information with the model predictions can improve the results of both model types.</p><p>The training dataset was also filtered for images that did not include snakes. This filtering strategy showed improvements in the post-competition experiments for the test set. The visual validation of the out-of-class images showed some misclassified images that contained snakes. It was planned to improve this filtering strategy in future work to prevent those misclassifications using more recently deep learning models for this step.</p><p>Due to time constraints, only one model was trained on the entire training and validation dataset. It was expected that a model training on the entire dataset would increase the results for the test dataset. Additionally, during the competition, most models did not reach a plateau and thus the optimal number of epochs and the optimal classification results were not achieved for the submissions. This fact may also lead to worse performances for the filtering strategy during the competition and could be surmounted in the post-competition experiments. Another drawback that resulted from time constraints was that no models were evaluated for the OD method during the challenge. Post-competition experiments overcome this limitation and the OD showed improved test results when using no image preprocessing after the OD.</p><p>As has already been prepared in the post-competition experiments, future work will address the implementation of a more uniform implementation pipeline to enable a fair comparison of EfficientNets and ViT models. Additionally, the impact of different mini-batch sizes and learning rates should be investigated more systematically. To improve the classification results, up-scaled EfficientNet and ViT architectures should also be used and the pipeline will be enhanced using EfficientNetv2 classifiers <ref type="bibr" coords="17,204.01,412.15,16.41,10.91" target="#b50">[46]</ref>. Furthermore, the results of using different Transfer-Learning strategies should be compared to each other.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,301.70,346.99,8.93"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Distribution of the snake species in the unified training and validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,89.29,331.14,279.87,8.93"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: ML workflow used to differentiate between snake species.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,119.98,606.01,86.05,10.91;9,207.55,603.44,29.42,6.99;9,206.03,611.87,4.23,6.99;9,238.35,606.01,267.63,10.91;9,89.29,619.56,416.70,10.91;9,88.96,633.10,417.02,10.91;9,88.96,646.65,417.02,10.91;9,89.29,660.20,416.70,10.91"><head>) 1 )</head><label>1</label><figDesc>and the filtered (ğ¹ ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘-ğ‘“ version of the validation dataset. The experiments included the use of deep learning-based classifiers (introduced in Sec. 4.5), metadata encoding strategies (introduced in Sec. 4.6), a filtering strategy (introduced in Sec. 4.1) and the used training dataset (t: only training dataset, t+v: unified training and validation dataset). For each model, the results of three versions were provided. The results of the base models are named by an "S" followed</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="11,89.29,235.12,148.05,9.15;11,238.72,233.55,18.09,6.12;11,237.34,239.63,3.97,6.12;11,257.31,235.39,248.90,8.87;11,89.29,247.34,416.70,8.87;11,88.93,259.30,373.89,8.87"><head>Figure 3 : 1 -</head><label>31</label><figDesc>Figure 3: Barplot to compare the ğ¹ ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘ 1 -scores achieved for different classifiers. The EfficientNet-B4 classifier was abbreviated as B4, the ViT Base model was abbreviated as ViT-B and the ViT Large model was abbreviated as ViT-L. The ensemble model was an ensemble of model S1 and model S5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="11,89.29,434.81,147.10,9.15;11,237.77,433.24,18.09,6.12;11,236.39,439.33,3.97,6.12;11,256.35,435.08,251.30,8.87;11,89.29,447.04,416.70,8.87;11,89.29,458.99,416.92,8.87;11,89.29,470.95,57.68,8.87"><head>Figure 4 : 1 -</head><label>41</label><figDesc>Figure 4: Barplot to compare the ğ¹ ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘ 1 -scores achieved for the different location information strategies. The EfficientNet-B4 classifier was abbreviated as B4, the ViT Base model was abbreviated as ViT-B and the ViT Large model was abbreviated as ViT-L. The ensemble model was an ensemble of model S1 and model S5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="12,89.29,250.87,146.25,9.15;12,236.92,248.12,28.19,6.12;12,235.54,255.57,3.97,6.12"><head>Figure 5 : 1 -</head><label>51</label><figDesc>Figure 5: Barplot to compare the ğ¹ ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘-ğ‘“ 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="13,304.24,584.16,9.35,10.91;13,100.20,597.71,405.79,10.91;13,89.29,611.26,418.54,10.91;13,89.29,624.80,416.69,10.91;13,89.29,638.35,416.69,10.91;13,89.29,651.90,100.35,10.91;13,100.20,665.45,405.79,10.91;14,89.29,86.97,418.38,10.91;14,88.96,100.52,112.57,10.91;14,203.05,99.29,13.96,6.99;14,201.53,106.18,4.23,6.99;14,217.51,100.52,100.99,10.91;14,320.01,99.29,13.96,6.99;14,318.49,106.18,9.52,7.94;14,337.19,100.52,105.33,10.91;14,443.30,99.29,13.96,6.99;14,460.48,100.52,47.19,10.91;14,89.29,114.06,361.63,10.91"><head></head><label></label><figDesc>%. In comparison to model FHDO-BCSG-B_S3, model FHDO-BCSG-B_S4 was trained on the unified training and validation dataset. Due to reasons of time limitations during the competition, this model was only trained for 25 instead of 113 epochs. This model showed that the classification results might be improved by training the EfficientNet-B4 model on the training and validation dataset. The best model submitted by FHDO BCSG was model FHDO-BCSG-B_S7, which was an ensemble of model S1 and model S5 and added binary location to the prediction probabilities. This model reached an ğ¹ ğ‘¡ğ‘’ğ‘ ğ‘¡ 1 -score of 78.75 %, an ğ¹ ğ‘¡ğ‘’ğ‘ ğ‘¡ 1 ğ¶ of 82.88 % and an ğ´ğ¶ğ¶ ğ‘¡ğ‘’ğ‘ ğ‘¡ of 90.42 %. Model FHDO-BCSG-B_S7 reached fourth place in the SnakeCLEF 2021 challenge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="14,483.28,616.58,4.23,6.99;14,504.07,610.92,3.76,10.91;14,89.29,624.46,416.69,10.91;14,89.29,638.01,110.87,10.91;14,201.67,635.44,29.42,6.99;14,200.16,643.88,4.23,6.99;14,232.48,638.01,273.51,10.91;14,89.29,651.56,324.57,10.91;14,415.37,650.34,18.77,6.99;14,413.86,657.22,4.23,6.99;14,434.64,651.56,71.69,10.91;14,89.29,666.32,39.79,10.91;14,130.59,663.75,29.42,6.99;14,129.08,672.19,4.23,6.99;14,161.39,666.32,76.06,10.91;15,178.63,91.51,3.58,5.67;15,301.13,91.51,17.55,5.67;15,428.57,91.51,21.64,5.67;15,148.09,211.26,7.49,5.67;15,135.62,217.73,32.44,5.67;15,205.25,211.26,7.49,5.67;15,197.04,217.73,23.92,5.67;15,277.57,211.26,7.49,5.67;15,265.10,217.73,32.44,5.67;15,334.73,211.26,7.49,5.67;15,326.52,217.73,23.92,5.67;15,407.06,211.26,7.49,5.67;15,394.58,217.73,32.44,5.67;15,464.22,211.26,7.49,5.67;15,456.00,217.73,23.92,5.67;15,110.78,200.80,3.41,5.67;15,107.37,173.14,6.81,5.67;15,107.37,145.48,6.81,5.67;15,107.37,117.82,6.81,5.67;15,95.33,182.94,6.93,4.58;15,99.22,179.91,5.04,3.03;15,93.07,167.24,5.04,15.70;15,93.14,161.74,9.76,4.11;15,95.33,142.04,6.93,18.31;15,95.33,131.64,6.93,8.32;15,95.33,120.40,6.93,9.15"><head>1 - 1 - 1 - 1 -</head><label>1111</label><figDesc>score of 70.92 % was achieved for model B_S9, which used the binary encoding of the location information. The best ğ¹ ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘-ğ‘“ score was 70.11 % reached by model C_S9. The best model trained on the entire training dataset was model B_S8, which reached an ğ¹ ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘ score of 68.55 % and an ğ¹ ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘-ğ‘“</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="15,89.29,247.19,220.42,9.15;15,311.10,244.44,28.20,6.12;15,309.71,251.88,3.97,6.12;15,340.57,247.46,165.41,8.87;15,89.29,259.41,35.18,8.87"><head>Figure 6 : 1 --</head><label>61</label><figDesc>Figure 6: Barplot to compare the post-competition ğ¹ ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘-ğ‘“ 1 -scores achieved for the dataset filtering strategy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12" coords="15,89.29,452.89,219.05,9.15;15,309.72,450.15,28.19,6.12;15,308.34,457.59,3.97,6.12"><head>Figure 7 : 1 -</head><label>71</label><figDesc>Figure 7: Barplot to compare the post-competition ğ¹ ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘-ğ‘“ 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,88.96,592.46,417.02,26.20"><head>Table 1</head><label>1</label><figDesc>summarizes the macro-averaging ğ¹ 1 -scores of the classification models for the original (ğ¹ ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘</figDesc><table coords="9,99.19,611.67,4.23,6.99"><row><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,88.99,90.49,331.25,22.37"><head>Table 1</head><label>1</label><figDesc>Macro-averaging ğ¹ 1 -scores achieved for the official validation dataset (ğ¹ ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘</figDesc><table coords="10,400.77,106.74,3.97,6.12"><row><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="13,95.27,191.99,427.21,198.36"><head></head><label></label><figDesc>This result was affected by a mistake during the softmax normalization in the initial submission. Post-competition investigations corrected this score to a value of 72.71 %.</figDesc><table coords="13,95.27,191.99,427.21,171.70"><row><cell>ID</cell><cell cols="3">Classifier Epochs Location Data-</cell><cell>Training</cell><cell>ğ¹ ğ‘¡ğ‘’ğ‘ ğ‘¡ 1</cell><cell>ğ¹ ğ‘¡ğ‘’ğ‘ ğ‘¡ 1 ğ¶</cell><cell>ğ´ğ¶ğ¶ ğ‘¡ğ‘’ğ‘ ğ‘¡</cell></row><row><cell></cell><cell></cell><cell></cell><cell>set</cell><cell>dataset</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>filter-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>ing</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FHDO-BCSG-B_S1</cell><cell>B4</cell><cell cols="2">123 binary -</cell><cell>t</cell><cell>75.28 %</cell><cell>78.33 %</cell><cell>89.14 %</cell></row><row><cell>FHDO-BCSG-B_S2</cell><cell>B4</cell><cell cols="2">113 binary -</cell><cell>t</cell><cell>74.45 %</cell><cell>76.16 %</cell><cell>89.08 %</cell></row><row><cell>FHDO-BCSG-B_S3</cell><cell>B4</cell><cell cols="2">113 binary yes</cell><cell>t</cell><cell>72.82 %</cell><cell cols="2">81.04 % 91.17 %</cell></row><row><cell>FHDO-BCSG-B_S4</cell><cell>B4</cell><cell cols="2">25 binary yes</cell><cell>t+v</cell><cell>75.20 %</cell><cell>76.62 %</cell><cell>89.58 %</cell></row><row><cell cols="2">FHDO-BCSG-C_S5 ViT-L</cell><cell cols="2">13 country -</cell><cell>t</cell><cell cols="3">a 30.22 % b 51.17 % c 59.80 %</cell></row><row><cell>FHDO-BCSG-B_S5</cell><cell>ViT-L</cell><cell cols="2">13 binary -</cell><cell>t</cell><cell>74.06 %</cell><cell>77.39 %</cell><cell>88.90 %</cell></row><row><cell>FHDO-BCSG-B_S6</cell><cell>ViT-B</cell><cell cols="2">50 binary -</cell><cell>t</cell><cell>64.98 %</cell><cell>69.46 %</cell><cell>82.96 %</cell></row><row><cell>FHDO-BCSG-S7</cell><cell>Ensemble</cell><cell>--</cell><cell>-</cell><cell>t</cell><cell>70.58 %</cell><cell>72.56 %</cell><cell>87.85 %</cell></row><row><cell cols="2">FHDO-BCSG-C_S7 Ensemble</cell><cell cols="2">-country -</cell><cell>t</cell><cell>76.27 %</cell><cell>82.04 %</cell><cell>90.10 %</cell></row><row><cell>FHDO-BCSG-B_S7</cell><cell>Ensemble</cell><cell cols="2">-binary -</cell><cell>t</cell><cell>78.75 %</cell><cell>82.88 %</cell><cell>90.42 %</cell></row></table><note coords="13,95.27,367.68,3.97,6.12"><p>a </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="16,88.99,90.49,418.66,239.51"><head>Table 3</head><label>3</label><figDesc>Post-competition macro-averaging ğ¹ 1 -scores achieved after the challenge deadline for the official validation dataset (ğ¹ ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘ 1 ) and the validation dataset which was filtered for out-of-class images (ğ¹ ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘-ğ‘“1). The EfficientNet-B4 classifier was abbreviated as B4. All models were trained on the training dataset. The best results are highlighted in bold.</figDesc><table coords="16,97.27,157.22,400.73,172.78"><row><cell>ID</cell><cell cols="2">OD Pre-</cell><cell cols="2">Classifier Epochs Location</cell><cell>Dataset</cell><cell>ğ¹ ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘ 1</cell><cell>ğ¹ ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘-ğ‘“ 1</cell></row><row><cell></cell><cell></cell><cell>processing</cell><cell></cell><cell></cell><cell>filtering</cell><cell></cell><cell></cell></row><row><cell>S8</cell><cell>-</cell><cell>-</cell><cell>B4</cell><cell>4 -</cell><cell>-</cell><cell>58.12 %</cell><cell>58.24 %</cell></row><row><cell>C_S8</cell><cell>-</cell><cell>-</cell><cell>B4</cell><cell>4 country</cell><cell>-</cell><cell>68.20 %</cell><cell>67.91 %</cell></row><row><cell>B_S8</cell><cell>-</cell><cell>-</cell><cell>B4</cell><cell>4 binary</cell><cell>-</cell><cell>68.55 %</cell><cell>68.85 %</cell></row><row><cell>S9</cell><cell>-</cell><cell>-</cell><cell>B4</cell><cell>6 -</cell><cell>yes</cell><cell>60.85 %</cell><cell>60.71 %</cell></row><row><cell>C_S9</cell><cell>-</cell><cell>-</cell><cell>B4</cell><cell>6 country</cell><cell>yes</cell><cell>69.92 %</cell><cell>70.92 %</cell></row><row><cell>B_S9</cell><cell>-</cell><cell>-</cell><cell>B4</cell><cell>6 binary</cell><cell>yes</cell><cell>70.11 %</cell><cell>70.36 %</cell></row><row><cell>S10</cell><cell cols="2">OD yes</cell><cell>B4</cell><cell>5 -</cell><cell>yes</cell><cell>48.99 %</cell><cell>49.88 %</cell></row><row><cell cols="3">C_S10 OD yes</cell><cell>B4</cell><cell>5 country</cell><cell>yes</cell><cell>60.30 %</cell><cell>61.80 %</cell></row><row><cell>B_S10</cell><cell cols="2">OD yes</cell><cell>B4</cell><cell>5 binary</cell><cell>yes</cell><cell>60.61 %</cell><cell>61.74 %</cell></row><row><cell>S11</cell><cell cols="2">OD -</cell><cell>B4</cell><cell>5 -</cell><cell>yes</cell><cell>59.70 %</cell><cell>60.49 %</cell></row><row><cell cols="3">C_S11 OD -</cell><cell>B4</cell><cell>5 country</cell><cell>yes</cell><cell>69.00 %</cell><cell>70.27 %</cell></row><row><cell>B_S11</cell><cell cols="2">OD -</cell><cell>B4</cell><cell>5 binary</cell><cell>yes</cell><cell>69.32 %</cell><cell>70.91 %</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="16,88.99,352.26,418.65,181.92"><head>Table 4</head><label>4</label><figDesc>Post-competition classification results achieved for the official test dataset, including macro-averaging ğ¹ 1 -scores (ğ¹ ğ‘¡ğ‘’ğ‘ ğ‘¡ 1 ), macro averaging ğ¹ 1 -scores across countries (ğ¹ ğ‘¡ğ‘’ğ‘ ğ‘¡ 1 ğ¶ ) and classification accuracy (ğ´ğ¶ğ¶ ğ‘¡ğ‘’ğ‘ ğ‘¡ ). The EfficientNet-B4 classifier was abbreviated as B4. All models were trained on the training dataset. The best results are highlighted in bold.</figDesc><table coords="16,89.29,417.90,416.67,115.32"><row><cell>ID</cell><cell cols="2">OD Pre-</cell><cell cols="3">Classifier Epochs Location Dataset</cell><cell>ğ¹ ğ‘¡ğ‘’ğ‘ ğ‘¡ 1</cell><cell>ğ¹ ğ‘¡ğ‘’ğ‘ ğ‘¡ 1 ğ¶</cell><cell>ğ´ğ¶ğ¶ ğ‘¡ğ‘’ğ‘ ğ‘¡</cell></row><row><cell></cell><cell></cell><cell>processing</cell><cell></cell><cell></cell><cell>filtering</cell><cell></cell><cell></cell></row><row><cell>B_S8</cell><cell>-</cell><cell>-</cell><cell>B4</cell><cell>4 binary</cell><cell>-</cell><cell cols="2">67.97 % 65.70 %</cell><cell>74.57 %</cell></row><row><cell>B_S9</cell><cell>-</cell><cell>-</cell><cell>B4</cell><cell>6 binary</cell><cell>yes</cell><cell cols="2">69.82 % 78.11 %</cell><cell>82.78 %</cell></row><row><cell>B_S10</cell><cell cols="2">OD yes</cell><cell>B4</cell><cell>5 binary</cell><cell>yes</cell><cell cols="2">64.66 % 68.65 %</cell><cell>72.65 %</cell></row><row><cell>B_S11</cell><cell cols="2">OD -</cell><cell>B4</cell><cell>5 binary</cell><cell>yes</cell><cell cols="3">72.16 % 78.44 % 83.11 %</cell></row><row><cell>ğ¹ ğ‘¡ğ‘’ğ‘ ğ‘¡ 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note coords="16,100.54,528.93,5.28,5.24;16,112.28,520.57,76.06,10.91"><p>ğ¶ -score of 78.11 %.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,108.93,649.10,398.57,8.97;2,89.29,660.06,11.53,8.97"><p>Google Image Search: https://images.google.com/imghp?hl=de&amp;gl=de&amp;gws_rd=ssl, [Last accessed: 2021-07-01]</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="2,105.65,669.45,2.78,5.98;2,108.93,671.02,298.13,8.97"><p><ref type="bibr" coords="2,105.65,669.45,2.78,5.98" target="#b3">4</ref> Tropical Herping: https://www.tropicalherping.com/, [Last accessed: 2021-07-01]</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_2" coords="5,108.93,670.92,250.07,8.97"><p>https://github.com/ermig1979/AntiDupl, [Last accessed: 2021-07-01]</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_3" coords="6,108.93,660.02,361.63,8.97"><p>DiffProML Mask R-CNN: https://github.com/DiffPro-ML/Mask_RCNN, [Last accessed: 2021-07-01]</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_4" coords="6,108.93,670.98,360.09,8.97"><p>Matterport Mask R-CNN: https://github.com/matterport/Mask_RCNN, [Last accessed: 2021-07-01]</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The work of <rs type="person">Louise Bloch</rs> was partially funded by a PhD grant from <rs type="funder">University of Applied Sciences and Arts Dortmund, Dortmund, Germany</rs>.</p><p>The authors thank <rs type="person">LukÃ¡Å¡ Picek</rs>, <rs type="affiliation">Department of Cybernetics, University of West Bohemia, Czechia</rs>, for post-competition evaluation.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="17,112.66,583.60,394.62,10.91;17,112.28,597.15,393.71,10.91;17,112.66,610.69,394.09,10.91;17,112.66,624.24,116.58,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="17,368.26,583.60,139.01,10.91;17,112.28,597.15,291.27,10.91">Overview of SnakeCLEF 2021: Automatic Snake Species Identification with Country-Level Focus</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De CastaÃ±eda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,426.79,597.15,79.20,10.91;17,112.66,610.69,323.09,10.91">Working Notes of the 12th International Conference of the CLEF Association (CLEF 2021)</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">09. 2021. 2021</date>
			<biblScope unit="page" from="21" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,637.79,393.33,10.91;17,112.66,651.34,394.53,10.91;17,112.66,664.89,393.33,10.91;18,112.66,86.97,393.33,10.91;18,112.66,100.52,394.52,10.91;18,112.66,114.06,67.18,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="17,166.28,664.89,339.70,10.91;18,112.66,86.97,259.88,10.91">Overview of LifeCLEF 2021: A System-oriented Evaluation of Automated Species Identification and Species Distribution Prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De CastaÃ±eda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>PlanquÃ©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dorso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,398.06,86.97,107.92,10.91;18,112.66,100.52,276.01,10.91">Proceedings of the 12th International Conference of the CLEF Association (CLEF 2021)</title>
		<meeting>the 12th International Conference of the CLEF Association (CLEF 2021)<address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">09. 2021. 2021</date>
			<biblScope unit="page" from="21" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,127.61,394.53,10.91;18,112.14,141.16,394.11,10.91;18,112.66,154.71,395.00,10.91;18,112.66,168.26,393.33,10.91;18,112.66,181.81,393.53,10.91;18,112.66,195.36,395.79,10.91;18,112.36,211.35,26.14,7.90" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="18,464.60,141.16,41.64,10.91;18,112.66,154.71,290.51,10.91">LifeCLEF 2021 Teaser: Biodiversity Identification and Prediction Challenges</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>De CastaÃ±eda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-72240-1_70</idno>
	</analytic>
	<monogr>
		<title level="m" coord="18,378.93,168.26,127.05,10.91;18,112.66,181.81,187.73,10.91">Proceedings of the European Conference on Information Retrieval (ECIR</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M.-F</forename><surname>Moens</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Mothe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Perego</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Potthast</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</editor>
		<meeting>the European Conference on Information Retrieval (ECIR<address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2021">2021. 2021. 2021</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="601" to="607" />
		</imprint>
	</monogr>
	<note>Online Event</note>
</biblStruct>

<biblStruct coords="18,112.66,222.46,395.17,10.91;18,112.66,236.01,394.51,10.91;18,112.66,252.00,67.81,7.90" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="18,136.76,236.01,102.29,10.91">Snakebite Envenoming</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>GutiÃ©rrez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Calvete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Habib</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Warrell</surname></persName>
		</author>
		<idno type="DOI">10.1038/nrdp.2017.63</idno>
	</analytic>
	<monogr>
		<title level="j" coord="18,251.27,236.01,149.14,10.91">Nature Reviews Disease Primers</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,263.11,395.17,10.91;18,112.66,276.66,393.32,10.91;18,112.66,290.20,277.73,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="18,137.84,276.66,368.15,10.91;18,112.66,290.20,45.42,10.91">The Urgent Need to Develop Novel Strategies for the Diagnosis and Treatment of Snakebites</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">F</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">J</forename><surname>Layfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Vallance</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Bicknell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Trim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vaiyapuri</surname></persName>
		</author>
		<idno type="DOI">10.3390/toxins11060363</idno>
	</analytic>
	<monogr>
		<title level="j" coord="18,166.55,290.20,29.72,10.91">Toxins</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,303.75,395.17,10.91;18,112.66,317.30,393.33,10.91;18,112.66,330.85,393.32,10.91;18,112.66,344.40,395.00,10.91;18,112.41,357.95,213.73,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="18,286.15,303.75,221.68,10.91;18,112.66,317.30,393.33,10.91;18,112.66,330.85,125.43,10.91">No More Fear of Every Snake: Applying Chatbot-Based Learning System for Snake Knowledge Promotion Improvement: A Regional Snake Knowledge Learning System</title>
		<author>
			<persName coords=""><forename type="first">H.-T</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-C</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICALT49669.2020.00029</idno>
	</analytic>
	<monogr>
		<title level="m" coord="18,261.65,330.85,244.33,10.91;18,112.66,344.40,219.29,10.91">Proceedings of the IEEE 20th International Conference on Advanced Learning Technologies (ICALT 2020)</title>
		<meeting>the IEEE 20th International Conference on Advanced Learning Technologies (ICALT 2020)<address><addrLine>Tartu, Estonia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">07.2020. 2020</date>
			<biblScope unit="page" from="72" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,371.50,395.17,10.91;18,112.66,385.05,393.33,10.91;18,112.66,398.60,393.33,10.91;18,112.66,412.15,394.53,10.91;18,112.66,425.70,281.06,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="18,210.32,385.05,295.67,10.91;18,112.66,398.60,247.53,10.91">Combination of Image and Location Information for Snake Species Identification using Object Detection and EfficientNets</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Boketta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Keibel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>An Alex Michailutschenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>RÃ¼ckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Willemeit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_201.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="18,384.74,398.60,121.24,10.91;18,112.66,412.15,261.93,10.91">Working Notes of the 11th Conference and Labs of the Evaluation Forum (CLEF 2020)</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">09. 2020. 2020</date>
			<biblScope unit="page" from="22" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,439.25,393.60,10.91;18,112.66,452.79,393.33,10.91;18,112.66,466.34,245.86,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="18,335.27,439.25,170.99,10.91;18,112.66,452.79,182.28,10.91">Discriminative Histogram Taxonomy Features for Snake Species Identification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mathews</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sugathan</surname></persName>
		</author>
		<idno type="DOI">10.1186/s13673-014-0003-0</idno>
	</analytic>
	<monogr>
		<title level="j" coord="18,303.46,452.79,202.53,10.91;18,112.66,466.34,37.51,10.91">Human-Centric Computing and Information Sciences</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,479.89,393.33,10.91;18,112.66,493.44,394.53,10.91;18,112.66,506.99,393.33,10.91;18,112.66,520.54,394.53,10.91;18,112.66,534.09,251.43,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="18,334.36,479.89,171.63,10.91;18,112.66,493.44,163.59,10.91">Image Classification for Snake Species Using Machine Learning Techniques</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">A H</forename><surname>Zahri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Yaakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Ahmad</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-48517-1_5</idno>
	</analytic>
	<monogr>
		<title level="m" coord="18,112.66,506.99,368.56,10.91">Proceedings of the Computational Intelligence in Information Systems Conference</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Phon-Amnuaisuk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T.-W</forename><surname>Au</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Omar</surname></persName>
		</editor>
		<meeting>the Computational Intelligence in Information Systems Conference<address><addrLine>CIIS; Brunei Darussalam; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016. 2016. 2017</date>
			<biblScope unit="page" from="52" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,547.64,393.71,10.91;18,112.66,561.19,395.00,10.91;18,112.33,574.74,394.94,10.91;18,112.41,588.29,395.25,10.91;18,112.66,601.84,225.75,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="18,282.37,547.64,223.99,10.91;18,112.66,561.19,238.67,10.91">CEDD: Color and Edge Directivity Descriptor: A Compact Descriptor for Image Indexing and Retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-79547-6_30</idno>
	</analytic>
	<monogr>
		<title level="m" coord="18,179.94,574.74,322.89,10.91">Proceedings of the International Computer Vision Systems (ICVS 2008)</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Gasteratos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vincze</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</editor>
		<meeting>the International Computer Vision Systems (ICVS 2008)<address><addrLine>Santorini, Greece; Berlin Heidelberg, Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="312" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,615.39,395.17,10.91;18,112.66,628.93,394.62,10.91;18,112.66,642.48,314.36,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="18,206.70,615.39,255.36,10.91">Snake Detection and Classification using Deep Learning</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sinnott</surname></persName>
		</author>
		<idno type="DOI">10.24251/hicss.2021.148</idno>
	</analytic>
	<monogr>
		<title level="m" coord="18,488.38,615.39,19.45,10.91;18,112.66,628.93,390.23,10.91">Proceedings of the 54th Hawaii International Conference on System Sciences (HICSS 2021)</title>
		<meeting>the 54th Hawaii International Conference on System Sciences (HICSS 2021)<address><addrLine>Maui, Hawaii, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">1.2021. 2021</date>
			<biblScope unit="page" from="5" to="08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,656.03,393.33,10.91;18,112.66,669.58,394.53,10.91;19,112.28,86.97,231.46,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="18,461.51,656.03,44.48,10.91;18,112.66,669.58,389.81,10.91">Revealing the Unknown: Real-Time Recognition of GalÃ¡pagos Snake Species Using Deep Learning</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Khatod</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Matijosaitiene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arteaga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Gilkey</surname></persName>
		</author>
		<idno type="DOI">10.3390/ani10050806</idno>
	</analytic>
	<monogr>
		<title level="j" coord="19,112.28,86.97,37.19,10.91">Animals</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">806</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,100.52,395.17,10.91;19,112.66,114.06,393.33,10.91;19,112.66,127.61,394.51,10.91;19,112.66,143.61,133.09,7.90" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="19,407.75,100.52,100.08,10.91;19,112.66,114.06,113.15,10.91">Snake Species Identification and Recognition</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Vasmatkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Zare</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kumbla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Pimpalkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="DOI">10.1109/IBSSC51096.2020.9332218</idno>
	</analytic>
	<monogr>
		<title level="m" coord="19,261.98,114.06,244.01,10.91;19,112.66,127.61,52.04,10.91">Proceedings of the IEEE Bombay Section Signature Conference</title>
		<meeting>the IEEE Bombay Section Signature Conference<address><addrLine>IBSSC; Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020. 2020</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,154.71,394.53,10.91;19,112.66,168.26,395.17,10.91;19,112.66,181.81,393.33,10.91;19,112.66,195.36,357.89,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="19,277.24,154.71,224.96,10.91">Snake Image Classification Using Siamese Networks</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Abeysinghe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Welivita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Perera</surname></persName>
		</author>
		<idno type="DOI">10.1145/3338472.3338476</idno>
	</analytic>
	<monogr>
		<title level="m" coord="19,129.69,168.26,378.14,10.91;19,112.66,181.81,76.31,10.91">Proceedings of the 3rd International Conference on Graphics and Signal Processing (ICGSP 2019)</title>
		<meeting>the 3rd International Conference on Graphics and Signal Processing (ICGSP 2019)<address><addrLine>Hong Kong; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="8" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,208.91,393.33,10.91;19,112.66,222.46,393.33,10.91;19,112.66,236.01,394.10,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="19,305.88,208.91,200.10,10.91;19,112.66,222.46,167.57,10.91">Image-Based Classification of Snake Species Using Convolutional Neural Network</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">S</forename><surname>Abdurrazaq</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Suyanto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">Q</forename><surname>Utama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,304.83,222.46,201.16,10.91;19,112.66,236.01,267.09,10.91">Proceedings of the International Seminar on Research of Information Technology and Intelligent Systems</title>
		<meeting>the International Seminar on Research of Information Technology and Intelligent Systems<address><addrLine>ISRITI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.34,249.56,395.32,10.91;19,112.66,263.11,236.61,10.91" xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Indonesia</forename><surname>Yogyakarta</surname></persName>
		</author>
		<idno type="DOI">10.1109/isriti48646.2019.9034633</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="97" to="102" />
		</imprint>
		<respStmt>
			<orgName>Institute of Electrical and Electronics Engineers</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,276.66,393.32,10.91;19,112.66,290.20,393.33,10.91;19,112.66,303.75,393.33,10.91;19,112.66,317.30,393.34,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="19,338.47,276.66,167.52,10.91;19,112.66,290.20,66.92,10.91">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2009.5206848</idno>
	</analytic>
	<monogr>
		<title level="m" coord="19,202.66,290.20,303.33,10.91;19,112.66,303.75,110.50,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2009)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2009)<address><addrLine>Miami Beach, Florida, US</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">06. 2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
		<respStmt>
			<orgName>Institute of Electrical and Electronics Engineers</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,330.85,393.33,10.91;19,112.26,344.40,393.73,10.91;19,112.66,357.95,313.32,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="19,268.47,330.85,237.51,10.91;19,112.26,344.40,143.20,10.91">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2016.2577031</idno>
	</analytic>
	<monogr>
		<title level="j" coord="19,266.42,344.40,239.56,10.91;19,112.66,357.95,51.98,10.91">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,371.50,395.17,10.91;19,112.66,385.05,394.61,10.91;19,112.66,398.60,36.01,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="19,255.75,371.50,209.16,10.91">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,488.44,371.50,19.38,10.91;19,112.66,385.05,335.63,10.91">Proceesings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>eesings of the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="27" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,148.67,398.60,357.31,10.91;19,112.33,412.15,245.33,10.91" xml:id="b19">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/cvpr.2016.90</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="770" to="778" />
			<pubPlace>Las Vegas, Nevada, US</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Institute of Electrical and Electronics Engineers</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,425.70,393.33,10.91;19,112.66,439.25,393.33,10.91;19,112.66,452.79,393.33,10.91;19,112.66,466.34,285.60,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="19,242.31,425.70,263.68,10.91;19,112.66,439.25,52.42,10.91">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.1556" />
	</analytic>
	<monogr>
		<title level="m" coord="19,317.59,439.25,188.40,10.91;19,112.66,452.79,302.62,10.91">Conference Track Proceedings of the 3rd International Conference on Learning Representations (ICLR 2015)</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego, California, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="7" to="09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,479.89,395.17,10.91;19,112.66,493.44,393.33,10.91;19,112.66,506.99,395.01,10.91;19,112.66,520.54,134.86,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="19,381.18,479.89,126.65,10.91;19,112.66,493.44,80.11,10.91">Densely Connected Convolutional Networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.243</idno>
	</analytic>
	<monogr>
		<title level="m" coord="19,221.48,493.44,284.50,10.91;19,112.66,506.99,89.46,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07">2017. 07.2017. 2017</date>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,534.09,393.33,10.91;19,112.66,547.64,393.53,10.91;19,112.30,561.19,234.86,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="19,399.80,534.09,106.19,10.91;19,112.66,547.64,146.08,10.91">MobileNetV2: Inverted Residuals and Linear Bottlenecks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,281.94,547.64,224.25,10.91;19,112.30,561.19,136.98,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="18" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,347.16,561.19,160.03,10.91;19,112.66,574.74,214.76,10.91" xml:id="b23">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/CVPR.2018.00474</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="4510" to="4520" />
			<pubPlace>Salt Lake City, Utah, US</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,588.29,394.62,10.91;19,112.66,601.84,394.61,10.91;19,112.66,615.39,395.00,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="19,186.64,588.29,296.86,10.91">GrabCut Color Image Segmentation Based on Region of Interest</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.1109/CISP.2014.7003812</idno>
	</analytic>
	<monogr>
		<title level="m" coord="19,112.66,601.84,337.15,10.91">Proceedings of the 7th International Congress on Image and Signal Processing</title>
		<meeting>the 7th International Congress on Image and Signal Processing<address><addrLine>ICISP; Cherburg, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014. 2014</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="392" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,628.93,393.33,10.91;19,112.66,642.48,394.53,10.91;19,112.28,656.03,394.91,10.91;19,112.39,669.58,394.31,10.91;20,112.66,86.97,208.44,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="19,367.66,628.93,138.32,10.91;19,112.66,642.48,176.20,10.91">Signature Verification Using a &quot;Siamese&quot; Time Delay Neural Network</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>SÃ¤ckinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/1993/file/288cc0ff022877bd3df94bc9360b9c5d-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="19,112.28,656.03,288.41,10.91">Advances in Neural Information Processing Systems (NIPS 1993)</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Cowan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Alspector</surname></persName>
		</editor>
		<meeting><address><addrLine>Denver, Colorado, US</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan-Kaufmann</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,100.52,395.17,10.91;20,112.66,114.06,393.32,10.91;20,112.66,127.61,394.62,10.91;20,112.66,141.16,255.32,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="20,274.69,100.52,233.14,10.91;20,112.66,114.06,25.23,10.91">Siamese Neural Networks for One-Shot Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="20,160.76,114.06,345.22,10.91;20,112.66,127.61,157.38,10.91">Proceedings of the Deep Learning workshop of the International Conference on Machine Learning (ICML 2015)</title>
		<meeting>the Deep Learning workshop of the International Conference on Machine Learning (ICML 2015)<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">06. 2015. 2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,154.71,394.53,10.91;20,112.66,168.26,393.32,10.91;20,112.66,181.81,393.33,10.91;20,112.66,195.36,259.97,10.91" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="20,112.66,168.26,393.32,10.91;20,112.66,181.81,283.96,10.91">Supervised Learning Computer Vision Benchmark for Snake Species Identification from Photographs: Implications for Herpetology and Global Health</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">P</forename><surname>Mohanty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>SalathÃ©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De CastaÃ±eda</surname></persName>
		</author>
		<idno type="DOI">10.3389/frai.2021.582110</idno>
	</analytic>
	<monogr>
		<title level="j" coord="20,408.38,181.81,97.61,10.91;20,112.66,195.36,51.98,10.91">Frontiers in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,208.91,394.62,10.91;20,112.14,222.46,395.69,10.91;20,112.66,236.01,393.41,10.91" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="20,193.34,208.91,290.60,10.91">Impact of Pretrained Networks for Snake Species Classification</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Krishnan</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_194.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="20,112.14,222.46,374.60,10.91">Working Notes of the 11th Conference and Labs of the Evaluation Forum (CLEF 2020)</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">09. 2020. 2020</date>
			<biblScope unit="page" from="22" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,249.56,393.59,10.91;20,112.66,263.11,393.33,10.91;20,112.66,276.66,394.53,10.91;20,112.66,290.20,281.06,10.91" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="20,380.27,249.56,125.97,10.91;20,112.66,263.11,254.98,10.91">Overview of the SnakeCLEF 2020: Automatic Snake Species Identification Challenge</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De CastaÃ±eda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">M</forename><surname>Sharada</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_258.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="20,396.53,263.11,109.46,10.91;20,112.66,276.66,261.93,10.91">Proceedings of the 11th Conference and Labs of the Evaluation Forum (CLEF 2020)</title>
		<meeting>the 11th Conference and Labs of the Evaluation Forum (CLEF 2020)<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">09. 2020. 2020</date>
			<biblScope unit="page" from="22" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,303.75,393.33,10.91;20,112.66,317.30,394.52,10.91;20,112.66,330.85,394.51,10.91;20,112.66,346.84,73.74,7.90" xml:id="b30">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R-Cnn</forename><surname>Mask</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.322</idno>
		<title level="m" coord="20,395.24,303.75,110.75,10.91;20,112.66,317.30,263.58,10.91">Proceedings of the IEEE International Conference on Computer Vision (ICCV 2017)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV 2017)<address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE</publisher>
			<date type="published" when="2017">10. 2017. 2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,357.95,394.53,10.91;20,112.66,371.50,393.33,10.91;20,112.66,385.05,219.41,10.91" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="20,176.85,357.95,325.35,10.91">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,293.89,371.50,212.10,10.91;20,112.66,385.05,125.93,10.91">Proceedings of the 36th International Conference on Machine Learning (ICML</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning (ICML</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,334.80,385.05,172.38,10.91;20,112.66,398.60,323.02,10.91" xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Long</forename><surname>Beach</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/tan19a.html" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
			<pubPlace>California, US</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,412.15,395.16,10.91;20,112.66,425.70,393.33,10.91;20,112.41,439.25,393.57,10.91;20,112.66,452.79,393.33,10.91;20,112.66,466.34,291.88,10.91" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="20,419.65,425.70,86.34,10.91;20,112.41,439.25,266.70,10.91">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=YicbFdNTTy" />
	</analytic>
	<monogr>
		<title level="m" coord="20,403.64,439.25,102.34,10.91;20,112.66,452.79,268.01,10.91">Proceedings of the 9th International Conference on Learning Representations (ICLR</title>
		<meeting>the 9th International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021. 2021</date>
			<biblScope unit="page" from="3" to="07" />
		</imprint>
	</monogr>
	<note>Online Event</note>
</biblStruct>

<biblStruct coords="20,112.66,479.89,393.33,10.91;20,112.28,493.44,159.11,10.91" xml:id="b34">
	<monogr>
		<title level="m" type="main" coord="20,249.28,479.89,69.39,10.91">Python Tutorial</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Van Rossum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">L</forename><surname>Drake</surname><genName>Jr</genName></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<pubPlace>The Netherlands</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Centrum voor Wiskunde en Informatica Amsterdam</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,506.99,394.53,10.91;20,112.28,520.54,393.70,10.91;20,112.66,534.09,394.51,10.91;20,112.66,550.08,104.78,7.90" xml:id="b35">
	<analytic>
		<title level="a" type="main" coord="20,321.59,520.54,184.39,10.91;20,112.66,534.09,43.31,10.91">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
	</analytic>
	<monogr>
		<title level="j" coord="20,164.70,534.09,187.68,10.91">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,561.19,395.17,10.91;20,112.66,574.74,394.53,10.91;20,112.66,588.29,394.67,10.91;20,112.66,601.84,395.17,10.91;20,112.66,615.39,16.43,10.91" xml:id="b36">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Gokula</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Krishnan</surname></persName>
		</author>
		<ptr target="https://medium.com/@Stormblessed/diving-into-deep-learning-part-3-a-deep-learning-practitioners-attempt-to-build-state-of-the-2460292bcfb" />
		<title level="m" coord="20,244.64,561.19,263.19,10.91;20,112.66,574.74,390.52,10.91">Diving into Deep Learning -Part 3 -A Deep Learning Practitioner&apos;s Attempt to Build State of the Art Snake-Species Image Classifier</title>
		<imprint>
			<date type="published" when="2019">2019. 2021-05-24</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,628.93,393.33,10.91;20,112.66,642.48,393.33,10.91;20,112.66,656.03,395.01,10.91;20,112.66,669.58,158.60,10.91" xml:id="b37">
	<analytic>
		<title level="a" type="main" coord="20,310.01,628.93,195.97,10.91;20,112.66,642.48,71.84,10.91">Deformable Part Models are Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298641</idno>
	</analytic>
	<monogr>
		<title level="m" coord="20,206.94,642.48,299.05,10.91;20,112.66,656.03,54.74,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Boston, Massachusetts, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015. 2015</date>
			<biblScope unit="page" from="437" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,86.97,394.53,10.91;21,112.34,100.52,395.49,10.91;21,112.39,114.06,393.80,10.91;21,112.30,127.61,394.88,10.91;21,112.66,141.16,394.03,10.91;21,112.66,154.71,284.50,10.91" xml:id="b38">
	<analytic>
		<title level="a" type="main" coord="21,295.91,100.52,211.92,10.91;21,112.39,114.06,125.77,10.91">Speed/Accuracy Trade-Offs for Modern Convolutional Object Detectors</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<ptr target="https://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_SpeedAccuracy_Trade-Offs_for_CVPR_2017_paper.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="21,270.50,114.06,235.69,10.91;21,112.30,127.61,209.25,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2017)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2017)<address><addrLine>Honolulu, Hawaii, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">07.2017. 2017</date>
			<biblScope unit="page" from="3296" to="3297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,168.26,394.53,10.91;21,112.66,181.81,395.17,10.91;21,112.66,195.36,393.68,10.91;21,112.66,208.91,394.52,10.91;21,112.66,222.46,242.09,10.91" xml:id="b39">
	<analytic>
		<title level="a" type="main" coord="21,112.66,181.81,202.86,10.91">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10602-1_48</idno>
	</analytic>
	<monogr>
		<title level="m" coord="21,167.82,195.36,338.53,10.91;21,112.66,208.91,21.72,10.91">Proceedings of the 13th European Conference on Computer Vision (ECCV 2014)</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</editor>
		<meeting>the 13th European Conference on Computer Vision (ECCV 2014)<address><addrLine>Zurich, Switzerland; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,236.01,393.33,10.91;21,112.66,249.56,394.53,10.91;21,112.66,263.11,393.33,10.91;21,112.66,276.66,393.33,10.91;21,112.33,290.20,394.86,10.91;21,112.66,303.75,226.01,10.91" xml:id="b40">
	<analytic>
		<title level="a" type="main" coord="21,224.68,236.01,281.31,10.91;21,112.66,249.56,100.74,10.91">Optimized Convolutional Neural Network Ensembles for Medical Subfigure Classification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-65813-1_5</idno>
	</analytic>
	<monogr>
		<title level="m" coord="21,245.56,263.11,260.42,10.91;21,112.66,276.66,393.33,10.91;21,112.33,290.20,50.51,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction: Proceedings of the 8th International Conference of the CLEF Association (CLEF 2017)</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Lawless</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Kelly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Mandl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting><address><addrLine>Dublin, Ireland; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017">09. 2017. 2017</date>
			<biblScope unit="page" from="57" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,317.30,394.53,10.91;21,112.48,330.85,293.94,10.91" xml:id="b41">
	<analytic>
		<title level="a" type="main" coord="21,251.82,317.30,250.76,10.91">A Survey on Image Data Augmentation for Deep Learning</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<idno type="DOI">10.1186/s40537-019-0197-0</idno>
	</analytic>
	<monogr>
		<title level="j" coord="21,112.48,330.85,85.59,10.91">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,344.40,395.17,10.91;21,112.66,357.95,392.83,10.91;21,112.66,373.94,44.07,7.90" xml:id="b42">
	<analytic>
		<title level="a" type="main" coord="21,181.44,344.40,209.81,10.91">Lanczos Filtering in One and Two Dimensions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">E</forename><surname>Duchon</surname></persName>
		</author>
		<idno type="DOI">10.1175/1520-0450(1979)018&lt;1016:lfioat&gt;2.0.co;2</idno>
	</analytic>
	<monogr>
		<title level="j" coord="21,402.00,344.40,105.83,10.91;21,112.66,357.95,43.67,10.91">Journal of Applied Meteorology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1016" to="1022" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,385.05,393.33,10.91;21,112.66,398.60,393.33,10.91;21,112.66,412.15,395.01,10.91;21,112.66,425.70,176.40,10.91" xml:id="b43">
	<analytic>
		<title level="a" type="main" coord="21,276.66,385.05,229.33,10.91;21,112.66,398.60,59.36,10.91">Self-Training with Noisy Student Improves ImageNet Classification</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.01070</idno>
	</analytic>
	<monogr>
		<title level="m" coord="21,199.05,398.60,306.93,10.91;21,112.66,412.15,91.23,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020. 2020</date>
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,439.25,393.33,10.91;21,112.66,452.79,393.33,10.91;21,112.66,466.34,393.32,10.91;21,112.66,479.89,57.22,10.91" xml:id="b44">
	<analytic>
		<title level="a" type="main" coord="21,270.61,439.25,149.29,10.91">Searching for Activation Functions</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SkBYYyZRZ" />
	</analytic>
	<monogr>
		<title level="m" coord="21,441.91,439.25,64.08,10.91;21,112.66,452.79,393.33,10.91;21,112.66,466.34,22.02,10.91">Proceedings of the Workshop Track of 6th International Conference on Learning Representations (ICLR 2018)</title>
		<meeting>the Workshop Track of 6th International Conference on Learning Representations (ICLR 2018)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">5.2018. 2018</date>
			<biblScope unit="page" from="4" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,493.44,393.32,10.91;21,112.66,506.99,394.09,10.91;21,112.66,520.54,258.34,10.91" xml:id="b45">
	<analytic>
		<title level="a" type="main" coord="21,234.03,493.44,166.55,10.91">A Method for Stochastic Optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m" coord="21,423.78,493.44,82.20,10.91;21,112.66,506.99,322.77,10.91">Proceedings of the 3rd International Conference for Learning Representations (ICLR 2014)</title>
		<meeting>the 3rd International Conference for Learning Representations (ICLR 2014)<address><addrLine>Banff, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="14" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,534.09,394.53,10.91;21,112.66,547.64,394.53,10.91;21,112.66,561.19,395.17,10.91;21,112.66,574.74,220.58,10.91" xml:id="b46">
	<analytic>
		<title level="a" type="main" coord="21,186.58,547.64,120.22,10.91">Attention is All you Need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,386.47,561.19,121.36,10.91;21,112.66,574.74,151.97,10.91">Advances in Neural Information Processing Systems (NIPS</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,615.39,340.34,10.91" xml:id="b47">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.4414861</idno>
	</analytic>
	<monogr>
		<title level="j" coord="21,176.93,615.39,99.26,10.91">PyTorch Image Models</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,628.93,394.53,10.91;21,112.66,642.48,394.52,10.91;21,112.66,656.03,394.53,10.91;21,112.66,669.58,394.53,10.91;22,112.66,86.97,395.17,10.91;22,112.66,100.52,155.66,10.91" xml:id="b48">
	<analytic>
		<title level="a" type="main" coord="21,368.74,656.03,138.45,10.91;21,112.66,669.58,183.88,10.91">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,292.29,86.97,215.54,10.91;22,112.66,100.52,21.11,10.91">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>AlchÃ©-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Neurips</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,271.06,100.52,236.61,10.91;22,112.66,114.06,394.67,10.91;22,112.66,127.61,178.37,10.91" xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Canada</forename><surname>Vancouver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Curran</forename><surname>Associates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Inc</forename></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,141.16,393.33,10.91;22,112.66,154.71,394.09,10.91;22,112.66,168.26,259.76,10.91" xml:id="b50">
	<analytic>
		<title level="a" type="main" coord="22,192.83,141.16,233.08,10.91">EfficientNetV2: Smaller Models and Faster Training</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2104.00298" />
	</analytic>
	<monogr>
		<title level="m" coord="22,450.78,141.16,55.20,10.91;22,112.66,154.71,323.19,10.91">Proceedings of the 38th International Conference on Machine Learning (ICML 2021)</title>
		<meeting>the 38th International Conference on Machine Learning (ICML 2021)</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
