<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.40,84.74,397.11,15.42;1,89.29,106.66,400.14,15.42">Weakly-Supervised Classification and Detection of Bird Sounds in the Wild. A BirdCLEF 2021 Solution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,82.62,11.96"><forename type="first">Marcos</forename><forename type="middle">V</forename><surname>Conde</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidad de Valladolid</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Equal contribution</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,189.85,134.97,82.12,11.96"><forename type="first">Kumar</forename><surname>Shubham</surname></persName>
							<email>kumar.shubham@alumni.iitd.ac.in</email>
							<affiliation key="aff1">
								<orgName type="institution">Jio Saavn</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,289.91,134.97,87.82,11.96"><forename type="first">Prateek</forename><surname>Agnihotri</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Clairvoyant.ai</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,395.67,134.97,75.23,11.96"><forename type="first">Nitin</forename><forename type="middle">D</forename><surname>Movva</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,148.92,84.28,11.96"><forename type="first">Szilard</forename><surname>Bessenyei</surname></persName>
						</author>
						<title level="a" type="main" coord="1,88.40,84.74,397.11,15.42;1,89.29,106.66,400.14,15.42">Weakly-Supervised Classification and Detection of Bird Sounds in the Wild. A BirdCLEF 2021 Solution</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">1F5209B45ABEA401842D6F289FD9803D</idno>
					<idno type="arXiv">arXiv:1512.03385.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Audio Pattern Recognition</term>
					<term>Audio Classification</term>
					<term>BirdCLEF 2021</term>
					<term>Birdcall identification</term>
					<term>Computer Vision</term>
					<term>Convolutional Neural Networks</term>
					<term>Deep Learning</term>
					<term>Sound Event Detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It is easier to hear birds than see them, however, they still play an essential role in nature and they are excellent indicators of deteriorating environmental quality and pollution. Recent advances in Machine Learning and Convolutional Neural Networks allow us to detect and classify bird sounds, by doing this, we can assist researchers in monitoring the status and trends of bird populations and biodiversity in ecosystems. We propose a sound detection and classification pipeline for analyzing complex soundscape recordings and identify birdcalls in the background. Our pipeline learns from weak labels, classifies finegrained bird vocalizations in the wild, and is robust against background sounds (e.g., airplanes, rain, etc). Our solution achieved 10th place of 816 teams at the BirdCLEF 2021 Challenge hosted on Kaggle. Code and models will be open-sourced at https://github.com/kumar-shubham-ml/kaggle-birdclef-2021.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The BirdCLEF 2021 Challenge <ref type="bibr" coords="1,227.56,454.30,11.48,10.91" target="#b0">[1,</ref><ref type="bibr" coords="1,241.75,454.30,9.03,10.91" target="#b1">2]</ref> proposes to identify bird calls in soundscape recordings. The challenge was hosted on Kaggle from April 1, 2021 to June 1, 2021 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset.</head><p>The training set consists of short audio recordings of 397 bird species generously uploaded by users of xenocanto.org. These audio files have been downsampled to 32 kHz and converted to the ùëúùëîùëî format. In Section 3.1 we explain how we preprocess this short audios and generate curated audios and their corresponding Mel Spectrogram. The test set contains approximately 80 soundscape recordings in ùëúùëîùëî format (over 10 minutes of recordings), note that participants cannot access these audios. Additionally, recordings have associated metadata as the location (longitude, latitude), author, date, etc. Some of this features as the location can be especially useful for identifying migratory birds.</p><p>Problem. Given a long audio in ùëúùëîùëî format, participants have to predict if there is a bird call in each 5-seconds segment of the given soundscape, and identify which of the 397 birds is in such segment, thus, once the call is detected in a segment, the task can be considered as fine-grained multi-label classification. Models infer on the test set with 3 hours run-time limit, to ensure the efficiency of the solutions.</p><p>Evaluation. The performance is measured using the "micro averaged F1 score" and reported on a Leaderboard (LB). Moreover, this leaderboard is divided into: "Public" which provides the score on 28 test recordings (35%), and Private, which provides the score on 52 test recordings (65%). During the competition, the participants only get feedback of their performance from the public leaderboard, this is done to prevent overfitting.</p><p>Train recordings were uploaded by the users of xenocanto.com from sites across the globe; however, test recordings were from four places only: We define some terms related with this challenge that we will use in this work:</p><p>‚Ä¢ Leaderboard denoted as LB (including its two variants, public and private)</p><p>‚Ä¢ Cross-Validation denoted as CV.</p><p>‚Ä¢ the so-called "score" or "metric" refers to the official challenge metric: "the row-wise micro averaged F1 score. ". ‚Ä¢ We refer to the "Cornell Birdcall Identification -Kaggle 2020" as the "previous competition", "last year challenge". ‚Ä¢ We define "nocall" as the class corresponding to the events in an audio where birdcalls are not detected. Other authors might also refer to this term as "nosound" or "background". This concept might be mentioned various times throughout the manuscript in different notations ("nocall", nocall, no_call, etc). ‚Ä¢ Train soundscapes (or "train soundscapes") are 20 audio files that are quite comparable to the test set. They are all roughly ten minutes long and in the ùëúùëîùëî format. We draw a red bounding box on the events in the audio where a birdcall is detected, the other parts of the audio might contain background sounds or noise, such parts where we do not detect a birdcall are also called "nocall" events. We detect the bircall in the audio, and also identify the corresponding bird, in this case a "Blue Jay" (Cyanocitta cristata).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Previous years BirdCLEF challenges proposed different problems related with large-scale bird recognition in soundscapes or complex acoustic environments <ref type="bibr" coords="3,380.16,394.28,11.48,10.91" target="#b0">[1,</ref><ref type="bibr" coords="3,395.19,394.28,7.52,10.91" target="#b2">3,</ref><ref type="bibr" coords="3,406.26,394.28,7.65,10.91" target="#b3">4]</ref>. Sprengel et.al. <ref type="bibr" coords="3,492.99,394.28,13.00,10.91" target="#b4">[5]</ref> and Lasseck <ref type="bibr" coords="3,146.40,407.83,11.45,10.91" target="#b5">[6,</ref><ref type="bibr" coords="3,160.59,407.83,9.01,10.91" target="#b6">7]</ref> introduced deep learning techniques for the "Bird species identification in soundscapes" problem. State-of-the-art solutions are based on Deep Convolutional Neural Networks (CNNs) <ref type="bibr" coords="3,173.59,434.93,11.49,10.91" target="#b7">[8,</ref><ref type="bibr" coords="3,188.29,434.93,7.52,10.91" target="#b8">9,</ref><ref type="bibr" coords="3,199.02,434.93,12.42,10.91" target="#b9">10]</ref>, usually, deep CNNs with attention mechanisms are selected as backbone in these experiments <ref type="bibr" coords="3,230.08,448.48,16.45,10.91" target="#b10">[11,</ref><ref type="bibr" coords="3,249.27,448.48,12.55,10.91" target="#b11">12,</ref><ref type="bibr" coords="3,264.55,448.48,12.55,10.91" target="#b12">13,</ref><ref type="bibr" coords="3,279.84,448.48,12.55,10.91" target="#b13">14,</ref><ref type="bibr" coords="3,295.13,448.48,12.34,10.91" target="#b14">15]</ref>. Pretrained audio neural networks (PANNs) <ref type="bibr" coords="3,89.29,462.03,17.91,10.91" target="#b13">[14]</ref> provide a multi-task state-of-the-art baseline for audio related tasks, in previous competitions these networks proved their generalization capability. Other approaches are focused on Sound Event Detection (SED) <ref type="bibr" coords="3,221.04,489.13,16.35,10.91" target="#b15">[16,</ref><ref type="bibr" coords="3,240.11,489.13,12.51,10.91" target="#b16">17,</ref><ref type="bibr" coords="3,255.34,489.13,12.51,10.91" target="#b13">14,</ref><ref type="bibr" coords="3,270.57,489.13,12.51,10.91" target="#b17">18,</ref><ref type="bibr" coords="3,285.80,489.13,12.26,10.91" target="#b17">18]</ref>, these approaches usually employ 2D CNNs to extract useful features from the input audio signal (log-melspectrogram), these features still contain information about frequency and time, then recurrent neural networks (RNNs) are used to model longer temporal context from the extracted features or use directly the feature map to predict, since it preserves time segment information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Solution</head><p>In this Section we explain the main components of our solution to the BirdCLEF 2021 Birdcall Identification Challenge. We base our solution on diverse and robust models trained on a complete audio dataset using custom augmentations, and on a postprocess algorithm that improves the predicted probabilities of bird appearances by using additional features as the site (longitude, latitude), rarity of the bird, appearance of other birds in the audio, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset Preprocessing</head><p>We converted all the raw audio data to Mel Spectrograms using the ùëôùëñùëèùëüùëúùë†ùëé library with each having a length of 7 seconds and having some overlap<ref type="foot" coords="4,347.49,119.33,3.71,7.97" target="#foot_0">2</ref> , we use this length instead of 5s to ensure that the birdcalls are present in the clip. The Spectrograms were generated using the following parameters: sample rate 32.000, 128 number of mels, minimum frequency 0 Hz, maximum frequency 16000 Hz, length of fast Fourier transform window (n-fft) 3200, and number of samples between successive frames (hop-length) set to 80. We use the Cornell Birdcall Identification 2020 Challenge dataset 3 as additional data, this dataset has 183 birds in common and allowed us to add 1300 extra audio files. After some visual inspections of the Mel Spectrograms, we determine a threshold such that the spectogram is considered to have weak a signal or no signal, attending to its mean and maximum values. All the spectrograms with no signal or very weak signals are removed and treated as noise. Once the above preprocessing steps are completed, we split the training data into 5 different stratified folds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Augmentations</head><p>We use 6 different types of augmentations in order to improve the robustness and generalization capability of our models. In Figure <ref type="figure" coords="4,252.71,319.85,5.17,10.91" target="#fig_2">3</ref> we show the effect of the proposed augmentations in the same order we apply them: Mixing of images, Random Power, White noise, Pink Noise, Bandpass noise, Lower the upper frequencies. First, 2 or 3 different training images are overlapped on each other with a random probability of mixing (default is 0.7). Once this is completed random power is applied on the mixed image to bring all the images to a certain contrast and brightness level. Next we add augmentations in the following order: white noise, pink noise, bandpass noise, reducing upper level frequencies, we found experimentally that this is the optimal order. All the augmentations mentioned above are added with a probability between 0.4 and 0.7 to ensure the diversity of the data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Models</head><p>In Table <ref type="table" coords="5,127.01,107.54,4.97,10.91" target="#tab_0">1</ref> we show the model architectures used in our experiments. All the models had similar performance on out-of-fold validation using 5 stratified folds. Single models perform reasonably good, but combining them into an ensemble provided best performance as we explain in Sections 3.5 and 4. In our experiments we found that bigger architectures did not necessarily provide better results. Hence, a lot of experimentation was done with smaller models such as ResNeSt-50 <ref type="bibr" coords="5,89.29,175.28,17.88,10.91" target="#b10">[11]</ref> and EfficientNet-B0 <ref type="bibr" coords="5,199.51,175.28,16.22,10.91" target="#b11">[12]</ref>. In addition to the proposed models, we use top models from the Cornell Birdcall Identification 2020 Challenge <ref type="foot" coords="5,298.25,187.08,3.71,7.97" target="#foot_1">4</ref> , in Section 4 we explain how we incorporate the following models into our ensemble:</p><p>1. The 1st place solution there are 14 models in total, all of which are PANN DenseNet-121 architecture with an added attention layer. The models were trained with 264 classes of bird data and augmentations such as SpecAugmentation, gaussian noise, gain (volume adjustment), along with mixup for some models were used to increase model's robustness.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Details</head><p>We use a GPU RTX-2070 with 8 GB VRAM for training our models, training time for each model on this device is reported on Table <ref type="table" coords="5,240.89,520.91,3.66,10.91" target="#tab_0">1</ref>. In all the experiments we train for 60 epochs, we use batch size 64 and Adam Optimizer <ref type="bibr" coords="5,215.40,534.46,16.10,10.91" target="#b18">[19]</ref>. We use a Binary Cross Entropy loss function implemented as in PyTorch with Label Smoothing <ref type="bibr" coords="5,238.20,548.01,16.08,10.91" target="#b19">[20]</ref>. Additionally we use a Learning Rate Scheduler based on Cosine Annealing with base LR of 0.001 <ref type="bibr" coords="5,273.44,561.56,16.41,10.91" target="#b20">[21]</ref>. During training we track the loss function, F1 score, Precision, Recall, Label ranking average precision score for both training and validation data. See Figure <ref type="figure" coords="5,162.73,588.66,5.07,10.91" target="#fig_5">5</ref> as an example of our training metrics monitoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Inference and Postprocessing</head><p>We use the provided "train soundscapes" audio samples as validation set. These audios were much noisier than the curated ones used for training (see Section 3.1) and closely resemble "test  soundscapes". The distribution of birds in these soundscapes was also different from the training short audios. However, there were only 20 soundscape clips, which covered only 48 of 397 bird classes and 2 of 4 possible sites, making it too challenging to train acoustic models on these clips. The training short audios were only labelled at clip level, but the long audio predictions were generated at frame level (frame of 5 seconds). The noisy labels led to a significant gap between the performance of our models on short audios (reported at Table <ref type="table" coords="6,414.51,538.93,4.08,10.91" target="#tab_0">1</ref>) and this validation set. For these reasons, the "train soundscapes" clips were used only for validation purposes and to achieve better generalization.</p><p>A series of post-processing strategies were employed to bridge the gap between performance on short, cleaner audio and soundscapes. Our post-processing improved the cross-validation (CV) and Leaderboard F1 score (LB) by 0.008-0.01.</p><p>Initially, we infer all 5-second clips at a stride of 1 second. Our final submission consists of an ensemble of 13 different models explained at Section 3.3. The ensemble optimized weights were calculted based on the validation set. A second-stage model, Support Vector Classifier, was trained with a leave-one-clip-out validation strategy on the 20 train soundscapes. This model computes calibrated confidence based on some frame-based, clip-based, and distance-based features generated from probabilities per inference step, in addition to latitude and longitude information for the sites. During the training of the second-stage model, bird information was masked to help the model generalize well on birds absent in train soundscapes. Finally, we further improve our performance by using a series of False-Positives and False-Negatives reduction techniques and the use of two different thresholds for bird call or "nocall" identification and bird categorization. We reduced the false negatives by increasing the confidence of most frequent birds from each site by 0.1. This strategy worked well both on CV and public/private LB. Therefore, there were three types of predictions, (a) only birds, (b) only nocall (no birds), and (c) both nocall and birds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1.">Second-Stage Model</head><p>The CNN model we trained had some limitations. The short train audio was labelled only at the clip level. On analysing train soundscapes, we found that if a bird is found anywhere in the clip, it increases the chances of finding the same bird at other places in the clip. Furthermore, the chances of finding the birds in immediate neighbour frames would also be high. This phenomenon encouraged us to train a second stage model on train soundscapes which calibrates the confidence using some frame-based and clip-based features. Some of the challenges in training the second stage model involved the limited number of birds and sites in train soundscapes, which could hurt the model's generalisation capability. To solve this, we converted the multi-label problem into a binary classification problem and masked the information about birds for this second stage model. We started with training a simple logistic regression model where each unique tuple of (clip, 5-second frame, bird) constitutes a single training sample. No information about bird class was passed in any way directly to the model. This post-processing alone gave a 0.005-0.007 boost on CV and public/private LB. We saw further improvement (+0.002) by adding location-based features and switching from logistic regression to support vector machines. Only four features were used for training the second-stage model. For example, let us denote the probability generated for a 5-second frame ending at ùëò seconds for any bird B in the clip by ùëÉ ùëò . Let the length of the clip be ùëõ seconds (ùëõ = 600 for all soundscapes). The calculation details of these features for the frame ending at ùëò seconds for the bird B are explained below:</p><p>1. Frame-based features: Rolling Mean 3 (ùëÖùëÄ 3 ) and Rolling Mean 9 (ùëÖùëÄ 9 )</p><formula xml:id="formula_0" coords="7,268.77,543.57,84.51,75.51">ùëÖùëÄ 3 = 1 3 ùëò+1 ‚àëÔ∏Å ùëñ=ùëò-1 ùëÉ ùëñ ùëÖùëÄ 9 = 1 9 ùëò+4 ‚àëÔ∏Å ùëñ=ùëò-4</formula><p>ùëÉ ùëñ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Clip-based features:</head><p>ùëÄ ùëéùë•ùëñùëöùë¢ùëö ùê∂ùëúùëõùëì ùëñùëëùëíùëõùëêùëí = ùëöùëéùë• (ùëÉ 5 , . . . , ùëÉ ùëõ )</p><p>3. Distance-based features (minimum Haversine distance) explained in Section 3.5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2.">Minimum Haversine Distance</head><p>The haversine distance <ref type="bibr" coords="8,196.95,107.54,18.07,10.91" target="#b23">[24]</ref> is an excellent approximation for the angular distance between two points expressed as latitudes and longitudes on earth. The minimum haversine distance is expressed as the distance between a site and a bird class. Let us suppose that a bird class has 400 samples in the train short audios. First, the haversine distance is calculated between the position of each of those birds and the site's location. The minimum of the set of these 400 distances is called minimum haversine distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.3.">False Positives Reduction</head><p>All the (bird, site) pairs satisfying at least one of the following conditions were discarded:</p><p>1. Minimum Haversine Distance between site and bird is greater than 100. Analysing train soundscapes, we found that only 3 (birds, site) pairs found in train soundscapes have a minimum haversine distance greater than 60. So, all the (birds,site) pairs with minimum haversine distance&gt;100 were rejected. 2. Probabilities generated directly from the ensemble for that frame were less than 0.01. This post processing helped us get small boost in CV and private LB and helped us reduced training data for Support Vector Classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Remove birds belonging to one of the following classes -(Great Horned Owl, Plumbeous</head><p>Pigeon). As we analysed the train soundscapes, we found that our models have a very high False Positives Rate for these species. Most of the time, when the model was predicting these classes, the actual target was nocall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.4.">Confidence Thresholds</head><p>Two sets of thresholds were used for calibrating confidence. The nocall confidence was determined as 1 -ùëöùëéùë•(calibrated confidence for all birds for that 5-second frame). The first threshold was applied on nocall confidence (no birds detected). All 5-second frames having nocall confidence above this threshold contained nocall as one of the predictions. The second threshold was applied on calibrated confidence for each bird.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation of Methods</head><p>We trained models using short train audios as explained in Section 3.1. We use long soundscape audios for training probability calibration (PC) model, for optimizing false-negative reduction (FNR) and false-positive reduction (FPR) methods, tuning thresholds, and for computing the validation scores (see Section 3.5). For model selection, we kept track of both call and nocall F1 scores to make sure that models are not heavily affected distribution of nocall-call samples. Note that Train soundscapes had around 63% nocall samples, and we estimate that the hidden test fraction corresponding to the public LB has 54% nocall samples. We rely on two different validation scores: the "High nocall Validation Score" denoted as HNVS and the "Low nocall Validation Score" denoted as LNVS. In Equation 1 we show the definition of both metrics: HNVS (CV@0.63) = 0.63 √ó F1-micro nocall + 0.37 √ó F1-micro call LNVS (CV@0.54) = 0.54 √ó F1-micro nocall + 0.46 √ó F1-micro call <ref type="bibr" coords="9,494.41,105.73,11.57,10.91" target="#b0">(1)</ref> In order to make sure that our models generalizes to unseen data (e.g. private LB), we separately calculated row-wise micro averaged F1-score for samples having bird calls and samples having no bird call. Table <ref type="table" coords="9,239.38,159.43,4.97,10.91">2</ref> shows the different metrics that we considered for selecting our models and experiments and the ablation study of the different postprocess steps. For further validation, we also tested our models on the Cornell Birdcall Identification 2020 Challenge leaderboard <ref type="foot" coords="9,145.79,198.32,3.71,7.97" target="#foot_2">5</ref> . Last year competition data had 3 different sites,after some analysis, we found that site2 was close to SSW site and site1 was close to SNE site. Also we estimated that this test data has around 57% nocall samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results and Comparison</head><p>Table <ref type="table" coords="9,117.03,276.64,5.17,10.91">2</ref> summarizes our experiments. We bagged 13 CNN-based models (Section 3.3) with CV@0.63 (HNVS) varying from 0.68 to 0.71. These 13 models were different in terms of augmentation strategy and architecture. Adding augmentations improved the true positive rate of these models and reduced the difference of scores between the predictions on short audios and train soundscapes(relatively noisier), thus making models robust against the anthropogenic noise. The bagging of these 13 models gave 0.74 CV@0.54 for COR site but was not that effective on SSW sites. For SNE &amp; SSW sites, we fine-tuned the last year competition first and second place models (only for birds having minimum Haversine distance lesser than 100 for these two sites). Using these models improved the CV@0.54 for SNE &amp; SSW sites from 0.69 to 0.75.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Row-wise micro averaged F1-score results of models on Public-Private LB, and "train soundscape" validation. For local validation, row-wise micro averaged F1-score was calculated on samples with call and no_call separately, and the metric CV@0.54 (LNVS) was also calculated. As shown in Table <ref type="table" coords="9,188.54,568.64,3.81,10.91">2</ref>, the time series-based probability calibration (PC) model provided a good improvement in CV and LB. Using the clip-level and the neighboring frames information, the probability calibration model improved CV on samples having bird call by +0.07, and raised Public LB to 0.774 and Private LB to 0.661. Then the bird-to-site mapping (site-info) using minimum Haversine distance helped in two ways: (i) reducing false-negatives by reducing the call identification thresholds of the most frequent birds, (ii) reducing false positives by removing the birds in the predictions which are not found at a particular site.  Further reducing false negatives via removing birds from the predictions which are most commonly confused with "nocall" helped in achieving CV@0.54=0.7836 and Private LB=0.6780. Additionally, we compare our current solution against previous state-of-the-art methods for this challenge by submitting our solution to Cornell Birdcall Identification 2020 Challenge. Our solution was able to give significantly better results than previous winning solutions of the Cornell Birdcall Identification 2020 Challenge (see Table <ref type="table" coords="10,336.78,474.44,3.50,10.91" target="#tab_2">3</ref>). There is a gain of +0.035 on Public LB and a gain of +0.018 on Private LB as compared to 2020 first place solution. We understand that our model is an extension of previous state of the art, and can generalize to detect all variety of birds from unknown sites and background sounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future work</head><p>We aim to help researchers monitoring birds and automatically intuit factors about an area's quality of life, levels of pollution, and the effectiveness of restoration efforts. We present a sound detection and classification pipeline for analyzing soundscape recordings that learns from weak labels, classifies fine-grained bird vocalizations and is robust against anthropogenic or natural noisy sounds (e.g., rain, cars, etc). Our solution achieved 10th place of 816 teams at the BirdCLEF 2021 Challenge. We would like to improve efficiency and usability, and thus, use this pipeline online or on smartphones. To achieve this, we are exploring Knowledge Distillation to reduce notably the hardware requirements and inference time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,466.90,340.75,8.93;2,91.28,359.68,104.16,78.12"><head>1 .Figure 1 :</head><label>11</label><figDesc>Figure 1: Photographs of the four different sites from which audios were recorded.</figDesc><graphic coords="2,91.28,359.68,104.16,78.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,89.29,270.78,416.69,9.15;3,89.29,283.00,416.70,8.87;3,89.29,294.96,416.70,8.87;3,88.93,306.91,417.05,8.87;3,89.29,318.87,318.83,8.87;3,161.21,84.19,260.09,168.38"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Problem example. The audio in ùëúùëîùëî format can be visualized as a waveform (top) or a Mel Spectrogram (bottom).We draw a red bounding box on the events in the audio where a birdcall is detected, the other parts of the audio might contain background sounds or noise, such parts where we do not detect a birdcall are also called "nocall" events. We detect the bircall in the audio, and also identify the corresponding bird, in this case a "Blue Jay" (Cyanocitta cristata).</figDesc><graphic coords="3,161.21,84.19,260.09,168.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,89.29,622.10,318.82,8.93;4,89.29,450.96,416.70,146.62"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualization of our augmentation pipeline explained in Section 3.2.</figDesc><graphic coords="4,89.29,450.96,416.70,146.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,103.64,274.80,402.34,10.91;5,116.56,288.35,389.71,10.91;5,116.56,301.90,259.63,10.91"><head>2 .</head><label>2</label><figDesc>The 2nd place solution. Two different models: ResNet-50 and EfficientNet-B0. Both these architectures were trained with different settings on 264 birds and were trained directly on mel spectrograms instead of training on the audio files.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,89.29,420.19,416.69,8.93;5,89.29,432.19,417.05,8.87;5,89.29,444.15,168.87,8.87;5,89.29,329.39,416.69,71.42"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example of multilabel classification model pipeline. During training, the generated Mel Spectrogram (see Section 3.1) is augmented as explained in Section 3.2. In this diagram we do not show additional postprocess of the predictions.</figDesc><graphic coords="5,89.29,329.39,416.69,71.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,89.29,245.81,416.69,8.93;6,89.29,257.81,418.23,8.87;6,89.29,84.19,416.69,143.07"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Loss and validation metrics evolution during training of ResNeSt-50 model. Note that the training loss is higher than the validation loss because augmentations were only applied during training.</figDesc><graphic coords="6,89.29,84.19,416.69,143.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="10,89.29,193.27,416.70,8.93;10,89.29,205.28,418.36,8.87;10,89.29,217.24,84.34,8.87;10,89.29,84.19,416.69,89.70"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Overview of our solution pipeline. We show an ensemble of various models, our SVC model for probability calibration (PC) and the proposed probability filters for False-Positives and False-Negatives reduction.</figDesc><graphic coords="10,89.29,84.19,416.69,89.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,88.98,290.91,418.54,153.57"><head>Table 1</head><label>1</label><figDesc>Ablation</figDesc><table coords="6,99.34,358.13,394.11,86.35"><row><cell>Architecture</cell><cell cols="4">OOF F1 Score TS F1 Score No. Parameters Time (min) √ó Epoch</cell></row><row><cell>ResNeSt-50 [11]</cell><cell>0.755</cell><cell>0.706</cell><cell>26,247.693</cell><cell>20</cell></row><row><cell>ResNeSt-101 [11]</cell><cell>0.748</cell><cell>0.705</cell><cell>47,039.469</cell><cell>34</cell></row><row><cell>ResNeXt-50_32x4d [13]</cell><cell>0.714</cell><cell>0.63</cell><cell>23,793.357</cell><cell>22</cell></row><row><cell>SeResNet-50 [22]</cell><cell>0.725</cell><cell>0.674</cell><cell>26,852.477</cell><cell>25</cell></row><row><cell>DenseNet-121 [23]</cell><cell>0.718</cell><cell>0.66</cell><cell>7,360.781</cell><cell>17</cell></row><row><cell>EfficientNet-B0 [12]</cell><cell>0.722</cell><cell>0.691</cell><cell>4,516.105</cell><cell>15</cell></row></table><note coords="6,127.00,302.92,378.99,8.87;6,89.29,314.87,416.70,8.87;6,88.99,326.83,418.53,8.87;6,89.29,338.78,373.47,8.87"><p>study of our models trained from scratch on the BirdCLEF 2021 dataset. The training time depends on the number of augmentations, architecture and batch size. The corresponding Out-Of-Fold (5-fold) F1 score for each model and the validation score using Train Soundscapes (TS) are provided. Models with ResNeSt as backbone have better performance than DenseNet or EfficientNet.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,88.99,250.34,417.17,129.65"><head>Table 3</head><label>3</label><figDesc>Comparison of our solution ot the BirdCLEF 2021 Challenge (see Section 3) and the winning solutions of the Cornell Birdcall Identification 2020 competition on its Leaderboard (public and private). Our solution generalizes to different sites and extends previous approaches improving performance.</figDesc><table coords="10,168.99,305.86,254.80,74.12"><row><cell>Model</cell><cell cols="2">Cornell Birdcall Identification 2020</cell></row><row><cell></cell><cell>Public LB</cell><cell>Private LB</cell></row><row><cell>Ours (BirdCLEF 2021)</cell><cell>0.659</cell><cell>0.699</cell></row><row><cell>Birdcall 2020 -1st place</cell><cell>0.624</cell><cell>0.681</cell></row><row><cell>Birdcall 2020 -2nd place</cell><cell>0.628</cell><cell>0.677</cell></row><row><cell>Birdcall 2020 -3rd place</cell><cell>0.626</cell><cell>0.675</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="4,108.93,660.08,202.99,8.97"><p>https://www.kaggle.com/kneroma/kkiller-birdclef-2021</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="5,108.93,671.04,176.99,8.97"><p>https://www.kaggle.com/c/birdsong-recognition</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="9,108.93,671.04,223.16,8.97"><p>https://www.kaggle.com/c/birdsong-recognition/leaderboard</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank <rs type="person">Kaggle</rs> and <rs type="person">Dr. Stefan Kahl</rs> for hosting the BirdCLEF 2021 Challenge. We also want to thank participants of the <rs type="institution">Cornell Birdcall Identification 2020 Challenge</rs> and this challenge for sharing insights, datasets, their solutions and open-sourced code, especially: <rs type="person">Ryan Wong</rs>, <rs type="person">Kramarenko Vladislav</rs>, <rs type="person">Hidehisa Arai</rs>, <rs type="person">Kossi Neroma</rs>, <rs type="person">Jean-Fran√ßois Puget</rs> (<rs type="affiliation">CPMP</rs>).</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="11,112.66,210.55,394.53,10.91;11,112.66,224.10,393.33,10.91;11,112.66,237.65,328.66,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,112.66,224.10,98.33,10.91;11,238.81,224.10,206.97,10.91">Bird call identification in soundscape recordings</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqu√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,467.81,224.10,38.18,10.91;11,112.66,237.65,297.96,10.91">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note>Overview of BirdCLEF</note>
</biblStruct>

<biblStruct coords="11,112.66,251.20,393.33,10.91;11,112.66,264.75,394.52,10.91;11,112.66,278.30,393.33,10.91;11,112.66,291.85,393.32,10.91;11,112.66,305.40,371.86,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,247.09,278.30,258.90,10.91;11,112.66,291.85,316.47,10.91">Overview of LifeCLEF 2021: a System-oriented Evaluation of Automated Species Identification and Species Distribution Prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Casta√±eda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqu√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dorso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,452.02,291.85,53.96,10.91;11,112.66,305.40,342.12,10.91">Proceedings of the Twelfth International Conference of the CLEF Association (CLEF 2021)</title>
		<meeting>the Twelfth International Conference of the CLEF Association (CLEF 2021)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,318.95,394.53,10.91;11,112.66,332.50,394.52,10.91;11,112.66,346.05,22.69,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,112.66,332.50,99.64,10.91">Overview of BirdCLEF</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Clapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hopping</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqu√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,240.77,332.50,261.46,10.91">Bird Sound Recognition in Complex Acoustic Environments</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,359.59,393.33,10.91;11,112.66,373.14,345.11,10.91" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="11,449.60,359.59,56.38,10.91;11,112.66,373.14,269.90,10.91">Overview of BirdCLEF 2019: Large-Scale Bird Recognition in Soundscapes</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F.-R</forename><surname>St√∂ter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Planqu√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,386.69,393.33,10.91;11,112.66,400.24,184.41,10.91" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="11,311.76,386.69,194.23,10.91;11,112.66,400.24,109.52,10.91">Audio based bird species identification using deep learning techniques</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sprengel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kilcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,413.79,313.10,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,167.73,413.79,182.90,10.91">Bird species identification in soundscapes</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,374.03,413.79,21.05,10.91">CLEF</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,427.34,395.17,10.91;11,112.26,440.89,100.14,10.91" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="11,168.33,427.34,339.50,10.91;11,112.26,440.89,24.88,10.91">Audio-based bird species identification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,454.44,394.52,10.91;11,112.66,467.99,22.69,10.91" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="11,164.21,454.44,293.26,10.91">Bird identification from timestamped, geotagged audio recordings</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schl√ºter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,481.54,394.53,10.91;11,112.66,495.09,66.36,10.91" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="11,217.13,481.54,265.34,10.91">Xception based method for bird sound recognition of birdclef</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,508.64,393.33,10.91;11,112.66,522.18,157.65,10.91" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="11,344.79,508.64,161.20,10.91;11,112.66,522.18,82.93,10.91">Bird species recognition via neural architecture search</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>M√ºhling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Korfhage</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Freisleben</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,535.73,394.53,10.91;11,112.66,549.28,349.74,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m" coord="11,185.89,549.28,146.12,10.91">Resnest: Split-attention networks</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,562.83,394.53,10.91;11,112.66,576.38,122.77,10.91" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="11,186.32,562.83,316.07,10.91">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,589.93,393.33,10.91;11,112.66,603.48,200.83,10.91" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="11,301.11,589.93,204.87,10.91;11,112.66,603.48,70.43,10.91">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05431</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,617.03,393.33,10.91;11,112.66,630.58,360.52,10.91" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="11,376.83,617.03,129.16,10.91;11,112.66,630.58,230.56,10.91">Panns: Large-scale pretrained audio neural networks for audio pattern recognition</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10211</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,644.13,395.01,10.91" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="11,266.99,644.13,208.54,10.91">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,86.97,393.33,10.91;12,112.66,100.52,397.48,10.91;12,112.66,116.51,43.94,7.90" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Drossos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mimilakis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gharib</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN48605.2020.9207532</idno>
		<title level="m" coord="12,341.36,86.97,164.63,10.91;12,112.66,100.52,158.18,10.91">Sound event detection with depthwise separable and dilated convolutions</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,127.61,393.33,10.91;12,112.66,141.16,322.12,10.91" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="12,406.83,127.61,99.15,10.91;12,112.66,141.16,192.66,10.91">Learning sound event classifiers from web audio with noisy labels</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Font</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Favory</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.01189</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,154.71,393.33,10.91;12,112.66,168.26,397.48,10.91;12,112.66,184.25,73.62,7.90" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,375.77,154.71,130.21,10.91;12,112.66,168.26,136.44,10.91">Robust sound event detection in bioacoustic sensor networks</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lostanlen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Farnsworth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kelling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bello</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0214168</idno>
	</analytic>
	<monogr>
		<title level="j" coord="12,257.94,168.26,49.15,10.91">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">214168</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,195.36,122.34,10.91;12,272.22,195.36,235.44,10.91" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m" coord="12,227.57,195.36,7.43,10.91;12,272.22,195.36,113.25,10.91">A for stochastic optimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,208.91,395.01,10.91;12,112.66,224.90,97.35,7.90" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="12,301.03,208.91,170.37,10.91">When does label smoothing help?</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02629</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,236.01,395.01,10.91;12,112.66,252.00,97.35,7.90" xml:id="b20">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m" coord="12,227.16,236.01,248.68,10.91">Sgdr: Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,263.11,395.01,10.91;12,112.66,279.10,97.35,7.90" xml:id="b21">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<title level="m" coord="12,323.29,263.11,150.72,10.91">Squeeze-and-excitation networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,290.20,393.32,10.91;12,112.66,303.75,169.59,10.91" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06993</idno>
		<title level="m" coord="12,357.97,290.20,148.01,10.91;12,112.66,303.75,39.20,10.91">Densely connected convolutional networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,317.30,393.33,10.91;12,112.66,330.85,393.33,10.91;12,112.33,344.40,249.39,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="12,222.04,317.30,283.95,10.91;12,112.66,330.85,94.83,10.91">Shortest path calculation: A comparative study for location-based recommender system</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mahmoud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Akkari</surname></persName>
		</author>
		<idno type="DOI">10.1109/WSCAR.2016.16</idno>
	</analytic>
	<monogr>
		<title level="m" coord="12,254.85,330.85,251.13,10.91;12,112.33,344.40,39.90,10.91">World Symposium on Computer Applications Research (WSCAR)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
