<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,406.61,15.42;1,89.29,106.66,220.27,15.42">UAIC-AI at SnakeCLEF 2021: Impact of convolutions in snake species recognition</title>
				<funder ref="#_ZPZPsHZ #_4J5aFTZ #_SeA5D6Z">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,105.21,11.96"><forename type="first">Lucia</forename><forename type="middle">Georgiana</forename><surname>Coca</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">Cuza&quot; University</orgName>
								<address>
									<settlement>Iasi</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,205.99,134.97,106.18,11.96"><forename type="first">Alexia</forename><forename type="middle">Theodora</forename><surname>Popa</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">Cuza&quot; University</orgName>
								<address>
									<settlement>Iasi</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,323.60,134.97,130.92,11.96"><forename type="first">Razvan</forename><forename type="middle">Contantin</forename><surname>Croitoru</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">Cuza&quot; University</orgName>
								<address>
									<settlement>Iasi</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,148.92,124.67,11.96"><forename type="first">Luciana</forename><surname>Paraschiva Bejan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">Cuza&quot; University</orgName>
								<address>
									<settlement>Iasi</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,241.42,148.92,65.04,11.96"><forename type="first">Adrian</forename><surname>Iftene</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">Cuza&quot; University</orgName>
								<address>
									<settlement>Iasi</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,93.29,174.91,53.75,7.99"><forename type="first">Alexandru</forename><surname>Ioan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">Cuza&quot; University</orgName>
								<address>
									<settlement>Iasi</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,406.61,15.42;1,89.29,106.66,220.27,15.42">UAIC-AI at SnakeCLEF 2021: Impact of convolutions in snake species recognition</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">9C23BCE4F07A5F4799B45A0652FBBDEC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>LifeCLEF</term>
					<term>SnakeCLEF</term>
					<term>Snake Identification</term>
					<term>snake bite</term>
					<term>health</term>
					<term>CNN</term>
					<term>Machine Learning</term>
					<term>Snakes</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Snake identification is crucial in quickly and effectively treating snake bites. With over 2.7 million snake envenomings happening yearly, medical personal are in desperate need of tools that will ease their work as well as save patient lives faster. SnakeCLEF 2021 challenge, which is part of the LifeCLEF laboratory, has exactly the aforementioned goal. This paper presents our team participation at SnakeCLEF 2021. We developed 3 models based on CNN: GoogLeNet, VGG16 and ResNet-18 and ranked 5 th with an F1-Country score of 0.785 using ResNet-18.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human expansion and the amplification of animal habitat destruction is resulting in more and more contact with wildlife in urban areas. Snakes are one species affected by this phenomena, if previously they lived in forests, swamps or even deserts, they are now forced to crawl in urban areas in search of food and shelter. The negative impact is felt not only in animals but also humans, more and more are we in contact with venomous snakes which leads to deadly scenarios that put us on a knife-edge. Being able to quickly identify the species of snakes that came in contact with people will not only give medical personnel precious time but also will give us a better understanding of certain species of snakes and their mobility in this new human habitat.</p><p>Annually, according to WHO <ref type="bibr" coords="1,230.43,493.55,11.28,10.91" target="#b0">[1]</ref>, over 5.4 million people are bitten by snakes (2.7 million are envenomings), of which around 81,000 to 138,000 are deaths not taking into account the many that become disabled or paralyzed. It is hence critical that medical personnel quickly identify the species of snake in order to administer the correct antivenom.</p><p>Manual identification is no easy feat, there are more than 3,500 species of snakes, 600 of which are venomous. Training doctors on each and every species is an impossible task, it would be not only time consuming but also very costly. Recent years have brought an exponential growth in A.I. research, of which image recognition seems to be the most benefited. Advances and expansion of the global smartphone market combined with high Internet Penetration Rate in low-income and middle-income countries has lead to an information boom. People now have access to sufficient computing power at the tips of their fingers making it possible to classify in real-time almost anything. Snake identification and classification will improve the quality of lives beyond high-income countries and significantly improve epidemiology data and treatment outcomes.</p><p>The SnakeCLEF 2021 challenge <ref type="bibr" coords="2,243.11,141.16,12.91,10.91" target="#b1">[2]</ref> as part of the LifeCLEF <ref type="bibr" coords="2,364.89,141.16,12.91,10.91" target="#b2">[3]</ref> laboratory aims to solve the aforementioned problem by identifying species of snakes from photographs. They provided an image collection of 414,424 photographs belonging to 772 snake species and taken in 188 countries.</p><p>This paper describes the participation of team "UAIC-AI", from the Faculty of Computer Science, "Alexandru Ioan Cuza" University of Iasi, Romania, at SnakeCLEF 2021 where we ranked 5 th with an F1-Country of 0.785. The remaining of this paper is organized as follows: Section 2 describes state-of-the-art methods in snake identification, Section 3 details the model we developed and the submitted runs and then Section 4 details the results we obtained, finally Section 5 concludes this paper and presents future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>SnakeCLEF 2020 <ref type="bibr" coords="2,163.90,321.73,12.68,10.91" target="#b3">[4]</ref> is the previous edition of the same competition where multiple state-of-theart systems have been presented. One such system is presented in <ref type="bibr" coords="2,385.34,335.28,12.84,10.91" target="#b4">[5]</ref> where the authors have developed a two stage preprocessing method; the first operation implies the transformation of rectangular images to squared ones followed by picture augmentation using location information, which helped them in the snake images recognition(as many species are spatially bound). The image classification algorithm is represented by a family of EfficientNet models <ref type="bibr" coords="2,448.38,389.48,12.92,10.91" target="#b5">[6]</ref> that have been extended with a flattening layer, a dense layer with 1000 neurons, a Swish <ref type="bibr" coords="2,445.98,403.03,12.86,10.91" target="#b6">[7]</ref> activation function and a dense layer of 783 neurons corresponding to each snake species. Their result was good, ranking 2 nd place in the competition with a F1 score of 0.4035. Last year's best result however was obtain by "Gokuleloop" team <ref type="bibr" coords="2,388.70,443.67,11.35,10.91" target="#b7">[8]</ref>. They used a ResNet50-V2 <ref type="bibr" coords="2,103.49,457.22,11.54,10.91" target="#b8">[9]</ref>, based on CNN architecture and trained the open source models on both ImageNet-1k and ImageNet-21k. The team was focused on domain-specific fine-tuning, experimenting with different pre-trained weights and performance impact. Location information, such as Country and Continent of the snake species, have also been integrated in the model with a final system being comprised of a ResNet-50-V2 architecture fine-tuned from ImageNet-21k weights and a naive probability weighting approach. Authors have found that integrating geographic data improves performance, achieving an F1 score of 0.625.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>The developed methods, that address the SnakeCLEF 2021 challenge, are all based on convolutional neural networks. We used 3 models, GoogLeNet <ref type="bibr" coords="2,348.71,610.69,16.32,10.91" target="#b9">[10]</ref>, VGG16 <ref type="bibr" coords="2,409.00,610.69,17.97,10.91" target="#b10">[11]</ref> and ResNet <ref type="bibr" coords="2,486.79,610.69,16.32,10.91" target="#b11">[12]</ref>, each with their own advantages and results. In this section we will take a look in the dataset that is comprised of 414,424 photos, analyze each model and discuss the experiments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>In order to have a clearer picture of the competition, it is mandatory to compare this year's dataset with the previous one. The 2020 dataset was much smaller in comparison to 2021, 245,185 training images were provided split into 783 species comparing to this year where there are now 414,424 images and 772 species. This means that in 2020 there were approximately 313 images per species and in 2021 there are now 536 images, a two-fold increase in images which will nonetheless result in a higher accuracy of the models. Additional geographical metadata (country and continent) for the image is also provided.</p><p>Analyzing the dataset yields important information related to the collection. Figure <ref type="figure" coords="3,470.58,351.32,5.02,10.91" target="#fig_0">1</ref> shows age variation between snakes whilst Figure <ref type="figure" coords="3,284.65,364.87,5.08,10.91" target="#fig_1">2</ref> illustrates geographic variations, demonstrating that the dataset has a variety of scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">GoogLeNet</head><p>GoogLeNet is a system that has shown to be powerful in many novel applications. In <ref type="bibr" coords="3,487.91,428.14,18.07,10.91" target="#b12">[13]</ref> they used GoogLeNet to remove streak artifacts due to projection missing in sparse-view CT reconstruction and found that the method is practical and reduces artifacts whilst preserving the quality of the image.</p><p>Our goal was to experiment with this method and build an architecture that has filters with multiple sizes that can operate on the same level. We kept the starting filters from GoogLeNet which used a (1 x 1) convolution because training is time-consuming and this method was used to compute reductions before the expensive (3 x 3), (5 x 5) convolutions and max-pooling.</p><p>The GoogLeNet architecture is 22 layers deep (and 5 pooling layers). These layers are grouped in 9 inception modules, and each of these is connected to the average pooling layer.</p><p>The GoogLeNet model used was implemented in PyTorch <ref type="bibr" coords="3,369.96,563.64,18.07,10.91" target="#b13">[14]</ref> and trained using Nvidia CUDA on GPU, a massive help was also Google Colab <ref type="bibr" coords="3,341.26,577.18,18.07,10.91" target="#b14">[15]</ref> which saved us a lot of time by training on their machines. We used a Cross Entropy Loss and Adam optimizer <ref type="bibr" coords="3,444.52,590.73,16.25,10.91" target="#b15">[16]</ref>.</p><p>We started the training process with a 0.001 learning rate and batch size of 64. Learning rate was adjusted at each epoch (in total we had 10 epochs) until we concluded that 0.001 is the best rate. We also enabled "aux_logits" which adds two auxiliary branches that can improve training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">VGG16</head><p>VGG16 is a well known model and although it is an older approach we wanted to compare it to the others as see how it performs, as in previous works <ref type="bibr" coords="4,334.17,210.23,17.79,10.91" target="#b16">[17]</ref> it showed potential. It is a 16 layer deep convolutional neural network with no residual blocks which improves on other models by replacing large kernel-sized filters with multiple 3×3 kernel-sized filters one after another. We used the model implemented in Keras that had the pre-trained weights from ImageNet-1k. The model implemented uses Adam optimizer and the hyperparameters were tuned experimentally. Initially the learning rate was set at a base value of 0.001 then initially decreased to 0.0001. The learning decay function used was ReduceLROnPlateau with a factor of 0.6 and patience of 4 while monitoring the validation accuracy. The batch size set was 16 as this gave us a good compromise between training speed and resource consumption.</p><p>In order to prevent overfitting we used a Dropout Regularization <ref type="bibr" coords="4,397.78,332.17,18.07,10.91" target="#b17">[18]</ref> strategy during the training phase. In an effort to reduce loss, a dropout layer with a probability of 0.5 was added between the last 2 dense layers of the VGG16 model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">ResNet</head><p>ResNet is the third and best model used. A study done in <ref type="bibr" coords="4,334.83,408.99,17.75,10.91" target="#b18">[19]</ref> outlines the performance overhead ResNet has over GoogLeNet, there is an almost 15% accuracy difference between the two, in favor of ResNet. The novelty and the performance of the method has lead us to experimenting with the model in order to test it in relationship with other neural networks. As other architectures often face the vanishing gradient problem, ResNet came with a solution called "identity shortcut connection" that skips one or more layers and acts like a much shallower network.</p><p>For this task a ResNet18 (from PyTorch) architecture pre-trained on ImageNet-1k <ref type="bibr" coords="4,467.86,490.29,17.94,10.91" target="#b16">[17]</ref> was fine-tuned. We decided to keep the pre-trained weights only for the first three layers and freeze them. The final fully connected layer was reset and reshaped to 772 nodes so that it match the current number of classes in the dataset.</p><p>The model was trained with 14 epochs with batches of 32 and a learning rate of 0.0002. To prevent overfitting a dropout regularization strategy was chosen during the training phase. We also used Adam Optimizer and cross entropy loss. To reduce loss, inside the basic blocks of ResNet, a dropout with a probability of 0.5 was applied.</p><p>Additional information available for each image regarding the country were taken into account in the model. Table <ref type="table" coords="4,217.43,612.23,5.16,10.91" target="#tab_0">1</ref> illustrate a dataframe fed to the network in the training phase. We included the species name, country, continent, genus and family but also the image path (that is not presented in the table). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation and comparisons</head><p>In this section we will discuss the results of the algorithms. Table <ref type="table" coords="5,403.36,210.15,5.17,10.91" target="#tab_1">2</ref> shows the results of our models. It is clearly seen that the best submission was "uaic_ai_submission_8" with an F1-Country of 0.785 according to the organizers, this ranked us 5 th .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Submissions information:</head><p>-uaic_ai_submission1: This submission was computed using a model with ResNet18 architecture resembling the training techniques from section 3.4 without taking into account the additional country information.</p><p>-uaic_ai_submission2: This submission was computed with GoogLeNet, consisted by all the information from the 3.2 subsection.</p><p>-uaic_ai_submission8: This submission was computed using a model with ResNet18 architecture resembling the training techniques from the 3.4 section and also taking into account the additional country information.</p><p>The best submission is represented by ResNet18 with country information. Further analysis into the GoogLeNet and VGG16 have revealed that due to aging architecture their performance is limited in comparison to more modern approaches such as ResNet. Scarce hardware resources and limited time have lead to slow progress in increasing ResNet performance and this will be the object of future work. Another research direction aims to address the imbalance of species in the dataset as we noticed that some species have more images than others. We would like to use augmentations from the Albumentation Library <ref type="bibr" coords="5,338.33,454.04,18.05,10.91" target="#b19">[20]</ref> for species that have few images in the dataset. The idea is simple, modifications are being done to dataset picture, like change of contrast, saturation, hue or brightness, in order to increase the dataset. Another technique we would like to use is MixUp augmentation <ref type="bibr" coords="5,290.49,494.69,16.23,10.91" target="#b20">[21]</ref>, the technique is simple and they describe it as follows "mixing up the features and their corresponding labels. Neural networks are prone to memorizing corrupt labels. MixUp relaxes this by combining different features with one another (same happens for the labels too) so that a network does not get overconfident about the relationship between the features and their labels" <ref type="bibr" coords="5,332.62,548.88,16.25,10.91" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>In conclusion, this paper is focused on team UAIC_AI's participation at SnakeCLEF 2021. We had good results with 3 submissions, the best one ranking 5 th with an F1-Country of 0.785. For future work we would like to try much novel image algorithms as well as improve the score of the current methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,198.30,104.79,8.93;3,98.46,84.19,187.51,101.55"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Age differences</figDesc><graphic coords="3,98.46,84.19,187.51,101.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,297.64,181.95,132.77,8.93;3,306.81,100.54,187.52,68.84"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Geographic difference</figDesc><graphic coords="3,306.81,100.54,187.52,68.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.99,90.49,410.65,61.26"><head>Table 1</head><label>1</label><figDesc>ResNet dataframe example for training</figDesc><table coords="4,95.67,118.58,403.98,33.17"><row><cell>Binomial name</cell><cell>Country</cell><cell>Continent</cell><cell>Genus</cell><cell>Family</cell></row><row><cell cols="2">Acanthophis antarcticus Australia</cell><cell>Australia</cell><cell cols="2">Acanthophis Elapidae</cell></row><row><cell>Agkistrodon conanti</cell><cell cols="4">United States of America North America Agkistrodon Viperidae</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,88.99,90.49,320.07,71.00"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="5,89.29,102.49,319.77,58.99"><row><cell>Official evaluation results</cell><cell></cell><cell></cell></row><row><cell>Submission name</cell><cell>F1</cell><cell cols="2">F1-Country Accuracy</cell></row><row><cell cols="3">uaic_ai_submission1 0.60 0.61</cell><cell>0.79</cell></row><row><cell cols="3">uaic_ai_submission2 0.21 0.29</cell><cell>0.51</cell></row><row><cell cols="3">uaic_ai_submission8 0.74 0.78</cell><cell>0.86</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Special thanks goes to A2 team. This work was supported by project <rs type="projectName">REVERT</rs> (<rs type="projectName">taRgeted thErapy for adVanced colorEctal canceR paTients</rs>), Grant Agreement number: <rs type="grantNumber">848098</rs>, <rs type="grantNumber">H2020-SC1-BHC-2018-2020/H2020-SC1-2019-Two-Stage-RTD</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_ZPZPsHZ">
					<orgName type="project" subtype="full">REVERT</orgName>
				</org>
				<org type="funded-project" xml:id="_4J5aFTZ">
					<idno type="grant-number">848098</idno>
					<orgName type="project" subtype="full">taRgeted thErapy for adVanced colorEctal canceR paTients</orgName>
				</org>
				<org type="funding" xml:id="_SeA5D6Z">
					<idno type="grant-number">H2020-SC1-BHC-2018-2020/H2020-SC1-2019-Two-Stage-RTD</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,112.66,197.00,28.65,10.91;6,159.87,197.00,44.16,10.91;6,222.60,197.00,58.92,10.91;6,304.03,197.00,202.67,10.91;6,112.66,210.55,133.16,10.91" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><surname>Who</surname></persName>
		</author>
		<ptr target="www.who.int/news-room/fact-sheets/detail/snakebite-envenoming/" />
		<title level="m" coord="6,159.87,197.00,44.16,10.91;6,222.60,197.00,53.56,10.91">Snakebite envenoming</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,224.10,394.62,10.91;6,112.28,237.65,393.70,10.91;6,112.66,251.20,288.62,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,376.16,224.10,131.11,10.91;6,112.28,237.65,288.08,10.91">Overview of snakeclef 2021: Automatic snake species identification with country-level focus</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Castañeda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,425.01,237.65,80.97,10.91;6,112.66,251.20,257.93,10.91">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,264.75,393.33,10.91;6,112.66,278.30,394.52,10.91;6,112.66,291.85,393.32,10.91;6,112.66,305.40,393.32,10.91;6,112.66,318.95,360.25,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,247.73,291.85,258.25,10.91;6,112.66,305.40,303.29,10.91">Overview of lifeclef 2021: a system-oriented evaluation of automated species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Castañeda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dorso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,439.18,305.40,66.80,10.91;6,112.66,318.95,330.51,10.91">Proceedings of the Twelfth International Conference of the CLEF Association (CLEF 2021)</title>
		<meeting>the Twelfth International Conference of the CLEF Association (CLEF 2021)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,332.50,393.33,10.91;6,112.66,346.05,395.17,10.91;6,112.66,359.59,232.29,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,384.50,332.50,121.49,10.91;6,112.66,346.05,238.13,10.91">Overview of the snakeclef 2020: Automatic snake species identification challenge</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Castaneda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sharada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,373.68,346.05,134.15,10.91;6,112.66,359.59,201.59,10.91">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,373.14,393.33,10.91;6,112.66,386.69,393.33,10.91;6,112.66,400.24,300.23,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,295.56,373.14,210.43,10.91;6,112.66,386.69,305.56,10.91">Combination of image and location information for snake species identification using object detection and efficientnets</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Boketta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Keibel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mense</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,440.09,386.69,65.90,10.91;6,112.66,400.24,269.53,10.91">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,413.79,394.52,10.91;6,112.66,427.34,393.33,10.91;6,112.66,440.89,394.52,10.91;6,112.66,454.44,295.07,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,178.09,413.79,324.20,10.91">EfficientNet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/tan19a.html" />
	</analytic>
	<monogr>
		<title level="m" coord="6,293.89,427.34,212.10,10.91;6,112.66,440.89,90.25,10.91;6,267.69,441.90,179.14,9.72">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct coords="6,112.66,467.99,395.01,10.91;6,112.66,483.98,97.35,7.90" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<title level="m" coord="6,308.54,467.99,163.82,10.91">Searching for activation functions</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,495.09,393.33,10.91;6,112.66,508.64,328.66,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,183.28,495.09,263.58,10.91">Impact of pretrained networks for snake species classification</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,467.81,495.09,38.18,10.91;6,112.66,508.64,297.96,10.91">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,522.18,393.33,10.91;6,112.66,535.73,266.31,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,249.53,522.18,191.77,10.91">Identity mappings in deep residual networks</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,463.99,522.18,41.99,10.91;6,112.66,535.73,136.06,10.91">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,549.28,394.52,10.91;6,112.28,562.83,395.00,10.91;6,112.31,576.38,207.88,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1409.4842" />
		<title level="m" coord="6,182.44,562.83,143.77,10.91">Going deeper with convolutions</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,589.93,393.33,10.91;6,112.66,603.48,173.19,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m" coord="6,246.14,589.93,259.84,10.91;6,112.66,603.48,49.16,10.91">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,617.03,395.17,10.91;6,112.66,630.58,395.01,10.91;6,112.41,644.13,38.81,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="6,259.74,617.03,203.38,10.91">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,488.38,617.03,19.45,10.91;6,112.66,630.58,347.24,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,657.68,393.32,10.91;7,112.66,86.97,393.98,10.91;7,112.41,100.52,18.52,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="6,432.21,657.68,73.77,10.91;7,112.66,86.97,269.86,10.91">Artifact removal using improved googlenet for sparse-view ct reconstruction</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,391.44,86.97,77.02,10.91">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,114.06,394.53,10.91;7,112.66,127.61,394.52,10.91;7,112.66,141.16,394.53,10.91;7,112.66,154.71,394.53,10.91;7,112.66,168.26,395.17,10.91;7,112.66,181.81,394.03,10.91;7,112.66,195.36,357.13,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="7,371.36,141.16,135.83,10.91;7,112.66,154.71,178.35,10.91">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="7,292.29,168.26,215.54,10.91;7,112.66,181.81,31.56,10.91">Advances in Neural Information Processing Systems 32</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,208.91,393.33,10.91;7,112.66,222.46,235.30,10.91" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="7,159.13,208.91,86.58,10.91;7,267.87,208.91,238.12,10.91;7,112.66,222.46,114.80,10.91">Building Machine Learning and Deep Learning Models on Google Cloud Platform</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Bisong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="59" to="64" />
		</imprint>
	</monogr>
	<note>Google colaboratory</note>
</biblStruct>

<biblStruct coords="7,112.66,236.01,395.01,10.91" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m" coord="7,227.57,236.01,157.91,10.91">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,249.56,394.53,10.91;7,112.28,263.11,395.55,10.91;7,112.66,276.66,236.19,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="7,249.07,263.11,214.65,10.91">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,472.07,263.11,35.76,10.91;7,112.66,276.66,147.19,10.91">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,290.20,393.33,10.91;7,112.26,303.75,393.73,10.91;7,112.41,317.30,323.32,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="7,424.65,290.20,81.34,10.91;7,112.26,303.75,215.02,10.91">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v15/srivastava14a.html" />
	</analytic>
	<monogr>
		<title level="j" coord="7,335.32,303.75,170.66,10.91">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,330.85,393.33,10.91;7,112.66,344.40,367.81,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="7,263.21,330.85,242.77,10.91;7,112.66,344.40,39.31,10.91">Analysis of resnet and googlenet models for malware detection</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">U</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,160.27,344.40,246.41,10.91">Journal of Computer Virology and Hacking Techniques</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="29" to="37" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,357.95,394.53,10.91;7,112.28,371.50,395.00,10.91;7,112.66,385.05,334.84,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="7,112.28,371.50,254.97,10.91">Albumentations: Fast and flexible image augmentations</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Buslaev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">I</forename><surname>Iglovikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Khvedchenya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Parinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Druzhinin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Kalinin</surname></persName>
		</author>
		<idno type="DOI">10.3390/info11020125</idno>
		<ptr target="https://www.mdpi.com/2078-2489/11/2/125.doi:10.3390/info11020125" />
	</analytic>
	<monogr>
		<title level="j" coord="7,379.45,371.50,54.58,10.91">Information</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,398.60,198.99,10.91" xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
