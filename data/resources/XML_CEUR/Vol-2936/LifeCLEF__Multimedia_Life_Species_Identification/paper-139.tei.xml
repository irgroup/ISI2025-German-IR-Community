<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.20,409.47,15.29;1,89.29,106.11,155.20,15.29">Learning to Monitor Birdcalls From Weakly-Labeled Focused Recordings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,88.99,135.73,64.22,10.56"><forename type="first">Jan</forename><surname>Schlüter</surname></persName>
							<email>jan.schlueter@jku.at</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computational Perception</orgName>
								<orgName type="institution">Johannes Kepler University Linz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.20,409.47,15.29;1,89.29,106.11,155.20,15.29">Learning to Monitor Birdcalls From Weakly-Labeled Focused Recordings</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">D559B9A0AD8FCE44D90EFA713E216868</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>birdcall identification</term>
					<term>soundscapes</term>
					<term>domain mismatch</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Passive acoustic monitoring can support biodiversity assessments, but requires automated analysis to be affordable at scale, which in turn requires labeled training data. Obtaining labeled data for each deployed device or location is expensive. The BirdCLEF 2021 scientific challenge tasked participants to train models on freely available weakly-labeled recordings of individual birds from xeno-canto, and apply them to recordings of passive devices. The ensemble of Convolutional Neural Networks (CNNs) described in this work achieved an F-Score of 0.672 across six recording locations, the twelth best entry among 816 teams.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>For some species, such as birds, passive acoustic monitoring is an interesting option to assess the status and trends of populations in an area. Detecting animal vocalizations and identifying the species in audio recordings is labor-intensive, prompting research for automated solutions to analyze recordings of monitoring devices. Many such solutions are based on machine learning, which requires fitting a prediction model to labeled recordings. Current prediction models are sensitive to recording conditions <ref type="bibr" coords="1,439.63,436.82,11.40,9.63" target="#b0">[1]</ref> and benefit from being constrained to the set of species to be expected. For ideal performance, the labeled recordings should thus be prepared with the same recording device and at the same location that is to be monitored later using the model. Since labeling recordings is labor-intensive, this represents a big hurdle for deployments at new locations.</p><p>For some species, such as birds, there are freely available online databases of audio recordings, such as the Macaulay Library [2], Tierstimmenarchiv <ref type="bibr" coords="1,405.70,518.12,11.00,9.63" target="#b1">[3]</ref>, or xeno-canto <ref type="bibr" coords="1,493.43,518.12,11.00,9.63" target="#b2">[4]</ref>. The BirdCLEF 2021 scientific challenge <ref type="bibr" coords="1,276.23,531.67,10.77,9.63" target="#b3">[5]</ref>, a part of the LifeCLEF initiative <ref type="bibr" coords="1,448.86,531.67,10.78,9.63" target="#b4">[6]</ref>, explored tapping into this resource for training prediction models for passive acoustic monitoring of birds. Specifically, participants were provided with 62 874 recordings of 397 bird species from xeno-canto as well as 20 labeled recordings of passive devices from two locations, and tasked to detect and identify birds in 5-second intervals on 80 passive recordings  from six locations. For all recordings, the geographic location and recording date and time are known. This scenario is highly interesting in practice: if prediction models achieve satisfying accuracy in this setting, they can be deployed to new locations without requiring matching training data. Solving this task poses two major challenges:</p><p>A. Most recordings from xeno-canto are focused recordings intended to capture the vocalizations of a particular bird, often done with directional microphones. In contrast, the recordings to predict on were done unattended and with omnidirectional microphones (sometimes referred to as "soundscapes" <ref type="bibr" coords="2,380.21,307.13,11.42,9.63" target="#b3">[5]</ref>). This creates a strong domain mismatch between training and test recordings. B. The xeno-canto recordings are weakly-labeled: There is no timing information regarding bird vocalizations, and only the species intended to be recorded is known for sure, other species occurring in the background may be labeled or omitted. In contrast, predictions on unattended recordings are to be done in 5-second intervals and include all audible species.</p><p>In this work, we attempt to tackle the first challenge with preprocessing and data augmentation, and the second one with multiple-instance learning and a two-level inference procedure.</p><p>The following section details our prediction models, followed by the training procedure in Section 3 and inference in Section 4. Section 5 describes our experimental setup and compares results both of single models and ensembles on a validation set and the official test set. Section 6 discusses ideas that did not work, and Section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Models</head><p>The general model architecture is depicted in Figure <ref type="figure" coords="2,359.47,548.30,4.33,9.63" target="#fig_1">1</ref>: From an (arbitrarily long) monophonic raw audio recording, a frontend computes a spectrogram-like representation. A Fully-Convolutional Network (FCN) processes this representation into a time series of logits for every class. When passed through a sigmoid, these would give us local predictions at every time step. Since we do not have temporally accurate labels to train these, only recording-wise labels, we apply a global pooling operation (over time) to obtain a single logit per class. Passed through a sigmoid, these serve as our recording-wise predictions.</p><p>There are several options for the frontend, the local predictor and the global pooling operation, which we will look at in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Frontend</head><p>The frontend processes monophonic audio recordings of sample rate 32 kHz (this is the rate the unattended recordings are done at, and all training recordings were resampled to). It consists of the following operations applied in sequence: Normalization: To ensure inputs are in a suitable range (even when the magnitude compression changes during training), each frequency band is normalized over the batch and time dimension with Batch Normalization <ref type="bibr" coords="3,371.96,463.14,10.91,9.63" target="#b5">[7]</ref>.</p><formula xml:id="formula_0" coords="3,89.29,157.19,46.05,9.69">A Short-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Local predictions</head><p>The purpose of the local predictor is to take the spectrogram computed by the frontend, and produce 397 time series of logits, one for each bird species. Regarding the input as an 80 pixel (or 64 pixel) high one-channel image, and the output as a 1 pixel high 397-channel image, the predictor should consist of a series of convolutions and pooling operations that reduces the image height to 1 pixel and produces 397 channels. We used four different such local predictors:</p><p>Vanilla ConvNet: An 8-layer network described in <ref type="bibr" coords="3,341.84,602.50,32.98,9.63">[8, p.3]</ref>, trained on 70 frames per second, 80 mel bands spectrograms. It has a receptive field of 79×103 (79 frequency bands, 103 spectrogram frames, about 1.5 seconds), a temporal stride of 9 spectrogram frames (7.778 predictions per second), and 3.74 million parameters.</p><p>Compared to <ref type="bibr" coords="3,183.53,656.70,11.01,9.63" target="#b6">[8]</ref>, it uses group normalization <ref type="bibr" coords="3,337.72,656.70,11.61,9.63" target="#b7">[9]</ref> with 16 groups instead of batch normalization.</p><p>ResNet: A 14-layer residual network with pre-activations described in <ref type="bibr" coords="4,444.37,87.63,33.62,9.63">[8, p.4]</ref>, also trained on 70 frames per second, 80 mel bands spectrograms. Its receptive field is 80 × 119, temporal stride is 9 frames, and it has 3.87 million parameters. Compared to <ref type="bibr" coords="4,129.96,128.28,10.95,9.63" target="#b6">[8]</ref>, it uses group normalization with 16 groups instead of batch normalization, and crops the shortcut connections instead of padding the convolutions.</p><p>Cnn14: A 16-layer network described in <ref type="bibr" coords="4,289.34,163.78,16.14,9.63" target="#b8">[10]</ref>, pretrained on AudioSet <ref type="bibr" coords="4,431.55,163.78,16.15,9.63" target="#b9">[11]</ref>, trained on 100 frames per second, 64 mel bands spectrograms (from 50 Hz to 8 kHz) with log(1 + 10 a x) magnitudes initialized to a = 5. The pretrained network's global max + average pooling and final two layers are replaced with two 1×1 convolutions of 1024 and 397 channels, respectively (with batch normalization and leaky rectification in between, and both convolutions preceded by 50% dropout <ref type="bibr" coords="4,394.86,231.53,16.08,9.63" target="#b10">[12]</ref>). It has a receptive field of 284×284 (2.84 seconds), stride of 32 frames (3.125 predictions per second), and 77.98 million parameters. The size of the receptive field exceeds the number of mel bands by far; this is possible because all convolutions are zero-padded.</p><p>ResNet38 : A 38-layer residual network described in <ref type="bibr" coords="4,347.27,294.13,16.00,9.63" target="#b8">[10]</ref>, also pretrained on AudioSet, trained on 100 frames per second, 64 meld bands spectrograms (from 50 Hz to 14 kHz). The pretrained network's final layers are replaced as described for Cnn14. Its receptive field is 2997×2997 (30 seconds), stride is 256 frames (one prediction every 2.56 seconds), and it has 71.01 million trainable parameters.</p><p>We optionally use 8-fold multi-sample dropout <ref type="bibr" coords="4,306.86,369.43,15.86,9.63" target="#b11">[13]</ref>, implemented by replicating the inputs and targets before the second to last dropout layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Global pooling</head><p>Up to here, the model produces a time series of logits for each class. The final step is to pool these logits into a single prediction per class for the full recording, such that we can compute (and minimize) the classification error wrt. the given global labels for the recording.</p><p>Global average pooling would distribute the gradient of the loss uniformly over all time steps, training the network to predict a labeled bird everywhere in the recording. Global max pooling would route the gradient only to the most confident detection of each species. As a compromise, log-mean-exp pooling with 1 a log 1 T T t=1 exp(ax t ) [14, Eq. 6] allows to interpolate between taking the maximum (a → ∞) and mean (a → 0. With a = 1, the output depends on the largest few values, which is also where the gradient is distributed to. This setting was used for most models. Some models updated a with backpropagation, possibly using a separate a per species (since some species might vocalize densely, warranting a small a, and others sparsely, requiring a large a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Training</head><p>The training procedure considers both challenges explained in Section 1: weak labels and domain mismatch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Excerpts</head><p>Ideally, the model would be trained on complete xeno-canto recordings -this is the only way we can be sure all weak labels are correct. If we pick a random excerpt, it is not guaranteed that all birds annotated to be present in the recording are also audible in the chosen excerpt. However, the longest recordings are 3 hours, which is impractical. As in <ref type="bibr" coords="5,118.18,162.42,11.04,9.63" target="#b6">[8]</ref>, we train on randomly selected 30-second excerpts instead, hoping that most annotated birds will be audible at least once. Too short files are looped to make up 30 seconds. Validation uses the central 30 seconds of a recording.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Augmentation</head><p>To help the model generalize, especially in the light of the domain mismatch from focused training recordings to unattended test recordings, we employ several data augmentation strategies:</p><p>Adding noise: To make the models work under low signal-to-noise ratios (i.e., the conditions found in the unattended recordings), we mix them with excerpts from the Chernobyl BiVA <ref type="bibr" coords="5,217.72,316.04,16.97,9.63" target="#b13">[15]</ref> and BirdVox-full-night <ref type="bibr" coords="5,349.80,316.04,16.97,9.63" target="#b14">[16]</ref> datasets. These datasets are precisely annotated with bird occurrences, so we can extract all parts void of birds. We started out carefully, but the best setting turned out to be mixing every training example with background noise, drawing a value p ∈ [0, 1) and scaling the noise with p and the bird recording excerpt with 1 -p. For some models, we also set p = 1 with 1% or 0.1% probability, setting the labels to all zero in this case. And for some models, the chosen noise excerpt is scaled to a maximum absolute value in [0, 1) before mixing it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random downmixing:</head><p>The model is trained on monophonic recordings, but xeno-canto usually has stereo recordings. <ref type="foot" coords="5,253.72,445.02,4.23,7.04" target="#foot_0">1</ref> We downmix them during training with a randomly uniform weight p for the left channel, and 1 -p for the right channel.</p><p>Filterbank pitch shifting: Some models are trained with pitch shifting, cheaply implemented by modifying the mel filterbank: Instead of precomputing the filterbank, it is constructed on-the-fly, scaling the minimum and maximum frequency by the same random factor chosen uniformly between 0.95 and 1.05. A separate filterbank is constructed for every example in a minibatch (and applied to the minibatch with a batched matrix multiplication).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Magnitude warping:</head><p>Drawing on an idea of Vladislav Kramarenko 2 , in the log(1 + 10 a x) magnitude transformation, for some models, we scaled a by a random factor between 0.5 and 1.5 during training, shifting the result such that the maximum output value matches the unmodified a (to not drastically change the value range).</p><p>High-frequency damping: Also drawing on an idea of Vladislav Kramarenko, for some models, we lowered up to 50% of the high-frequency part of the spectrum by up to 50% magnitude, with a linear fade to the lowest frequency bin of the spectrum (the only bin not damped at all). His reasoning was that in the unattended recordings, birds are often farther away than in focused recordings, and high frequencies are damped more strongly with distance (which is indeed the case in forests <ref type="bibr" coords="6,462.01,155.38,16.08,9.63" target="#b15">[17]</ref>). We only apply this damping to models without median subtraction -it is applied before compressing magnitudes, so for models with approximately logarithmic magnitudes (log(1 + 10 5 x)) and median subtraction its effect would be canceled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Optimization</head><p>To be able to monitor training progress, we split off 10% of the xeno-canto recordings as a validation set, ensuring that no recordist is part of both the training and validation set (as they tend to visit the same locations using the same equipment).</p><p>Optimization uses ADAM with mini-batches of 16 examples, an initial learning rate of 10 -3 , β 1 = 0.9, β 2 = 0.999, = 10 -8 , minimizing binary cross-entropy against all species (for a single model, we reduced targets for background species to 0.6). The validation loss is computed every 1000 update steps. If it does not improve over the current best value for 10 such evaluations in a row, the learning rate is reduced to a tenth, and training is continued. Training is stopped when the learning rate reaches 10 -6 . For the two pretrained models, we tried reducing the learning rate for the pretrained layers to 1% or 10% compared to the novel layers or to freeze the pretrained layers for some time, but it turned out that using the full learning rate for all the layers from the start works best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Inference</head><p>While the model is trained on 30-second excerpts, at test time, it needs to predict the set of birds for a 10-minute recording in non-overlapping 5-second windows. We perform this in two steps: establishing a set of species present in the 10-minute recording, and detecting these species in 5-second windows. This allows to tune a separate threshold for the second step, to reduce false negatives without impacting false positives too much.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Species set</head><p>The way the model is constructed (Section 2), it can be applied to arbitrarily long recordings. We could thus apply it directly to a 10-minute recording to obtain a set of species. However, the employed global pooling method (log-mean-exp pooling) includes a division by the input length. If a bird appears only in the first two minutes of a recording, its pooled prediction is lower than if it appeared throughout the ten minutes. We could thus not distinguish low-confidence detections from high-confidence detections. Our solution is to apply the model to 30-second windows overlapping by 50% with a threshold of 0.5 and taking the union of all detections (or, equivalently, taking the maximum over the prediction windows and applying the threshold afterwards).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Windowed detections</head><p>Again, the way the model is constructed, we can apply it directly to 5-second windows even when it was trained on 30-second excerpts. Since log-mean-exp pooling is dependent on the input length, we need to adjust the threshold to make up for the mismatch. As we have established a set of bird species in the previous step, we can afford to set the threshold very low, removing detections that are outside the established set. Optimized for a single model on the 20 unattended recordings available for training, we found a threshold of 0.18 to be optimal; when optimizing an ensemble of 18 models, the optimal threshold was 0.08.</p><p>As an alternative, we can opt to use max pooling, not distinguishing single detections from repeated detections within a 5-second window and relying on the established set of species to filter out false positives. For the same 18-model ensemble, the optimal threshold on the 20 unattended recording using max pooling is 0.55.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ensembling</head><p>To combine results from multiple models, we average their predicted logits directly after pooling, and apply thresholds afterwards. Applying thresholds first and combining models by vote counts per species performed worse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Implementation</head><p>For improved performance, instead of splitting up the audio recording into overlapping 30-second and non-overlapping 5-second windows and applying the models to each excerpt, we apply the model to the full 10-minute recording up to the global pooling. Using information on the model's receptive field size, padding and stride, we compute a timestamp for every prediction time step. This way we can extract windows from the series of logits and pool them as needed. For an 18-model ensemble, inference takes 6 seconds for a 10-minute file on an NVIDIA GTX 1080 Ti. In addition to improving computational efficiency, this method also limits potential artifacts from zero-padding the audio excerpts in the two pretrained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>As mentioned in Section 3.3, we formed a train/validation split, grouped by recordist, using about 90% of the 62 874 recordings for training and the remainder for validation. We use the 20 provided unattended recordings (which are labeled in 5-second windows) as additional validation files, separated into the two recording locations Costa Rica (COR) and Sapsucker Woods (SSW).</p><p>Evaluation is done in terms of F-score: The numbers of true positives, false positives and false negatives are determined and added up across all 397 species as well as a "nocall" class for 5-second windows without any audible bird vocalization. From the total numbers, precision, recall and F-score are computed.</p><p>We trained several combinations of frontend, predictor, global pooling, and data augmentation and evaluated them both on the xeno-canto recordings left for validation (using the central 30 seconds only), and the 20 unattended recordings (using the full inference procedure of Section 4 with a second-level threshold of 0.18). Table <ref type="table" coords="8,456.01,128.28,4.16,9.63">1</ref>, columns "F-score" show the results. The variations were chosen in search of good models for the challenge, not for evaluating the effect of any particular measure. Thus, we can only draw limited conclusions from these results:</p><p>• Unsurprisingly, results vary when repeating an experiment (rows 5 and 6 differ by up to 0.01 F-score with the same hyperparameters). Comparing single experiments with close results is thus meaningless. • Comparing the predictors, on the xeno-canto recordings, Cnn14 performs best, followed by ResNet38, the small ResNet, and Vanilla ConvNet. • On the unattended recordings (from locations COR and SSW), the order is the same, except that ResNet38 performs much worse than all others. A possible explanation is its large receptive field and large stride, which may make the series of logits too inaccurate for 5-second window predictions. • None of the augmentations (except for adding noise) has a clear positive or negative effect when tested in isolation. It is possible that the augmentations diversify the set of models in a way that is helpful for ensembling.</p><p>For the challenge, we formed ensembles of multiple models. An initial ensemble of 5 models was picked by hand (Table <ref type="table" coords="8,259.55,376.03,4.31,9.63">1</ref>, column "5-model ensemble"). It performs better than all Cnn14 models combined, even when adding all small ResNets, and all Vanilla ConvNets, and achieved an F-score of 0.667 on the official challenge data. Finally, a slightly better ensemble of 18 models was found in three trials of starting from an all-model ensemble (except ResNet38) and greedily removing randomly chosen single models until no improvement on SSW and COR combined was observed. It obtained an F-score of 0.672 on the challenge dataset, the 12th best entry. After observing that the second inference stage performs better using max pooling (Section 4.2), another three trials found an 11-model ensemble slightly improving on this with an F-score of 0.676 submitted after the end of the challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Failed ideas</head><p>For each xeno-canto recording and for each unattended recording (both in the training and test set), the geographic location is known. It would thus be possible to focus on predicting those species that are likely to be present at each recording site. Exploring the data, we saw that most species present in the 20 labeled recordings from COR and SSW have also been uploaded to xeno-canto within short distance of the recording site, and only few species have not been uploaded within 60 km of the site. We tried to make use of this in three ways:</p><p>1. Computing the set of species uploaded within 60 km of sites COR and SSW results in reduced lists of 133 and 89 species, respectively. We filtered the predictions of a 397-species model using these site-specific lists, but it did not improve results on the 20 recordings. 2. Using the reduced species lists, we trained site-specific prediction models. Both for single models and for a 5-model ensemble, this worked worse than the generic 397-species models. 3. We weighted xeno-canto recordings by their distance to a particular site during training, giving close recordings higher importance in the loss function. We computed these weights as 1/ 1 + (d/500) k , where d is the distance in kilometers, setting k = 2 for a softer and k = 3 for a harder distance dependance (inspired by the shape of a Butterworth filter response). These site-specific models performed comparable to unspecific models, not warranting the extra effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>We obtained competitive results in the BirdCLEF 2021 challenge following a common recipe for such competitions: Training a lot of prediction models with varying settings, then blending them into an ensemble using some means of automatic model selection.</p><p>For practical purposes, we deem the following to be important: <ref type="bibr" coords="9,408.58,327.30,14.05,9.63" target="#b0">(1)</ref> Training on long enough excerpts to increase the chance that the weak labels are correct, (2) including a form of global pooling in the prediction model that encourages proper credit assignment to local predictions, (3) augmenting data by adding noise, (4) performing inference on two timescales to filter short-term detections using long-term information, (5) using pretrained audio models as a basis. While these aspects were followed in our work, each offers room for improvement. For example, the global log-mean-exp pooling is oblivious to the typical frequency of calls. Even with a trainable a hyperparameter per species, it is forced to use the same pooling for different call types. The noise used for augmentation in our work is limited in diversity, as it stems from only two locations. Extending it requires manual screening of data for the absence of birds, or a highly accurate bird detector. The pretrained Cnn14 performs comparably well, but has an overly large receptive field in frequency dimension, and high computational demands. Another interesting route for future work is to make more use of long-term temporal structure. The two-stage inference procedure only captures the assumption that a particular bird (or bird species) will be audible multiple times during a 10-minute recording, but some bird calls have more complex regularities that could help detecting or distinguishing them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Results for several variants on the validation data (columns "F-score"). The last six columns show model selections for six ensembles (columns "ensemble"). The last three rows display ensemble results on the two validation set locations and the challenge test set.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,89.29,156.73,230.88,8.85"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Outline of the prediction model architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,89.29,157.19,418.21,266.21"><head>Time Fourier Transform (STFT) computes</head><label></label><figDesc></figDesc><table coords="3,89.29,157.24,418.21,266.15"><row><cell>a linear spectrogram. For two of</cell></row><row><cell>the predictors, windows are 1486 samples long and start every 457 samples, resulting</cell></row><row><cell>in 70 frames per second. For two pretrained predictors, the window and hop size</cell></row><row><cell>is 1024 and 320, respectively, giving 100 frames per second. In both cases, Hann</cell></row><row><cell>windowing is used and only magnitudes are kept.</cell></row><row><cell>A mel filterbank transforms spectrograms to mel spectrograms. Depending on the</cell></row><row><cell>predictor, it has 80 bands from 27.5 Hz to 10 kHz, 80 bands from 27.5 Hz to 15 kHz,</cell></row><row><cell>64 bands from 50 Hz to 8 kHz, or 64 bands from 50 Hz to 14 kHz.</cell></row><row><cell>A pointwise nonlinearity compresses magnitude values by passing them through either</cell></row><row><cell>y = log(1 + 10 a x) or y = x σ(a) , where σ(a) = 1/(1 + exp(-a)) denotes the logistic</cell></row><row><cell>sigmoid, a is initialized to zero and learned by backpropagation.</cell></row><row><cell>Denoising: The recordings have very different background noise floors, both due to</cell></row><row><cell>the environment and recording equipment. To help generalization, recordings</cell></row><row><cell>are preprocessed by subtracting the median over time from each frequency band</cell></row><row><cell>(separately for each recording or excerpt). For pretrained models, this step is either</cell></row><row><cell>skipped, or a global offset is added after median subtraction to retain the maximum</cell></row><row><cell>input value (otherwise the value range would not match what the models were</cell></row><row><cell>trained on).</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,108.93,649.64,57.83,7.92"><p>For BirdCLEF</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1" coords="5,169.75,649.64,336.24,7.92;5,88.96,660.60,114.84,7.92;5,104.78,669.81,3.65,5.28;5,108.93,671.56,272.14,7.92"><p>2021, only downmixed xeno-canto recordings were provided; we partly replaced them with the original stereo files.2 https://www.kaggle.com/c/birdsong-recognition/discussion/183269</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>First of all, thanks a lot to the organizers of BirdCLEF 2021: <rs type="person">Stefan Kahl</rs>, <rs type="person">Tom Denton</rs>, <rs type="person">Holger Klinck</rs>, <rs type="person">Hervé Glotin</rs>, <rs type="person">Hervé Goëau</rs>, <rs type="person">Willem-Pier Vellinga</rs>, <rs type="person">Robert Planqué</rs> and <rs type="person">Alexis Joly</rs>. Apart from that, thanks to <rs type="person">Paul Primus</rs> for suggesting the datasets used for background noises, and to <rs type="person">Khaled Koutini</rs> for the idea of using a batched matrix multiplication to allow different pitch shifts for each minibatch item.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="10,111.72,111.98,394.26,9.63;10,111.44,125.53,396.06,9.63;10,111.72,139.08,394.54,9.63;10,111.16,152.63,347.24,9.63" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,310.30,111.98,195.68,9.63;10,111.44,125.53,370.32,9.63">Acoustic scene classification in DCASE 2020 challenge: generalization across devices and low complexity solutions</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2005.14623" />
	</analytic>
	<monogr>
		<title level="m" coord="10,111.72,139.08,394.54,9.63;10,111.16,152.63,116.58,9.63">Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)</title>
		<meeting>the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.72,193.28,396.09,9.63;10,111.72,206.83,28.48,9.63" xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Tierstimmenarchiv</surname></persName>
		</author>
		<ptr target="https://www.tierstimmenarchiv.de" />
		<imprint>
			<date type="published" when="1951">1951. 2021-07-02</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.72,220.38,359.94,9.63" xml:id="b2">
	<monogr>
		<ptr target="https://www.xeno-canto.org" />
		<title level="m" coord="10,111.72,220.38,49.58,9.63">xeno-canto</title>
		<imprint>
			<date type="published" when="2005">2005. 2021-07-02</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.72,233.93,395.79,9.63;10,111.31,247.48,396.49,9.63;10,111.72,261.02,394.27,9.63;10,111.72,274.57,62.61,9.63" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,152.67,247.48,355.14,9.63;10,111.72,261.02,17.85,9.63">Overview of BirdCLEF 2021: Bird call identification in soundscape recordings</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,157.71,261.02,348.28,9.63;10,111.72,274.57,28.44,9.63">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.72,288.12,395.79,9.63;10,111.72,301.67,395.78,9.63;10,111.72,315.22,394.62,9.63;10,111.44,328.77,394.54,9.63;10,111.72,342.32,394.27,9.63;10,111.72,355.87,204.67,9.63" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,393.77,315.22,112.57,9.63;10,111.44,328.77,394.54,9.63;10,111.72,342.32,105.88,9.63">Overview of LifeCLEF 2021: a system-oriented evaluation of automated species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Castañeda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dorso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,244.02,342.32,261.96,9.63;10,111.72,355.87,171.33,9.63">Proceedings of the Twelfth International Conference of the CLEF Association (CLEF 2021)</title>
		<meeting>the Twelfth International Conference of the CLEF Association (CLEF 2021)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.72,369.42,394.27,9.63;10,111.72,382.97,394.27,9.63;10,111.72,396.52,394.26,9.63;10,111.12,410.07,396.97,9.63;10,111.72,423.62,129.63,9.63" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,221.11,369.42,284.87,9.63;10,111.72,382.97,173.64,9.63">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v37/ioffe15.html" />
	</analytic>
	<monogr>
		<title level="m" coord="10,317.66,382.97,188.32,9.63;10,111.72,396.52,201.36,9.63;10,391.01,396.52,114.97,9.63;10,111.12,410.07,82.46,9.63">Proceedings of the 32nd International Conference on Machine Learning (ICML)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML)<address><addrLine>PMLR, Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct coords="10,111.72,437.16,395.79,9.63;10,111.16,450.71,231.63,9.63" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,171.38,437.16,314.35,9.63">Bird identification from timestamped, geotagged audio recordings</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schlüter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,111.16,450.71,112.88,9.63">Working Notes of CLEF</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.72,464.26,394.27,9.63;10,111.72,477.81,171.75,9.63" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,186.03,464.26,98.24,9.63">Group normalization</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,311.10,464.26,194.88,9.63;10,111.72,477.81,137.14,9.63">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.72,491.36,396.09,9.63;10,111.72,504.91,394.27,9.63;10,111.32,518.46,396.78,9.63;10,111.72,532.01,169.51,9.63" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,438.63,491.36,69.17,9.63;10,111.72,504.91,326.80,9.63">Panns: Largescale pretrained audio neural networks for audio pattern recognition</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<idno type="DOI">10.1109/TASLP.2020.3030497</idno>
	</analytic>
	<monogr>
		<title level="j" coord="10,448.04,504.91,57.95,9.63;10,111.32,518.46,287.18,9.63">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2880" to="2894" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.72,545.56,395.78,9.63;10,111.72,559.11,394.27,9.63;10,111.72,572.66,394.27,9.63;10,111.72,586.21,376.00,9.63" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,217.51,559.11,288.48,9.63;10,111.72,572.66,28.41,9.63">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2017.7952261</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,167.96,572.66,338.03,9.63;10,111.72,586.21,99.32,9.63">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="776" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.72,599.75,395.78,9.63;10,111.72,613.30,394.55,9.63;10,111.72,626.85,331.18,9.63" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>arXiv e-prints abs/1207.0580</idno>
		<ptr target="http://arxiv.org/abs/1207.0580" />
		<title level="m" coord="10,111.72,613.30,358.56,9.63">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.72,640.40,396.09,9.63;10,111.72,653.95,396.38,9.63;10,111.72,668.48,94.67,8.96" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="10,171.73,640.40,336.07,9.63;10,111.72,653.95,31.37,9.63">Multi-sample dropout for accelerated training and better generalization</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Inoue</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1905.09788" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,111.72,87.63,396.09,9.63;11,111.72,101.18,394.26,9.63;11,111.72,114.73,361.15,9.63" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,253.64,87.63,254.16,9.63;11,111.72,101.18,70.57,9.63">From image-level to pixel-level labeling with convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,208.95,101.18,297.03,9.63;11,111.72,114.73,158.12,9.63">Proceedings of the 28th IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the 28th IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1713" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,111.72,128.28,394.27,9.63;11,111.72,141.83,394.27,9.63;11,111.72,155.38,396.38,9.63;11,111.72,168.93,274.60,9.63" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="11,424.20,128.28,81.78,9.63;11,111.72,141.83,394.27,9.63;11,111.72,155.38,19.38,9.63">Bird vocalisation activity (biva) database: annotated soundscapes from the chernobyl exclusion zone</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kendrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Barçante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Beresford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gashchak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wood</surname></persName>
		</author>
		<idno type="DOI">10.5285/be5639e9-75e9-4aa3-afdd-65ba80352591</idno>
		<ptr target="https://doi.org/10.5285/be5639e9-75e9-4aa3-afdd-65ba80352591.doi:10.5285/be5639e9-75e9-4aa3-afdd-65ba80352591" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,111.72,182.48,396.09,9.63;11,111.72,196.03,394.27,9.63;11,111.72,209.57,395.79,9.63;11,111.72,223.12,396.38,9.63;11,111.72,236.67,175.24,9.63" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,446.41,182.48,61.40,9.63;11,111.72,196.03,309.53,9.63">Birdvox-fullnight: A dataset and benchmark for avian flight call detection</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lostanlen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Farnsworth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kelling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2018.8461410</idno>
		<ptr target="https://doi.org/10.1109/ICASSP.2018.8461410.doi:10.1109/ICASSP.2018.8461410" />
	</analytic>
	<monogr>
		<title level="m" coord="11,479.25,196.03,26.73,9.63;11,111.72,209.57,390.00,9.63">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="266" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,111.72,250.22,394.27,9.63;11,111.72,263.77,394.26,9.63;11,111.31,277.32,395.82,9.63;11,111.43,291.85,54.58,8.96" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,184.90,250.22,321.08,9.63;11,111.72,263.77,186.16,9.63">Reverberation and frequency attenuation in forests-implications for acoustic communication in animals</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Padgham</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.1629304</idno>
		<ptr target="https://doi.org/10.1121/1.1629304.doi:10.1121/1.1629304" />
	</analytic>
	<monogr>
		<title level="j" coord="11,308.51,263.77,197.48,9.63;11,111.31,277.32,38.93,9.63">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="402" to="410" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
