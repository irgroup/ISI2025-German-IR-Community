<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,416.69,15.42;1,89.29,106.66,285.96,15.42">Incorporation of object detection models and location data into snake species classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,64.92,11.96"><forename type="first">RegÅ‘</forename><surname>Borsodi</surname></persName>
							<email>rego.borsodi@edu.bme.hu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Telecommunications and Media Informatics</orgName>
								<orgName type="institution">Budapest University of Technology and Economics</orgName>
								<address>
									<settlement>Budapest</settlement>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,166.85,134.97,56.40,11.96"><forename type="first">DÃ¡vid</forename><surname>Papp</surname></persName>
							<email>pappd@tmit.bme.hu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Telecommunications and Media Informatics</orgName>
								<orgName type="institution">Budapest University of Technology and Economics</orgName>
								<address>
									<settlement>Budapest</settlement>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,416.69,15.42;1,89.29,106.66,285.96,15.42">Incorporation of object detection models and location data into snake species classification</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">063B9627F209E972CC9D88D3BDBD86B2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>classification</term>
					<term>object detection</term>
					<term>convolutional neural networks</term>
					<term>snake species identification</term>
					<term>SnakeCLEF</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Photo-based automatic snake species identification could assist in clinical management of snakebites. LifeCLEF announced the SnakeCLEF 2021 challenge, which aimed attention on this task and provided location metadata for most snake images. This paper describes the participation of the BME-TMIT team in this challenge. In order to reduce clutter and drop the unnecessary background, we employed the state-of-the-art EfficientDet object detector, which was fine-tuned on manually annotated images. Detected snakes were then classified by EfficientNet weighted with the likelihood of location information. Based on the official evaluation of SnakeCLEF 2021, our solution achieved an ğ¹1 country score of 0.903, which placed our team at rank 1 position in the challenge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Snakebite envenoming is a potentially life-threatening disease caused by toxins in the bite of a venomous snake. Envenoming can also be caused by having venom sprayed into the eyes by certain species of snakes that have the ability to spit venom as a defense measure <ref type="bibr" coords="1,453.60,417.64,11.43,10.91" target="#b0">[1]</ref>. Most of these occur in Africa, Asia, and Latin America. In Asia, up to 2 million people are envenomed by snakes each year, while in Africa, there are an estimated 435 000 to 580 000 snakebites annually that need treatment. Identification of the snake is essential in planning treatment in certain areas of the world, but it is a difficult task and not always possible <ref type="bibr" coords="1,383.00,471.84,11.34,10.91" target="#b1">[2]</ref>. Ideally the snake would be brought in with the person, but attempting to catch or kill the offending snake also puts one at risk for re-envenomation or creating a second person bitten. On the other hand, taking a photo of the snake is much more feasible, less dangerous, and generally is recommended. The three types of venomous snakes that cause the majority of injuries are vipers, kraits, and cobras. Knowledge of what species are present locally can be crucial in searching for adequate medicine <ref type="bibr" coords="1,132.48,553.13,11.43,10.91" target="#b2">[3]</ref>. Developing a robust system for identifying species of snakes from photographs and geographical information could significantly improve snakebite eco-epidemiological data and correct antivenom administration <ref type="bibr" coords="1,244.69,593.78,11.48,10.91" target="#b1">[2,</ref><ref type="bibr" coords="1,258.91,593.78,7.65,10.91" target="#b3">4]</ref>. In 2021, the fifth round of SnakeCLEF <ref type="bibr" coords="1,447.01,593.78,12.99,10.91" target="#b4">[5]</ref> challenge was announced by the LifeCLEF <ref type="bibr" coords="2,238.05,86.97,12.99,10.91" target="#b5">[6]</ref> campaign, where the goal was to create a system that is capable of automatically categorizing snakes on the species level. In this paper, we describe the solution of our team (BME-TMIT) in detail, which achieved rank 1 in the challenge. We first perform object detection on the images to separate the snake(s) from the background; then, the next step is to classify each detected snake. Finally, classification data is fused with location information using a likelihood weighting method. These steps are specified in Sections 4, 5, and 6, respectively. Section 2 briefly outlines the corresponding literature in the field; lastly, Section 7 summarizes our conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Common tasks in computer vision, and therefore in image-based snake species identification, include object detection and object classification. The former aims to locate the region of an image where the object of interest may appear <ref type="bibr" coords="2,318.27,267.54,11.59,10.91" target="#b6">[7]</ref>, while the latter involves categorizing an input image into several predefined classes <ref type="bibr" coords="2,301.11,281.08,11.58,10.91" target="#b7">[8]</ref>. In the past decade, Convolutional Neural Networks (CNN) became the most popular approach for visual recognition due to their superior performance. Architectures of CNN that are often used for classification in practice are among others VGGNet <ref type="bibr" coords="2,164.14,321.73,11.58,10.91" target="#b8">[9]</ref>, Inception <ref type="bibr" coords="2,230.81,321.73,16.41,10.91" target="#b9">[10]</ref>, Residual Network (ResNet) <ref type="bibr" coords="2,384.51,321.73,16.42,10.91" target="#b10">[11]</ref>, EfficientNet <ref type="bibr" coords="2,467.22,321.73,18.07,10.91" target="#b11">[12]</ref> and MobileNet <ref type="bibr" coords="2,138.71,335.28,16.25,10.91" target="#b12">[13]</ref>.</p><p>Regarding object detection, numerous deep learning frameworks have been proposed in the literature, and they can be organized into two categories: (i) two-stage detectors and (ii) one-stage detectors. Two-stage detection models first generate region proposals, and then they are forwarded to a specific network for further classification. The most prominent twostage detectors are Region-based CNN (R-CNN) <ref type="bibr" coords="2,311.98,403.03,18.07,10.91" target="#b13">[14]</ref> and its' successors: Fast R-CNN <ref type="bibr" coords="2,483.32,403.03,18.07,10.91" target="#b14">[15]</ref> , Faster R-CNN <ref type="bibr" coords="2,155.45,416.58,16.42,10.91" target="#b15">[16]</ref>, Mask R-CNN <ref type="bibr" coords="2,242.01,416.58,16.41,10.91" target="#b16">[17]</ref>. Differently, there is no separate procedure for region proposal in one-stage detection models, as they are designed to predict the bounding boxes and the corresponding class probabilities at once <ref type="bibr" coords="2,306.16,443.67,16.14,10.91" target="#b17">[18]</ref>. Popular one-stage detectors include You Only Look Once (YOLO) <ref type="bibr" coords="2,202.65,457.22,16.52,10.91" target="#b18">[19,</ref><ref type="bibr" coords="2,221.90,457.22,12.58,10.91" target="#b19">20,</ref><ref type="bibr" coords="2,237.21,457.22,12.58,10.91" target="#b20">21,</ref><ref type="bibr" coords="2,252.51,457.22,12.39,10.91" target="#b21">22]</ref>, Single Shot Detector (SSD) <ref type="bibr" coords="2,395.01,457.22,16.38,10.91" target="#b22">[23]</ref>, RetinaNet <ref type="bibr" coords="2,466.13,457.22,16.37,10.91" target="#b23">[24]</ref>, and EfficientDet <ref type="bibr" coords="2,144.97,470.77,16.25,10.91" target="#b24">[25]</ref>.</p><p>Automated image-based snake species identification is challenging due to small inter-class variance, high intra-class variance, and a large number of categories (species) <ref type="bibr" coords="2,426.52,497.87,16.09,10.91" target="#b25">[26]</ref>. Furthermore, labeled snake image collections usually contain only several hundred images. 38 different taxonomically relevant features were manually identified to categorize six snake species in <ref type="bibr" coords="2,487.08,524.97,16.08,10.91" target="#b26">[27]</ref>, while Amir et al. <ref type="bibr" coords="2,165.36,538.52,17.79,10.91" target="#b27">[28]</ref> used automated feature extraction to get texture features and distinguish 22 different snake species. Other researchers applied deep learning techniques. For example, Faster R-CNN and ResNet were used to classify nine different snake species occurring on the GalÃ¡pagos Islands of Ecuador <ref type="bibr" coords="2,219.32,579.17,16.23,10.91" target="#b28">[29]</ref>; authors of <ref type="bibr" coords="2,288.97,579.17,17.76,10.91" target="#b29">[30]</ref> experimented with three different-sized CNN architectures; recently, Yang and Sinnott classified Australian snakes using deep networks <ref type="bibr" coords="2,487.49,592.72,16.14,10.91" target="#b30">[31]</ref>. In cases when only a small amount of labeled snake images are available, it could be beneficial to use a few-shot learning approach, such as the Siamese network <ref type="bibr" coords="2,376.90,619.81,16.09,10.91" target="#b31">[32]</ref>, or the recently proposed Double-View Matching Network (DVMN) <ref type="bibr" coords="2,275.13,633.36,16.10,10.91" target="#b32">[33]</ref>, although, the latter was tested on X-ray images. Abeysinghe et al. <ref type="bibr" coords="2,172.04,646.91,18.06,10.91" target="#b33">[34]</ref> performed single-shot learning by applying the Siamese network to categorize 84 snake species, where only 3 to 16 training images were available per species. On the other hand, Putra et al. <ref type="bibr" coords="3,213.09,86.97,18.06,10.91" target="#b34">[35]</ref> aimed to build a system to recognize the existing bite points on the snake bite image and then classified to venomous snake bite or non-venomous using Chain Code and K Nearest Neighbor (KNN) classification. Their bite mark recognition method achieved 65% accuracy while distinguishing venomous from non-venomous bites was possible with 80% accuracy.</p><p>For the SnakeCLEF 2020 challenge, 287 632 photographs were prepared that belong to 783 snake species and were taken in 145 countries <ref type="bibr" coords="3,297.50,168.26,16.27,10.91" target="#b25">[26]</ref>. The qualified teams used object detection on the images as a preprocessing step. Gokula Krishnan used an object detection model with ResNet-50 backbone <ref type="bibr" coords="3,182.29,195.36,16.23,10.91" target="#b35">[36]</ref>, and team FHDO-BCSG utilized a Mask R-CNN model reaching 39.0 mAP (mean average precision) <ref type="bibr" coords="3,228.60,208.91,17.91,10.91" target="#b36">[37]</ref> on the COCO (Common Objects in Context) dataset <ref type="foot" coords="3,480.14,207.15,3.71,7.97" target="#foot_0">1</ref> .</p><p>The dataset was expanded to a size of 414 424 images in SnakeCLEF 2021, which is approximately a 44% increase compared to the previous challange, while slightly lowering the number of snake species to 772; however, the photographs were taken in 188 countries. The full dataset was split into a training subset with 347 405 images, a validation subset with 38 601 images, and a test subset with 28 418 images. Each subset has the same class distribution, while the minimum number of training, validation, and test images per class is nine, one, and one, respectively. Furthermore, the test subset contains all 772 classes and observations from almost all the countries presented in training and validation sets. Similarly to the teams of SnakeCLEF 2020, we applied object detection and then object classification on the images. The main contributions of our work are (i) training the EfficientDet-D1 for accurate snake detection, (ii) using double thresholding to categorize the predicted bounding boxes, and (iii) the fusion of class membership vectors with the likelihood of location data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Evaluation metrics</head><p>Various metrics were used to measure the performance of classification. The ğ¹ 1 score is computed for the ğ‘–-th species as:</p><formula xml:id="formula_0" coords="3,236.64,465.86,269.34,27.66">ğ¹ ğ‘– 1 = 2 â€¢ precision ğ‘– â€¢ recall ğ‘– precision ğ‘– + recall ğ‘–<label>(1)</label></formula><p>The macro ğ¹ 1 is calculated by averaging the ğ¹ 1 values over all the species (ğ‘ is the number of species):</p><formula xml:id="formula_1" coords="3,253.77,529.80,252.21,33.71">macro ğ¹ 1 = ğ‘ âˆ‘ï¸ ğ‘–=1 ğ¹ ğ‘– 1 ğ‘<label>(2)</label></formula><p>Let ğ» ğ‘– be a set containing the indices of species that might appear in the ğ‘–-th country (these data were available, provided by the organizers). Then the country-averaged ğ¹ ğ‘ for the given country is defined as: The ğ¹ 1 country score is the average of the ğ¹ ğ‘ values for all countries (let their number be ğ‘€ ):</p><formula xml:id="formula_2" coords="3,263.23,608.61,242.76,33.77">ğ¹ ğ‘– ğ‘ = âˆ‘ï¸ ğ‘—âˆˆğ» ğ‘– ğ¹ ğ‘— 1 |ğ» ğ‘– |<label>(3)</label></formula><formula xml:id="formula_3" coords="4,250.06,234.78,255.92,33.71">ğ¹ 1 country = ğ‘€ âˆ‘ï¸ ğ‘–=1 ğ¹ ğ‘– ğ‘ ğ‘€<label>(4)</label></formula><p>The primary metric in the challenge is the ğ¹ 1 country score, and the secondary is the macro ğ¹ 1 value. For evaluating the models, we also use classification accuracy defined as:</p><formula xml:id="formula_4" coords="4,223.78,315.20,282.20,25.77">Accuracy = #correct predictions #samples (5)</formula><p>Categorical cross entropy is used both as an evaluation metric and as the function to be optimized during training the network (loss). If the output of the network for a sample is y and the ground truth is y ^, categorical cross entropy is calculated as:</p><formula xml:id="formula_5" coords="4,237.74,403.15,268.24,33.71">Î“(y ^, y) = - ğ‘ âˆ‘ï¸ ğ‘–=1 y ^i â€¢ log y i<label>(6)</label></formula><p>When evaluating multiple samples, their losses are averaged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Object detection</head><p>Object detection can improve snake classification accuracy as results from earlier rounds of the SnakeCLEF competition show. As mentioned in Section 2, top-scoring teams applied ResNet-50 and Mask R-CNN in round 4 (SnakeCLEF 2020). We conducted experiments with a lightweight SSD MobileNet v2 and a state-of-the-art EfficientDet-D1 model. More complex models (D2 to D7) might improve the results in the presence of sufficient training data; however, they are prone to overfitting. Table <ref type="table" coords="4,213.64,574.20,5.17,10.91" target="#tab_0">1</ref> shows the comparison of the described models. Both SSD and EfficientDet-D1 are one-stage networks, which generally run faster than the two-stage Mask R-CNN. A subset of the train and val datasets consisting of 4700 images was annotated with the help of the labelImg utility <ref type="bibr" coords="4,192.23,628.40,16.41,10.91" target="#b37">[38]</ref>. These samples were split in an 85% -15% ratio to construct a training and a validation input for the object detection models. The models were initialized with weights pre-trained on the COCO 2017 dataset and then fine-tuned on the annotated part of the SnakeCLEF dataset using the Tensorflow Object Detection API. Both models were trained to detect two classes (snake and background) and used with mostly the default settings. For data augmentation, only vertical flipping was used. The validation mAP's plateaued before reaching 30000 steps in both cases. The best results are shown in Table <ref type="table" coords="5,376.67,421.44,3.81,10.91" target="#tab_0">1</ref>. The EfficientDet recorded higher mAP; however, the difference is not as big as in the case of the COCO dataset, possibly because there are only two classes. Some examples of detected snakes are shown in Figure <ref type="figure" coords="5,494.53,448.54,3.74,10.91" target="#fig_0">1</ref>.</p><p>As the EfficientDet-D1 performed better, only this model was put to use in image classification. After running the inference on a picture, the model's output consists of the coordinates of bounding boxes and the ğ‘ 0 , ğ‘ 1 probabilities of the box containing background or snake, respectively. One image could contain multiple snakes (most likely of the same species) or none at all; therefore, our method of using the boxes (ordered descending by ğ‘ 1 ) is the following:</p><p>â€¢ If for the first box ğ‘ 1 â‰¥ ğ‘ â„ğ‘–ğ‘”â„ , then we take this, and the next boxes having ğ‘ 1 â‰¥ ğ‘ â„ğ‘–ğ‘”â„ (up to a maximum of three boxes, although there was not a single image in the dataset for which at least 3 boxes met this constraint for ğ‘ â„ğ‘–ğ‘”â„ = 0.75)<ref type="foot" coords="5,395.69,567.13,3.71,7.97" target="#foot_1">2</ref> . â€¢ If for the first box ğ‘ ğ‘™ğ‘œğ‘¤ â‰¤ ğ‘ 1 &lt; ğ‘ â„ğ‘–ğ‘”â„ we take only the first box.</p><p>â€¢ If ğ‘ 1 &lt; ğ‘ ğ‘™ğ‘œğ‘¤ for the first box, then we take the whole image. During training, we experimented with dropping such images, but it is not possible in the case of validation or testing. If multiple boxes are cropped from training or validation images, all of them are taken to the dataset with the ground truth label. In the case of testing, the classifier is run for all boxes, and the results are combined by first summing the prediction vectors and then normalizing it to a unit vector. More accurately, if the prediction vectors for the ğ‘› different boxes are: y 1 , y 2 , . . . , y n , then the combined prediction is y (where || â€¢ || means the Euclidean norm):</p><formula xml:id="formula_6" coords="6,260.12,411.97,245.87,30.21">y = âˆ‘ï¸€ ğ‘› ğ‘–=1 y i || âˆ‘ï¸€ ğ‘› ğ‘–=1 y i ||<label>(7)</label></formula><p>The EfficientDet detector was run on the whole dataset after training. Figure <ref type="figure" coords="6,452.20,448.87,5.17,10.91" target="#fig_1">2</ref> shows the cumulative distribution functions (CDF) of the ğ‘ 1 probabilities of the first bounding boxes on the test data and the combined training and validation datasets (trainval). The medians are 0.87 and 0.629, respectively. For the second bounding boxes, the medians are 0.068 and 0.146. Comparing these numbers with the assumption that the vast majority of the images contain a single snake, the network was more accurate on the test dataset.</p><p>Another conclusion is that the straightforward ğ‘ ğ‘™ğ‘œğ‘¤ = 0.5 choice (i.e., only keeping boxes for which the network predicts a higher probability of containing a snake than background) is not necessarily the best. For this value, 30% of the training images would be preserved without cropping, as the CDF shows. In most cases, these images contained a snake that is harder to recognize or has a bounding box with an unusual aspect ratio (as the network was fitting boxes with aspect ratios of 0.5, 1.0, and 2.0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Classification</head><p>During classification, intermediate results were evaluated on the validation dataset, while scores on the test set are only available for those attempts, which were submitted for the challenge and evaluated by the organizers. Accuracy, ğ¹ 1 country, and macro ğ¹ 1 scores were calculated in both cases (as defined in Section 3), while loss values are only available on the validation data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Preprocessing</head><p>The first preprocessing step in the case of Full-train networks (see Tables <ref type="table" coords="7,413.00,311.57,5.00,10.91" target="#tab_1">2</ref> and<ref type="table" coords="7,439.66,311.57,4.10,10.91" target="#tab_2">3</ref>) was running the EfficientDet object detector on the trainval dataset as described in Section 4, and saving the results in the TFRecord format. For validation and test datasets, ğ‘ ğ‘™ğ‘œğ‘¤ = 0.2 and ğ‘ â„ğ‘–ğ‘”â„ = 0.75 were used, while for training data, ğ‘ ğ‘™ğ‘œğ‘¤ = 0.5 was the default. Before running the network on a batch of images, some preprocessing steps were needed. The following steps were executed 3 each time an image was read into memory (the images in the dataset were not modified/overwritten): </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Models</head><p>The parameters used to train the different models are shown in Table <ref type="table" coords="7,387.79,541.20,4.97,10.91" target="#tab_1">2</ref> (note that EfficientNet-B0 networks were used in each case). The Baseline model, trained on a dataset of a reduced size, was provided by the organizers. Min-train was trained with the Tensorflow library using mostly the same parameters and the same dataset as the Baseline, without using object detection. The Full-train networks were trained on the complete training dataset with object detection included. The loss function was categorical cross entropy in all cases. Adam optimizer was used for training all the models with a learning rate decay from ğœ‚ ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ to ğœ‚ ğ‘’ğ‘›ğ‘‘ . The ğ›½ 1 = 0.9 and ğ›½ 2 = 0.999 values were used as the exponential decay rate for the first and second moment estimates, respectively. The parameter epochs shows the number of epochs before the results on the validation data plateaued, and the training was terminated. Some models started the training process with frozen layers, leaving only the classification head trainable to prevent the big initial gradients from destroying the pre-trained weights. However, all Batch Normalization layers were kept frozen in all the models even after unfreezing the others. This is a common technique suggested by Keras developers, in order to prevent the pre-trained weights from being destroyed <ref type="bibr" coords="8,273.61,311.67,16.30,10.91" target="#b38">[39,</ref><ref type="bibr" coords="8,292.56,311.67,12.23,10.91" target="#b39">40]</ref>, which is not preferable with a dataset of this size, due to the risk of overfitting. As Table <ref type="table" coords="8,141.44,338.77,5.07,10.91" target="#tab_2">3</ref> shows, the Full-train networks performed quite similarly. Full-train 1 converged to the worst optimum, possibly due to the low initial learning rate and the frozen layers in the beginning. Interestingly, the ğ¹ 1 scores were not inferior to the other networks on the validation data. This might be caused by some noise in the validation data, or another option is that the loss does not correlate well with the ğ¹ 1 scores. However, the networks with lower loss generally predict higher probability for the correct species <ref type="foot" coords="8,351.98,404.76,3.71,7.97" target="#foot_2">4</ref> , thus, they are a better candidate for reweighting using location data, as described in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Submission 1</head><p>The first submission included the Full-train 2 network. The learning rate was 0.001 in the first 6 epochs, then multiplied by 0.5 or 0.75 in alteration. During object detection, the parameters were ğ‘ ğ‘™ğ‘œğ‘¤ = 0.5 and ğ‘ â„ğ‘–ğ‘”â„ = 0.75, and images having a best bounding box under ğ‘ ğ‘™ğ‘œğ‘¤ were preserved without cropping. Despite recording the highest ğ¹ 1 country score on the validation data, the loss remained higher than in the next attempts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Submission 3</head><p>The Full-train 3 model was trained with a similar approach as in the case of Submission 1. A notable difference was that the batch size was increased to 128 and the initial learning rate to 0.003. On the validation data, this resulted in a major difference in the loss values, and a top 92.13% accuracy on the test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Submission 4</head><p>The fourth submission was the model Full-train 4. In this attempt, ğ‘ ğ‘™ğ‘œğ‘¤ was modified to 0.2 on the training data, and images falling below this value during object detection (5008 altogether) were dropped. The learning rate schedule followed the same pattern as in Submission 1, but started from a higher value of 0.01. The results were lower on the validation and the test set than those of the previous submission.</p><p>Submission <ref type="bibr" coords="9,151.27,267.21,5.72,9.77" target="#b4">5</ref> This submission included the Full-train 3 network (as Submission 3), but during the evaluation of the test set, object detection was not used. This attempt performed the worst in all metrics (see Table <ref type="table" coords="9,239.80,293.33,3.52,10.91" target="#tab_3">4</ref>), except accuracy on the test set. The difference from other submissions is higher on the validation set, which can be explained by the lower quality of the images, where the object detection might greatly improve the visibility of the snake.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Use of location data</head><p>The models described in Section 5.2 did not utilize the location data provided with the images. This information can be used to predict the class label independently, using frequentist statistics.</p><p>Let ğ‘Œ be the random variable of the image label (class) and ğ¶ of the country, then the probability that a sample belongs to class ğ‘– can be approximated from ğ¶ as: </p><formula xml:id="formula_7" coords="9,129.33,442.62,136.98,16.95">ğ‘ƒ (ğ‘Œ = ğ‘– | ğ¶ = ğ‘) â‰ˆ #images</formula><p>If the prediction of the neural network for the ğ‘–-th class is ğ‘¦ ğ‘– and the image is from country ğ‘, a new prediction can be created by multiplication with probabilities from the location data:</p><formula xml:id="formula_9" coords="9,236.33,515.23,269.66,14.19">ğ‘¦ * ğ‘– = ğ‘¦ ğ‘– â€¢ ğ‘ƒ (ğ‘Œ = ğ‘– | ğ¶ = ğ‘)<label>(9)</label></formula><p>Normalizing ğ‘¦ * to unit length gives the new prediction vector. However, one could apply Bayes' Theorem to split the probabilities in the following way:</p><formula xml:id="formula_10" coords="9,182.20,578.49,323.79,24.43">ğ‘ƒ (ğ‘Œ = ğ‘– | ğ¶ = ğ‘) = ğ‘ƒ (ğ¶ = ğ‘ | ğ‘Œ = ğ‘–) â€¢ ğ‘ƒ (ğ‘Œ = ğ‘–) ğ‘ƒ (ğ¶ = ğ‘)<label>(10)</label></formula><p>As ğ‘ƒ (ğ¶ = ğ‘) is only a normalizing constant (the same for all ğ‘–), and the prediction vectors are normalized to unit length anyway, it can be omitted. The ğ‘ƒ (ğ‘Œ = ğ‘–) factor is the prior probability of the ğ‘–-th class. It puts more weight on more frequent classes and less on the least frequent ones, which might not harm classification accuracy, but is clearly not preferable for macro ğ¹ 1 </p><p>The more training images are present for a class, the more accurate the approximation is. However, multiplication by 0 is not preferred, as (i) a snake of a particular species might appear anywhere with a small probability (e.g., a captive snake), (ii) for classes with only a few training samples, there is a high chance that we do not have samples from each country where the species is native. Therefore, a small ğœ€ constant is used:</p><formula xml:id="formula_12" coords="10,214.84,376.23,291.14,14.19">ğ‘¦ * ğ‘– = ğ‘¦ ğ‘– â€¢ max {ğ‘ƒ (ğ¶ = ğ‘ | ğ‘Œ = ğ‘–), ğœ€}<label>(12)</label></formula><p>The method was tested for the prediction of the Full-train 3 network, as this model had the lowest loss on the validation dataset. The predictions were evaluated for three different ğœ€ values: ğœ€ = 0 (submission 7), ğœ€ = 7 Ã— 10 -6 (submission 2) and ğœ€ = 7 Ã— 10 -4 (submission 6). The results are shown in Table <ref type="table" coords="10,178.98,438.67,3.81,10.91" target="#tab_4">5</ref>. Adding location data to the model increased all the metrics to a great extent, affecting ğ¹ 1 country the most, which saw an increase of 0.089 on the test data.</p><p>On the test data, there was another major increase in the ğ¹ 1 country score when the ğœ€ was not set to 0, while the macro ğ¹ 1 and accuracy scores also rose. The results did not differ much between ğœ€ = 7 Ã— 10 -4 and ğœ€ = 7 Ã— 10 -6 ; however, 7 Ã— 10 -6 seems to be better in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We elaborated an automated snake species identification system that first applies object detection to separate the snake(s) from the background then incorporates visual information and location metadata into a classification algorithm to categorize the detected snakes. EfficientDet and EfficientNet were used for object detection and classification, respectively. Based on our experiments, we can conclude that object detection can positively influence snake identification; moreover, the inclusion of geographical data showed further significant improvement. Our best submission achieved an ğ¹ 1 country score of 0.903 and almost 95% classification accuracy in the official evaluation of SnakeCLEF 2021.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,89.29,334.16,417.79,9.96;5,89.29,346.12,418.35,9.96;5,89.29,358.07,13.90,9.96;5,126.31,213.32,170.08,114.30"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Snakes detected by the EfficientDet model. Â©Lili, iNaturalist, CC-BY-NC; Â©Gerson Herrera, iNaturalist, CC-BY-NC; Â©oranglautan, iNaturalist, CC-BY-NC; Â©Vipul Ramanuj, iNaturalist, CC-BY-NC</figDesc><graphic coords="5,126.31,213.32,170.08,114.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,89.29,303.75,219.46,10.32;6,311.74,303.75,123.85,9.96;6,141.73,84.19,311.82,207.88"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Cumulative distribution functions of the ğ‘ 1 probabilities of the first boxes.</figDesc><graphic coords="6,141.73,84.19,311.82,207.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,103.64,412.67,187.10,10.91;7,103.64,427.39,338.97,10.91;7,97.78,442.11,267.12,10.91;7,97.78,456.83,243.25,10.91;7,102.63,471.55,404.55,10.91;7,116.56,485.10,255.00,10.91"><head>1 . 5 .</head><label>15</label><figDesc>grayscale to RGB conversion, if needed 2. Rescaling to 224 * 224 * 3 (the standard input format for EfficientNet-B0) â€ 3. Vertical and horizontal flip with probabilities of ğ‘ = 0.5 â€ 4. Brightness and contrast modification with ğ‘ = 0.2 Float conversion. (scaling from [0, 255] to [0.0, 1.0] and normalization were not executed, as these are part of the EfficientNet model in Keras [39])</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,269.04,441.89,195.71,10.91;9,249.87,456.75,191.09,10.91"><head></head><label></label><figDesc>in the training set from class ğ‘– and country ğ‘ #images in the training set from country ğ‘</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.99,90.49,328.98,93.94"><head>Table 1</head><label>1</label><figDesc>Comparison of object detection models</figDesc><table coords="4,177.30,121.17,240.67,63.26"><row><cell>Model</cell><cell>COCO mAP</cell><cell cols="3">SnakeCLEF mAP AP@0.5 AP@0.75</cell></row><row><cell>Mask R-CNN</cell><cell>39.0</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>SSD MobileNet v2</cell><cell>20.2</cell><cell>51.2</cell><cell>90.0</cell><cell>53.4</cell></row><row><cell>EfficientDet-D1</cell><cell>38.4</cell><cell>56.9</cell><cell>91.0</cell><cell>64.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,88.99,90.49,408.29,131.80"><head>Table 2</head><label>2</label><figDesc>Parameters (with ğœ‚ meaning the learning rate) and properties of the trained EfficientNet-B0 models</figDesc><table coords="7,99.42,123.16,393.95,99.13"><row><cell></cell><cell>ğœ‚ ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡</cell><cell>ğœ‚ ğ‘’ğ‘›ğ‘‘</cell><cell>batch size</cell><cell>checkpoint</cell><cell>frozen layers</cell><cell>epochs</cell></row><row><cell>Baseline</cell><cell>0.01</cell><cell>1 Ã— 10 -5</cell><cell>64</cell><cell>ImageNet-1k</cell><cell>-</cell><cell>15</cell></row><row><cell>Min-train</cell><cell>0.01</cell><cell>1 Ã— 10 -4</cell><cell>64</cell><cell>ImageNet-1k</cell><cell>5 epochs</cell><cell>15</cell></row><row><cell cols="2">Full-train 1 1 Ã— 10 -4</cell><cell>8 Ã— 10 -5</cell><cell>64</cell><cell></cell><cell>4 epochs</cell><cell>12</cell></row><row><cell>Full-train 2</cell><cell>0.001</cell><cell>1 Ã— 10 -4</cell><cell>64</cell><cell>Min-train</cell><cell>only BatchNorm</cell><cell>15</cell></row><row><cell>Full-train 3</cell><cell>0.003</cell><cell>1 Ã— 10 -4</cell><cell>128</cell><cell>Min-train</cell><cell>only BatchNorm</cell><cell>10</cell></row><row><cell>Full-train 4</cell><cell>0.01</cell><cell>3.5 Ã— 10 -5</cell><cell>128</cell><cell>Min-train</cell><cell>only BatchNorm</cell><cell>14</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,88.99,90.49,404.62,127.58"><head>Table 3</head><label>3</label><figDesc>Evaluation of the EfficientNet-B0 models on the validation and test datasets validation test accuracy macro ğ¹ 1 ğ¹ 1 country loss accuracy macro ğ¹ 1 ğ¹ 1 country</figDesc><table coords="8,99.18,148.33,383.05,69.74"><row><cell>Baseline</cell><cell>44.7%</cell><cell>0.327</cell><cell>-</cell><cell>2.340</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Min-train</cell><cell>42.8%</cell><cell>0.294</cell><cell>0.276</cell><cell>3.132</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Full-train 1</cell><cell>69.5%</cell><cell>0.512</cell><cell>0.513</cell><cell>1.577</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Full-train 2</cell><cell>70.0%</cell><cell>0.512</cell><cell>0.513</cell><cell>1.530</cell><cell>85.86%</cell><cell>0.738</cell><cell>0.773</cell></row><row><cell>Full-train 3</cell><cell>70.4%</cell><cell>0.515</cell><cell>0.496</cell><cell>1.253</cell><cell>92.13%</cell><cell>0.786</cell><cell>0.789</cell></row><row><cell>Full-train 4</cell><cell>68.3%</cell><cell>0.498</cell><cell>0.488</cell><cell>1.344</cell><cell>85.55%</cell><cell>0.737</cell><cell>0.752</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,88.99,90.49,415.00,105.90"><head>Table 4</head><label>4</label><figDesc>Comparison of the submissions not using location data, evaluated on the validation and test datasets validation test accuracy macro ğ¹ 1 ğ¹ 1 country loss accuracy macro ğ¹ 1 ğ¹ 1 country</figDesc><table coords="9,96.18,150.55,391.54,45.83"><row><cell>Submission 1</cell><cell>70.0%</cell><cell>0.512</cell><cell>0.513</cell><cell>1.530</cell><cell>85.86%</cell><cell>0.738</cell><cell>0.773</cell></row><row><cell>Submission 3</cell><cell>70.4%</cell><cell>0.515</cell><cell>0.496</cell><cell>1.253</cell><cell>92.13%</cell><cell>0.786</cell><cell>0.789</cell></row><row><cell>Submission 4</cell><cell>68.3%</cell><cell>0.498</cell><cell>0.488</cell><cell>1.344</cell><cell>85.55%</cell><cell>0.737</cell><cell>0.752</cell></row><row><cell>Submission 5</cell><cell>60.6%</cell><cell>0.425</cell><cell>0.416</cell><cell>1.662</cell><cell>87.78%</cell><cell>0.705</cell><cell>0.727</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,88.99,90.49,418.27,194.95"><head>Table 5</head><label>5</label><figDesc>Evaluation of the predictions with location data for different ğœ€ values validation test accuracy macro ğ¹ 1 ğ¹ 1 country loss accuracy macro ğ¹ 1 ğ¹ 1 country ğ¹ 1 scores, which put equal weights on classes. Therefore, the ğ‘ƒ (ğ¶ = ğ‘ | ğ‘Œ = ğ‘–) likelihood remains, which is approximated as: ğ‘ƒ (ğ¶ = ğ‘ | ğ‘Œ = ğ‘–) â‰ˆ #images in the training set from class ğ‘– and country ğ‘ #images in the training set from class ğ‘–</figDesc><table coords="10,89.29,150.55,399.13,84.28"><row><cell>Full-train 3</cell><cell>70.40%</cell><cell>0.515</cell><cell>0.496</cell><cell>1.253</cell><cell>92.13%</cell><cell>0.786</cell><cell>0.789</cell></row><row><cell>ğœ€ = 0</cell><cell>75.90%</cell><cell>0.652</cell><cell>0.680</cell><cell>0.951</cell><cell>94.39%</cell><cell>0.840</cell><cell>0.878</cell></row><row><cell>ğœ€ = 7 Ã— 10 -6</cell><cell>75.94%</cell><cell>0.654</cell><cell>0.685</cell><cell>0.930</cell><cell>94.82%</cell><cell>0.855</cell><cell>0.903</cell></row><row><cell>ğœ€ = 7 Ã— 10 -4</cell><cell>75.66%</cell><cell>0.644</cell><cell>0.669</cell><cell>0.941</cell><cell>94.94%</cell><cell>0.864</cell><cell>0.901</cell></row><row><cell>and country</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,108.93,671.00,219.99,8.97"><p>Datasets are available at: https://cocodataset.org/#download</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,108.93,660.08,397.06,8.97;5,89.29,671.04,165.07,8.97"><p>The reason for this is that one-phase object detection networks use non-max suppression to prevent overlapping boxes containing the same class in the result.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="8,108.93,649.12,397.05,8.97;8,89.29,660.08,418.07,8.97;8,88.98,671.04,274.83,8.97"><p>Considering categorical cross entropy loss, the y ^vector contains a single nonzero element referring to the correct class, where the value is 1. Substituting into the formula, the loss is -log y i , if the ground truth class is ğ‘–. As y i â‰¤ 1, the -log y i loss value decreases, as the y i prediction increases.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,112.66,111.28,394.03,10.91;11,112.66,124.83,238.73,10.91" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="11,186.89,111.28,168.20,10.91">Snakebite envenoming -Key Facts 2021</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">H O</forename><surname>Who</surname></persName>
		</author>
		<ptr target="https://www.who.int/news-room/fact-sheets/detail/snakebite-envenoming" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,138.38,394.53,10.91;11,112.66,151.93,393.32,10.91;11,112.66,165.48,395.01,10.91;11,112.66,179.03,268.45,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,112.66,151.93,393.32,10.91;11,112.66,165.48,243.00,10.91">Identifying the snake: First scoping review on practices of communities and healthcare providers confronted with snakebite across the world</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Botero</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Mesa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alcoba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Chappuis</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ruiz De CastaÃ±eda</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0229989</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0229989" />
	</analytic>
	<monogr>
		<title level="j" coord="11,366.45,165.48,50.14,10.91">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">229989</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,192.57,394.53,10.91;11,112.66,206.12,393.53,10.91;11,112.66,219.67,393.33,10.91;11,112.41,233.22,351.65,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,112.66,206.12,393.53,10.91;11,112.66,219.67,84.88,10.91">Identifying the biting species in snakebite by clinical features: an epidemiological tool for community surveys</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pathmeswaran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kasturiratne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fonseka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nandasena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lalloo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">De</forename><surname>Silva</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.trstmh.2005.10.003</idno>
		<ptr target="https://doi.org/10.1016/j.trstmh.2005.10.003" />
	</analytic>
	<monogr>
		<title level="j" coord="11,206.04,219.67,299.94,10.91">Transactions of the Royal Society of Tropical Medicine and Hygiene</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="874" to="878" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,246.77,395.17,10.91;11,112.66,260.32,393.33,10.91;11,112.66,273.87,394.52,10.91;11,112.66,287.42,306.41,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,236.98,260.32,269.00,10.91;11,112.66,273.87,205.25,10.91">Snakebite and snake identification: empowering neglected communities and health-care providers with ai</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>De CastaÃ±eda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>FernÃ¡ndez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Alcoba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chappuis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>SalathÃ©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<idno type="DOI">10.1016/s2589-7500(19)30086-x</idno>
		<ptr target="https://doi.org/10.1016/s2589-7500(19)30086-x" />
	</analytic>
	<monogr>
		<title level="j" coord="11,325.25,273.87,116.41,10.91">THe Lancet Digital Health</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="202" to="e203" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,300.97,394.62,10.91;11,112.28,314.52,393.70,10.91;11,112.66,328.07,288.62,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,376.16,300.97,131.11,10.91;11,112.28,314.52,288.08,10.91">Overview of snakeclef 2021: Automatic snake species identification with country-level focus</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De CastaÃ±eda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,425.01,314.52,80.97,10.91;11,112.66,328.07,257.93,10.91">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,341.62,393.33,10.91;11,112.66,355.17,394.53,10.91;11,112.66,368.71,393.32,10.91;11,112.66,382.26,393.33,10.91;11,112.66,395.81,306.11,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,199.20,368.71,306.78,10.91;11,112.66,382.26,249.83,10.91">Overview of lifeclef 2021: a system-oriented evaluation of automated species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De CastaÃ±eda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">H</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Isabelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>PlanquÃ©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dorso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>MÃ¼ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,385.48,382.26,120.51,10.91;11,112.66,395.81,276.37,10.91">Proceedings of the Twelfth International Conference of the CLEF Association (CLEF 2021)</title>
		<meeting>the Twelfth International Conference of the CLEF Association (CLEF 2021)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,409.36,393.54,10.91;11,112.66,422.91,393.33,10.91;11,112.33,436.46,312.30,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,240.42,409.36,265.78,10.91;11,112.66,422.91,183.82,10.91">A survey of deep learning methods and software tools for image classification and object detection</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Druzhkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kustikova</surname></persName>
		</author>
		<idno type="DOI">10.1134/s1054661816010065</idno>
		<ptr target="https://doi.org/10.1134/s1054661816010065" />
	</analytic>
	<monogr>
		<title level="j" coord="11,307.24,422.91,185.06,10.91">Pattern Recognition and Image Analysis</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="9" to="15" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,450.01,393.71,10.91;11,112.66,463.56,394.51,10.91;11,112.36,479.55,118.19,7.90" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,212.35,450.01,294.02,10.91;11,112.66,463.56,95.44,10.91">Deep convolutional neural networks for image classification: A comprehensive review</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco_a_00990</idno>
		<ptr target="https://doi.org/10.1162/neco_a_00990" />
	</analytic>
	<monogr>
		<title level="j" coord="11,216.39,463.56,87.64,10.91">Neural computation</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2352" to="2449" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,490.66,393.33,10.91;11,112.66,504.21,226.28,10.91" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m" coord="11,247.99,490.66,258.00,10.91;11,112.66,504.21,49.16,10.91">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,517.76,394.53,10.91;11,112.28,531.30,393.70,10.91;11,112.66,544.85,394.51,10.91;11,112.66,560.85,97.48,7.90" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,179.05,531.30,137.57,10.91">Going deeper with convolutions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2015.7298594</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2015.7298594" />
	</analytic>
	<monogr>
		<title level="m" coord="11,338.68,531.30,167.30,10.91;11,112.66,544.85,173.84,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,571.95,395.17,10.91;11,112.66,585.50,395.01,10.91;11,112.41,599.05,262.52,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,259.74,571.95,203.38,10.91">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.90</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2016.90" />
	</analytic>
	<monogr>
		<title level="m" coord="11,488.38,571.95,19.45,10.91;11,112.66,585.50,347.24,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,612.60,394.53,10.91;11,112.66,626.15,352.61,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,178.42,612.60,323.86,10.91">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,127.29,626.15,207.49,10.91">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,639.70,394.53,10.91;11,112.66,653.25,395.16,10.91;11,112.66,666.80,202.19,10.91" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m" coord="11,158.75,653.25,349.08,10.91;11,112.66,666.80,20.39,10.91">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,86.97,393.33,10.91;12,112.66,100.52,395.17,10.91;12,112.66,114.06,394.51,10.91;12,112.66,130.06,67.81,7.90" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,311.82,86.97,194.16,10.91;12,112.66,100.52,167.28,10.91">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2014.81</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2014.81" />
	</analytic>
	<monogr>
		<title level="m" coord="12,305.13,100.52,202.71,10.91;12,112.66,114.06,154.65,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,141.16,393.54,10.91;12,112.39,154.71,352.48,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,167.48,141.16,42.22,10.91">Fast r-cnn</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2015.169</idno>
		<ptr target="https://doi.org/10.1109/iccv.2015.169" />
	</analytic>
	<monogr>
		<title level="m" coord="12,232.40,141.16,273.80,10.91;12,112.39,154.71,25.20,10.91">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,168.26,393.33,10.91;12,112.66,181.81,394.51,10.91;12,112.36,197.80,147.99,7.90" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2016.2577031</idno>
		<idno type="arXiv">arXiv:1506.01497</idno>
		<ptr target="https://doi.org/10.1109/tpami.2016.2577031" />
		<title level="m" coord="12,264.19,168.26,241.80,10.91;12,112.66,181.81,108.27,10.91">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,208.91,393.33,10.91;12,112.66,222.46,394.51,10.91;12,112.36,238.45,118.31,7.90" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,313.56,208.91,50.90,10.91">Mask r-cnn</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.322</idno>
		<ptr target="https://doi.org/10.1109/iccv.2017.322" />
	</analytic>
	<monogr>
		<title level="m" coord="12,394.00,208.91,111.98,10.91;12,112.66,222.46,190.14,10.91">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,249.56,393.53,10.91;12,112.66,263.11,393.98,10.91;12,112.66,276.66,38.81,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,428.49,249.56,77.70,10.91;12,112.66,263.11,153.06,10.91">Deep learning for generic object detection: A survey</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Fieguth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>PietikÃ¤inen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,274.69,263.11,183.42,10.91">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="261" to="318" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,290.20,393.33,10.91;12,112.66,303.75,393.33,10.91;12,112.66,317.30,360.63,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,330.35,290.20,175.63,10.91;12,112.66,303.75,68.89,10.91">You only look once: Unified, real-time object detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.91</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2016.91" />
	</analytic>
	<monogr>
		<title level="m" coord="12,204.50,303.75,301.49,10.91;12,112.66,317.30,49.16,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,330.85,393.33,10.91;12,112.66,344.40,396.29,10.91;12,112.07,360.39,174.76,7.90" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="12,222.35,330.85,145.98,10.91">Yolo9000: better, faster, stronger</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.690</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2017.690" />
	</analytic>
	<monogr>
		<title level="m" coord="12,395.28,330.85,110.71,10.91;12,112.66,344.40,243.64,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,371.50,110.63,10.91;12,238.41,371.50,184.46,10.91;12,437.99,371.50,68.00,10.91;12,112.66,385.05,107.17,10.91" xml:id="b20">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m" coord="12,238.41,371.50,179.18,10.91">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,398.60,393.33,10.91;12,112.66,412.15,221.40,10.91" xml:id="b21">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-Y</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10934</idno>
		<title level="m" coord="12,304.83,398.60,201.15,10.91;12,112.66,412.15,39.31,10.91">Yolov4: Optimal speed and accuracy of object detection</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,425.70,393.32,10.91;12,112.66,439.25,395.00,10.91;12,112.66,452.79,270.32,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="12,435.57,425.70,70.42,10.91;12,112.66,439.25,75.40,10.91">Ssd: Single shot multibox detector</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46448-0_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46448-0_2" />
	</analytic>
	<monogr>
		<title level="m" coord="12,210.63,439.25,178.76,10.91">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,466.34,394.61,10.91;12,112.66,479.89,395.01,10.91;12,112.66,493.44,226.91,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="12,327.53,466.34,159.84,10.91">Focal loss for dense object detection</title>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.324</idno>
		<ptr target="https://doi.org/10.1109/iccv.2017.324" />
	</analytic>
	<monogr>
		<title level="m" coord="12,112.66,479.89,299.48,10.91">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,506.99,395.17,10.91;12,112.66,520.54,395.01,10.91;12,112.41,534.09,330.28,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="12,227.21,506.99,219.17,10.91">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.01079</idno>
		<ptr target="https://doi.org/10.1109/cvpr42600.2020.01079" />
	</analytic>
	<monogr>
		<title level="m" coord="12,469.11,506.99,38.72,10.91;12,112.66,520.54,347.82,10.91">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10781" to="10790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,547.64,394.62,10.91;12,112.28,561.19,250.25,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="12,351.32,547.64,129.42,10.91">Overview of the SnakeCLEF</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>De CastaÃ±eda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,112.28,561.19,218.92,10.91">Automatic Snake Species Identification Challenge</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,574.74,395.17,10.91;12,112.66,588.29,393.33,10.91;12,112.66,601.84,361.83,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="12,363.97,574.74,143.86,10.91;12,112.66,588.29,191.87,10.91">Discriminative histogram taxonomy features for snake species identification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mathews</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sugathan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">K</forename><surname>Raveendran</surname></persName>
		</author>
		<idno type="DOI">10.1186/s13673-014-0003-0</idno>
		<ptr target="https://doi.org/10.1186/s13673-014-0003-0" />
	</analytic>
	<monogr>
		<title level="j" coord="12,312.09,588.29,193.90,10.91;12,112.66,601.84,37.51,10.91">Human-Centric Computing and Information Sciences</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,615.39,393.33,10.91;12,112.66,628.93,393.32,10.91;12,112.66,642.48,218.25,10.91" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="12,320.46,615.39,185.53,10.91;12,112.66,628.93,124.34,10.91">Image classification for snake species using machine learning techniques</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">A H</forename><surname>Zahri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Yaakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Ahmad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,259.88,628.93,246.10,10.91;12,112.66,642.48,97.41,10.91">International Conference on Computational Intelligence in Information System</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="52" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,656.03,393.33,10.91;12,112.66,669.58,393.33,10.91;13,112.41,86.97,283.59,10.91" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="12,444.88,656.03,61.11,10.91;12,112.66,669.58,348.36,10.91">Revealing the unknown: real-time recognition of galÃ¡pagos snake species using deep learning</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Khatod</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Matijosaitiene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arteaga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Gilkey</surname></persName>
		</author>
		<idno type="DOI">10.3390/ani10050806</idno>
		<ptr target="https://doi.org/10.3390/ani10050806" />
	</analytic>
	<monogr>
		<title level="j" coord="12,469.28,669.58,36.71,10.91">Animals</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">806</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,100.52,393.33,10.91;13,112.66,114.06,393.33,10.91;13,112.66,127.61,395.01,10.91;13,112.66,141.16,292.19,10.91" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="13,309.99,100.52,196.00,10.91;13,112.66,114.06,166.83,10.91">Image-based classification of snake species using convolutional neural network</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">S</forename><surname>Abdurrazaq</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Suyanto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">Q</forename><surname>Utama</surname></persName>
		</author>
		<idno type="DOI">10.1109/isriti48646.2019.9034633</idno>
		<ptr target="https://doi.org/10.1109/isriti48646.2019.9034633" />
	</analytic>
	<monogr>
		<title level="m" coord="13,342.08,114.06,163.90,10.91;13,112.66,127.61,277.26,10.91">International Seminar on Research of Information Technology and Intelligent Systems (ISRITI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,154.71,393.33,10.91;13,112.66,168.26,396.29,10.91;13,112.07,184.25,186.62,7.90" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="13,198.76,154.71,232.27,10.91">Snake detection and classification using deep learning</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sinnott</surname></persName>
		</author>
		<idno type="DOI">10.24251/hicss.2021.148</idno>
		<ptr target="https://doi.org/10.24251/hicss.2021.148" />
	</analytic>
	<monogr>
		<title level="m" coord="13,452.94,154.71,53.05,10.91;13,112.66,168.26,277.65,10.91">Proceedings of the 54th Hawaii International Conference on System Sciences</title>
		<meeting>the 54th Hawaii International Conference on System Sciences</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">1212</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,195.36,393.33,10.91;13,112.66,208.91,393.32,10.91;13,112.66,222.46,335.32,10.91" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="13,366.55,195.36,139.44,10.91;13,112.66,208.91,155.44,10.91">Signature verification using a&quot; siamese&quot; time delay neural network</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>SÃ¤ckinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
		<idno type="DOI">10.1142/s0218001493000339</idno>
		<ptr target="https://doi.org/10.1142/s0218001493000339" />
	</analytic>
	<monogr>
		<title level="j" coord="13,277.01,208.91,228.97,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="737" to="744" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,236.01,395.17,10.91;13,112.66,249.56,395.01,10.91;13,112.66,263.11,224.01,10.91" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="13,220.76,236.01,287.07,10.91;13,112.66,249.56,132.46,10.91">Double-view matching network for few-shot learning to classify covid-19 in x-ray images</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>SzÅ±cs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>NÃ©meth</surname></persName>
		</author>
		<idno type="DOI">10.36244/icj.2021.1.4</idno>
		<ptr target="https://doi.org/10.36244/icj.2021.1.4" />
	</analytic>
	<monogr>
		<title level="j" coord="13,257.71,249.56,171.93,10.91">INFOCOMMUNICATIONS JOURNAL</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="26" to="34" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,276.66,394.61,10.91;13,112.66,290.20,394.52,10.91;13,112.66,303.75,309.77,10.91" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="13,273.26,276.66,215.28,10.91">Snake image classification using siamese networks</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Abeysinghe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Welivita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Perera</surname></persName>
		</author>
		<idno type="DOI">10.1145/3338472.3338476</idno>
		<ptr target="https://doi.org/10.1145/3338472.3338476" />
	</analytic>
	<monogr>
		<title level="m" coord="13,112.66,290.20,389.99,10.91">Proceedings of the 2019 3rd International Conference on Graphics and Signal Processing</title>
		<meeting>the 2019 3rd International Conference on Graphics and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,317.30,393.33,10.91;13,112.66,330.85,394.53,10.91;13,112.66,344.40,338.81,10.91" xml:id="b34">
	<analytic>
		<title level="a" type="main" coord="13,256.41,317.30,249.58,10.91;13,112.66,330.85,42.98,10.91">Snake bite classification using chain code and k nearest neighbour</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Putra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">Q</forename><surname>Utama</surname></persName>
		</author>
		<idno type="DOI">10.1088/1742-6596/1192/1/012015</idno>
		<ptr target="https://doi.org/10.1088/1742-6596/1192/1/012015" />
	</analytic>
	<monogr>
		<title level="j" coord="13,179.05,330.85,165.10,10.91">Journal of Physics: Conference Series</title>
		<imprint>
			<biblScope unit="volume">1192</biblScope>
			<biblScope unit="page">12015</biblScope>
			<date type="published" when="2019">2019</date>
			<publisher>IOP Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,357.95,395.17,10.91;13,112.66,371.50,394.62,10.91;13,112.66,385.05,394.70,10.91;13,112.66,398.60,253.39,10.91" xml:id="b35">
	<monogr>
		<author>
			<persName coords=""><surname>Gokulakrishnan</surname></persName>
		</author>
		<ptr target="https://medium.com/@Stormblessed/diving-into-deep-learning-part-3-a-deep-learning-practitioners-attempt-to-build-state-of-the-2460292bcfb" />
		<title level="m" coord="13,196.75,357.95,311.09,10.91;13,112.66,371.50,335.16,10.91">Diving into deep learning -part 3 -a deep learning practitioner&apos;s attempt to build state of the art snake-species image classifier</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,412.15,394.53,10.91;13,112.66,425.70,393.33,10.91;13,112.66,439.25,393.33,10.91;13,112.66,452.79,291.28,10.91" xml:id="b36">
	<analytic>
		<title level="a" type="main" coord="13,238.82,425.70,267.17,10.91;13,112.66,439.25,275.39,10.91">Combination of image and location information for snake species identification using object detection and efficientnets</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Boketta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Keibel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mense</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Michailutschenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>RÃ¼ckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Willemeit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,412.65,439.25,93.34,10.91;13,112.66,452.79,259.36,10.91">CLEF: Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>CLEF working notes 2020</note>
</biblStruct>

<biblStruct coords="13,112.66,466.34,303.19,10.91" xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Labelimg</forename><surname>Tzutalin</surname></persName>
		</author>
		<ptr target="https://github.com/tzutalin/labelImg" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,479.89,394.61,10.91;13,112.66,493.44,354.53,10.91" xml:id="b38">
	<monogr>
		<title level="m" type="main" coord="13,112.66,479.89,339.39,10.91">Keras documentation: Image classification via fine-tuning with EfficientNet</title>
		<ptr target="https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,506.99,394.03,10.91;13,112.66,520.54,83.24,10.91" xml:id="b39">
	<analytic>
		<title level="a" type="main" coord="13,211.74,506.99,136.72,10.91">Transfer learning &amp; fine-tuning</title>
		<ptr target="https://keras.io/guides/transfer_learning/" />
	</analytic>
	<monogr>
		<title level="m" coord="13,112.66,506.99,90.99,10.91">Keras documentation</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
