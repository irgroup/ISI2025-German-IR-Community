<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,378.14,15.42;1,89.29,106.66,188.08,15.42">Recognizing bird species in diverse soundscapes under weak supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,78.17,11.96"><forename type="first">Christof</forename><surname>Henkel</surname></persName>
							<email>chenkel@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,185.40,134.97,68.83,11.96"><forename type="first">Pascal</forename><surname>Pfeiffer</surname></persName>
							<email>pascal.pfeiffer1@rwth-aachen.de</email>
							<affiliation key="aff1">
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,290.52,134.97,69.36,11.96"><forename type="first">Philipp</forename><surname>Singer</surname></persName>
							<email>philipp.singer@h2o.ai</email>
							<affiliation key="aff2">
								<orgName type="department">H2O.ai</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,378.14,15.42;1,89.29,106.66,188.08,15.42">Recognizing bird species in diverse soundscapes under weak supervision</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">4D6D8121761A6756CE86EDD41BF47612</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>BirdCLEF2021</term>
					<term>LifeCLEF</term>
					<term>audio</term>
					<term>bioacoustics</term>
					<term>fine-grained classification</term>
					<term>bird recognition</term>
					<term>Kaggle</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a robust classification approach for avian vocalization in complex and diverse soundscapes, achieving second place in the BirdCLEF2021 challenge. We illustrate how to make full use of pre-trained convolutional neural networks, by using an efficient modeling and training routine supplemented by novel augmentation methods. Thereby, we improve the generalization of weakly labeled crowd-sourced data to productive data collected by autonomous recording units. As such, we illustrate how to progress towards an accurate automated assessment of avian population which would enable global biodiversity monitoring at scale, impossible by manual annotation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Avian species are important members of our ecosystem providing regulating services like pollinating plants, dispersing seeds and controlling pests. To understand the functioning of a local ecosystems, it is important to detect and identify individual bird species. Since manual monitoring of avian species is impossible at a large scale it is crucial to develop autonomous systems for monitoring and classification.</p><p>The yearly Bird Recognition challenge (BirdCLEF) puts attention on the reliable detection of avian vocalizations in various soundscapes. The BirdCLEF2021 edition <ref type="bibr" coords="1,354.22,419.59,11.85,9.96" target="#b0">[1]</ref> as part of LifeCLEF2021 <ref type="bibr" coords="1,466.07,419.59,10.47,9.96" target="#b1">[2,</ref><ref type="bibr" coords="1,479.03,419.59,6.86,9.96" target="#b2">3,</ref><ref type="bibr" coords="1,488.39,419.59,6.86,9.96" target="#b3">4,</ref><ref type="bibr" coords="1,497.75,419.59,8.24,9.96" target="#b4">5]</ref> especially focuses on generalization of audio data from the crowd-sourced bird sound database xeno-canto <ref type="bibr" coords="1,89.29,443.50,11.73,9.96" target="#b5">[6]</ref> towards analyzing complex and long soundscape recordings generated in different contexts.</p><p>The goal of this year's competition is to predict the presence of individual bird species in five second intervals for recordings across four distinct locations in North and South America. Contrarily, the provided training data only contains short recordings of individual bird calls extracted from user uploads on the xeno-canto platform. Consequently, competitors had to address a weak label problem to bridge the gap between training and testing data discrepancy.</p><p>In this work, we discuss an efficient model architecture and training routine as part of the second place solution of the BirdCLEF2021 competition. Our solution builds upon a robust bootstrapped validation setup, and consists of an ensemble of convolutional neural networks (CNN) trained on extracted spectrograms of short audio clips. Our developed modeling architecture successfully bridges the gap between the weakly labeled crowd-sourced training data and the more accurately hand-labeled test data with hard labels at five second intervals. To further account for label noise in training data, we employ several augmentation and training methods such as novel mixup variations, the addition of background noise, or label smoothing. Final predictions are supplemented by a binary bird presence classifier and several post processing steps. Implementation details can be found on github 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Competition Data</head><p>The final test set of this competition includes approximately 80 soundscape recordings across the four unique locations Columbia (COL), Costa Rica (COR), Sierra Nevada (SNE) in California, USA and Sapsucker Woods area (SSW) in New York, USA. Each recording lasts ten minutes and predictions had to be provided for each five second interval of each audio file addressing the presence or absence of individual bird species. Additional meta data including the time of recording as well as the approximate coordinates of the location is provided for the test data.</p><p>Contrarily, the training data only contains weak labels for around 63, 000 short recordings of individual bird calls shared by users on the xeno-canto platform. Overall, 397 bird species are captured in this training set, out of which only an unknown set is also present in the test set. For each recording, a primary label is specified, as well as an optional list of secondary labels. Additional meta data including the date of recording, geo-location, the user uploading the data, as well as a quality rating from 0 -5, where 5 indicates best quality, is present.</p><p>To allow for better local validation of models, the competition data also includes a separate validation soundscape dataset which is similarly structured as the final test dataset. However, it includes only 20 soundscape files across two out of the four test data locations and does not necessarily capture the same bird species as the final test set. Yet, it allows to roughly bridge the gap between the difference of weak labels used for training, and hard labels used for testing, and allows for better validation as discussed in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">External Data</head><p>To improve the generalization of any model trained on the crowd-sourced database xeno-canto and apply it to new unseen data we augmented the training data with background noise not containing avian calls. In particular, we used two additional external datasets freefield1010 <ref type="bibr" coords="2,367.88,403.17,11.58,9.96" target="#b6">[7]</ref> and BirdVox-DCASE-20k <ref type="bibr" coords="2,494.40,403.17,11.58,9.96" target="#b7">[8]</ref> which had binary labels for the presence/absence of bird sounds and were advertised in the DCASE2018 Challenge <ref type="bibr" coords="2,132.55,427.08,10.42,9.96" target="#b8">[9]</ref>. Moreover, we extracted consecutive 30 second clips containing no avian sound from the validation set of the previous BirdCLEF2020 competition <ref type="bibr" coords="2,321.56,439.04,16.36,9.96" target="#b9">[10]</ref> which was hosted on aicrowd<ref type="foot" coords="2,463.77,437.66,3.24,6.97" target="#foot_0">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Validation</head><p>In Kaggle code competitions, the test dataset is hidden in such a way, that only very limited information can be gathered about it other than the public leaderboard score, which resembles the score for 35% of the total test data described in Section 2, also called public test data. The final score is calculated on the remaining 65% of the data, also called private test data. In this particular machine learning competition, the training data is very different to the test data, as the audio files only contain weak labels without time information and differ in length. Nevertheless, for two of the four test sites, a small validation set resembling the test set is provided by the hosts of the competition to give competitors a general understanding of the test data's nature. The metric of choice according to which competitors are evaluated in this competition is the row-wise micro averaged F1 score.</p><p>Consequently, our models are only trained on the short training clips, and locally evaluated on the provided validation soundscapes. This validation scheme was further improved by omitting three full soundscapes (out of 20) that did not contain any bird calls at all. In the following, this validation scheme is referred to as CV-3. To receive an even more robust feedback from validation, a bootstrap sampling method was introduced. Our final validation setup included the following steps:</p><p>‚Ä¢ Remove three songs without calls.</p><p>‚Ä¢ For k times (e.g., 10) sample 80% of the remaining songs-this should emulate the full test dataset (public+private). ‚Ä¢ Apply any kind of threshold selection technique or post processing on this data resembling the application in inference for the full public and private test data. ‚Ä¢ For j times (e.g., 50) sample 65% of the remaining songs-this should emulate the private test dataset. ‚Ä¢ Calculate the score on each of these j samples.</p><p>‚Ä¢ Report average, median, min, max, std scores across all k times j (e.g., 500) subsets</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Modeling</head><p>Our final solution is a an ensemble of CNNs which were trained on extracted spectrograms of the audio clips. Their predictions were further post-processed using a binary CNN for classifying the presence/absence of avian sound and available meta data with respect to date and region of the test soundscapes. In the following, we discuss design and training routine of the CNNs. All our models were trained on GPUs utilizing the Pytorch framework and backbones that were pre-trained on ImageNet and that are available at timm <ref type="bibr" coords="3,195.31,329.15,14.81,9.96" target="#b10">[11]</ref>. In order to perform computational efficient training, we also performed the spectrogram transformation on GPU using torchaudio<ref type="foot" coords="3,325.21,339.73,3.24,6.97" target="#foot_1">3</ref> and mixed precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Bird Classifier</head><p>The core part of our solution addresses the main task-specific challenges: Firstly, training data is weakly labeled with bird sound classes being annotated on audio clips of varying length, while prediction needs to be performed on five second time windows of several minute long recordings. Secondly, due to the complex soundscapes and broad range of data contributors, the training labels contain a significant amount of low quality annotations. Lastly, the test data is of significantly different context and of different quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprocessing</head><p>The training data contains short sound clips of different length with weak primary and secondary labels. To preprocess the training data for modeling, we randomly cropped a 30 second time window from a recording and applied a mel-spectrogram transformation. Hereby, 30 seconds proved to be a good compromise between label accuracy and generalization provided by the augmentation. To account for the five second time window required for inference of the test data, we reshaped the crops into six parts. Before feeding the spectrograms into the CNN-backbone, we applied a two dimensional version of mixup <ref type="bibr" coords="3,160.66,542.67,14.84,9.96" target="#b11">[12]</ref>. In particular, we not only mixed between different recordings (up to two times), but also within a recording by mixing the six parts. Our preprocessing was performed on GPU using functionalities of torchaudio for mel-spectrogram transformation and a custom implementation for mixup. We illustrate our preprocessing pipeline in Figure <ref type="figure" coords="3,324.37,578.53,3.41,9.96" target="#fig_0">1</ref>.</p><p>Architecture Given a batch of mel-spectrograms, the model extracts features with a CNN-backbone pretrained on ImageNet, where in our final solution we used backbones from the resnet <ref type="bibr" coords="3,431.99,616.33,16.21,9.96" target="#b12">[13]</ref> efficientnet v2 <ref type="bibr" coords="3,89.29,628.29,16.22,9.96" target="#b13">[14]</ref> and nfnet <ref type="bibr" coords="3,148.56,628.29,16.22,9.96" target="#b14">[15]</ref> family, which are all available in the timm repository <ref type="bibr" coords="3,381.02,628.29,14.69,9.96" target="#b10">[11]</ref>. We re-arrange the tensors to the 30 second representation by concatenating the respective time segments and use generalized mean pooling (GeM) <ref type="bibr" coords="3,149.60,652.20,16.22,9.96" target="#b15">[16]</ref> of time and frequency dimension before forwarding through a simple one layer head  which results in a prediction of 397 bird classes. For inference we directly fed five second snippets to the CNN-backbone and applied the head without reshaping. See Figure <ref type="figure" coords="4,365.84,422.24,4.63,9.96" target="#fig_1">2</ref> for an illustration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training routine</head><p>We did not differentiate between primary and secondary labels and set the multilabel target as the union of both. We trained our models with a binary cross-entropy loss using Adam optimizer and a 11 to 20 epochs lasting cosine annealing schedule with a batch size varying from 16 to 32 per model. While the model is not very sensitive to hyperparameters of optimizer and learning rate schedule, several adjustments lead to significant improvement in classification quality.</p><p>Firstly, we used the quality rating, which is given as meta information for the training data for weighting a recording's contribution to the loss. The assumption is that recordings with a lower rating have worse quality with respect to audio and label and should contribute less to model training. In detail, we weight each sample by rating/max(ratings).</p><p>Secondly, we used one-sided label smoothing, i.e., adding 0.01 -0.025 across all negative labels while positive class is unchanged. This accounts for noisy annotations and absence of birds in "unlucky" 30 second crops. Similar to solutions of past BirdCLEF challenges, we mixed the training data with additional background noise containing no avian calls. Specifically, we used recordings from the freefield dataset labeled for bird absence, three recordings of this years validation set containing no bird calls and 30 second crops of past years validation data containing no bird calls according to their annotations; for details, please refer to Section 2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Binary classifier</head><p>A binary classifier was trained to distinguish any bird call from other environment sounds and predictions were used in the ensembling/post-processing step together with the bird classifier models. For this task, we only used external data, the binary labeled freefield1010 <ref type="bibr" coords="5,333.38,296.19,11.58,9.96" target="#b6">[7]</ref> and BirdVox-DCASE-20k <ref type="bibr" coords="5,459.16,296.19,11.58,9.96" target="#b7">[8]</ref> datasets, both containing ten second audio recordings with or without avian sounds (see Section 2).</p><p>Figure <ref type="figure" coords="5,129.72,320.10,4.73,9.96" target="#fig_2">3</ref> gives an overview of the binary classifier. After transforming the recordings to melspectrograms, mixup augmentation is applied before feeding into a pretrained CNN-backbone. The output feature map is mean pooled on the frequency axis before applying attention pooling on time segments. A single linear layer is applied to output a binary bird presence / absence prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Postprocessing</head><p>As F1 metric is evaluated on hard predictions without taking probabilities into account, appropriate thresholds for detections in a five second segment in soundscapes had to be found. Optimizing the thresholds on CV-3 and applying it to the test set already indicated good correlation. Nevertheless, submitting to the public leaderboard with lower thresholds (higher number of predictions) yielded even better F1 scores. Additionally, one major issue we observed was when blending new models, the threshold needs to be re-optimized each time. Hard thresholds are not necessarily consistent across models, as each one can exhibit certain shifts in probabilities and their averages. Consequently, it is challenging to properly judge if new models work well in an ensemble based on the merit of the models, or only based on arbitrary probability / threshold shifts that emerged from it. For a more robust approach regarding shifts in model probabilities, we decided to apply a percentile based thresholding approach. In detail, we flattened all predictions and calculated the threshold for a certain percentile which we optimized independently for maximum F1 score on CV-3 and on public leaderboard.</p><p>The more birds a set contains, the lower the percentile should be if predictions are decently ranked. With this approach, we kept the percentile fixed, thus always predicting the same amount of bird calls, and just exchanged models, blends and other post processing steps. If the quality in our ranking of predictions improved, also the score improved.</p><p>On our bootstrapped CV-3, we found a percentile of 0.9987 to be optimal. Based on public leaderboard feedback, we decided to decrease the percentile a bit further to 0.9981 for our test predictions expecting a few more birds to be predicted. In retrospective, a slightly higher percentile would have further improved our solution. By and large, our percentile based thresholding approach appeared to be robust across CV and LB and generalized well to the unseen private part of the test set.</p><p>Additionally, we employed several smaller post processing steps to improve predictions including: (1) increasing the probability of birds in songs based on their average prediction probability, (2) smoothing neighboring predictions, and (3) we also removed some unlikely predictions based on distance in space and time given the provided metadata. Additionally, we utilized our binary models predicting whether a bird song is present in a given window or not and adjusted our predictions by ùëù ùëèùëñùëüùëë = ùëù ùëèùëñùëüùëë *(1+ùëù ùëèùëñùëõùëéùëüùë¶ *0.8). Overall, we fitted 15 binary models and averaged them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Ensembling</head><p>The ensembling <ref type="bibr" coords="6,158.52,383.51,16.51,9.96" target="#b16">[17]</ref> of our models was straightforward since all outputs have the same shapes; we took the arithmetic mean of the predictions. Post-processing steps ( <ref type="formula" coords="6,361.21,395.46,3.47,9.96">1</ref>) and ( <ref type="formula" coords="6,391.41,395.46,3.47,9.96">2</ref>) as described in Section 3.3 were applied individually on each model, all other steps were applied after blending. Our final solution contains nine models differing only on hyperparameters and backbones. Each type of model was fitted with six different seeds accounting for variability in predictions. Our final inference Kaggle kernel <ref type="foot" coords="6,486.17,429.95,3.24,6.97" target="#foot_2">4</ref> ran only one hour out of three hours allowed giving us further potential room for improvements. In the following Section 4, we describe according results and go into more detail about the individual models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and discussion</head><p>As described in Section 3, our final solution is based on an ensemble of multiple models with different hyperparameter settings and seeds. In Figure <ref type="figure" coords="6,276.03,521.87,3.43,9.96" target="#fig_3">4</ref>, we visualize our bootstrapped CV-3 validation results; the x-axis depicts the score, and the y-axis respective density across all 500 bootstrap iterations. The distribution exhibits a mean of 0.800, a median of 0.801, a minimum of 0.706, a maximum of 0.906, as well as a standard deviation of 0.0365. As mentioned, the percentile for validation was set to 0.9987, while for test we used a percentile of 0.9981. Post-processing parameters are optimized on validation, and accordingly set for test inference.</p><p>In Table <ref type="table" coords="6,137.10,593.60,3.48,9.96">1</ref>, we highlight all individual models as well as the final blend with according settings, and validation and test scores. For all models, we employed the same thresholding, binary prediction adjustments, as well as post-processing, meaning that individual results only differ on the trained models used. Again, note that each individual model averages six different seeds to better account for individual randomness. For further details please also refer to respective configurations in our code base 5 .</p><p>The results further highlight the difficulty of robust local validation generalizing well to the unseen test data. While we can observe a trend of higher CV leading to higher test scores, the best individual local validation models are not necessarily also the best models on public and private leaderboard scores. Also, while validation and public LB scores are more aligned, private LB scores exhibit significantly lower scores even exceeding the lower bounds of our bootstrapped validation scores as shown in Figure <ref type="figure" coords="7,500.56,362.59,3.48,9.96" target="#fig_3">4</ref>. With test data being hidden, we can only speculate that some of the differences can be explained by the presence of two additional test sites (COL &amp; SNE) for which we can neither robustly judge the presence / absence of certain bird species nor the amount of bird segments in respective data strongly driving absolute scores. It is also unclear, whether public and private test data is split randomly, or based on a certain logic. However, note that we could gather these insights only after the competition ended, as we were only able to acquire limited insights into public LB scores (two submissions per day), and no insights about private LB during competition. Yet, these observations further strengthen the importance of robust solutions in this competition, that are able to generalize well to partly unseen data and locations.</p><p>While we can observe some (partly random) fluctuations in individual model scores (although each one consists of six individual seeds), our final ensemble exhibits the best local validation score, best public LB score, and best private LB score. The ensemble successfully captures the weaknesses and strengths of each individual model and was a robust choice to generalize well on the unseen test data. A further testament of the robustness of our solution is given by the fact that we improved our rank on private leaderboard compared to our initial rank on public leaderboard compared to other contestants. The final leaderboard results are highlighted in Table <ref type="table" coords="7,307.37,541.92,3.41,9.96">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Leaderboard results -This table shows the ranking and scores of the top five teams on the private leaderboard including their respective public leaderboard scores. The team of this article is highlighted in bold. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this work, we discussed how to address challenges of using the crowd-sourced dataset xeno-canto for productive bird sound recognition as part of the second place solution to BirdCLEF2021 <ref type="bibr" coords="8,443.06,121.42,10.35,9.96" target="#b0">[1]</ref>. Specifically, we showed how to efficiently finetune a pre-trained CNN with weakly labeled data by reshaping input and output tensors and how to handle noisy labels by using quality rating, as well as label smoothing. Furthermore, we demonstrated that mixing train dataset recordings internally as well as with external data significantly improves generalization to distinct acoustic environments. Thus, we gave suggestions to enhance autonomous monitoring of avian population which is essential for assessing the biodiversity and healthiness of ecosystems and a sustainable development of humanity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,191.16,418.07,8.03;4,89.01,202.16,418.46,7.98;4,89.29,213.12,194.25,7.98;4,89.29,84.19,416.69,88.63"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Visualization of preprocessing steps for the bird classification model -Input is a 30 second wav crop. After mel-spectrogram transformation, the input data is split into six equal five second parts, before mixup augmentation is applied within and between recordings.</figDesc><graphic coords="4,89.29,84.19,416.69,88.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.29,352.95,418.18,8.03;4,89.29,363.95,416.70,7.98;4,89.29,374.91,58.34,7.98;4,89.29,239.04,416.70,95.56"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of bird classification model -five second mel-spectrograms are fed into a pretrained CNNbackbone and re-arranged to 30 second time segments before GeM pooling is applied. A single linear layer outputs the bird classes.</figDesc><graphic coords="4,89.29,239.04,416.70,95.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,89.29,194.57,416.69,8.03;5,89.29,205.58,416.70,7.98;5,89.29,216.53,340.69,7.98;5,89.29,84.19,416.70,92.04"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of binary classification model -Ten second recordings are transformed to spectrograms before augmented by mixup. Using a pre-trained CNN-backbone features are extracted which are pooled by mean and attention before a single linear layer outputs a binary bird presence / absence prediction</figDesc><graphic coords="5,89.29,84.19,416.70,92.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,89.29,253.06,416.69,8.03;6,89.29,264.06,192.53,7.98;6,150.68,84.19,291.69,161.49"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visualization of bootstrapped validation results -This figure depicts the distribution of scores over 500 iterations based on our bootstrap validation routine.</figDesc><graphic coords="6,150.68,84.19,291.69,161.49" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,107.22,671.77,222.00,7.97"><p>https://www.aicrowd.com/challenges/lifeclef-2020-bird-monophone</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="3,107.22,671.79,141.20,7.97"><p>https://pytorch.org/audio/stable/index.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="6,107.22,662.32,215.24,7.97"><p>https://www.kaggle.com/ilu000/2nd-place-birdclef2021-inference/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank <rs type="institution">Kaggle</rs> and the competition hosts for conducting this interesting competition as well as all other competitors on Kaggle for the challenge. Additionally, a big thank you goes to all users of xeno-canto generously uploading and sharing their bird recordings with the world. Without them this work would not have been possible.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Model results -This table highlights individual model and final ensemble results sorted by CV score. All models exhibit same postprocessing steps, but differ in utilized backbones and hyperparameters. The main hyperparameter differences stem from melspec settings, background augmentation, and label smoothing (LS). For melspec settings, S1 refers to window_size=1024, hop_size=320, fmin=50, fmax=14000, mel_bins=256, power=2, and top_db=80, while S2 refers to window_size=2048, hop_size=512, fmin=16, fmax=16386, mel_bins=64, power=2, and top_db=None. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,107.83,332.41,398.15,7.97;8,107.83,341.88,399.03,7.97;8,107.83,351.34,16.58,7.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,384.80,332.41,73.46,7.97;8,479.08,332.41,26.90,7.97;8,107.83,341.88,126.84,7.97">Bird call identification in soundscape recordings</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqu√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,252.62,341.88,250.39,7.97">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note>Overview of BirdCLEF</note>
</biblStruct>

<biblStruct coords="8,107.83,360.81,399.03,7.97;8,107.83,370.27,398.15,7.97;8,107.83,379.74,398.15,7.97;8,107.59,389.20,250.83,7.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,406.66,370.27,99.32,7.97;8,107.83,379.74,319.24,7.97">Overview of LifeCLEF 2021: a system-oriented evaluation of automated species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Casta√±eda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqu√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dorso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,445.15,379.74,60.84,7.97;8,107.59,389.20,229.11,7.97">Proceedings of the Twelfth International Conference of the CLEF Association (CLEF 2021)</title>
		<meeting>the Twelfth International Conference of the CLEF Association (CLEF 2021)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.83,398.66,398.35,7.97;8,107.83,408.13,191.21,7.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,205.00,398.66,205.66,7.97">Overview of PlantCLEF 2021: cross-domain plant identification</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,428.27,398.66,77.91,7.97;8,107.83,408.13,168.79,7.97">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.83,417.59,398.15,7.97;8,107.54,427.06,365.64,7.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,281.58,417.59,224.41,7.97;8,107.54,427.06,78.33,7.97">Overview of SnakeCLEF 2021: Automatic snake species identification with country-level focus</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Casta√±eda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,202.61,427.06,248.15,7.97">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.83,436.52,398.15,7.97;8,107.83,445.99,385.20,7.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,276.56,436.52,85.63,7.97;8,383.52,436.52,122.46,7.97;8,107.83,445.99,97.57,7.97">Predicting species distribution from 2 million remote sensing images</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,222.46,445.99,248.15,7.97">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note>Overview of GeoLifeCLEF</note>
</biblStruct>

<biblStruct coords="8,107.83,455.45,219.57,7.97" xml:id="b5">
	<monogr>
		<ptr target="https://www.xenocanto.org" />
		<title level="m" coord="8,107.83,455.45,104.40,7.97">X. canto Foundation, xeno-canto</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.83,464.92,398.15,7.97;8,107.83,474.38,366.11,7.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,200.08,464.92,248.89,7.97">freefield1010 -an open dataset for research on audio field recording archives</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Stowell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,466.44,464.92,39.54,7.97;8,107.83,474.38,253.59,7.97">Proceedings of the Audio Engineering Society 53rd Conference on Semantic Audio (AES53)</title>
		<meeting>the Audio Engineering Society 53rd Conference on Semantic Audio (AES53)</meeting>
		<imprint>
			<publisher>Audio Engineering Society</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.83,483.85,398.15,7.97;8,107.83,493.31,140.07,7.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,310.64,483.85,195.35,7.97;8,107.83,493.31,41.98,7.97">Birdvox-full-night: a dataset and benchmark for avian flight call detection</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lostanlen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Farnsworth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kelling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,166.56,493.31,58.97,7.97">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.83,502.77,399.09,7.97;8,107.83,512.24,399.38,7.97;8,107.83,523.48,71.13,5.78" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,303.71,502.77,203.21,7.97;8,107.83,512.24,125.70,7.97">Automatic acoustic detection of birds through deep learning: the first bird audio detection challenge</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Stowell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Stylianou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Pamu≈Ça</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05812</idno>
		<ptr target="https://arxiv.org/abs/1807.05812" />
	</analytic>
	<monogr>
		<title level="j" coord="8,240.96,512.24,113.83,7.97">Methods in Ecology and Evolution</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.83,531.17,398.15,7.97;8,107.83,540.63,242.62,7.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,396.51,531.17,109.48,7.97;8,107.83,540.63,170.66,7.97">Overview of BirdCLEF 2020: Bird sound recognition in complex acoustic environments</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Clapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hopping</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqu√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,295.65,540.63,32.92,7.97">CLEF 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.83,550.10,401.19,7.97;8,107.62,561.34,32.10,5.78" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.4414861</idno>
		<idno>doi:</idno>
		<ptr target="10.5281/zenodo.4414861" />
		<title level="m" coord="8,155.59,550.10,71.25,7.97">Pytorch image models</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.83,569.03,398.15,7.97;8,107.83,578.49,78.30,7.97" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m" coord="8,294.07,569.03,149.54,7.97">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,107.83,587.95,341.71,7.97" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<title level="m" coord="8,209.89,587.95,144.68,7.97">Deep residual learning for image recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.83,597.42,312.54,7.97" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.00298</idno>
		<title level="m" coord="8,163.13,597.42,162.46,7.97">EfficientNetV2: Smaller models and faster training</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.83,606.88,399.38,7.97;8,107.83,618.13,71.13,5.78" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06171</idno>
		<title level="m" coord="8,249.91,606.88,234.63,7.97">High-performance large-scale image recognition without normalization</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.83,625.81,399.38,7.97" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Radenoviƒá</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02512</idno>
		<title level="m" coord="8,217.46,625.81,194.79,7.97">Fine-tuning CNN image retrieval with no human annotation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.83,635.28,398.15,7.97;8,107.59,644.74,53.63,7.97" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="8,195.90,635.28,85.78,7.97">Neural network ensembles</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">K</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Salamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,289.23,635.28,207.06,7.97">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="993" to="1001" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
