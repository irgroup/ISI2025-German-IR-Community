<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.69,84.74,377.72,15.42;1,89.29,106.66,129.80,15.42">A Deep Learning Method for Visual Recognition of Snake Species</title>
				<funder ref="#_htFrZmp">
					<orgName type="full">OP VVV</orgName>
				</funder>
				<funder ref="#_9HXK3Ff">
					<orgName type="full">UWB</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,82.60,11.96"><forename type="first">Rail</forename><surname>Chamidullin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Cybernetics</orgName>
								<orgName type="department" key="dep2">Faculty of Electrical Engineering</orgName>
								<orgName type="institution">Czech Technical University</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,184.53,134.97,51.78,11.96"><forename type="first">Milan</forename><surname>Å ulc</surname></persName>
							<email>sulcmila@fel.cvut.cz</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Cybernetics</orgName>
								<orgName type="department" key="dep2">Faculty of Electrical Engineering</orgName>
								<orgName type="institution">Czech Technical University</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,248.96,134.97,47.16,11.96"><forename type="first">JiÅ™Ã­</forename><surname>Matas</surname></persName>
							<email>matas@fel.cvut.cz</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Cybernetics</orgName>
								<orgName type="department" key="dep2">Faculty of Electrical Engineering</orgName>
								<orgName type="institution">Czech Technical University</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,327.12,134.97,58.19,11.96"><forename type="first">LukÃ¡Å¡</forename><surname>Picek</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Cybernetics</orgName>
								<orgName type="department" key="dep2">Faculty of Applied Sciences</orgName>
								<orgName type="institution">University of West Bohemia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.69,84.74,377.72,15.42;1,89.29,106.66,129.80,15.42">A Deep Learning Method for Visual Recognition of Snake Species</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">D10B959E1C3947BE571966554AB89CB8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Snake Species Identification</term>
					<term>Fine-grained Classification</term>
					<term>Computer Vision</term>
					<term>Convolutional Neural Networks</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The paper presents a method for image-based snake species identification. The proposed method is based on deep residual neural networks -ResNeSt, ResNeXt and ResNet -fine-tuned from ImageNet pre-trained checkpoints. We achieve performance improvements by: discarding predictions of species that do not occur in the country of the query; combining predictions from an ensemble of classifiers; and applying mixed precision training, which allows training neural networks with larger batch size. We experimented with loss functions inspired by the considered metrics: soft F1 loss and weighted cross entropy loss. However, the standard cross entropy loss achieved superior results both in accuracy and in F1 measures. The proposed method scored third in the SnakeCLEF 2021 challenge, achieving 91.6% classification accuracy, Country F1 Score of 0.860, and F1 Score of 0.830.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The paper describes a method for automatic image-based snake species identification submitted by the CMP team to the SnakeCLEF 2021 challenge <ref type="bibr" coords="1,316.53,428.00,12.76,10.91" target="#b0">[1]</ref> -a part of LifeCLEF 2021 workshop <ref type="bibr" coords="1,492.54,428.00,11.34,10.91" target="#b1">[2]</ref>. The problem of identifying snake species from images is difficult because the classification is fine-grained, some species look very similar, and up to hundreds of different snake species live in one country.</p><p>Taxonomic knowledge about snakes is crucial in diagnosis and medical response to snakebites. Accurate identification of the snake species is important for the appropriate treatment of snakebite victims since specific antivenoms are effective against specific venomous snakes. Moreover, antivenoms should not be used to treat bites from non-venomous snakes because of side effects such as allergic reactions <ref type="bibr" coords="1,257.20,536.39,11.58,10.91" target="#b2">[3]</ref>. Snakebites are a global health problem that kills or disables half a million people a year in developing countries <ref type="bibr" coords="1,357.84,549.94,11.43,10.91" target="#b2">[3]</ref>.</p><p>This paper is structured as follows: Section 2 describes related work focusing on snake species identification. Section 3 introduces the input data and evaluation methodology of the SnakeCLEF 2021 challenge. Section 4 describes the adopted architecture of deep neural network and the optimization procedure. Section 5 covers all experiments, ranging from preliminary experiments to the final challenge submissions. Finally, the results are summarized in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Before the existence of large-scale image datasets for snake species classification, Abeysinghe et al. <ref type="bibr" coords="2,112.81,172.69,13.00,10.91" target="#b3">[4]</ref> proposed a one-shot learning approach for fine-tuning a Convolutional Neural Network (CNN) for the task of snake species identification. The authors used a small dataset of 84 snake species, with most species having no more than 3 training images. The authors utilize a Siamese network <ref type="bibr" coords="2,177.22,213.34,13.00,10.91" target="#b4">[5]</ref> that ranks similarity between two inputs: The network is trained by binary cross entropy minimization to estimate the probability of the query image belonging to the same class as the reference image. At test time the query image is compared against all annotated reference images of each class.</p><p>In 2020, the first year of the SnakeCLEF challenge <ref type="bibr" coords="2,332.73,267.54,11.59,10.91" target="#b5">[6]</ref>, introduced a dataset with 287,632 images of 783 snake species taken in 145 countries. Only two teams presented their recognition systems for identifying snake species.</p><p>The best scoring team in SnakeCLEF 2020, gokuleloop <ref type="bibr" coords="2,341.04,308.18,11.35,10.91" target="#b6">[7]</ref>, fine-tuned ResNet-50-V2 <ref type="bibr" coords="2,469.24,308.18,12.76,10.91" target="#b7">[8]</ref> from ImageNet-1K and ImageNet-21K <ref type="bibr" coords="2,233.51,321.73,12.71,10.91" target="#b8">[9]</ref> pre-trained checkpoints, the latter leading to better results. The author applied the following training techniques:</p><p>â€¢ Gradient accumulation -a technique that accumulates gradients from small mini-batches allowing larger effective mini-batch size. â€¢ Mixup augmentation <ref type="bibr" coords="2,214.09,375.93,18.07,10.91" target="#b9">[10]</ref> -an augmentation technique that combines random image pairs from the training dataset. â€¢ Group normalization <ref type="bibr" coords="2,209.60,403.03,17.76,10.91" target="#b10">[11]</ref> -differently from batch normalization, GN divides the channels into groups and computes the mean and variance within each group. The second team in SnakeCLEF 2020, FHDO_BCSG <ref type="bibr" coords="2,328.20,430.13,16.15,10.91" target="#b11">[12]</ref>, first detected regions where snakes occur using a Mask R-CNN <ref type="bibr" coords="2,215.94,443.67,18.07,10.91" target="#b12">[13]</ref> object detector, and then classified the snake species in the regions using EfficientNet <ref type="bibr" coords="2,208.90,457.22,16.41,10.91" target="#b13">[14]</ref>. The authors adjusted the output probabilities of EfficientNet based on the geographic location of the image: The softmax values for each image were multiplied by the species a priori probability for a given geographic location. To clean the training dataset from noisy samples, the authors utilized an ImageNet-1K pre-trained ResNet-50 network and discarded images not classified as snake and reptile classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Challenge Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>The training dataset provided by SnakeCLEF 2021 covers 772 snake species and contains annotated images from three different sources: iNaturalist, HerpMapper and Flickr. Examples of images are in Figure <ref type="figure" coords="2,194.05,618.11,3.78,10.91" target="#fig_0">1</ref>. The majority of images are from iNaturalist and HerpMapper, with 277,025 and 58,351 images, respectively. Their labels are confirmed by human annotators. The Flickr dataset is the smallest, with 50,630 web-scraped images that contain noisy data. In total, 386,006 images with annotations were provided. Training with external data was not allowed.</p><p>The challenge organizers suggested a subset of 70,208 images, referenced as a mini-subset in the rest of this paper, made of samples from INaturalist and HerpMapper. The experiments described in Section 5 are based on the said subset.</p><p>In addition to the images, the dataset contains metadata with information about the country where the image was taken. In total, the training dataset includes images from 188 countries. The dataset is fine-grained with a long tail class distribution. More than 22,000 images represent the most frequent species, while the least frequent species have only 10 images. The least represented species are often found in regions such as Middle and South America, South Africa and Australia. Table <ref type="table" coords="3,185.03,491.05,5.17,10.91" target="#tab_0">1</ref> shows the distribution of images in geographical regions. For some images, information about the geographical location is missing.</p><p>Furthermore, the challenge organizers provided 28,418 images without annotations. Top one species predictions for the test images were sent to the organizers to participate in the challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data Preparation</head><p>During the data exploration phase, we discovered that training and validation datasets contain noisy data from Flickr. The noisy data are non-relevant images with various animal species or objects. We estimate<ref type="foot" coords="3,196.44,606.77,3.71,7.97" target="#foot_0">1</ref> that the percentage of non-relevant images is 10.6 Â± 0.1, with 95% confidence interval. We decided to remove all Flickr images and proceeded with verified images from iNaturalist and HerpMapper. The challenge organizers suggested a data split with 90% training and 10% validation samples. However, after removing Flickr images, it turned out that some species were not represented in the proposed validation set. Table <ref type="table" coords="4,237.21,303.33,4.97,10.91" target="#tab_1">2</ref> displays the number of snake classes represented, i.e. classes with at least one image, in the dataset sources. iNaturalist and HerpMapper combined have 768 classes which are all represented across the training set but only 733 classes in the validation set. We thus created a new dataset split where all classes are represented in both training and validation splits if more than one image of the species is available. If not, the image is placed in the training set.</p><p>Technically, the last 10% of images, ordered by metadata ID, for every species and country combination were selected as the validation data. One validation image was selected for the cases that had fewer than 10 images. We assume the ID ordering is random w.r.t. image content and properties. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Evaluation Metrics</head><p>The challenge used two metrics for the final evaluation. The primary metric is the macro averaged F1 Score across countries ("Country F1 Score"), shown in equation 4. The secondary metric is the macro averaged F1 Score ("F1 Score"), shown in equation 2.</p><p>The F1 Score for each species ğ‘  = 1, 2, ..., ğ‘˜ is computed as a harmonic mean of precision ğ‘ ğ‘  and recall ğ‘Ÿ ğ‘  :</p><formula xml:id="formula_0" coords="5,263.54,184.04,242.45,25.50">ğ¹ 1ğ‘  = 2ğ‘ ğ‘  ğ‘Ÿ ğ‘  ğ‘ ğ‘  + ğ‘Ÿ ğ‘  .<label>(1)</label></formula><p>The macro averaged F1 Score is the average of the ğ¹ 1 scores of all species:</p><formula xml:id="formula_1" coords="5,243.13,239.32,262.86,33.58">macro(ğ¹ 1 ) = 1 ğ‘˜ ğ‘˜ âˆ‘ï¸ ğ‘ =1 ğ¹ 1ğ‘  .<label>(2)</label></formula><p>Country F1 Score ğ¶ğ¹ 1ğ‘ for each country ğ‘ = 1, 2, ..., ğ‘š is the macro averaged F1 Score computed only for species living in country ğ‘:</p><formula xml:id="formula_2" coords="5,247.04,315.74,255.09,32.08">CF 1ğ‘ = âˆ‘ï¸€ ğ‘˜ ğ‘ =1 ğ¹ 1ğ‘  ğ´ ğ‘ğ‘  âˆ‘ï¸€ ğ‘˜ ğ‘ =1 ğ´ ğ‘ğ‘  , (<label>3</label></formula><formula xml:id="formula_3" coords="5,502.13,325.50,3.86,10.91">)</formula><p>where ğ´ is a ğ‘˜ Ã— ğ‘š matrix with elements ğ´ ğ‘ğ‘  = {ï¸ƒ 1, country ğ‘ is a habitat of species ğ‘  0, otherwise .</p><p>Similarly, macro averaged Country F1 Score is obtained by averaging CF 1ğ‘ over all countries:</p><formula xml:id="formula_4" coords="5,236.15,424.13,269.83,33.58">macro(CF 1 ) = 1 ğ‘š ğ‘š âˆ‘ï¸ ğ‘=1 CF 1ğ‘ .<label>(4)</label></formula><p>The macro averaged Country F1 Score thus increases the importance of species that appear in more countries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head><p>The proposed method is based on the state-of-the-art Convolutional Neural Networks (CNNs) for image classification, described in Subsection 4.1. The following subsections describe the optimization procedure, loss functions, the post-processing of the predictions, applying mixed precision training and implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Deep Residual Networks</head><p>All experiments are based on deep residual neural networks, namely the original ResNet <ref type="bibr" coords="5,486.80,626.75,16.30,10.91" target="#b14">[15]</ref>, the ResNeXt <ref type="bibr" coords="5,147.50,640.30,16.37,10.91" target="#b15">[16]</ref>, and the recent ResNeSt <ref type="bibr" coords="5,277.29,640.30,16.37,10.91" target="#b16">[17]</ref>. The ResNet architecture consists of a stack of residual blocks -building modules with residual connections that combine input and output by element-wise addition. The ResNeXt additionally includes a split-transform-merge strategy,</p><p>where each block performs a set of transformations with the same topology whose outputs are aggregated by element-wise addition. For example, a single transformation can be a group of convolutions. The ResNeSt incorporates a channel-wise attention strategy within each splittransform-merge block: Each transformation consists of split groups over which the network calculates the channel-wise split attention weights. All networks in our experiments were fine-tuned from ImageNet-1K <ref type="bibr" coords="6,405.29,154.71,17.89,10.91" target="#b17">[18]</ref> pre-trained checkpoints. Residual networks typically <ref type="bibr" coords="6,253.08,168.26,16.55,10.91" target="#b14">[15,</ref><ref type="bibr" coords="6,272.78,168.26,14.11,10.91" target="#b15">16]</ref> use input size about 224 Ã— 224, the pre-trained ResNeSt-101 and ResNeSt-200 are available with a larger input sizes of 256 Ã— 256 and 320 Ã— 320, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Optimization Procedure</head><p>We use two optimization algorithms for training CNN models: stochastic gradient descent with momentum (SGD) and Adam <ref type="bibr" coords="6,217.25,258.64,16.09,10.91" target="#b18">[19]</ref>. Our preliminary experiments showed that Adam optimizer is able to converge quickly, but the prediction score is inferior compared to SGD. The application of the one cycle schedule policy <ref type="bibr" coords="6,218.08,285.73,17.76,10.91" target="#b19">[20]</ref> (one cycle) improved the results when applied with the Adam optimizer while applying it with SGD did not work well in our preliminary experiments.</p><p>The training hyper-parameters, such as learning rate, momentum and weight decay, are listed in Table <ref type="table" coords="6,126.12,326.38,4.97,10.91" target="#tab_2">3</ref> and were set the same as in the network pre-training. Batch sizes were adjusted to fit the network on the graphics processing unit (GPU). The input image size stays the same as in the pre-trained networks.</p><p>During the training, we select the best checkpoint based on the highest validation Country F1 Score. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Country-specific Removal of Predictions</head><p>For each image, the dataset metadata include the country where the image was taken. Additionally, the dataset comes with a list of countries and snake species that live there. We utilize this information to adjust the model predictions to the country of the query as follows:</p><p>The classifier predictions are set to 0 for all species that do not live in the country of the query. This adjustment is applied only at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Mixed Precision Training</head><p>When training large CNN architectures, fitting the model into limited GPU memory is a bottleneck. We considered the following workarounds: selecting a smaller batch size or applying mixed precision training <ref type="bibr" coords="7,200.48,134.63,16.25,10.91" target="#b20">[21]</ref>. Both approaches have an accuracy trade-off. Mixed precision training is a technique that combines single-precision (32-bit floats, "FP32") and half-precision (16-bit floats, "FP16") float numbers. In order to lower the memory requirements, the forward and backward pass with the large batch size only use a half-precision version of the model. Then, the gradient descent is applied to the single-precision version of the model. In every training step following procedure is applied:</p><p>1. Apply the forward pass, compute the loss and apply backward pass on a model in FP16.</p><p>2. Convert the gradients from FP16 to FP32.</p><p>3. Apply the update on the primary model in FP32. 4. Create a copy of the primary model in FP16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Loss Functions</head><p>The baseline loss function for training the classifiers is the standard cross entropy loss:</p><formula xml:id="formula_5" coords="7,251.01,326.58,254.97,33.71">â„“ ce = - ğ‘› âˆ‘ï¸ ğ‘–=1 log ğ‘¦ ğ‘–,ğ‘¡ ğ‘– ,<label>(5)</label></formula><p>where ğ‘¡ ğ‘– is the ground truth target and y ğ‘– are the classifier predictions for the ğ‘–-th example, and ğ‘¦ ğ‘–,ğ‘¡ ğ‘– is the prediction for the ground truth class of the ğ‘–-th example.</p><p>The following subsections describe the loss functions proposed to use the challenge metrics, described in Section 3.3, as a loss measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1.">F1 Loss with Soft Assignments</head><p>The F1 Score from Equation 2 is not differentiable and thus cannot be utilized as a loss function for back-propagation. We use an approximation of the F1 Score, referenced as soft F1 loss in the rest of this paper, which uses soft assignments that make the function differentiable:</p><p>â€¢ the true positives for species ğ‘  are estimated using the softmax predictions y and one-hot encoded target vector t as follows: Ì‚ï¸</p><formula xml:id="formula_6" coords="7,273.80,521.85,64.66,27.17">TP ğ‘  = ğ‘› âˆ‘ï¸€ ğ‘–=1 y ğ‘– t ğ‘–</formula><p>â€¢ the false positives for species ğ‘  are estimated using the softmax predictions y and one-hot encoded target vector t as follows: Ì‚ï¸</p><formula xml:id="formula_7" coords="7,273.80,568.38,91.21,27.17">FP ğ‘  = ğ‘› âˆ‘ï¸€ ğ‘–=1 y ğ‘– (1 -t ğ‘– )</formula><p>â€¢ the false negatives for species ğ‘  are estimated using the softmax predictions y and one-hot </p><formula xml:id="formula_8" coords="7,116.56,622.55,37.03,10.91">encoded</formula><p>The macro averaged soft F1 Score is obtained by averaging Ì‚ï¸ ğ¹ 1ğ‘  over all species:</p><formula xml:id="formula_10" coords="8,243.13,157.27,262.86,33.58">macro( Ì‚ï¸ ğ¹ 1 ) = 1 ğ‘˜ ğ‘˜ âˆ‘ï¸ ğ‘ =1 Ì‚ï¸ ğ¹ 1ğ‘  .<label>(7)</label></formula><p>The final loss function is â„“</p><formula xml:id="formula_11" coords="8,214.82,198.53,86.23,14.96">Ì‚ï¸€ ğ¹ 1 = 1 -macro( Ì‚ï¸ ğ¹ 1</formula><p>), so that it ranges from 0 (perfect) to 1 (worst).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2.">Weighted Cross Entropy</head><p>Because the macro averaged Country F1 Score from Equation <ref type="formula" coords="8,371.96,247.20,5.17,10.91" target="#formula_4">4</ref>increases the importance of species appearing in more countries, we propose a weighted variant of the cross entropy loss with species weights ğ‘¤ ğ‘  based on the number of countries in which it appears:</p><formula xml:id="formula_12" coords="8,238.81,296.10,267.18,33.71">â„“ wce = - ğ‘› âˆ‘ï¸ ğ‘–=1 ğ‘¤ ğ‘¡ ğ‘– log ğ‘¦ ğ‘–,ğ‘¡ ğ‘– ,<label>(8)</label></formula><p>The Maximum Likelihood Estimation (MLE) of ğ‘¤ ğ‘  would simply count the relative frequencies ğ‘“ ğ‘  in the provided species-country incidence list. In order to avoid zero weights, we add Laplace smoothing:</p><formula xml:id="formula_13" coords="8,255.69,384.15,250.29,41.98">ğ‘¤ ğ‘  = ğ‘“ ğ‘  + 1 ğ‘˜ âˆ‘ï¸€ ğ‘—=1 (ğ‘“ ğ‘— + 1) .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Implementation Details</head><p>The proposed method was developed using the PyTorch <ref type="bibr" coords="8,338.09,463.44,17.84,10.91" target="#b21">[22]</ref> machine learning framework and the fastai framework <ref type="bibr" coords="8,186.09,476.99,18.07,10.91" target="#b22">[23]</ref> built on top of PyTorch. The code is available online<ref type="foot" coords="8,446.40,475.24,3.71,7.97" target="#foot_2">2</ref> . All models were fine-tuned from ImageNet-1K <ref type="bibr" coords="8,243.97,490.54,17.75,10.91" target="#b17">[18]</ref> pre-trained PyTorch Image Models <ref type="bibr" coords="8,417.55,490.54,17.76,10.91" target="#b23">[24]</ref> on one NVIDIA Tesla V100 with 32GB graphic memory.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Comparison of Residual Networks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results of Mixed Precision Training</head><p>As observed in the previous section, ResNeSt-101 with a higher input size achieves the highest scores of the experimented residual networks. Since its deeper version, ResNeSt-200, does not fit into our GPU memory with larger batch sizes, we experiment with the mixed precision training from Section 4.4. Table <ref type="table" coords="9,126.61,344.52,5.17,10.91">5</ref> compares the training time and accuracy of ResNeSt-101 and ResNeSt-200 when training with and without the mixed precision technique. Note that in our computational environment, mixed precision runs slower than single precision. The prediction scores after 10 epochs show that mixed precision has little impact on prediction accuracy in setups with the same architecture and batch size. Increasing the batch size from 32 to 64 has a much larger impact on the accuracy. Thus the network trained on a larger batch size with mixed precision achieves better scores than the single-precision network with a smaller batch size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 5</head><p>Classification scores and training times when fine-tuning for 10 epochs with and without the mixed precision technique. Cells with "Ã—" denote setups for which the network did not fit into the 32GB GPU memory. The networks are fine-tuned on the mini-subset from Section 3.1 and the results are computed on our validation set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture BS Precision type</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Evaluation of Different Loss Functions</head><p>The loss functions introduced in Section 4.5, namely the soft F1 loss and the weighted cross entropy loss, resulted in inferior classification scores compared to cross entropy loss, see Table <ref type="table" coords="10,500.35,257.05,3.66,10.91" target="#tab_6">6</ref>.</p><p>We, therefore, fine-tune the CNN classifiers with cross entropy loss, and then choose the best training checkpoint based on the highest validation Country F1 Score. One possible explanation for the failure of the soft F1 loss is that the batch size of 64 is significantly smaller than the total number of classes, 772. This leads to the classes not being represented in every mini-batch, making the approximation of the F1 loss inaccurate. Figure <ref type="figure" coords="10,500.92,324.80,5.06,10.91" target="#fig_1">2</ref> illustrates the inaccurate approximation of the F1 loss on an example, where the loss values are mostly 0s or 1s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Evaluation of Country-specific Removal of Predictions</head><p>We measure the prediction scores of ResNeSt-200 with and without the removal of species predictions based on the country incidence information. Table <ref type="table" coords="10,363.03,415.17,4.97,10.91">7</ref> compares the prediction scores on our validation set. The improvement is 0.150 in F1 Score and 0.193 in Country F1 Score. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 7</head><p>Comparing classification scores of ResNeSt-200 with and without the removal of species predictions based on the country incidence information. The networks are fine-tuned on the mini-subset from Section 3.1 and the results are computed on our validation set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Country</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Challenge Submissions</head><p>We submitted the following five runs to the SnakeCLEF 2021 challenge:</p><p>CMP_S1: ResNeSt-200 fine-tuned for 20 epochs on the full dataset with SGD. CMP_S2: ResNeSt-200 from CMP_S1 fine-tuned for additional 10 epochs on the full dataset with SGD. CMP_S3: ResNet-101 fine-tuned for 25 epochs on the full dataset with Adam and one cycle. CMP_S4: ResNeXt-101 fine-tuned for 30 epochs on the mini-subset from Section 3.1 with Adam and one cycle. CMP_S5: An ensemble of all four previous runs, combining the top one predictions by majority voting strategy. In case of ties, predictions of CMP_S1 are preferred.</p><p>Table <ref type="table" coords="11,125.67,359.47,4.98,10.91" target="#tab_8">8</ref> shows the final challenge scores on the test set. While different in accuracy, the CNN architectures ResNeSt-200, ResNeXt-101 and ResNet-101 achieve similar results in the primary challenge metric, the Country F1 Score. The highest scores are achieved by the ensemble.</p><p>We recognize a shortcoming of the ensemble submission (CMP_S5), which inclines towards the ResNeSt-200 submissions related to each other (CMP_S2 is fine-tuned from CMP_S1). The remaining networks cannot outvote an agreement of CMP_S1 and CMP_S2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>The paper presents a deep learning method for image-based snake species identification, a finegrained classification problem with a long tail class distribution. The method is based on deep residual neural networks -ResNeSt, ResNeXt and ResNet -fine-tuned from ImageNet pre-trained checkpoints. We achieve performance improvements by: discarding predictions of species that do not occur in the country of the query; combining predictions from an ensemble of classifiers; and applying mixed precision training, which allows training neural networks with larger batch size. The experimented soft F1 loss and weighted cross entropy loss produced inferior results compared to the standard cross entropy minimization. Thus, the competition submissions are fine-tuned with the standard cross entropy loss.</p><p>The proposed method scored third in the SnakeCLEF 2021 challenge, achieving 91.6% classification accuracy, Country F1 Score of 0.860, and F1 Score of 0.830.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,293.16,416.87,9.96;3,89.29,305.12,417.79,9.96;3,88.66,317.07,247.02,9.96;3,93.89,186.61,100.01,100.01"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Image examples from the SnakeCLEF 2021 dataset. The images are resized and center cropped to 224 Ã— 224. CC-BY-NC images from iNaturalist: Â©srhein, Â©jance, Â©roig10, Â©arturobtzz, Â©John Clough, Â©William Wimley, Â©nobiscuits, Â©feistygirl75.</figDesc><graphic coords="3,93.89,186.61,100.01,100.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="10,89.29,615.12,416.69,9.96;10,89.29,627.07,415.82,9.96;10,89.29,459.40,416.71,150.02"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: F1 Scores for Acrochordus granulatus across all training iterations in one epoch. The example illustrates the inaccurate approximation of the F1 loss: the loss rarely takes values other than 0 and 1.</figDesc><graphic coords="10,89.29,459.40,416.71,150.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.99,90.49,285.20,159.19"><head>Table 1</head><label>1</label><figDesc>Geographical distribution of SnakeCLEF 2021 images.</figDesc><table coords="4,221.08,121.17,153.12,128.51"><row><cell>Region</cell><cell>Number of images</cell></row><row><cell>North America</cell><cell>258,732</cell></row><row><cell>Europe</cell><cell>18,689</cell></row><row><cell>Middle America</cell><cell>17,403</cell></row><row><cell>Asia</cell><cell>16,518</cell></row><row><cell>South America</cell><cell>12,735</cell></row><row><cell>Africa</cell><cell>6,017</cell></row><row><cell>Australia</cell><cell>4,313</cell></row><row><cell>Oceania</cell><cell>538</cell></row><row><cell>unknown</cell><cell>51,061</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,88.99,462.25,418.53,159.19"><head>Table 2</head><label>2</label><figDesc>Number of species included in SnakeCLEF 2021 dataset sources: iNaturalist, HerpMapper and Flickr. The last row represents our new dataset split, after removing all Flickr images due to noisy labels and sampling a new validation set covering as many species as possible.</figDesc><table coords="4,110.23,516.84,374.81,104.60"><row><cell>Source</cell><cell cols="3">Training set Validation set Number of images</cell></row><row><cell>iNaturalist</cell><cell>762</cell><cell>716</cell><cell>277,025</cell></row><row><cell>HerpMapper</cell><cell>603</cell><cell>357</cell><cell>58,351</cell></row><row><cell>Flickr</cell><cell>730</cell><cell>585</cell><cell>50,630</cell></row><row><cell>iNaturalist + HerpMapper + Flickr</cell><cell>772</cell><cell>772</cell><cell>386,006</cell></row><row><cell>iNaturalist + HerpMapper</cell><cell>768</cell><cell>733</cell><cell>335,376</cell></row><row><cell>Mini-subset (introduced in Section 3.1)</cell><cell>768</cell><cell>763</cell><cell>70,208</cell></row><row><cell>iNaturalist + HerpMapper (new split)</cell><cell>768</cell><cell>765</cell><cell>335,376</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,88.99,407.14,362.83,100.42"><head>Table 3</head><label>3</label><figDesc>The hyper-parameter setting used for training the challenge submissions.</figDesc><table coords="6,143.45,437.82,308.37,69.74"><row><cell>Network</cell><cell cols="4">ResNeSt-101 ResNeSt-200 ResNeXt-101 ResNet-101</cell></row><row><cell>Optimizer</cell><cell>SGD</cell><cell>SGD</cell><cell>Adam</cell><cell>Adam</cell></row><row><cell>LR Scheduler</cell><cell>-</cell><cell>-</cell><cell>one cycle</cell><cell>one cycle</cell></row><row><cell>Learning Rate</cell><cell>0.1</cell><cell>0.1</cell><cell>0.01</cell><cell>0.01</cell></row><row><cell>Weight Decay</cell><cell>0.0001</cell><cell>0.0001</cell><cell>0.01</cell><cell>0.01</cell></row><row><cell>Batch Size</cell><cell>128</cell><cell>64</cell><cell>128</cell><cell>128</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,89.29,614.92,416.97,69.39"><head></head><label></label><figDesc>target vector t as follows: Ì‚ï¸ FN ğ‘  =</figDesc><table coords="7,89.29,614.92,416.97,69.39"><row><cell>ï¸€ ğ‘ ğ‘  =</cell><cell>Ì‚ï¸ TP ğ‘  TP ğ‘  + Ì‚ï¸ ï¸ FP ğ‘ </cell><cell>,</cell><cell>Ì‚ï¸€ ğ‘Ÿ ğ‘  =</cell><cell cols="2">Ì‚ï¸ TP ğ‘  TP ğ‘  + Ì‚ï¸ ï¸ FN ğ‘ </cell><cell>,</cell><cell>Ì‚ï¸ ğ¹ 1ğ‘  =</cell><cell>2Ì‚ï¸€ ğ‘ ğ‘  Ì‚ï¸€ ğ‘Ÿ ğ‘  ï¸€ ğ‘ ğ‘  + Ì‚ï¸€ ğ‘Ÿ ğ‘ </cell><cell>.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ğ‘›</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>âˆ‘ï¸€</cell><cell cols="3">(1 -y ğ‘– )t ğ‘–</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ğ‘–=1</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">Notice, that Ì‚ï¸ TP, Ì‚ï¸ FP, and Ì‚ï¸ FN are now real valued. Soft F1 Score for species ğ‘ , Ì‚ï¸ ğ¹ 1ğ‘  , is obtained by</cell></row><row><cell cols="7">computing the harmonic mean of precision Ì‚ï¸€ ğ‘ ğ‘  and recall Ì‚ï¸€ ğ‘Ÿ ğ‘  :</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,88.96,583.03,418.86,78.66"><head>Table 4</head><label>4</label><figDesc>shows classification scores of residual networks ResNet, ResNeXt and ResNeSt with 50 and 101 layers. All networks are fine-tuned for 30 epochs on images of size 224 Ã— 224, minimizing the cross-entropy loss using SGD with momentum. One ResNeSt-101 version is fine-tuned on a larger image size 256 Ã— 256 to match the image size of the ImageNet pretrained checkpoint. Both ResNeSt versions, ResNeSt-50 and ResNeSt-101, achieve higher scores compared to the corresponding ResNet and ResNeXt architectures.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,88.99,90.49,418.53,152.72"><head>Table 4</head><label>4</label><figDesc>Classification scores of residual networks fine-tuned for 30 epochs on the mini-subset from Section 3.1. The results are computed on our validation set.</figDesc><table coords="9,155.19,133.12,284.90,110.08"><row><cell cols="5">Architecture Input Size Accuracy F1 Score Country F1 Score</cell></row><row><cell>ResNet-50</cell><cell>224</cell><cell>44.0%</cell><cell>0.331</cell><cell>0.300</cell></row><row><cell>ResNeXt-50</cell><cell>224</cell><cell>47.2%</cell><cell>0.352</cell><cell>0.333</cell></row><row><cell>ResNeSt-50</cell><cell>224</cell><cell>53.8%</cell><cell>0.447</cell><cell>0.409</cell></row><row><cell>ResNet-101</cell><cell>224</cell><cell>42.4%</cell><cell>0.290</cell><cell>0.273</cell></row><row><cell>ResNeXt-101</cell><cell>224</cell><cell>50.5%</cell><cell>0.428</cell><cell>0.396</cell></row><row><cell>ResNeSt-101</cell><cell>224</cell><cell>56.7%</cell><cell>0.475</cell><cell>0.432</cell></row><row><cell>ResNeSt-101</cell><cell>256</cell><cell>58.8%</cell><cell>0.500</cell><cell>0.455</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,104.91,527.12,385.46,116.56"><head>Table 6</head><label>6</label><figDesc>Classification scores on ResNeSt-101 with different loss functions. Standard cross entropy achieves superior results. The networks are fine-tuned on the mini-subset from Section 3.1 and the results are computed on our validation set.</figDesc><table coords="9,104.91,527.12,385.46,116.56"><row><cell></cell><cell></cell><cell cols="4">Accuracy F1 Score Country F1 Score Epoch time</cell></row><row><cell>ResNeSt-101 128</cell><cell>Mixed</cell><cell>48.0%</cell><cell>0.387</cell><cell>0.355</cell><cell>14 min</cell></row><row><cell>ResNeSt-101 128</cell><cell>Single</cell><cell>47.6%</cell><cell>0.385</cell><cell>0.348</cell><cell>10 min</cell></row><row><cell>ResNeSt-200 128</cell><cell>Mixed</cell><cell>Ã—</cell><cell>Ã—</cell><cell>Ã—</cell><cell>Ã—</cell></row><row><cell>ResNeSt-200 128</cell><cell>Single</cell><cell>Ã—</cell><cell>Ã—</cell><cell>Ã—</cell><cell>Ã—</cell></row><row><cell>ResNeSt-200 64</cell><cell>Mixed</cell><cell>52.7%</cell><cell>0.424</cell><cell>0.398</cell><cell>40 min</cell></row><row><cell>ResNeSt-200 64</cell><cell>Single</cell><cell>Ã—</cell><cell>Ã—</cell><cell>Ã—</cell><cell>Ã—</cell></row><row><cell>ResNeSt-200 32</cell><cell>Mixed</cell><cell>46.9%</cell><cell>0.376</cell><cell>0.345</cell><cell>41 min</cell></row><row><cell>ResNeSt-200 32</cell><cell>Single</cell><cell>46.9%</cell><cell>0.371</cell><cell>0.345</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="11,88.93,464.19,418.72,141.76"><head>Table 8</head><label>8</label><figDesc>Classification scores of the submitted challenge runs on the SnakeCLEF 2021 challenge test set. The networks are fine-tuned either on the full dataset (Full) or on the mini-subset (Mini) from Section 3.1. The CNN architectures ResNeSt-200, ResNeXt-101 and ResNet-101 achieve similar results in the Country F1 Score. The highest scores are achieved by the ensemble of all networks.</figDesc><table coords="11,106.08,530.74,383.11,75.22"><row><cell cols="2">Submission Architecture</cell><cell cols="4">Dataset Accuracy F1 Score Country F1 Score</cell></row><row><cell>CMP_S1</cell><cell>ResNeSt-200</cell><cell>Full</cell><cell>90.6%</cell><cell>0.772</cell><cell>0.839</cell></row><row><cell>CMP_S2</cell><cell>ResNeSt-200</cell><cell>Full</cell><cell>89.5%</cell><cell>0.779</cell><cell>0.819</cell></row><row><cell>CMP_S3</cell><cell>ResNet-101</cell><cell>Full</cell><cell>90.7%</cell><cell>0.795</cell><cell>0.837</cell></row><row><cell>CMP_S4</cell><cell>ResNeXt-101</cell><cell>Mini</cell><cell>77.6%</cell><cell>0.796</cell><cell>0.839</cell></row><row><cell>CMP_S5</cell><cell cols="2">Ensemble of CMP_S1-S4 -</cell><cell>91.6%</cell><cell>0.830</cell><cell>0.860</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,108.93,660.07,169.29,8.97"><p>We used the Student's t-distribution with ğ‘› =</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20" xml:id="foot_1" coords="3,292.46,660.07,213.52,8.97;3,89.29,671.03,322.92,8.97"><p>samples and ğ‘› -1 degrees of freedom where each sample denotes the percentage of non-relevant images in a set of randomly selected 100 images.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2" coords="8,108.93,671.04,223.12,8.97"><p>https://github.com/chamidullinr/snake-species-identification</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was supported by the <rs type="funder">OP VVV</rs> funded project <rs type="grantNumber">CZ.02.1.01/0.0/0.0/16_019/0000765</rs>. LP was supported by the <rs type="funder">UWB</rs> grant, project No. <rs type="grantNumber">SGS-2019-027</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_htFrZmp">
					<idno type="grant-number">CZ.02.1.01/0.0/0.0/16_019/0000765</idno>
				</org>
				<org type="funding" xml:id="_9HXK3Ff">
					<idno type="grant-number">SGS-2019-027</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,112.66,391.12,394.62,10.91;12,112.28,404.67,393.71,10.91;12,112.66,418.22,288.62,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,368.26,391.12,139.01,10.91;12,112.28,404.67,291.27,10.91">Overview of SnakeCLEF 2021: Automatic Snake Species Identification with Country-Level Focus</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De CastaÃ±eda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,426.79,404.67,79.20,10.91;12,112.66,418.22,257.93,10.91">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,431.77,393.33,10.91;12,112.66,445.32,394.52,10.91;12,112.66,458.87,393.33,10.91;12,112.66,472.42,393.33,10.91;12,112.66,485.97,306.11,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,167.22,458.87,338.77,10.91;12,112.66,472.42,251.45,10.91">Overview of LifeCLEF 2021: a System-oriented Evaluation of Automated Species Identification and Species Distribution Prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De CastaÃ±eda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">H</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Isabelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>PlanquÃ©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dorso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>MÃ¼ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,386.92,472.42,119.06,10.91;12,112.66,485.97,276.37,10.91">Proceedings of the Twelfth International Conference of the CLEF Association (CLEF 2021)</title>
		<meeting>the Twelfth International Conference of the CLEF Association (CLEF 2021)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,499.52,394.53,10.91;12,112.66,513.06,393.32,10.91;12,112.66,526.61,394.61,10.91;12,112.31,540.16,362.88,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,112.66,513.06,393.32,10.91;12,112.66,526.61,238.12,10.91">Identifying the snake: First scoping review on practices of communities and healthcare providers confronted with snakebite across the world</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Botero</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Mesa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alcoba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Chappuis</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ruiz De CastaÃ±eda</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0229989</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0229989.doi:10.1371/journal.pone.0229989" />
	</analytic>
	<monogr>
		<title level="j" coord="12,359.51,526.61,49.26,10.91">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,553.71,394.53,10.91;12,112.66,567.26,394.53,10.91;12,112.66,580.81,229.48,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,277.24,553.71,224.96,10.91">Snake Image Classification Using Siamese Networks</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Abeysinghe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Welivita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Perera</surname></persName>
		</author>
		<idno type="DOI">10.1145/3338472.3338476</idno>
		<ptr target="https://doi.org/10.1145/3338472.3338476" />
	</analytic>
	<monogr>
		<title level="m" coord="12,126.65,567.26,376.06,10.91">Proceedings of the 2019 3rd International Conference on Graphics and Signal Processing</title>
		<meeting>the 2019 3rd International Conference on Graphics and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,594.36,393.33,10.91;12,112.66,607.91,244.69,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,291.57,594.36,214.41,10.91;12,112.66,607.91,51.39,10.91">Siamese Neural Networks for One-Shot Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,187.28,607.91,139.30,10.91">ICML Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,621.46,394.62,10.91;12,112.28,635.01,395.00,10.91;12,112.66,648.56,232.29,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,361.65,621.46,145.62,10.91;12,112.28,635.01,220.33,10.91">Overview of the snakeclef 2020: Automatic snake species identification challenge</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De CastaÃ±eda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,358.68,635.01,148.59,10.91;12,112.66,648.56,201.59,10.91">CLEF: Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>CLEF task overview 2020</note>
</biblStruct>

<biblStruct coords="13,112.66,86.97,393.59,10.91;13,112.26,100.52,354.15,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,181.60,86.97,278.16,10.91">Impact of Pretrained Networks For Snake Species Classification</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,482.34,86.97,23.91,10.91;13,112.26,100.52,323.46,10.91">CLEF: Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>CLEF working notes 2020</note>
</biblStruct>

<biblStruct coords="13,112.66,114.06,394.62,10.91;13,112.66,127.61,165.97,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,269.35,114.06,212.64,10.91">Identity Mappings in Deep Residual Networks</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,112.66,127.61,136.01,10.91">Computer Vision -ECCV 2016</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,141.16,393.33,10.91;13,112.66,154.71,159.68,10.91" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10972</idno>
		<title level="m" coord="13,351.27,141.16,154.71,10.91;13,112.66,154.71,29.30,10.91">ImageNet-21K Pretraining for the Masses</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,168.26,393.61,10.91;13,112.66,181.81,394.62,10.91;13,112.66,195.36,201.92,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,361.20,168.26,145.07,10.91;13,112.66,181.81,58.73,10.91">mixup: Beyond Empirical Risk Minimization</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1Ddp1-Rb" />
	</analytic>
	<monogr>
		<title level="m" coord="13,200.61,181.81,248.86,10.91">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,208.91,345.19,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,175.75,208.91,92.83,10.91">Group Normalization</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,291.87,208.91,136.01,10.91">Computer Vision -ECCV 2018</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,222.46,394.53,10.91;13,112.66,236.01,393.32,10.91;13,112.66,249.56,393.33,10.91;13,112.66,263.11,287.18,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,246.68,236.01,259.30,10.91;13,112.66,249.56,276.99,10.91">Combination of image and location information for snake species identification using object detection and EfficientNets</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Boketta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Keibel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mense</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Michailutschenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>RÃ¼ckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Willemeit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,413.04,249.56,92.95,10.91;13,112.66,263.11,256.49,10.91">CLEF: Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>CLEF working notes 2020</note>
</biblStruct>

<biblStruct coords="13,112.66,276.66,393.33,10.91;13,112.66,290.20,352.10,10.91" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R-Cnn</forename><surname>Mask</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.322</idno>
		<title level="m" coord="13,421.30,276.66,84.69,10.91;13,112.66,290.20,175.62,10.91">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,303.75,394.53,10.91;13,112.66,317.30,394.62,10.91;13,112.66,330.85,202.36,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,176.85,303.75,325.35,10.91">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/tan19a.html" />
	</analytic>
	<monogr>
		<title level="m" coord="13,128.01,317.30,322.91,10.91">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,344.40,394.62,10.91;13,112.66,357.95,394.53,10.91;13,112.66,371.50,22.69,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="13,267.98,344.40,214.89,10.91">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,112.66,357.95,389.59,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,385.05,393.33,10.91;13,112.66,398.60,393.33,10.91;13,112.66,412.15,117.06,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,298.43,385.05,207.56,10.91;13,112.66,398.60,71.84,10.91">Aggregated Residual Transformations for Deep Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,206.94,398.60,299.05,10.91;13,112.66,412.15,86.61,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,425.70,394.53,10.91;13,112.66,439.25,356.40,10.91" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m" coord="13,185.89,439.25,152.58,10.91">ResNeSt: Split-Attention Networks</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,452.79,393.33,10.91;13,112.66,466.34,394.53,10.91;13,112.66,479.89,22.69,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="13,345.70,452.79,160.29,10.91;13,112.66,466.34,66.03,10.91">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,224.88,466.34,277.63,10.91">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,493.44,393.32,10.91;13,112.33,506.99,29.19,10.91" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="13,236.43,493.44,170.14,10.91">A Method for Stochastic Optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename></persName>
		</author>
		<idno>CoRR abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,520.54,393.32,10.91;13,112.66,534.09,222.37,10.91" xml:id="b19">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Topin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07120</idno>
		<title level="m" coord="13,211.04,520.54,294.95,10.91;13,112.66,534.09,92.48,10.91">Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,547.64,394.53,10.91;13,112.66,561.19,393.33,10.91;13,112.66,574.74,382.12,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="13,265.93,561.19,108.29,10.91">Mixed Precision Training</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1gs9JgRZ" />
	</analytic>
	<monogr>
		<title level="m" coord="13,396.52,561.19,109.46,10.91;13,112.66,574.74,125.91,10.91">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,588.29,394.53,10.91;13,112.66,601.84,394.52,10.91;13,112.66,615.39,394.53,10.91;13,112.66,628.93,394.53,10.91;13,112.66,642.48,395.17,10.91;13,112.66,656.03,394.03,10.91;13,112.66,669.58,357.13,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="13,368.74,615.39,138.45,10.91;13,112.66,628.93,183.88,10.91">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="13,292.29,642.48,215.54,10.91;13,112.66,656.03,31.56,10.91">Advances in Neural Information Processing Systems 32</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>AlchÃ©-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,86.97,395.01,10.91;14,112.66,100.52,338.36,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="14,209.73,86.97,175.47,10.91">Fastai: A Layered API for Deep Learning</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<idno type="DOI">10.3390/info11020108</idno>
		<ptr target="http://dx.doi.org/10.3390/info11020108.doi:10.3390/info11020108" />
	</analytic>
	<monogr>
		<title level="j" coord="14,393.68,86.97,52.44,10.91">Information</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">108</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,114.06,8.98,10.91;14,138.55,114.06,51.02,10.91;14,210.03,114.06,37.69,10.91;14,264.64,114.06,27.72,10.91;14,309.28,114.06,35.35,10.91;14,365.09,114.06,141.60,10.91;14,112.66,127.61,395.17,10.91;14,112.66,141.16,12.55,10.91" xml:id="b23">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.4414861</idno>
		<idno>doi:</idno>
		<ptr target="10.5281/zenodo.4414861" />
		<title level="m" coord="14,210.03,114.06,37.69,10.91;14,264.64,114.06,27.72,10.91;14,309.28,114.06,30.30,10.91">PyTorch Image Models</title>
		<imprint>
			<date type="published" when="2019">2019. 2021-06-28</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
