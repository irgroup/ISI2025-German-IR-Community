<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.78,84.74,375.17,15.42;1,89.29,106.66,404.79,15.42;1,89.29,128.58,347.68,15.43">TUC Media Computing at BirdCLEF 2021: Noise augmentation strategies in bird sound classification in combination with DenseNets and ResNets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.87,156.89,145.14,11.96"><forename type="first">Arunodhayan</forename><surname>Sampathkumar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universität Chemnitz</orgName>
								<address>
									<addrLine>Str. der Nationen 62</addrLine>
									<postCode>09111</postCode>
									<settlement>Chemnitz</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,246.65,156.89,80.04,11.96"><forename type="first">Danny</forename><surname>Kowerko</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universität Chemnitz</orgName>
								<address>
									<addrLine>Str. der Nationen 62</addrLine>
									<postCode>09111</postCode>
									<settlement>Chemnitz</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.78,84.74,375.17,15.42;1,89.29,106.66,404.79,15.42;1,89.29,128.58,347.68,15.43">TUC Media Computing at BirdCLEF 2021: Noise augmentation strategies in bird sound classification in combination with DenseNets and ResNets</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">F3660DA31B55FFEDE7EDB042547A3318</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Convolutional Neural Network)</term>
					<term>Bird Recognition</term>
					<term>Deep learning</term>
					<term>Data Augmentation</term>
					<term>Soundscapes</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This research paper presents deep learning techniques for bird recognition to classify 397 species in the BirdCLEF 2021 challenge. The proposed method was inspired by the DCASE2019 audio tagging challenge, which classifies and recognizes different sound events. Data augmentations methods like noise augmentation, spectrogram augmentation techniques are used to avoid overfitting and hence generalize the model. The final solution is based on an ensemble of different backbone models and splitting the dataset based on geographic locations provided in the test set. Furthermore, framewise post-processing predictions are used to identify the bird events. The best results were obtained from 12 model ensembles with a public and private score of 0.6487 and 0.6034, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Manual monitoring of birds species requires lots of human labor and is difficult in many forest areas. An automated approach in an ecosystem for continuous recordings would allow monitoring species in different locations, over longer period of time, and in deeper forest areas <ref type="bibr" coords="1,492.37,439.56,11.47,10.91" target="#b0">[1]</ref>. In 2021, BirdCLEF has organized a challenge to classify 397 bird species in 5-300 s snippets of continuous audio recordings in different location around the globe. The test data contain 80 soundscapes recording each of 10 minutes length, recorded in 4 different locations namely COL (Jardín, Departamento de Antioquia, Colombia), COR (Alajuela, San Ramón, Costa Rica), SNE (Sierra Nevada, California, USA), SSW(Ithaca, New York, USA) <ref type="bibr" coords="1,356.21,507.30,14.56,10.91" target="#b1">[2,</ref><ref type="bibr" coords="1,373.20,507.30,7.49,10.91" target="#b2">3]</ref>. The soundscape recordings contain high quality overlapping sounds of different species. A challenging part and motivation of the competition are the weakly labeled train data, and there are multiple distribution domain shifts present, namely shifts in input space, shifts in prior probability of labels, and shifts in the function which connects train and test recordings. Domain shifts in this competition are large differences in data characteristics between train (clean recordings) and test (noisy recordings) making generalization of models on unseen data difficult.</p><p>The paper is organized as follows, section 2 describes the recognition of the birds including data preparation. Section 3 describes feature extraction, data-augmentation, neural network architecture and training steps. Section 4 presents the evaluation results followed by the conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Dataset</head><p>All the recordings are first converted from ogg to wav format with a sampling rate of 32 kHz. The soundscape recordings are prepared for validation by cutting them into 5 s chunks according to the annotations. The background noises are separated from the soundscape recordings based on parts without bird activity using the provided metadata. Later, these background noises are used for data augmentation. Some parts of validation soundscape recordings are merged with Xeno-canto training set <ref type="bibr" coords="2,493.05,253.99,12.94,10.91" target="#b2">[3]</ref> for training, while the rest of the recordings are used for cross validation. The training set and validation set are splitted by 5 stratified folds. To create more diverse models, 6 different sub-datasets are formed targeting different locations, which are later ensembeled. As presented in Table <ref type="table" coords="2,202.20,510.50,3.81,10.91" target="#tab_0">1</ref>, sub-datasets are divided based on locations, e.g. Dataset-1 and 2 are prepared based on the locations of test set. With the given latitude and longitude from test data, a 200 and 400 km radius is marked and most likely occurring species within the radius are taken into account. Dataset-3 consist of species that mainly occur in the given test recording locations. Dataset-4 belongs to species that mainly present in the recording locations of the United States. Dataset-5 belongs to species that mainly present in the recording locations of Costa Rica. Dataset 6 belongs to species that mainly present in the recording locations of Colombia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Spectrogram Extraction</head><p>The recordings are sampled at 32 kHz sample rate and trimmed to 30 s long chunks because if we use a shorter window size, it may not include any sound events or include some sound events which may be a noisy event or a background species as shown in Figure <ref type="figure" coords="3,433.58,159.35,3.66,10.91" target="#fig_0">1</ref>, for this reason longer chunks are preferred. To make the model learn correctly, we need to make each label correspond to call-events of each species. First, we compute a Short Time Fourier Transform (STFT) with a Hann window of 1024 samples and hop size of 384 samples and mel bins of 64, retain only the magnitude and then followed by applying a log-mel filter banks from 150 Hz to 15 kHz. To make train recordings more robust, different noises are incorporated as background noise. Besides the noise extracted from soundscape recordings, recordings without bird activity from BAD (Bird Audio Detection) are used <ref type="bibr" coords="3,256.75,637.13,11.40,10.91" target="#b3">[4]</ref>. Apart from these two noise systems, generated pink noises are used <ref type="bibr" coords="3,159.58,650.68,11.43,10.91" target="#b4">[5]</ref>.</p><p>Secondary background noise: Mixing various (bursts of overlapping) short audios in the train recording with random pauses between. Noises like wind, car sounds, insects, rain and thunder are used. Mixup: Audio chunks from random files are mixed together, and their corresponding labels are added as shown in Figure <ref type="figure" coords="4,204.92,141.16,3.72,10.91" target="#fig_1">2</ref>. The mixup augmentations are constructed using the formulae <ref type="bibr" coords="4,492.47,141.16,11.39,10.91" target="#b5">[6]</ref>.</p><formula xml:id="formula_0" coords="4,241.93,168.26,264.05,31.39">𝑥 = 𝛼 • 𝑥 𝑖 + (1 -𝛼) • 𝑥 𝑗 (1) 𝑦 = 𝛼 • 𝑦 𝑖 + (1 -𝛼) • 𝑦 𝑗 (2)</formula><p>where (𝑥 𝑖 , 𝑦 𝑖 ) and (𝑥 𝑗 , 𝑦 𝑗 ) are the two randomly selected recordings for mixup, and 𝛼 is the mix ratio with values from [0, 1]. Mixup increases the robustness of the model and generalizes well in real time data because soundscape data typically contain more than one species occurring in the event window. Gaussian SNR: The Gaussian noise applied to the samples with random signal to Noise Ratio (SNR). Spectrogram augmentation Time stretching and pitch shifting are the augmentations tried on spectrograms. Time stretching is the process of changing the speed/duration of sound without affecting the pitch of sound. It takes the wave samples and a factor by which the input is stretched by a factor of 0.4 which has a small difference with the original sample. Pitch shifting is the process of changing the pitch of the sound without affecting the speed. It takes wave samples, sample rate and number of steps(-4 to +4) through which the pitch must be shifted. These methods are performed using LIBROSA library <ref type="bibr" coords="4,329.96,610.75,11.36,10.91" target="#b6">[7,</ref><ref type="bibr" coords="4,344.04,610.75,7.57,10.91" target="#b7">8]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Architecture</head><p>In recent years Convolutional Neural Networks (CNNs) have been successfully used for audio recognition and detection. The architecture design for the bird recognition task was inspired from DCASE2019 PANNs (Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition) <ref type="bibr" coords="5,148.37,390.52,11.35,10.91" target="#b6">[7]</ref>. PANNs are developed based on cross talk CNN with an extra fully connected layer added to the penultimate layer of the CNN.</p><p>From the previous BirdCLEF challenges, deeper CNN networks performed well when compared with wider or shallow CNNs. Hence the backbone network for this challenge used are ResNets <ref type="bibr" coords="5,89.29,444.72,12.84,10.91" target="#b8">[9]</ref> and DenseNets <ref type="bibr" coords="5,174.50,444.72,16.25,10.91" target="#b9">[10]</ref>.</p><p>ResNet: Deeper CNNs perform well on audio recognition tasks. The challenge in very deep CNNs is that the gradients do not propagate properly. To solve this issue, ResNets introduced shortcut connections between convolutional layers. DenseNets: DenseNets were designed to improve the information flow between layers, a different connectivity pattern was introduced with direct connections from any layer to all subsequent layers. The change of feature maps is facilitated by down-sampling the architecture by dividing the network into multiple densely connections, making the network deeper.</p><p>In this task, after log-mel feature extraction, the inputs are passed to ResNets/DenseNets by removing the last fully connected layers and extract only features. Then, a modified 1D attention based fully connected layer is attached to ResNet. The output of this network is a dictionary which contains clipwise and framewise outputs. Table <ref type="table" coords="5,391.96,634.41,5.17,10.91" target="#tab_2">3</ref> illustrates the modified networks used in this research. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training setup</head><p>Our CNNs used a model pretrained on ImageNet <ref type="bibr" coords="6,312.15,330.97,11.61,10.91" target="#b6">[7]</ref>, and were fine-tuned with training data previously converted to log-mel scaled spectrogram images. Machine learning functionality was implemented using the PyTorch library, while audio (pre-)processing functionality like spectrogram decomposition was realized using the Librosa library.</p><p>The networks are trained for 75 epochs without mixup augmentation and 150 epochs with mixup augmentation. The loss function used here is BCE-focal-2way loss (binary cross entropy) and sed-scaled-pos-neg-focalloss(FL) <ref type="bibr" coords="6,258.29,412.27,16.25,10.91" target="#b10">[11]</ref>. SED-Scaled-Pos-Neg-Focal loss: It focuses on primary labels and secondary labels loss.</p><formula xml:id="formula_1" coords="6,104.34,450.33,401.65,75.47">𝑏𝑐𝑒𝑙𝑜𝑠𝑠 = (𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑒𝑑𝑙𝑎𝑏𝑒𝑙𝑠𝑝𝑟𝑖𝑚𝑎𝑟𝑦, 𝐺𝑟𝑜𝑢𝑛𝑑𝑡𝑟𝑢𝑡ℎ) (3) 𝐹 𝑜𝑐𝑎𝑙𝑙𝑜𝑠𝑠 -𝑜𝑛𝑒𝑠 -𝑙𝑖𝑘𝑒 = (1 -(𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑒𝑑𝑙𝑎𝑏𝑒𝑙𝑠𝑝𝑟𝑖𝑚𝑎𝑟𝑦)) • (1 -𝑏𝑐𝑒𝑙𝑜𝑠𝑠) • 𝑏𝑐𝑒𝑙𝑜𝑠𝑠 (4) 𝐹 𝑜𝑐𝑎𝑙𝑙𝑜𝑠𝑠 -𝑧𝑒𝑟𝑜𝑠 -𝑙𝑖𝑘𝑒 = (𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑒𝑑𝑙𝑎𝑏𝑒𝑙𝑠𝑝𝑟𝑖𝑚𝑎𝑟𝑦) • (1 -𝑏𝑐𝑒𝑙𝑜𝑠𝑠) • 𝑏𝑐𝑒𝑙𝑜𝑠𝑠 (5) 𝐹 𝑜𝑐𝑎𝑙𝑙𝑜𝑠𝑠 = (4) + (5)<label>(6)</label></formula><p>𝑀 𝑎𝑠𝑘𝑒𝑑𝑃 𝑟𝑒𝑑𝑖𝑐𝑡𝑒𝑑𝑆𝑒𝑐𝑜𝑛𝑑𝑎𝑟𝑦𝑙𝑎𝑏𝑒𝑙𝑠 = (𝑃 𝑟𝑒𝑑𝑖𝑐𝑡𝑒𝑑𝑠𝑒𝑐𝑜𝑛𝑑𝑎𝑟𝑦𝑙𝑎𝑏𝑒𝑙𝑠 &gt; 0.0, 𝑜𝑛𝑒𝑠 -𝑙𝑖𝑘𝑒, 𝑧𝑒𝑟𝑜𝑠 -𝑙𝑖𝑘𝑒) <ref type="bibr" coords="6,494.41,534.91,11.57,10.91" target="#b6">(7)</ref> 𝐹 𝑜𝑐𝑎𝑙𝑙𝑜𝑠𝑠 -𝑆𝑐𝑎𝑙𝑒𝑑 = 𝑀 𝑎𝑠𝑘𝑒𝑑𝑃 𝑟𝑒𝑑𝑖𝑐𝑡𝑒𝑑𝑆𝑒𝑐𝑜𝑛𝑑𝑎𝑟𝑦𝑙𝑎𝑏𝑒𝑙𝑠 • 𝐹 𝑜𝑐𝑎𝑙𝑙𝑜𝑠𝑠 <ref type="bibr" coords="6,494.41,554.93,11.57,10.91" target="#b7">(8)</ref> Oneslike are tensors filled with the scalar value '1'and zerolike are tensors filled with the scalar value '0'. The grouped output losses are Focalloss-scaled, bceloss, Focalloss. The optimizer used here is AdamW optimizer with weight decay 0.1. The learning rate scheduler is a combination of merging Cosine Annealing Scheduler with warmup (cycle-size is epochlength 𝑙𝑖𝑏𝑟𝑎𝑟𝑦 number of epochs) + LinearCyclicalScheduler (cycle-size is epoch-length 𝑙𝑖𝑏𝑟𝑎𝑟𝑦 2). The initial learning rate is 0.001. Background species metadata are not taken into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation results</head><p>This section illustrates the combination of models, ensemble techniques and evaluation score on the test set. Table <ref type="table" coords="7,183.34,124.83,3.70,10.91" target="#tab_3">4</ref>, Table <ref type="table" coords="7,220.07,124.83,3.70,10.91" target="#tab_4">5</ref>, and Table <ref type="table" coords="7,275.79,124.83,5.02,10.91" target="#tab_5">6</ref> illustrate the different strategies used in the model and their respective results based on public and private leadership board. The ensemble method used here is voting.      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>The current approach attained an F1 score of 0.6034 in the private leadership board. Recognizing all bird species is still challenging because of domain shift in train (clean audio) and test (noisy audio) data. The train dataset consists of weakly labeled (clipwise labeling) and there were many background species present. A multi-label annotation of train files could have significantly improved the models in bird recognition. There are several techniques to improve this bird recognition task, methods like vision transforms and removal of no bird activity from the train dataset. A promising approach would be the feature extraction by merging two different features in combination with polyphonic event detection. Better inference techniques could focus more on locations, e.g. using the ebird API and thresholds for each species separately, to achieve better recognition of bird events.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,354.70,416.94,8.93;3,89.29,366.71,370.89,8.87;3,108.88,247.35,375.03,100.76"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The figure presents a log-mel spectrogram, where 30 s long chunk window represents many bird events, whereas a random 5 s short (from 20 s-25 s) chunk window represents no event</figDesc><graphic coords="3,108.88,247.35,375.03,100.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.29,458.18,416.69,8.93;4,89.29,470.18,254.52,8.87;4,203.88,378.34,187.51,73.26"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The top left indicates the spectrogram of species acafly and top right indicates brewee. The bottom spectrogram indicates the mixup of acafly and brewee</figDesc><graphic coords="4,203.88,378.34,187.51,73.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,88.99,334.74,408.96,156.91"><head>Table 1</head><label>1</label><figDesc>The Table illustrates the description of the datasets.</figDesc><table coords="2,100.02,362.78,397.94,128.87"><row><cell>ID</cell><cell cols="2">No of Classes No of training</cell><cell>Location</cell><cell>Radius</cell></row><row><cell></cell><cell></cell><cell>samples</cell><cell></cell><cell>(km)</cell></row><row><cell>Dataset-1</cell><cell>273</cell><cell>47168</cell><cell>United states, Costa Rica</cell><cell>200</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Colombia</cell><cell></cell></row><row><cell>Dataset-2</cell><cell>345</cell><cell>57302</cell><cell>United states, Costa Rica</cell><cell>400</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Colombia</cell><cell></cell></row><row><cell>Dataset-3</cell><cell>391</cell><cell>32963</cell><cell>United states, Costa Rica</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Colombia</cell><cell></cell></row><row><cell>Dataset-4</cell><cell>263</cell><cell>44913</cell><cell>United states</cell><cell>-</cell></row><row><cell>Dataset-5</cell><cell>162</cell><cell>25760</cell><cell>Costa Rica</cell><cell>-</cell></row><row><cell>Dataset-6</cell><cell>187</cell><cell>29831</cell><cell>Colombia</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,88.99,90.49,416.99,214.46"><head>Table 2</head><label>2</label><figDesc>Influence of data augmentations on the model results. The backbone architecture used here is DenseNet 161.</figDesc><table coords="5,110.95,128.26,79.43,8.93"><row><cell>ID</cell><cell>Description</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,88.99,90.49,392.74,195.56"><head>Table 3</head><label>3</label><figDesc>Configuration of ResNet 50 and DenseNets used here for bird recognition.</figDesc><table coords="6,114.85,118.53,366.88,167.52"><row><cell>ResNet 50</cell><cell>DenseNet121</cell><cell>DenseNet161</cell></row><row><cell>Log mel spectrogram 1024</cell><cell>Log mel spectrogram 1024</cell><cell>Log mel spectrogram 1024</cell></row><row><cell>window size x 64 mel bins</cell><cell>window size x 64 mel bins</cell><cell>window size x 64 mel bins</cell></row><row><cell>ResNet-50 features</cell><cell>DenseNet-121 features</cell><cell>DenseNet-161 features</cell></row><row><cell>Maxpooling 1D</cell><cell>Maxpooling 1D</cell><cell>Maxpooling 1D</cell></row><row><cell>AveragePooling 1D</cell><cell>AveragePooling 1D</cell><cell>AveragePooling 1D</cell></row><row><cell>Merge</cell><cell>Merge</cell><cell>Merge</cell></row><row><cell>Maxpooling+Average Pool-</cell><cell>Maxpooling+Average Pool-</cell><cell>Maxpooling+Average Pool-</cell></row><row><cell>ing</cell><cell>ing</cell><cell>ing</cell></row><row><cell cols="3">Fully connected layer+ReLu Fully connected layer+ReLu Fully connected layer+ReLu</cell></row><row><cell>Attention 1D+ no of</cell><cell>Attention 1D+ no of</cell><cell>Attention 1D+ no of</cell></row><row><cell>classes+Sigmoid</cell><cell>classes+Sigmoid</cell><cell>classes+Sigmoid</cell></row><row><cell>Framewise output</cell><cell>Framewise output</cell><cell>Framewise output</cell></row><row><cell>Clipwise output</cell><cell>Clipwise output</cell><cell>Clipwise output</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,88.99,205.35,438.07,165.32"><head>Table 4</head><label>4</label><figDesc>Illustration of Dense161 models used for submissions. NS denotes Noisy Soundscapes.</figDesc><table coords="7,96.91,233.17,430.15,137.50"><row><cell>Model ID</cell><cell>M1</cell><cell>M2</cell><cell>M3</cell><cell>M4</cell><cell>M5</cell><cell>M6</cell><cell>M7</cell><cell>M8</cell></row><row><cell>Ensemble RUN</cell><cell>1</cell><cell>1,4</cell><cell>2,4</cell><cell>2,4</cell><cell>2</cell><cell>3,4</cell><cell>3,4</cell><cell>3,4</cell></row><row><cell>Network Type</cell><cell cols="8">Dense161 Dense161 Dense161 Dense161 Dense161 Dense161 Dense161 Dense161</cell></row><row><cell>Chunk duration[s]</cell><cell>30</cell><cell>30</cell><cell>30</cell><cell>30</cell><cell>30</cell><cell>30</cell><cell>30</cell><cell>30</cell></row><row><cell>No of Classes</cell><cell>397</cell><cell>397</cell><cell>391</cell><cell>343</cell><cell>273</cell><cell>162</cell><cell>187</cell><cell>263</cell></row><row><cell>Primary</cell><cell>BAD</cell><cell>NS</cell><cell>NS</cell><cell>NS</cell><cell>NS</cell><cell>NS</cell><cell>NS</cell><cell>NS</cell></row><row><cell>Background noise</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Secondary</cell><cell>BAD</cell><cell>NS</cell><cell>BAD+</cell><cell>BAD+</cell><cell>BAD+</cell><cell>BAD+</cell><cell>BAD+</cell><cell>BAD+</cell></row><row><cell>Background noise</cell><cell></cell><cell></cell><cell>other</cell><cell>other</cell><cell>other</cell><cell>other</cell><cell>other</cell><cell>other</cell></row><row><cell></cell><cell></cell><cell></cell><cell>noises</cell><cell>noises</cell><cell>noises</cell><cell>noises</cell><cell>noises</cell><cell>noises</cell></row><row><cell cols="2">OtherAugmentations yes</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell></row><row><cell>Public Score</cell><cell>0.5981</cell><cell>0.6134</cell><cell>0.6013</cell><cell>0.6214</cell><cell>0.6298</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Private Score</cell><cell>0.5638</cell><cell>0.5789</cell><cell>0.5609</cell><cell>0.5822</cell><cell>0.5899</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,88.99,402.15,376.88,180.82"><head>Table 5</head><label>5</label><figDesc>Illustration of Dense121 models used for submissions. NS denotes Noisy Soundscapes.</figDesc><table coords="7,134.35,430.19,331.53,152.78"><row><cell>Model ID</cell><cell>M9</cell><cell>M10</cell><cell>M11</cell><cell>M12</cell><cell>M13</cell></row><row><cell>Ensemble RUN</cell><cell>1</cell><cell>,4</cell><cell>2,4</cell><cell>2,4</cell><cell>2</cell></row><row><cell>Network Type</cell><cell cols="5">Dense121 Dense121 Dense121 Dense121 Dense121</cell></row><row><cell>Chunk duration[s]</cell><cell>30</cell><cell>30</cell><cell>30</cell><cell>30</cell><cell>30</cell></row><row><cell>No of Classes</cell><cell>397</cell><cell>397</cell><cell>391</cell><cell>343</cell><cell>273</cell></row><row><cell>Primary</cell><cell>BAD</cell><cell>NS</cell><cell>NS</cell><cell>NS</cell><cell>NS</cell></row><row><cell>Background noise</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Secondary</cell><cell>BAD</cell><cell>NS</cell><cell>BAD+</cell><cell>BAD+</cell><cell>BAD+</cell></row><row><cell>Background noise</cell><cell></cell><cell></cell><cell>other</cell><cell>other</cell><cell>other</cell></row><row><cell></cell><cell></cell><cell></cell><cell>noises</cell><cell>noises</cell><cell>noises</cell></row><row><cell cols="2">OtherAugmentations yes</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell></row><row><cell>Public Score</cell><cell>0.5800</cell><cell>0.5949</cell><cell>0.5866</cell><cell>0.6033</cell><cell>0.6046</cell></row><row><cell>Private Score</cell><cell>0.5598</cell><cell>0.5698</cell><cell>0.5587</cell><cell>0.5789</cell><cell>0.5699</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,88.99,90.49,416.99,241.18"><head>Table 6</head><label>6</label><figDesc>Illustration of ResNet50 models used for submissions. NS denotes Noisy Soundscapes. Models M1, M2, M9, M10, M14, and M15 are used. This model contains 397 classes and used different background noises. The clipwise threshold, framewise threshold and number of votes are discussed in the table 7.</figDesc><table coords="8,100.20,118.53,365.12,185.83"><row><cell>Model ID</cell><cell>M14</cell><cell>M15</cell><cell>M16</cell><cell>M17</cell><cell>M18</cell></row><row><cell>Ensemble RUN</cell><cell>1</cell><cell>1,4</cell><cell>2,4</cell><cell>2,4</cell><cell>2</cell></row><row><cell>Network Type</cell><cell cols="5">ResNet50 ResNet50 ResNet50 ResNet50 ResNet50</cell></row><row><cell>Chunk duration[s]</cell><cell>30</cell><cell>30</cell><cell>30</cell><cell>30</cell><cell>30</cell></row><row><cell>No of Classes</cell><cell>397</cell><cell>397</cell><cell>391</cell><cell>343</cell><cell>273</cell></row><row><cell>Primary</cell><cell>BAD</cell><cell>NS</cell><cell>NS</cell><cell>NS</cell><cell>NS</cell></row><row><cell>Background noise</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Secondary</cell><cell>BAD</cell><cell>NS</cell><cell>BAD+</cell><cell>BAD+</cell><cell>BAD+</cell></row><row><cell>Background noise</cell><cell></cell><cell></cell><cell>other</cell><cell>other</cell><cell>other</cell></row><row><cell></cell><cell></cell><cell></cell><cell>noises</cell><cell>noises</cell><cell>noises</cell></row><row><cell cols="2">OtherAugmentations yes</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell></row><row><cell>Public Score</cell><cell>0.5914</cell><cell>0.6089</cell><cell>0.6088</cell><cell>0.6366</cell><cell>0.6277</cell></row><row><cell>Private Score</cell><cell>0.56878</cell><cell>0.5677</cell><cell>0.5789</cell><cell>0.5977</cell><cell>0.5989</cell></row><row><cell>Ensemble RUN 1:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,88.99,346.31,417.00,193.64"><head>Table 7</head><label>7</label><figDesc>The table illustrates different voting strategies and thresholds their respective scores</figDesc><table coords="8,89.29,374.35,416.70,165.60"><row><cell>6 Models</cell><cell>Clipwise</cell><cell>Framewise</cell><cell cols="2">Public score Private</cell></row><row><cell></cell><cell>Threshold</cell><cell>Threshold</cell><cell></cell><cell>score</cell></row><row><cell>3 Votes</cell><cell>0.3</cell><cell>0.3</cell><cell>0.6466</cell><cell>0.5993</cell></row><row><cell>3 Votes</cell><cell>0.5</cell><cell>0.5</cell><cell>0.6502</cell><cell>0.5989</cell></row><row><cell>4 Votes</cell><cell>0.3</cell><cell>0.3</cell><cell>0.6314</cell><cell>0.5891</cell></row><row><cell>4 Votes</cell><cell>0.5</cell><cell>0.5</cell><cell>0.6389</cell><cell>0.5904</cell></row><row><cell cols="5">Ensemble RUN 2: Models M3, M4, M5, M11, M12, M13, M16, M17, and M18 are used. This</cell></row><row><cell cols="5">model contains 391, 345 and 273 classes, and used different background noises. The clipwise</cell></row><row><cell cols="5">threshold, framewise threshold and number of votes are discussed in the table 8. The 9 model</cell></row><row><cell cols="5">ensemble is a combination of different classes which are split based on location and achieved</cell></row><row><cell cols="5">the top score of 0.6741 in our public score with less False Positives and 0.6024 in the private</cell></row><row><cell>score.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="8,88.99,554.60,350.86,97.13"><head>Table 8</head><label>8</label><figDesc>The table illustrates different voting strategies and thresholds their respective scores Models M7, M8, and M9 are used. This model contains 162, 187 and 263 classes and used different background noises. The clipwise threshold, framewise threshold and number of votes are discussed in the table 9. This ensemble method comprises of 3 different locations. The 3 model ensemble based on location split has a score of 0.6799 on public score with less FP compared to 0.5951 on private score.</figDesc><table coords="8,131.20,582.64,308.66,69.09"><row><cell>9 Models</cell><cell>Clipwise</cell><cell>Framewise</cell><cell cols="2">Public score Private</cell></row><row><cell></cell><cell>Threshold</cell><cell>Threshold</cell><cell></cell><cell>score</cell></row><row><cell>4 Votes</cell><cell>0.3</cell><cell>0.3</cell><cell>0.6436</cell><cell>0.6024</cell></row><row><cell>4 Votes</cell><cell>0.5</cell><cell>0.5</cell><cell>0.6474</cell><cell>0.6007</cell></row><row><cell>3 Votes</cell><cell>0.3</cell><cell>0.3</cell><cell>0.67741</cell><cell>0.5988</cell></row><row><cell>3 Votes</cell><cell>0.5</cell><cell>0.5</cell><cell>0.6741</cell><cell>0.5899</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="9,88.99,221.42,417.00,140.47"><head>Table 9</head><label>9</label><figDesc>The table illustrates different voting strategies and thresholds their respective scores This RUN takes best performing models which used different background noises. The clipwise threshold, framewise threshold and number of votes are discussed in Table10. This ensemble method comprises of 12 different models with different backbone models and different classes split based on location yields the best private score of 0.6034.</figDesc><table coords="9,89.29,249.46,350.56,71.78"><row><cell>9 Models</cell><cell>Clipwise</cell><cell>Framewise</cell><cell cols="2">Public score Private</cell></row><row><cell></cell><cell>Threshold</cell><cell>Threshold</cell><cell></cell><cell>score</cell></row><row><cell>1 Vote</cell><cell>0.3</cell><cell>0.3</cell><cell>0.6799</cell><cell>0.5951</cell></row><row><cell>1 Vote</cell><cell>0.5</cell><cell>0.5</cell><cell>0.6743</cell><cell>0.5823</cell></row><row><cell>used.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="9,88.99,378.17,350.86,97.13"><head>Table 10</head><label>10</label><figDesc>The table illustrates different voting strategies, thresholds, and their respective scores</figDesc><table coords="9,131.20,406.20,308.66,69.09"><row><cell>12 Models</cell><cell>Clipwise</cell><cell>Framewise</cell><cell cols="2">Public score Private</cell></row><row><cell></cell><cell>Threshold</cell><cell>Threshold</cell><cell></cell><cell>score</cell></row><row><cell>5 Votes</cell><cell>0.3</cell><cell>0.3</cell><cell>0.6487</cell><cell>0.6034</cell></row><row><cell>5 Votes</cell><cell>0.5</cell><cell>0.5</cell><cell>0.6484</cell><cell>0.6030</cell></row><row><cell>6 Votes</cell><cell>0.3</cell><cell>0.3</cell><cell>0.6474</cell><cell>0.6024</cell></row><row><cell>6 Votes</cell><cell>0.5</cell><cell>0.5</cell><cell>0.6453</cell><cell>0.6013</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,112.66,111.28,393.33,10.91;10,112.66,124.83,393.33,10.91;10,112.66,138.38,394.52,10.91;10,112.66,151.93,395.00,10.91;10,112.66,165.48,218.47,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,344.79,111.28,161.20,10.91;10,112.66,124.83,84.85,10.91">Bird species recognition via neural architecture search</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mühling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Korfhage</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Freisleben</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_188.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="10,466.22,124.83,39.77,10.91;10,112.66,138.38,293.34,10.91">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="10,287.49,151.93,151.17,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25, 2020. 2696. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,179.03,393.33,10.91;10,112.66,192.57,394.52,10.91;10,112.66,206.12,393.32,10.91;10,112.66,219.67,393.32,10.91;10,112.66,233.22,360.25,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,247.73,206.12,258.25,10.91;10,112.66,219.67,303.29,10.91">Overview of lifeclef 2021: a system-oriented evaluation of automated species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Castañeda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dorso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,439.18,219.67,66.80,10.91;10,112.66,233.22,330.51,10.91">Proceedings of the Twelfth International Conference of the CLEF Association (CLEF 2021)</title>
		<meeting>the Twelfth International Conference of the CLEF Association (CLEF 2021)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,246.77,394.53,10.91;10,112.66,260.32,393.33,10.91;10,112.66,273.87,328.66,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,112.66,260.32,331.54,10.91">Overview of birdclef 2021: Bird call identification in soundscape recordings</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,467.18,260.32,38.81,10.91;10,112.66,273.87,297.96,10.91">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,287.42,393.33,10.91;10,112.66,300.97,393.33,10.91;10,112.66,314.52,393.33,10.91;10,112.41,328.07,395.25,10.91;10,112.66,341.62,179.18,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,380.48,287.42,125.51,10.91;10,112.66,300.97,204.43,10.91">Birdvox-full-night: A dataset and benchmark for avian flight call detection</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lostanlen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Farnsworth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kelling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2018.8461410</idno>
		<ptr target="https://doi.org/10.1109/ICASSP.2018.8461410.doi:10.1109/ICASSP.2018.8461410" />
	</analytic>
	<monogr>
		<title level="m" coord="10,366.52,300.97,139.47,10.91;10,112.66,314.52,261.46,10.91">IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2018</title>
		<meeting><address><addrLine>Calgary, AB, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-04-15">2018. April 15-20, 2018. 2018</date>
			<biblScope unit="page" from="266" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,355.17,396.29,10.91;10,112.66,368.71,228.52,10.91" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">O</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="http://ccrma.stanford.edu/jos/-sasp/" />
		<title level="m" coord="10,167.95,355.17,148.88,10.91">Spectral Audio Signal Processing</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>edition</note>
</biblStruct>

<biblStruct coords="10,112.66,382.26,395.17,10.91;10,112.66,395.81,394.53,10.91;10,112.14,409.36,393.85,10.91;10,112.66,422.91,394.53,10.91;10,112.66,436.46,394.41,10.91;10,112.66,450.01,395.01,10.91;10,112.66,463.56,187.11,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,416.26,382.26,91.58,10.91;10,112.66,395.81,330.55,10.91">Mixup-based acoustic scene classification using multi-channel convolutional neural network</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00764-5_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00764-5_2.doi:10.1007/978-3-030-00764-5" />
	</analytic>
	<monogr>
		<title level="m" coord="10,336.51,409.36,169.48,10.91;10,112.66,422.91,323.45,10.91">Advances in Multimedia Information Processing -PCM 2018 -19th Pacific-Rim Conference on Multimedia</title>
		<title level="s" coord="10,224.09,436.46,94.38,10.91;10,406.84,437.47,100.23,9.72;10,112.66,451.02,54.41,9.72">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Ngo</surname></persName>
		</editor>
		<meeting><address><addrLine>Hefei, China</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">September 21-22, 2018. 2018</date>
			<biblScope unit="volume">11166</biblScope>
			<biblScope unit="page" from="14" to="23" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct coords="10,112.66,477.11,393.33,10.91;10,112.66,490.66,394.62,10.91;10,112.66,504.21,239.89,10.91" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="10,376.93,477.11,129.06,10.91;10,112.66,490.66,233.39,10.91">Panns: Large-scale pretrained audio neural networks for audio pattern recognition</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1912.10211" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,517.76,394.62,10.91;10,112.28,531.30,393.70,10.91;10,112.66,544.85,122.28,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,474.12,517.76,33.16,10.91;10,112.28,531.30,185.47,10.91">librosa: Audio and music signal analysis in python</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mcvicar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,321.11,531.30,184.87,10.91;10,112.66,544.85,46.30,10.91">Proceedings of the 14th python in science conference</title>
		<meeting>the 14th python in science conference</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,558.40,394.53,10.91;10,112.66,571.95,394.61,10.91;10,112.66,585.50,244.15,10.91" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="10,196.84,571.95,148.36,10.91">Resnest: Split-attention networks</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="2004.08955" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,599.05,393.33,10.91;10,112.66,612.60,365.41,10.91" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="10,277.19,599.05,193.94,10.91">Densely connected convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1608.06993" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,626.15,393.33,10.91;10,112.66,639.70,365.41,10.91" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="10,319.21,626.15,154.19,10.91">Focal loss for dense object detection</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1708.02002" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
