<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.69,84.74,410.66,15.42;1,88.69,106.66,81.21,15.43">Automatic Snake Classification using Deep Learning Algorithm</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,97.94,11.96"><forename type="first">Lekshmi</forename><surname>Kalinathan</surname></persName>
							<email>lekshmik@ssn.edu.</email>
							<affiliation key="aff0">
								<orgName type="department">Department of CSE</orgName>
								<orgName type="institution">SSN College of Engineering</orgName>
								<address>
									<addrLine>Rajiv Gandhi Salai</addrLine>
									<settlement>Chennai</settlement>
									<region>Tamil Nadu</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,199.87,134.97,127.97,11.96"><forename type="first">Prabavathy</forename><surname>Balasundaram</surname></persName>
							<email>prabavathyb@ssn.edu.</email>
							<affiliation key="aff0">
								<orgName type="department">Department of CSE</orgName>
								<orgName type="institution">SSN College of Engineering</orgName>
								<address>
									<addrLine>Rajiv Gandhi Salai</addrLine>
									<settlement>Chennai</settlement>
									<region>Tamil Nadu</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,340.48,134.97,78.96,11.96"><forename type="first">Pradeep</forename><surname>Ganesh</surname></persName>
							<email>pradeep19077@cse.ssn.edu.</email>
							<affiliation key="aff0">
								<orgName type="department">Department of CSE</orgName>
								<orgName type="institution">SSN College of Engineering</orgName>
								<address>
									<addrLine>Rajiv Gandhi Salai</addrLine>
									<settlement>Chennai</settlement>
									<region>Tamil Nadu</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,148.92,117.14,11.96"><forename type="first">Sandeep</forename><forename type="middle">Sekhar</forename><surname>Bathala</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of CSE</orgName>
								<orgName type="institution">SSN College of Engineering</orgName>
								<address>
									<addrLine>Rajiv Gandhi Salai</addrLine>
									<settlement>Chennai</settlement>
									<region>Tamil Nadu</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,237.42,148.92,106.65,11.96"><forename type="first">Rahul</forename><forename type="middle">Kumar</forename><surname>Mukesh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of CSE</orgName>
								<orgName type="institution">SSN College of Engineering</orgName>
								<address>
									<addrLine>Rajiv Gandhi Salai</addrLine>
									<settlement>Chennai</settlement>
									<region>Tamil Nadu</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.69,84.74,410.66,15.42;1,88.69,106.66,81.21,15.43">Automatic Snake Classification using Deep Learning Algorithm</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">AF08B08D6446D9B41BADBBEFE8AE8DD5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Snake Classification</term>
					<term>Deep Learning</term>
					<term>ResNet-50</term>
					<term>ResNeXt</term>
					<term>Keras</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic snake classification is the process of identifying snake species using image processing techniques. This system is helpful in reducing the death by snake bites and to suggest appropriate antivenom for the victim in a short span of time. The previous works have built systems with relatively smaller databases using ML and older deep architectures. The systems were capable of identifying only a few snake species and/or had lower accuracies. However, the Snake classification can further be improved in order to make the system more robust. The proposed system is capable of identifying 772 classes of snake species and is built with a relatively larger dataset using newer deep learning architecture ResNeXt50-V2. An ensembled model is used to further improve the system to achieve an accuracy of 85.7% and F1-score of 0.68.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Death and amputation caused by snake bites is major cause of concern in health care institutions. There are approximately 1.8 to 2.7 million cases of envenoming each year of which 435,000 to 580,000 snake bites need treatment, for they can cause permanent disability and disfigurement.</p><p>Although only about 20% of snake species worldwide are medically important, identifying the biting snake is challenging, especially due to:</p><p>• The high diversity of snake species in snakebite endemic countries (e.g. 310 snake species in India) • The limited herpetological knowledge of communities and healthcare providers confronted with snakebite • Incomplete knowledge of their epidemiological importance Often people who are bitten by snakes, are unable to get the required treatment immediately as they are ineffectual in neither identifying the snake nor giving adequate information about the snake that bit them. This leads to the physician to guess the type of snake involved in the situation. If the identification of snake determined by the medical practitioner based on the clues given is wrong, it will lead to further complications in the case, and, unfortunately can lead to the death of the patient. This is one of the reasons that decreases the survival rate of patients of snake bite cases.</p><p>Hence, if we are able to identify a snake using pictures taken in low resolution, it would improve the survival rate of the patients. Moreover, the system will also be useful for conservation of wildlife as it can be used to identify endangered species of snakes.</p><p>When snake species identification is automated, it could ameliorate public health as there are only a few initiatives that seek to identify snakes using computer vision techniques. So far only a handful of computer vision and machine learning algorithms specific to snakes have been developed. The best existing model as of 2020 achieved an F1-score of 0.625 using a pre-trained model, with Vanilla ResNet50-V2 architecture. However, none of these are yet usable in real-world situations where lives may be at stake. Hence, creating an automatic and robust system for snake species identification is an important goal for biodiversity, conservation, and global health.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Alex James et al. <ref type="bibr" coords="2,171.99,348.83,13.00,10.91" target="#b0">[1]</ref> presented a parallel processed inter-feature product similarity fusion based automatic classification of kinds of snakes such as Spectacled Cobra, Russel's Viper and King Cobra to name a few. The authors used a database of 88 images of cobra and 48 images of viper for the initial feature taxonomy analysis and identified 31 different taxonomically relevant features from snake images for automated snake classification studies. In this paper, the authors use the nearest-neighbour classifier. This classifier identifies the class of unknown data sample from its nearest neighbour, whose class is already known. For automatic classification, the taxonomically relevant features are selected from the snake images and are normalized using mean-variance filtering. The histograms of gradients and orientation of these normalized features are used as feature vectors. These feature vectors are evaluated using a proposed minimum distance product similarity metric classifier. The proposed system was able to achieve an F-score of 0.91 when 5% of the class samples are used as gallery and remaining 95% of sample are used as test on snake image database. The authors analysed the scalability and real-time implementation of the classifier through GPU enabled parallel computing environment. The developed systems found their application in wild life studies, analysis of snake bites and in management of snake population.</p><p>Alex Pappachen James et al. <ref type="bibr" coords="2,231.36,565.62,13.00,10.91" target="#b1">[2]</ref> in his paper presented the automatic snake identification problem by developing a taxonomy-based feature, targeted for use by computer scientists and herpetologists. The feature database contained 38 taxonomically relevant features of each sample. Out of these 38 features, top features that have highest impact on classification were determined. In order to find the top features from the complete database, twelve attribute evaluators (ChiSquared AttributeEval and CfsSubsetEval to name a few) were used. Along with this, a combination of certain search methods like Genetic Search, Greedy Step-wise and Linear Forward Selection was used. The feature-subset analysis on the dataset indicated that only 15 features are sufficient for snake identification. It was found that these features were almost equally distributed from the logical grouping of top, side and body views of snake images. Features from the bottom view of snakes had the least role in the snake identification. In order to perform automated snake classification, 13 classifiers (Bayes Net, Naïve Bayes and Multilayer perception to name a few) were used. Using these classifiers, the best F-score obtained was about 0.94.</p><p>Louise Bloch et al. <ref type="bibr" coords="3,186.96,168.26,13.00,10.91" target="#b2">[3]</ref> implemented a machine learning workflow that uses Mask Regionbased Convolutional Neural Network (Mask R-CNN) for object detection, and EfficientNets for classification. The best model submitted by them in the SnakeCLEF 2020 <ref type="bibr" coords="3,446.32,195.36,13.00,10.91" target="#b3">[4]</ref> challenge achieved an F1-score of 0.404. After the expiration of this deadline, it was found that it could be improved to achieve an F1-score of 0.594. The main improvements in snake species classification presented by them were based on increasing the image size, combining location and image information as well as upscaled model architecture.</p><p>Moorthy Gokula Krishnan <ref type="bibr" coords="3,215.80,263.11,12.68,10.91" target="#b4">[5]</ref> wrote a report on the improvement of F1-score using pre-trained network on the large dataset for pre training. Vanilla ResNet50-V2 was used for pretraining of the database and comparison was drawn between F1-scores of the model which used pretrained network and model which did not use pre trained network. The conclusion of the comparison was that model with pretrained network had performed better, the F1-score improved from 0.5813 to 0.6018. The test dataset on which the final scores were calculated, follows a data distribution similar to validation dataset and the model achieves an F1-score of 0.625 when tested on AICrowd platform <ref type="bibr" coords="3,217.48,357.95,11.43,10.91" target="#b5">[6]</ref>.</p><p>Patel A et al. <ref type="bibr" coords="3,163.78,371.50,13.00,10.91" target="#b6">[7]</ref> used deep learning methods to develop an application for smartphones which distinguishes images of 9 different snake species that dwell on the Galápagos Islands in Ecuador. In order to achieve this, object detection as well as classification algorithms have been used. The images from the dataset were collected from Tropical Herping collection of image and Web scraping images from Google and Flickr sites. Different combinations of architecture models such as Faster R-CNN, Inception V2, ResNet, MobileNet, and VGG16 have been tested for object detection and image classification. The model which was based on Faster R-CNN with ResNet achieved the best classification accuracy of 75 %.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>This section discusses about the dataset and base methodology utilized for implementing snake classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>The data set provided by SnakeCLEF2021 <ref type="bibr" coords="3,265.44,588.25,15.17,10.91" target="#b7">[8,</ref><ref type="bibr" coords="3,283.32,588.25,8.88,10.91" target="#b8">9]</ref> -Snake species Identification Challenge consists of a total of 412,537 images. This dataset contains images of 772 different snake species from 188 countries. The majority of data was gathered from online biodiversity platforms such as iNaturalist and HerpMapper. It was further extended by data scraped from Flickr and images collected from private collections and museums. The final dataset has a heavy long-tailed class distribution, where the most frequent species (Thamnophis sirtalis) was represented by 22,163 images and the least frequent by just 10 images (Achalinus formosanus). The final data set was split into training and validation sets consisting of 347,405 and 38,601 images respectively. Both subsets have the same class distribution, while the minimum number of validation images per class is one. A set of 26,531 images containing all 772 classes with a similar class distribution was used as the testing set. These images are associated with metadata that provides information about the continent and country of the place where the image has been taken. For some snake depictions, the information is not given and only "UNKNOWN" is provided in the metadata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Deep Network Architecture used</head><p>A baseline model of Convolutional Neural Network (CNN) can be scaled up in hopes of achieving better performance and to solve complex problems. However, it has been found that adding more layers in the network potentially degrades the performance. This may be due to optimization function, initialization of the network and more specifically vanishing gradient problem. This performance degradation has been addressed by ResNet architecture <ref type="bibr" coords="4,397.28,258.64,16.25,10.91" target="#b9">[10]</ref>.</p><p>The core idea of ResNet is residual network block. This block consists of skip connection which skips some layers in the neural network by feeding the output of one layer to the subsequent layer not necessarily the adjacent layer. By using a skip connection, an alternative path is provided for the gradient. This helps us in avoiding the vanishing gradient problem. To improve the effectiveness of ResNet architecture without increasing the number of parameters, ResNeXt <ref type="bibr" coords="4,178.35,652.12,18.07,10.91" target="#b10">[11]</ref> architecture was proposed. In ResNeXt architecture, each residual network block shown in Figure <ref type="figure" coords="4,234.22,665.66,5.17,10.91" target="#fig_0">1</ref> (a) is split into n number of paths. Number of paths inside the ResNeXt block is defined as cardinality. In Figure <ref type="figure" coords="5,329.43,86.97,5.11,10.91" target="#fig_0">1</ref> (b), the cardinality is 32. All the paths contain the same topology. Both the ResNet and ResNeXt network blocks have different width. Layer-1 in ResNet has one convolution layer with 64 width, while layer-1 in ResNeXt has 32 different convolution layers with 4 width (32*4 width). Validation error will be decreased with the increase of cardinality. Hence, the performance of the network will be improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation</head><p>The proposed Automatic Snake Classification system was developed by following the steps:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Preprocessing the data • Batch Accumulation • Building the model • Ensemble Model for testing</head><p>The automatic snake classification system was developed using ResNeXt50-V2 model with Keras framework <ref type="bibr" coords="5,171.38,303.08,16.41,10.91" target="#b11">[12]</ref>. This model was trained using a machine with Intel Xeon Processor W-2145, 11M cache, 3.70GHz, 2*16GB DDR4 -2666Mhz ECC REG DIMM 2TB SATA 7.2K RPM 3.5" HDDs, 240 GB SATA SSD 2.5" HDDs and NVIDIA GeForce RTX2080 Ti, 11 GB. In our implementation, as per the number of images given in Table <ref type="table" coords="5,395.54,483.56,3.66,10.91" target="#tab_0">1</ref>, training data is used to train the model. Validation data is used to test the model obtained during every epoch to fine tune the weight parameters. Test data is used to test the final model obtained after the entire training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Preprocessing the data</head><p>The images provided by the SnakeClef2021 dataset are of varied sizes. The training images are first resized to 224 × 224 × 3 dimensions using a random crop resized operation. The images were also horizontally and vertically flipped with a probability of 0.5. Further, the images were scaled and rotated with a probability of 0.5. The images were also normalized with a standard deviation and mean of 0.23 and 0.5. Similarly, the images of validation and the testing dataset were resized to 224 × 224 × 3 dimensions using resize function. The images were also normalized with a standard deviation and mean of 0.23 and 0.5 respectively. Both the training and testing images were augmented using the Albumentations <ref type="bibr" coords="5,421.48,632.60,17.84,10.91" target="#b12">[13]</ref> python library with the parameters of augmentation method such as RandomResizedCrop, Transpose, Resize, HorizontalFlip. VerticalFlip, ShiftScaleRotate and Normalize. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Batch Accumulation</head><formula xml:id="formula_0" coords="6,123.48,173.19,348.31,335.52">-[64, 1 × 1, 128] L2-[128, 3 × 3, 128] L3-[128, 1 × 1, 256] L1-[256, 1 × 1, 128] L2-[128, 3 × 3, 128] L3-[128, 1 × 1, 256] L1-[256, 1 × 1, 128] L2-[128, 3 × 3, 128] L3-[128, 1 × 1, 256] Conv3 4 Stacked Residual Blocks L1-[256, 1 × 1, 256] L2-[256, 3 × 3, 256] L3-[256, 1 × 1, 512] Downsampling L0 -[256, 1 × 1, 512] L1-[512, 1 × 1, 256] L2-[256, 3 × 3, 256] L3-[256, 1 × 1, 512] L1-[512, 1 × 1, 256] L2-[256, 3 × 3, 256] L3-[256, 1 × 1, 512] L1-[512, 1 × 1, 256] L2-[256, 3 × 3, 256] L3-[256, 1 × 1, 512] Conv4 6 Stacked Residual Blocks L1-[512, 1 × 1, 512] L2-[512, 3 × 3, 512] L3-[512, 1 × 1, 1024] Downsampling L0 -[512, 1 × 1, 1024] L1-[1024, 1 × 1, 512] L2-[512, 3 × 3, 512] L3-[512, 1 × 1, 1024] L1-[1024, 1 × 1, 512] L2-[512, 3 × 3, 512] L3-[512, 1 × 1, 1024] L1-[1024, 1 × 1, 512] L2-[512, 3 × 3, 512] L3-[512, 1 × 1, 1024] L1-[1024, 1 × 1, 512] L2-[512, 3 × 3, 512] L3-[512, 1 × 1, 1024] L1-[1024, 1 × 1, 512] L2-[512, 3 × 3, 512] L3-[512, 1 × 1, 1024] Conv5 3 Stacked Residual Blocks L1-[1024, 1 × 1, 1024] L2-[1024, 3 × 3, 1024] L3-[1024, 1 × 1, 1024] Downsampling L0 -[1024, 1 × 1, 2048] L1-[2048, 1 × 1, 1024] L2-[1024, 3 × 3, 1024] L3-[1024, 1 × 1, 2048] L1-[2048, 1 × 1, 1024] L2-[1024, 3 × 3, 1024] L3-[1024, 1 × 1, 2048] Softmax L1 -[2048, -, 772]</formula><p>Training data is split into 3860 batches where each batch consists of 5 folds or mini-batches. Training will be inefficient due to noisy gradients, if the mini-batch is small and consists of a set of images without the inclusion of all the classes. Hence, the images in mini-batch have been chosen in such a way that it covers all the classes. A stratified k-fold cross-validation where k = 5, has been adapted to enforce the class distribution in each split of the data to match the distribution in the complete training dataset. Cosine annealing with warm restarts scheduler has been used to update the learning rate. Learning rate of 1e-4 and Adam optimizer <ref type="bibr" coords="6,463.94,608.73,17.81,10.91" target="#b13">[14]</ref> were used to fine-tune the weight matrices of the architecture.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Analysis</head><p>This section discusses the analysis of training and testing processes through various performance measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Analysis of training process</head><p>The training process was analysed in order to determine whether the model is stable and free from overfitting.</p><p>During the training process, F1-Score and training accuracy were measured for every epoch and plotted as shown in Figure <ref type="figure" coords="8,233.67,336.23,10.26,10.91" target="#fig_2">2a</ref> and 2b respectively. It is inferred from Figure <ref type="figure" coords="8,457.14,336.23,10.26,10.91" target="#fig_2">2a</ref> that the F1-Score is increasing right from the very first epoch and get stabilized to 0.6008 from 15 𝑡ℎ epoch. From Figure <ref type="figure" coords="8,179.91,363.33,8.74,10.91" target="#fig_2">2b</ref>, it is inferred that the training accuracy is stabilized from 11 𝑡ℎ epoch. Similarly, the training and validation losses were also measured for every epoch and plotted as shown in Figure <ref type="figure" coords="8,162.77,390.43,7.93,10.91" target="#fig_2">2c</ref>. It is clearly found from the graph that the validation loss for every epoch is lesser when compared to the training loss. From this it is inferred that the generated model is free from overfitting. During the testing process, instead of generating predictions with the recent model, our proposed ensemble model has been utilized. In order for ensembling, the models corresponding to the last 5 epochs i.e models 13, 14, 15, 16 and 17 were considered. These models were chosen to be the top models as the accuracy and F1-Score were stabilized during the corresponding epochs. In the process of ensembling, the predictions generated from every model are averaged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Analysis of testing process</head><p>Experiments were conducted to generate the predictions from every individual and ensembled model as shown in Table <ref type="table" coords="9,201.41,100.52,3.71,10.91" target="#tab_2">3</ref>. Models 13, 14, 15, 16 and 17 generate different predictions for each image due to the fact that they have different weight matrices. Each model may have specialised in predictions for distinct set of images. In the ensembled model, the average of the predictions generated from all the models in consideration is taken. This increases the accuracy of the model manifold. The same can be deduced from the table. The ensembled model generated an Accuracy and F1-Country Score of 85.77% and 0.724 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Future Work</head><p>In conclusion, it can be stated that snake species identification is a challenging task, primarily because of the high diversity of snake species and the data set having a heavy long-tailed class distribution. Automatic classification has been built using ResNeXt50-V2 Architecture and the model achieved an F1-country score of 0.724. In future, accuracy of the system can be enhanced through preprocessing of the images. Subsequently, alternate deep architectures can be implemented and tested for better accuracies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,202.18,608.92,191.17,8.93;4,121.41,335.54,352.46,260.81"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: ResNet and ResNeXt Network Block</figDesc><graphic coords="4,121.41,335.54,352.46,260.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,160.74,263.86,87.21,9.96;7,367.16,263.86,91.06,9.96;7,262.54,461.57,69.90,9.96"><head></head><label></label><figDesc>(a) Epoch vs F1-Score (b) Epoch vs Accuracy (c) Epoch vs Loss</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,166.99,480.05,261.29,8.93;7,182.44,281.90,230.40,172.80"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance Measures of ResNeXt50-V2 Architecture</figDesc><graphic coords="7,182.44,281.90,230.40,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,88.99,395.59,354.02,71.80"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="5,152.27,407.60,290.75,59.79"><row><cell></cell><cell cols="2">Details of SnakeClef 2021 dataset</cell><cell></cell></row><row><cell>Subset</cell><cell cols="3"># of images % of data min. # of images/class</cell></row><row><cell>Training data</cell><cell>347,405</cell><cell>84.21%</cell><cell>9</cell></row><row><cell>Validation data</cell><cell>38,601</cell><cell>9.36%</cell><cell>1</cell></row><row><cell>Test data</cell><cell>26,531</cell><cell>6.43%</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,88.99,120.66,305.42,61.66"><head>Table 2</head><label>2</label><figDesc>Model Summary of ResNeXt50-V2 Architecture</figDesc><table coords="6,123.48,148.70,257.69,33.62"><row><cell>Conv1</cell><cell>L1 -[224, 7 x 7, 64]</cell></row><row><cell>Conv2</cell><cell>3 Stacked Residual Blocks</cell></row><row><cell>L1</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,88.99,482.94,295.26,34.74"><head>Table 3</head><label>3</label><figDesc>Performance measures of different models ResNeXt50</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,149.54,508.76,296.20,83.05"><head>-V2 Models F1-Country Score F1-Score Accuracy Model-13</head><label></label><figDesc></figDesc><table coords="8,149.54,521.16,283.46,70.64"><row><cell></cell><cell>0.42</cell><cell>0.38</cell><cell>0.66</cell></row><row><cell>Model-14</cell><cell>0.51</cell><cell>0.40</cell><cell>0.69</cell></row><row><cell>Model-15</cell><cell>0.44</cell><cell>0.40</cell><cell>0.68</cell></row><row><cell>Model-16</cell><cell>0.40</cell><cell>0.38</cell><cell>0.68</cell></row><row><cell>Model-17</cell><cell>0.37</cell><cell>0.37</cell><cell>0.68</cell></row><row><cell>Ensembled Model</cell><cell>0.67</cell><cell>0.68</cell><cell>0.86</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7.">Acknowledgement</head><p>We thank the <rs type="institution">CSE department of SSN College of Engineering</rs> for letting us utilize the GPU server machine extensively to implement this task.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,425.43,303.43,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,157.94,425.43,142.63,10.91">Snake classification from images</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>James</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,309.30,425.43,67.07,10.91">PeerJ Preprints</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,438.98,393.33,10.91;9,112.66,452.53,393.33,10.91;9,112.66,466.08,133.46,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,388.92,438.98,117.07,10.91;9,112.66,452.53,232.61,10.91">Discriminative histogram taxonomy features for snake species identification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mathews</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sugathan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">K</forename><surname>Raveendran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,358.46,452.53,147.53,10.91;9,112.66,466.08,93.74,10.91">Human-Centric Computing and Information Sciences</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,479.63,394.53,10.91;9,112.66,493.18,393.33,10.91;9,112.66,506.73,394.53,10.91;9,112.66,520.28,22.69,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,238.82,493.18,267.17,10.91;9,112.66,506.73,274.98,10.91">Combination of image and location information for snake species identification using object detection and efficientnets</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Boketta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Keibel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mense</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Michailutschenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Willemeit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,411.54,506.73,91.16,10.91">CLEF working notes</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,533.83,393.33,10.91;9,112.66,547.38,395.17,10.91;9,112.66,560.93,232.29,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,384.50,533.83,121.49,10.91;9,112.66,547.38,238.13,10.91">Overview of the snakeclef 2020: Automatic snake species identification challenge</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Castaneda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sharada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,373.68,547.38,134.15,10.91;9,112.66,560.93,201.59,10.91">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,574.48,394.62,10.91;9,112.14,588.02,370.35,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,194.78,574.48,284.50,10.91">Impact of pretrained networks for snake species classification</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,112.14,588.02,339.66,10.91">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,601.57,394.53,10.91;9,112.66,615.12,393.33,10.91;9,112.66,628.67,393.33,10.91;9,112.66,642.22,104.57,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,112.66,615.12,393.33,10.91;9,112.66,628.67,281.21,10.91">Supervised learning computer vision benchmark for snake species identification from photographs: Implications for herpetology and global health</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">P</forename><surname>Mohanty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Salathé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Castañeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,407.34,628.67,98.64,10.91;9,112.66,642.22,51.98,10.91">Frontiers in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,86.97,393.33,10.91;10,112.66,100.52,393.33,10.91;10,112.41,114.06,60.01,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,444.88,86.97,61.11,10.91;10,112.66,100.52,348.36,10.91">Revealing the unknown: real-time recognition of galápagos snake species using deep learning</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Khatod</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Matijosaitiene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arteaga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Gilkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,469.28,100.52,36.71,10.91">Animals</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">806</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,127.61,394.62,10.91;10,112.28,141.16,393.70,10.91;10,112.66,154.71,288.62,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,376.16,127.61,131.11,10.91;10,112.28,141.16,288.08,10.91">Overview of snakeclef 2021: Automatic snake species identification with country-level focus</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Castañeda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,425.01,141.16,80.97,10.91;10,112.66,154.71,257.93,10.91">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,168.26,393.33,10.91;10,112.66,181.81,394.52,10.91;10,112.66,195.36,393.32,10.91;10,112.66,208.91,393.32,10.91;10,112.66,222.46,360.25,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,247.73,195.36,258.25,10.91;10,112.66,208.91,303.29,10.91">Overview of lifeclef 2021: a system-oriented evaluation of automated species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Castañeda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dorso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,439.18,208.91,66.80,10.91;10,112.66,222.46,330.51,10.91">Proceedings of the Twelfth International Conference of the CLEF Association (CLEF 2021)</title>
		<meeting>the Twelfth International Conference of the CLEF Association (CLEF 2021)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,236.01,393.33,10.91;10,112.66,249.56,393.33,10.91;10,112.66,263.11,130.69,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,307.29,236.01,198.70,10.91;10,112.66,249.56,152.62,10.91">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,287.99,249.56,217.99,10.91;10,112.66,263.11,50.10,10.91">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,276.66,393.32,10.91;10,112.66,290.20,393.33,10.91;10,112.66,303.75,81.48,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,301.73,276.66,204.25,10.91;10,112.66,290.20,68.94,10.91">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,204.45,290.20,301.54,10.91;10,112.66,303.75,51.39,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,317.30,307.89,10.91" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="10,181.79,317.30,111.53,10.91">Deep learning with Keras</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gulli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Packt Publishing Ltd</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,330.85,394.53,10.91;10,112.28,344.40,368.43,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,112.28,344.40,243.36,10.91">Albumentations: fast and flexible image augmentations</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Buslaev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">I</forename><surname>Iglovikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Khvedchenya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Parinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Druzhinin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Kalinin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,364.46,344.40,53.51,10.91">Information</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">125</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,357.95,393.33,10.91;10,112.66,371.50,155.93,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,218.14,357.95,157.79,10.91">A method for stochastic optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,397.07,357.95,108.92,10.91;10,112.66,371.50,125.91,10.91">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
