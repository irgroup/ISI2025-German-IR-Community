<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,403.65,15.42;1,89.29,106.66,198.80,15.42">Overview of BirdCLEF 2021: Bird call identification in soundscape recordings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,56.05,11.96"><forename type="first">Stefan</forename><surname>Kahl</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Cornell Lab of Ornithology</orgName>
								<orgName type="laboratory">K. Lisa Yang Center for Conservation Bioacoustics</orgName>
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,157.98,134.97,61.33,11.96"><forename type="first">Tom</forename><surname>Denton</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Google LLC</orgName>
								<address>
									<settlement>San Francisco</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,231.96,134.97,68.40,11.96"><forename type="first">Holger</forename><surname>Klinck</surname></persName>
							<email>holger.klinck@cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Cornell Lab of Ornithology</orgName>
								<orgName type="laboratory">K. Lisa Yang Center for Conservation Bioacoustics</orgName>
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,313.00,134.97,63.84,11.96"><forename type="first">Hervé</forename><surname>Glotin</surname></persName>
							<email>herve.glotin@univ-tln.fr</email>
							<affiliation key="aff2">
								<orgName type="department">AMU</orgName>
								<orgName type="institution" key="instit1">University of Toulon</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">LIS</orgName>
								<address>
									<settlement>Marseille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,389.48,134.97,64.34,11.96"><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
							<email>herve.goeau@cirad.fr</email>
							<affiliation key="aff3">
								<orgName type="laboratory">CIRAD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,88.72,148.92,101.89,11.96"><forename type="first">Willem-Pier</forename><surname>Vellinga</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Xeno-canto Foundation</orgName>
								<address>
									<settlement>Groningen</settlement>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,203.26,148.92,75.03,11.96"><forename type="first">Robert</forename><surname>Planqué</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Xeno-canto Foundation</orgName>
								<address>
									<settlement>Groningen</settlement>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,309.28,148.92,52.66,11.96"><forename type="first">Alexis</forename><surname>Joly</surname></persName>
							<email>alexis.joly@inria.fr</email>
							<affiliation key="aff5">
								<orgName type="laboratory" key="lab1">Inria</orgName>
								<orgName type="laboratory" key="lab2">LIRMM</orgName>
								<orgName type="institution" key="instit1">University of Montpellier</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,403.65,15.42;1,89.29,106.66,198.80,15.42">Overview of BirdCLEF 2021: Bird call identification in soundscape recordings</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">9BA71FF2303C04C0D06204230D90B1B7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>LifeCLEF</term>
					<term>bird</term>
					<term>song</term>
					<term>call</term>
					<term>species</term>
					<term>retrieval</term>
					<term>audio</term>
					<term>collection</term>
					<term>identification</term>
					<term>fine-grained classification</term>
					<term>evaluation</term>
					<term>benchmark</term>
					<term>bioacoustics</term>
					<term>passive acoustic monitoring</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conservation of bird species requires detailed knowledge of their spatiotemporal occurrence and distribution patterns. Over the past decade, passive acoustic monitoring (PAM) has become an essential tool to collect data on birds on ecologically relevant scales. However, these PAM efforts generate extensive datasets, and their comprehensive analysis remains challenging. Improved and fully automated acoustic analysis frameworks are needed to advance the field of avian conservation. The 2021 BirdCLEF challenge focused on developing and assessing automated analysis frameworks for avian vocalizations in continuous soundscape data. The primary task of the challenge was to detect and identify all bird calls within the hidden test dataset. This paper describes how the various algorithms were evaluated and synthesizes the results and lessons learned.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Birds are widely used to monitor ecosystem health because they live in most environments and occupy almost every niche within those environments. Traditionally, human observers monitor bird populations by conducting point count surveys in an area of interest. At sampling locations along a transect, the domain expert will visually and aurally count every bird in a given time window (e.g., 3 or 5 minutes). However, conducting these surveys is time-consuming and requires expert knowledge in the identification of birds. Because the number of observers is typically limited, the spatiotemporal resolution of the surveys is limited as well.</p><p>In contrast, passive acoustic monitoring (PAM) uses autonomous recording units (ARUs) to monitor the acoustic environment, often continuously, in the vicinity of the deployment location over extended periods (weeks to months). These data sets complement traditional bird surveys and help to improve our ability to accurately monitor the status and trends of bird populations and avian diversity more broadly.</p><p>While PAM surveys are very cost-effective to conduct, the handling and analysis of vast amounts of collected data (often tens or even hundreds of Terabytes) remains challenging. In the past, researchers frequently subsampled the collected data or focused on specific call types to circumnavigate this challenge. However, as a consequence, large amounts of data remain untouched, and new analysis frameworks are required to mine these datasets thoroughly. Effective analysis frameworks coming out of BirdCLEF and other competitions have the potential to revolutionize how we monitor and conserve birds and biodiversity in the future.</p><p>The LifeCLEF Bird Recognition Challenge (BirdCLEF) focuses on the development of reliable analysis frameworks to detect and identify avian vocalizations in continuous soundscape data. Launched in 2014, it has become one of the largest bird sound recognition competition in terms of dataset size and species diversity, with multiple tens of thousands of recordings covering up to 1,500 species <ref type="bibr" coords="2,160.81,303.75,11.36,10.91" target="#b0">[1,</ref><ref type="bibr" coords="2,174.89,303.75,7.57,10.91" target="#b1">2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BirdCLEF 2021 Competition Overview</head><p>Recent advances in the development of machine listening approaches to identify animal vocalizations have improved our ability to comprehensively analyze long-term acoustic datasets <ref type="bibr" coords="2,89.29,389.48,11.29,10.91" target="#b2">[3,</ref><ref type="bibr" coords="2,103.31,389.48,7.53,10.91" target="#b3">4]</ref>. However, it remains difficult to generate analysis outputs with high precision and recall, especially when targeting a high number of species simultaneously. Bridging the domain gap between high-quality training samples (focal recordings) and noisy test samples (soundscape recordings) is one of the most challenging tasks in the area of acoustic event detection and identification. The 2021 BirdCLEF competition tackled this complex task and was held on Kaggle. This year's edition was a so-called 'code competition' which encourages participants to publish their code for the benefit of the community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Goal and Evaluation Protocol</head><p>The 2021 BirdCLEF challenge focused on developing and assessing automated analysis frameworks for avian vocalizations in continuous soundscape data. The primary task of the challenge was to detect and identify all bird calls within the hidden test dataset. Each soundscape was divided into 5 second segments, and participants were tasked to return a list of audible species for each segment. The row-wise micro-averaged F1 score was used for evaluation. In previous editions, ranking metrics were used to assess the overall classification performance. However, when applying bird call identification frameworks to real-world data, a suitable confidence threshold must be set to balance precision and recall. The F1 score reflects this circumstance best. However, the selected threshold can significantly impact the overall performance, especially when applied to the hidden test dataset.</p><p>Precision and recall were determined based on the total number of true positives (TP), false positives (FP), and false negatives (FN) for each segment (i.e., row of the submission). More formally:</p><formula xml:id="formula_0" coords="3,158.79,136.80,275.32,24.43">Micro-Precision = 𝑇 𝑃 𝑇 𝑃 + 𝐹 𝑃 , Micro-Recall = 𝑇 𝑃 𝑇 𝑃 + 𝐹 𝑁</formula><p>The micro-F1 score as harmonic mean of the micro-precision and micro-recall for each segment was defined as:</p><formula xml:id="formula_1" coords="3,193.58,217.59,206.92,24.87">Micro-F1 = 2 × Micro-Precision × Micro-Recall Micro-Precision + Micro-Recall</formula><p>The average across all (segment-wise) micro-F1 scores was used as the final metric. Segments that did not contain a bird vocalization had to be marked with the 'nocall' label, which acted as an additional class label for non-events. The micro-averaged F1 score reduced the impact of rare events, which only contributed slightly to the overall metric if misidentified. The classification performance on common classes (i.e., species with high vocal presence) was well reflected in the metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Dataset</head><p>The 2021 BirdCLEF challenge featured one of the largest, fully annotated collections of soundscape recordings from four different recording locations in North and South America. Concerning real-world use cases, labels and metrics were chosen to reflect the vast diversity of bird vocalizations and variable ambient noise levels in omnidirectional recordings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Training Data</head><p>As in previous editions, training data were provided by the Xeno-canto community and consisted of more than 60,000 (high-quality, focal) recordings covering 397 species from two continents (North and South America). The maximum number of recordings for one species was limited to 500, which only affected a dozen species and resulted in a highly unbalanced dataset. Nine species contained less than 25 recordings, making it difficult to train reliable classifiers without an appropriate few-shot approach. Participants were allowed to use various metadata to develop their frameworks. Most notably, we provided detailed location information on recording sites of focal and soundscape recordings, allowing participants to account for migration and spatial distribution of bird species. Other metadata as secondary labels, call type, and recording quality were also provided, allowing participants to apply pre-and post-processing schemes which were not only based on audio inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Test Data</head><p>In this edition, test data were hidden and only accessible to participants during the inference process. This required participants to fine-tune their systems without knowing the value distribution of the test data. This approach more closely resembles real-world use cases where vast majorities of the recorded audio data have an unknown species composition. The hidden test data contained 80 soundscape recordings of 10-minute duration covering four distinct recording locations. All audio data were collected with passive acoustic recorders (SWIFT recorders, K. Lisa Yang Center for Conservation Bioacoustics, Cornell Lab of Ornithology<ref type="foot" coords="4,499.12,336.26,3.71,7.97" target="#foot_0">1</ref> ) deployed in Colombia (COL), Costa Rica (COR), the Sierra Nevada (SNE) of California, USA and the Sapsucker Woods Sanctuary (SSW) in Ithaca, New York, USA. Expert ornithologists provided annotations for a variety of quiet and extremely dense acoustic scenes (see Figure <ref type="figure" coords="4,443.75,378.66,3.50,10.91" target="#fig_0">1</ref>). In addition, a validation dataset with 200 minutes (20x 10-minute recordings) of soundscape data were also provided to allow participants to get a better understanding of the acoustic target domain. Participants were allowed to use these data for validation or during training. Soundscapes from the validation data only covered two (COR, SSW) of the four recording locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3.">Colombia (COL) &amp; Costa Rica (COR)</head><p>The Nespresso Biodiversity Index aims at quantifying the impact of eco-friendly coffee farms on the avian diversity in surrounding areas. In collaboration with the Cornell Lab of Ornithology, passive acoustic recorders were deployed on coffee farms in Colombia and Costa Rica to measure the transformative effects of sustainable farming by analyzing large amounts of acoustic data. Surveys are carried out twice a year to be able to capture how bird species are using the coffee landscape when both Neotropical resident and migratory birds are present (Nov-Mar) and around the peak of the breeding season for resident birds (April-Jun) <ref type="bibr" coords="4,389.08,563.48,11.28,10.91" target="#b4">[5]</ref>. Developing automated detection and identification frameworks can help to provide reliable results over relevant spatiotemporal scales and help researchers and decision-makers meet their conservation goals. Expert annotators provided annotations for 40 soundscape recordings of 10-minute duration collected at various recordings sites in Sep-Nov 2019. Additionally, ten fully annotated recordings from Costa Rica were provided to participants as training or validation data. Soundscapes from Colombian recording locations were exclusively part of the hidden test data. In contrast to many  other tropical recordings sites, these soundscapes did not contain a very high vocal diversity (due to the proximity to farmland). However, some rare species, for which only very few training examples were available, were present in the data. Therefore, the data from these two recording sites could be considered the most challenging of the competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4.">Sierra Nevada (SNE)</head><p>Measuring the effects of landscape management activities in the Sierra Nevada, California, USA can reveal a potential correlation with avian population density and diversity. Passive acoustic monitoring can help to reduce the costs of observational studies and expand the scale at which these studies can be conducted, provided there are robust bird call recognition systems <ref type="bibr" coords="5,492.22,594.96,11.58,10.91" target="#b5">[6]</ref>. For this dataset, passive acoustic surveys were conducted in the Lassen and Plumas National Forests in May-August 2018. Survey grid cells (4 km 2 ) were randomly selected from a 6,000-km 2 area, and recording units were deployed at acoustically advantageous locations (e.g., ridges rather than gullies) within those cells. The recordings were made from 04:00 to 08:00 for 5-7 days between May 9 and June 10 (sunrise was roughly 05:35-05:50 during that time) <ref type="bibr" coords="5,492.24,662.70,11.57,10.91" target="#b6">[7]</ref>.</p><p>Because of this, call density was particularly high in this dataset -most soundscapes reflected the species diversity during the dawn chorus. We randomly selected 20 expertly annotated 10-minute soundscape recordings, which were exclusively part of the hidden test data. Although sufficient amounts of training data were available for most annotated species, the high number of overlapping sounds posed a significant challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.5.">Sapsucker Woods (SSW)</head><p>As part of the Sapsucker Woods Acoustic Monitoring Project (SWAMP), the K. Lisa Yang Center for Conservation Bioacoustics at the Cornell Lab of Ornithology deployed 30 SWIFT recorders in the surrounding bird sanctuary area in Ithaca, NY, USA. This ongoing study aims to investigate the vocal activity patterns and diversity of local bird species. The data are also used to assess the impact of noise pollution on the behavior of birds. In 2018, expert birders annotated 20 full days of audio data recorded between January and June 2017 and provided almost 80,000 labels across randomly selected recordings. The 2019 edition of BirdCLEF <ref type="bibr" coords="6,397.74,271.79,12.99,10.91" target="#b7">[8]</ref> used twelve of these days as test data and three as validation data. This year, the amount of test data were limited to twenty 10-minute recordings, including previously unreleased data from this deployment. This reduction became necessary to balance the test data and to reduce the bias towards a specific dataset. Additionally, ten randomly selected recordings were provided as validation data to allow participants to fine-tune their frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head><p>1,004 participants from 70 countries on 816 teams entered the BirdCLEF 2021 competition and submitted a total of 9,307 runs. Figure <ref type="figure" coords="6,267.72,411.71,5.17,10.91" target="#fig_3">3</ref> illustrates the performance achieved by the top 50 collected runs. The private leaderboard score is the primary metric and was computed on roughly 65% of the test data (based on a random split). It was revealed to participants after the submission deadline to avoid probing the hidden test data. Public leaderboard scores were visible to participants over the course of the entire challenge and were determined on 35% of the entire test data.</p><p>The baseline F1 score in this year's edition was 0.4799 (public 0.5467), with all segments marked as non-events (i.e., nocall), and 686 teams managed to score above this threshold. The best submission achieved an F1 score of 0.6932 (public 0.7736), and the top 10 best-performing systems were within only 2% difference in score. Top-scoring participants were required to publish their code and associated write-up, lower-ranked participants opted to do so as well, which resulted in a vast collection of publicly available online resources. It also allowed organizers to inspect frameworks and approaches to assess the current state-of-the-art in this domain. Unsurprisingly, deep convolutional neural networks were the go-to tool in this competition, similar to previous editions. In many cases, participants chose to use off-the-shelve architectures pre-trained on ImageNet (like EfficientNet <ref type="bibr" coords="6,346.74,614.95,11.59,10.91" target="#b8">[9]</ref>, DenseNet <ref type="bibr" coords="6,412.54,614.95,16.41,10.91" target="#b9">[10]</ref>, or ResNet <ref type="bibr" coords="6,483.83,614.95,15.89,10.91" target="#b10">[11]</ref>). The vast majority of systems used mel scale spectrograms as input data, applied mixup <ref type="bibr" coords="6,469.56,628.50,17.76,10.91" target="#b11">[12]</ref> and specaugment <ref type="bibr" coords="6,151.88,642.04,18.06,10.91" target="#b12">[13]</ref> to diversify the training data. Provided metadata like time and location of training recordings were used to estimate the occurrence probability of individual bird species to post-filter predictions in many submissions. In addition to code repositories and online write-ups, eight teams also submitted full working notes, which are summarized below:</p><p>Murakami, Tanaka &amp; Nishimori [14], Team Dr.北村の愉快な仲間たち: This team factorized the problem into three tasks: Nocall detection, bird call classification, and post-processing based on provided metadata. The detection and classification backbone was a ResNet50, which used pre-computed mel scale spectrograms as inputs. This team employed a sophisticated scheme of post-processing, using gradient boosting decision trees to eliminate false detections. The overall approach is computationally very efficient and required only few resources for training and inference. The final submission achieved an F1 score of 0.6932 (public 0.7736). <ref type="bibr" coords="7,219.10,502.35,18.03,9.76" target="#b14">[15]</ref>, Team new baseline: Off-the-shelve model architectures pre-trained on ImageNet worked well in this competition. This team used an ensemble of 9 different pre-trained CNN architectures which used 30-second mel scale spectrograms as input. Most notably, this team used a sample mixup scheme which diversified the training data within and across samples by using non-event samples from previous editions and the freefield1010 dataset <ref type="bibr" coords="7,184.70,569.16,16.40,10.91" target="#b15">[16]</ref>. Context windows were split into 5-second segments for inference. The final submission achieved an F1 score of 0.6893 (public 0.7998).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Henkel, Pfeiffer &amp; Singer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conde, et al. [17]</head><p>, Team Ed and Satoru: This team's solution also relied on the performance of pre-trained models. In this case, the backbone consisted of a ResNeSt which used spectrograms as input. During post-processing, the rolling mean of model confidence scores and clip wise confidence scores were used to eliminate false positives. The final submission achieved an F1 score of 0.6738 (public 0.7801) and consisted of 13 different models, including best performing models from the 2020 Kaggle bird call recognition competition.</p><p>Puget <ref type="bibr" coords="8,120.17,87.90,18.03,9.76" target="#b17">[18]</ref>, Team CPMP: Transformers are the go-to model architecture for text processing. Only recently, vision transformers achieved state-of-the-art results on ImageNet <ref type="bibr" coords="8,445.73,100.52,17.84,10.91" target="#b18">[19]</ref> and even for acoustic event recognition <ref type="bibr" coords="8,224.85,114.06,16.24,10.91" target="#b19">[20]</ref>. This team tried to adapt vision transformers to the task of bird call recognition and achieved very strong results without the need for large CNN model ensembles. Again, mel scale spectrograms were used as input data, however, patch extraction which accounted for the sequential nature of acoustic data allowed the use of pre-trained transformer models despite visually distorted input data. The final input consisted of a 256x576 pixel spectrogram in which each of the 576 time steps contains 16x16 pixels. This way the entire spectrogram can be reshaped to 24x24 patches of size 16x16 -the input size of pre-trained vision transformers -while still exploiting the sequential structure of an audio signal. The best performing submission achieved an F1 score of 0.6736 (public 0.8015) with the best performing single transformer model achieving an F1 score of 0.6667 (public 0.7569).</p><p>Schlüter <ref type="bibr" coords="8,132.53,256.47,18.03,9.76" target="#b20">[21]</ref>, Team Jan Schlüter: This team used random 30-second crops from each training recording with binary labels for primary and secondary species to train an 18 model ensemble of CNNs. Notably, among mixup as a basic augmentation method, other strategies such as magnitude warping and linear fading of high frequencies were used to emulate variations seen in soundscape recordings. Predictions were made per file by pooling scores over consecutive windows. These per-file predictions were then used to post-filter predictions for 5-second segments. Using additional metadata such as location and time did not help to improve the results. The best submission achieved an F1 score of 0.6715 (public 0.7595).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shugaev, et al. [22], Team Just do it:</head><p>Strong nocall detection performance appeared to have significant impact on the overall score in this year's competition. This team manually labeled non-event segments in the training data and used these segments to train a binary bird/no bird detection system. Additionally, the nocall probability is used to weight weakly labeled training samples during the main model training. This team explored different combinations of spectrogram window length and threshold tuning to improve scores. The best submission achieved an F1 score of 0.6605 (public 0.7736). <ref type="bibr" coords="8,172.21,471.67,18.03,9.76" target="#b22">[23]</ref>, Team Error_404: This team designed custom convolutional model architectures which used raw audio samples as 1D inputs. In addition, an elaborated scheme of different attention mechanisms was employed. The two-step recognition process consisted of a binary bird/no bird detector and a species classification model. Specaugment and Mixup were used to diversify the training data, and the final submission achieved an F1 score of 0.6179 (public 0.6878) through the combination of 1D and 2D convolutional classifiers. <ref type="bibr" coords="8,230.28,558.94,18.03,9.76" target="#b23">[24]</ref>, Team Arunodhayan: Domain-specific augmentation appeared to be key to improve the overall recognition performance. This team focused on data augmentation, exploring the impact of different methods on the classification accuracy. During local evaluation, this team was able to improve their baseline F1 score of 0.58 to 0.64 by adding background samples comprised of a variety of non-events from different data sources. This team also explored different schemes of weighting ensemble predictions, the best ensemble consisted of 9 models with ResNet and DenseNet backbones, and achieved an F1 score of 0.5890 (public 0.6799).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Das &amp; Aggarwal</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sampathkumar &amp; Kowerko</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Per-Species Analysis</head><p>Because the test-sets and labels were hidden from competitors, their approaches were mostly blind to the test set composition. While this was by design (to encourage the creation of strong general-purpose classifiers, avoiding overfitting to dataset-specific priors), it does inhibit the kind of iterative problem-solving which is usually available in a research context.</p><p>A correct identification requires correctly classifying whether a segment contains a bird, assigning a logit to the presence of each particular species, and then determining from the set of logits which birds are actually present. For the micro-averaged F1 score, the number of "nocall" segments in the soundscapes dominates the number of segments containing any particular species, so any solution's performance on the nocall label has a very large impact on the model's success in the competition.</p><p>Taking the top submissions, per-species F1 scores can be computed for each submission. All species with less than five observations in the test were discarded, to reduce variance; this leaves 92 species. These per-species F1 scores over the top 15 submissions were then aggregated. A histogram of the max and mean per-species F1 scores is given in Figure <ref type="figure" coords="9,419.55,283.68,3.74,10.91" target="#fig_5">4</ref>.</p><p>Had the metric been computed only on segments with birds (removing the no-bird classification problem), the top submissions would have ranked very differently: The eight-place submission (Team KDL<ref type="foot" coords="9,192.81,322.57,3.71,7.97" target="#foot_1">2</ref> ) had the best average F1 over species. We acknowledge, though, that changing the metric would have also changed team's tuning strategies, limiting the usefulness of this contra-positive scenario.</p><p>The per-species F1 scores was highly dependent on a submission's (hidden) choice of thresholds. As a result, it was hard to compare performance of particular models on a given species: A small change of threshold could have a large impact on the F1 score. However, we examined the species for which the top submissions were uniformly poor.</p><p>The Fox Sparrow (Passerella iliaca, foxspa) and Green-Tailed Towhee (Pipilo chlorurus, gnttow) are an interesting pair. Both had very low mean and max F1 scores across all submissions. Their complex songs are well-known to be easily confused by human listeners. The Fox Sparrow has a mean F1 score of 0.01; all but one competitor scored 0. Meanwhile, for gnttow the scores are a bit better, with mean F1 0.17 and max F1 0.48. In addition to the complex song, the gnttow has a diagnostic call which is easily identifiable.</p><p>This indicates that models could improve significantly if they find some way to better distinguish easily-confused species. There are a range of reasons to believe that this is possible. Some easily-confused species can be distinguished by human experts. Secondly, the birds themselves are likely able to distinguish their own species from other species <ref type="bibr" coords="9,394.41,541.11,16.42,10.91" target="#b24">[25]</ref>. Thirdly, birds have superior temporal integration for consecutive tones of different frequency <ref type="bibr" coords="9,429.50,554.66,16.41,10.91" target="#b25">[26]</ref>. This means that there could be fine structure in these songs which a high-resolution ML algorithm may be able to distinguish. Finally, in some cases, hard-to-distinguish species have non-overlapping geographical distribution ranges. Inclusion of species-specific metadata in decision-making can help in these cases.</p><p>The second class of common failure was on birds with very few training resources, especially in the tropical regions, where coverage in Xeno-canto is less comprehensive. The Steely-Vented Hummingbird (Amazilia saucerottei, stvhum2) is a good example: The max (and mean) F1 score  This indicates that improved few-shot learning may help with identification tasks, especially for identification in geo-regions systemically lacking in training data. Improved few-shot learning models can help with species where there are few training examples, but could also help in cases where a species has an extremely variable song structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Per-Location Analysis</head><p>Recording equipment and annotation scheme were identical for all four recording sites. Because of this, variation in recognition performance based on location-specific differences could be explored. Figure <ref type="figure" coords="10,162.88,474.21,4.97,10.91" target="#fig_6">5</ref> shows average scores achieved by the top-15 participants across all recording locations. Despite the uniform recording and label quality of all four datasets, significant differences in achieved scores were observed. When considering all ground truth annotations (incl. nocall), submitted systems performed best on the SSW data. The SSW test data had the highest number of nocall annotations (i.e., the highest number of 5-second segments without bird vocalization) and by far the largest amount of focal training data across all audible bird species. The performance of an automated distinction between bird calls and non-events (i.e., nocall-detector) can be considered one of the primary reason for the drop of 0.182 in F1 score when only segments with a bird call were considered.</p><p>This drop in scores can be observed for all locations. However, call density does not seem to affect recognition performance as strongly as other factors. The SNE test dataset almost entirely consisted of dawn chorus recordings with the highest call density of all four sites (1.19 calls per 5-second segment compared to 0.66 for COL, 0.5 for COR, and 0.52 for SSW). Yet, performance across all segments that contained a bird was still very strong, with almost no drop in precision. It appears that other dataset properties had significantly more impact on the overall recognition performance. Differences in scores were largest for the COR dataset, which had the highest species diversity of all four sites. Additionally, the ground truth for this site contained 6 species with less than 25 training samples each. These 6 species accounted for 15% of all annotations, and we can assume that the combination of species diversity and lack of training data significantly impacts the overall performance. The availability of target domain (soundscape) training data does not seem to help to overcome the lack of focal training data. The COL test data had the least amount of focal training samples across all audible bird species, and the training data did not contain soundscape recordings from this site. However, performance was significantly higher (+0.144 in F1 score) compared to the COR data for which soundscape training data were available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions and Lessons Learned</head><p>The 2021 BirdCLEF competition held on Kaggle featured a vast collection of training and test audio data. Participants were asked to develop robust bird call recognition frameworks to identify avian vocalizations within 5-second segments of soundscape recordings. The Xenocanto community provided the primary training data. The test datasets were collected during passive acoustic surveys in North and South America. Species diversity, variability in call densities, and lack of training data for rare species posed significant challenges. Deep artificial neural networks which used spectrogram as input data were used across the board and provided remarkable results despite the domain gap between training and test data. Post-processing of detections and the use of additional metadata were key to achieve top results. However, the overall impact of metadata (e.g., location and time) was only incremental and significantly lower than expected. It appears that these data may be more useful in scenarios with significantly higher species diversity. Additionally, the competition setup encouraged the use of large model ensembles, which might not have real-world applicability.</p><p>Despite the high vocal activity in some test recordings, segments without audible bird vocalizations dominated the count. Because of this, threshold tuning (especially for 'nocall' segments) had a significant impact, often masking the real performance of the algorithms. As a result, many participants relied on separate 'nocall' detection systems to improve the overall score. Additionally, in this year's edition, off-the-shelve CNN backbones pre-trained on ImageNet provided strong results without the need to investigate the design of domain-specific architectures further. Hence, only very few participants explored alternative approaches like transformers or 1D convolutional networks. We will try to address this in upcoming editions. Providing introductory code repositories and write-ups greatly improved participation and encouraged fast workflow development without the need for domain knowledge. We noticed that this year's participants quickly adapted to the core challenges of the competition and greatly appreciated the code notebooks provided by the organizers. In addition, prize money for highest scoring solutions, gamification elements on Kaggle, and the overall outreach of the platform had a significant impact on participation and helped attract a broader audience.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,210.35,416.94,8.93;4,89.29,222.35,416.70,8.87;4,89.29,234.31,418.36,8.87;4,89.29,246.26,70.40,8.87;4,89.29,84.19,416.69,118.73"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Dawn chorus soundscapes (like this example from the SNE dataset) often have an extremely high call density. The 2021 BirdCLEF dataset contained 100 fully annotated 10-minute soundscapes recorded in North and South America. Expert ornithologists provided bounding box labels for all soundscape recordings.</figDesc><graphic coords="4,89.29,84.19,416.69,118.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,143.77,235.26,94.95,8.97;5,355.63,235.26,96.25,8.97;5,144.69,397.34,93.12,8.97;5,355.95,397.34,95.62,8.97"><head></head><label></label><figDesc>(a) COL recording habitat (b) COR recording habitat (c) SNE recording habitat (d) SSW recording habitat</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,89.29,414.78,416.69,8.93;5,89.29,426.78,395.54,8.87;5,89.29,256.23,204.19,136.18"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Test data recording locations. ARU were used to collect audio data of targeted ecosystems at large spatial scales. Photos: Fernando Cediel, Alejandro Quesada, Connor Wood, Brian Maley.</figDesc><graphic coords="5,89.29,256.23,204.19,136.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,89.29,295.65,418.36,8.93;7,89.29,307.65,416.69,8.87;7,89.29,319.61,416.69,8.87;7,89.29,331.56,291.15,8.87;7,89.29,84.19,416.69,202.04"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Scores achieved by the best systems evaluated within the bird identification task of Life-CLEF 2021. Public and private test data were split randomly, private scores remained hidden until the submission deadline. Participants were able to optimize the recognition performance of their systems based on public scores, which might explain some differences in scores.</figDesc><graphic coords="7,89.29,84.19,416.69,202.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="10,89.29,252.99,416.69,8.93;10,89.29,264.99,78.93,8.87;10,89.29,90.57,204.19,140.04"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Aggregated per-species max (a) and mean (b) F1 scores over the best submissions from the top 15 competitors.</figDesc><graphic coords="10,89.29,90.57,204.19,140.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="11,89.29,245.80,416.70,8.93;11,89.29,257.80,416.94,8.87;11,88.99,269.76,25.72,8.87;11,89.29,84.19,416.70,152.19"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Per-location scores for top-15 participants. High species diversity and lack of focal training data (COR) appeared to have significantly more impact on the overall performance than call density (SNE).</figDesc><graphic coords="11,89.29,84.19,416.70,152.19" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,108.93,671.03,161.52,8.97"><p>https://www.birds.cornell.edu/ccb/swiftone/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="9,108.93,671.04,282.90,8.97"><p>https://www.kaggle.com/hidehisaarai1213/birdclef2021-infer-between-chunk</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Compiling these extensive datasets was a major undertaking, and we are very thankful to the many domain experts who helped to collect and manually annotate the data for this competition. Specifically, we would like to thank (institutions and individual contributors in alphabetic order): <rs type="institution">Center for Avian Population Studies</rs> at the <rs type="institution">Cornell Lab of Ornithology</rs> (<rs type="person">José Castaño</rs>, <rs type="person">Fernando Cediel</rs>, <rs type="person">Jean-Yves Duriaux</rs>, <rs type="person">Viviana Ruiz-Gutiérrez</rs>, <rs type="person">Álvaro Vega-Hidalgo</rs>, <rs type="person">Ingrid Molina</rs>, and <rs type="person">Alejandro Quesada</rs>), <rs type="affiliation">Google Bioacoustics Group</rs> (<rs type="person">Julie Cattiau</rs>), <rs type="person">K. Lisa Yang Center</rs> for <rs type="affiliation">Conservation Bioacoustics at the Cornell Lab of Ornithology</rs> (<rs type="person">Russ Charif</rs>, <rs type="person">Rob Koch</rs>, <rs type="person">Jim Lowe</rs>, <rs type="person">Ashik Rahaman</rs>, <rs type="person">Yu Shiu</rs>, and <rs type="person">Laurel Symes</rs>), <rs type="person">Macaulay Library</rs> at the <rs type="affiliation">Cornell Lab of Ornithology</rs> (<rs type="person">Jessie Barry</rs>, <rs type="person">Sarah Dzielski</rs>, <rs type="person">Cullen Hanks</rs>, <rs type="person">Jay McGowan</rs>, and <rs type="person">Matt Young</rs>), <rs type="affiliation">Nespresso AAA Sustainable Quality Program, Peery Lab at the University of Wisconsin, Madison</rs> (<rs type="person">Phil Chaon</rs>, <rs type="person">Michaela Gustafson</rs>, <rs type="person">M. Zach Peery</rs>, and <rs type="person">Connor Wood</rs>), and the outstanding Xeno-canto community.</p><p>We would also like to thank <rs type="institution">Kaggle</rs> for helping us host this competition and sponsoring the prize money. We are especially grateful for the incredible support and efforts of <rs type="person">Addison Howard</rs> and <rs type="person">Sohier Dane</rs>, who helped process the dataset and set up the competition website. Thanks to everyone who participated in this contest and shared their code base and write-ups with the <rs type="institution">Kaggle</rs> community.</p><p>All results, code notebooks and forum posts are publicly available at: https://www.kaggle.com/c/birdclef-2021</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="13,112.66,111.28,393.33,10.91;13,112.66,124.83,394.52,10.91;13,112.66,138.38,393.33,10.91;13,112.66,151.93,393.32,10.91;13,112.66,165.48,371.86,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="13,247.09,138.38,258.90,10.91;13,112.66,151.93,316.40,10.91">Overview of LifeCLEF 2021: a System-oriented Evaluation of Automated Species Identification and Species Distribution Prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Castañeda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dorso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,451.97,151.93,54.01,10.91;13,112.66,165.48,342.12,10.91">Proceedings of the Twelfth International Conference of the CLEF Association (CLEF 2021)</title>
		<meeting>the Twelfth International Conference of the CLEF Association (CLEF 2021)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,179.03,394.53,10.91;13,112.66,192.57,394.53,10.91;13,112.66,206.12,395.01,10.91;13,112.66,219.67,148.09,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,112.66,192.57,389.54,10.91">Overview of BirdCLEF 2020: Bird sound recognition in complex acoustic environments</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Clapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hopping</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,127.48,206.12,353.66,10.91">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-09">Sep. 2020. 2020</date>
		</imprint>
	</monogr>
	<note>CLEF task overview 2020</note>
</biblStruct>

<biblStruct coords="13,112.66,233.22,393.33,10.91;13,112.66,246.77,275.66,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,301.86,233.22,204.13,10.91;13,112.66,246.77,89.67,10.91">BirdNET: A deep learning solution for avian diversity monitoring</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,211.05,246.77,99.32,10.91">Ecological Informatics</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page">101236</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,260.32,394.52,10.91;13,112.66,273.87,393.32,10.91;13,112.66,287.42,182.20,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,215.56,273.87,290.43,10.91;13,112.66,287.42,29.84,10.91">Deep neural networks for automated detection of marine mammal species</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Roch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fleishman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E.-M</forename><surname>Nosal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Helble</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cholewiak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gillespie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,150.69,287.42,75.46,10.91">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,300.97,310.79,10.91" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="13,268.18,300.97,125.63,10.91">The Sounds of Sustainability</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Ruíz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Román</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-Y</forename><surname>Duriaux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,314.52,394.52,10.91;13,112.66,328.07,393.61,10.91;13,112.66,341.62,232.15,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,112.66,328.07,393.61,10.91;13,112.66,341.62,46.30,10.91">Detecting small changes in populations at landscape scales: A bioacoustic site-occupancy framework</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">D</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Keane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gutiérrez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">C</forename><surname>Sawyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">Z</forename><surname>Peery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,168.03,341.62,92.84,10.91">Ecological Indicators</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="492" to="507" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,355.17,393.33,10.91;13,112.66,368.71,394.53,10.91;13,112.66,382.26,237.92,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,347.76,355.17,158.23,10.91;13,112.66,368.71,389.98,10.91">Survey coverage, recording duration and community composition affect observed species richness in passive acoustic surveys</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Chaon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">Z</forename><surname>Peery</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,112.66,382.26,153.98,10.91">Methods in Ecology and Evolution</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="885" to="896" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,395.81,393.33,10.91;13,112.66,409.36,394.61,10.91;13,112.66,422.91,377.20,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,412.47,395.81,93.51,10.91;13,112.66,409.36,222.39,10.91">Overview of birdclef 2019: Large-scale bird recognition in soundscapes</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F.-R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,358.78,409.36,148.49,10.91;13,112.66,422.91,201.59,10.91">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09">Sep. 2019. 2019</date>
		</imprint>
	</monogr>
	<note>CLEF working notes 2019</note>
</biblStruct>

<biblStruct coords="13,112.66,436.46,394.53,10.91;13,112.66,450.01,352.61,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,178.42,436.46,323.86,10.91">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,127.29,450.01,207.49,10.91">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,463.56,395.17,10.91;13,112.66,477.11,393.33,10.91;13,112.66,490.66,147.08,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,377.68,463.56,130.16,10.91;13,112.66,477.11,67.72,10.91">Densely connected convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,203.84,477.11,302.14,10.91;13,112.66,490.66,49.16,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,504.21,395.17,10.91;13,112.66,517.76,395.01,10.91;13,112.41,531.30,38.81,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,259.74,504.21,203.38,10.91">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,488.38,504.21,19.45,10.91;13,112.66,517.76,347.24,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,544.85,395.17,10.91;13,112.66,558.40,197.93,10.91" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m" coord="13,331.02,544.85,176.81,10.91;13,112.66,558.40,16.17,10.91">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,112.66,571.95,394.61,10.91;13,112.28,585.50,393.71,10.91;13,112.66,599.05,107.17,10.91" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="13,444.21,571.95,63.06,10.91;13,112.28,585.50,318.58,10.91">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08779</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,112.66,612.60,393.33,10.91;13,112.66,626.15,394.53,10.91;13,112.66,639.70,394.90,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,294.30,612.60,211.68,10.91;13,112.66,626.15,254.72,10.91">Birdcall Identification using CNN and Gradient Boosting Decision Trees with Weak and Noisy Supervision</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Murakami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Nishimori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,390.01,626.15,112.72,10.91;13,112.66,639.70,227.04,10.91">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-09">Sep. 2021. 2021</date>
		</imprint>
	</monogr>
	<note>CLEF Working Notes 2021</note>
</biblStruct>

<biblStruct coords="13,112.66,653.25,393.53,10.91;13,112.26,666.80,393.73,10.91;14,112.66,86.97,247.92,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="13,257.89,653.25,248.29,10.91;13,112.26,666.80,78.34,10.91">Recognizing bird species in diverse soundscapes under weak supervision</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Henkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,219.15,666.80,286.83,10.91;14,112.66,86.97,77.06,10.91">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-09">Sep. 2021. 2021</date>
		</imprint>
	</monogr>
	<note>CLEF Working Notes 2021</note>
</biblStruct>

<biblStruct coords="14,112.66,100.52,394.61,10.91;14,112.66,114.06,231.90,10.91" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Stowell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.5275</idno>
		<title level="m" coord="14,234.23,100.52,273.05,10.91;14,112.66,114.06,55.03,10.91">An open dataset for research on audio field recording archives: freefield1010</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,112.66,127.61,393.33,10.91;14,112.66,141.16,394.62,10.91;14,112.66,154.71,394.53,10.91;14,112.66,168.26,116.58,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="14,418.09,127.61,87.89,10.91;14,112.66,141.16,374.03,10.91">Weakly-Supervised Classification and Detection of Bird Sounds in the Wild. A BirdCLEF 2021 Solution</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">V</forename><surname>Conde</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">D</forename><surname>Movva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Agnihotri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bessenyei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shubham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,112.66,154.71,344.36,10.91">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-09">Sep. 2021. 2021</date>
		</imprint>
	</monogr>
	<note>CLEF Working Notes 2021</note>
</biblStruct>

<biblStruct coords="14,112.66,181.81,394.52,10.91;14,112.66,195.36,394.90,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="14,163.88,181.81,201.35,10.91">STFT Transformers for Bird Song Recognition</title>
		<author>
			<persName coords=""><forename type="first">J.-F</forename><surname>Puget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,388.36,181.81,114.32,10.91;14,112.66,195.36,227.04,10.91">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-09">Sep. 2021. 2021</date>
		</imprint>
	</monogr>
	<note>CLEF Working Notes 2021</note>
</biblStruct>

<biblStruct coords="14,112.66,208.91,395.16,10.91;14,112.66,222.46,395.17,10.91;14,112.66,236.01,349.55,10.91" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m" coord="14,330.08,222.46,177.76,10.91;14,112.66,236.01,167.84,10.91">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,112.66,249.56,393.33,10.91;14,112.66,263.11,107.17,10.91" xml:id="b19">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01778</idno>
		<title level="m" coord="14,262.03,249.56,165.90,10.91">Ast: Audio spectrogram transformer</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,112.66,276.66,394.62,10.91;14,112.66,290.20,394.53,10.91;14,112.66,303.75,116.58,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="14,166.97,276.66,318.35,10.91">Learning to monitor birdcalls from weakly-labeled focused recordings</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schlüter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,112.66,290.20,344.36,10.91">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-09">Sep. 2021. 2021</date>
		</imprint>
	</monogr>
	<note>CLEF Working Notes 2021</note>
</biblStruct>

<biblStruct coords="14,112.66,317.30,393.33,10.91;14,112.66,330.85,393.33,10.91;14,112.66,344.40,318.70,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="14,343.77,317.30,43.90,10.91;14,420.50,317.30,85.49,10.91;14,112.66,330.85,177.64,10.91">Building a birdcall segmentation model based on weak labels</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Shugaev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tanahashi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,310.77,330.85,195.22,10.91;14,112.66,344.40,147.84,10.91">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-09">2021. Sep. 2021. 2021</date>
		</imprint>
	</monogr>
	<note>CLEF Working Notes 2021</note>
</biblStruct>

<biblStruct coords="14,112.66,357.95,394.61,10.91;14,112.66,371.50,394.53,10.91;14,112.66,385.05,116.58,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="14,208.31,357.95,279.19,10.91">Bird-Species Audio Identification, Ensembling 1D + 2D Signals</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,112.66,371.50,344.36,10.91">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-09">Sep. 2021. 2021</date>
		</imprint>
	</monogr>
	<note>CLEF Working Notes 2021</note>
</biblStruct>

<biblStruct coords="14,112.66,398.60,395.17,10.91;14,112.66,412.15,394.52,10.91;14,112.66,425.70,395.01,10.91;14,112.66,439.25,142.00,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="14,254.36,398.60,158.92,10.91;14,441.67,398.60,66.15,10.91;14,112.66,412.15,389.79,10.91">Noise augmentation strategies in bird sound classification in combination with DenseNets and ResNets</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sampathkumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kowerko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,127.31,425.70,354.19,10.91">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-09">2021. Sep. 2021. 2021</date>
		</imprint>
	</monogr>
	<note>CLEF Working Notes 2021</note>
</biblStruct>

<biblStruct coords="14,112.66,452.79,393.33,10.91;14,112.66,466.34,98.77,10.91" xml:id="b24">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">K</forename><surname>Catchpole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Slater</surname></persName>
		</author>
		<title level="m" coord="14,246.19,452.79,201.38,10.91">Bird song: biological themes and variations</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,479.89,394.61,10.91;14,112.66,493.44,205.33,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="14,263.58,479.89,125.09,10.91">Hearing in birds and reptiles</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Dooling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Lohr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">L</forename><surname>Dent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,411.05,479.89,96.23,10.91;14,112.66,493.44,75.35,10.91">Comparative hearing: birds and reptiles</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="308" to="359" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
