<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,380.53,15.42;1,89.29,106.66,291.34,15.42">PUC Chile team at Concept Detection: K Nearest Neighbors with Perceptual Similarity</title>
				<funder ref="#_fE6RhVZ">
					<orgName type="full">FONDECYT</orgName>
				</funder>
				<funder ref="#_yRGr2P6">
					<orgName type="full">ANID</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,74.37,11.96"><forename type="first">Gregory</forename><surname>Schuit</surname></persName>
							<email>gkschuit@uc.cl</email>
							<affiliation key="aff0">
								<orgName type="institution">Pontificia Universidad Católica de Chile</orgName>
								<address>
									<addrLine>Av. Vicuña Mackena 4860</addrLine>
									<postCode>7820244</postCode>
									<settlement>Macul</settlement>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,176.31,134.97,72.09,11.96"><forename type="first">Vicente</forename><surname>Castro</surname></persName>
							<email>vvcastro@uc.cl</email>
							<affiliation key="aff0">
								<orgName type="institution">Pontificia Universidad Católica de Chile</orgName>
								<address>
									<addrLine>Av. Vicuña Mackena 4860</addrLine>
									<postCode>7820244</postCode>
									<settlement>Macul</settlement>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,261.04,134.97,52.21,11.96"><forename type="first">Pablo</forename><surname>Pino</surname></persName>
							<email>pdpino@uc.cl</email>
							<affiliation key="aff0">
								<orgName type="institution">Pontificia Universidad Católica de Chile</orgName>
								<address>
									<addrLine>Av. Vicuña Mackena 4860</addrLine>
									<postCode>7820244</postCode>
									<settlement>Macul</settlement>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,325.90,134.97,57.39,11.96"><forename type="first">Denis</forename><surname>Parra</surname></persName>
							<email>dparra@ing.puc.cl</email>
							<affiliation key="aff0">
								<orgName type="institution">Pontificia Universidad Católica de Chile</orgName>
								<address>
									<addrLine>Av. Vicuña Mackena 4860</addrLine>
									<postCode>7820244</postCode>
									<settlement>Macul</settlement>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,414.28,134.97,55.17,11.96"><forename type="first">Hans</forename><surname>Lobel</surname></persName>
							<email>halobel@ing.puc.cl</email>
							<affiliation key="aff0">
								<orgName type="institution">Pontificia Universidad Católica de Chile</orgName>
								<address>
									<addrLine>Av. Vicuña Mackena 4860</addrLine>
									<postCode>7820244</postCode>
									<settlement>Macul</settlement>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,380.53,15.42;1,89.29,106.66,291.34,15.42">PUC Chile team at Concept Detection: K Nearest Neighbors with Perceptual Similarity</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">1FA251EE9E92A11F0A4C7E9266A7BF4B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image Captioning</term>
					<term>Concept Detection</term>
					<term>Medical Artifitial Intelligence</term>
					<term>Deep Learning</term>
					<term>Perceptual Similarity</term>
					<term>Convolutional Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article describes PUC Chile team's participation in the Concept Detection task of ImageCLEFmedical challenge 2021, which resulted in the team earning the fourth place. We made two submissions, the first one based on a naive approach which resulted in a F-1 score of 0.141, and an improved version which leveraged the Perceptual Similarity among images and obtained a final F-1 score of 0.360. We describe in detail our data analysis, our different approaches, and conclude by discussing some ideas for future work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>ImageCLEF <ref type="bibr" coords="1,144.76,368.57,12.99,10.91" target="#b0">[1]</ref> is an initiative with the aim of advancing the field of image retrieval (IR) as well as enhancing the evaluation of technologies for annotation, indexing and retrieval of visual data. The initiative takes the form of several challenges, and it is especially aware of the changes in the IR field in recent years, which have brought about tasks requiring the use of different types of data such as text, images and other features moving towards multi-modality. ImageCLEF has been running annually since 2003, and since the second version (2004) there are medical images involved in some tasks, such as medical image retrieval. Since those versions, the ImageCLEFmedical challenge group of tasks <ref type="bibr" coords="1,302.74,463.42,12.71,10.91" target="#b1">[2]</ref> has integrated new ones involving medical images, with the medical image captioning task taking place since 2017. It consists of two subtasks: concept detection and caption prediction. Although there have been changes in the data used for the newest versions of the challenge, the goal of this task is the same: help physicians reduce the burden of manually translating visual medical information (such as radiology images) into textual descriptions. This year, the concept detection task within the ImageCLEFmedical challenge aims at identifying the presence of relevant biomedical concepts in medical images.</p><p>In this document we describe the participation of our team from the HAIVis group 1 within the artificial intelligence laboratory<ref type="foot" coords="2,249.85,85.21,3.71,7.97" target="#foot_0">2</ref> at PUC Chile (PUC Chile team) in the concept detection task at ImageCLEFmedical 2021 <ref type="bibr" coords="2,235.00,100.52,11.53,10.91" target="#b1">[2]</ref>. Our team reached fourth place in the challenge, and our best submission was a combination of deep learning techniques to visually encode the medical images with a VGG convolutional neural network <ref type="bibr" coords="2,320.66,127.61,11.58,10.91" target="#b2">[3]</ref>, followed by a KNN similarity search using Perceptual Similarity <ref type="bibr" coords="2,212.67,141.16,12.84,10.91" target="#b3">[4]</ref> rather than traditional cosine similarity. The rest of the paper is structured as follows: section 2 briefly describes related work, section 3 our data analysis, and section 4 details of our proposed methods. In section 5 we describe our results, and finally in section 6 we conclude this article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In previous versions of the competition, the best participants have used a wide variety of techniques, mainly based on Convolutional Neural Networks, Natural Language Processing, K-Nearest Neighbors and Clustering <ref type="bibr" coords="2,239.65,267.54,11.28,10.91" target="#b4">[5]</ref>. In the 2020 concept detection challenge, the best F1 score achieved was 0.394 by the AUEB NLP Group. This winning approach consisted of a variation of ChexNet and DenseNet-121 <ref type="bibr" coords="2,231.64,294.63,12.99,10.91" target="#b5">[6]</ref> with a feed-forward neural network as the classification head <ref type="bibr" coords="2,113.27,308.18,11.43,10.91" target="#b6">[7]</ref>. This team also won the second place in the competition. The second best group was PwC_Healthcare, with a F1 score of 0.3924. They submitted three approaches, using CNN and NLP techniques and clustering, achieving 3rd, 4th and 5th place. Looking at previous years, the best submissions achieved an F1 score of 0.1108 in 2018, 0.2823 in 2019 and 0.3940 in 2020 <ref type="bibr" coords="2,489.94,348.83,11.43,10.91" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data set inspection</head><p>The data set provided for this challenge consisted of 2,756 images of varying size for the training set and 500 for the validation set. Since the task is a multi-target classification problem, each sample has one or more labels. Figure <ref type="figure" coords="2,256.51,434.55,4.97,10.91" target="#fig_0">1</ref> shows the distribution of number of labels per image in the training set. Each label corresponds to a UMLS concept <ref type="bibr" coords="3,299.88,86.97,11.59,10.91" target="#b7">[8]</ref>. There are 1,315 different concepts in the training set and 817 in the validation set. Furthermore, only 547 concepts are present in both sets. This is worthy of consideration because it indicates an intrinsic skew in the dataset. We can represent this situation using Figure <ref type="figure" coords="3,271.56,127.61,3.82,10.91" target="#fig_1">2</ref>: In the training set, the most frequent concept is C0040398 (Tomography and Emission-Computed), with 1,159 images. This corresponds to 42% of the images in the training set. The ten most frequent concepts can be seen in Figure <ref type="figure" coords="3,309.84,354.18,3.74,10.91" target="#fig_2">3</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Naive approach</head><p>The data set analysis motivated us to try a naive method as a benchmark that consisted in classifying all inputs as the most frequent concept in the training set, C0040398 (Tomography and Emission-Computed). When submitted, this approach gave us an F1 score of 0.141 over the testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Perceptual Similarity</head><p>We tried using a KNN algorithm over the images to detect concepts. This is, given a test image, we find the K closest training examples according to a specific distance metric, which is detailed further in the article. Then, the concepts assigned to the test image are the concepts that are present among all selected (K) neighbours. Figure <ref type="figure" coords="4,312.96,423.91,5.07,10.91">4</ref> shows an example using K=3 neighbours.</p><p>The metric used to calculate the distance between different images was Learned Perceptual Image Path Similarity (LPIPS) <ref type="bibr" coords="4,230.08,451.01,13.00,10.91" target="#b3">[4]</ref> <ref type="foot" coords="4,243.08,449.26,3.71,7.97" target="#foot_1">3</ref> , a learned metric based on the similarity between deep features from several layers of a neural network, typically a VGG model <ref type="bibr" coords="4,407.58,464.56,11.31,10.91" target="#b2">[3]</ref>. This distance tries to capture the similarity between two images according to human perception. The use of this metric as a good estimator for this dataset arose in the caption prediction task, where a similar approach meant a significant performance gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Multi-Label classification</head><p>Another method used at an early stage of the study was the ResNet-18 <ref type="bibr" coords="4,398.33,554.93,12.68,10.91" target="#b8">[9]</ref> model pre-trained on the ImageNet dataset, from PyTorch<ref type="foot" coords="4,250.28,566.73,3.71,7.97" target="#foot_2">4</ref> . The model was fine-tuned with the ImageCLEFmedical 2021 data set. However, this approach strongly overfitted to the training set. Over the validation set, the model almost always predicted the class to be the most frequent concept in the training set, C0040398 (Tomography and Emission-Computed), so the results achieved on the testing set was 0.141 F1 score, same as the naive approach. This is probably due to the imbalanced nature of the dataset, and that no regularization or drop-out layers were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>Using Perceptual Similarity as the distance metric for the KNN algorithm leads to images representing similar concepts being grouped together in a neighbourhood. Figure <ref type="figure" coords="5,453.53,124.83,5.05,10.91" target="#fig_4">5</ref> shows the performance in F1 score of the KNN approach for different number of neighbours. The best performance of the KNN classifier results when only choosing the closest neighbour.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Runs</head><p>Two submissions were made for the task, the first one using a naïve approach in order to get insights of the results with a baseline, and a second approach based on K-NN using the perceptual similarity implemented in LPIPS, as shown in Table <ref type="table" coords="6,372.20,134.63,3.76,10.91" target="#tab_0">1</ref>. We improve the score more than twice by K-NN employing LPIPS. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This article describes the participation of the PUC Chile team for the concept detection task in ImageCLEFmedical challenge 2021. Our best submission included a VGG deep neural network for visual encoding integrated with a KNN image search using Perceptual Similarity <ref type="bibr" coords="6,480.50,332.17,13.00,10.91" target="#b3">[4]</ref> to select concepts. This metric, as the original article states, provides a robust similarity approach, although more expensive to calculate than traditional approaches such as cosine-based similarity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,619.45,155.73,8.93;2,192.22,470.82,208.35,136.07"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Number of labels per image</figDesc><graphic coords="2,192.22,470.82,208.35,136.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,89.29,297.43,254.36,8.93;3,213.06,150.32,166.68,134.54"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Number of different concepts on each data set split.</figDesc><graphic coords="3,213.06,150.32,166.68,134.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,89.29,550.80,243.53,8.93;3,129.72,376.89,333.36,161.35"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Top 10 most frequent concepts in the training set</figDesc><graphic coords="3,129.72,376.89,333.36,161.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,89.29,210.01,237.49,8.93;4,89.29,84.19,416.69,101.30"><head>Figure 4 : 3 -</head><label>43</label><figDesc>Figure 4: 3-Nearest neighbours for test image with LPIPS</figDesc><graphic coords="4,89.29,84.19,416.69,101.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,89.29,432.75,242.55,8.93;5,171.38,174.64,250.02,245.55"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Change in F1 Score with respect to K neighbours</figDesc><graphic coords="5,171.38,174.64,250.02,245.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="5,89.29,659.43,250.16,8.93;5,89.29,525.76,416.69,109.15"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Sample neighborhood of a test image and F1 Score</figDesc><graphic coords="5,89.29,525.76,416.69,109.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,88.99,177.19,369.78,66.41"><head>Table 1</head><label>1</label><figDesc>Results of the two submission made by the PUC Chile team to the concept detection task.</figDesc><table coords="6,165.64,205.28,264.00,38.32"><row><cell></cell><cell>Method</cell><cell>Score</cell></row><row><cell cols="3">Submission 1 Naive Approach -most popular concept 0.141</cell></row><row><cell>Submission 2</cell><cell>KNN with LPIPS as similarity metric</cell><cell>0.360</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,108.93,670.98,80.33,8.97"><p>http://ialab.ing.puc.cl/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="4,108.93,660.08,359.70,8.97"><p>The implementation is available for PyTorch @ https://github.com/richzhang/PerceptualSimilarity</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="4,108.93,671.03,171.01,8.97"><p>https://pytorch.org/hub/pytorch_vision_resnet</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was partially funded by <rs type="programName">ANID -Millennium Science Initiative Program</rs> -Code <rs type="grantNumber">ICN17_002</rs> and by <rs type="funder">ANID</rs>, <rs type="funder">FONDECYT</rs> grant <rs type="grantNumber">1191791</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_yRGr2P6">
					<idno type="grant-number">ICN17_002</idno>
					<orgName type="program" subtype="full">ANID -Millennium Science Initiative Program</orgName>
				</org>
				<org type="funding" xml:id="_fE6RhVZ">
					<idno type="grant-number">1191791</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,107.59,490.07,400.08,10.91;6,107.59,503.62,399.60,10.91;6,107.41,517.17,400.26,10.91;6,107.59,530.72,399.59,10.91;6,107.20,544.27,398.78,10.91;6,107.59,557.82,400.24,10.91;6,107.59,571.37,398.40,10.91;6,107.59,584.92,399.60,10.91;6,107.59,598.47,116.58,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,261.21,544.27,244.78,10.91;6,107.59,557.82,247.89,10.91">Overview of the ImageCLEF 2021: Multimedia retrieval in medical, nature, internet and social media applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tauteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Stefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Moustahfid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,378.03,557.82,129.79,10.91;6,107.59,571.37,398.40,10.91;6,107.59,584.92,137.45,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 12th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="6,276.45,584.92,183.53,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.59,612.02,399.59,10.91;6,107.59,625.57,398.65,10.91;6,107.06,639.11,394.57,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,107.59,625.57,328.37,10.91">Overview of the ImageCLEFmed 2021 concept &amp; caption prediction task</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,460.88,625.57,45.35,10.91;6,107.06,639.11,65.11,10.91">CLEF2021 Working Notes</title>
		<title level="s" coord="6,179.59,639.11,175.50,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.59,652.66,398.40,10.91;6,107.59,666.21,226.28,10.91" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m" coord="6,245.68,652.66,260.31,10.91;6,107.59,666.21,49.16,10.91">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,107.59,86.97,398.40,10.91;7,107.59,100.52,398.60,10.91;7,107.23,114.06,202.98,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,355.13,86.97,150.86,10.91;7,107.59,100.52,156.06,10.91">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,286.42,100.52,219.77,10.91;7,107.23,114.06,172.53,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,107.59,127.61,398.40,10.91;7,107.59,141.16,399.60,10.91;7,107.59,154.71,292.55,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,374.59,127.61,131.39,10.91;7,107.59,141.16,258.90,10.91">Overview of the imageclefmed 2020 concept detection task: Medical image understanding</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcıa Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="org" />
	</analytic>
	<monogr>
		<title level="m" coord="7,389.86,141.16,112.62,10.91">CLEF2020 Working Notes</title>
		<title level="s" coord="7,107.59,154.71,185.32,10.91">CEUR Workshop Proceedings. CEUR-WS.</title>
		<meeting><address><addrLine>Thessaloniki</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,107.59,168.26,398.40,10.91;7,107.59,181.81,399.60,10.91;7,107.59,195.36,237.02,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,357.69,168.26,148.30,10.91;7,107.59,181.81,38.84,10.91">Densely connected convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.243</idno>
	</analytic>
	<monogr>
		<title level="m" coord="7,192.54,181.81,309.66,10.91">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,107.59,208.91,398.40,10.91;7,107.59,222.46,399.60,10.91;7,107.59,236.01,22.69,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,358.23,208.91,147.75,10.91;7,107.59,222.46,33.20,10.91">AUEB NLP group at imageclefmed caption</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,184.83,222.46,115.25,10.91">CLEF 2020 Working Notes</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-09-22">2020. September 22-25, 2020, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,107.59,249.56,398.40,10.91;7,107.59,263.11,258.51,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,186.00,249.56,319.99,10.91;7,107.59,263.11,51.86,10.91">The unified medical language system (UMLS): integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,168.09,263.11,98.78,10.91">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="267" to="D270" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,107.59,276.66,398.40,10.91;7,107.59,290.20,371.32,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,240.71,276.66,190.97,10.91">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,452.95,276.66,53.04,10.91;7,107.59,290.20,340.87,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
