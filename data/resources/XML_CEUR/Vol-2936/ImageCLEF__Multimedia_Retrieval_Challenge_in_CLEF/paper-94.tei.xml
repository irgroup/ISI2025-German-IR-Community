<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.69,84.74,381.96,15.42;1,89.29,106.66,354.10,15.42">Attention-based CNN-GRU Model For Automatic Medical Images Captioning: ImageCLEF 2021</title>
				<funder ref="#_mv2bCMU">
					<orgName type="full">Academy of Finland Profi5 DigiHealth</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Center for machine vision and signal analysis</orgName>
								<orgName type="institution">University of Oulu</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">MIPT</orgName>
								<orgName type="department" key="dep2">Faculty of Medicine</orgName>
								<orgName type="institution">University of Oulu</orgName>
								<address>
									<settlement>Oulu</settlement>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.69,84.74,381.96,15.42;1,89.29,106.66,354.10,15.42">Attention-based CNN-GRU Model For Automatic Medical Images Captioning: ImageCLEF 2021</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">B4F20825DF048B5BA0532FDDD3ABAF11</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Automatic image captioning</term>
					<term>Medical images</term>
					<term>Concept detection</term>
					<term>Radiology</term>
					<term>Multi-label Classification</term>
					<term>Encoder-decoder</term>
					<term>Attention Mechanism</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The action of understanding and interpretation of medical images is a very important task in the medical diagnosis generation. However, manual description of medical content is a major bottleneck in clinical diagnosis. Many research studies were devoted to develop automated alternatives to this process, which would have enormous impact in terms of efficiency, cost and accuracy in the clinical workflows. Different approaches and techniques have been presented in the literature ranging from traditional machine learning methods to deep learning based models. Inspired by the outperforming results of the later techniques, we present in the current paper, our team participation (RomiBed) to the ImageCLEF medical caption prediction task. We addressed the challenge of medical image captioning by combining a CNN encoder model with an attention-based GRU language generator model whereas a multi-label CNN classifier is used for the concept detection task. Using the provided data in the training, validation and test subsets, we obtain an average F_measure of 14.3% and a BLEU score of 0.243 on the ImageCLEF concept detection and the caption prediction challenges, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the increasing number of medical images generated worldwide from different modalities in hospitals and health centers, the need to analyse and discover their content is crucial. Indeed, medical images offer a safe environment to explore patient's health state without the need for a surgery or any other invasive procedures <ref type="bibr" coords="1,326.89,487.06,11.57,10.91" target="#b0">[1]</ref>. Besides, this also helps clinicians in their daily routine by expediting clinical workflows and trigger automated alerts associated to potentially dangerous diseases. Recently, many research was devoted to the process of automatically generating clinically sound interpretations of medical images. Roughly speaking, generating clinically explainable and understandable analysis for medical images may enrich medical knowledge systems and facilitate the human-machine interactive diagnosis practice <ref type="bibr" coords="1,492.63,554.80,11.28,10.91" target="#b1">[2]</ref>. Therefore, automatic medical image captioning is one of the main focus of the interdisciplinary research in medical imaging field <ref type="bibr" coords="1,237.74,581.90,11.33,10.91" target="#b1">[2]</ref>. Especially, medical image captioning uses visual features of images to generate a concise textual description of the content of the medical image by highlighting the clinically important observations. It represents a convergence of computer vision and natural language processing (NLP) with an emphasis on medical image processing <ref type="bibr" coords="2,89.29,114.06,11.29,10.91" target="#b2">[3]</ref>. In this regard, the ImageCLEFmedical task <ref type="bibr" coords="2,296.07,114.06,11.24,10.91" target="#b3">[4,</ref><ref type="bibr" coords="2,310.05,114.06,8.89,10.91" target="#b4">5]</ref> is organized each year as part of the CLEF initiative labs aiming at developing machine learning methods for medical image understanding and description. It includes two sub-tasks: the concept detection sub-task which aims to identify the UMLS Concept Unique Identifiers (CUIs) for a given medical image. Whereas the caption prediction sub-task aims to generate coherent caption based on the clinical concept vocabulary created in the first sub-task and the visual content of the image.</p><p>Motivated by the recent advances made in deep neural networks in different tasks of computer vision and NLP, especially due to their promising results in the machine language translation models, we present in this paper our contribution to the ImageCLEF 2021 medical task under the team name 'RomiBed'. We proposed a multi-label classification CNN model for the first sub-task after applying an augmentation technique based on the center cropping of the medical images. Features were extracted using a pre-trained model, while the classification is performed using a CNN network. For the second sub-task, we proposed an encoder-decoder model with an attention layer where the encoder is based on a CNN feature extractor and the decoder is composed of a GRU network with an attention mechanism.</p><p>This paper is organized as follows. First, we briefly review the related medical image captioning studies from the literature in Section. 2. In Section. 3, we provide a brief description of the ImageCLEF dataset used in this study. Next, we detail the methodology we followed to construct the concept detection model as well as the caption prediction model in Section. 4. We discuss each step of the process and deliver the results in terms of F_measure for the concept detection and BLEU for the caption prediction. Finally, we finish with a conclusion where we highlight some key insights and future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Automatic image captioning (AIC) in the medical field has gained a particular attention from researchers due to its importance and its huge impact on health care centers by allowing instantaneous understanding of medical images for doctors as well as patients. In addition, the significant progress made to date in artificial intelligence due to deep learning models contributed greatly to the AIC task <ref type="bibr" coords="2,244.30,511.42,11.28,10.91" target="#b1">[2]</ref>. Therefore, different techniques ranging from traditional template-based and/or retrieval-based systems to generative models based on deep-neural networks passing through various hybrid models that combine different techniques <ref type="bibr" coords="2,452.70,538.52,12.69,10.91" target="#b5">[6]</ref> emerged. During the last years, many systems have been proposed to compete for the ImageCLEF medical challenge. For the first step towards medical image captioning, which consists of concept detection in ImageCLEF medical task, the multi-label classifications is found to play a leading role. For instance, <ref type="bibr" coords="2,222.94,592.72,11.48,10.91" target="#b1">[2,</ref><ref type="bibr" coords="2,237.99,592.72,9.03,10.91" target="#b6">7]</ref> exploited the transfer learning to perform a multi-label classification by extracting significant features from medical images using pre-trained models such as the Resnet50, InceptionV3 . . . . In addition, Wang et al. <ref type="bibr" coords="2,372.67,619.81,13.00,10.91" target="#b1">[2]</ref> explored a retrieval-based topic modelling method to extract the most relevant clinical concepts from images similar to the input image. Encoder-decoder (CNN-RNN) architectures were explored by many studies to generate appropriate captions. In many cases, attention-mechanism is added to the baseline encoder-decoder model as in <ref type="bibr" coords="3,223.37,370.93,12.99,10.91" target="#b7">[8]</ref> who contributed to the ImageCLEF 2017 edition. Similarly, Hasan et al. <ref type="bibr" coords="3,144.54,384.48,12.90,10.91" target="#b8">[9]</ref> enriched the soft attention-based encoder-decoder model by inputting, to the decoder, the output of the classification model on image modalities. This allowed them to supplement the decoder with more fine grained details on the data to make the generation process more focused. Indeed, supplementary information such as image modalities or body parts could enhance the classification model as reported in Lyndon et al. <ref type="bibr" coords="3,422.92,438.67,16.41,10.91" target="#b9">[10]</ref>. Furthermore, the RNN decoder <ref type="bibr" coords="3,168.44,452.22,17.91,10.91" target="#b10">[11]</ref> is replaced by different variants such as LSTM <ref type="bibr" coords="3,397.00,452.22,17.91,10.91" target="#b11">[12]</ref> Xu et al. <ref type="bibr" coords="3,455.33,452.22,10.92,10.91" target="#b6">[7]</ref>, or GRU Ambati and Dudyala <ref type="bibr" coords="3,181.68,465.77,17.75,10.91" target="#b12">[13]</ref> who used the captioning module to resolve the task of visual question answering. Likewise, Benzarti et al. <ref type="bibr" coords="3,252.03,479.32,18.04,10.91" target="#b13">[14]</ref> employed the captioning model to medical retrieval systems in order to obtain the query terms. From another perspective, Rahman <ref type="bibr" coords="3,434.25,492.87,17.75,10.91" target="#b14">[15]</ref> proposed to extract textual and visual information using RNN-based and CNN-based networks respectively, and then merge the outputs of both models to generate relevant captions. Similarly, Mishra and Banerjee <ref type="bibr" coords="3,149.19,533.52,17.77,10.91" target="#b15">[16]</ref> adopted the same technique aiming to detect retinal diseases and to generate appropriate medical reports. In other techniques, generative models are combined with retrieval systems for AIC. For example, Kougia et al. <ref type="bibr" coords="3,279.13,560.62,17.75,10.91" target="#b16">[17]</ref> proposed to exploit the image visual features to retrieve similar images with their known concepts and then combine them to predict enhanced captions of the input image. The prediction is performed using an encoder-decoder generative model. Likewise, <ref type="bibr" coords="3,164.68,601.26,16.30,10.91" target="#b17">[18,</ref><ref type="bibr" coords="3,183.19,601.26,13.95,10.91" target="#b18">19]</ref> suggested to use a retrieval policy module that makes a choice between generating new sentence or retrieving a template sentence. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data analysis</head><p>The data used for this year edition is shared between both the ImageCLEFmed Caption and the ImageCLEF-VQAMed tasks. The dataset include three sets: the training set composed of the VQA-Med 2020 training data with 2756 medical images; the validation set consisting of 500 radiology images and the test set consisting of 444 radiology images. In addition, for the concept detection task, an excel file containing the medical image ID and the corresponding concepts CUIs is given to map each medical image onto its related concepts. Similarly, an excel file containing the captions of each medical image is provided for the caption prediction task. We present in Fig. <ref type="figure" coords="4,175.00,527.53,5.17,10.91" target="#fig_0">1</ref> sample images with their underlying concepts and in Fig. <ref type="figure" coords="4,449.30,527.53,5.17,10.91" target="#fig_1">2</ref> samples of images with their captions.</p><p>On further analysis of the datsaet, we illustrate in Fig. <ref type="figure" coords="4,353.96,554.63,5.17,10.91" target="#fig_2">3</ref> the number of images per each CUI concept. It is obvious that the most frequent CUI is the 'C0040398' corresponding to 'Tomography, Emission-Computed' with 1159 images. Moreover, Fig. <ref type="figure" coords="4,395.91,581.73,5.02,10.91" target="#fig_3">4</ref> presents the number of images by the number of CUIs associated to each image. We can see from this image that most of the medical images are attached to 2 to 3 concepts whereas the maximum number of concept CUIs per image is 10.</p><p>Moreover, we noticed that the maximum number of sentences per image caption is 5 and the maximum length of any caption is 47 words before pre-processing whereas it is 33 words after the pre-processing.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head><p>For this year edition of the ImageCLEFmed Caption challenge <ref type="bibr" coords="5,360.17,514.27,11.27,10.91" target="#b4">[5]</ref>, two subtasks are put forward: the concept detection and the caption prediction. To resolve each of these challenges, we present in the current paper a model for the concept detection based on a multi-label classification using a CNN architecture <ref type="bibr" coords="5,209.74,554.91,18.07,10.91" target="#b19">[20]</ref> and an encoder-decoder-based model for caption prediction. First, data is pre-processed to transform the text into understandable units. Images are as well pre-processed by performing a data augmentation technique on the training set. The detail is provided in the next subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data Pre-processing</head><p>Text Pre-processing: We apply a pre-processing scheme on the image to concept matching file to organize the concepts of each medical image into a list and create data-frames from image IDs and their underlying CUIs. In addition, we pre-process the captions using the NLTK package, by performing tokenization, punctuation and stop-words removal (using the default NLTK's "english" stopword list), lower casing each token and finally applying the stemming to obtain filtered sentences for each medical image using the NLTK's Snowball stemmer. Morover, for the RNN decoder, we add two tokens: '&lt;start&gt;' and '&lt;end&gt;' to identify the beginning and the end of each caption.</p><p>Image Pre-processing: Image data generators are created for the three sets of data to preprocess the images before feature extraction. These generators iterate over the data subsets and normalize the images to facilitate the features calculation. Then we apply horizontal and vertical flip in addition to a crop-center based data augmentation technique that we implement with a fraction of 87.5%. The implemented data augmentation approach allowed us to expand the training data without altering the visual content of the image. Then, the images are resized to fit in the feature extractor size which is 224 Ã— 224 Ã— 3 in our case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Concept Detection</head><p>As we mentioned before, the first task of concept detection aims at identifying and localizing the relevant concepts present in each medical image. Therefore, we exploit the visual image content to extract significant visual features that allow us to distinguish the underlying concepts. These concepts are used further to construct image captions and could as well be utilized for the context-based images and information retrieval purposes.</p><p>To achieve the first step towards caption prediction, we performed a multi label classification. In the first run, we extracted image features using the pre-trained MobileNet-V2 and then performed the classification using a GRU network <ref type="bibr" coords="6,308.32,409.34,16.09,10.91" target="#b20">[21]</ref>. In the second run, we performed feature extraction using the pre-trained Inception-V3 model followed by a classification using a CNN network. ImageNet weights were used for both models and features were extracted from the last convolutional layer. As illustrated by Fig. <ref type="figure" coords="6,191.93,625.00,3.66,10.91" target="#fig_4">5</ref>, the features extracted from the radiology images using the pre-trained MobileNet-V2 model are passed through a GRU layer, a flatten layer and then a fully connected layer with a Relu activation function and dropout. Finally, the labels of each medical image are predicted using a fully connected layer with a Sigmoid activation function. Likewise, the features extracted from the radiology images using the pre-trained Inception-V3 model are passed through a flatten layer, a fully connected layer with batch normalization, Relu activation function, and dropout. Then, the probability of each class is calculated using a fully connected layer with a Sigmoid activation function (as shown by Fig. <ref type="figure" coords="7,405.38,285.15,3.65,10.91" target="#fig_5">6</ref>). If the probability is greater than 20%, we assert the input image belongs to that class. This probability was fixed to 20% after experimenting with different thresholds. If this threshold is fixed to a higher value, we would get a lot of false negatives where many images are not classified in their correct classes. However, if it is fixed to a smaller value, we would get a lot of false positives where many images are categorized into incorrect classes. Finally, we map the labels to their corresponding concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Caption Prediction</head><p>The second sub-task relies on the concept vocabulary detected in the first sub-task in addition to the visual features extracted from the medical images to establish relationships between them and predict descriptive caption for each medical image. We attempted to address the issue of caption generation using an encoder-decoder architecture with attention mechanism.</p><p>The visual features are extracted from the medical images using a pre-trained model 'Incep-tionV3' where weights from the ImageNet were employed. Then, these features are passed through a CNN model that is composed of a fully connected layer to flatten the feature vector. Next, an attention mechanism is employed to focus on the most important parts of the image and a context vector is constructed. Captions are pre-processed as we mentioned before and passed to an embedding layer. A concatenation layer is farther used to merge the context vector with the resulting embedding vector and the output is passed to a GRU layer. A flatten layer and two fully connected layers with dropout and a Relu for the first one and a Sigmoid activation function for the second were employed. Finally, relevant captions are generated word by word until the '&lt;end&gt;' token is met. Figure . 7 illustrates the attention-based encoder-decoder architecture we used to construct new captions for the medical images. Moreover, we exploit the teacher forcing during the training by using the ground truth sequences at every step rather than the sequence of newly generated words at previous steps. The attention-based encoder decoder architecture used for caption prediction. Features are extracted from the medical images and passed to an attention mechanism to select the most important parts of the image. Then, the constructed context vector is concatenated with the embedding vector obtained from the captions and inputted to a GRU layer and captions are finally generated word by word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment and Results</head><p>We used the data provided by the ImageCLEF medical task to evaluate the performance of our models. Three subsets are used for the training, the validation and the test respectively. We report in this section, the performance metrics calculated as well as the results we obtained for both models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Performance metrics</head><p>We calculate the F_Measure, using the default 'binary' averaging method, for the concept detection task as follows:</p><formula xml:id="formula_0" coords="9,207.34,173.09,298.65,24.43">ğ¹ _ğ‘€ ğ‘’ğ‘ğ‘ ğ‘¢ğ‘Ÿğ‘’ = 2 â€¢ ğ‘ƒ ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› â€¢ ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ ğ‘ƒ ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› + ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™<label>(1)</label></formula><p>Where the recall and the precision are calculated as follow and TP, FN, TN, FP correspond to true positive, false negative, true negative and false positive respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ =</head><p>ğ‘‡ ğ‘ƒ ğ‘‡ ğ‘ƒ + ğ¹ ğ‘</p><p>(2)</p><formula xml:id="formula_1" coords="9,241.77,275.39,264.22,24.43">ğ‘ƒ ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› = ğ‘‡ ğ‘ƒ ğ‘‡ ğ‘ƒ + ğ¹ ğ‘ƒ<label>(3)</label></formula><p>For the caption prediction task, we calculate the BLEU score by assuming that each caption is a single sentence even if it is actually composed of several sentences. For that we use the default implementation of the Python NLTK based on <ref type="bibr" coords="9,331.31,332.65,16.39,10.91" target="#b21">[22]</ref>:</p><formula xml:id="formula_2" coords="9,215.33,356.47,290.66,33.58">ğµğ¿ğ¸ğ‘ˆ = ğµğ‘ƒ â€¢ exp( ğ‘ âˆ‘ï¸ ğ‘›=1 ğ‘¤ ğ‘› â€¢ log ğ‘ ğ‘› )<label>(4)</label></formula><p>Where BP refers to the brevity penalty, N refers to the number of n_grams (uni-gram, bi-gram, 3-gram and 4-gram), ğ‘Š ğ‘› refers to the weight of each modified precision and ğ‘ƒ ğ‘› refers to the modified precision. By default N=4 and ğ‘Š ğ‘› = 1/N = 1/4.</p><p>Brevity Penalty (BP) allows us to pick the candidate caption which is most likely close in length, word choice and word order to the reference caption. It is an exponential decay and is calculated as follows:</p><formula xml:id="formula_3" coords="9,235.74,487.01,270.25,30.47">ğµğ‘ƒ = {ï¸ƒ 1 ğ‘ &gt; ğ‘Ÿ exp (1-ğ‘Ÿ/ğ‘) ğ‘ â©½ ğ‘Ÿ<label>(5)</label></formula><p>Where r refers to the count of words in the reference caption and c refers to the count of words in the candidate caption.</p><p>Modified precision is computed for each n_gram as the sum of clipped n_gram counts of the candidate sentences in the corpus divided by the number of candidate n_grams as shows "(6)" <ref type="bibr" coords="9,89.29,580.87,16.31,10.91" target="#b21">[22]</ref>. It allows us to compute the adequacy and the fluency of the candidate translation to the reference translation.</p><formula xml:id="formula_4" coords="9,183.49,617.80,322.49,57.53">ğ‘ ğ‘› = âˆ‘ï¸ ğ¶âˆˆ{ğ¶ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘ } âˆ‘ï¸ ğ‘›_ğ‘”ğ‘Ÿğ‘ğ‘šâˆˆğ¶ ğ¶ğ‘œğ‘¢ğ‘›ğ‘¡ ğ‘ğ‘™ğ‘–ğ‘ (ğ‘›_ğ‘”ğ‘Ÿğ‘ğ‘š) âˆ‘ï¸ ğ¶ â€² âˆˆ{ğ¶ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘ } âˆ‘ï¸ ğ‘›_ğ‘”ğ‘Ÿğ‘ğ‘š â€² âˆˆğ¶ â€² ğ¶ğ‘œğ‘¢ğ‘›ğ‘¡(ğ‘›_ğ‘”ğ‘Ÿğ‘ğ‘š â€² ) (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results</head><p>We obtained an average F_measure of 24.49% during the training process and a value of 14.3% during the inference process for the concept detection task using the MobileNetV2 as a feature extractor and the GRU network as a classifier. Similarly, we obtained an average F_measure of 23.28% during the training process and a value of 13.7% during the inference process using the InceptionV3 model as a feature extractor and a CNN network as a classifier. However, our F_measure results are comparatively lower compared to the leading group (IALab_PUC) with a score of 50.5 %. Results are illustrated by Table . 1 where the run ID 136011 corresponds to the first configuration and the 136025 corresponds to the second configuration.     Finally, we show an example of a random medical image from the validation set with its real caption and the newly generated caption in Fig. <ref type="figure" coords="11,301.75,618.56,8.25,10.91" target="#fig_0">10</ref>. We observed a BLEU score of 0.339 for this image, where 8 words were correctly generated but the order of the words in the generated caption is different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>We presented in this paper our contribution to the ImageCLEF 2021 medical task where we proposed a CNN based multi-label classification model for the concept detection task and an attention-based encoder-decoder model for the caption prediction task. For both models, a transfer learning is used to extract significant features from the real radiology images and a data augmentation based on center-cropping is applied to expand the used training subset. The evaluation of the caption detection task is conducted using the mean F_measure for which we obtained a score of 14.3%. Furthermore, BLEU score is used to evaluate the reliability of the generated captions for the caption prediction task for which we obtained a score of 0.243. We believe that we did not obtain promising results due to the small amount of data used and the fact that we did not explore more fine-tuned parameters for both models for time constraints. In addition, we did not include the textual features substituted by the medical concepts to generate the new captions. In future work, we will integrate the textual features of the images to the visual information to obtain more relevant captions. We will also investigate more advanced deep learning algorithms inline with more fine-tuned parameters. In addition, we will investigate our model performance on larger scale dataset for medical image captioning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,321.37,416.87,8.93;3,89.29,333.38,155.56,8.87;3,89.29,84.19,416.70,218.64"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Radiology image samples from the ImageCLEF dataset where CUIs of each image and their respective UMLS terms are presented.</figDesc><graphic coords="3,89.29,84.19,416.70,218.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.29,358.82,416.69,8.93;4,89.29,370.82,51.48,8.87;4,89.29,84.19,416.72,256.09"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Radiology image samples from the ImageCLEF dataset where caption describing each image is presented.</figDesc><graphic coords="4,89.29,84.19,416.72,256.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,89.29,254.84,289.29,8.93;5,89.29,84.19,416.69,146.13"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Number of medical images attached to each clinical concept</figDesc><graphic coords="5,89.29,84.19,416.69,146.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,89.29,452.35,366.28,8.93;5,89.29,281.70,416.69,146.13"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Distribution of the number of clinical concepts attached to each medical image</figDesc><graphic coords="5,89.29,281.70,416.69,146.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,89.29,583.34,416.69,8.93;6,89.29,595.34,119.41,8.87;6,89.29,472.64,416.69,92.16"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: GRU based multi-label classification of features extracted from the MobilNetV2 pre-trained model into medical concepts.</figDesc><graphic coords="6,89.29,472.64,416.69,92.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="7,89.29,194.94,416.69,8.93;7,89.29,206.95,119.41,8.87;7,89.29,84.19,416.71,92.21"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: CNN based multi-label classification of features extracted from the InceptionV3 pre-trained model into medical concepts.</figDesc><graphic coords="7,89.29,84.19,416.71,92.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="8,89.29,522.83,416.69,8.93;8,89.29,534.84,416.70,8.87;8,89.29,546.79,416.88,8.87;8,89.29,558.75,416.94,8.87;8,88.93,570.70,23.02,8.87;8,89.29,84.19,416.71,419.26"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7:The attention-based encoder decoder architecture used for caption prediction. Features are extracted from the medical images and passed to an attention mechanism to select the most important parts of the image. Then, the constructed context vector is concatenated with the embedding vector obtained from the captions and inputted to a GRU layer and captions are finally generated word by word.</figDesc><graphic coords="8,89.29,84.19,416.71,419.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="10,205.41,320.01,300.57,10.91;10,89.29,333.56,122.84,10.91"><head></head><label></label><figDesc>Figure. 8 shows the evolution of the multi-label classification model accuracy across the epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="10,89.29,514.37,314.54,8.93;10,192.22,356.27,208.35,145.53"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Evolution of the accuracy per epoch for the concept detection task</figDesc><graphic coords="10,192.22,356.27,208.35,145.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="10,406.39,557.56,99.59,10.91;10,89.29,571.11,416.69,10.91;10,89.29,584.66,250.53,10.91"><head>2 .</head><label>2</label><figDesc>In addition, Figure. 9 shows the loss calculated during the training process of the encoder-decoder model. We noticed the decrease of the cross-entropy loss across the epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="11,89.29,244.51,330.64,8.93;11,192.22,84.19,208.35,147.76"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Figure 9: Evolution of the training loss per epoch for the caption prediction task</figDesc><graphic coords="11,192.22,84.19,208.35,147.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="10,88.99,161.19,308.10,69.87"><head>Table 1</head><label>1</label><figDesc>F_measure results for the concept detection task</figDesc><table coords="10,198.19,192.81,198.90,38.25"><row><cell cols="4">TeamName Run ID Validation set Test set</cell></row><row><cell>RomiBed</cell><cell>136025 136011</cell><cell>23.65% 24.49%</cell><cell>13.7% 14.3%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,88.99,613.67,308.10,57.92"><head>Table 2</head><label>2</label><figDesc>BLEU score results for the caption prediction task</figDesc><table coords="10,198.19,645.29,198.90,26.30"><row><cell cols="4">TeamName Run ID Validation set Test set</cell></row><row><cell>RomiBed</cell><cell>135896</cell><cell>0.287</cell><cell>0.243</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work is supported by the <rs type="funder">Academy of Finland Profi5 DigiHealth</rs> project (#<rs type="grantNumber">326291</rs>), which is gratefully acknowledged.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_mv2bCMU">
					<idno type="grant-number">326291</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,112.66,431.77,393.32,10.91;12,112.66,445.32,393.60,10.91;12,112.28,458.87,121.88,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,366.16,431.77,139.82,10.91;12,112.66,445.32,84.19,10.91">Automatic Caption Generation for Medical Images</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Allaouzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ben Ahmed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Benamrou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ouardouz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,220.96,445.32,285.30,10.91;12,112.28,458.87,54.34,10.91">Proceedings of the 3rd International Conference on Smart City Applications</title>
		<meeting>the 3rd International Conference on Smart City Applications</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,472.42,393.33,10.91;12,112.66,485.97,393.33,10.91;12,112.66,499.52,269.16,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,262.12,472.42,243.86,10.91;12,112.66,485.97,106.32,10.91">Medical Image Labelling and Semantic Understanding for Clinical Applications</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,241.98,485.97,264.00,10.91;12,112.66,499.52,138.12,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,513.06,393.33,10.91;12,112.66,526.61,393.33,10.91;12,112.66,540.16,393.33,10.91;12,112.66,553.71,169.08,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,289.29,513.06,216.69,10.91;12,112.66,526.61,313.57,10.91">Feature Difference Makes Sense: A Medical Image Captioning Model Exploiting Feature Difference and Tag Information</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,450.78,526.61,55.20,10.91;12,112.66,540.16,393.33,10.91;12,112.66,553.71,85.49,10.91">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="95" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,567.26,394.53,10.91;12,112.66,580.81,394.53,10.91;12,112.66,594.36,394.53,10.91;12,112.66,607.91,67.18,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,164.72,580.81,337.73,10.91">Overview of the ImageCLEFmed 2021 Concept &amp; Caption Prediction Task</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>GarcÃ­a Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,127.66,594.36,114.61,10.91">CLEF2021 Working Notes</title>
		<title level="s" coord="12,249.86,594.36,179.10,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,621.46,395.01,10.91;12,112.66,635.01,394.53,10.91;12,112.48,648.56,395.18,10.91;12,112.66,662.11,394.53,10.91;13,112.28,86.97,393.70,10.91;13,112.66,100.52,393.33,10.91;13,112.66,114.06,393.33,10.91;13,112.66,127.61,393.53,10.91;13,112.66,141.16,197.61,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,264.32,86.97,241.66,10.91;13,112.66,100.52,261.75,10.91">Overview of the ImageCLEF 2021: Multimedia retrieval in medical, nature, internet and social media applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Peteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tauteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Åtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Moustahfid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,401.27,100.52,104.72,10.91;13,112.66,114.06,393.33,10.91;13,112.66,127.61,201.15,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 12th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="13,348.61,127.61,157.57,10.91;13,112.66,141.16,31.10,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,154.71,395.16,10.91;13,112.66,168.26,393.33,10.91;13,112.66,181.81,344.45,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,486.27,154.71,21.55,10.91;13,112.66,168.26,179.04,10.91">Captioning Ultrasound Images Automatically</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Alsharid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Drukker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Chatelain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">T</forename><surname>Papageorghiou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,314.74,168.26,191.25,10.91;13,112.66,181.81,213.94,10.91">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="338" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,195.36,393.33,10.91;13,112.66,208.91,394.52,10.91;13,112.66,222.46,143.25,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,376.53,195.36,129.46,10.91;13,112.66,208.91,386.02,10.91">Concept Detection based on Multi-label Classification and Image Captioning Approach-DAMO at ImageCLEF 2019</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,127.29,222.46,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,236.01,394.53,10.91;13,112.28,249.56,393.71,10.91;13,112.66,263.11,223.17,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,238.30,249.56,267.68,10.91;13,112.66,263.11,67.46,10.91">PRNA at ImageCLEF 2017 Caption Prediction and Concept Detection Tasks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sreenivasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">R</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Swisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,207.21,263.11,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,276.66,394.53,10.91;13,112.66,290.20,393.61,10.91;13,112.66,303.75,395.17,10.91;13,112.66,317.30,365.79,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,199.39,290.20,306.88,10.91;13,112.66,303.75,197.60,10.91">Attention-based Medical Caption Generation with Image Modality Classification and Clinical Concept Mapping</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sreenivasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">R</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Swisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,334.26,303.75,173.58,10.91;13,112.66,317.30,234.75,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="224" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,330.85,393.33,10.91;13,112.66,344.40,201.01,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,245.61,330.85,260.38,10.91;13,112.66,344.40,44.86,10.91">Neural Captioning for the ImageCLEF 2017 Medical Image Challenges</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lyndon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,185.05,344.40,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,357.95,393.33,10.91;13,112.66,371.50,151.04,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,305.67,357.95,200.32,10.91;13,112.66,371.50,24.96,10.91">Learning representations by back-propagating errors</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,145.70,371.50,28.99,10.91">nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,385.05,393.98,10.91;13,112.41,398.60,48.96,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,252.72,385.05,113.97,10.91">Long Short-term Memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,377.08,385.05,91.31,10.91">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,412.15,393.58,10.91;13,112.66,425.70,393.33,10.91;13,112.66,439.25,257.18,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,239.31,412.15,266.93,10.91;13,112.66,425.70,228.16,10.91">A Sequence-to-Sequence Model Approach for ImageCLEF 2018 Medical Domain Visual Question Answering</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ambati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Dudyala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,370.37,425.70,135.62,10.91;13,112.66,439.25,162.28,10.91">2018 15th IEEE India Council International Conference (INDICON)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,452.79,393.33,10.91;13,112.66,466.34,393.33,10.91;13,112.66,479.89,204.07,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,307.24,452.79,198.74,10.91;13,112.66,466.34,123.61,10.91">Cross-Model Retrieval Via Automatic Medical Image Diagnosis Generation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Benzarti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B A</forename><surname>Karaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">H B</forename><surname>Ghezala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,259.58,466.34,246.41,10.91;13,112.66,479.89,73.48,10.91">International Conference on Intelligent Systems Design and Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="561" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,493.44,393.32,10.91;13,112.66,506.99,326.46,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="13,170.64,493.44,335.34,10.91;13,112.66,506.99,171.65,10.91">A Cross Modal Deep Learning Based Approach for Caption Prediction and Concept Detection by CS Morgan State</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,310.51,506.99,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,520.54,393.33,10.91;13,112.66,534.09,395.01,10.91;13,112.41,547.64,23.60,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,214.21,520.54,291.78,10.91;13,112.66,534.09,79.49,10.91">Automatic Caption Generation of Retinal Diseases with Self-trained RNN Merge Model</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,215.47,534.09,204.88,10.91">Advanced Computing and Systems for Security</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,561.19,393.33,10.91;13,112.66,574.74,210.50,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="13,334.00,561.19,171.98,10.91;13,112.66,574.74,35.51,10.91">AUEB NLP Group at ImageCLEFmed Caption</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,194.54,574.74,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,588.29,393.53,10.91;13,112.66,601.84,393.33,10.91;13,112.66,615.39,394.53,10.91;13,112.66,628.93,85.06,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="13,277.20,588.29,228.99,10.91;13,112.66,601.84,148.26,10.91">Hybrid Retrieval-Generation Reinforced Agent for Medical Image Report Generation</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,284.25,601.84,221.74,10.91;13,112.66,615.39,283.90,10.91">Proceedings of the 32nd International Conference on Neural Information Processing Systems (NIPS 2018), NIPS&apos;18</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems (NIPS 2018), NIPS&apos;18</meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1537" to="1547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,642.48,393.32,10.91;13,112.66,656.03,397.48,10.91;13,112.36,672.02,132.96,7.90" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="13,259.01,642.48,246.98,10.91;13,112.66,656.03,169.69,10.91">Unifying Relational Sentence Generation and Retrieval for Medical Image Report Composition</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCYB.2020.3026098</idno>
	</analytic>
	<monogr>
		<title level="j" coord="13,291.20,656.03,150.33,10.91">IEEE Transactions on Cybernetics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,86.97,394.52,10.91;14,112.33,100.52,307.13,10.91" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="14,234.74,86.97,272.44,10.91;14,112.33,100.52,229.18,10.91">Convolutional networks for images, speech, and time series, The handbook of brain theory and neural networks</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995. 1995</date>
			<biblScope unit="page">3361</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,114.06,395.16,10.91;14,112.66,127.61,393.32,10.91;14,112.66,141.16,223.38,10.91" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="14,132.79,127.61,373.19,10.91;14,112.66,141.16,46.51,10.91">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Van MerriÃ«nboer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,112.66,154.71,393.32,10.91;14,112.66,168.26,393.53,10.91;14,112.66,181.81,203.57,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="14,308.66,154.71,197.32,10.91;14,112.66,168.26,91.33,10.91">Bleu: a Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,227.24,168.26,278.95,10.91;14,112.66,181.81,116.04,10.91">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
