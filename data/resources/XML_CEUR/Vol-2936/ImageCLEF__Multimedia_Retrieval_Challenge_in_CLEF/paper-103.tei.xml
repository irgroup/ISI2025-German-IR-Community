<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,402.19,15.42;1,89.29,106.66,375.79,15.42;1,89.29,128.58,351.62,15.43;1,89.29,150.49,236.37,15.43">NLIP-Essex-ITESM at ImageCLEFcaption 2021 task : Deep Learning-based Information Retrieval and Multi-label Classification towards improving Medical Image Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.10,178.81,106.65,11.96"><forename type="first">Janadhip</forename><surname>Jacutprakart</surname></persName>
							<email>j.jacutprakart@essex.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Essex</orgName>
								<address>
									<addrLine>Wivenhoe Park</addrLine>
									<postCode>CO4 3SQ</postCode>
									<settlement>Colchester</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,208.40,178.81,130.31,11.96"><forename type="first">Francisco</forename><surname>Parrilla Andrade</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Instituto Tecnologico y</orgName>
								<orgName type="institution">de Estudios Superiores de Monterrey</orgName>
								<address>
									<addrLine>Av. Eugenio Garza Sada 2501 Sur</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Tecnológico</orgName>
								<address>
									<postCode>64849</postCode>
									<settlement>Monterrey</settlement>
									<region>N.L</region>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,351.35,178.81,67.09,11.96"><forename type="first">Rodolfo</forename><surname>Cuan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Instituto Tecnologico y</orgName>
								<orgName type="institution">de Estudios Superiores de Monterrey</orgName>
								<address>
									<addrLine>Av. Eugenio Garza Sada 2501 Sur</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Tecnológico</orgName>
								<address>
									<postCode>64849</postCode>
									<settlement>Monterrey</settlement>
									<region>N.L</region>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,88.87,192.76,114.46,11.96"><forename type="first">Arely</forename><forename type="middle">Aceves</forename><surname>Compean</surname></persName>
							<email>arelyac01@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Instituto Tecnologico y</orgName>
								<orgName type="institution">de Estudios Superiores de Monterrey</orgName>
								<address>
									<addrLine>Av. Eugenio Garza Sada 2501 Sur</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Tecnológico</orgName>
								<address>
									<postCode>64849</postCode>
									<settlement>Monterrey</settlement>
									<region>N.L</region>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,215.98,192.76,111.28,11.96"><forename type="first">Giorgos</forename><surname>Papanastasiou</surname></persName>
							<email>g.papanastasiou@essex.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Essex</orgName>
								<address>
									<addrLine>Wivenhoe Park</addrLine>
									<postCode>CO4 3SQ</postCode>
									<settlement>Colchester</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,358.25,192.76,117.48,11.96"><forename type="first">Alba</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Essex</orgName>
								<address>
									<addrLine>Wivenhoe Park</addrLine>
									<postCode>CO4 3SQ</postCode>
									<settlement>Colchester</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,402.19,15.42;1,89.29,106.66,375.79,15.42;1,89.29,128.58,351.62,15.43;1,89.29,150.49,236.37,15.43">NLIP-Essex-ITESM at ImageCLEFcaption 2021 task : Deep Learning-based Information Retrieval and Multi-label Classification towards improving Medical Image Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5604EAF28472D4806E86B91B640F9AAD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ImageCLEF</term>
					<term>image understanding</term>
					<term>concept detection</term>
					<term>medical image retrieval</term>
					<term>Densenet</term>
					<term>EfficientNet</term>
					<term>k-NN</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work presents the NLIP-Essex-ITESM team's participation in the concept detection sub-task of the ImageCLEFcaption 2021 task. We developed a method to predict health outcomes from medical images by processing concepts from radiology reports and their associated medical images. Our aim is to improved medical image understanding and provide sophisticated tools to automate the thorough analysis of multi-modal medical images. In this paper, two deep learning-and k-NN-based methods of a) Information Retrieval and b) Multi-label Classification were developed and assessed. In addition, a Densenet-121 and an EfficientNet were used to train and extract imaging features. Our team achieved the second-highest score when the Information Retrieval method was used (F1-score bench-marking was 0.469). Further investigations are underway in the setting of improving health outcome predictions from multi-modal medical images. Code and pre-trained models are available at https://github.com/ fjpa121197/ImageCLEF2021.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper presents the contributions of the NLIP-Essex-ITESM team in the ImageCLEFmed caption 2021 task. The team is composed of the Natural Language and Information Processing research group 1 at the School of Computer Science and Electronic Engineering (CSEE) of the University of Essex, and the Instituto Tecnológico y de Estudios Superiores de Monterrey. Since 2003, ImageCLEF <ref type="bibr" coords="1,168.28,556.98,11.32,10.91" target="#b0">[1,</ref><ref type="bibr" coords="1,182.33,556.98,8.93,10.91" target="#b1">2]</ref> held an evaluation campaign as part of the Cross Language Evaluation Forum (CLEF), creating a free online resource on topics and subjects related to cross-language information retrieval. The ImageCLEFmed caption task 2021 edition has two sub-tasks: concept detection and caption prediction. The NLIP-Essex-ITESM team participated in 2021 in the concept detection sub-task. The concept detection sub-task focuses on detecting concepts (i.e. UMLS®Concept Unique Identifiers) in a large corpus of radiology images. A detailed description of this year's sub-tasks and data are provided by Pelka et al. <ref type="bibr" coords="2,358.12,127.61,11.43,10.91" target="#b2">[3]</ref>.</p><p>For 2021, we propose two methods: one based on Information Retrieval (IR) and the other based on Multi-label classification (MLC). The IR method used two deep learning models (Densenet-121 <ref type="bibr" coords="2,155.65,168.26,12.84,10.91" target="#b3">[4]</ref> and EfficientNet <ref type="bibr" coords="2,246.02,168.26,12.07,10.91" target="#b4">[5]</ref>) whilst the MLC used only a Densenet-121 model.</p><p>In the ImageCLEFcaption 2020 edition <ref type="bibr" coords="2,271.49,181.81,11.35,10.91" target="#b5">[6]</ref>, the best results were achieved by the AUEB_NLP team <ref type="bibr" coords="2,114.18,195.36,11.50,10.91" target="#b6">[7]</ref>. They examined various Convolutional Neural Network (CNN) models such as Con-ceptCXN and DenseNet121, combined with two different approaches: feed-forward Neural Network (FFNN) or k-Nearest-Neighbours (k-NN). In this work, we applied the k-NN technique based on the AUEB_NLP team model and referred to our last year submission <ref type="bibr" coords="2,441.02,236.01,12.91,10.91" target="#b7">[8]</ref> to evaluate the data and compute the distance. We also implemented the k-NN approach with various metric types to improve the computational time on calculating the distances between a query image and an indexed image to retrieve a similar image in the IR method. Additionally, a new approach using semantic types implemented on MLC.</p><p>We developed and implemented several methods in the train and validation dataset. We selected the best approaches based on the top rank information retrieval F1-score evaluation performance. Code and pre-trained models used in this paper are fully publicly available <ref type="foot" coords="2,483.43,329.10,3.71,7.97" target="#foot_0">2</ref> .</p><p>The paper is structured as follows. Section 2 presents the data collections used in this work. Section 3 details the overall methodology and the two main modelling techniques proposed in this paper (IR and Multi-label classification), including a detailed description of the runs submitted to the ImageCLEFmed caption 2021 task. The results are presented in Section 4 and the conclusions in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Collection &amp; evaluation</head><p>In this work, we used the dataset provided by the ImageCLEFmed caption 2021 task <ref type="bibr" coords="2,469.86,457.22,11.55,10.91" target="#b2">[3]</ref>. The dataset consists of:</p><p>• Training set including 2,756 images-concepts pairs; • Validation set including 500 images-concepts pairs; • Test set including 444 images.</p><p>Each image is associated with multiple Unified Medical Language System ® (UMLS) Concept Unique Identifiers (CUIs). The UMLS CUIs (associated with the medical images) and 3,256 medical images were included in the training and validation datasets. However, the UMLS CUIs from the test dataset were not distributed where the ImageCLEFcaption task <ref type="bibr" coords="2,442.64,592.24,13.00,10.91" target="#b2">[3]</ref> organisers used F1-score to evaluate the submitted runs (see Section 3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>This work proposed two distinct methods: an Information Retrieval (IR)-and a Multi-label classification-based approach. Both methods used two different deep learning models (Densenet-121 and EfficientNet) for image training and feature extraction. In addition, we implemented k-NN to evaluate differences between a query image (whose features were obtained using the same extraction process) and each element in the set with different metrics for the concept selection part. The modelling pipeline uses the preprocessed images in the input and the corresponding concepts in both methods' output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Information Retrieval based approach</head><p>This approach is based on how a content-based image retrieval system works. Both deep learning models implemented were used for optimally extracting imaging features. The overall approach was separated into three main processes: model training, feature extraction and concept selection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Model Training.</head><p>For this process, both Densenet-121 and EfficientNet models, were used along with pre-trained weights (ImageNet) as base models. For Densenet-121, the input image remained at the maximum input size, 224×224; for EfficientNet, following careful performance evaluations across different input image resolutions, two different input size images were finally selected and examined further: EfficientNet B0 (224 × 224 ) and EfficientNet B3 (300 × 300).</p><p>As shown in Figure <ref type="figure" coords="3,192.07,561.19,3.81,10.91" target="#fig_0">1</ref>, the process starts by modifying a pre-trained model, Densenet-121 and EfficientNet, and replacing the classification layer with a layer that is allowed to converge to the unique concept output (1585 concepts). In addition, following transfer learning (via fine-tuning) principles, the pre-trained layers of the base model were kept frozen, allowing only the classification model layer to be trained <ref type="bibr" coords="3,285.95,615.39,16.41,10.91" target="#b10">[11]</ref>. By default, the activation function used for classification on the last layer uses sigmoid for Densenet-121 <ref type="bibr" coords="3,353.41,628.93,12.68,10.91" target="#b3">[4]</ref> and softmax for EfficientNet <ref type="bibr" coords="3,492.63,628.93,11.28,10.91" target="#b4">[5]</ref>. The classification layer of the model was first trained for 15 epochs. Once the initial training was over, a portion of the frozen layers was unfrozen, resulting in more trainable layers. The model was then re-trained again for 13 epochs, resulting in a fine-tuned model that was subsequently  This process consists of using a fine-tuned model to extract features for a set of images. A fine-tuned model (based on Densenet-121 and EfficientNet) was trained as described in the previous process. The same preprocessing steps used in training were followed, and the images were passed to the fine-tuned model. In last year's (ImageCLEF 2020) participation, the output from the batch-normalisation layer was used to get the features for the images. However, following evaluation, we decided that for the Densenet-121, the average pool layer would be used to get the features. On the contrary, EfficientNet used no pooling, which is the default value of the model; as a result, the output of the model will be in the 4D tensor output of the last convolutional layer (sample_size, image width, image height, color_depth). Afterwards, the features were saved to prepare for the next step (the concept selection process, see the following subsection). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Concept selection.</head><p>For the concept selection process, the set of features obtained from the feature extraction process were indexed by the corresponding imaging data. These indexed features served as the database used to evaluate differences between a query image (whose features were obtained using the same extraction process) and each element in the set. In order to evaluate the differences, distance calculation was implemented using the k-NN algorithm. Different experiments were generated using several metrics to find the one that yielded the best results. The distance metrics considered were Canberra, Cosine and Bray-Curtis <ref type="bibr" coords="5,326.14,364.46,16.41,10.91" target="#b11">[12]</ref>. We used Bray-Curtis to measured the differences between samples. Based on the F1 score, the Cosine and the Bray-Curtis were the best methods for the Densenet-121 and the EfficientNet, respectively. Although this year's concept selection process had an overall resemblance to our team's method from last year <ref type="bibr" coords="5,491.93,405.11,11.44,10.91" target="#b7">[8]</ref>, there was also a distance comparison followed by selecting a set of most similar images, where the process was modified to be more efficient. It has generated similar results to the last year in term of computing time using k-NN. Along with k-NN, through the experiments, a number of different values of k were applied, and we achieved the best outcome with k=1 and using this year's dataset only. As a result, the concepts assigned to the query image correspond to the concepts from the closest indexed image. Since only one image was retrieved, also the ranking process used for last year's concept selection was removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-label Classification</head><p>The main characteristic of this approach is that it only uses deep learning to predict the concepts for an image. Since an image can have multiple concepts assigned to it, the problem results in a multi-label classification problem, and consequently, a pre-trained Densenet-121 model has been adapted and used for this task. This approach consists of three main processes: concepts file preprocessing , model training and concepts prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Concepts preprocessing .</head><p>In this process, we propose that the concepts which present as output in the competition can be classified by their semantic type (Diagnostic Procedure and Body Part or Organ). We used umls-api, which is a Python package in order to retrieve the UMLS REST API. The UMLS REST API and Json output that offer links for important UMLS entities such as CUIs, atoms, and subsets. By using the UMLS REST API, an application programming interface (API) will retrieve a collection of convenient Uniform Resource Identifier (URI) patterns and obtained information related to each concept using its Concept Unique Identifiers (CUI), which consists of the following information: name from the source vocabulary, URLs that refer to the definition and relation(s), date added, semantic type, status, and Unique Identifiers (UI). The python library umls-api<ref type="foot" coords="6,128.13,370.22,3.71,7.97" target="#foot_1">3</ref> was used to access the mentioned API. In order to optimise the number of labels, the semantic type was selected to obtain each concept from the train and validation dataset. As a result, 33 different semantic types were obtained. The following list shows the top 5 most frequent semantic types: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Model training.</head><p>As previously mentioned, the preprocessing step creates six new concepts files, each one with the image and its corresponding concepts (of that semantic type). As a result, six models were be trained, where each model corresponds to a specific semantic type. Nonetheless, the model's training process remains the same (except for finding the best threshold).</p><p>As illustrated in Figure <ref type="figure" coords="7,204.36,204.04,3.70,10.91" target="#fig_8">7</ref>, the process starts by modifying a pre-trained Densenet-121 model (with ImageNet weights) for which the base layers have been frozen. A classification layer is added to the model, which has N number of outputs (this is the unique number of concepts per concept file). The model (only with the classification layer unfrozen) is trained for a certain number of epochs, varying according to each semantic type. Then, a particular portion of the base model is unfrozen, and the model is put into unfrozen again until it exceeds the maximum number of epochs (100) or the callback assigned to it. The specific parameters used for training are the following: Following two training phases, the output given by the classification layer is a probability (between 0 and 1) that a concept is present given an image; a threshold needs to be found using unseen data. After finding the threshold that gives the best F1-score on unseen data, the model and this threshold will be saved for the concept prediction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Concepts prediction.</head><p>For the prediction of the concepts, the query image passed through the same preprocessing and training method as IR but using only DenseNet-121 model. In this process, we have selected the best threshold found in each model, used only the concepts that passed in that threshold and assigned to the image. In the end, the predictions of each model were merged and used as the final output.</p><p>While testing the models and their scores on the validation set, it was noted that using the semantic types result in a positive effect in the overall F1-score; therefore, it was decided only to include these two models when predicting for the test set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Runs</head><p>This section provides a detailed description of the runs submitted to ImageCLEFcaption 2021 task.</p><p>• Run 132945: For this run, the IR method was implemented, using DenseNet-121 as the feature extraction (average pooling layer) and image inputs are loaded as 224 × 224. The length of the resulting feature vector for each image is 1024. Then, (k-NN) (k = 1 and metric = cosine) is used to retrieve the closest image and the concepts of the closest image are assigned to the query image. • Run 136379: For this run, the IR method is implemented, using EfficientNet B3 as feature extraction (no pooling layers) and image inputs are loaded as 300 × 300. The (k-NN) (k = 1 and metric = canberra) is used to retrieve the closest image and the concepts of the closest image are assigned to the query image. • Run 136400: In this run, the IR method is implemented, using EfficientNet • Run 133912: For this run, the MLC method is implemented, and based on F1-score obtained from the validation set, only the models for the diagnostic procedure and body part or organ were included in the finals predictions. The threshold used for assigning a concept for the diagnostic procedure model was 0.4 and for the body part or organ was 0.1. The other semantic types were not included because when testing on the validation set, adding the additional semantic types affected negatively the overall F1-score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>Table <ref type="table" coords="9,116.19,295.08,5.15,10.91" target="#tab_0">1</ref> summarises the techniques used by each run. Table <ref type="table" coords="9,359.04,295.08,5.15,10.91" target="#tab_0">1</ref> presents the official results our team achieved in the ImageCLEF 2021 Concept Detection Task and ranking along other 29 runs submitted by the five different teams. Our team received the second place in overall score with the best results on 132945 with F1-score of 0.469, close to a couple of submission from the first place team, AUEBs_NLP_Group who achieved F1-score of 0.505, 0.495, 0.493, 0.490, respectively. Notably, our best submission used DenseNet-121 as the feature extraction (average pooling layer)and image inputs are loaded as 224 × 224 for fine-tuning and image extraction with IR method using k-NN method with Cosine distance to retrieve the closest image and the concepts of the closest image are assigned to the query image. Based on the final results that our team has achieved, it is clear that using the further IR method with different distance metrics also improve the score differently between DenseNet-121 and EfficientNet. However, with another method using Multi-label classification, we use the same DenseNet-121 with the IR method but with a different process in the latter part. It might be due to the size of the dataset, in which a bigger dataset is required in order for the Multi-label classification method to be efficient. Besides the two different deep learning models we used this year, there is a slight difference in both results from using DenseNet-121 and EfficientNet. Similarly, using three different similarity metrics (Canberra, Cosine and BrayCurtis) also resulting in a slightly best score using Cosine on DenseNet-121 and BrayCurtis in the EfficientNet model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>This paper describes our contributions in the ImageCLEFcaption 2021 task. Two different methods were developed and used in this paper. An information retrieval method using two deep learning models, a DenseNet-121 and an EfficientNet ,to train and extract features from the data collections. At the same time, multi-label classification was implemented using a DenseNet-121 only. Considering the baseline model from last year <ref type="bibr" coords="10,388.31,213.34,11.47,10.91" target="#b7">[8]</ref>, we have differentiated and optimised our modelling pipeline to further generalise our approach and improve outcomes. Our DenseNet-121 model showed the highest performance when incorporated in IR method. Followig this method, we achieved the second-best performance (F1-score of 0.469. Unlike the previous year, no additional modality information was provided, which added additional complexixity in our processing pipeline. Further investigations on developing and customising deep learning model arcitectures and fine-tuning are already underway, so that we will further improve model performannce.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,428.34,205.87,8.93;3,89.29,305.34,416.68,98.49"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the model training process.</figDesc><graphic coords="3,89.29,305.34,416.68,98.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.29,180.80,199.41,8.93;4,89.29,206.88,416.71,112.70"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2:The architecture of EfficientNet B0<ref type="bibr" coords="4,276.87,180.85,11.83,8.87" target="#b8">[9]</ref> </figDesc><graphic coords="4,89.29,206.88,416.71,112.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,89.29,344.10,376.34,8.93"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3:The architecture of DenseNet-121 with 4 dense blocks and 3 transition layers<ref type="bibr" coords="4,449.18,344.15,16.46,8.87" target="#b9">[10]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,89.29,225.00,218.31,8.93;5,89.29,84.19,416.69,116.28"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Overview of the feature extraction process.</figDesc><graphic coords="5,89.29,84.19,416.69,116.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,89.29,239.53,216.04,8.93;6,89.29,84.19,416.69,130.82"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Overview of the concept selection process.</figDesc><graphic coords="6,89.29,84.19,416.69,130.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,89.29,487.84,201.89,8.93;6,89.29,394.69,416.70,68.63"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Overview of the preprocessing process.</figDesc><graphic coords="6,89.29,394.69,416.70,68.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="6,103.64,564.91,108.67,10.91;6,103.64,579.82,188.73,10.91;6,103.64,594.72,46.93,10.91;6,103.64,609.62,123.70,10.91;6,103.64,624.53,107.09,10.91;6,100.20,644.85,405.78,10.91;7,89.29,86.97,416.69,10.91;7,89.29,100.52,366.59,10.91"><head>1. Diagnostic Procedure 2 .</head><label>2</label><figDesc>Body Part, Organ or Organ Component 3. Finding 4. Body Location or Region 5. Disease or Syndrome An individual dataset was made for each type based on the top 5 categories, with an extra dataset for the remaining semantic types. Each dataset consisted of medical images as input and related concepts, based on each image (related to each specific semantic type).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="7,107.28,324.39,85.06,10.91;7,107.28,339.29,269.15,10.91;7,107.28,354.20,209.08,10.91;7,107.28,369.10,97.81,10.91;7,107.28,384.01,68.80,10.91;7,107.28,398.91,169.59,10.91;7,107.28,413.81,288.39,10.91"><head>•</head><label></label><figDesc>Optimiser: Adam • Initial Learning rate used only in classification layer: 0.0001 • Learning rate used in second training: 0.00001 • Validation Split : 0.2 • Batch size: 32 • Loss function: Binary Cross-Entropy • Epochs: This varies depending on the semantic type (see code).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="7,89.29,624.10,177.99,8.93;7,89.29,502.68,416.67,96.90"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Overview of the training process.</figDesc><graphic coords="7,89.29,502.68,416.67,96.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="8,89.29,377.51,225.55,8.93;8,89.29,224.58,416.69,128.41"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Overview of the concepts prediction process.</figDesc><graphic coords="8,89.29,224.58,416.69,128.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="8,448.06,600.91,57.93,10.91;8,116.56,614.46,389.71,10.91;8,116.56,628.01,389.43,10.91;8,116.56,641.56,206.30,10.91;8,107.28,656.03,398.70,10.91;8,116.56,669.58,389.42,10.91;9,116.31,86.97,389.67,10.91;9,116.56,100.52,173.69,10.91;9,107.28,114.54,398.71,10.91;9,116.56,128.08,389.42,10.91;9,116.31,141.63,389.67,10.91;9,116.56,155.18,206.30,10.91"><head></head><label></label><figDesc>B0 as feature extraction (no pooling layers) and image inputs are loaded as 224 × 224. The (k-NN) (k = 1 and metric = canberra) is used to retrieve the closest image and the concepts of the closest image are assigned to the query image. • Run 136404: For this run, the IR method is implemented, using EfficientNet B0 as feature extraction (no pooling layers) and image inputs are loaded as 224 × 224. The (k-NN) (k = 1 and metric = cosine) is used to retrieve the closest image and the concepts of the closest image are assigned to the query image. • Run 136429: For this run, the IR method is implemented, using EfficientNet B0 as feature extraction (no pooling layers) and image inputs are loaded as 224 × 224. The (k-NN) (k = 1 and metric = braycurtis) is used to retrieve the closest image and the concepts of the closest image are assigned to the query image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,88.99,323.03,416.99,139.45"><head>Table 1</head><label>1</label><figDesc>Description and performance of the runs submitted to ImageCLEF 2021 Concept Detection Task and their ranks compared with all the 29 runs submitted by the 5 participating teams.</figDesc><table coords="9,99.98,366.17,393.33,96.31"><row><cell cols="3">Run ID Size Input Image Method</cell><cell>DL Model</cell><cell cols="3">Similarity measure F1-Score Ranking</cell></row><row><cell>132945</cell><cell>224 × 224</cell><cell>IR</cell><cell>Densenet-121</cell><cell>Cosine</cell><cell>0.469</cell><cell>6</cell></row><row><cell>133912</cell><cell>224 × 224</cell><cell cols="2">Multi-label classification Densenet-121</cell><cell>N/A</cell><cell>0.412</cell><cell>15</cell></row><row><cell>136379</cell><cell>300 × 300</cell><cell>IR</cell><cell cols="2">EfficienNetB3 Canberra</cell><cell>0.355</cell><cell>21</cell></row><row><cell>136400</cell><cell>224 × 224</cell><cell>IR</cell><cell cols="2">EfficienNetB0 Canberra</cell><cell>0.423</cell><cell>13</cell></row><row><cell>136404</cell><cell>224 × 224</cell><cell>IR</cell><cell cols="2">EfficienNetB0 Cosine</cell><cell>0.440</cell><cell>12</cell></row><row><cell>136429</cell><cell>224 × 224</cell><cell>IR</cell><cell cols="2">EfficienNetB0 BrayCurtis</cell><cell>0.451</cell><cell>11</cell></row><row><cell cols="2">Best ImageCLEF2021</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.505</cell><cell>1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,108.93,670.95,171.91,8.97"><p>https://github.com/fjpa121197/ImageCLEF2021</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="6,108.93,671.03,280.03,8.97"><p>Python library can be found in https://github.com/odwyersoftware/umls-api</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,112.66,366.81,394.53,10.91;10,112.66,380.36,393.33,10.91;10,112.66,393.91,393.33,10.91;10,112.66,407.46,132.86,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,160.23,380.36,345.76,10.91;10,112.66,393.91,244.80,10.91">Evaluating performance of biomedical image retrieval systems-an overview of the medical image retrieval task at imageclef 2004-2013</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,366.27,393.91,139.72,10.91;10,112.66,407.46,59.07,10.91">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="55" to="61" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,421.01,395.01,10.91;10,112.66,434.55,394.53,10.91;10,112.48,448.10,395.18,10.91;10,112.66,461.65,394.53,10.91;10,112.28,475.20,393.70,10.91;10,112.66,488.75,393.33,10.91;10,112.66,502.30,393.33,10.91;10,112.66,515.85,393.53,10.91;10,112.66,529.40,197.61,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,264.32,475.20,241.66,10.91;10,112.66,488.75,261.75,10.91">Overview of the ImageCLEF 2021: Multimedia retrieval in medical, nature, internet and social media applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Peteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tauteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Moustahfid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,401.27,488.75,104.72,10.91;10,112.66,502.30,393.33,10.91;10,112.66,515.85,201.15,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 12th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="10,348.61,515.85,157.57,10.91;10,112.66,529.40,31.10,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,542.95,394.53,10.91;10,112.66,556.50,394.53,10.91;10,112.66,570.05,394.53,10.91;10,112.66,583.60,67.18,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,167.43,556.50,335.54,10.91">Overview of the ImageCLEFmed 2021 concept &amp; caption prediction task</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,127.66,570.05,114.61,10.91">CLEF2021 Working Notes</title>
		<title level="s" coord="10,249.86,570.05,179.10,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,597.15,395.17,10.91;10,112.66,610.69,393.33,10.91;10,112.66,624.24,147.08,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,377.68,597.15,130.16,10.91;10,112.66,610.69,67.72,10.91">Densely connected convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,203.84,610.69,302.14,10.91;10,112.66,624.24,49.16,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,637.79,394.53,10.91;10,112.66,651.34,352.61,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,178.42,637.79,323.86,10.91">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,127.29,651.34,207.49,10.91">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,664.89,394.53,10.91;11,112.66,86.97,394.53,10.91;11,112.30,100.52,395.53,10.91;11,112.66,114.06,394.53,10.91;11,112.66,127.61,393.33,10.91;11,112.66,141.16,393.33,10.91;11,112.66,154.71,393.32,10.91;11,112.66,168.26,393.32,10.91;11,112.66,181.81,249.02,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,303.77,127.61,202.22,10.91;11,112.66,141.16,294.88,10.91">Overview of the ImageCLEF 2020: Multimedia retrieval in medical, lifelogging, nature, and internet applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V.-T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D.-T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,432.20,141.16,73.78,10.91;11,112.66,154.71,233.84,10.91;11,428.42,155.73,77.56,9.72;11,112.66,169.28,262.00,9.72">Proceedings of the 11th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="11,405.27,168.26,100.72,10.91;11,112.66,181.81,78.83,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 11th International Conference of the CLEF Association (CLEF<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">12260</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="11,112.66,195.36,395.17,10.91;11,112.66,208.91,393.53,10.91;11,112.66,222.46,77.21,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,380.29,195.36,127.54,10.91;11,112.66,208.91,81.66,10.91">AUEB NLP Group at Image-CLEFmed Caption</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,238.42,208.91,115.27,10.91">CLEF 2020 Working Notes</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-09-22">2020. September 22-25, 2020, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,236.01,393.33,10.91;11,112.66,249.56,394.53,10.91;11,112.33,263.11,226.53,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,382.04,236.01,123.95,10.91;11,112.66,249.56,40.28,10.91">Essex at Image-CLEFcaption 2020 task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">P</forename><surname>Andrade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bentley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Compean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,175.92,249.56,326.83,10.91">CLEF2020 Working Notes. CEUR Workshop Proceedings, CEUR-WS. org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,276.66,393.33,10.91;11,112.66,290.20,393.98,10.91;11,112.41,303.75,59.11,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,264.30,276.66,241.69,10.91;11,112.66,290.20,283.62,10.91">Enhanced Skin Condition Prediction Through Machine Learning Using Dynamic Training and Testing Augmentation</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Putra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">I</forename><surname>Rufaida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-S</forename><surname>Leu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,409.86,290.20,56.55,10.91">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="40536" to="40546" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,317.30,393.33,10.91;11,112.66,330.85,240.16,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Sarker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hannan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ahmed</surname></persName>
		</author>
		<title level="m" coord="11,308.70,317.30,197.29,10.91;11,112.66,330.85,208.24,10.91">Covid-densenet: A deep learning architecture to detect covid-19 from chest radiology images</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,344.40,395.17,10.91;11,112.66,357.95,393.32,10.91;11,112.66,371.50,381.36,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,306.30,344.40,201.54,10.91;11,112.66,357.95,288.05,10.91">Medical image analysis using deep convolutional neural networks: CNN architectures and transfer learning</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Khalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,423.84,357.95,82.14,10.91;11,112.66,371.50,266.96,10.91">2020 International Conference on Inventive Computation Technologies (ICICT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="175" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,385.05,394.53,10.91;11,112.66,398.60,393.33,10.91;11,112.66,412.15,394.52,10.91;11,112.66,425.70,123.33,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,313.71,385.05,193.48,10.91;11,112.66,398.60,393.33,10.91;11,112.66,412.15,45.68,10.91">Analysis and Implementation of the Bray-Curtis Distance-Based Similarity Measure for Retrieving Information from the Medical Repository</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,181.40,412.15,320.62,10.91">International Conference on Innovative Computing and Communications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="117" to="125" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
