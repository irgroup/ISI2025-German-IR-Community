<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,397.74,15.42;1,89.29,106.66,271.18,15.42">Multi-Classification Study of the Tuberculosis with 3D CBAM-ResNet and EfficientNet</title>
				<funder ref="#_DdUdRpq">
					<orgName type="full">Office of the Assistant Secretary of Defense for Health Affairs</orgName>
				</funder>
				<funder ref="#_WEZuVSF">
					<orgName type="full">in Military Medicine Program</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.90,134.97,39.24,11.96"><forename type="first">Xing</forename><surname>Lu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,140.78,134.97,64.39,11.96"><forename type="first">Eric</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
							<email>e8chang@health.ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">San Diego VA Health Care System</orgName>
								<address>
									<settlement>San Diego</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,223.10,134.97,74.05,11.96"><forename type="first">Chun-Nan</forename><surname>Hsu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">San Diego VA Health Care System</orgName>
								<address>
									<settlement>San Diego</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,315.08,134.97,42.73,11.96"><forename type="first">Jiang</forename><surname>Du</surname></persName>
							<email>jiangdu@health.ucsd.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,388.80,134.97,80.84,11.96"><forename type="first">Amilcare</forename><surname>Gentili</surname></persName>
							<email>agentili@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">San Diego VA Health Care System</orgName>
								<address>
									<settlement>San Diego</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,397.74,15.42;1,89.29,106.66,271.18,15.42">Multi-Classification Study of the Tuberculosis with 3D CBAM-ResNet and EfficientNet</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">58F32C1A808DB1C41C102A1554A49B8D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Tuberculosis</term>
					<term>Computed Tomography</term>
					<term>Image Classification</term>
					<term>Tuberculosis Type</term>
					<term>CBAM</term>
					<term>Efficient Net</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The detection and characterization of tuberculosis along with the evaluation of tuberculosis lesion characteristics are challenging. To provide a solution for this multi-classification task, we performed a deep learning study that relied on the use of 3D Resnet and EfficientNet. With proper application of the provided masks, lung images were cropped, masked, and rearranged with different windowing. Stratified sampling for train/validation split and a balanced sampler in each batch sampler during training were used to address the data imbalance problem. A convolutional block attention model (CBAM) was used to add an attention mechanism in each block of the Resnet to further improve the performance of the convolutional neural network (CNN).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Tuberculosis (TB) is a bacterial infection caused by the germ Mycobacterium tuberculosis, and is a leading cause of death from infectious disease worldwide. An epidemic in many developing regions such as Africa and Southeast Asia, TB was responsible for 1.6 million deaths in 2017 alone. There are different manifestations of TB which require different treatments, making the detection and characterization of TB disease and the evaluation of lesion characteristics critically important tasks in the monitoring, control, and treatment of the disease. An accurate and automated method for classification of TB from computed tomography (CT) images would be especially useful in regions of the world with few radiologists.</p><p>The ImageCLEF 2021 Tuberculosis -CT report challenge <ref type="bibr" coords="1,356.55,500.93,12.73,10.91" target="#b0">[1]</ref> aimed to automatically categorize each TB case into one of the following five types: (1) Infiltrative, (2) focal, (3) tuberculoma, (4) miliary, and (5) fibro-cavernous <ref type="bibr" coords="1,249.28,528.02,11.45,10.91" target="#b1">[2]</ref>. In this edition, a dataset containing chest CT scans of 1338 TB patients was used: 917 images for the Training (development) dataset and 421 for the Test set. Because each CT scan corresponded to only one patient, each CT image corresponded to only one TB type at a time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Data</head><p>The datasets provided for the tuberculosis task training set contained a total of 917 patients, with labeling provided for five categories. To avoid bias in the training and validation cohorts, a balanced train/validation strategy was employed to split each class according to an 8:2 ratio, as shown in Figure <ref type="figure" coords="2,166.28,172.89,3.81,10.91" target="#fig_0">1</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Preprocessing</head><p>The preprocessing of the images for the deep learning model is shown in Figure <ref type="figure" coords="2,431.67,427.20,3.66,10.91" target="#fig_2">2</ref>. The images for the ImageCLEF TB task were provided as NIFTI 3D datasets. Two versions of lung segmentation masks were also provided. The first version of segmentation (denoted as Mask-1) provided more accurate masks, containing individual masks for left and right laterality (values equal 1 for left and 2 for right), but in the most severe TB cases, there was a tendency to miss large abnormal regions in the lungs <ref type="bibr" coords="2,230.97,494.95,11.58,10.91" target="#b2">[3]</ref>. On the other hand, the second segmentation (denoted as Mask-2) provided less precise boundaries, given that it contained the entire lung area (i.e., both left and right sides of the lung), but was more stable in terms of including lesion areas <ref type="bibr" coords="2,475.92,522.05,11.41,10.91" target="#b3">[4]</ref>. As there was no need to locate lesions in terms of lung side, only Mask-2 was used in this study.</p><p>As shown in Fig. <ref type="figure" coords="2,178.67,549.15,3.81,10.91" target="#fig_2">2</ref>, the original NIFTI-formatted dataset was transformed into image data by first applying the NiBabel package. Next, the reformatted images were adjusted to three different window levels, namely baseline, lung, and soft tissue, and then normalized. For baseline window level, the foreground was obtained via the Otsu thresholding algorithm provided in the openCV package; for lung and soft tissue, the image levels were set as [-600,1500] and [50,350], respectively. Then, images were normalized to [0,1] with their mean and std values. Finally, all three windows and levels of data were saved, and annotation files were rearranged for use in further training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Network and Training</head><p>In this study, a 3D convolutional block attention module (CBAM)-Resnet and a 3D EfficientNet were employed to train the model for 5-class classification based on the PyTorch framework. Similar to our last year's work <ref type="bibr" coords="3,228.25,433.78,11.51,10.91" target="#b4">[5]</ref>, a standard 3D-resnet34 <ref type="bibr" coords="3,355.64,433.78,12.92,10.91" target="#b5">[6]</ref> was used as the convolutional neural network (CNN) backbone, with three fc layers as the classifier. CBAM <ref type="bibr" coords="3,440.29,447.33,15.24,10.91" target="#b6">[7]</ref>was used to implement channel and spatial attention mechanisms for each block of the Resnet, and sigmoid was used as the activation function for binary classification. according to our computing resources, EfficientNet B5 was the optimal 3D EfficientNet for training <ref type="bibr" coords="3,408.17,487.98,11.43,10.91" target="#b7">[8]</ref>.</p><p>To train the neural networks, we used a workstation with 4 Nvidia GTX 1080 Ti video cards, 128 GB RAM, and a 1 TB solid state drive. During the training process, to avoid overfitting, image augmentation and a balanced sampler were implemented in each batch. For the image augmentation, traditional data augmentation methods, including brightness, shear, scale, and flip, were applied. The balanced sampler strategy, which equalized the data sampled from all five classes for each batch, was adopted during the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Experiments and Model Selection</head><p>Three experiments were conducted during model training. For 3D Resnet, 20 datasets that fed into the network were dynamically generated from saved metadata with different window levels as a single channel and were interpolated into two kinds of data size: 3×64×256×256 and 3×16×384×384. For 3D EfficientNet, only 3×64×256×256 was used. For each experiment, 60 epochs with a cosine annealing warm-up learning rate were performed to train the model. To find the best model for each experiment during training, epochs with either minimum loss or highest accuracy were selected and saved for further submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results and Submissions</head><p>The provided TST dataset included 421 image files for testing. With our preprocessing pipeline, the TST data were cropped according to Mask-2 to generate calibrated image files. After evaluation of the trained model, the results were rearranged according to the requirement and saved as a .txt file to be submitted. As was mentioned in the Methods, we saved six models with different metrics for evaluating the TST datasets; their performances are displayed in Table <ref type="table" coords="4,497.16,226.89,3.74,10.91" target="#tab_0">1</ref>. Per the submitted results, 3D Resnet34 achieved both better accuracy and Kappa than Effi-cientNet B5. For 3D Resnet34, the model with tensor size 3×64×256×256 and a loss-based selection model achieved a superior Kappa result of 0.190 and accuracy of 0.371, with submission name of 154940_loss. We also tried to assemble the results from different models into a single submission,137652, but the result was not significantly improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion and Conclusion</head><p>To provide a deep learning solution for a multi-classification task of tuberculosis, we performed experiments using 3D CBAM Resnet and 3D EfficientNet as CNN backbones. There were several challenges for this task, such as the severe class imbalance and 3D dimensionality of the CT images, so we tried several techniques to improve the models' performance. First, we properly applied stratified sampling of each class for the train/validation split to mitigate bias in the training and validation cohorts. Furthermore, a balanced sampler in each batch sampler was used to address the data imbalance problem. Second, CBAM was used to add an attention mechanism to each block of the Resnet to further improve the performance of the CNN. Third, different windowings of the CT images were concatenated to further focus the CNN on features of the illness according to a radiologist. Using all the aforementioned techniques, we achieved a kappa of 0.190 in the evaluation of the test dataset and placed third in this competition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,177.74,172.89,328.24,10.91;2,89.29,186.44,417.89,10.91;2,89.29,199.99,416.69,10.91;2,89.29,213.54,353.32,10.91"><head>Figure 1 (</head><label>1</label><figDesc>a) shows the results of a random train/validation split, whereas Figure 1(b) shows the balanced split. As can be seen in Figure 1(a), in the fibro-cavernous class, there were only a few examples in the validation dataset, whereas the balanced split shown in Figure 1(b) resulted in each class having similar validation and training dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,89.29,361.38,299.62,8.93;2,137.63,236.25,317.52,112.56"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Stratified sampling of different classes for train/validation split</figDesc><graphic coords="2,137.63,236.25,317.52,112.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,89.29,348.51,162.72,8.93;3,102.71,84.19,387.36,251.76"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Preprocessing for the images.</figDesc><graphic coords="3,102.71,84.19,387.36,251.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.99,255.90,391.01,120.64"><head>Table 1</head><label>1</label><figDesc>Submission model types and results</figDesc><table coords="4,115.27,283.54,364.73,93.00"><row><cell>Submission name</cell><cell>Model type</cell><cell>Tensor size</cell><cell>Accuracy</cell><cell>Kappa</cell></row><row><cell>194900_loss</cell><cell>Resnet34_loss</cell><cell>3×16×384×384</cell><cell>0.371</cell><cell>0.179</cell></row><row><cell>194900_acc</cell><cell>Resnet34_acc</cell><cell>3×16×384×384</cell><cell>0.354</cell><cell>0.163</cell></row><row><cell>154940_loss</cell><cell>Resnet34_loss</cell><cell>3×16×384×384</cell><cell>0.371</cell><cell>0.190</cell></row><row><cell>154940_acc</cell><cell>Resnet34_acc</cell><cell>3×16×384×384</cell><cell>0.363</cell><cell>0.177</cell></row><row><cell>060907_loss</cell><cell>Efficient B5_loss</cell><cell>3×64×256×256</cell><cell>0.352</cell><cell>0.130</cell></row><row><cell>060907_acc</cell><cell>Efficient B5_acc</cell><cell>3×64×256×256</cell><cell>0.361</cell><cell>0.155</cell></row><row><cell>135762</cell><cell>Model_ensembling</cell><cell>-</cell><cell>0.359</cell><cell>0.171</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5.">Acknowledgments</head><p>This work was supported in part by the <rs type="funder">Office of the Assistant Secretary of Defense for Health Affairs</rs> through the <rs type="programName">Accelerating Innovation</rs> <rs type="funder">in Military Medicine Program</rs> under Award No. (<rs type="grantNumber">W81XWH-20-1-0693</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_DdUdRpq">
					<orgName type="program" subtype="full">Accelerating Innovation</orgName>
				</org>
				<org type="funding" xml:id="_WEZuVSF">
					<idno type="grant-number">W81XWH-20-1-0693</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,107.59,197.00,400.08,10.91;5,107.59,210.55,399.60,10.91;5,107.41,224.10,400.26,10.91;5,107.59,237.65,399.59,10.91;5,107.20,251.20,398.78,10.91;5,107.59,264.75,400.24,10.91;5,107.59,278.30,398.40,10.91;5,107.59,291.85,399.60,10.91;5,107.59,305.40,116.58,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,261.21,251.20,244.78,10.91;5,107.59,264.75,247.89,10.91">Overview of the ImageCLEF 2021: Multimedia retrieval in medical, nature, internet and social media applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Peteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tauteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Moustahfid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,378.03,264.75,129.79,10.91;5,107.59,278.30,398.40,10.91;5,107.59,291.85,137.45,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 12th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="5,276.45,291.85,183.53,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,107.59,318.95,400.24,10.91;5,107.59,332.50,399.60,10.91;5,107.59,346.05,399.60,10.91;5,107.59,359.59,22.69,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,387.91,318.95,119.91,10.91;5,107.59,332.50,258.95,10.91">Overview of ImageCLEFtuberculosis 2021 -CT-based tuberculosis type classification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="5,389.21,332.50,113.25,10.91">CLEF2021 Working Notes</title>
		<title level="s" coord="5,107.59,346.05,184.28,10.91">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,107.59,373.14,398.68,10.91;5,107.59,386.69,399.60,10.91;5,107.20,400.24,398.78,10.91;5,107.59,413.79,400.24,10.91;5,107.18,427.34,113.86,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,423.56,373.14,82.71,10.91;5,107.59,386.69,224.38,10.91">Efficient and fully automatic segmentation of the lungs in ct volumes</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">Dicente</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">A</forename><surname>Jiménez Del Toro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Depeursinge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="5,299.71,400.24,206.27,10.91;5,107.59,413.79,136.22,10.91">Proceedings of the VISCERAL Anatomy Grand Challenge at the 2015 IEEE ISBI</title>
		<title level="s" coord="5,250.46,413.79,156.90,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">O</forename><surname>Goksel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Jiménez Del Toro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Foncubierta-Rodríguez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<meeting>the VISCERAL Anatomy Grand Challenge at the 2015 IEEE ISBI</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,107.59,440.89,398.40,10.91;5,107.59,454.44,400.24,10.91;5,106.53,467.99,233.74,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,216.50,440.89,289.49,10.91;5,107.59,454.44,86.60,10.91">Imageclef 2017: Supervoxels and co-occurrence for tuberculosis ct image classification</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="5,216.98,454.44,113.62,10.91">CLEF2017 Working Notes</title>
		<title level="s" coord="5,338.08,454.44,169.75,10.91;5,106.53,467.99,16.54,10.91">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,107.59,481.54,398.40,10.91;5,107.59,495.09,398.40,10.91;5,107.59,508.64,399.60,10.91;5,107.06,522.18,400.12,10.91;5,107.59,535.73,399.60,10.91;5,107.59,549.28,238.81,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,343.94,481.54,162.05,10.91;5,107.59,495.09,398.40,10.91;5,107.59,508.64,144.04,10.91">Imageclef2020: Laterality-reduction three-dimensional cbam-resnet with balanced sampler for multi-binary classification of tuberculosis and CT auto reports</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gentili</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_70.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="5,107.06,522.18,334.28,10.91">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="5,314.02,535.73,150.47,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25, 2020. 2696. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,107.59,562.83,400.08,10.91;5,107.59,578.82,97.35,7.90" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<title level="m" coord="5,264.91,562.83,210.20,10.91">Deep residual learning for image recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,107.59,589.93,400.08,10.91;5,107.59,605.92,97.35,7.90" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06521</idno>
		<title level="m" coord="5,275.30,589.93,201.45,10.91">Cbam: Convolutional block attention module</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,107.59,617.03,399.60,10.91;5,107.59,630.58,122.77,10.91" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="5,182.54,617.03,319.80,10.91">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
