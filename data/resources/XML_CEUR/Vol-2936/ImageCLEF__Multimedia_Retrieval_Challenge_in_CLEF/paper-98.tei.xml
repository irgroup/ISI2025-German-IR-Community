<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.78,84.74,347.99,15.42;1,89.29,106.66,374.78,15.42">TeamS at VQA-Med 2021: BBN-Orchestra for Long-tailed Medical Visual Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,138.55,80.31,5.42"><forename type="first">Sedigheh</forename><surname>Eslami</surname></persName>
							<email>sedigheh.eslami@data4life.care@hpi.de</email>
							<affiliation key="aff0">
								<orgName type="department">D4L data4life gGmbH</orgName>
								<address>
									<addrLine>Charlottenstraße 109</addrLine>
									<postCode>14467</postCode>
									<settlement>Potsdam</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Hasso Plattner Institute</orgName>
								<address>
									<addrLine>Prof.-Dr.-Helmert-Straße 2-3</addrLine>
									<postCode>14482</postCode>
									<settlement>Potsdam</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,187.53,138.55,75.77,5.42"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
							<email>gerard.demelo@hpi.de</email>
							<affiliation key="aff1">
								<orgName type="institution">Hasso Plattner Institute</orgName>
								<address>
									<addrLine>Prof.-Dr.-Helmert-Straße 2-3</addrLine>
									<postCode>14482</postCode>
									<settlement>Potsdam</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,294.30,138.55,85.53,5.42"><forename type="first">Christoph</forename><surname>Meinel</surname></persName>
							<email>christoph.meinel@hpi.de</email>
							<affiliation key="aff1">
								<orgName type="institution">Hasso Plattner Institute</orgName>
								<address>
									<addrLine>Prof.-Dr.-Helmert-Straße 2-3</addrLine>
									<postCode>14482</postCode>
									<settlement>Potsdam</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.78,84.74,347.99,15.42;1,89.29,106.66,374.78,15.42">TeamS at VQA-Med 2021: BBN-Orchestra for Long-tailed Medical Visual Question Answering</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">A4E78D17F7A488AAED105CAF1B105FA3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical visual question answering</term>
					<term>Long-tailed visual recognition</term>
					<term>Ensemble learning</term>
					<term>Bilateral neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work describes our (TeamS) participation in the Medical Domain Visual Question Answering challenge (VQA-Med) at ImageCLEF 2021. We translate the VQA problem to long-tailed multi-class image classification for categorizing abnormalities present in medical images. Our proposed BBN-Orchestra is an ensemble of bilateral-branch networks (BBN) and successfully reduces overfitting to train and validation data in addition to effectively modeling the imbalanced long-tailed image distribution. BBN-Orchestra employs a voting mechanism to assign final predicted classes in the inference phase. Our proposed method achieved a test accuracy of 34.8% and a BLEU score of 39.1%, ranking 3 rd in the competition. Our source code is available at https://github.com/d4l-data4life/BBNOrchestra-for-VQAmed2021.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Digitized medical data brings the potential to develop multi-modal tools such as Visual Question Answering (VQA) systems that can assist patients, clinicians, and radiology trainees in order to expedite patient care. Medical VQA systems are capable of answering questions about a given medical image and thereby aid in assessing and interpreting a radiology image. Despite this enormous potential, the development of medical VQA systems remains in its infancy due to complications arising from the scarcity of available training data, the distribution of this data, as well as the disparity between the natural language question and the medical image modalities. Recent progress has been driven by the VQA-RAD <ref type="bibr" coords="1,318.82,499.59,12.94,4.94" target="#b0">[1]</ref> and SLAKE <ref type="bibr" coords="1,388.66,499.59,12.94,4.94" target="#b1">[2]</ref> datasets, as well as the ImageCLEF initiative, which has been releasing VQA datasets extracted from PubMed Central articles and hosting challenges for developing task-oriented medical VQA systems. Still, due to the wide range of possible answers and the imbalanced distribution of the training data with respect to such answers, it is non-trivial to train a model to perform well on this task. For many sorts of answers, there are only very few example instances in the training data.</p><p>In this work, we investigate the effectiveness of deep learning approaches that overcome these challenges and are able to cope with long-tailed medical VQA data. Our BBN-Orchestra approach recasts the VQA task as a long-tailed multi-class image classification problem and learns an ensemble of bilateral-branch networks (BBNs) to better model imbalanced long-tailed training data and mitigate overfitting. This paper presents our submissions at VQA-MED 2021 challenge <ref type="bibr" coords="2,406.76,117.33,11.59,4.94" target="#b2">[3]</ref>. We developed an ensemble model using Bilateral-Branch Networks (BBN) in order to simultaneously learn effective representative image features and achieve accurate classifiers considering the longtailed class distribution. We further compare our results with single BBN using different backbone architectures. Our model was the 3 rd ranked one in the VQA-Med 2021 Challenge, with 34.8% accuracy and 39.1% BLEU score on the test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Medical visual question answering is a challenging problem due to the diversity of the questions, diversity of the image data, as well as the insufficient and imbalanced annotated data distributions. This task has been previously investigated by classifying multi-modal features obtained by fusing encoded questions and images. Vu et al. <ref type="bibr" coords="2,360.81,282.10,11.79,9.72" target="#b3">[4]</ref> use pre-trained CNN models and Skip-Thought Vectors to encode the image and question, respectively, and combine them via attention mechanisms. Zhan et al. <ref type="bibr" coords="2,260.81,309.20,11.80,9.72" target="#b4">[5]</ref> enhance the attention-based fusion strategies via a novel question-type-specific conditional reasoning module that further highlights the important segments of the questions. Nguyen et al. <ref type="bibr" coords="2,270.42,336.30,11.72,9.72" target="#b5">[6]</ref> propose to use publicly available unlabeled image datasets in an unsupervised fashion using meta learning in order to enhance image features and thereby overcome data constraints. The winning team of the VQA-Med 2020 challenge <ref type="bibr" coords="2,493.22,365.64,12.77,4.94" target="#b6">[7]</ref> firstly maps similar questions into unified backbones in order to detect the type of the questions in a rule-based fashion. Afterwards, an ensemble multi-task classification network with ResNet, ResNext, VGG and MobileNet backbones is applied for image classification. Kovaleva et al. <ref type="bibr" coords="2,494.25,404.04,11.73,9.72" target="#b7">[8]</ref> utilize the MIMIC-CXR dataset to create the first publicly available visual dialogue dataset for radiology, which is not only useful for medical VQA, but also draws on the medical history of patients in order to better answer visual-based questions. In the VQA-Med 2021 challenge <ref type="bibr" coords="2,493.10,446.94,12.88,4.94" target="#b2">[3]</ref> hosted by ImageCLEF <ref type="bibr" coords="2,187.68,460.49,11.29,4.94" target="#b8">[9]</ref>, we propose to solve the VQA problem purely by image classification, since the dataset consists entirely of questions sharing a common semantic interpretation, despite being presented in different syntactical forms: "What abnormality is present in the image?". Inspired by the HCP-MIC team's work <ref type="bibr" coords="2,304.61,501.13,16.26,4.94" target="#b9">[10]</ref>, we adopt Bilateral-Branch Networks for classification in the presence of an imbalanced long-tail class distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>By exploring the datasets released in the challenge, we observe that the training data includes two general types of questions:</p><p>1. "yes/no" questions, asking about the presence of a medical abnormality in an image, 2. "what" questions, asking about the category of abnormality present in an image. Although these questions appear in different syntactical forms, their semantics can be categorized into the two mentioned types and can be detected by simple rule-based mechanisms. Furthermore, we notice that the questions in the validation and test sets are only of the aforementioned "what" type. Therefore, we decided to translate the VQA setting in this specific challenge to a multi-class image classification problem. Denote by 𝐷 T = {(𝑥 𝑖 , 𝑞 𝑖 , 𝑎 𝑖 )} 𝑛 T 𝑖=1 the training dataset for a generic VQA model, where 𝑛 T is the number of samples in the training set, and 𝑥, 𝑞, 𝑎 represent image, question, and answer, respectively. We can enumerate the set of answers and assume 𝑎 𝑖 ∈ {1, 2, . . . , 𝐶}, where 𝐶 is the total number of candidate answers and is typically large. In this task, since all questions solicit the same kind of information, we relax the VQA problem to learn a function 𝑓 that maps each 𝑥 𝑖 to 𝑎 𝑖 and thereby, classify the abnormality for each medical image.</p><p>BBN-Orchestra is an ensemble deep learning solution using Bilateral-Branch Networks (BBNs) <ref type="bibr" coords="3,89.29,198.62,16.41,4.94" target="#b10">[11]</ref>. As previous work <ref type="bibr" coords="3,195.93,198.62,16.56,4.94" target="#b9">[10,</ref><ref type="bibr" coords="3,215.22,198.62,12.59,4.94" target="#b10">11,</ref><ref type="bibr" coords="3,230.55,198.62,14.11,4.94" target="#b11">12]</ref> suggests, BBNs achieve effective results when classifying data with long-tail distribution, i.e., when a few classes form most of the data, whereas most classes have very few samples. BBNs consist of three main components:</p><p>1 In BBN-Orchestra, we use ensemble multiple BBNs in order to prevent potential over-fitting with regard to the training and validation sets. To achieve this, the training and validation splits are combined first to form 𝐷 = 𝐷 T ∪ 𝐷 V = {(𝑥 𝑖 , 𝑎 𝑖 )} 𝑛 𝑖=1 , where 𝐷 V is the validation set and 𝑛 = 𝑛 T + 𝑛 V . <ref type="foot" coords="3,191.39,595.60,3.71,3.61" target="#foot_1">2</ref> We train 𝐾 different BBN models with diverse backbone networks using different random splits from 𝐷. In the inference phase, the voting mechanism selects the most frequently predicted class by the 𝐾 trained BBNs as the final predicted label for an unseen sample. Training and inference phases of BBN-Orchestra are summarized in Algorithms 1 and 2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 BBN Orchestra: Inference</head><formula xml:id="formula_0" coords="4,96.10,126.40,169.89,53.70">Input: 𝐷 ′ = {(𝑥 ′ 𝑖 )} 𝑛 ′ 𝑖=1 , 𝑚𝑒𝑚𝑏𝑒𝑟𝑠 Output: {(𝑥 ′ 𝑖 , 𝑎 ˆ𝑖)} 𝑛 ′ 𝑖=1 1: procedure Predict(𝐷 ′ , 𝑚𝑒𝑚𝑏𝑒𝑟𝑠) 2:</formula><p>𝑑𝑎𝑡𝑎_𝑠𝑖𝑧𝑒 ← len (𝐷 ′ ) 3:</p><p>𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑖𝑜𝑛𝑠 ← dict()</p><formula xml:id="formula_1" coords="4,96.10,203.14,6.29,4.06">4:</formula><p>𝑎 ˆ← dict()</p><formula xml:id="formula_2" coords="4,96.10,216.69,6.29,4.06">5:</formula><p>for 𝑚𝑜𝑑𝑒𝑙 in 𝑚𝑒𝑚𝑏𝑒𝑟𝑠 do 6:</p><p>𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑒𝑑_𝑙𝑎𝑏𝑒𝑙𝑠 ← predict(𝑚𝑜𝑑𝑒𝑙, 𝐷 ′ ) 7:</p><p>for 𝑑 in 𝐷 ′ do 8:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑖𝑜𝑛𝑠[𝑑].𝑎𝑝𝑝𝑒𝑛𝑑(𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑒𝑑_𝑙𝑎𝑏𝑒𝑙𝑠[𝑑])</head><p>9:</p><p>for 𝑑 in 𝐷 ′ do 10:</p><formula xml:id="formula_3" coords="4,124.20,283.76,200.52,20.73">𝑎 ˆ[𝑑] ← most_frequent(𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑖𝑜𝑛𝑠[𝑑]) return (𝐷 ′ , 𝑎 ˆ)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>We conduct our experiments using the datasets released in the VQA-Med 2021 challenge. The train, validation, and test datasets respectively include 4 500, 500, and 500 images as well as question-answer pairs. This means that for each image, there exists one question-answer pair among all sets. Additionally, we exploited the train, validation, and test datasets from the VQA-Med 2019 challenge <ref type="bibr" coords="4,203.53,447.87,17.85,4.94" target="#b12">[13]</ref> to increase the amount of data available for training. Since both validation and test sets only include "what" questions asking about the type of abnormality in the image, we omit the "yes/no" questions via the simple rule of checking whether the answer is "yes" or "no" and retain only "what" questions. The final training set includes a total of 5 435<ref type="foot" coords="4,501.78,485.89,3.71,3.61" target="#foot_2">3</ref> training samples with 330 distinct answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Data setup and augmentation</head><p>In order to develop ensemble models, we combine the 5 435 train and 500 validation samples.</p><p>In each iteration, 10% of the combined data is randomly selected to serve as a validation set and the rest is used for training. Similar to the original BBN experiments <ref type="bibr" coords="4,394.55,599.46,16.18,4.94" target="#b10">[11]</ref>, we perform random resized cropping with size 224 and random horizontal flipping with probability 0.5 for data augmentation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">BBN-Orchestra setup</head><p>We evaluated three ensemble models: All backbones are trained end-to-end from scratch with the medical VQA data described above. The adaptive parameter in BBN that controls the attention between learning universal features and the long-tail class distribution is directly proportional to the epoch number <ref type="bibr" coords="5,487.25,337.36,16.33,4.94" target="#b10">[11]</ref>. Hence, we aim to set the maximum number of epochs relatively high, namely 450, in order to give BBN a better chance to model the tail distribution. <ref type="foot" coords="5,339.80,361.82,3.71,3.61" target="#foot_4">5</ref> For all BBNs, we use cross-entropy loss. Stochastic gradient descent optimization is used with momentum 0.9 and a weight decay of 0.0004. The initial learning rate is set to 0.1 decaying at the 150 th , 250 th and 300 th epochs by a factor of 0.1. All pipeline implementations are based on the PyTorch framework <ref type="bibr" coords="5,456.05,405.11,16.25,4.94" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results and Insights</head><p>The experimental results of our submissions are given in Tables <ref type="table" coords="5,364.38,454.75,4.97,4.94" target="#tab_0">1</ref> and<ref type="table" coords="5,389.73,454.75,3.66,4.94" target="#tab_1">2</ref>. We provide the accuracy and BLEU evaluation scores, as they are the official evaluation metrics in the VQA-Med 2021 challenge. The reported values for the validation case of BBN-Orchestras are the averaged performance on different random validation splits over 𝐾 models. In contrast, for single BBNs, the scores are computed using the original validation set released by the challenge.</p><p>The results show that all three orchestrated BBN models achieve better test accuracy in comparison to single BBN networks. The best performance on the test set is achieved by BBN-Orchestra 2. Comparing BBN-Orchestra 2 with a single BBN-ResNeSt 50 and BBN-Orchestra 1 with a single BBN-ResNet 34, we observe that the increase in test accuracy occurs while the validation accuracy decreases. This means that the orchestrated models were partially able to mitigate the potential overfitting with respect to the validation set. Furthermore, using the ResNeSt backbone performs better in comparison to ResNet 34. The reason for this is two-fold: 1. fewer layers in ResNet 34 leads to underfitting, 2. as shown in an empirical analysis <ref type="bibr" coords="5,469.06,617.34,16.14,4.94" target="#b14">[15]</ref>, the Split-Attention mechanism of the ResNeSt architecture improves the performance of residual networks, e.g., the mean average precision of Cascade-RCNN is improved by 3% when using ResNeSt 50 instead of ResNet 50. With the same reasoning, Orchestra 3 achieves better results in comparison to Orchestra 1, since it benefits from ResNeSt, but cannot outperform Orchestra 2, since it also uses ResNet 34 models that underfit the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This work describs the submissions of our team (TeamS) to the VQA-Med challenge at ImageCLEF 2021. Considering the simplicity of the questions in the challenge datasets, we mapped the medical VQA problem to a multi-class image classification problem and mainly utilized Bilateral-Branch Networks to effectively address the resulting long-tailed abnormality classification task.</p><p>In order to prevent potential overfitting, we developed BBN-Orchestra, an ensemble version of BBN. Our best submission exploited BBN-Orchestra with ResNeSt 50 backbone, which achieved 34.8% accuracy on the test data and ranked 3 rd in the competition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,107.37,239.27,280.35,4.94;3,103.64,252.82,383.73,4.94;3,103.64,266.37,402.35,4.94;3,116.56,279.92,389.71,4.94;3,116.56,290.83,132.02,7.58;3,88.89,316.18,160.03,9.76;3,108.38,331.13,100.35,10.63;3,208.73,329.18,5.14,6.99;3,208.73,336.20,13.71,6.99;3,222.94,331.13,186.26,9.57;3,108.38,344.68,108.81,9.97;3,96.10,360.41,329.32,9.97;3,96.10,377.37,6.29,4.06;3,124.20,373.96,66.76,9.57;3,96.10,390.92,6.29,4.06;3,124.20,387.51,107.49,9.97;3,96.10,404.47,6.29,4.06;3,140.56,401.06,274.96,9.61;3,96.10,418.02,6.29,4.06;3,140.56,414.61,106.36,9.61;3,96.10,431.57,6.29,4.06;3,140.56,428.16,217.39,9.61;3,96.10,445.12,6.29,4.06;3,140.56,441.71,152.69,9.97;3,96.10,458.67,6.29,4.06;3,156.93,455.26,298.47,9.61;3,96.10,472.22,6.29,4.06;3,156.93,468.81,293.86,9.61;3,91.93,485.76,10.45,4.06;3,156.93,482.36,139.74,9.97;3,91.93,499.31,10.45,4.06;3,173.29,495.91,99.06,9.57;3,91.93,514.56,10.45,4.06;3,140.56,511.16,187.34,9.57;3,129.66,527.43,82.67,9.61"><head>1 Algorithm 1 3 : 4 : 5 : 6 : 7 : 8 : 9 : 12 :</head><label>11345678912</label><figDesc>. a conventional network for effective representation learning, 2. a re-balancing network for modeling the tail class distribution by reverse sampling, 3. an adaptive cumulative learning component that controls how to shift the attention between the two former components during different epochs and train the classifier by minimizing the training loss. BBN Orchestra: Train Input: 𝐷 = {(𝑥 𝑖 , 𝑎 𝑖 )} 𝑛 𝑖=1 , 𝐾, 𝑏𝑎𝑐𝑘𝑏𝑜𝑛𝑒_𝑡𝑦𝑝𝑒, 𝑐𝑟𝑖𝑡𝑒𝑟𝑖𝑜𝑛, 𝑛_𝑒𝑝𝑜𝑐ℎ𝑠 Output: 𝐾 BBN models 1: procedure Orchestrate(𝐷, 𝐾, 𝑏𝑎𝑐𝑘𝑏𝑜𝑛𝑒_𝑡𝑦𝑝𝑒, 𝑐𝑟𝑖𝑡𝑒𝑟𝑖𝑜𝑛, 𝑛_𝑒𝑝𝑜𝑐ℎ𝑠) for 𝑘 in {1, ..., 𝐾} do 𝑡𝑟𝑎𝑖𝑛_𝑑𝑎𝑡𝑎, 𝑣𝑎𝑙_𝑑𝑎𝑡𝑎 ← random_split(𝐷, 𝑣𝑎𝑙_𝑠𝑖𝑧𝑒 = 0.1) 𝐶 ← num_classes(𝐷) 𝑚𝑜𝑑𝑒𝑙 ← initialize_BBN(𝐶, 𝑏𝑎𝑐𝑘𝑏𝑜𝑛𝑒_𝑡𝑦𝑝𝑒) for 𝑒𝑝𝑜𝑐ℎ in {1, ..., 𝑛_𝑒𝑝𝑜𝑐ℎ𝑠} do 𝑚𝑜𝑑𝑒𝑙 ← train_BBN(𝑚𝑜𝑑𝑒𝑙, 𝑡𝑟𝑎𝑖𝑛_𝑑𝑎𝑡𝑎, 𝑐𝑟𝑖𝑡𝑒𝑟𝑖𝑜𝑛, 𝑒𝑝𝑜𝑐ℎ) 𝑣𝑎𝑙_𝑎𝑐𝑐, 𝑣𝑎𝑙_𝑙𝑜𝑠𝑠 ← validate(𝑚𝑜𝑑𝑒𝑙, 𝑣𝑎𝑙_𝑑𝑎𝑡𝑎, 𝑐𝑟𝑖𝑡𝑒𝑟𝑖𝑜𝑛) 𝑚𝑒𝑚𝑏𝑒𝑟𝑠 ← 𝑚𝑒𝑚𝑏𝑒𝑟𝑠 ∪ {𝑏𝑒𝑠𝑡_𝑚𝑜𝑑𝑒𝑙} return 𝑚𝑒𝑚𝑏𝑒𝑟𝑠</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,103.64,240.86,279.22,9.67;5,103.64,263.31,280.31,9.57;5,103.64,285.65,401.56,9.57"><head>1. Orchestra 1 :</head><label>1</label><figDesc>𝐾 = 4 with ResNet34 [14] backbone 4 in BBNs, 2. Orchestra 2: 𝐾 = 4 with ResNeSt50 [15] backbone in BBNs, 3. Orchestra 3: 𝐾 = 8 with 4 BBNs with ResNet34 and 4 BBNs with ResNeSt50 backbone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,88.98,90.49,393.45,81.23"><head>Table 1</head><label>1</label><figDesc>Accuracy scores on validation and test sets</figDesc><table coords="5,114.48,121.99,367.96,49.72"><row><cell></cell><cell>BBN</cell><cell>BBN</cell><cell>BBN</cell><cell>BBN</cell><cell>BBN</cell></row><row><cell></cell><cell>ResNet 34</cell><cell>ResNeSt 50</cell><cell>Orchestra 1</cell><cell>Orchestra 2</cell><cell>Orchestra 3</cell></row><row><cell>Validation</cell><cell>59.8%</cell><cell>61.3%</cell><cell>54.4%</cell><cell>57.7%</cell><cell>55.9%</cell></row><row><cell>Test</cell><cell>29.9%</cell><cell>30.4%</cell><cell>32.2%</cell><cell>34.8%</cell><cell>32.8%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,88.99,90.49,393.44,79.01"><head>Table 2</head><label>2</label><figDesc>BLEU scores on validation and test sets</figDesc><table coords="6,114.48,119.77,367.96,49.72"><row><cell></cell><cell>BBN</cell><cell>BBN</cell><cell>BBN</cell><cell>BBN</cell><cell>BBN</cell></row><row><cell></cell><cell>ResNet 34</cell><cell>ResNeSt 50</cell><cell>Orchestra 1</cell><cell>Orchestra 2</cell><cell>Orchestra 3</cell></row><row><cell>Validation</cell><cell>63.1%</cell><cell>64.6%</cell><cell>57.3%</cell><cell>61.9%</cell><cell>59.3%</cell></row><row><cell>Test</cell><cell>33.2%</cell><cell>33.8%</cell><cell>35.8%</cell><cell>39.1%</cell><cell>36.6%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,108.93,662.76,297.78,4.06"><p>For further information on BBNs, the reader is referred to the original paper<ref type="bibr" coords="3,390.02,662.76,13.36,4.06" target="#b10">[11]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,108.93,673.71,291.79,4.06"><p>Since the training and validation sets are independent and have no intersection.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,108.93,673.66,313.00,4.06"><p>Including VQA-Med 2019. Before mixing with the validation set from VQA-Med 2021.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,108.93,651.80,197.74,4.06"><p>Both conventional learning and rebalancing branches.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="5,108.93,662.76,397.06,4.06;5,89.29,673.72,64.40,4.06"><p>In early epochs, BBN focuses on learning the universal features and in later epochs, it learns to model the tail class distribution.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>We would like to thank <rs type="person">Matthias Steinbrecher</rs> for his helpful comments and discussions.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="6,112.66,496.70,393.32,4.94;6,112.39,510.25,369.31,4.94" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,360.86,496.70,145.12,4.94;6,112.39,510.25,234.39,4.94">A dataset of clinically generated visual questions and answers about radiology images</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gayen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,355.50,510.25,62.55,4.94">Scientific data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,523.80,393.33,4.94;6,112.66,537.35,393.33,4.94;6,112.66,550.90,107.17,4.94" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,363.02,523.80,142.96,4.94;6,112.66,537.35,312.73,4.94">SLAKE: A semantically-labeled knowledge-enhanced dataset for medical visual question answering</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-M</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09542</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,112.66,564.45,393.73,4.94;6,112.66,578.00,393.33,4.94;6,112.66,591.55,394.53,4.94;6,112.66,605.10,184.68,4.94" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,461.99,564.45,44.40,4.94;6,112.66,578.00,393.33,4.94;6,112.66,591.55,100.38,4.94">Overview of the VQA-Med task at ImageCLEF 2021: Visual question answering and generation in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,241.03,591.55,119.21,4.94">CLEF 2021 Working Notes</title>
		<title level="s" coord="6,368.60,591.55,138.59,4.94;6,112.66,605.10,38.13,4.94">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,618.65,393.33,4.94;6,112.66,632.20,395.01,4.94" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,313.61,618.65,192.38,4.94;6,112.66,632.20,129.15,4.94">A question-centric model for visual question answering in medical imaging</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Löfstedt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Nyholm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sznitman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,250.26,632.20,165.42,4.94">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2856" to="2868" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,645.75,393.33,4.94;6,112.66,659.29,393.33,4.94;6,112.66,672.84,147.11,4.94" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,329.96,645.75,176.02,4.94;6,112.66,659.29,97.04,4.94">Medical visual question answering via conditional reasoning</title>
		<author>
			<persName coords=""><forename type="first">L.-M</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,236.97,659.29,269.01,4.94;6,112.66,672.84,48.78,4.94">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2345" to="2354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,90.23,393.33,4.94;7,112.66,103.78,393.32,4.94;7,112.66,117.33,374.36,4.94" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,427.46,90.23,78.53,4.94;7,112.66,103.78,207.53,4.94">Overcoming data limitation in medical visual question answering</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">X</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Tjiputra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,343.52,103.78,162.47,4.94;7,112.66,117.33,243.84,4.94">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="522" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,130.88,393.33,4.94;7,112.66,144.43,393.33,4.94;7,112.66,157.97,145.48,4.94" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="7,347.61,130.88,158.38,4.94;7,112.66,144.43,393.33,4.94;7,112.66,157.97,84.94,4.94">AIML at VQA-Med 2020: Knowledge inference via a skeleton-based sentence mapping approach for medical domain visual question answering</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Verjans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
		<respStmt>
			<orgName>CLEF</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,171.52,394.53,4.94;7,112.34,185.07,393.64,4.94;7,112.41,198.62,359.72,4.94" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,239.94,185.07,158.38,4.94">Towards visual dialog for radiology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Shivade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kanjaria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ballah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Coy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karargyris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">B</forename><surname>Beymer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,421.75,185.07,84.23,4.94;7,112.41,198.62,281.99,4.94">Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing</title>
		<meeting>the 19th SIGBioMed Workshop on Biomedical Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="60" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,212.17,395.01,4.94;7,112.66,225.72,394.53,4.94;7,112.48,239.27,395.18,4.94;7,112.66,252.82,394.53,4.94;7,112.28,266.37,393.70,4.94;7,112.66,279.92,393.33,4.94;7,112.66,293.47,393.33,4.94;7,112.66,307.02,393.53,4.94;7,112.66,320.56,197.61,4.94" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,264.32,266.37,241.66,4.94;7,112.66,279.92,261.75,4.94">Overview of the ImageCLEF 2021: Multimedia retrieval in medical, nature, internet and social media applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tauteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Moustahfid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,401.27,279.92,104.72,4.94;7,112.66,293.47,393.33,4.94;7,112.66,307.02,201.15,4.94">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 12th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="7,348.61,307.02,157.57,4.94;7,112.66,320.56,31.10,4.94">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,334.11,393.54,4.94;7,112.66,347.66,211.81,4.94" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="7,224.25,334.11,281.95,4.94;7,112.66,347.66,151.27,4.94">HCP-MIC at VQA-Med 2020: Effective visual representation for medical visual question answering</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,361.21,393.32,4.94;7,112.66,374.76,393.33,4.94;7,112.66,388.31,297.29,4.94" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,290.99,361.21,214.99,4.94;7,112.66,374.76,185.56,4.94">BBN: Bilateral-branch network with cumulative learning for long-tailed visual recognition</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z.-M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,321.36,374.76,184.62,4.94;7,112.66,388.31,199.18,4.94">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9719" to="9728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,401.86,393.33,4.94;7,112.66,415.41,217.56,4.94" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Qian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00781</idno>
		<title level="m" coord="7,202.29,401.86,303.70,4.94;7,112.66,415.41,34.86,4.94">Recommending accurate and diverse items using bilateral branch network</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,112.66,428.96,394.61,4.94;7,112.66,442.51,393.58,4.94;7,112.33,456.06,101.72,4.94" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,459.84,428.96,47.43,4.94;7,112.66,442.51,341.15,4.94">VQA-Med: Overview of the medical visual question answering task at ImageCLEF 2019</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,481.58,442.51,24.66,4.94;7,112.33,456.06,71.82,4.94">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,469.61,393.33,4.94;7,112.66,483.16,393.33,4.94;7,112.66,496.70,164.50,4.94" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="7,269.62,469.61,236.37,4.94;7,112.66,483.16,95.55,4.94">A deep residual networks classification algorithm of fetal heart CT images</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,256.55,483.16,249.44,4.94;7,112.66,496.70,70.90,4.94">IEEE international conference on imaging systems and techniques (IST)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,510.25,394.53,4.94;7,112.66,523.80,356.38,4.94" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m" coord="7,140.30,523.80,146.12,4.94">Resnest: Split-attention networks</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,112.66,537.35,394.53,4.94;7,112.66,550.90,394.52,4.94;7,112.66,564.45,394.53,4.94;7,112.66,578.00,394.53,4.94;7,112.66,591.55,393.32,4.94;7,112.66,605.10,252.86,4.94" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="7,371.36,564.45,135.83,4.94;7,112.66,578.00,178.35,4.94">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="7,303.80,591.55,202.18,4.94;7,112.66,605.10,36.36,4.94">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
			<date type="published" when="2019">2019</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
