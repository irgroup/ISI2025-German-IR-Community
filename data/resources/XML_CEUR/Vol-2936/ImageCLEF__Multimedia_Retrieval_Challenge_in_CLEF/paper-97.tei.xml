<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,407.52,15.42;1,89.29,106.66,339.42,15.42;1,89.29,128.58,323.32,15.43">Chabbiimen at VQA-Med 2021: Visual Generation of Relevant Natural Language Questions from Radiology Images for Anomaly Detection</title>
				<funder ref="#_q9xgjuV">
					<orgName type="full">Tunisian Ministry of Higher Education and Scientific Research</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,89.29,156.89,62.33,11.96"><forename type="first">Imen</forename><surname>Chebbi</surname></persName>
							<email>chabbiimen@yahoo.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Higher Institute of Informatics and Communication Technologies (ISITCOM)</orgName>
								<address>
									<settlement>Hammam Sousse</settlement>
									<country key="TN">Tunisia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,407.52,15.42;1,89.29,106.66,339.42,15.42;1,89.29,128.58,323.32,15.43">Chabbiimen at VQA-Med 2021: Visual Generation of Relevant Natural Language Questions from Radiology Images for Anomaly Detection</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">57E21CD9352232F9665B15D49AA14036</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visual Question Generation</term>
					<term>Natural Language</term>
					<term>Anomaly Detection</term>
					<term>Radiology Images</term>
					<term>Medical Questions</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The new progress in the domain of artificial intelligence has created recent opportunity in support clinical decision. In exceptional, appropriate solutions for the automated analysis of medical pictures are increasing interest expected to their applications potential in picture research and in the diagnosis assisted. In addition, systems able to understand clinical pictures and answer the questions about its content can assist objective decision making, objective education. In my article, I will describe my technique for produce visual questions on radiology images. I have used augmentation techniques for increasing data and VGG19 for extraction of the feature from a picture and prediction. VQGR is implemented using Tensorflow. My model is quite similar to GRNN [1].</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Nowadays, the task of Visual Question Generation becomes the very important task in the medical domain, which seek to produce human-like questions from the picture and potentially other side information (e.g. answer type or answer itself). The last few years have seen a surge of interests in VQG because it is particularly useful for providing high-quality synthetic training data for Visual Dialog system and Visual Question Answering (VQA) <ref type="bibr" coords="1,411.33,464.64,11.58,10.91" target="#b1">[2,</ref><ref type="bibr" coords="1,422.91,464.64,7.72,10.91" target="#b2">3]</ref>. It seems like a challenging task because the generated questions are not only required to be consistent with the image content but also meaningful and answerable to human. Even with promising results have been achieved, previous works still encounter two major issues. First, all of existing methods significantly suffer from one image to many questions mapping problem rendering the failure of generating referential and meaningful questions from an image. Extant VQG methods can be generally categorized into three classes with respect to what hints are used for generating visual questions:</p><p>1) The whole image as the only context input <ref type="bibr" coords="1,293.56,573.04,11.43,10.91" target="#b3">[4]</ref>.</p><p>2) All images and required answers <ref type="bibr" coords="1,248.89,586.58,11.43,10.91" target="#b4">[5]</ref>.</p><p>3) The whole image with the desired answer types <ref type="bibr" coords="1,316.33,600.13,11.43,10.91" target="#b5">[6]</ref>.</p><p>The picture is worth a thousand words, and it can be potentially mapped to many different questions, leading to the generation of diverse non-informative questions with poor quality.</p><p>Even with the answer type or desired answer information, the similar one-to-many mapping issue remains, partially because the answer hints are often very short or too broad. As a consequence, these side informations are often not informative enough for guiding question generation process, rendering the failure of generating referential and meaningful questions from an image. The second serious issue for the existing VQG methods is that they ignore the rich correlations between the visual objects in an image and potential interactions between the side information and image <ref type="bibr" coords="2,140.34,208.91,11.66,10.91" target="#b5">[6]</ref>. In the concept, the implicit relations among the visual objects (e.g., spatial, semantic) could be the key to generate meaningful and relevant questions. This is partially since when human annotators ask questions about a given image, they often focus on these kinds of interactions. Additionally, another important factor for producing informative and referential questions is about how to make entire use of side information to align with the targeted image. Such modeling potential interactions between the side information and an image becomes a critical component for generating referential and meaningful questions. The mission is to generate relevant questions in natural language on the radiological images using their visual content. In my article, I will describe my method for produce visual questions on radiology images. I have used augmentation techniques for the augmentation of data and VGG19 convolutional neural network for image feature extraction and prediction. The structure of this article is arranged as follows: The part 1 and part 2 contain introduction and surveys work related, part 3 Methods, part 4 The Results Experimental, part 5 and part 6 contain discussion of my results, the Conclusion and my Future Work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Question generation, an increasingly important area, is the task of automatically creating natural language questions from a range of inputs, such as natural language text <ref type="bibr" coords="2,412.49,457.22,7.93,10.91" target="#b6">[7]</ref><ref type="bibr" coords="2,420.42,457.22,3.96,10.91" target="#b7">[8]</ref><ref type="bibr" coords="2,424.39,457.22,7.93,10.91" target="#b8">[9]</ref>, structured data <ref type="bibr" coords="2,89.29,470.77,18.25,10.91" target="#b9">[10]</ref> and images <ref type="bibr" coords="2,164.22,470.77,16.56,10.91" target="#b10">[11]</ref>. Concerning VQG in the medical sector, Sarrouti et al. <ref type="bibr" coords="2,432.34,470.77,18.25,10.91" target="#b11">[12]</ref> suggested a technique for the task of VQG on radiological pictures named VQGR. His proposition was based on VAE. For augmenting the amount of the dataset, this author <ref type="bibr" coords="2,376.62,497.87,18.20,10.91" target="#b11">[12]</ref> used data augmentation for both pictures and questions <ref type="bibr" coords="2,238.94,511.42,16.58,10.91">[60]</ref>. In this work, I'm interested for the task of produce questions from medical images. VQG in the open-domain benefited from the available large annotated datasets <ref type="bibr" coords="2,172.61,538.52,12.55,10.91" target="#b12">[13]</ref><ref type="bibr" coords="2,185.15,538.52,4.18,10.91" target="#b13">[14]</ref><ref type="bibr" coords="2,189.34,538.52,12.55,10.91" target="#b14">[15]</ref>. There is a variety of work studying generative models for generating visual questions in the open domain <ref type="bibr" coords="2,251.18,552.07,12.66,10.91" target="#b15">[16]</ref><ref type="bibr" coords="2,268.06,552.07,12.66,10.91" target="#b16">[17]</ref>. Recent VQG approaches have used auto encoders architecture for the purposes of VQG <ref type="bibr" coords="2,266.04,565.62,13.06,10.91" target="#b17">[18]</ref><ref type="bibr" coords="2,279.10,565.62,4.35,10.91" target="#b18">[19]</ref><ref type="bibr" coords="2,283.45,565.62,13.06,10.91" target="#b19">[20]</ref>. The success of these systems is mainly the result of variational auto encoders (VAEs) <ref type="bibr" coords="2,274.31,579.17,17.73,10.91" target="#b20">[21]</ref>.Conversely, VQG in the medical domain is still a challenging and under-explored task <ref type="bibr" coords="2,255.22,592.72,12.80,10.91" target="#b21">[22]</ref><ref type="bibr" coords="2,268.02,592.72,4.27,10.91" target="#b22">[23]</ref><ref type="bibr" coords="2,272.29,592.72,12.80,10.91" target="#b23">[24]</ref>. Despite the fact that a high-quality manually created medical VQA dataset exists, VQA-RAD <ref type="bibr" coords="2,89.29,619.81,16.58,10.91" target="#b22">[23]</ref>, this dataset is too small for training and there is a need for VQG approaches to create training datasets of sufficient size. Generating new training data from the existing examples through data augmentation is an effective approach that has been widely used to manipulate the data poverty problem in the open domain <ref type="bibr" coords="2,278.50,660.46,16.54,10.91" target="#b24">[25,</ref><ref type="bibr" coords="2,295.05,660.46,12.41,10.91" target="#b25">26]</ref>. In my paper, I will describe my method for produce visual questions on radiology pictures named VQGR. I have used VGG19 for extraction feature from image and for prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>In this study, the goal is to generate natural language questions based on radiology image contents. The overview of VQGR is shown in Figure number three.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Technique Of Data Augmentation</head><p>Technique Of Data Augmentation is a method utilized for augmenting the size of data by adding on moderately adapted duplicate of already available data or recently generated artificial data from available data. It worked as a regularize and assist to lower over fitting when training a model of machine learning. In data analysis, data augmentation is nearly associated with oversampling. I have used this technique for augmenting the dataset of images and questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Questions.</head><p>I generated new training examples by using question augmentation technology. I create a dataset of new questions for medical question q. For my approach, during the augmenting process, I have use all the Dataset VQA-Med 2021/ VQA-Med 2020/ VQA-Med 2019/ VQA-Med 2018. For instance, for a given question "Are the kidneys normal?", we produce the followings questions:</p><p>-"Were the kidneys normal?" -"Are the pancreas normal? -"Are the intestines normal?" -"Are the isografted normal?" -"Are the livers normal? " -"Are the lungs normal?" -"Are the organs normal?" -etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Images.</head><p>I have also created a new learning instance based on image enhancement technology. For this reason, in my approach I have applied all Datasets VQA-Med 2021 / VQA-Med 2020 / VQA-Med 2019/ VQA-Med 2018 educational image inversion, rotation, movement and blur techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Visual Question Generation</head><p>To move work back founded on the representation exact of the figure, a technique for asking natural questions, from a picture, was created in 2016 <ref type="bibr" coords="3,338.14,628.24,11.66,10.91" target="#b0">[1]</ref>. This task of VQG is, therefore, a supplement of available VQA systems, to understand the method image questions might relate to inference and common sense reasoning. This system which has been developed presents models based on generation and retrieval to deal with the mission of generating questions on visual pictures. A second application of the produce of Visual Question related with the identification of objects in a picture <ref type="bibr" coords="4,256.91,100.52,16.58,10.91" target="#b26">[27]</ref>. Because, is impossible to prepare picture models recognition with all feasible objects in the earth, the generation of visual questions founded on mysterious entity can be employed for class acquisition. Such class acquisition operation is accomplished by answering humans questions based on mysterious objects for every picture. I will present in this paper my approach to produce visual questions on radiology images quite similar to GRNN <ref type="bibr" coords="4,166.25,168.26,11.43,10.91" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">VGG-19 Model.</head><p>From Oxford University Simonyan and Zisserman developed a 19-layer CNN ( 3 fully connected, 16 conv.) that exclusively used 3 * 3 filters along pad and stride of 1, as well as 2 * 2 layers max pooling along stride 2, named model VGG19. If I compare VGG19 to AlexNet, the VGG19 (As seen in Figure <ref type="figure" coords="4,154.90,258.24,4.18,10.91" target="#fig_0">1</ref>) is a profound CNN along more layers. For lowing the parameters number in these deep networks, it utilize little 3 * 3 filters in every convolutional layer and is supreme used along its error rate of 7.3%. This model was not the champion in the ILSVRC 2014, nevertheless, the VGG Net is the most powerful articles since it strengthened the idea that CNNs continue to get a deep network of layers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Proposed Architecture.</head><p>My architecture contains two main parts (see Figure <ref type="figure" coords="4,322.57,591.34,3.54,10.91" target="#fig_1">2</ref>), namely the preprocessing unit and the question visual generation (VQG) part.The module of preprocessing needs picture processing techniques to train picture and transform it to an easier to use format. For the mining of the image features, I have utilize the convolutional neural network VGG19 <ref type="bibr" coords="4,430.79,631.99,16.58,10.91" target="#b9">[10]</ref>. In addition, preprocessing of annotations and questions is performed to produce vocabulary. The VQG unit will get the features visual that will study how to produce a question integration. Similar visual characteristics are displayed in the model for all stride lengths, generating one question word at a time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Proposed Model.</head><p>Visual question generation is important task has currently been studied as a supplement of image caption. VQG techniques are utilized to produce questions that are dissimilar from image caption techniques that explain a picture in a some words (Eg, Are the organs normal? ). The methods proposed in this article use a data augmentation and VGG19 (As seen in Figure <ref type="figure" coords="5,498.15,357.83,4.25,10.91" target="#fig_2">3</ref>) , so the module of question generation is the node of my approach, which employs a data augmentation and a VGG19 model which takes a picture as input and produce questions from its characteristics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this study, I have used the dataset <ref type="bibr" coords="5,249.80,624.31,17.33,10.91" target="#b30">[31]</ref>:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>For training set I have explored the existing datasets used in VQG-Med 2020, VQG-Med 2019 and VQG-Med 2018 <ref type="bibr" coords="6,160.58,121.08,16.43,10.91" target="#b30">[31]</ref>. For validation set I have used the VQG 2021 validation <ref type="bibr" coords="6,431.66,121.08,18.11,10.91" target="#b30">[31]</ref> set contains 200 questions associated with 85 radiology images and for Test Set the VQG test <ref type="bibr" coords="6,436.08,134.63,16.63,10.91" target="#b30">[31]</ref> set includes 76 radiology images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>For evaluating efficiency of VQGR approach, I will utilize bilingual evaluation understudy BLEU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">BLEU score</head><p>It captures the resemblance allying the response generated by the model and ground truth response. BLUE scores are between zero and one. Scores closer to the value of one indicate more resemblance allying the answer and ground truth answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results and Discussion</head><p>The table number 1 represents all my runs submitted in the Task VQG ImageCLEF2021 <ref type="bibr" coords="6,463.58,324.06,20.03,10.91" target="#b31">[32]</ref>. My best score of Bleu metric from Aicrowd system for ImageCLEF2021 <ref type="bibr" coords="6,394.94,337.61,18.20,10.91" target="#b31">[32]</ref> is 0.383. My score is the best one in Task VQG for ImageCLEF2021 <ref type="bibr" coords="6,295.78,351.16,16.25,10.91" target="#b31">[32]</ref>. In first run number 133658 I get score 0.383 in this run I have used 2300 questions and I have generate seven questions for each image. In second run number 133896 I have used the same database of questions but I have generate three questions for each picture and I have get 0.212 score. In the third run number 134438 I have used 3,200 questions but I have generate seven questions. The fourth and fifth runs represents the same runs with run number 134438. I have noticed that in all my runs for every dataset of question used I get score 3.383 but when I change the number of predictions my score change.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>For every result of my model in Figure <ref type="figure" coords="7,264.77,504.11,3.76,10.91" target="#fig_2">3</ref>, I can view that for one image I can get one to seven questions in output. More interestingly, I have presented a model to generate visual questions in the medical domain. The results of automatic and manual assessment shows that VQGR surpass the baseline model for generating fluency and related questions. In this work, I have presented the initial trial to generate visual questions in the medical domain by using VGG19.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>I have presented in this article my method for generating visual questions in the medical domain by using VGG19. I showed a data expansion method for generating new training questions and images from the VQA-RAD dataset <ref type="bibr" coords="7,246.99,644.04,16.15,10.91" target="#b30">[31]</ref>. Next, I introduced the VQGR model, which generates questions from radiographs. Automated and manual evaluation results showed that the VQGR fluency-related questions outperformed the baseline model. I will develop this model in the future and investigate the use of produced questions intended to process VQA in healthcare.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,525.91,241.04,8.93;4,154.19,335.15,284.40,178.20"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The architecture network of VGG-19 model [28].</figDesc><graphic coords="4,154.19,335.15,284.40,178.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,89.29,249.31,137.81,8.93;5,168.83,120.78,255.12,115.97"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Complete Architecture.</figDesc><graphic coords="5,168.83,120.78,255.12,115.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,89.29,547.13,140.20,8.93;5,150.55,418.74,291.69,115.83"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Proposed Model VQGR.</figDesc><graphic coords="5,150.55,418.74,291.69,115.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,89.29,599.51,416.69,10.91;6,89.29,613.06,163.87,10.91"><head>Figure 4</head><label>4</label><figDesc>Figure4represents examples of some images with natural questions and automatically generated questions by using my model VQGR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,89.29,442.20,406.54,8.93;7,150.55,84.19,291.68,345.45"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Examples of some images with natural questions and automatically generated questions.</figDesc><graphic coords="7,150.55,84.19,291.68,345.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,88.98,475.01,240.14,105.74"><head>Table 1</head><label>1</label><figDesc>All my runs in ImageCLEF2021 Task VQG<ref type="bibr" coords="6,253.80,487.02,19.56,8.87" target="#b31">[32]</ref>.</figDesc><table coords="6,266.16,506.63,62.96,74.12"><row><cell>Run</cell><cell>Score</cell></row><row><cell cols="2">133658 0.383</cell></row><row><cell cols="2">133896 0.212</cell></row><row><cell cols="2">134438 0.383</cell></row><row><cell cols="2">134446 0.383</cell></row><row><cell cols="2">134449 0.383</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7.">Acknowledgments</head><p>This research has received funding according to the funding agreement number <rs type="grantNumber">LR11ES48</rs> of the <rs type="funder">Tunisian Ministry of Higher Education and Scientific Research</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_q9xgjuV">
					<idno type="grant-number">LR11ES48</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,112.66,231.32,393.33,10.91;8,112.66,244.87,324.21,10.91" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="middle">N</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">L</forename><surname>Vanderwende</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06059</idno>
		<title level="m" coord="8,455.53,231.32,50.45,10.91;8,112.66,244.87,142.77,10.91">Generating natural questions about animage</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,112.66,258.42,393.32,10.91;8,112.33,271.96,393.65,10.91;8,112.66,285.51,197.86,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,344.33,258.42,161.65,10.91;8,112.33,271.96,150.79,10.91">Visual Question Generation as Dual Task of Visual Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Duan</forename><forename type="middle">Y N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chu</forename><forename type="middle">B X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ouyang</forename><forename type="middle">W X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,268.07,271.96,237.91,10.91;8,112.66,285.51,86.75,10.91">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6116" to="6124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,299.06,395.17,10.91;8,112.66,312.61,393.33,10.91;8,112.66,326.16,257.21,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,270.98,299.06,236.85,10.91;8,112.66,312.61,193.01,10.91">Two can play this game: Visual dialog with discriminative question generation and answering</title>
		<author>
			<persName coords=""><forename type="middle">U</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">A G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,326.18,312.61,179.81,10.91;8,112.66,326.16,220.26,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,339.71,393.32,10.91;8,112.66,353.26,187.35,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,316.53,339.71,189.45,10.91;8,112.66,353.26,116.82,10.91">Towards automatic generation of question answer pairs from images</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">I M P</forename><surname>Mora</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>De La Puente</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">X</forename><surname>Giro-I Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,234.99,353.26,27.58,10.91">CVPRW</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,366.81,393.33,10.91;8,112.39,380.36,379.35,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,362.30,366.81,143.69,10.91;8,112.39,380.36,234.39,10.91">A dataset of clinically generated visual questions and answers about radiology images</title>
		<author>
			<persName coords=""><forename type="middle">J J</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">S</forename><surname>Gayen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,354.30,380.36,64.72,10.91">Scientific data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,393.91,395.01,10.91;8,112.66,407.46,394.53,10.91;8,112.66,421.01,103.71,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,273.56,393.91,229.47,10.91">Information maximizing visual question generation</title>
		<author>
			<persName coords=""><forename type="first">Krishna</forename><forename type="middle">R M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,125.50,407.46,376.92,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2008" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,434.55,393.33,10.91;8,112.66,448.10,394.53,10.91;8,112.39,461.65,127.00,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,251.73,434.55,254.26,10.91;8,112.66,448.10,41.59,10.91">Natural language question generation using syntax and keywords</title>
		<author>
			<persName coords=""><forename type="middle">S</forename><surname>Kalady</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">A</forename><surname>Elikkottil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">R</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,176.34,448.10,325.99,10.91">Proceedings of QG2010: The Third Workshop on Question Generation</title>
		<meeting>QG2010: The Third Workshop on Question Generation</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="5" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,475.20,395.17,10.91;8,112.66,488.75,393.33,10.91;8,112.66,502.30,78.48,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,242.12,475.20,265.70,10.91;8,112.66,488.75,16.22,10.91">Improving neural question generation using answer separation</title>
		<author>
			<persName coords=""><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">K</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,147.54,488.75,275.11,10.91">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6602" to="6609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,515.85,393.32,10.91;8,112.66,529.40,393.33,10.91;8,112.66,542.95,393.33,10.91;8,112.66,556.50,206.14,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,314.30,515.85,191.68,10.91;8,112.66,529.40,58.55,10.91">Improving question generation with to the point context</title>
		<author>
			<persName coords=""><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">L</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">L</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">R</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,190.73,529.40,315.26,10.91;8,112.66,542.95,393.33,10.91;8,112.66,556.50,45.66,10.91">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3216" to="3226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,570.05,274.87,10.91" xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="middle">I</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">G</forename><surname>Alberto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sarath</forename><forename type="middle">C</forename><surname>Caglargulcehre</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Aaron</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,583.60,393.33,10.91;8,112.66,597.15,393.32,10.91;8,112.66,610.69,395.01,10.91;8,112.28,624.24,220.14,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,423.64,583.60,82.35,10.91;8,112.66,597.15,110.92,10.91">Generating natural questions about an image</title>
		<author>
			<persName coords=""><forename type="middle">N</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">L</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,243.05,597.15,262.94,10.91;8,112.66,610.69,130.88,10.91;8,289.04,610.69,62.17,10.91">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1802" to="1813" />
		</imprint>
	</monogr>
	<note>: Long Papers)</note>
</biblStruct>

<biblStruct coords="8,112.66,637.79,395.17,10.91;8,112.66,651.34,393.33,10.91;8,112.66,664.89,119.56,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,333.97,637.79,173.86,10.91;8,112.66,651.34,48.15,10.91">Visual question generation from radiology images</title>
		<author>
			<persName coords=""><forename type="middle">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,183.27,651.34,322.71,10.91;8,112.66,664.89,37.60,10.91">Proceedings of the First Workshop on Advances in Language and Vision Research</title>
		<meeting>the First Workshop on Advances in Language and Vision Research</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="12" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,86.97,393.33,10.91;9,112.66,100.52,394.52,10.91;9,112.66,114.06,103.71,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,477.44,86.97,28.54,10.91;9,112.66,100.52,89.36,10.91">Visual Question Answering</title>
		<author>
			<persName coords=""><forename type="middle">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">C L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Vqa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,209.88,100.52,292.54,10.91">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,127.61,393.71,10.91;9,112.66,141.16,395.00,10.91;9,112.66,154.71,172.71,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,410.29,127.61,96.08,10.91;9,112.66,141.16,362.26,10.91">Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering</title>
		<author>
			<persName coords=""><forename type="middle">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,479.73,141.16,27.94,10.91;9,112.66,154.71,67.84,10.91">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="398" to="414" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,168.26,393.71,10.91;9,112.66,181.81,393.58,10.91;9,112.66,195.36,393.57,10.91;9,112.33,208.91,29.19,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,498.63,168.26,7.73,10.91;9,112.66,181.81,356.13,10.91">A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName coords=""><forename type="first">Johnson</forename><forename type="middle">J B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">L V</forename><surname>Dermaaten</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">C L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Clevr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,476.23,181.81,30.00,10.91;9,112.66,195.36,313.65,10.91">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1988" to="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,222.46,393.32,10.91;9,112.66,236.01,394.53,10.91;9,112.66,249.56,154.16,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,359.65,222.46,146.34,10.91;9,112.66,236.01,153.55,10.91">Towards automatic generation of question answer pairs from images</title>
		<author>
			<persName coords=""><forename type="middle">I</forename><surname>Masuda-Mora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">S</forename><surname>Pascual-Delapuente</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gir O I Nietxx</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,285.62,236.01,216.20,10.91">Visual Question Answering Challenge Workshop</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,263.11,395.17,10.91;9,112.66,276.66,393.33,10.91;9,112.66,290.20,211.23,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="9,296.18,263.11,211.65,10.91;9,112.66,276.66,20.80,10.91">Automatic generation of grounded visual questions</title>
		<author>
			<persName coords=""><forename type="middle">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,153.64,276.66,352.35,10.91;9,112.66,290.20,97.76,10.91">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI-17)</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI-17)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4235" to="4243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,303.75,393.33,10.91;9,112.66,317.30,393.33,10.91;9,112.66,330.85,395.17,10.91;9,112.66,344.40,126.36,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="9,246.38,303.75,259.60,10.91;9,112.66,317.30,61.17,10.91">Creativity: Generating diverse questions using variational auto encoders</title>
		<author>
			<persName coords=""><forename type="middle">U</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">A</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,193.63,317.30,312.35,10.91;9,112.66,330.85,106.30,10.91">Proceedings -30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017</title>
		<meeting>-30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017</meeting>
		<imprint>
			<publisher>United States. Institute of Electrical and Electronics Engineers Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5415" to="5424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,357.95,393.33,10.91;9,112.33,371.50,393.65,10.91;9,112.66,385.05,200.58,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="9,346.16,357.95,159.83,10.91;9,112.33,371.50,150.79,10.91">Visual Question Generation as Dual Task of Visual Question Answering</title>
		<author>
			<persName coords=""><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,268.07,371.50,237.91,10.91;9,112.66,385.05,86.75,10.91">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6116" to="6124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,398.60,395.01,10.91;9,112.66,412.15,394.53,10.91;9,112.66,425.70,103.71,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="9,273.56,398.60,229.47,10.91">Information maximizing visual question generation</title>
		<author>
			<persName coords=""><forename type="first">Krishna</forename><forename type="middle">R M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,125.50,412.15,376.92,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2008" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,439.25,393.58,10.91;9,112.33,452.79,29.19,10.91" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="9,224.17,439.25,143.97,10.91">Auto-encoding variational bayes</title>
		<author>
			<persName coords=""><forename type="middle">D P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,466.34,393.33,10.91;9,112.66,479.89,395.16,10.91;9,112.66,493.44,393.32,10.91;9,112.66,506.99,241.02,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="9,381.66,466.34,124.33,10.91;9,112.66,479.89,206.36,10.91">Overview of image clef 2018 medical domain visual question answering task</title>
		<author>
			<persName coords=""><forename type="first">Ling</forename><forename type="middle">S A Y</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lungre</forename><forename type="middle">M</forename><surname>Hr</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Pn</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="9,337.58,479.89,170.24,10.91;9,112.66,493.44,171.50,10.91">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="9,124.27,506.99,161.17,10.91">CEUR Workshop Proceedings.CEUR</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14. 2125. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,520.54,393.32,10.91;9,112.39,534.09,360.28,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="9,360.30,520.54,145.68,10.91;9,112.39,534.09,234.39,10.91">A dataset of clinically generated visual questions and answers about radiology images</title>
		<author>
			<persName coords=""><forename type="first">Gayen</forename><forename type="middle">J J S</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,354.30,534.09,62.39,10.91">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,547.64,394.62,10.91;9,112.66,561.19,393.33,10.91;9,112.66,574.74,394.52,10.91;9,112.66,588.29,394.83,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="9,112.66,561.19,318.01,10.91">Overview of the medical visual question answering task at image clef</title>
		<author>
			<persName coords=""><forename type="first">.</forename><forename type="middle">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">S A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">V V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">H</forename><surname>Uller</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Vqa-Med</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="9,459.54,561.19,46.45,10.91;9,112.66,574.74,294.32,10.91">InWorking Notes of CLEF 2019 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="9,279.47,588.29,157.35,10.91">CEUR Workshop Proceedings. CEUR</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09-09">2019. September 9-12, 2019. 2019</date>
			<biblScope unit="volume">2380</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,601.84,393.53,10.91;9,112.66,615.39,393.32,10.91;9,112.66,628.93,355.55,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="9,254.18,601.84,252.01,10.91;9,112.66,615.39,102.76,10.91">Data augmentation via dependency tree morphing for low-resource languages</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">L</forename><surname>Srk Steedmam.N</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,234.66,615.39,271.32,10.91;9,112.66,628.93,128.11,10.91">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,642.48,393.33,10.91;9,112.66,656.03,393.33,10.91;9,112.28,669.58,393.71,10.91;10,112.33,86.97,288.48,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="9,171.92,642.48,334.07,10.91;9,112.66,656.03,37.77,10.91">Contextual augmentation: Data augmentation by words with paradigmatic relations</title>
		<author>
			<persName coords=""><forename type="middle">S</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,169.42,656.03,336.57,10.91;9,112.28,669.58,342.45,10.91">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct coords="10,112.66,100.52,393.33,10.91;10,112.66,114.06,393.33,10.91;10,112.66,127.61,207.97,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="10,347.56,100.52,158.43,10.91;10,112.66,114.06,141.36,10.91">Visual question generation for class acquisition of unknown objects</title>
		<author>
			<persName coords=""><forename type="middle">K</forename><surname>Uehara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">A</forename><surname>Tejero-De-Pablos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,276.91,114.06,187.34,10.91">Computer Vision ECCV 2018. ECCV 2018</title>
		<title level="s" coord="10,471.85,114.06,34.14,10.91;10,112.66,127.61,116.12,10.91">Lecture Notesin Computer Science</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="492" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,141.16,395.17,10.91;10,112.66,154.71,395.01,10.91;10,112.66,168.26,137.45,10.91" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="10,270.64,141.16,237.19,10.91;10,112.66,154.71,233.45,10.91">Breast cancer screening using convolutional neural network and follow-up digital mammography</title>
		<author>
			<persName coords=""><forename type="middle">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">A</forename><surname>Merkulov</surname></persName>
		</author>
		<idno type="DOI">10.1117/12.2304564</idno>
	</analytic>
	<monogr>
		<title level="j" coord="10,378.62,154.71,125.95,10.91">Computational Imaging III</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,181.81,393.33,10.91;10,112.66,195.36,394.52,10.91;10,112.66,208.91,97.22,10.91" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="10,284.79,181.81,221.20,10.91;10,112.66,195.36,144.28,10.91">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</title>
		<author>
			<persName coords=""><forename type="middle">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sun</forename><forename type="middle">S J</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,268.85,195.36,173.32,10.91">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,222.46,393.61,10.91;10,112.66,236.01,355.36,10.91" xml:id="b29">
	<monogr>
		<title level="m" type="main" coord="10,328.43,222.46,177.84,10.91;10,112.66,236.01,213.70,10.91">COVID-19 Detection from Chest X-ray Images Using Feature Fusion and Deep Learning</title>
		<author>
			<persName coords=""><forename type="middle">M</forename><surname>Ahsean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">Md A</forename><surname>Based</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">J</forename><surname>Haider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">M</forename><surname>Kowalski</surname></persName>
		</author>
		<idno type="DOI">10.3390/s21041480</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,249.56,393.33,10.91;10,112.30,263.11,393.68,10.91;10,112.66,276.66,395.17,10.91;10,111.60,290.20,233.12,10.91" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="10,434.88,249.56,71.11,10.91;10,112.30,263.11,393.68,10.91;10,112.66,276.66,71.11,10.91">Overview of the VQA-Med Task at ImageCLEF 2021: Visual Question Answering and Generation in the Medical Domain</title>
		<author>
			<persName coords=""><forename type="first">.</forename><forename type="middle">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">S</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">H</forename><surname>Mller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,192.00,276.66,254.80,10.91">CLEF 2021 Working Notes, CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-09-21">2021. September 21-24. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,303.75,395.01,10.91;10,112.66,317.30,395.17,10.91;10,112.66,330.85,395.17,10.91;10,112.66,344.40,395.17,10.91;10,112.66,357.95,393.32,10.91;10,112.66,371.50,393.33,10.91;10,112.66,385.05,395.17,10.91;10,112.66,398.60,393.54,10.91;10,112.66,412.15,278.33,10.91" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="10,297.41,357.95,208.57,10.91;10,112.66,371.50,311.33,10.91">Overview of the ImageCLEF 2021: Multimedia Retrieval in Medical, Nature, Internet and Social Media Applications</title>
		<author>
			<persName coords=""><forename type="middle">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">H</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">R</forename><surname>Peteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">S</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">Y</forename><surname>Dicente</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">A G S</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">C M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">A</forename><surname>Tauteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">L D</forename><surname>tefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">M G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">T A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">H</forename><surname>Moustahfid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,431.96,371.50,74.03,10.91;10,112.66,385.05,395.17,10.91;10,112.66,398.60,215.65,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 12th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="10,359.20,398.60,147.00,10.91;10,112.66,412.15,31.10,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021-09-21">2021. September 21-24. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
