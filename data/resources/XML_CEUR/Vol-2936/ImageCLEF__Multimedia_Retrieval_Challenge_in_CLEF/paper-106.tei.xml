<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.78,84.74,416.22,15.42;1,89.29,106.66,394.56,15.42;1,88.69,128.58,84.79,15.43">TAM at VQA-Med 2021: A Hybrid Model with Feature Extraction and Fusion for Medical Visual Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.95,158.01,37.05,11.96"><forename type="first">Yong</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">South China Normal University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,138.64,158.01,70.89,11.96"><forename type="first">Zhenguo</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Guangdong University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,240.53,158.01,70.07,11.96"><forename type="first">Tianyong</forename><surname>Hao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">South China Normal University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.78,84.74,416.22,15.42;1,89.29,106.66,394.56,15.42;1,88.69,128.58,84.79,15.43">TAM at VQA-Med 2021: A Hybrid Model with Feature Extraction and Fusion for Medical Visual Question Answering</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">A48116AD6AD52316E0497474E1AA700C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>VQA</term>
					<term>ResNet</term>
					<term>LSTM</term>
					<term>Co-Attention</term>
					<term>MFB</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper briefly describes our model for the ImageCLEF Medical Visual Question Answering Task 2021 (ImageCLEF VQA-Med task 2021). Our method is based on a universal VQA framework and consists of image feature extraction module, question feature extraction module and feature fusion module. We employ the modified ResNet-34 as the backbone to construct an image feature extractor, which effectively extracts pixel-level features and enhances the model performance in a deep network. For question feature extraction, we firstly use word embedding to map question tokens to high dimension vectors, and then input them to a long-short-term memory (LSTM) to extract high-level question features. In addition, we leverage Multi-modal Factorized Bilinear Pooling (MFB) with a co-Attention mechanism to fuse these features to predict final answers. Our model achieves the accuracy score of 0.222 and bleu score of 0.255, ranking at the eighth among all participating teams in the VQA-Med task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, the applications of deep learning in Computer Vision (CV) and Natural Language Processing (NLP) have gained remarkable progress. The development of deep learning in single modality facilitates researchers to explore multimodal studies, e.g., image-text retrieval <ref type="bibr" coords="1,491.74,464.37,11.58,10.91" target="#b0">[1]</ref>, image captioning <ref type="bibr" coords="1,169.55,477.91,12.88,10.91" target="#b1">[2]</ref> and Visual Question Answering (VQA) <ref type="bibr" coords="1,361.70,477.91,11.47,10.91" target="#b2">[3]</ref>. These techniques have been applied to the domains of finance <ref type="bibr" coords="1,247.53,491.46,11.58,10.91" target="#b3">[4]</ref>, traffic <ref type="bibr" coords="1,296.78,491.46,13.00,10.91" target="#b4">[5]</ref> and medical <ref type="bibr" coords="1,372.15,491.46,11.58,10.91" target="#b5">[6]</ref>, which are all prosperous. When applying the VQA technique on medical domain, it can fulfill automatic interpretation of radiology images and make clinical decisions, thereby alleviating the shortage of medical resources.</p><p>In the ImageCLEF VQA-Med task 2021 <ref type="bibr" coords="1,274.47,545.66,11.44,10.91" target="#b6">[7]</ref>, given a radiology image with a related question, the class of diseases indicated in the image is needed to be predicted. Compared to other ordinary benchmark datasets such as VQA <ref type="bibr" coords="1,237.87,572.76,11.30,10.91" target="#b7">[8]</ref>, TDIUC <ref type="bibr" coords="1,289.76,572.76,12.71,10.91" target="#b8">[9]</ref> and Visual7W <ref type="bibr" coords="1,369.46,572.76,16.11,10.91" target="#b9">[10]</ref>, the ImageCLEF VQA-Med task 2021 appears to be more challenging. The images of ordinary datasets contain abundant prior knowledge, such as the object labels including coordinates and category information. However, the images of ImageCLEF VQA-Med task 2021 dataset <ref type="bibr" coords="1,369.57,613.41,17.75,10.91" target="#b10">[11]</ref> do not contain object-level labels. Besides, questions of ordinary VQA datasets usually have many entity names. On the contrary, the questions of the ImageCLEF VQA-Med task 2021 have no entity names of diseases. The intuitive comparison of data examples of common VQA tasks and the ImageCLEF VQA-Med task 2021 can be showed in Figure <ref type="figure" coords="2,239.57,127.61,3.66,10.91" target="#fig_1">1</ref>. In addition, the amount of data in the two kinds of datasets is dramatically different (Tens of thousands of the former and only 4500 of the latter).</p><p>As for the universal VQA benchmark datasets, we can utilize the object-level and pixel-level information of images as well as the entity information of questions for answer prediction. This prior knowledge can greatly enhance the performances of the VQA models. Due to the shortages of ImageCLEF VQA-Med task 2021 dataset, we utilize the modified ResNet-34 <ref type="bibr" coords="2,476.33,195.36,17.81,10.91" target="#b11">[12]</ref> as the image feature extraction module. The basic structure of ResNet-34 is convolution neural network (CNN), which can learn the data bias and pixel-level features through a small number of images. Besides, the residual structure can stabilize the information flow during training, which benefits to high-level feature extraction. We utilize long-short-term memory (LSTM) <ref type="bibr" coords="2,476.00,249.56,18.07,10.91" target="#b12">[13]</ref> to extract the question features from embedded vectors followed by the word embedding module. After that, Multi-modal Factorized Bilinear pooling (MFB) with co-Attention mechanism <ref type="bibr" coords="2,483.28,276.66,12.79,10.91" target="#b2">[3]</ref> is introduced to fuse the image features and question features. Finally, we predict the final answer through doing softmax on the fused features.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>CNN has been widely used in image feature extraction throughout computer vision. Since LeNet <ref type="bibr" coords="3,89.29,124.83,18.01,10.91" target="#b13">[14]</ref> was introduced to extract image features, there have been more and more CNN variants (AlexNet, VGG, GoogleNet) applying in computer vision tasks. With the solutions to gradient disappearing (residual learning and dense learning <ref type="bibr" coords="3,322.61,151.93,15.89,10.91" target="#b14">[15]</ref>), deeper CNN can be constructed to promote the model performances on different vision tasks. Recently, many researchers have focused on utilizing transformer <ref type="bibr" coords="3,240.38,179.03,18.07,10.91" target="#b15">[16]</ref> as the image features encoder and gained remarkable performances. In transformer, the input images are split into patches and treated as sequences, and then input to the transformer for feature extraction. However, this requires a large number of training data to learn the distribution of the datasets, which is not effective in this task.</p><p>The researches of NLP tasks have been greatly promoted by the proposal of transformer. Transformer introduces the full-connected layer (FC) to build the self-attention mechanism, which replaces RNN to learn contextual information of a long-length sentence. Later, Devlin J et al. <ref type="bibr" coords="3,114.23,273.87,18.07,10.91" target="#b16">[17]</ref> proposed Bert by stacking encoders of Transformer. With a deeper structure, Bert adds word embedding, segment embedding and position embedding together as input to reach better performances in NLP tasks. In the next years, XLNet <ref type="bibr" coords="3,355.74,300.97,16.27,10.91" target="#b17">[18]</ref>, GPT-2 <ref type="bibr" coords="3,409.60,300.97,16.28,10.91" target="#b18">[19]</ref>, GPT-3 <ref type="bibr" coords="3,463.47,300.97,17.94,10.91" target="#b19">[20]</ref> were proposed to further promote the performances in NLP tasks. However, both of these models require large amount of training data to fit the distribution of the input text. For the LSTM <ref type="bibr" coords="3,487.08,328.07,16.08,10.91" target="#b12">[13]</ref>, it can well preserve the contextual features of long-time sequences with a small number of texts. Therefore, we use LSTM as our question feature extractor instead.</p><p>The simple methods of feature fusion in deep neural networks include concatenating the different kinds of feature in channel dimension, making element-wise sum or producting with same feature map sizes. But these might not be expressive enough to fully capture the complex associations between the two different modalities. The Multimodal Compact Bilinear pooling (MCB) <ref type="bibr" coords="3,119.91,422.91,17.75,10.91" target="#b20">[21]</ref> projects the images and text representations to a higher dimensional space, and then convolves both vectors by using element-wise product in Fast Fourier Transform (FFT) space. The Multi-modal Factorized Bilinear Pooling (MFB) <ref type="bibr" coords="3,316.89,450.01,17.78,10.91" target="#b21">[22]</ref> introduces co-Attention mechanism to jointly learn both image and question attention. The co-Attention mechanism can effectively learn which regions are important for the images related to the questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>The overview of the architecture of our proposed model is shown in Figure <ref type="figure" coords="3,443.25,535.73,3.81,10.91" target="#fig_2">2</ref>. Our model consists of three components: A image feature extraction module, a question feature extraction module and an attention feature fusion module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Image Feature Extraction Module</head><p>The main component of the original ResNet-34 is convolution neural network (CNN). With the characteristics of translation invariant, translation equivalence, scale invariance and rotation invariance, CNN can learn strong data bias with a small number of data. Besides, the residual structure has a large receptive field and it keeps gradient from vanishing in model training process. Benefitting from these strengthens, this image feature extraction module effectively learns pixel features and spatial features of the medical images. Although it could be easy to overfit because of the small number of data, we alleviate this situation with a dropout operation. The output of the original ResNet-34 is a 1000-dimension vectors used for 1000 classification, which is not suitable in this task.</p><p>Compared to the original ResNet-34, we remove the global average pooling layer (GAP) and full-connected layer (FC), as showed in Figure <ref type="figure" coords="4,304.75,372.88,3.81,10.91" target="#fig_3">3</ref>. Before the images are fed to this module, each image is resized to 128칑128 with the INTER_AREA algorithm. In order to fit the input size of the feature fusion module, we reshape the shape of output image feature maps from B칑512칑16칑16 to a new shape of B칑64칑2048, where B represents the batch size in training. We take this reshaped feature maps as the extracted image features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Text Feature Extraction Module</head><p>In this task, we introduce the word embedding and LSTM to extract the question features. Given the questions of the raw data, the preprocessing of them includes two steps: tokenizing the words of questions and fixing the length of sentences to 12. After that, the tokenized sequences are sent to the word embedding module and generate the embedded word vectors in the dimension of B칑12칑600 (Note the tokens are encoded using GLOVE word embeddings). These embedded vectors are fed into the LSTM module to acquire high-level question features. The input layer dim and hidden layer dim are set to 600 and 1024 respectively. The number of LSTM unit in feature extraction module is set to 1. In this task, we use the whole sequence output features instead of the last token output features to guarantee the information integrity of sentence structure.</p><p>While inputting the word vectors to the LSTM, the forget gate of LSTM determines whether the information flows from previous moment can pass through to the next moment with a sigmoid function, which prompts LSTM to keep useful information and filter useless ones. The input gate of LSTM determines which information needs to be updated at current moment, and the output gate outputs updated information to next moment or as final output. With the gate units, LSTM has a strong ability in storing state information, which benefits to catch contextual correlation in long-time sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Attentional Feature Fusion Module</head><p>After extracting the image features and question features, we feed them to the Multi-modal Factorized Bilinear Pooling (MFB) with the co-Attention mechanism together to obtain the fused features.</p><p>The MFB keeps the robust expressive capacity when compacts the features from different modalities by using the matrix factorization tricks. The co-Attention mechanism consists of a self-attention mechanism (SA) and a guided-attention (GA). Given two modalities features 洧녦 and 洧녧 , 洧녦 first makes the self-attention operation to generate attention features, denoted as 洧녦 洧녩洧노洧노洧녵 . Furthermore, the 洧녦 洧녩洧노洧노洧녵 is used to guide the attention learning to obtain attention features of 洧녧 , denoted as 洧녧 洧녩洧노洧노洧녵 . This operation enhances the connection of two modalities in the learning process. By introducing the co-Attention mechanism to the MFB, the joint feature representation learning can be more accurate and effective.</p><p>In this task, we input the image features and question features (denoted as 洧냪 洧녭 洧뉧롐뀛롐 and 洧녟 洧녭 洧뉧롐뀛롐 ) extracted from the modified ResNet-34 and LSTM to the MFB with the co-Attention mechanism, as showed in Figure <ref type="figure" coords="5,181.36,488.97,3.77,10.91" target="#fig_4">4</ref>. We firstly made a self-attention operation on 洧녟 洧녭 洧뉧롐뀛롐 to obtain question attention features 洧녟 洧녭 洧뉧롐뀛롐 _ 洧녩洧노洧노洧녵 , then we used the MFB to fuse the 洧녟 洧녭 洧뉧롐뀛롐 _ 洧녩洧노洧노洧녵 and 洧냪 洧녭 洧뉧롐뀛롐 to guide the image features attention learning to generate image attention features 洧냪 洧녭 洧뉧롐뀛롐 _ 洧녩洧노洧노洧녵 . After that, we used the MFB to fuse the 洧냪 洧녭 洧뉧롐뀛롐 _ 洧녩洧노洧노洧녵 and 洧녟 洧녭 洧뉧롐뀛롐 _ 洧녩洧노洧노洧녵 with vector multiplication and projected the fused features to a linear dimension. At last, we employed a softmax function on the fused features to predict the probability of each answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data Description</head><p>This dataset of ImageCLEF VQA-Med task 2021 contains a training set of 4000 image-question pairs, a validation set of 500 pairs and a test set of 500 pairs. The triplet data of images, questions and answers are one-to-one associated. The training set of ImageCLEF VQA-Med task 2021 is  <ref type="table" coords="6,305.87,629.74,3.74,10.91" target="#tab_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Optimization</head><p>Our model was optimized with BCE loss function on RTX2080 GPU devices by training 800 epochs. We trained the network from the scratch without using any pretrained model weights.</p><p>We employed Adam with 洧띻1 = 0.9 and 洧띻2 = 0.999 as the optimizer, and used dynamic learning rate to update the weights. In terms of convergence, we visualized the optimizations of the objective of our proposed model in Figure <ref type="figure" coords="7,283.04,273.79,3.81,10.91" target="#fig_6">5</ref>, from which we could observe that the training loss was decreasing, the accuracy score and bleu score were increasing. We utilized the model weight that generated the highest validation accuracy score as the final model weight. We submitted the result generated by this trained model on the test set and achieved the accuracy score of 0.222 and bleu score of 0.255. The top 10 result of this competition is shown in Table <ref type="table" coords="7,500.29,327.98,3.69,10.91" target="#tab_1">2</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Discussion</head><p>An intuitive observation of Figure <ref type="figure" coords="7,251.12,555.93,5.17,10.91" target="#fig_6">5</ref> (b) and (c) is that the performance of our method on validation set is much better than on test set. The reason could be summarized as follows:</p><p>The sampling method of the test set was different from that of the training and validation set. Besides, in order to fit the input size of the image feature extraction model, the input images were resized to 128칑128 roughly, which might lose some spatial information and introduce the noises. In addition, the questions of the dataset provided little entity information of the related classes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper describes the model designed in the ImageCLEF VQA-Med task 2021 competition. We proposed a modified ResNet-34 + LSTM + MFB with a co-Attention mechanism to predict final answers of VQA-Med. In image features extraction process, CNN was employed to learn the image bias and dropout function was introduced to suppress the over-fitting situation. For question feature extraction module, LSTM was utilized to extract the question features that did not rely on a large number of data. The two modalities features were fused by the MFB with co-Attention mechanism to generate the fused features for answer prediction. The best result of our model is 0.222 in accuracy score and 0.255 in bleu score.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,196.42,447.04,204.92,8.97;2,224.86,579.43,148.04,8.97"><head></head><label></label><figDesc>(a) Data example of ImageCLEF 2021 VQA-Med dataset. (b) Data example of common VQA tasks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,89.29,600.68,386.41,8.93;2,91.78,466.84,416.67,107.36"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison of ImageCLEF VQA-Med task 2021 dataset and common VQA datasets.</figDesc><graphic coords="2,91.78,466.84,416.67,107.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,89.29,267.53,272.37,8.93;4,89.29,84.19,458.36,158.82"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The overview of the architecture of our proposed model.</figDesc><graphic coords="4,89.29,84.19,458.36,158.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,89.29,568.61,193.76,8.93;4,89.29,449.78,416.69,94.31"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The structure of modified ResNet-34.</figDesc><graphic coords="4,89.29,449.78,416.69,94.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,89.29,443.09,218.11,8.93;6,129.72,84.19,333.36,346.34"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The co-Attention mechanism in our model.</figDesc><graphic coords="6,129.72,84.19,333.36,346.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="7,133.84,468.91,50.90,8.97;7,244.87,468.91,108.84,8.97;7,394.97,468.91,91.13,8.97"><head></head><label></label><figDesc>(a) Train loss. (b) Validation accuracy score. (c) Validation bleu score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="7,89.29,490.11,151.63,8.93;7,91.78,355.68,144.00,108.00"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Optimization of our model</figDesc><graphic coords="7,91.78,355.68,144.00,108.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,88.99,90.49,338.25,81.83"><head>Table 1</head><label>1</label><figDesc>Statistics of the ImageCLEF VQA-Med task 2021 dataset.</figDesc><table coords="7,168.03,122.10,259.22,50.21"><row><cell>Dataset</cell><cell cols="2">Training set Validation set</cell><cell>Test set</cell></row><row><cell>Open-ended</cell><cell>4412</cell><cell>500</cell><cell>500</cell></row><row><cell>Close-ended</cell><cell>88</cell><cell>0</cell><cell>0</cell></row><row><cell>Classes of Answers</cell><cell>332</cell><cell>236</cell><cell>Unknown</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,88.99,90.49,295.21,165.51"><head>Table 2</head><label>2</label><figDesc>Official results of ImageCLEF VQA-Med task 2021.</figDesc><table coords="8,211.08,122.10,173.12,133.90"><row><cell>Participants</cell><cell cols="2">Accuracy bleu</cell></row><row><cell>duadua</cell><cell>0.382</cell><cell>0.416</cell></row><row><cell>Zhao_Ling_Ling</cell><cell>0.362</cell><cell>0.402</cell></row><row><cell>TeamS</cell><cell>0.348</cell><cell>0.391</cell></row><row><cell>jeanbenoit_delbrouck</cell><cell>0.348</cell><cell>0.384</cell></row><row><cell>riven</cell><cell>0.332</cell><cell>0.361</cell></row><row><cell>Zhao_Shi</cell><cell>0.316</cell><cell>0.352</cell></row><row><cell>IALab_PUC</cell><cell>0.236</cell><cell>0.276</cell></row><row><cell>TAM (ours)</cell><cell>0.222</cell><cell>0.255</cell></row><row><cell>sliencec</cell><cell>0.220</cell><cell>0.235</cell></row><row><cell>sheerin</cell><cell>0.196</cell><cell>0.227</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,112.66,460.49,393.33,10.91;8,112.66,474.04,393.33,10.91;8,112.66,487.59,337.35,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,283.62,460.49,222.37,10.91;8,112.66,474.04,36.17,10.91">Context-aware attention network for image-text retrieval</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.00359</idno>
	</analytic>
	<monogr>
		<title level="m" coord="8,171.54,474.04,334.45,10.91;8,112.66,487.59,51.39,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3536" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,501.14,393.33,10.91;8,112.66,514.69,395.01,10.91;8,112.66,528.24,155.44,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,253.17,501.14,142.20,10.91">Unsupervised image captioning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00425</idno>
	</analytic>
	<monogr>
		<title level="m" coord="8,421.03,501.14,84.96,10.91;8,112.66,514.69,297.65,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4125" to="4134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,541.79,393.33,10.91;8,112.66,555.34,393.33,10.91;8,112.66,568.88,307.68,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,263.50,541.79,242.49,10.91;8,112.66,555.34,43.33,10.91">Deep modular co-attention networks for visual question answering</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00644</idno>
	</analytic>
	<monogr>
		<title level="m" coord="8,179.19,555.34,326.79,10.91;8,112.66,568.88,51.39,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6281" to="6290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,582.43,393.33,10.91;8,112.66,595.98,394.51,10.91;8,112.66,611.97,85.48,7.90" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="8,455.41,582.43,50.57,10.91;8,112.66,595.98,243.70,10.91">Multi-scale two-way deep neural network for stock trend prediction</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/621</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4555" to="4561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,623.08,395.17,10.91;8,112.66,636.63,259.65,10.91" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09224</idno>
		<title level="m" coord="8,271.81,623.08,236.02,10.91;8,112.66,636.63,77.59,10.91">Multi-modal fusion transformer for end-to-end autonomous driving</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,112.66,650.18,393.33,10.91;8,112.66,663.73,393.32,10.91;9,112.66,86.97,397.48,10.91;9,112.36,102.96,152.76,7.90" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,427.46,650.18,78.53,10.91;8,112.66,663.73,207.53,10.91">Overcoming data limitation in medical visual question answering</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">X</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Tjiputra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">D</forename><surname>Tran</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32251-9_57</idno>
	</analytic>
	<monogr>
		<title level="m" coord="8,343.52,663.73,162.47,10.91;9,112.66,86.97,236.05,10.91">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="522" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,114.06,395.01,10.91;9,112.66,127.61,394.53,10.91;9,112.48,141.16,395.18,10.91;9,112.66,154.71,394.53,10.91;9,112.28,168.26,393.70,10.91;9,112.66,181.81,393.33,10.91;9,112.66,195.36,393.33,10.91;9,112.66,208.91,393.53,10.91;9,112.66,222.46,197.61,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,264.32,168.26,241.66,10.91;9,112.66,181.81,261.75,10.91">Overview of the ImageCLEF 2021: Multimedia retrieval in medical, nature, internet and social media applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M칲ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Peteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tauteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>룞efan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Moustahfid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,401.27,181.81,104.72,10.91;9,112.66,195.36,393.33,10.91;9,112.66,208.91,201.15,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 12th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="9,348.61,208.91,157.57,10.91;9,112.66,222.46,31.10,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,236.01,393.33,10.91;9,112.66,249.56,393.53,10.91;9,112.39,263.11,269.15,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,453.81,236.01,52.18,10.91;9,112.66,249.56,86.51,10.91">Vqa: Visual question answering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.279</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,222.96,249.56,283.23,10.91;9,112.39,263.11,25.20,10.91">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,276.66,393.32,10.91;9,112.66,290.20,397.48,10.91;9,112.36,306.20,109.22,7.90" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,198.96,276.66,230.15,10.91">An analysis of visual question answering algorithms</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.217</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,452.18,276.66,53.80,10.91;9,112.66,290.20,260.42,10.91">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1965" to="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,317.30,393.33,10.91;9,112.66,330.85,394.53,10.91;9,112.66,344.40,237.02,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,307.31,317.30,198.68,10.91;9,112.66,330.85,28.18,10.91">Visual7w: Grounded question answering in images</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.540</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,162.43,330.85,340.38,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4995" to="5004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,357.95,393.33,10.91;9,112.39,371.50,393.60,10.91;9,112.66,385.05,394.53,10.91;9,112.66,398.60,116.58,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,435.39,357.95,70.60,10.91;9,112.39,371.50,393.60,10.91;9,112.66,385.05,31.38,10.91">Overview of the vqa-med task at imageclef 2021: Visual question answering and generation in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M칲ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,171.48,385.05,118.94,10.91">CLEF 2021 Working Notes</title>
		<title level="s" coord="9,298.64,385.05,180.76,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,412.15,394.62,10.91;9,112.66,425.70,394.51,10.91;9,112.66,441.69,123.08,7.90" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,273.28,412.15,207.59,10.91">Identity mappings in deep residual networks</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46493-0_38</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,112.66,425.70,189.04,10.91">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,452.79,393.98,10.91;9,112.41,466.34,224.93,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,253.72,452.79,112.17,10.91">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,376.65,452.79,91.46,10.91">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,479.89,393.33,10.91;9,112.66,493.44,397.48,10.91;9,112.36,509.43,79.55,7.90" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,302.90,479.89,203.08,10.91;9,112.66,493.44,50.14,10.91">Gradient-based learning applied to document recognition</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<idno type="DOI">10.1109/5.726791</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,187.85,493.44,107.26,10.91">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,520.54,395.17,10.91;9,112.66,534.09,393.33,10.91;9,112.66,547.64,293.38,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,377.68,520.54,130.16,10.91;9,112.66,534.09,67.72,10.91">Densely connected convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.243</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,203.84,534.09,302.14,10.91;9,112.66,547.64,49.16,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,561.19,395.17,10.91;9,112.66,574.74,326.92,10.91" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m" coord="9,149.36,574.74,107.76,10.91">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,588.29,393.33,10.91;9,112.66,601.84,363.59,10.91" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="9,353.43,588.29,152.55,10.91;9,112.66,601.84,181.08,10.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,615.39,393.33,10.91;9,112.66,628.93,393.57,10.91;9,112.33,642.48,29.19,10.91" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m" coord="9,419.52,615.39,86.47,10.91;9,112.66,628.93,242.29,10.91">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,656.03,393.61,10.91;9,112.66,669.58,394.52,10.91;10,112.41,86.97,145.25,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="9,383.17,656.03,123.10,10.91;9,112.66,669.58,186.89,10.91">Gpt2: Empirical slant delay model for radio space geodetic techniques</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lagler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schindelegger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>B칬hm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kr치sn치</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Nilsson</surname></persName>
		</author>
		<idno type="DOI">10.1002/grl.50288</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,308.16,669.58,126.92,10.91">Geophysical research letters</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1069" to="1073" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,100.52,394.53,10.91;10,112.66,114.06,393.59,10.91;10,112.66,127.61,146.44,10.91" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="10,290.06,114.06,179.48,10.91">Language models are few-shot learners</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,141.16,393.33,10.91;10,112.66,154.71,393.33,10.91;10,112.66,168.26,107.17,10.91" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="10,414.89,141.16,91.09,10.91;10,112.66,154.71,314.26,10.91">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01847</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,181.81,395.17,10.91;10,112.66,195.36,393.32,10.91;10,112.66,208.91,182.33,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="10,228.76,181.81,279.07,10.91;10,112.66,195.36,148.34,10.91">Multi-modal factorized bilinear pooling with co-attention learning for visual question answering</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,284.70,195.36,221.29,10.91;10,112.66,208.91,84.69,10.91">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1821" to="1830" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
