<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,400.43,15.42;1,88.59,106.66,381.30,15.42;1,88.71,128.58,213.26,15.43">SYSU-HCP at VQA-Med 2021: A Data-centric Model with Efficient Training Methodology for Medical Visual Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,156.89,84.37,11.96"><roleName>Co-</roleName><forename type="first">Haifan</forename><surname>Gong</surname></persName>
							<email>gonghf@mail2.sysu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,243.79,156.89,91.46,11.96"><roleName>Co-</roleName><forename type="first">Ricong</forename><surname>Huang</surname></persName>
							<email>huangrc3@mail2.sysu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,405.37,156.89,64.70,11.96"><forename type="first">Guanqi</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,170.84,54.64,11.96"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
							<email>liguanbin@mail.sysu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,400.43,15.42;1,88.59,106.66,381.30,15.42;1,88.71,128.58,213.26,15.43">SYSU-HCP at VQA-Med 2021: A Data-centric Model with Efficient Training Methodology for Medical Visual Question Answering</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">9A22851C76F5687C78CF5A3D15C69A92</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Med-2021 Medical visual question answering</term>
					<term>Classification</term>
					<term>Curriculum learning</term>
					<term>Mixup</term>
					<term>Label smoothing</term>
					<term>Ensemble learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our contribution to the Visual Question Answering Task in the Medical Domain at ImageCLEF 2021. We propose the method with a core idea that the model design and the training should best suit the feature of the data. Specifically, we design a hierarchical feature extraction structure to capture multi-scale features of medical images. To alleviate the issue of data limitation, we apply the mixup strategy for data augmentation during the training process. Based on the observation that there exist hard samples, we introduce the curriculum learning paradigm to resolve this issue. Last but not least, we apply label smoothing and ensemble training to avoid the model bias on the data. The proposed method achieves 1st place in the competition with 0.382 in accuracy and 0.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The task of Visual Question Answering (VQA) is designed to answer the questions by understanding the intrinsic meaning of the corresponding images. By taking advantage of the larger-scale VQA dataset and the ingenious cross-modal feature fusion modules, researchers have made great achievements in the general VQA task. For the sake of promoting the patients' understanding of their disease and supporting the clinical decision, Hasan et al. <ref type="bibr" coords="1,453.99,491.56,13.00,10.91" target="#b0">[1]</ref> brought the VQA task to the medical domain. To facilitate the lack of the benchmark in the medical VQA, ImageCLEF organizes the 4th edition of the Medical Domain Visual Question Answering Competition named VQA-Med 2021. The examples of VQA-Med 2021 are shown in Figure <ref type="figure" coords="1,500.04,532.21,3.81,10.91" target="#fig_0">1</ref>. Since the data from medical VQA is relatively limited comparing to general VQA, we design a data-centric model to collect and make full use of the limited data. Besides, as the questions of VQA-Med 2021 reside in the abnormalities of the medical images, we mainly focus on the design of visual representation to classify the abnormality of the medical images.</p><p>To address the issue of data limitation, we expand the dataset with the data in the previous VQA-Med competition (i.e., VQA-Med 2019 <ref type="bibr" coords="2,292.63,340.15,13.00,10.91" target="#b1">[2]</ref> and VQA-Med 2020 <ref type="bibr" coords="2,403.40,340.15,11.25,10.91" target="#b2">[3]</ref>). To make full use of the limited data, we apply mixup technology to create more samples. For efficient visual feature extraction, we apply label smoothing to stabilize the training progress. Furthermore, we discover the phenomenon that hard samples restrict the performance of the model and use a curriculum learning-based loss function to resolve this issue. Last but not least, to prevent the model from the infliction of intrinsic model bias, we propose to ensemble different types of models in exchange for higher model accuracy(e.g., VGG <ref type="bibr" coords="2,344.73,421.45,11.43,10.91" target="#b3">[4]</ref>, ResNet <ref type="bibr" coords="2,396.91,421.45,11.09,10.91" target="#b4">[5]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>A retrospective analysis is conducted in this section. We first literately review the research of general VQA, then we conclude the methods of VQA in the medical domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">General VQA</head><p>The prevailing VQA framework in the general domain is mainly composed of four components: a visual encoder, a linguistic encoder, a cross-modal feature fusion module, and a classifier. The visual encoders are usually based on deep CNNs such as ResNet <ref type="bibr" coords="2,371.65,570.45,11.29,10.91" target="#b4">[5]</ref>, VGG <ref type="bibr" coords="2,413.45,570.45,11.28,10.91" target="#b3">[4]</ref>, Faster-RCNN <ref type="bibr" coords="2,492.14,570.45,11.29,10.91" target="#b5">[6]</ref>, etc. To extract the linguistic feature, researchers apply the Transformer or RNN based models (e.g., Bert <ref type="bibr" coords="2,135.68,597.55,11.58,10.91" target="#b6">[7]</ref>, LSTM <ref type="bibr" coords="2,185.02,597.55,11.25,10.91" target="#b7">[8]</ref>). The cross-modal feature fusion modules are dominant in current VQA systems. To capture the relationship between images and languages, Fukui et al. <ref type="bibr" coords="2,473.92,611.10,12.86,10.91" target="#b8">[9]</ref> and Kim et al. <ref type="bibr" coords="2,134.62,624.65,17.86,10.91" target="#b9">[10]</ref> apply the compact bilinear pooling methods. In the meanwhile, Yang et al. <ref type="bibr" coords="2,486.94,624.65,16.19,10.91" target="#b10">[11]</ref>, Cao et al. <ref type="bibr" coords="2,137.91,638.20,16.41,10.91" target="#b11">[12]</ref>, and Anderson et al. <ref type="bibr" coords="2,257.55,638.20,18.07,10.91" target="#b12">[13]</ref> investigate to design the model that focus on the question-related region of the image. MLP-liked classifier is usually used to select the final answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Medical VQA</head><p>Different from general VQA, medical VQA usually suffers from limited data and the distinction between common-sense knowledge and medical domain-specific knowledge. Thus, we first discuss the current methods to alleviate the data limitation in medical VQA. After that, we summarize the previous methods for the medical VQA.</p><p>Data limitation in medical VQA. Data limitation is an unavoidable topic in the field of medical image analysis, especially in the domain of medical VQA. To address this issue, Nguyen et al. <ref type="bibr" coords="3,113.22,188.83,17.76,10.91" target="#b13">[14]</ref> combine the meta-learning and the denoising auto-encoder to make use of large-scale unlabeled data. Nevertheless, they neglect the compatibility between the visual concept and the questions. Gong et al. <ref type="bibr" coords="3,187.19,215.93,17.76,10.91" target="#b14">[15]</ref> propose a novel multi-task pre-train framework, the image encoders of which are mandatory to not only learn the linguistic compatibility feature but also the vision concept by performing the original task (i.e., classification &amp; segmentation) on the external dataset. Still, the easiest way to overcome the data limitation is to collect more data as was done by Chen et al. <ref type="bibr" coords="3,176.76,270.13,16.09,10.91" target="#b15">[16]</ref>. In this work, we not only collect the data from the previous VQA-Med competition but also apply the mixup strategy for data augmentation.</p><p>Previous methods on VQA-Med challenges. In the 2018 VQA-Med challenge <ref type="bibr" coords="3,457.20,297.22,11.29,10.91" target="#b0">[1]</ref>, the top three groups applied the analogous pipeline as the VQA in the general domain. Specifically, they apply CNNs (i.e., ResNet-152 <ref type="bibr" coords="3,246.12,324.32,11.58,10.91" target="#b4">[5]</ref>, VGG <ref type="bibr" coords="3,289.46,324.32,11.58,10.91" target="#b3">[4]</ref>, Inception-ResNet-v2 <ref type="bibr" coords="3,404.02,324.32,17.11,10.91" target="#b16">[17]</ref>) for visual feature extraction, LSTM <ref type="bibr" coords="3,170.75,337.87,12.99,10.91" target="#b7">[8]</ref> or Bi-LSTM for language modeling, and attention based feature fusion modules (i.e., MFH <ref type="bibr" coords="3,175.57,351.42,16.25,10.91" target="#b17">[18]</ref>, SAM <ref type="bibr" coords="3,223.36,351.42,16.25,10.91" target="#b10">[11]</ref>, BAN <ref type="bibr" coords="3,270.75,351.42,15.71,10.91" target="#b9">[10]</ref>).</p><p>In the 2019 VQA-Med challenge <ref type="bibr" coords="3,243.17,364.97,11.36,10.91" target="#b1">[2]</ref>, the leading three groups <ref type="bibr" coords="3,371.51,364.97,16.37,10.91" target="#b18">[19,</ref><ref type="bibr" coords="3,390.61,364.97,12.52,10.91" target="#b19">20,</ref><ref type="bibr" coords="3,405.86,364.97,13.99,10.91" target="#b20">21]</ref> used the Inception-Resnet-v2 <ref type="bibr" coords="3,135.09,378.52,17.76,10.91" target="#b16">[17]</ref> or a tailor-designed VGG <ref type="bibr" coords="3,267.59,378.52,12.69,10.91" target="#b3">[4]</ref> to extract image feature. Bert <ref type="bibr" coords="3,413.72,378.52,12.69,10.91" target="#b6">[7]</ref> is used to capture the semantic of the questions, and MFH <ref type="bibr" coords="3,269.47,392.07,17.94,10.91" target="#b17">[18]</ref> is applied for feature fusion. The tailor-designed VGG proposed by Yan et al. <ref type="bibr" coords="3,220.54,405.62,18.07,10.91" target="#b18">[19]</ref> replaced the conventional global average pooling layer of VGG with the hierarchical average pooling layer, which could efficiently capture the multi-scale feature. Besides, it is worth noting that the top 3 teams apply the question classification method to figure out the category of the questions.</p><p>In the 2020 VQA-Med challenge <ref type="bibr" coords="3,248.62,459.81,11.58,10.91" target="#b2">[3]</ref>, two <ref type="bibr" coords="3,287.55,459.81,16.56,10.91" target="#b21">[22,</ref><ref type="bibr" coords="3,307.14,459.81,14.11,10.91" target="#b22">23]</ref> of the top three teams abandon the conventional VQA framework. Only Bumjun et al. <ref type="bibr" coords="3,299.52,473.36,17.90,10.91" target="#b23">[24]</ref> applies the conventional VQA framework, which uses VGG backbone for visual feature extraction, BioBert <ref type="bibr" coords="3,369.28,486.91,17.76,10.91" target="#b24">[25]</ref> for question encoding, and MFH <ref type="bibr" coords="3,113.31,500.46,17.75,10.91" target="#b17">[18]</ref> for feature fusion. The other two teams chose to direct classify the image with the deep neural networks. The winner of VQA-Med-2020 <ref type="bibr" coords="3,309.58,514.01,18.06,10.91" target="#b21">[22]</ref> designed a question skeleton-based approach to take full use of the linguistic feature, and integrate multi-scale and multi-architecture models to achieve the best result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Datasets</head><p>In VQA-Med task <ref type="bibr" coords="3,170.65,599.74,18.04,10.91" target="#b25">[26]</ref> of ImageCLEF 2021 <ref type="bibr" coords="3,281.41,599.74,16.38,10.91" target="#b26">[27]</ref>, the original dataset includes a training set of 4500 radiology images with 4500 question-answer (QA) pairs, a validation set of 500 radiology images with 500 QA pairs, and a test set of 500 radiology images with 500 questions. These questions focus on the abnormalities of medical images. Figure <ref type="figure" coords="3,371.88,640.38,5.07,10.91" target="#fig_0">1</ref> shows three examples in the dataset.</p><p>Since the previous dataset <ref type="bibr" coords="3,213.63,667.48,11.23,10.91" target="#b1">[2,</ref><ref type="bibr" coords="3,226.82,667.48,8.88,10.91" target="#b2">3]</ref> in the VQA-Med competition is allowed to use, we leverage the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head><p>As this competition focus on the questions about abnormalities, we discard the conventional VQA framework and regard VQA as an image classification task. To make full use of the data, we design a data-centric model which is shown in Figure <ref type="figure" coords="4,342.51,543.35,3.69,10.91" target="#fig_1">2</ref>. This framework mainly consists of four parts: data preparation, network design, data-centric training methodologies, and model ensemble. The data preparation is illustrated in Section 3. Other parts are detailed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Network Architecture</head><p>Inspired by Yan et al. <ref type="bibr" coords="4,184.22,620.18,17.76,10.91" target="#b18">[19]</ref> and Aisha et al. <ref type="bibr" coords="4,275.45,620.18,17.76,10.91" target="#b22">[23]</ref> that multi-scale features contain more abundant information of medical images, we design a hierarchical feature extraction architecture to capture the multi-scale features of medical images. Different from the conventional highlevel semantic feature representation architecture with fully-connected layers (Fig. <ref type="figure" coords="4,463.03,660.82,5.11,10.91" target="#fig_2">3</ref>  proposed architecture replaces the fully-connected layers with hierarchical adaptive global average pooling layers (Fig. <ref type="figure" coords="5,222.11,376.12,19.48,10.91" target="#fig_2">3 (b)</ref>). Compared with a similarity work <ref type="bibr" coords="5,412.14,376.12,18.06,10.91" target="#b18">[19]</ref> that uses global average pooling to construct feature vector, the proposed method with adaptive global average pooling is more flexible to receive arbitrary input size of the image. This hierarchically adaptive global average pooling (HAGAP) structure is applied to ResNet-50 <ref type="bibr" coords="5,378.89,416.77,11.28,10.91" target="#b4">[5]</ref>, ResNeSt-50 <ref type="bibr" coords="5,448.54,416.77,16.09,10.91" target="#b27">[28]</ref>, VGG-16 and VGG-19 <ref type="bibr" coords="5,146.68,430.32,12.84,10.91" target="#b3">[4]</ref> to extract image feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Data-centric efficient training</head><p>In this part, we introduce three efficient strategies to better utilize the training data according to their characteristic.</p><p>Mixup. To alleviate the issue of data limitation in medical image representation learning, we adopt a simple yet effective data augmentation method called Mixup <ref type="bibr" coords="5,397.65,520.69,16.27,10.91" target="#b28">[29]</ref>. Given two samples (𝑥 𝑖 , 𝑦 𝑖 ) and (𝑥 𝑗 , 𝑦 𝑗 ), we create a new image 𝑥 ˆwith label 𝑦 ˆby linear interpolation with the following operation:</p><formula xml:id="formula_0" coords="5,250.47,572.55,251.66,27.17">𝑥 ˆ= 𝜆𝑥 𝑖 + (1 -𝜆)𝑥 𝑗 𝑦 ˆ= 𝜆𝑦 𝑖 + (1 -𝜆)𝑦 𝑗 (<label>1</label></formula><formula xml:id="formula_1" coords="5,502.13,580.11,3.86,10.91">)</formula><p>where 𝜆 ∈ [0, 1] is a random value drawn from the 𝐵𝑒𝑡𝑎(𝛼, 𝛼) distribution with the hyperparameter 𝛼 = 0.2. It is worth noting that we only use the newly created images during the training process. Curriculum learning. Based on the observation that in the training set of this competition, one disease could occur in various images modalities (e.g., CT, MRI.). Some modalities are of numerous samples while others are of few. Thus, the imaging modality of the diseases that occur infrequently is hard to learn. Furthermore, the training set is unavoidable to contain noise. To resolve these issues, we introduce the idea of curriculum learning <ref type="bibr" coords="6,377.91,114.06,17.76,10.91" target="#b29">[30]</ref> into the training process. To simplify this process, we apply the SuperLoss <ref type="bibr" coords="6,306.91,127.61,16.33,10.91" target="#b30">[31]</ref>, which automatically down-weights the hard samples with a larger loss.</p><p>Label smoothing. The label smoothing methodology is first proposed to train the Inception-V2 <ref type="bibr" coords="6,103.31,168.26,17.91,10.91" target="#b31">[32]</ref> network. It works by adjusting the probability of the target label by:</p><formula xml:id="formula_2" coords="6,222.55,191.45,279.58,25.30">𝑝 𝑖 = {︂ 1 -𝜀 if 𝑖 = 𝑦 𝜀/(𝐾 -1) otherwise (<label>2</label></formula><formula xml:id="formula_3" coords="6,502.13,199.08,3.86,10.91">)</formula><p>where 𝜀 is a small constant, 𝐾 is the number of classes, and 𝑝 𝑖 denotes the possibility of category 𝑖. As label smoothing groups the representations of the examples from the same class into tight clusters, the model could achieve a better generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Model Ensemble</head><p>As the model unavoidably contains bias, we apply multi-architecture ensemble to further improve the model performance. Comparing to the winner of VQA-Med 2020 <ref type="bibr" coords="6,440.98,315.40,18.03,10.91" target="#b21">[22]</ref> that takes more than 30 models to an ensemble, our best submission only contains 8 models by taking the advantage of the HAGAP structure. Specifically, the 8 models are ResNet-50, ResNeSt-50, VGG- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation details</head><p>As for training data, we leverage the data in Section 3. The models for our best submission are trained with the combination of mixup loss, SuperLoss, and Label smoothing loss. We used the SGD optimizer with momentum set to 0.9. The initial learning rate is set to 1e-3, and the weight decay is 5e-4. All models are trained for 60 epochs, and we select the model for inference on a fixed epoch (e.g., 50).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation</head><p>The VQA-Med competition applies accuracy and BLEU <ref type="bibr" coords="6,325.91,593.76,21.41,10.91" target="#b32">[33]</ref> as the evaluation metrics. Accuracy is calculated as the number of correct predicted answers among all answers. BLEU measures the similarity between the predicted answers and ground truth answers. As shown in Fig. <ref type="figure" coords="6,482.76,620.86,3.87,10.91" target="#fig_4">4</ref>, we achieved an accuracy of 0.382 and a BLEU score of 0.416 in the VQA-Med-2021 test set, which won the first place in this competition.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation study</head><p>To demonstrate the effectiveness of our proposed model, we conduct ablation study with the VGG-16 network, which is shown in Table <ref type="table" coords="7,285.63,568.19,3.81,10.91" target="#tab_0">1</ref>. Specifically, the input image size is 224 × 224.</p><p>The training set contains 5664 images while the validation set contains 500 images. The original VGG-16 network is set as the baseline, which achieves an accuracy of 66.6%. We utilize the mixup strategy for data augmentation, which surpasses the baseline by 1.8%. Furthermore, we adopt label smoothing to avoid the over-fitting of the model, which improves the accuracy to 68.8%. After that, we apply hierarchical architecture to represent the feature of the medical image, and introduce the curriculum learning paradigm into the framework, which brings an accuracy gain of 0.4%. With the efforts mentioned above, we achieve 69.2% accuracy on the VQA-Med-2021 validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>The VQA-Med is challenging task due to the limited data. As this work mainly focused on distinguish the abnormality between the medical images, we focus on designing the training scheduler and the feature extract module to make better use of the limited data. It is worth noting that as the training set, the validation set, and test set may not obey the same distribution, the Table <ref type="table" coords="8,115.66,199.79,5.05,10.91" target="#tab_0">1</ref> is of limited value. In other words, Label smoothing, HAGAP, and curriculum learning may be effective in the test set, but it not brings significant improvement on the validation set.</p><p>For the same distribution inconsistent issue, we directly classify the images rather than use the long-tailed based methods <ref type="bibr" coords="8,207.46,240.44,16.15,10.91" target="#b15">[16]</ref>. Though we achieves 1 st place at this competition, our score is not high and there is still a long way to go to achieve applicable medical VQA.</p><p>For the future works in the medical VQA, we may digger deeper into better feature representation of image or words with the help of large amount unlabeled data. Besides, generating the answer word by word rather than regard the answer as a label is more valuable research topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we describe our participation at the ImageCLEF 2021 VQA-Med challenge. Considering most of the questions are about abnormality, we abandon the conventional complex cross-modal fusion methodologies. With the firm brief that the characteristics of the data should be fully considered in the construction of the model, we design a data-centric model with efficient training strategies. Our proposed method achieves the best results among all participating groups with an accuracy of 0.382 and a BLEU score of 0.416.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,275.45,303.99,8.93"><head>Q:Figure 1 :</head><label>1</label><figDesc>Figure 1: Three examples from the dataset of ImageCLEF 2021 VQA-Med.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.29,345.10,416.69,8.93;4,89.29,357.11,416.69,8.87;4,89.29,369.07,416.70,8.87;4,89.29,381.02,416.70,8.87;4,89.29,392.98,214.12,8.87"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Overview of the proposed data-centric medical VQA framework. Followed by the instruction of the organizers, we collect the data from the previous competition. Then we design a hierarchical structure to better represent the feature of the image. After that, we adopt three efficient training strategies according to the characteristic of the data. Finally, we ensemble the conventional CNN and the hierarchical CNN towards eliminating data bias.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,89.29,315.24,418.36,8.93;5,89.29,327.24,125.75,8.87"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The conventional high-level semantic representation architecture and the proposed hierarchical multi-scale architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,104.39,356.05,403.28,10.91;6,88.96,369.60,417.02,10.91;6,89.29,383.15,418.37,10.91;6,89.29,396.69,416.70,10.91;6,89.29,410.24,185.29,10.91"><head></head><label></label><figDesc>VGG-19, ResNet-50-HAGAP, ResNeSt-50-HAGAP, VGG-16-HAGAP and VGG-19-HAGAP. The input size of ResNet-based networks is 256 × 256 while the input size of VGG based networks is 224 × 224. All backbones are initialized with the ImageNet pre-trained weight. Since the data is limited, we do not set the validation set during the training process and select the model of a fixed epoch for evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,89.29,380.43,293.37,8.93;7,89.29,84.19,416.69,271.72"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The leaderboard of the ImageCLEF VQA-Med 2021 challenge.</figDesc><graphic coords="7,89.29,84.19,416.69,271.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,88.98,413.58,337.59,93.78"><head>Table 1</head><label>1</label><figDesc>Ablation study on the VQA-Med-2021 validation set.</figDesc><table coords="7,166.22,445.20,260.36,62.16"><row><cell>Method</cell><cell cols="2">Accuracy Improvement</cell></row><row><cell>VGG16</cell><cell>66.6%</cell><cell>-</cell></row><row><cell>+Mixup strategy</cell><cell>68.4%</cell><cell>+1.8%</cell></row><row><cell>+Label smoothing</cell><cell>68.8%</cell><cell>+0.4%</cell></row><row><cell>+HAGAP and Curriculum Learning</cell><cell>69.2%</cell><cell>+0.4%</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,112.66,479.63,393.33,10.91;8,112.66,493.18,395.17,10.91;8,112.66,506.73,394.52,10.91;8,112.39,520.28,290.50,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,402.45,479.63,103.54,10.91;8,112.66,493.18,235.24,10.91">Overview of imageclef 2018 medical domain visual question answering task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="8,370.80,493.18,137.03,10.91;8,112.66,506.73,204.28,10.91">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="8,182.74,520.28,151.15,10.91">CEUR Workshop Proceedings, CEUR</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14, 2018. 2125. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,533.83,394.61,10.91;8,112.66,547.38,393.32,10.91;8,112.66,560.93,394.52,10.91;8,112.66,574.48,389.87,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,465.37,533.83,41.90,10.91;8,112.66,547.38,309.52,10.91">Vqa-med: Overview of the medical visual question answering task at imageclef</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="8,466.54,547.38,39.45,10.91;8,112.66,560.93,294.32,10.91">Working Notes of CLEF 2019 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="8,282.38,574.48,151.15,10.91">CEUR Workshop Proceedings, CEUR</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09-09">2019. September 9-12, 2019. 2019</date>
			<biblScope unit="volume">2380</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,588.02,393.33,10.91;8,112.39,601.57,393.60,10.91;8,112.66,615.12,394.52,10.91;8,112.33,628.67,394.85,10.91;8,112.66,642.22,90.78,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,434.73,588.02,71.26,10.91;8,112.39,601.57,393.60,10.91;8,112.66,615.12,30.33,10.91">Overview of the vqa-med task at imageclef 2020: Visual question answering and generation in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="8,166.51,615.12,335.47,10.91">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="8,380.61,629.69,126.57,9.72;8,112.66,642.22,21.79,10.91">CEUR Workshop Proceedings, CEUR</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25, 2020. 2696. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,655.77,393.33,10.91;8,112.66,669.32,393.33,10.91;9,112.66,86.97,395.17,10.91;9,112.66,100.52,66.49,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,247.99,655.77,258.00,10.91;8,112.66,669.32,50.14,10.91">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,314.40,669.32,191.59,10.91;9,112.66,86.97,121.31,10.91">3rd International Conference on Learning Representations, ICLR 2015</title>
		<title level="s" coord="9,405.43,86.97,102.40,10.91;9,112.66,100.52,36.51,10.91">Conference Track Proceedings</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">May 7-9, 2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,114.06,395.17,10.91;9,112.66,127.61,395.01,10.91;9,112.41,141.16,38.81,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,259.74,114.06,203.38,10.91">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,488.38,114.06,19.45,10.91;9,112.66,127.61,347.24,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,154.71,393.33,10.91;9,112.66,168.26,394.53,10.91;9,112.66,181.81,45.01,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,264.19,154.71,241.80,10.91;9,112.66,168.26,112.14,10.91">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,248.35,168.26,228.61,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,195.36,393.33,10.91;9,112.66,208.91,393.33,10.91;9,112.66,222.46,393.32,10.91;9,112.66,236.01,357.71,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,353.43,195.36,152.55,10.91;9,112.66,208.91,186.91,10.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,327.87,208.91,178.11,10.91;9,112.66,222.46,393.32,10.91;9,112.66,236.01,102.30,10.91">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="9,112.66,249.56,393.98,10.91;9,112.41,263.11,48.96,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,253.72,249.56,112.17,10.91">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,376.65,249.56,91.46,10.91">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,276.66,393.33,10.91;9,112.66,290.20,393.33,10.91;9,112.66,303.75,107.17,10.91" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="9,414.89,276.66,91.09,10.91;9,112.66,290.20,314.26,10.91">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01847</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,317.30,393.33,10.91;9,112.66,330.85,239.00,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,259.14,317.30,124.76,10.91">Bilinear attention networks</title>
		<author>
			<persName coords=""><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,414.42,317.30,91.57,10.91;9,112.66,330.85,140.70,10.91">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1564" to="1574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,344.40,393.33,10.91;9,112.66,357.95,393.33,10.91;9,112.66,371.50,126.79,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,298.65,344.40,207.33,10.91;9,112.66,357.95,44.87,10.91">Stacked attention networks for image question answering</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,188.20,357.95,317.78,10.91;9,112.66,371.50,49.16,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,385.05,393.60,10.91;9,112.66,398.60,395.00,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,279.42,385.05,226.84,10.91;9,112.66,398.60,15.38,10.91">Visual question reasoning on general dependency tree</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,150.25,398.60,327.40,10.91">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,412.15,393.33,10.91;9,112.66,425.70,393.32,10.91;9,112.66,439.25,394.65,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,439.42,412.15,66.56,10.91;9,112.66,425.70,316.11,10.91">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,452.18,425.70,53.80,10.91;9,112.66,439.25,296.73,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,452.79,393.33,10.91;9,112.66,466.34,393.32,10.91;9,112.66,479.89,374.36,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,427.46,452.79,78.53,10.91;9,112.66,466.34,207.53,10.91">Overcoming data limitation in medical visual question answering</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">X</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Tjiputra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,343.52,466.34,162.47,10.91;9,112.66,479.89,243.84,10.91">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="522" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,493.44,395.17,10.91;9,112.66,506.99,393.33,10.91;9,112.66,520.54,156.71,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,291.02,493.44,216.81,10.91;9,112.66,506.99,211.11,10.91">Cross-modal self-attention with multi-task pretraining for medical visual question answering</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,349.80,506.99,156.18,10.91;9,112.66,520.54,126.33,10.91">ACM International Conference on Multimedia Retrieval (ICMR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,534.09,393.53,10.91;9,112.66,547.64,393.33,10.91;9,112.66,561.19,393.33,10.91;9,112.66,574.74,163.66,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,225.86,534.09,280.34,10.91;9,112.66,547.64,154.77,10.91">HCP-MIC at vqa-med 2020: Effective visual representation for medical visual question answering</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,292.45,547.64,213.53,10.91;9,112.66,561.19,128.24,10.91">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="9,124.27,575.75,122.23,9.72">CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25, 2020. 2696. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,588.29,393.33,10.91;9,112.66,601.84,393.33,10.91;9,112.66,615.39,393.33,10.91;9,112.66,628.93,270.13,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="9,331.70,588.29,174.29,10.91;9,112.66,601.84,180.49,10.91">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,452.94,601.84,53.05,10.91;9,112.66,615.39,279.55,10.91">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Markovitch</surname></persName>
		</editor>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017">February 4-9, 2017. 2017</date>
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,642.48,393.33,10.91;9,112.66,656.03,393.54,10.91;9,112.30,669.58,395.36,10.91;10,112.41,86.97,48.96,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="9,241.67,642.48,264.32,10.91;9,112.66,656.03,170.61,10.91">Multi-modal factorized bilinear pooling with co-attention learning for visual question answering</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,306.81,656.03,199.39,10.91;9,112.30,669.58,80.91,10.91">IEEE International Conference on Computer Vision, ICCV 2017</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017">October 22-29, 2017. 2017</date>
			<biblScope unit="page" from="1839" to="1848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,100.52,393.33,10.91;10,112.66,114.06,393.33,10.91;10,112.66,127.61,393.33,10.91;10,112.66,141.16,220.14,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,268.54,100.52,237.44,10.91;10,112.66,114.06,150.57,10.91">Zhejiang university at imageclef 2019 visual question answering in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="10,290.31,114.06,215.68,10.91;10,112.66,127.61,126.57,10.91">Working Notes of CLEF 2019 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="10,112.66,141.16,151.15,10.91">CEUR Workshop Proceedings, CEUR</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">September 9-12, 2019. 2019</date>
			<biblScope unit="volume">2380</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,154.71,393.33,10.91;10,112.66,168.26,393.33,10.91;10,112.66,181.81,393.33,10.91;10,112.66,195.36,220.14,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="10,224.09,154.71,281.90,10.91;10,112.66,168.26,150.07,10.91">TUA1 at imageclef 2019 vqa-med: a classification and generation model based on transfer learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="10,289.62,168.26,216.37,10.91;10,112.66,181.81,126.57,10.91">Working Notes of CLEF 2019 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="10,112.66,195.36,151.15,10.91">CEUR Workshop Proceedings, CEUR</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">September 9-12, 2019. 2019</date>
			<biblScope unit="volume">2380</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,208.91,393.33,10.91;10,112.66,222.46,394.61,10.91;10,112.14,236.01,395.05,10.91;10,112.66,249.56,395.17,10.91;10,111.60,263.11,60.89,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="10,326.95,208.91,179.04,10.91;10,112.66,222.46,373.80,10.91">Ensemble of streamlined bilinear visual question answering models for the imageclef 2019 challenge in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sznitman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Nyholm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Löfstedt</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="10,112.14,236.01,350.09,10.91">Working Notes of CLEF 2019 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="10,346.50,249.56,155.13,10.91">CEUR Workshop Proceedings, CEUR</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">September 9-12, 2019. 2019</date>
			<biblScope unit="volume">2380</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,276.66,393.33,10.91;10,112.66,290.20,393.33,10.91;10,112.66,303.75,393.33,10.91;10,112.66,317.30,393.33,10.91;10,112.66,330.85,191.99,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="10,352.12,276.66,153.86,10.91;10,112.66,290.20,393.33,10.91;10,112.66,303.75,87.67,10.91">AIML at vqa-med 2020: Knowledge inference via a skeleton-based sentence mapping approach for medical domain visual question answering</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Verjans</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="10,230.53,303.75,275.46,10.91;10,112.66,317.30,78.90,10.91">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="10,480.56,318.32,25.43,9.72;10,112.66,330.85,122.99,10.91">CEUR Workshop Proceedings, CEUR</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25, 2020. 2696. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,344.40,393.33,10.91;10,112.30,357.95,393.68,10.91;10,110.82,371.50,396.37,10.91;10,112.66,385.05,315.91,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="10,297.96,344.40,208.02,10.91;10,112.30,357.95,243.37,10.91">The inception team at vqa-med 2020: Pretrained VGG with data augmentation for medical VQA and VQG</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Al-Sadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Al-Theiabat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Al-Ayyoub</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="10,379.75,357.95,126.23,10.91;10,110.82,371.50,211.41,10.91">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="10,208.43,385.05,151.15,10.91">CEUR Workshop Proceedings, CEUR</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25, 2020. 2696. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,398.60,393.33,10.91;10,112.66,412.15,393.32,10.91;10,112.66,425.70,393.32,10.91;10,112.66,439.25,254.77,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="10,231.54,398.60,274.44,10.91;10,112.66,412.15,183.06,10.91">bumjun_jung at vqa-med 2020: VQA model based on feature extraction and multi-modal feature fusion</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="10,318.46,412.15,187.53,10.91;10,112.66,425.70,148.75,10.91">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="10,147.28,439.25,151.15,10.91">CEUR Workshop Proceedings, CEUR</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25, 2020. 2696. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,452.79,393.33,10.91;10,112.66,466.34,395.01,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="10,361.64,452.79,144.35,10.91;10,112.66,466.34,249.17,10.91">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,369.84,466.34,46.74,10.91">Bioinform</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,479.89,393.33,10.91;10,112.39,493.44,393.60,10.91;10,112.66,506.99,394.53,10.91;10,112.66,520.54,116.58,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="10,435.39,479.89,70.60,10.91;10,112.39,493.44,393.60,10.91;10,112.66,506.99,31.38,10.91">Overview of the vqa-med task at imageclef 2021: Visual question answering and generation in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,171.48,506.99,118.94,10.91">CLEF 2021 Working Notes</title>
		<title level="s" coord="10,298.64,506.99,180.76,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,534.09,395.01,10.91;10,112.66,547.64,395.17,10.91;10,112.66,561.19,394.53,10.91;10,112.66,574.74,394.53,10.91;10,112.28,588.29,393.70,10.91;10,112.66,601.84,393.33,10.91;10,112.66,615.39,393.33,10.91;10,112.66,628.93,394.53,10.91;10,112.66,642.48,116.58,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="10,264.32,588.29,241.66,10.91;10,112.66,601.84,261.75,10.91">Overview of the ImageCLEF 2021: Multimedia retrieval in medical, nature, internet and social media applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tauteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Moustahfid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,401.27,601.84,104.72,10.91;10,112.66,615.39,393.33,10.91;10,112.66,628.93,158.01,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 12th International Conference of the CLEF Association</title>
		<title level="s" coord="10,277.92,628.93,182.37,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,656.03,395.17,10.91;10,112.66,669.58,395.00,10.91;11,112.66,89.41,97.35,7.90" xml:id="b27">
	<monogr>
		<title level="m" type="main" coord="10,228.14,669.58,145.27,10.91">Resnest: Split-attention networks</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,100.52,395.17,10.91;11,112.66,114.06,197.93,10.91" xml:id="b28">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m" coord="11,331.02,100.52,176.81,10.91;11,112.66,114.06,16.17,10.91">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,127.61,393.33,10.91;11,112.66,141.16,394.52,10.91;11,112.66,154.71,393.33,10.91;11,112.66,168.26,128.55,10.91" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="11,334.16,127.61,91.49,10.91">Curriculum learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,450.78,127.61,55.20,10.91;11,112.66,141.16,344.22,10.91">Proceedings of the 26th Annual International Conference on MachineLearning, ICML 2009</title>
		<title level="s" coord="11,326.01,155.73,179.98,9.72;11,112.66,169.28,23.02,9.72">ACM International Conference Proceeding Series</title>
		<meeting>the 26th Annual International Conference on MachineLearning, ICML 2009<address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">June 14-18, 2009. 2009</date>
			<biblScope unit="volume">382</biblScope>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,181.81,393.33,10.91;11,112.66,195.36,393.32,10.91;11,112.66,208.91,394.53,10.91;11,112.39,222.46,57.36,10.91" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="11,289.05,181.81,216.94,10.91;11,112.66,195.36,34.70,10.91">Superloss: A generic loss for robust curriculum learning</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Castells</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,170.17,195.36,335.81,10.91;11,112.66,208.91,219.52,10.91">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<meeting><address><addrLine>NeurIPS; virtual</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. December 6-12, 2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,236.01,395.17,10.91;11,112.66,249.56,393.32,10.91;11,112.66,263.11,394.53,10.91;11,112.66,276.66,90.72,10.91" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="11,364.66,236.01,143.18,10.91;11,112.66,249.56,121.52,10.91">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,257.11,249.56,248.88,10.91;11,112.66,263.11,107.44,10.91">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016">June 27-30, 2016. 2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,290.20,393.33,10.91;11,112.66,303.75,393.53,10.91;11,112.66,317.30,228.72,10.91" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="11,307.20,290.20,198.79,10.91;11,112.66,303.75,88.22,10.91">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,223.88,303.75,282.32,10.91;11,112.66,317.30,116.04,10.91">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
