<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,413.74,15.42;1,89.29,106.66,393.70,15.42;1,89.29,128.58,37.91,15.43">PUC Chile team at VQA-Med 2021: approaching VQA as a classification task via fine-tuning a pretrained CNN</title>
				<funder ref="#_pJhE75R">
					<orgName type="full">FONDECYT</orgName>
				</funder>
				<funder ref="#_FJf8Dqr">
					<orgName type="full">ANID</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,156.89,82.85,11.96"><forename type="first">Ricardo</forename><surname>Schilling</surname></persName>
							<email>reschilling@uc.cl</email>
							<affiliation key="aff0">
								<orgName type="institution">Pontificia Universidad Católica</orgName>
								<address>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,184.78,156.89,69.88,11.96"><forename type="first">Pablo</forename><surname>Messina</surname></persName>
							<email>pamessina@uc.cl</email>
							<affiliation key="aff0">
								<orgName type="institution">Pontificia Universidad Católica</orgName>
								<address>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,267.31,156.89,57.39,11.96"><forename type="first">Denis</forename><surname>Parra</surname></persName>
							<email>dparra@ing.puc.cl</email>
							<affiliation key="aff0">
								<orgName type="institution">Pontificia Universidad Católica</orgName>
								<address>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,355.69,156.89,55.17,11.96"><forename type="first">Hans</forename><surname>Löbel</surname></persName>
							<email>halobel@ing.puc.cl</email>
							<affiliation key="aff0">
								<orgName type="institution">Pontificia Universidad Católica</orgName>
								<address>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,413.74,15.42;1,89.29,106.66,393.70,15.42;1,89.29,128.58,37.91,15.43">PUC Chile team at VQA-Med 2021: approaching VQA as a classification task via fine-tuning a pretrained CNN</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">D4CDDF05667AC6F6CA58DFDB464764C7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the submission of the IALab group of the Pontifical Catholic University of Chile to the Medical Domain Visual Question Answering (VQA-Med) task. Our participation was rather simple: we approached the problem as image classification. We took a DenseNet121 with its weights pre-trained in ImageNet and fine-tuned it with the VQA-Med 2020 dataset labels to predict the answer. Different answers were treated as different classes, and the questions were disregarded for simplicity since essentially they all ask for abnormalities. With this very simple approach we ranked 7th among 11 teams, with a test set accuracy of 0.236.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>ImageCLEF <ref type="bibr" coords="1,144.76,365.94,12.99,10.91" target="#b0">[1]</ref> is an initiative with the aim of advancing the field of image retrieval (IR) as well as enhancing the evaluation in various fields of IR. The initiative takes the form of several challenges, and it is specially aware of the changes in the IR field in recent years, which have brought about tasks requiring the use of different types of data such as text, images and other features moving towards multi-modality. ImageCLEF has been running annually since 2003, and since the second version (2004) there are medical images involved in some tasks, such as medical image retrieval. Since then, new tasks involving medical images have been integrated into the ImageCLEFmedical challenge group of tasks <ref type="bibr" coords="1,329.92,460.79,11.55,10.91" target="#b1">[2]</ref>, and that is how the task of medical visual question-answering (VQA) has been taking place since 2018. The goal of this task is as follows: given a medical image accompanied with a clinically relevant question, participating systems are tasked with answering the question based on the visual image content <ref type="bibr" coords="1,467.17,501.43,11.58,10.91" target="#b2">[3]</ref>. This task could help patients get a second opinion by an automated system, helping them better understand their conditions and supporting clinical discisions <ref type="bibr" coords="1,366.83,528.53,11.43,10.91" target="#b1">[2]</ref>.</p><p>In this document we describe the participation of our team from HAIVis group 1 within the artificial intelligence laboratory 2 at PUC Chile (PUC Chile team) in the VQA-Med task at MedicalImageCLEF 2021 <ref type="bibr" coords="1,204.44,569.18,11.58,10.91" target="#b1">[2]</ref>. Our team earned the 7th place out of 11 in the challenge, and our best submission was as simple as a DenseNet121 for visual input encoding to a classifier of answers, which did not take into account the questions and obtained a test accuracy of 0.236.</p><p>The rest of the paper is structured as follows: Section 1 presents relevant related work, Section 2 describes our data analysis, in section 3 we provide a description of our method, and section 4 describes our results. Lastly, in section 5 we conclude our article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>We addressed the task of medical VQA by modeling it as a classification task. Previously, there have been other attempts at medical image classification, such as the work of Kumar et al <ref type="bibr" coords="2,493.08,213.34,12.91,10.91" target="#b3">[4]</ref> that used an ensemble of fine tuned models to classify medical images, using the ImageCLEF 2016 medical image public dataset and achieving over 82% accuracy on its predictions.</p><p>Also the work of Li <ref type="bibr" coords="2,185.35,253.99,12.68,10.91" target="#b4">[5]</ref> which solved a similar problem but worked on lung images exclusively using the ILD dataset, reporting a significant improvement to regular image descriptors.</p><p>Similar works, such as Ren <ref type="bibr" coords="2,217.56,281.08,12.69,10.91" target="#b5">[6]</ref> also use a classification approach for visual question answering, in this case for the COCO-QA dataset, using a CNN together with an LSTM which predicts a single word.</p><p>Other similar approaches include the work of Yan, Xin, et al <ref type="bibr" coords="2,368.71,321.73,11.39,10.91" target="#b6">[7]</ref>, using a VGG network with BERT <ref type="bibr" coords="2,111.15,335.28,16.39,10.91" target="#b7">[8]</ref> encodings for the 2019 Imageclef Med task.</p><p>Unlike these approaches, we attempted to see how a regular CNN would work in a more general Visual Question Answering problem, where the answers are previously known and we rank them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset</head><p>Our dataset consisted of triples of {image, question, answer}, where the question asks about the possible existence of abnormalities in the image, and the answer provides a list of relevant abnormalities plus some possible additional details. Because in this task every question asks about the abnormalities present in the image, we ignored the questions and treated the problem as classification.</p><p>In the 2020 dataset <ref type="bibr" coords="2,179.62,655.05,12.52,10.91" target="#b8">[9]</ref>, there are a total of 3940 medical images with associated answers. After some basic preprocessing, such as simplifying whitespaces, removing punctuation and so on, we obtained a total of 325 distinct labels. As Figure <ref type="figure" coords="3,148.31,283.89,5.17,10.91" target="#fig_2">3</ref> shows, the amount of images per label varied, with the least frequent label appearing 5 times, and the most frequent label up to 80 times.</p><p>We also tried including data from the 2019 dataset <ref type="bibr" coords="3,319.40,310.99,17.08,10.91" target="#b9">[10]</ref>, however this ended up adding over a thousand new labels, with a total of over 1400 distinct labels, while only doubling our amount of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methods</head><p>We tried out two distinct approaches, and applied them to both the normal and extended datasets.</p><p>The best results were achieved with our first approach, where we finetuned a densenet-121 <ref type="bibr" coords="3,487.19,423.81,18.79,10.91" target="#b10">[11]</ref> pre-trained on imagenet, to predict the image label.</p><p>We first treated each of our distint labels as a different class and tasked the network with predicting the corresponding label, which we then mapped into the corresponding label. To achieve this, we changed the last layer of the pretrained model with three dense layers, size 1024, 1024 and a final classification layer with 325 neurons.</p><p>For this approach, the network perfomed slighly better using only the smaller dataset. We finetuned the network for around 100 epochs achieving around 30% accuracy and similar BLEU in the validation set. We used simple techniques such as early stopping, l2 normalization and dropout to avoid unnecessary overfitting of the networks.</p><p>We also tried using perceptual similarity <ref type="bibr" coords="3,279.39,559.30,17.15,10.91" target="#b11">[12]</ref> in an attempt to classify the image accordingly, however this approach failed to generalize properly and didn't manage to beat our initial approach.</p><p>This approach consisted in using a simple k-Neighbors Classifier to try to predict the corresponding image label, using an image vector obtained directly from the lpips module. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>With our simple approach, we were able to achieve 30% accuracy and a similar BLEU score on the 2021 validation set <ref type="bibr" coords="4,188.18,381.32,11.89,10.91" target="#b1">[2]</ref>, however we realized that adding the data from 2019 only decreased both metrics to around 20%.</p><p>The model's performance on the test set ended up being slightly lower at a 23% accuracy and 0.276 BLEU score, which was expected and shows the model's inability to generalize well.</p><p>As for the perceptual similarity model, unfortunately the results were very bad, not even making over 5% accuracy. This however, was expected as it was only an initial approach and this technique proved useful for some similar tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Although our approach did not attain high accuracy, we believe it is simple enough and could serve as the base for a more complex model which includes actual text generation for better generalization. In the future, we plan to also include the question as input along the image to output the generated answer.</p><p>We strongly believe that this task can be distilled into a simpler classification or captioning problem, however we failed to make a model that could accurately determine the exact description of the input image. One of the reasons we believe this is the case is that our model does not currently generate new text, but could serve as the base of a bigger, more complex model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,598.30,148.65,8.93;2,198.57,484.36,198.14,101.38"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of dataset triples</figDesc><graphic coords="2,198.57,484.36,198.14,101.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,89.29,251.79,204.37,8.93;3,202.14,107.23,191.00,132.00"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Distribution of labels in the training set</figDesc><graphic coords="3,202.14,107.23,191.00,132.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,89.29,305.85,342.21,8.93;4,211.24,84.19,172.80,209.10"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Early results of the evolution of model accuracy for the extended dataset.</figDesc><graphic coords="4,211.24,84.19,172.80,209.10" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was partially funded by <rs type="programName">ANID -Millennium Science Initiative Program</rs> -Code <rs type="grantNumber">ICN17_002</rs> and by <rs type="funder">ANID</rs>, <rs type="funder">FONDECYT</rs> grant <rs type="grantNumber">1191791</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_FJf8Dqr">
					<idno type="grant-number">ICN17_002</idno>
					<orgName type="program" subtype="full">ANID -Millennium Science Initiative Program</orgName>
				</org>
				<org type="funding" xml:id="_pJhE75R">
					<idno type="grant-number">1191791</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,112.66,183.45,395.01,10.91;5,112.66,197.00,394.53,10.91;5,112.48,210.55,395.18,10.91;5,112.66,224.10,394.53,10.91;5,112.28,237.65,393.70,10.91;5,112.66,251.20,393.33,10.91;5,112.66,264.75,393.33,10.91;5,112.66,278.30,393.53,10.91;5,112.66,291.85,197.61,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,264.32,237.65,241.66,10.91;5,112.66,251.20,261.75,10.91">Overview of the ImageCLEF 2021: Multimedia retrieval in medical, nature, internet and social media applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tauteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Stefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Moustahfid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,401.27,251.20,104.72,10.91;5,112.66,264.75,393.33,10.91;5,112.66,278.30,201.15,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 12th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="5,348.61,278.30,157.57,10.91;5,112.66,291.85,31.10,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,112.66,305.40,393.33,10.91;5,112.39,318.95,393.60,10.91;5,112.66,332.50,394.53,10.91;5,112.66,346.05,116.58,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,435.39,305.40,70.60,10.91;5,112.39,318.95,393.60,10.91;5,112.66,332.50,31.38,10.91">Overview of the vqa-med task at imageclef 2021: Visual question answering and generation in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,171.48,332.50,118.94,10.91">CLEF 2021 Working Notes</title>
		<title level="s" coord="5,298.64,332.50,180.76,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,112.66,359.59,393.33,10.91;5,112.66,373.14,362.94,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,384.40,359.59,121.59,10.91;5,112.66,373.14,208.08,10.91">Overview of imageclef 2018 medical domain visual question answering task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,346.99,373.14,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,112.66,386.69,393.33,10.91;5,112.66,400.24,393.33,10.91;5,112.66,413.79,294.85,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,328.88,386.69,177.11,10.91;5,112.66,400.24,213.47,10.91">An ensemble of fine-tuned convolutional neural networks for medical image classification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lyndon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fulham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<idno type="DOI">10.1109/JBHI.2016.2635663</idno>
	</analytic>
	<monogr>
		<title level="j" coord="5,334.06,400.24,171.92,10.91;5,112.66,413.79,51.02,10.91">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="31" to="40" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,112.66,427.34,393.33,10.91;5,112.26,440.89,393.73,10.91;5,112.28,454.44,397.86,10.91;5,112.36,470.43,43.94,7.90" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,376.30,427.34,129.69,10.91;5,112.26,440.89,156.50,10.91">Medical image classification with convolutional neural network</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICARCV.2014.7064414</idno>
	</analytic>
	<monogr>
		<title level="m" coord="5,293.31,440.89,212.68,10.91;5,112.28,454.44,171.03,10.91">2014 13th International Conference on Control Automation Robotics Vision (ICARCV)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="844" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,112.66,481.54,395.01,10.91;5,112.66,497.53,97.35,7.90" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="5,231.36,481.54,246.64,10.91">Exploring models and data for image question answering</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.02074</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,112.66,508.64,393.33,10.91;5,112.66,522.18,300.49,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,268.54,508.64,237.44,10.91;5,112.66,522.18,143.74,10.91">Zhejiang university at imageclef 2019 visual question answering in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,284.53,522.18,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,112.66,535.73,393.33,10.91;5,112.66,549.28,311.37,10.91" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="5,326.58,535.73,179.40,10.91;5,112.66,549.28,181.08,10.91">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,112.66,562.83,393.33,10.91;5,112.39,576.38,393.60,10.91;5,112.66,589.93,394.53,10.91;5,112.33,603.48,120.27,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="5,434.73,562.83,71.26,10.91;5,112.39,576.38,393.60,10.91;5,112.66,589.93,31.38,10.91">Overview of the vqa-med task at imageclef 2020: Visual question answering and generation in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,171.48,589.93,118.94,10.91">CLEF 2020 Working Notes</title>
		<title level="s" coord="5,298.64,589.93,180.76,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,112.66,617.03,395.17,10.91;5,112.66,630.58,394.62,10.91;5,112.66,644.13,395.17,10.91;5,112.26,657.68,162.37,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="5,482.36,617.03,25.47,10.91;5,112.66,630.58,370.59,10.91">VQA-Med: Overview of the medical visual question answering task at imageclef 2019</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="5,112.66,644.13,115.96,10.91">CLEF2019 Working Notes</title>
		<title level="s" coord="5,237.06,644.13,189.13,10.91">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,86.97,393.32,10.91;6,112.66,100.52,169.59,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06993</idno>
		<title level="m" coord="6,357.97,86.97,148.01,10.91;6,112.66,100.52,39.20,10.91">Densely connected convolutional networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,114.06,393.33,10.91;6,112.66,127.61,287.88,10.91" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.03924</idno>
		<title level="m" coord="6,356.61,114.06,149.37,10.91;6,112.66,127.61,157.95,10.91">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
