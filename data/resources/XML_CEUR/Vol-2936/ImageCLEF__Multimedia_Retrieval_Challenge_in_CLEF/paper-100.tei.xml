<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.02,75.44,450.96,17.04;1,72.02,96.20,114.17,17.04">A Convolutional Neural Networks based Coral Reef Annotation and Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.02,129.10,71.13,10.80"><forename type="first">Rohit</forename><forename type="middle">R</forename><surname>Gunti</surname></persName>
							<email>rgunti@albany.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University at Albany</orgName>
								<address>
									<settlement>Albany, UAlbany</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">CLEF</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,171.38,129.10,70.25,10.80"><forename type="first">Abebe</forename><surname>Rorissa</surname></persName>
							<email>arorissa@albany.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University at Albany</orgName>
								<address>
									<settlement>Albany, UAlbany</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Tennessee</orgName>
								<address>
									<settlement>Knoxville, UAlbany</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">CLEF</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.02,75.44,450.96,17.04;1,72.02,96.20,114.17,17.04">A Convolutional Neural Networks based Coral Reef Annotation and Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C96B548E026D0E2033C0D171F2A4DBD2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image classification</term>
					<term>coral reef</term>
					<term>convolutional neural networks</term>
					<term>annotation and localization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The purpose of this study was to examine the effectiveness and flexibility of our image retrieval system by participating in the ImageCLEFcoral 2021 challenge. The system was developed for object detection and identification in any dataset. Initially, multiple trials were conducted to train the system with patterns of thirteen substrates and searching the relationships between them. Since the key for better performance of machine learning systems is repeated inputs, we used datasets with three distinct groups, each group having a different characterization of substrates. For submissions to the ImageCLEFcoral challenge, we tested the system with the provided test dataset where it was able to find the patterns and relationships between the substrates in a massive amount of data that was also too complex. We sought to extract highlevel image features and use deep learning and the CNN-RNN neural network. We obtained an acceptable range of accuracies for each characterization of substrates, although the average accuracy was 70 percent.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Conventional classification techniques employ spectral responses from coral types based on singlepixel values without capturing spatial information <ref type="bibr" coords="1,303.21,456.09,11.68,9.94" target="#b0">[1]</ref>. Several attempts, such as the use of fractal, spatial autocorrelation, and spatial co-occurrence matrices, have been made to improve the spectral analysis of remotely sensed image data, especially when dealing with complex spatial features and many different coral substrates like fire coral and branching coral that share a similar spatial pattern. In recent years, wavelet transform has been investigated and applied in image processing due to its innovative mathematical framework for multiscale time-frequency signal analysis. The literature on image processing and analysis with wavelet transform generally focus on image compression, image fusion, image watermarking, face-image-detection, image noise removal, identification of tumorous regions and microcalcification clusters, image segmentation, etc. However, image classification, a type of categorization of image data using spectral, spatial, and temporal information that assigns the pixels of a continuous raster image to discrete categories concerning wavelet transforms, has not been explored enough. Wavelet transform opens the possibility for capturing image features at different scales and are more discriminative than spectral features.</p><p>In this working note, a novel frequency-based classification framework and algorithm is proposed using an overcomplete decomposition procedure at multiple scales to examine if the proposed method can effectively identify detailed coral reef substrates in the Marine Technology Research Unit dataset <ref type="bibr" coords="1,72.02,658.56,12.91,9.94" target="#b1">[2]</ref>  <ref type="bibr" coords="1,87.48,658.56,11.69,9.94" target="#b2">[3]</ref>. Because overcomplete wavelet can generate spatial arrangements of objects and features at any scale level (infinite-scale analysis), a frequency-based multiscale classification algorithm using an overcomplete wavelet proposed by <ref type="bibr" coords="2,226.53,87.28,12.78,9.94" target="#b3">[4]</ref> and superior to the dyadic-based wavelet classifier approach was utilized for the current study. Furthermore, because the approach has the flexibility to use any window size, we believe that it can be applied at any level and at either high spatial or medium-resolution structure-from-motion photogrammetry (action cameras attached to drones). The newly developed algorithm is hereafter referred to as Wave-CLASS <ref type="bibr" coords="2,296.44,137.92,11.77,9.94" target="#b3">[4]</ref>. This study's classification decision rule computes one distance for each class among all bands; hence, five distances are computed for the classification decision. The training sample with the shortest distance to the unknown feature vector wins the new texture. In other words, the training class that is closest to the local window is assigned to the center of that window.</p><p>Mallet <ref type="bibr" coords="2,117.25,201.16,12.78,9.94" target="#b4">[5]</ref> initiated the multiresolution analysis theory using the orthonormal wavelet basis. Wavelet decompositions can be categorized into dyadic and overcomplete approaches. The idea of employing overcomplete wavelet decomposition is motivated by the fact that it can provide translational invariant features. The Overcomplete decomposition approach employed in this study omits the down sampling procedure and produces four-texture information with the same dimension of the original image at an infinite scale. This advantage is crucial for classification. A higher decomposition level can be expected to improve accuracy. A set of feature vectors is used to identify detailed coral reef substrates.</p><p>An entire image, which consists of both unknown blocks (blocks that need to be classified) and sample blocks (training samples), undergoes a multiscale overcomplete wavelet transform. Several training samples need to be selected for each coral reef substrate class. A spatial measure can be used as a feature to represent unknown blocks and sample blocks. The distance between feature vectors leads to supervised classification, such as the Euclidean distance classifier.</p><p>The overall accuracy, producer's accuracy, user's accuracy, and kappa coefficient were generated using the error matrix. A minimum of 50 sample points for each benthic substrate is generally suggested for any image classification accuracy assessment. Thus, a total of 525 samples were generated using a stratified random sampling approach with a minimum per size sample size of 35 points, resulting in an average of 75 points per class (a total of seven classes). Table <ref type="table" coords="2,344.35,403.65,5.52,9.94" target="#tab_3">3</ref> shows the overall accuracy, producer's accuracy, user's accuracy, and kappa coefficient produced by the Wave-CLASS algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Test dataset</head><p>The dataset for the ImageCLEFcoral 2021 task <ref type="bibr" coords="2,303.43,473.97,12.91,9.94" target="#b5">[6]</ref> consists of images from coral reefs collected through the Marine Technology Research Unit at the University of Essex from around the world. While the images vary in terms of their quality, with some that are blurry and others having poor color balance, they come with annotations of 13 types of substrates ranging from hard coral branching to soft-coral -Gorgonian to Sponge -Barrel. With no oceanographers on our team to identify the substrates while training the images, we used every possible online resource for substrate detection. Table <ref type="table" coords="2,478.04,537.24,5.52,9.94" target="#tab_3">3</ref> presents three types of sample images for each substrate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Tasks performed</head><p>For our project, we initially conducted satellite image classification where we trained a MATLAB based system with high spatial resolution images as our datasets. For the annotation and localization challenge, we trained the system using high spatial resolution images along with medium resolution ones to increase system flexibility. Additionally, we used some google images from the Web to train and identify individual substrates.</p><p>Initially, the training and testing was done using a training dataset to analyze system performance and calculate the standardized metrics like precision, recall, Intersection over union, dice coefficient, and coefficient kappa.</p><p>After training the system with images with single substrates that were downloaded from the web, subsequent trainings used images that had a fewer number of substrates, although some training images had more substrates. To improve our results and the accuracy of the system in detecting some of the substrates, we started the training and testing from scratch by removing third-party images and adding images that have fewer number of substrates. The first training phase consisted of eight (8) trials covering a total of 13 substrates, 120 images, and 405 annotations (see Table <ref type="table" coords="4,413.25,201.16,3.96,9.94" target="#tab_1">1</ref>).</p><p>For each of these eight trials, we used different images that had varying substrate configurations to test the system's classification and substrate differentiation accuracies. Better results from the trials provided a reliable foundation for the annotation and localization challenge and, thereby, obtain better results. Because our first phase training involved saving the annotations in array format, removing and retraining was important in analyzing the training patterns. During the 2 nd phase, with input from the 1 st phase, we used 96 additional training images (which included images with some individual substrates and many grouped substrates) and 515 annotations. During the 2 nd phase, grouped substrates showed better classification results. Hence, we decided to train the system for the annotation and localization challenge using the most grouped substrates (i.e., the actual training dataset images in the training Zip folder with some additional images downloaded from online sources). For the actual submission of our results to ImageCLEFcoral (AIcrowd), we divided the training dataset and test dataset (downloaded from the challenge's resources) into four groups The system was at its highest accuracy classifying test images when substrates trained in groups compared to the classification that was done during the trial rounds with many individual substrates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>We used six methods to identify patterns in the datasets that allowed us to extract information automatically. In this paper, we are focusing on only six approaches to classify an image into different substrates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.1.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approaches Used</head><p>A six-step method was used to process and conduct the classification of an image: 1. Image Acquisition: Capturing the input image from the source file using the "uigetfile" and "imread" functions. However, if the image has not been acquired satisfactorily, the intended tasks may not be achievable, even with some form of image enhancement. 2. Image Resizing or Scaling: A process that involves a trade-off between efficiency, smoothness, and sharpness. 3. Fuzzy C Means Clustering: If the Marine Technology Research Unit dataset input image bandwidth and cell value are between 0.5 to 0.9 and 0.1 to 0.5, respectively, then that class is identified as hard coral mushroom. The same goes for branching, soft coral, sponge and every other annotated class. 4. Image Segmentation: The process of partitioning an image into multiple segments (set of pixels or superpixels). The goal is to represent the image as something more meaningful and to assign a label to every pixel in an image such that pixels with the same label share specific characteristics. 5. Feature Extraction: A convolutional layer condenses the input by extracting features of interest from it and produces feature maps in response to different feature detectors. 6. Classification: This is the top layer of a network that collects the final convoluted feature and returns a column vector where each row points towards a class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Resources Employed</head><p>We used a Windows 10 PC with Intel Processor and 8 GB RAM. For training and classification purposes, we used MATLAB for Windows. For errors and calculations, we used the Anaconda distribution of Python IDE 3.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>A total of 216 images in the trial round and 485 images with 2611 annotations in the actual run round were tested. Several of them were successfully classified while some were partially classified. The first step is the processing of the input image by resizing it. Figure <ref type="figure" coords="5,347.21,512.25,5.52,9.94">1</ref> shows the input image during the trial round (left) and the resized image (right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1: Input image for the trial round and the resized version of the image</head><p>To determine the exact part of size of the substrates in an image, we drew the region boundaries by forming the mask and applying the Sobel edge detector to a labelled image. In short, we applied thresholding to narrow in the resulting image. To find the precise boundaries of the regions, we converted the resulting RGB image to a binary format and measured the data points at the corners of the substrate. In the images (Figure <ref type="figure" coords="5,231.91,750.74,3.96,9.94">2</ref>), each data point was measured from the binary image covering the exact corners of the substrates. Noting the values of X axes and Y axes, a bounding box was drawn covering the area of the substrate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2: Input image for the trial round and the resized version of the image</head><p>Because objects had to be detected in every image, we used the label2rgb function to determine colors of each object based on the number of objects in the label matrix of the resized images. Additionally, we used optional perimeters like 'spring' setting the background pixels to the color cyan and randomizing color assignment to the labels. Through these color filters, we were able to average all colormap values within the range of 0.5 to 1 and average all the values to get the highest confidence value. After training the system using half of the images from the training dataset, we then tested the system on the other half. The coordinates of the tested images were noted in a text editor to validate the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3: Transforming the resized image into a colored label image</head><p>For the annotation and localization task, we separately trained 515 images with 3276 annotations and tested 485 images with 2611 annotations. The output of the tests was submitted to the AIcrowd system in the following format: The height and width of the boundaries is measured through the bounding boxes by subtracting the lowest x datapoints (Xmin, Ymin) from the highest datapoints (Xmax, Ymax) respectively. We noted all the values X min, Y min, height, and width in the given format. A scale of 1 unit per 50 px was used for both the x and y axes.</p><p>[image_ID];</p><formula xml:id="formula_0" coords="6,108.02,676.80,413.61,35.28">[substrate1] [[confidence1,1]:][width1,1]x[height1,1]+[xmin1,1]+[ymin1,1],[[confidence1,2]:][width1,2]x [height1,2]+[xmin1,2]</formula><p>Each RGB image was converted to a binary format to locate the points around the boundaries. A data tip pointer tool was used to draw the polygons and all the data tip information was exported to a MATLAB workspace. Figure <ref type="figure" coords="6,205.37,752.78,5.52,9.94" target="#fig_0">4</ref> presents the results for the annotations and localizations task.    Overall, our results from the training dataset show that the producer's and user's accuracies for all the categories are about equal for all image subsets, which is one of the critical criteria for systematic substrate mapping. Because both the training and ImageCLEFcoral data consist of coral substrate images from four different locations and some images were downloaded from the Internet, they were classified into three categories: <ref type="bibr" coords="8,216.40,383.49,12.80,9.94" target="#b0">(1)</ref> substrates that are uniquely grown or long distanced from other substrates in the Marine Technology Research Unit dataset; (2) coral reef substrates that grow together or short distanced from other substrates; and (3) single individual substrates that are downloaded from online sources.</p><p>The producer's and user's accuracies of the classification of all the selected classes in the unique substrates (#1), grouped substrates (#2), and individual substrates (#3) achieved above 75% accuracy, except the user's accuracy of uniquely grown substrates (#1) (75%) and grouped substrates (#2) (71%). The user accuracy of 75% indicates that only 75% of the animals identified as a specific substrate within the uniquely grown class (#1) produced more accurate classification results, although the producer's accuracy of the same category reached 88%, which can be considered adequate. In other words, remote sensing analysis of the algorithm shows that 88% of the time, a coral reef that was identified as a substrate, say mushroom, grown with no other different coral substrate nearby was uniquely grown, whereas a user of the output map would argue that only 75% of the time the classification correctly identifies "mushroom" as actually being a mushroom. In contrast, individual class substrates from the Internet achieved high producer's (94%) and user's (99%) accuracies. We anticipated that encrusting class that are grown alone and in a group with other substrates, could potentially lower the overall accuracy. The relatively low user's accuracy could be due to the spectral similarity among substrates like a fully grown hard coral branching and partially grown fire corals having similar tree like structures, and coexistence of hard coral branching on the hard-coral table.</p><p>Another critical factor is that almost all encrustings in these subsets are grouped. These encrustings are generally much smaller than those grown uniquely, where most are large stone like structures and purple in color. Most of the hard-coral encrusting that were grouped with other substrates are bright (red, blue, and so forth), surrounded by dark impervious surfaces (algae). Whereas, uniquely grown substrates have many shapes and colors that are mixed or interlocked with diverse materials or complex background objects and features (e.g., stones, soil, grass, plants). The pool category results achieved 100% for both producer's and user's accuracies in both the individual and uniquely grown subsets because the spectral signatures of adherence to hard rocky surfaces are highly distinguishable from those of other coral substrate features.</p><p>The relatively low classification accuracy for rock surfaces in the grouped substrates is likely due to its smaller size, which inevitably causes mixed pixels in the training samples. The next highest producer's accuracy is 98% for the individual subset, 99% for the uniquely grown subset, and 94% for the grouped subset.</p><p>The challenge test dataset produced low scores for precision, recall, and other standardized metrics. The system was not able to predict the values for type II errors (False negatives) in a confusion matrix which led to no results for recall, dice coefficient, and Jaccard coefficient. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Future work</head><p>Our findings indicate that it will be possible to improve the accuracy of the Wave-CLASS classifier we utilized and build a supervised/unsupervised system. We believe that the algorithm and classifier can also be used in other domains and for applications in medicine, agriculture, astronomy and so on. In medicine, it could help detect specific diseases and their characteristics. In agriculture, it could be used to study land-use for cultivating and farming. In astronomy, one could use it to detect land that is destroyed by natural disasters. The system is flexible enough that by using the wavelet algorithm to classify substrates, we could use other algorithms such as random forest in conjunction with the Wave-CLASS classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>Overall, the developed methodology and training procedures we utilized produced better classification rates in identifying the substrates of different classes. Although all the test dataset results were not favorable, the system's sensitivity, specificity, and flexibility when adapting medium resolutions of datasets was a success. For future work, more significant effort needs to be expended in implementing different algorithms that support different computer vision tools and brings additional features like processing 3D texture models and so forth.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,72.02,330.77,252.32,11.04;7,72.00,72.00,340.65,256.54"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results of the annotations and localizations task</figDesc><graphic coords="7,72.00,72.00,340.65,256.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,484.76,382.65,38.42,9.94;7,72.02,395.25,326.94,9.94"><head></head><label></label><figDesc>Figure 5 presents the increased axes results for the annotation and localization task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,72.02,673.06,317.91,11.04;7,72.00,405.22,246.97,265.60"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Increased axes results for the annotation and localization task</figDesc><graphic coords="7,72.00,405.22,246.97,265.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,72.02,163.24,112.79,9.94;9,72.02,175.84,110.33,9.94;9,72.02,188.56,157.02,9.94;9,72.02,201.16,119.45,9.94;9,72.02,213.76,131.68,9.94"><head></head><label></label><figDesc>Overall Precision: 0.0011 Overall Recall: No result Average Kappa Coefficient: 0.1284 Dice Coefficient: No result Jaccard Coefficient: No result</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="5,86.20,534.79,450.92,136.00"><head></head><label></label><figDesc></figDesc><graphic coords="5,86.20,534.79,450.92,136.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,72.02,574.66,389.45,152.78"><head>Table 2 Table title</head><label>2title</label><figDesc></figDesc><table coords="2,112.34,604.32,349.13,123.12"><row><cell>Hard Coral -Encrusting</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hard Coral -Table</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hard Coral -Foliose</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hard Coral -Mushroom</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Soft Coral</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Soft Coral -Gorgonian</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sponge</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sponge -Barrel</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fire Coral -Millepora</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Substrates Algae -Macro or Leaves</cell><cell>Type 1</cell><cell>Type 2</cell><cell>Type 3</cell></row><row><cell>Hard Coral -Branching</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hard Coral -Submassive</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hard Coral -Boulder</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,72.02,276.53,432.22,66.36"><head>Table 1</head><label>1</label><figDesc>Images used for the trials in the first training phase</figDesc><table coords="4,88.82,304.30,415.42,38.59"><row><cell>Trials</cell><cell cols="4">1 st trial 2 nd trial 3 rd trial 4 th trial</cell><cell>5 th trial</cell><cell>6 th trial</cell><cell>7 th trial</cell><cell>8 th trial</cell></row><row><cell>Images</cell><cell>12</cell><cell>15</cell><cell>18</cell><cell>13</cell><cell>11</cell><cell>18</cell><cell>18</cell><cell>15</cell></row><row><cell>trained</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,72.02,474.33,451.13,174.49"><head></head><label></label><figDesc>1. Group 1: 100 training and 201 testing images from the dominica-cabrits sub-folder of the Zip folder with thirteen additional online training images -hard coral encrusting, algae, sponge barrel, sponge, two fire coral, hard coral boulder, two soft coral, two submissive, and two soft coral gorgorian. 2. Group 2: 100 training and 166 testing images from the spermonde-keke sub-folder with eleven additional online training imageshard coral branching, hard coral table, two fire coral, two hard coral submassive, two soft coral, hard coral foliose, hard coral encrusting, sponge coral, sponge barrel. 3. Group 3: 100 training and 20 testing images from the Seychelles-BL sub-folder with five additional online training images -two hard coral submassive, algae, hard coral branching, hard coral encrusting. 4. Group 4: 172 training and 98 testing images from the PK-20180729-02 sub-folder with fourteen additional online training imageshard coral mushroom, hard coral branching, hard coral encrusting, two hard coral submassive, two soft coral, sponge coral, sponge barrel, two fire coral, two soft coral gorgorian, hard coral foliose.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,52.44,112.10,487.52,203.31"><head>Table 3</head><label>3</label><figDesc>Results of tests for the training dataset</figDesc><table coords="8,52.44,140.92,487.52,174.49"><row><cell>Substrates</cell><cell cols="2">Annotations Precision</cell><cell>Recall</cell><cell>Jaccard</cell><cell>Dice</cell><cell>Kappa</cell><cell>Hue</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>coefficient</cell><cell>coefficient</cell><cell>coefficient</cell><cell></cell></row><row><cell>Hard coral -Branching</cell><cell>531</cell><cell>0.117927</cell><cell>0.17712</cell><cell>0.12914</cell><cell>0.12866</cell><cell>0.12814</cell><cell>0.7</cell></row><row><cell>Hard Coral -Submassive</cell><cell>138</cell><cell>0.117782</cell><cell>0.17727</cell><cell>0.12914</cell><cell>0.12866</cell><cell>0.12847</cell><cell>0.6</cell></row><row><cell>Hard Coral -Boulder</cell><cell>346</cell><cell>0.118244</cell><cell>0.17687</cell><cell>0.12914</cell><cell>0.12866</cell><cell>0.12815</cell><cell>0.8</cell></row><row><cell>Hard Coral -encrusting</cell><cell>117</cell><cell>0.115589</cell><cell>0.17755</cell><cell>0.12914</cell><cell>0.12865</cell><cell>0.12847</cell><cell>0.5</cell></row><row><cell>Hard Coral -table</cell><cell>322</cell><cell>0.116410</cell><cell>0.17725</cell><cell>0.12914</cell><cell>0.12866</cell><cell>0.12847</cell><cell>0.6</cell></row><row><cell>Hard Coral -Foliose</cell><cell>68</cell><cell>0.117296</cell><cell>0.17703</cell><cell>0.12914</cell><cell>0.12866</cell><cell>0.12847</cell><cell>0.7</cell></row><row><cell>Hard Coral -Mushroom</cell><cell>287</cell><cell>0.114476</cell><cell>0.178439</cell><cell>0.12914</cell><cell>0.12865</cell><cell>0.12814</cell><cell>0.8</cell></row><row><cell>Soft Coral</cell><cell>559</cell><cell>0.115979</cell><cell>0.17808</cell><cell>0.12914</cell><cell>0.12865</cell><cell>0.12847</cell><cell>0.9</cell></row><row><cell>Gorgorian</cell><cell>23</cell><cell>0.118785</cell><cell>0.17700</cell><cell>0.12914</cell><cell>0.12866</cell><cell>0.12847</cell><cell>0.8</cell></row><row><cell>Sponge</cell><cell>514</cell><cell>0.116738</cell><cell>0.17759</cell><cell>0.12914</cell><cell>0.12866</cell><cell>0.12847</cell><cell>0.8</cell></row><row><cell>Barrel Sponge</cell><cell>138</cell><cell>0.117568</cell><cell>0.17726</cell><cell>0.12914</cell><cell>0.12866</cell><cell>0.12847</cell><cell>0.6</cell></row><row><cell>Fire Coral</cell><cell>66</cell><cell>0.117440</cell><cell>0.17760</cell><cell>0.12914</cell><cell>0.12866</cell><cell>0.12847</cell><cell>0.7</cell></row><row><cell>Algae</cell><cell>167</cell><cell>0.118422</cell><cell>0.17632</cell><cell>0.12914</cell><cell>0.12866</cell><cell>0.12847</cell><cell>0.8</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,90.02,577.68,432.78,9.94;9,90.02,590.28,432.69,9.94;9,90.02,602.88,327.95,9.94" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,474.75,577.68,48.06,9.94;9,90.02,590.28,334.01,9.94">Contextual techniques for classification of high and low resolution remote sensing data</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kartikeyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Gopalakrishna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">H</forename><surname>Kalubarme</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">L</forename><surname>Majumder</surname></persName>
		</author>
		<idno type="DOI">10.1080/01431169408954132</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,431.27,590.28,91.44,9.94;9,90.02,602.88,81.80,9.94">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1037" to="1051" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.02,615.60,433.11,9.94;9,90.02,628.20,432.94,9.94;9,90.02,640.92,384.15,9.94" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,201.24,628.20,178.04,9.94">Overview of ImageCLEFcoral 2019 task</title>
		<author>
			<persName coords=""><forename type="first">Jon</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wright</forename><surname>Jessica</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Clift</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Clark</forename><surname>Adrian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Seco</forename><surname>De Herrera</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="9,400.98,628.20,117.25,9.94">CLEF2019 Working Notes</title>
		<title level="s" coord="9,139.69,640.92,132.27,9.94">CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">2380</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.02,653.52,433.00,9.94;9,90.02,666.12,432.99,9.94;9,90.02,678.84,433.12,9.94;9,90.02,691.46,432.79,9.94;9,90.02,704.18,432.93,9.94;9,90.02,716.78,139.47,9.94" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,287.69,666.12,235.33,9.94;9,90.02,678.84,219.25,9.94">Overview of the ImageCLEFcoral 20201 Task: Coral Reef Image Annotation of a 3D environment</title>
		<author>
			<persName coords=""><forename type="first">Jon</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">García</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Moustahfid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hassan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,336.08,678.84,187.06,9.94;9,90.02,691.46,432.79,9.94;9,90.02,704.18,116.03,9.94">Proceedings of the 12th International Conference of the CLEF Association (CLEF 2021)</title>
		<title level="s" coord="9,314.64,704.18,208.31,9.94;9,90.02,716.78,24.69,9.94">Springer Lecture Notes in Computer Science LNCS</title>
		<meeting>the 12th International Conference of the CLEF Association (CLEF 2021)<address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">September 21-24, 2021</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="9,90.02,729.38,433.03,9.94;9,90.02,742.10,432.86,9.94;9,90.02,754.70,286.76,9.94" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,311.70,729.38,211.35,9.94;9,90.02,742.10,152.18,9.94">A Novel Image Classification Algorithm Using Overcomplete Wavelet Transforms</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Soe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tong</forename><surname>Myint</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Baojuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2015.2390133</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,265.58,742.10,201.16,9.94">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1232" to="1236" />
			<date type="published" when="2015-06">June 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.02,74.57,433.05,10.05;10,90.02,87.17,433.16,10.05;10,90.02,100.00,24.84,9.94" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,219.96,74.57,303.11,10.05;10,90.02,87.17,64.43,10.04">A Theory of Multiresolution Signal Decomposition: The Wavelet Representation</title>
		<author>
			<persName coords=""><forename type="first">Stephane</forename><surname>Georges</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mallat</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,167.50,87.17,252.60,10.05">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="674" to="693" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.02,112.60,432.92,9.94;10,90.02,125.21,433.05,10.05;10,90.02,137.81,433.23,10.05;10,90.02,150.52,135.08,9.94" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,209.88,125.21,313.20,10.05;10,90.02,137.81,19.34,10.04">Overview of the ImageCLEFcoral 2021 Annotation and Localisation Task</title>
		<author>
			<persName coords=""><forename type="first">Jon</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jessica</forename><forename type="middle">P</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louis</forename><forename type="middle">G</forename><surname>Clift</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Seco</forename><surname>De Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,124.33,137.81,269.57,10.04">CLEF2021 Working Notes, CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-09">September 2021</date>
		</imprint>
	</monogr>
	<note>CEUR-WS.org</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
