<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.67,84.32,367.05,16.17;1,88.76,106.24,90.50,16.17">AUEB NLP Group at ImageCLEFmed Caption Tasks 2021</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,136.79,107.68,10.37"><forename type="first">Foivos</forename><surname>Charalampakos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<addrLine>76, Patission Street</addrLine>
									<postCode>104 34</postCode>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,210.40,136.79,75.71,10.37"><forename type="first">Vasilis</forename><surname>Karatzas</surname></persName>
							<email>karatzas.basil@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<addrLine>76, Patission Street</addrLine>
									<postCode>104 34</postCode>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,299.52,136.79,73.35,10.37"><forename type="first">Vasiliki</forename><surname>Kougia</surname></persName>
							<email>kouyiav@aueb.gr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<addrLine>76, Patission Street</addrLine>
									<postCode>104 34</postCode>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Computer and Systems Sciences</orgName>
								<orgName type="department" key="dep2">DSV</orgName>
								<orgName type="institution">Stockholm University</orgName>
								<address>
									<addrLine>Postbox 7003</addrLine>
									<postCode>SE-164 07</postCode>
									<settlement>Kista</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,392.26,136.79,83.20,10.37"><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<addrLine>76, Patission Street</addrLine>
									<postCode>104 34</postCode>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Computer and Systems Sciences</orgName>
								<orgName type="department" key="dep2">DSV</orgName>
								<orgName type="institution">Stockholm University</orgName>
								<address>
									<addrLine>Postbox 7003</addrLine>
									<postCode>SE-164 07</postCode>
									<settlement>Kista</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,150.73,101.28,10.37"><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<addrLine>76, Patission Street</addrLine>
									<postCode>104 34</postCode>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.67,84.32,367.05,16.17;1,88.76,106.24,90.50,16.17">AUEB NLP Group at ImageCLEFmed Caption Tasks 2021</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">8A41BFC632D2B724D7241DD68ED60134</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical Images</term>
					<term>Concept Detection</term>
					<term>Image Captioning</term>
					<term>Image Retrieval</term>
					<term>Multi-label Classification</term>
					<term>Ensemble</term>
					<term>Convolutional Neural Network (CNN)</term>
					<term>Natural Language Processing</term>
					<term>Machine Learning</term>
					<term>Deep Learning</term>
					<term>Contrastive Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the systems that were implemented for the participation of AUEB's NLP Group in the sub-tasks of the 2021 ImageCLEFmed Caption task, which included a Concept Detection and a Caption Prediction sub-task. The goal of the Concept Detection sub-task was to identify medical terms that best describe each image, as a first step towards generating image captions or to improve the interpretation of medical images and help medical diagnosis. The objective of the Caption Prediction sub-task was to generate captions that describe medical images, which could assist medical experts in analyzing those images. The systems we implemented extend our previous work. For the Concept Detection sub-task, they employ convolutional neural network (CNN) image encoders, combined with an image retrieval module or a feed-forward neural network (FFNN) classifier. For the Caption Prediction we employed similar image encoders with image retrievals modules, and text generation models that either utilize the images or not. We ranked 1st in Concept Detection, and 2nd in Caption Prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>ImageCLEF <ref type="bibr" coords="1,147.20,464.82,12.87,9.46" target="#b0">[1]</ref> is an evaluation campaign that is annually organized as part of CLEF. 1 In the 2021 edition, ImageCLEF consisted of 4 main tasks with ImageCLEFmed being one of them. ImageCLEFmed consists of a series of tasks that are associated with the study of medical images. The ImageCLEFmed Caption Task <ref type="bibr" coords="1,252.08,505.47,12.86,9.46" target="#b1">[2]</ref> ran for the 5th year in 2021. It included a Concept Detection sub-task, that concerns multi-label classification of medical images by assigning medical terms (called concepts) to each image. The concepts stem from the Unified Medical Language System (UMLS) <ref type="bibr" coords="1,212.41,546.12,11.73,9.46" target="#b2">[3]</ref>. 2 Selecting the appropriate concepts can be a first step towards automatically generating coherent image captions and assisting the medical experts by reducing the time needed for the diagnosis. This year, ImageCLEFmed also included a Caption Prediction sub-task <ref type="bibr" coords="2,128.41,101.60,11.82,9.46" target="#b1">[2,</ref><ref type="bibr" coords="2,142.95,101.60,8.18,9.46" target="#b3">4,</ref><ref type="bibr" coords="2,153.85,101.60,7.88,9.46" target="#b4">5]</ref>, which was not included in the two previous years. This sub-task, that we also competed in, aims at assisting medical experts in interpreting radiology images, by automatically generating captions of the images. This kind of technology may eventually be able to generate draft diagnostic text from a medical image, which could help medical experts analyze more efficiently the large volumes of medical images (e.g., X-rays, MRI scans) they confront <ref type="bibr" coords="2,472.91,155.79,11.59,9.46" target="#b5">[6]</ref>.</p><p>In this paper, we describe the systems of the AUEB NLP Group that were submitted to the ImageCLEFmed Caption sub-tasks, which extend our previous work <ref type="bibr" coords="2,394.22,182.89,11.88,9.46" target="#b6">[7,</ref><ref type="bibr" coords="2,408.82,182.89,8.21,9.46" target="#b7">8,</ref><ref type="bibr" coords="2,419.75,182.89,8.21,9.46" target="#b8">9,</ref><ref type="bibr" coords="2,430.68,182.89,13.00,9.46" target="#b9">10]</ref>. For Concept Detection, our submissions were based on three methods. The first method extended the retrieval component of <ref type="bibr" coords="2,153.56,209.99,11.94,9.46" target="#b7">[8,</ref><ref type="bibr" coords="2,168.28,209.99,14.61,9.46" target="#b9">10]</ref> and consisted of an ensemble of several 1-NN systems that use different image encoders. The second method extended the TagCXN classification system of <ref type="bibr" coords="2,464.12,223.54,11.94,9.46" target="#b7">[8,</ref><ref type="bibr" coords="2,478.89,223.54,8.24,9.46" target="#b8">9,</ref><ref type="bibr" coords="2,489.95,223.54,13.05,9.46" target="#b9">10]</ref>, now using a ResNet-50 convolutional neural network (CNN) <ref type="bibr" coords="2,363.82,237.09,18.32,9.46" target="#b10">[11]</ref> to encode the images and a feed-forward neural network (FFNN) classifier on top. This year, we implemented a pre-training stage, where we trained the CNN encoder using supervised contrastive learning <ref type="bibr" coords="2,441.02,264.19,16.79,9.46" target="#b11">[12]</ref>. The third method consists of a combination of the previous two in which, for each test image, we used a similarity threshold to decide whether to employ the retrieval or the classification component. For Caption Prediction, we observed that image retrieval approaches were the best. We also tried text generation baselines that did not use information from the image and text generation models that did. For image retrieval, we used a 𝑘-NN model and experimented with different image encoders and ways of combining the captions returned. The Concept Detection task included the same images as the Caption Prediction task, so our team utilized the encoders trained for the Concept Detection task. As text generation baselines, we employed Bidirectional Encoder Representations from Transformers (BERT) <ref type="bibr" coords="2,282.44,386.13,16.65,9.46" target="#b12">[13]</ref>, Generative Pre-trained Transformer 2 (GPT-2) <ref type="bibr" coords="2,89.29,399.68,16.81,9.46" target="#b13">[14]</ref>, and GPT Neo, but the former one was dropped, because it was outperformed by the other two in early development stages. For the image-aware text generation models, we implemented both a simplistic architecture and an architecture based on Show, Attend and Tell <ref type="bibr" coords="2,434.01,426.78,16.56,9.46" target="#b14">[15]</ref>. <ref type="foot" coords="2,454.72,424.73,3.99,6.91" target="#foot_0">3</ref> Following the success of previous years <ref type="bibr" coords="2,223.10,440.33,11.94,9.46" target="#b7">[8,</ref><ref type="bibr" coords="2,238.24,440.33,8.24,9.46" target="#b8">9,</ref><ref type="bibr" coords="2,249.69,440.33,13.05,9.46" target="#b9">10]</ref>, our best performing systems were ranked 1st among the best performing systems of 5 participating teams in Concept Detection, and 2nd among the best performing systems of 8 participating teams in Caption Prediction. The rest of the paper provides insights about the dataset, a description of the methods we used, experimental results and discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Data</head><p>The ImageCLEFmed Caption 2021 dataset includes real clinical radiology images from original medical cases that were annotated by medical doctors. The organizers state that the dataset is the same with the one that was used in the ImageCLEF-VQAMed 2020 task <ref type="bibr" coords="2,407.93,581.58,16.71,9.46" target="#b15">[16]</ref>. The images stem from the Med-Pix database. <ref type="foot" coords="2,207.89,593.08,3.99,6.91" target="#foot_1">4</ref> Additionally, a subset of the Radiology Objects in COntext (ROCO) dataset <ref type="bibr" coords="2,122.02,608.67,18.10,9.46" target="#b16">[17]</ref> is provided for training purposes <ref type="bibr" coords="2,286.52,608.67,11.51,9.46" target="#b1">[2]</ref>. The ROCO dataset consists of medical images extracted from open access biomedical journal articles of PubMed Central. <ref type="foot" coords="2,413.55,620.18,3.99,6.91" target="#foot_2">5</ref> The same dataset is used for the Caption Prediction sub-task. There are several different modalities present in the dataset (e.g., X-rays, CT-Scans), but unlike the previous year, this year's dataset was not provided classified into different modality categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Concept Detection</head><p>All images in the dataset are accompanied by the unique identifier (CUI) of the UMLS <ref type="bibr" coords="3,493.12,563.81,12.86,9.46" target="#b2">[3]</ref> concepts. These concepts, essentially medical terms, were extracted from the processed text of the respective image caption. An image can be associated with multiple CUIs (see <ref type="bibr" coords="3,451.41,590.90,25.21,9.46">Fig 1)</ref>. Also, Table <ref type="table" coords="3,118.96,604.45,5.56,9.46" target="#tab_0">1</ref> shows the 5 most frequent concepts of the training set and how many training images they were assigned to. <ref type="foot" coords="3,189.66,615.96,3.99,6.91" target="#foot_3">6</ref> It has to be noted that, although the dataset is imbalanced (e.g., 632 concepts appear only one time, 1 concept appears 1,400 times), there is no concept appearing in  every image, unlike in the last year's dataset. The number of unique concepts was reduced once again compared to previous years. There were 111,156 possible concepts in 2018 <ref type="bibr" coords="4,264.68,449.19,16.65,9.46" target="#b17">[18]</ref>, 5,528 in 2019 <ref type="bibr" coords="4,350.75,449.19,11.52,9.46" target="#b7">[8]</ref>, 3,047 in 2020 <ref type="bibr" coords="4,431.36,449.19,18.11,9.46" target="#b9">[10]</ref> and 1,585 in 2021. The average number of concepts assigned to each image was 3.48. The minimum number of concepts assigned to an image was 1 (found in 10 images), and the maximum was 12 (found in 2 images). Fig. <ref type="figure" coords="4,158.78,489.84,5.45,9.46" target="#fig_1">2</ref> shows the distribution of the assigned concepts.</p><p>A training set of 2,756 images and a validation set of 500 images were provided. A separate test set comprised of 444 images and the concepts for the test images were unknown. For our experiments, we merged the provided training and validation sets and used 10% of the merged data as our validation set, and another 10% of the merged data as our development set in which we evaluated the performance of our models. The remaining 80% served as the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Caption Prediction</head><p>The maximum number of words in a caption was 43 (in 10 images), while the minimum number of words was 1 (also in 10 images). The distribution of the number of words per image caption can be seen in Figure <ref type="figure" coords="4,186.98,635.50,4.17,9.46" target="#fig_2">3</ref>. The total number of distinct words, after lower-casing, was 3,515. In Table <ref type="table" coords="4,116.00,649.05,5.53,9.46" target="#tab_1">2</ref> we can see the most common words in the merged dataset (training and validation sets combined) and in Table <ref type="table" coords="4,196.76,662.59,5.55,9.46">3</ref> the most common captions. Out of the 3,256 captions of the dataset, 1,141 (about 35%) were duplicates (existed, exactly the same, as another image's caption), which indicated that retrieval-based approaches would probably be advantageous. For the calculation of the final score, which is calculated with the BLEU-4 metric, the organizers mention that the captions (both the predicted, and the gold ones) pass through these preprocessing steps:</p><p>1. Each text is lower-cased. 2. Punctuation is removed and the text is tokenised. <ref type="foot" coords="5,329.89,540.51,3.99,6.91" target="#foot_4">7</ref>3. Stopwords are removed, using the NLTK "english" stopword list. 4. Stemming is applied, using the Snowball Stemmer from NLTK. <ref type="foot" coords="5,394.68,570.32,3.99,6.91" target="#foot_5">8</ref>From now on, whenever we mention the preprocessing we will actually refer to bullets 2-4. This preprocessing raises a question: should we train our models on the original text or perform the preprocessing before the training? The second approach seems to be away from our main goal which is to help medical experts by giving them comprehensive captions that describe the Table <ref type="table" coords="6,116.88,90.03,5.54,9.35">3</ref> The 5 most common captions of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Caption</head><p>Occurrences fusion of multiple disc spaces squaring of the vertebral bodies fusion of si joints 14 extensive white matter lesions involving both cerebral hemispheres 11 fracture through the left c4 lateral mass and laminar arch with unilateral perched c45 facets on the right herniated and disrupted disk c45 torn intracapsular ligaments and ligament flavum 11 multilevel vertebral body lesions which are low signal on t1 and t2 scan sequences mediastinal adenopathy and perihilar nodular infiltrates on ct of chest 11 traumatic dislocation cervical spine at c1c2 level with marked widening of disc space and facet joints soft tissue edema anterior to spine and in posterior paraspinal locations edema and hemorrhage noted in lower medulla and upper cervical cord 10</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4</head><p>Example caption without (1st row) and with preprocessing (2nd row).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NORMAL</head><p>focal oval of fusiform activity along the medial aspect of the bilateral tibia PREPROCESSED focal oval fusiform activ along medial aspect bilater tibia images. By predicting already preprocessed sentences, we may lose semantic value (see Table <ref type="table" coords="6,495.74,377.77,3.87,9.46">4</ref>), thus one could argue that the preprocessing before the evaluation and the metric used for such a task should not reward models that generate preprocessed captions.</p><p>As in the other sub-task, we merged the given training and validation sets. We then used 60% of the merged data as our training set; 20% as our development set, to tune any hyperparameters; and 20% as our validation set, to test our models to decide which one we should submit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>In this section, we describe the systems that were used in our submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Concept Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">System 1: 2xCNN+FFNN</head><p>This system constitutes an extension of our previous work <ref type="bibr" coords="6,352.37,577.20,18.33,9.46" target="#b9">[10]</ref> for the same task. Last year's implementation was an ensemble of two instances of a classifier that employed a DenseNet-121 <ref type="bibr" coords="6,89.29,604.30,18.12,9.46" target="#b18">[19]</ref> backbone encoder pre-trained on ImageNet <ref type="bibr" coords="6,300.65,604.30,16.66,9.46" target="#b19">[20]</ref>. The classifier was fine-tuned on the task's data and the two instances were combined using the UNION and the INTERSECTION of the predicted concepts. The ensemble that used the INTERSECTION was ranked 1st in 2020.</p><p>This year, a pre-training step that used supervised contrastive learning was integrated in the pipeline. Similarly to <ref type="bibr" coords="6,187.54,658.50,16.88,9.46" target="#b11">[12]</ref>, we used a ResNet-50 <ref type="bibr" coords="6,308.91,658.50,18.32,9.46" target="#b10">[11]</ref> with weights initialized from ImageNet <ref type="bibr" coords="7,89.29,88.05,18.07,9.46" target="#b19">[20]</ref> as our backbone encoder, which mapped each image to a dense representation vector of size 𝐷 𝐸 = 2048, and normalized it to the unit hypersphere before passing it through a projection (a single dense layer) of size 𝐷 𝑃 = 128. We trained the model for 300 epochs using supervised contrastive loss with a temperature value of 𝜏 = 0.1 and Adam <ref type="bibr" coords="7,384.70,128.69,18.32,9.46" target="#b20">[21]</ref> with its default hyperparameters as the optimizer. For the contrastive pre-training, we created a larger augmented version of the training set where, for each image, we added four additional variations: we split the image in four horizontal patches and applied Gaussian blur in each patch separately, resulting in four extra noisy images. Furthermore, the pre-training data included random horizontal flip and random rotation. <ref type="foot" coords="7,178.74,194.39,3.99,6.91" target="#foot_6">9</ref> Contrastive pre-training aimed to bring visual representations belonging in the same class closer together than representations from different classes.</p><p>At the end of the pre-training phase, we discarded the projection layer, froze the encoder, and added a dense layer on top of it with |𝐶| outputs, where 𝐶 is the number of all possible concepts. We trained the dense layer by minimizing the binary cross entropy loss. We once again used Adam <ref type="bibr" coords="7,170.52,264.19,18.31,9.46" target="#b20">[21]</ref> as our optimizer and decayed the learning rate by a factor of 10 when the loss showed no improvement, as in <ref type="bibr" coords="7,265.91,277.74,11.94,9.46" target="#b7">[8,</ref><ref type="bibr" coords="7,280.86,277.74,13.05,9.46" target="#b9">10]</ref>. We also used early stopping on the validation set, with patience of 3 epochs <ref type="bibr" coords="7,223.13,291.28,11.93,9.46" target="#b7">[8,</ref><ref type="bibr" coords="7,237.78,291.28,13.04,9.46" target="#b9">10]</ref>. A classification threshold for all the concepts was tuned by optimizing the F1 score. We used the same threshold for all the concepts. Any concepts for which the respective output values exceeded that threshold, were assigned to the corresponding image during inference.</p><p>Following <ref type="bibr" coords="7,148.64,345.48,16.88,9.46" target="#b9">[10]</ref>, we trained 5 models using cross-validation and kept the 2 best performing ones, according to their F1 score. We then created an ensemble, using the UNION of the concepts returned by these two models. Hereafter, this ensemble will be called 2xCNN+FFNN@U. We also considered an ensemble with the INTERSECTION of the concepts that will be called 2xCNN+FFNN@I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">System 2: 1-NN ensemble</head><p>In this system, we followed a retrieval approach, extending the work of our previous systems in <ref type="bibr" coords="7,101.66,463.49,11.95,9.46" target="#b6">[7,</ref><ref type="bibr" coords="7,117.33,463.49,8.24,9.46" target="#b7">8,</ref><ref type="bibr" coords="7,129.29,463.49,8.24,9.46" target="#b8">9,</ref><ref type="bibr" coords="7,141.24,463.49,13.05,9.46" target="#b9">10]</ref>. We employed four different CNN encoders. One of the four encoders was the encoder used in System 1 (see Section 3.1.1), whereas the rest were a ResNet-50 <ref type="bibr" coords="7,476.42,477.04,16.88,9.46" target="#b10">[11]</ref>, a DenseNet-201 <ref type="bibr" coords="7,153.82,490.59,16.57,9.46" target="#b18">[19]</ref>, and an EfficientNet-B0 <ref type="bibr" coords="7,279.07,490.59,16.57,9.46" target="#b21">[22]</ref>, all of which were pre-trained on ImageNet <ref type="bibr" coords="7,487.95,490.59,18.04,9.46" target="#b19">[20]</ref> and fine-tuned on our training set in a purely supervised setting. <ref type="foot" coords="7,372.88,502.09,7.97,6.91" target="#foot_7">10</ref> For each encoder, we again trained 5 models using cross-validation, resulting in a number of 20 models in total. Having fine-tuned all the encoders, we used each one of them to obtain dense vector encodings, called image embeddings, of all the training images. The image embeddings are extracted from the last average pooling layer of the encoders. Following <ref type="bibr" coords="7,306.71,558.34,11.61,9.46" target="#b7">[8]</ref>, we tuned the value of 𝑘 in the range from 1 to 200 for each encoder separately, using the validation set, which led to 𝑘 = 1. Therefore, given a test image, we retrieved the training image with the highest cosine similarity (computed on image embeddings) to the test image, resulting in a total of 20 images, one retrieved from each encoder. In order to make the submissions, we considered four different strategies for the concepts assignment. We submitted four systems using 20 encoders, one for each assignment strategy. We also submitted two additional 1-NN ensemble systems where we used a smaller number of models. In the first, we used four different encoders (i.e., the best out of the 5 models per encoder). In the second, we used three different encoders (discarding the encoder that yielded the lowest F1 score on the validation set). Our submitted systems will be discussed in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">System 3: Combination of 1-NN and 2xCNN+FFNN</head><p>This system is a combination of a retrieval and a classification method. We used the encoder that was trained in System 1 (see Section 3.1.1) in order to obtain the image embeddings for all training images as in System 2 (see Section 3.1.2). During inference, we retrieved the training image with the highest cosine similarity to the query image embedding and applied a similarity threshold 𝑠. If the similarity exceeded that threshold, the concepts of the closest neighbor were assigned to the query image. If not, we fed the query image to the classification component and assigned the predicted concepts to it. We tuned the value of 𝑠 in the interval [0.65, 1] with 𝑠𝑡𝑒𝑝 = 0.01 which led to 𝑠 = 0.8.</p><p>We also experimented with a variation where the 1-NN retrieval component was replaced by a 1-NN ensemble as in System 2 (see Section 3.1.2) using the version with the four different encoders. For each encoder, we retrieved one training image. If the similarity of each retrieved image to the query image exceeded the similarity threshold 𝑠 = 0.8, the 1-NN ensemble was used for the assignment of the concepts. Otherwise, we fed the query image to the 2xCNN+FFNN@U ensemble and used its predictions for the concept assignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Caption Prediction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Text Generation Baselines</head><p>We trained each text generation baseline as a language model, on all the captions of the training set. Then, we simply used it to generate text, disregarding the image of the respective example. That is, these baselines generate a likely text per image, without actually considering the image. The training set for these models was preprocessed, since we observed that this led to better results. We used beam search, with the beam size for each step equal to 3. The baseline models we trained were GPT-2 <ref type="bibr" coords="8,192.71,481.17,18.17,9.46" target="#b13">[14]</ref> and GPT Neo.<ref type="foot" coords="8,276.33,479.13,7.97,6.91" target="#foot_8">11</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 5</head><p>The hyperparameters of the text generation baseline models. The batch size for GPT-2 and GPT Neo is dictated by their architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Huggingface The hyperparameters we used for each model, after light tuning, can be seen in Table <ref type="table" coords="8,484.29,624.02,4.17,9.46">5</ref>. In all our models, it was better to merge the training captions. An example can be seen in Table <ref type="table" coords="8,499.09,637.57,4.13,9.46">6</ref>, where instead of feeding the model one caption at a time (and padding or truncating the input based on its length), we merged all the captions and then created equally-sized inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 6</head><p>Two approaches (I, II) of creating the training input for the text generation baselines. Assuming a toy dataset with two captions and models with a maximum length of eight tokens. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Image-Aware Text Generation</head><p>We experimented with two image captioning architectures that utilize the image. One was inspired by Show, Attend and Tell <ref type="bibr" coords="9,202.25,318.89,16.69,9.46" target="#b14">[15]</ref>, and its architecture can be seen in Figure <ref type="figure" coords="9,406.32,318.89,4.07,9.46" target="#fig_4">4</ref>. <ref type="foot" coords="9,414.46,316.85,7.97,6.91" target="#foot_9">12</ref> The second model was a token classifier, with image, and previous text inputs. <ref type="foot" coords="9,346.84,330.39,7.97,6.91" target="#foot_10">13</ref> Models of the second architecture do not predict full sentences, but rather the next token for a given unfinished sentence and an image. In other words, a tuple consisting of an image and a caption of 𝑛 words, will be used to create 𝑛 different image-text inputs, with the text length varying from 1 to 𝑛 (after adding a start token to the caption). The input images are passed through an encoder, then dropout is applied with 0.5 probability, and finally a dense layer yields the image representation. The text's words are transformed to word embeddings through a trainable embedding layer, then a dropout of 0.5 probability is applied, and the word embeddings are given to a Long Short-Term Memory (LSTM). The image representation and the text encoding (from the last output of the LSTM) are concatenated and then passed through a multilayer perceptron (MLP) with one hidden layer. In early stages of development, we saw that the model was predicting the next token correctly about 8% of the times (in the validation data), so later development stages only considered the Show, Attend and Tell inspired model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Retrieval Methods</head><p>Following our previous work <ref type="bibr" coords="9,221.36,545.45,11.94,9.46" target="#b6">[7,</ref><ref type="bibr" coords="9,236.08,545.45,8.24,9.46" target="#b7">8,</ref><ref type="bibr" coords="9,247.09,545.45,13.05,9.46" target="#b9">10]</ref>, we implemented a retrieval approach, based on 𝑘-NN. First, a pre-trained encoder generated an embedding for each training image. The images were reshaped beforehand according to the encoder used (Table <ref type="table" coords="9,361.62,572.55,4.02,9.46" target="#tab_3">7</ref>). During inference, the same encoder generated an embedding for the test image, and the 𝑘 training images with the most similar embeddings were retrieved (we used cosine similarity). The captions of the retrieved embedded images were then combined to yield a caption for the test image. We experimented both with 𝑘 = 1 and 𝑘 ̸ = 1. For the latter, we split each retrieved caption into sentences and then concatenated the 𝑟 most frequent sentences (the most frequent first) to form a caption. Alternatively, we summarized the 𝑘 retrieved captions using an off-the-shelf summarizer. <ref type="foot" coords="10,469.80,568.53,7.97,6.91" target="#foot_11">14</ref> 1-NN models performed best, but we plan to further investigate the effect of summarization in the future, and more specifically by using a model that utilizes Bidirectional and Auto-Regressive Transformers (BART), <ref type="bibr" coords="10,187.95,611.22,16.56,9.46" target="#b23">[24]</ref>, inspired by its application in Retrieval-Augmented Generation (RAG) <ref type="bibr" coords="10,89.29,624.77,16.72,9.46" target="#b24">[25]</ref>. The 1-NN models were outperforming other competing methods, so we focused on experi- menting with pre-trained encoders to better represent the images. We used encoders from the trained models of Section 3.1.1 (we named them Tag-Trained encoders), and ensembles of 1-NNs with different encoders. Since combining the predictions did not seem to work well, for the ensembles we just predicted the caption that most of the 1-NN methods of the ensemble predicted for each image. If every prediction was different for an image, i.e., if all the 1-NNs disagreed on it, we either used the prediction of the best 1-NN model (evaluated on validation data), or the prediction of the GPT-2 text generation model. GPT-2 was our next best model, after 1-NNs, but we observed that it always generated the same sentence (not one that appears in the dataset though).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Submissions and results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Concept Detection</head><p>We used our development set to evaluate all our models and submitted those that performed best. Six out of our ten submissions used the 1-NN ensemble (System 2). We considered four different concept assignment strategies and made four submissions, one for each strategy, using an ensemble of twenty encoders:</p><p>• MAJORITY VOTING: For each concept, we count how many of the 20 images(one retrieved per encoder) are associated with it. If the concept is present in the majority (i.e., more than 10), we assign it to the test image. We will refer to this system as 1-NN@20xMJ. • UNION OF INTERSECTIONS: We used the INTERSECTION of the concepts of the 5 images that were retrieved from the 5 models that used the same backbone encoder. Then, we used the UNION of the above INTERSECTION. We will refer to this system as 1-NN@20xUoI. • INTERSECTION OF UNIONS: Here, we reversed the calculation. We used the UNION of the concepts of the 5 images that were retrieved from the 5 models that used the same backbone encoder and then used the INTERSECTION of the UNION. This system will be referred to as 1-NN@20xIoU.</p><p>• UNION OF UNIONS: We used the UNION of the concepts of the 5 images that were retrieved from the 5 models that used the same backbone encoder and then used the UNION of the UNION. This system will be referred to as 1-NN@20xUoU.</p><p>The two additional 1-NN ensemble submissions, that used three and four encoders respectively, were made using the MAJORITY VOTING strategy. For the concept assignment, we used 2 and 3 votes as the majority for the three and four encoders respectively. We will refer to these systems as 1-NN@3xMJ and 1-NN@4xMJ.</p><p>Three submissions were based on the combination of a retrieval and a classification method (System 3). In the first submission, the classification component was the 2xCNN+FFNN@U ensemble, while in the second submisson we used the 2xCNN+FFNN@I as the classifier. Both of these methods used 1-NN as the retrieval component. The third submission, discussed in Section 3.1.3, was the variation that used a 1-NN ensemble as the retrieval component and the 2xCNN+FFNN@U as the classifier.</p><p>Additionally, one submission was made using an ensemble classification system (System 1). We only submitted the 2xCNN+FFNN@U system as it performed better than 2xCNN+FFNN@I in our development set.</p><p>The official measure of the competition was F1, calculated by comparing the binary multihot vectors 𝑦 𝑡𝑟𝑢𝑒 and 𝑦 𝑝𝑟𝑒𝑑 of each test image and then macro-averaging over all test images. To generate the predictions for the test set, we merged the training with the development set. We used a held-out set (15% of the merged data) to tune the threshold of the 2xCNN+FFNN model. This resulted in the values 𝑡 1 = 0.268 and 𝑡 2 = 0.304. Table <ref type="table" coords="12,128.15,616.47,5.56,9.46" target="#tab_4">8</ref> presents the scores of our systems on the development and the official test set, as well their position according to the official ranking. 1-NN@20XMJ had the best results on the development and test set. As noted in Section 2, the number of the unique concepts present in the data is 1,585. Furthermore, the number of the total assigned concepts is 11,361. There is a large overlap between the assigned concepts, which means that many images have almost, or completely, the same concepts. This fact can explain why our retrieval approaches worked very well in this task. Furthermore, the reduced (in comparison with previous years) number of unique concepts and the slight improvement regarding the class imbalance, probably help the systems achieve better results. This is indicated by the increasing score of the winning systems (see Fig. <ref type="figure" coords="13,456.80,365.71,4.63,9.46" target="#fig_5">5</ref>) and also supported by the low performance of the MOST FREQUENT BASELINE on our development set. This baseline assigned to every test image the same three (as the average number of concepts is 3.48) most frequent concepts of the training data (see Table <ref type="table" coords="13,351.09,406.36,3.94,9.46" target="#tab_4">8</ref>).</p><p>We also have to note that we only made use of the data that was provided by the organizers, without using external medical datasets to avail our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Caption Prediction</head><p>The official evaluation measure for caption prediction was BLEU-4, so we evaluated our models using that measure on our development set to decide which ones we should submit (each participant was allowed at most 10 submissions). For the 1-NN submissions, we combined the training, validation and development sets, since there were no hyperparameters to tune for those models. In Table <ref type="table" coords="13,191.52,538.07,4.11,9.46" target="#tab_5">9</ref>, we can see the scores on the development set, and in Table <ref type="table" coords="13,461.62,538.07,10.95,9.46" target="#tab_6">10</ref> we can see the final scores of our submitted models in the unknown test captions. Some interesting points to be made are the following.</p><p>1. Encoders do not always benefit from an increase in their architecture complexity. We see that some of the 1-NN encoders actually scored higher in our development set when they had fewer trainable parameters. 2. The encoders that were further trained for the Concept Detection task (where our team took the first place) did not seem to have any significant lead over the others (our best model still remained an Ensemble without the Tag-Trained encoders). 3. Surprisingly, the models that were not aware of the image were better than the image-aware models (an image-aware model was even dropped, as mentioned in Section 3.2.2, due to very low scores). Could this mean that the task and data made it difficult to extract information from the images, and thus the images became noise, or were the image-aware models we tested not good enough? Due to time constraints, we couldn't investigate this issue further, but we intend to do so in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and future work</head><p>This article described the submissions of AUEB's NLP Group to the 2021 ImageCLEFmed Caption sub-tasks. In the Concept Detection sub-task, our top system, ranked 1st amongst all submissions of 5 teams. It was a 1-NN retrieval-based ensemble using majority voting and several different image encoders. Our submissions also included an ensemble of classifiers trained using supervised contrastive learning <ref type="bibr" coords="15,229.64,242.85,16.86,9.46" target="#b11">[12]</ref>, as well as combinations of the retrieval and classification modules that were implemented. In the Caption Prediction sub-task, by mainly focusing on retrieval methods, we managed to take the 2nd place amongst the 8 competing teams. Although we tried different approaches, including both image-aware and image-unaware generation models, nothing was able to beat our retrieval models, showing how powerful they can be, as they also helped us win the Concept Detection sub-task of the same campaign in previous years. Finally, we also observed that encoders do not always benefit from larger architectures with more trainable parameters.</p><p>In future work, we aim to assess our models on additional medical datasets and experiment more with retrieval-based methods that have proved promising. Our future plans also include a further analysis of image-aware captioning models, with the addition of image-aware pretrained Transformer models like GPT-2 and BERT, which we are currently working on. We also want to improve our retrieval models for captioning by studying methods of summarizing or combining text. Through these model experiments, and the analysis of more datasets, our research on diagnostic captioning <ref type="bibr" coords="15,185.65,432.53,12.72,9.46" target="#b5">[6]</ref> will also continue.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,434.89,416.69,9.35;3,89.29,446.94,82.34,9.22;3,89.29,84.19,416.72,332.50"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Three images of the dataset (1st row) with their corresponding tags (2nd row) and captions (3rd row).</figDesc><graphic coords="3,89.29,84.19,416.72,332.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.29,240.35,243.69,9.35;4,188.25,84.19,216.00,144.00"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Distribution of assigned concepts in the data.</figDesc><graphic coords="4,188.25,84.19,216.00,144.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,89.29,248.29,416.69,9.35;5,89.29,260.34,382.55,9.22;5,89.29,84.19,416.70,145.96"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: On the left, we show a histogram of the number of images that contain captions of a specific length. On the right, we show a boxplot of the number of words in the captions.</figDesc><graphic coords="5,89.29,84.19,416.70,145.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,232.07,168.73,151.24,9.35;9,210.48,180.68,194.42,9.35;9,124.61,199.43,3.88,8.96;9,166.41,193.74,282.56,8.64;9,143.03,205.70,329.33,8.64;9,122.67,223.74,7.75,8.96;9,144.01,218.05,327.37,8.64;9,162.55,230.01,290.28,8.64"><head>Caption 1 :</head><label>1</label><figDesc>'Left Upper lobe mass' Caption 2: 'Duplicated Right Renal System' I INPUT 1: [START], LEFT, UPPER, LOBE, MASS, [PAD], [PAD], [PAD] INPUT 2:[START], DUPLICATED, RIGHT, RENAL, PHRASE, [PAD], [PAD], [PAD] II INPUT 1: [START], LEFT, UPPER, LOBE, MASS, [START], DUPLICATED, RIGHT INPUT 2: RENAL, SYSTEM, [PAD], [PAD], [PAD], [PAD], [PAD], [PAD]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="10,89.29,492.35,416.70,9.35;10,89.29,504.41,344.15,9.22;10,89.29,84.19,416.69,390.02"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The architecture of the Show, Attend and Tell inspired model. The image encoder we used was InceptionV3 and the image attention mechanism was based on [23].</figDesc><graphic coords="10,89.29,84.19,416.69,390.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="12,89.29,573.66,416.69,9.35;12,89.29,585.72,138.27,9.22;12,192.08,411.22,208.35,156.26"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The top F1 score achieved each year in the Concept Detection sub-task of the ImageCLEFmed Caption tasks.</figDesc><graphic coords="12,192.08,411.22,208.35,156.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.98,273.53,417.00,110.99"><head>Table 1</head><label>1</label><figDesc>The 5 most frequent concepts (CUIs) in the training set of ImageCLEFmed Caption 2021 and how many training images they are assigned to.</figDesc><table coords="4,165.95,313.73,263.37,70.79"><row><cell>CUI</cell><cell>UMLS term</cell><cell>Images</cell></row><row><cell cols="2">C0040398 TOMOGRAPHY, EMISSION-COMPUTED</cell><cell>1,400</cell></row><row><cell>C0024485</cell><cell>MAGNETIC RESONANCE IMAGING</cell><cell>796</cell></row><row><cell>C1306645</cell><cell>PLAIN X-RAY</cell><cell>627</cell></row><row><cell>C0041618</cell><cell>ULTRASONOGRAPHY</cell><cell>373</cell></row><row><cell>C0009924</cell><cell>CONTRAST MEDIA</cell><cell>283</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,88.82,340.22,419.11,110.44"><head>Table 2</head><label>2</label><figDesc>The 10 most common words found in the captions of the (whole) dataset, w/ and w/o stopwords. With stopwords, there are 1,071 words with only 1 occurrence.</figDesc><table coords="5,94.71,380.01,405.86,70.65"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Most common words w/ stopword</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Word</cell><cell>the</cell><cell>of</cell><cell>with</cell><cell>and</cell><cell>a</cell><cell>right</cell><cell>in</cell><cell>left</cell><cell>to</cell><cell>mass</cell></row><row><cell cols="5">Occurrences 2,139 1,770 1,179 1,149</cell><cell>891</cell><cell>800</cell><cell>763</cell><cell>666</cell><cell>630</cell><cell>621</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Most common words w/o stopwords</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Word</cell><cell>right</cell><cell>left</cell><cell>mass</cell><cell>ct</cell><cell cols="6">demonstrates axial images image contrast within</cell></row><row><cell>Occurrences</cell><cell>800</cell><cell>666</cell><cell>621</cell><cell>616</cell><cell>511</cell><cell>451</cell><cell>385</cell><cell>379</cell><cell>365</cell><cell>302</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,88.98,90.03,373.19,159.97"><head>Table 7</head><label>7</label><figDesc>Image encoders and image input shapes in the image-aware text generation models.</figDesc><table coords="11,222.39,118.27,150.50,131.74"><row><cell>Model</cell><cell>Input Shape</cell></row><row><cell>EfficientNetB0</cell><cell>224x224</cell></row><row><cell>EfficientNetB7</cell><cell>600x600</cell></row><row><cell>DenseNet121 DenseNet201</cell><cell>224x224</cell></row><row><cell>InceptionV3</cell><cell>299x299</cell></row><row><cell>ResNet50 ResNet152V2</cell><cell>224x224</cell></row><row><cell>NASNetLarge</cell><cell>331x331</cell></row><row><cell>InceptionResNetV2</cell><cell>299x299</cell></row><row><cell>Xception</cell><cell>299x299</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="13,88.98,90.03,369.10,183.91"><head>Table 8</head><label>8</label><figDesc>The results and rankings of our systems on the development and test set.</figDesc><table coords="13,140.52,118.27,317.56,155.68"><row><cell>ID</cell><cell>Approach</cell><cell cols="2">F1 Score Development Test</cell><cell>Rank</cell></row><row><cell>cd1</cell><cell>1-NN@20XMJ</cell><cell>61.99</cell><cell>50.5</cell><cell>1</cell></row><row><cell>cd2</cell><cell>1-NN@20XUOI</cell><cell>55.73</cell><cell>45.6</cell><cell>9</cell></row><row><cell>cd3</cell><cell>1-NN@20XIOU</cell><cell>60.85</cell><cell>49.5</cell><cell>2</cell></row><row><cell>cd4</cell><cell>1-NN@20XUOU</cell><cell>43.33</cell><cell>34.8</cell><cell>23</cell></row><row><cell>cd5</cell><cell>1-NN@4XMJ</cell><cell>59.08</cell><cell>49.3</cell><cell>3</cell></row><row><cell>cd6</cell><cell>1-NN@3XMJ</cell><cell>60.76</cell><cell>49.0</cell><cell>5</cell></row><row><cell>cd7</cell><cell>2XCNN+FFNN@U</cell><cell>57.24</cell><cell>45.9</cell><cell>8</cell></row><row><cell>cd8</cell><cell>1-NN/2XCNN+FFNN@U</cell><cell>59.53</cell><cell>46.6</cell><cell>7</cell></row><row><cell>cd9</cell><cell>1-NN/2XCNN+FFNN@I</cell><cell>59.46</cell><cell>45.1</cell><cell>11</cell></row><row><cell cols="2">cd10 1-NN@4XMJ/2XCNN+FFNN@U</cell><cell>59.08</cell><cell>49.3</cell><cell>4</cell></row><row><cell>cd11</cell><cell>MOST FREQUENT BASELINE</cell><cell>27.92</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="14,88.98,90.03,359.39,356.72"><head>Table 9</head><label>9</label><figDesc>The scores of all of our Caption Prediction systems on our development set.</figDesc><table coords="14,146.91,118.15,301.47,328.60"><row><cell>ID</cell><cell>Approach</cell><cell>BLEU-4 Score</cell></row><row><cell>cp1</cell><cell>GPT-2 (117M parameters)</cell><cell>34.923</cell></row><row><cell>cp2</cell><cell>GPT Neo (125M parameters)</cell><cell>25.540</cell></row><row><cell>cp3</cell><cell>Show, Attend and Tell inspired</cell><cell>20.471</cell></row><row><cell>cp4</cell><cell>DenseNet121 1-NN</cell><cell>51.405</cell></row><row><cell>cp5</cell><cell>DenseNet201 1-NN</cell><cell>52.755</cell></row><row><cell>cp6</cell><cell>ResNet50 1-NN</cell><cell>52.256</cell></row><row><cell>cp7</cell><cell>ResNet152V2 1-NN</cell><cell>42.120</cell></row><row><cell>cp8</cell><cell>InceptionV3 1-NN</cell><cell>49.342</cell></row><row><cell>cp9</cell><cell>InceptionResNetV2 1-NN</cell><cell>49.250</cell></row><row><cell>cp10</cell><cell>Xception 1-NN</cell><cell>48.963</cell></row><row><cell>cp11</cell><cell>NASNetLarge 1-NN</cell><cell>45.728</cell></row><row><cell>cp12</cell><cell>EfficientNetB0 1-NN</cell><cell>51.747</cell></row><row><cell>cp13</cell><cell>EfficientNetB7 1-NN</cell><cell>51.099</cell></row><row><cell>cp14</cell><cell>Tag-Trained ResNet50 1-NN</cell><cell>50.988</cell></row><row><cell>cp15</cell><cell>Tag-Trained DenseNet201 1-NN</cell><cell>53.381</cell></row><row><cell>cp16</cell><cell>Tag-Trained EfficientNetB0 1-NN</cell><cell>52.641</cell></row><row><cell>cp17</cell><cell>Ensemble of cp5, cp8 and cp10</cell><cell>53.634</cell></row><row><cell>cp18</cell><cell>Ensemble of cp4, cp5, cp8, cp9 and cp10</cell><cell>54.153</cell></row><row><cell>cp19</cell><cell>Ensemble of cp4, cp5, cp8, cp9 and cp10 GPT-2 on non-Agreement</cell><cell>55.342</cell></row><row><cell>cp20</cell><cell>Ensemble of cp14, cp15 and cp16 GPT-2 on non-Agreement</cell><cell>54.877</cell></row><row><cell>cp21</cell><cell>Ensemble of cp5, cp6, cp14, cp15 and cp16 GPT-2 on non-Agreement</cell><cell>55.023</cell></row><row><cell>cp22</cell><cell>cp23 with 2 most frequent sentences instead of most frequent caption</cell><cell>51.161</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="14,88.98,478.36,417.00,135.15"><head>Table 10</head><label>10</label><figDesc>The final scores of our 6 submissions, along with their rank on the test set based on submissions from all participants.</figDesc><table coords="14,220.83,518.43,156.94,95.07"><row><cell>ID</cell><cell cols="2">BLEU-4 Score Development Test</cell><cell>Rank</cell></row><row><cell>cp19</cell><cell>55.342</cell><cell>46.1</cell><cell>3</cell></row><row><cell>cp21</cell><cell>55.023</cell><cell>45.2</cell><cell>4</cell></row><row><cell>cp22</cell><cell>52.161</cell><cell>44.8</cell><cell>5</cell></row><row><cell>cp17</cell><cell>53.634</cell><cell>44</cell><cell>7</cell></row><row><cell>cp4</cell><cell>51.405</cell><cell>37.5</cell><cell>18</cell></row><row><cell>cp3</cell><cell>20.471</cell><cell>19.9</cell><cell>38</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,108.93,649.61,246.47,7.77"><p>https://huggingface.co/transformers/master/model_doc/gpt_neo.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="2,108.93,660.76,99.76,7.77"><p>https://medpix.nlm.nih.gov/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="2,108.93,671.91,127.31,7.77"><p>https://www.ncbi.nlm.nih.gov/pmc/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3" coords="3,108.93,660.95,397.06,7.77;3,89.00,671.91,207.09,7.77"><p>We used the REST API (https://documentation.uts.nlm.nih.gov/rest/home.html) of the UMLS Metathesaurus (uts.nlm.nih.gov/home.html) to map each CUI to its term.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4" coords="5,108.93,660.76,330.73,7.77"><p>http://www.nltk.org/_modules/nltk/tokenize/punkt.html#PunktLanguageVars.word_tokenize</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5" coords="5,108.93,671.91,197.06,7.77"><p>http://www.nltk.org/_modules/nltk/stem/snowball.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6" coords="7,108.93,660.78,368.32,7.77"><p>Each image is rescaled to 224x224 and normalized with the mean and standard deviation of ImageNet.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_7" coords="7,108.93,671.93,313.29,7.77"><p>We did not use contrastive learning for the three extra encoders due to time restrictions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_8" coords="8,108.93,671.90,246.47,7.77"><p>https://huggingface.co/transformers/master/model_doc/gpt_neo.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_9" coords="9,108.93,638.85,398.58,7.77;9,89.29,649.81,20.34,7.77"><p>https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/image_captioning. ipynb</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_10" coords="9,108.93,660.96,398.05,7.77;9,89.29,671.92,194.71,7.77"><p>Inspired by: https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describepictures-c88a46a311b8 (accessed on May 23rd, 2021)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_11" coords="10,108.93,671.90,203.57,7.77"><p>https://www.geeksforgeeks.org/python-text-summarizer/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="15,112.92,492.49,394.87,9.46;15,112.92,506.04,394.87,9.46;15,112.92,519.59,394.43,9.46;15,112.58,533.14,393.40,9.46;15,112.92,546.69,313.28,9.46" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="15,112.58,533.14,393.40,9.46;15,112.92,546.69,114.58,9.46">The 2021 ImageCLEF Benchmark: Multimedia Retrieval in Medical, Nature, Internet and Social Media Applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Peteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Moustahfid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="15,235.93,546.69,155.73,9.46">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.92,560.23,394.43,9.46;15,112.92,573.78,393.06,9.46;15,112.41,587.33,395.49,9.46" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="15,112.92,573.78,323.59,9.46">Overview of the ImageCLEFmed 2021 Concept and Caption Prediction Task</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="org" />
	</analytic>
	<monogr>
		<title level="m" coord="15,458.46,573.78,47.52,9.46;15,112.41,587.33,62.87,9.46">CLEF2020 Working Notes</title>
		<title level="s" coord="15,182.59,587.33,180.27,9.46">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Bucharest -Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.92,600.88,393.06,9.46;15,112.92,614.43,214.48,9.46" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="15,187.11,600.88,318.88,9.46;15,112.92,614.43,51.39,9.46">The Unified Medical Language System (UMLS): integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,172.91,614.43,98.14,9.46">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.92,627.98,393.06,9.46;15,112.92,641.53,394.58,9.46;15,112.92,655.08,394.43,9.46;15,112.92,668.63,24.55,9.46" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="15,360.87,627.98,145.11,9.46;15,112.92,641.53,368.93,9.46">Overview of ImageCLEFcaption 2017 -Image Caption Prediction and Concept Detection for Biomedical Images</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,112.92,655.08,113.24,9.46">CLEF2017 Working Notes</title>
		<title level="s" coord="15,233.50,655.08,174.84,9.46">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.92,88.05,393.37,9.46;16,112.92,101.60,394.43,9.46;16,112.92,115.14,174.13,9.46" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="16,380.11,88.05,126.18,9.46;16,112.92,101.60,122.79,9.46">Overview of the ImageCLEF 2018 caption prediction tasks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,256.97,101.60,112.03,9.46">CLEF2018 Working Notes</title>
		<title level="s" coord="16,375.88,101.60,131.47,9.46;16,112.92,115.14,40.19,9.46">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.92,128.69,393.45,9.46;16,112.92,142.24,168.64,9.46" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="16,395.51,128.69,110.86,9.46;16,112.92,142.24,26.38,9.46">Diagnostic captioning: A survey</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Papamichail</surname></persName>
		</author>
		<idno>CoRR abs/2101.07299</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.92,155.79,394.43,9.46;16,112.92,169.34,393.07,9.46;16,112.92,182.89,394.43,9.46;16,112.92,196.44,123.94,9.46" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="16,318.48,155.79,184.29,9.46">A Survey on Biomedical Image Captioning</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,128.05,169.34,377.94,9.46;16,112.92,182.89,329.00,9.46">Workshop on Shortcomings in Vision and Language of the Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="26" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.92,209.99,393.06,9.46;16,112.92,223.54,394.87,9.46;16,112.41,237.09,159.73,9.46" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="16,331.19,209.99,174.79,9.46;16,112.92,223.54,35.24,9.46">AUEB NLP Group at ImageCLEFmed Caption</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,201.88,223.54,119.69,9.46">CLEF2019 Working Notes</title>
		<title level="s" coord="16,330.43,223.54,177.37,9.46;16,112.41,237.09,10.33,9.46">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.92,250.64,393.06,9.46;16,112.92,264.19,393.07,9.46;16,112.92,277.74,393.37,9.46;16,112.92,291.28,153.00,9.46" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="16,321.06,250.64,184.92,9.46;16,112.92,264.19,57.14,9.46">Medical Image Tagging by Deep Learning and Retrieval</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,193.30,264.19,312.69,9.46;16,112.92,277.74,393.37,9.46;16,112.92,291.28,23.48,9.46">Experimental IR Meets Multilinguality, Multimodality, and Interaction Proceedings of the Eleventh International Conference of the CLEF Association (CLEF 2020)</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.92,304.83,394.87,9.46;16,112.92,318.38,394.43,9.46;16,112.92,331.93,193.71,9.46" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="16,379.71,304.83,128.08,9.46;16,112.92,318.38,84.37,9.46">AUEB NLP Group at Image-CLEFmed Caption</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,244.32,318.38,263.03,9.46;16,112.92,331.93,40.19,9.46">CLEF2020 Working Notes, CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.92,345.48,388.11,9.46" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<title level="m" coord="16,258.96,345.48,207.53,9.46">Deep Residual Learning for Image Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.92,359.03,394.87,9.46;16,112.92,372.58,393.06,9.46;16,112.92,386.13,108.17,9.46" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="16,134.61,372.58,138.14,9.46">Supervised Contrastive Learning</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,280.92,372.58,225.06,9.46">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18661" to="18673" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.92,399.68,393.06,9.46;16,112.92,413.23,393.06,9.46;16,112.92,426.78,393.06,9.46;16,112.92,440.33,394.43,9.46;16,112.92,453.87,68.18,9.46" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="16,321.69,399.68,184.29,9.46;16,112.92,413.23,182.46,9.46">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,326.00,413.23,179.98,9.46;16,112.92,426.78,393.06,9.46;16,112.92,440.33,99.80,9.46">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="16,112.92,467.42,393.06,9.46;16,112.92,480.97,228.23,9.46" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="16,409.47,467.42,96.51,9.46;16,112.92,480.97,136.75,9.46">Language models are unsupervised multitask learners</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.92,494.52,394.43,9.46;16,112.92,508.07,393.06,9.46;16,112.92,521.62,394.70,9.46;16,112.92,535.17,24.55,9.46" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="16,112.92,508.07,319.54,9.46">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,453.73,508.07,52.26,9.46;16,112.92,521.62,255.42,9.46">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.92,548.72,393.06,9.46;16,112.53,562.27,393.46,9.46;16,112.92,575.82,227.93,9.46" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="16,435.40,548.72,70.58,9.46;16,112.53,562.27,393.46,9.46;16,112.92,575.82,70.94,9.46">Overview of the VQA-Med Task at ImageCLEF 2020: Visual Question Answering and Generation in the Medical Domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,208.10,575.82,100.94,9.46">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.92,589.37,393.06,9.46;16,112.56,602.92,393.76,9.46;16,112.92,616.47,393.06,9.46;16,112.92,630.01,336.33,9.46" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Friedrich</surname></persName>
		</author>
		<title level="m" coord="16,368.38,589.37,137.60,9.46;16,112.56,602.92,393.76,9.46;16,112.92,616.47,166.36,9.46;16,356.44,616.47,149.54,9.46;16,112.92,630.01,19.64,9.46">Radiology Objects in COntext (ROCO): A Multimodal Image Dataset: 7th Joint International Workshop, CVII-STENT 2018 and Third International Workshop</title>
		<meeting><address><addrLine>LABELS; Granada, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Proceedings</publisher>
			<date type="published" when="2018-09-16">2018. September 16, 2018. 2018</date>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
	<note>Held in Conjunction with MICCAI 2018</note>
</biblStruct>

<biblStruct coords="16,112.92,643.56,394.87,9.46;16,112.92,657.11,393.06,9.46;16,112.92,670.66,232.91,9.46" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="16,276.74,643.56,231.06,9.46;16,112.92,657.11,160.52,9.46">ImageSem at ImageCLEF 2018 Caption Task: Image Retrieval and Transfer Learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,300.51,657.11,118.73,9.46">CLEF2018 Working Notes</title>
		<title level="s" coord="16,427.51,657.11,78.47,9.46;16,112.92,670.66,98.96,9.46">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.92,88.05,393.06,9.46;17,112.92,101.60,126.24,9.46" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<title level="m" coord="17,358.19,88.05,147.79,9.46;17,112.92,101.60,39.88,9.46">Densely connected convolutional networks</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.92,115.14,393.06,9.46;17,112.92,128.69,393.06,9.46;17,112.92,142.24,291.82,9.46" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="17,342.36,115.14,163.62,9.46;17,112.92,128.69,65.33,9.46">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,201.66,128.69,304.33,9.46;17,112.92,142.24,89.05,9.46">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Miami Beach, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.92,155.79,393.06,9.46;17,112.92,169.34,342.21,9.46" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="17,240.21,155.79,168.10,9.46">A Method for Stochastic Optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,431.99,155.79,73.99,9.46;17,112.92,169.34,211.03,9.46">3rd International Conference on Learning Representations (ICLR)</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.92,182.89,393.06,9.46;17,112.56,196.44,83.63,9.46" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="17,191.85,182.89,314.14,9.46">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.92,209.99,393.07,9.46;17,112.92,223.54,235.16,9.46" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<title level="m" coord="17,264.80,209.99,241.19,9.46;17,112.92,223.54,54.20,9.46">Neural machine translation by jointly learning to align and translate</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="17,112.92,237.09,394.43,9.46;17,112.92,250.64,393.06,9.46;17,112.92,264.19,274.51,9.46" xml:id="b23">
	<monogr>
		<title level="m" type="main" coord="17,184.40,250.64,321.58,9.46;17,112.92,264.19,188.15,9.46">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.92,277.74,394.43,9.46;17,112.41,291.28,395.38,9.46;17,112.92,304.83,227.43,9.46" xml:id="b24">
	<monogr>
		<title level="m" type="main" coord="17,303.06,291.28,204.73,9.46;17,112.92,304.83,85.47,9.46">Retrieval-augmented generation for knowledgeintensive NLP tasks</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">S H</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<idno>CoRR abs/2005.11401</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
