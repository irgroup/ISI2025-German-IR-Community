<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,412.84,15.42;1,89.29,106.66,406.49,15.42;1,89.29,128.58,43.07,15.43">PUC Chile team at Caption Prediction: ResNet visual encoding and caption classification with Parametric ReLU</title>
				<funder ref="#_kyKc2ee">
					<orgName type="full">FONDECYT</orgName>
				</funder>
				<funder ref="#_UAHz34H">
					<orgName type="full">ANID</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,88.90,156.89,70.20,11.96"><forename type="first">Vicente</forename><surname>Castro</surname></persName>
							<email>vvcastro@uc.cl</email>
							<affiliation key="aff0">
								<orgName type="institution">Pontificia Universidad Cat√≥lica de Chile</orgName>
								<address>
									<addrLine>Av. Vicu√±a Mackena 4860</addrLine>
									<postCode>7820244</postCode>
									<settlement>Macul</settlement>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,170.10,156.89,50.26,11.96"><forename type="first">Pablo</forename><surname>Pino</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Pontificia Universidad Cat√≥lica de Chile</orgName>
								<address>
									<addrLine>Av. Vicu√±a Mackena 4860</addrLine>
									<postCode>7820244</postCode>
									<settlement>Macul</settlement>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,231.40,156.89,57.38,11.96"><forename type="first">Denis</forename><surname>Parra</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Pontificia Universidad Cat√≥lica de Chile</orgName>
								<address>
									<addrLine>Av. Vicu√±a Mackena 4860</addrLine>
									<postCode>7820244</postCode>
									<settlement>Macul</settlement>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,316.25,156.89,55.17,11.96"><forename type="first">Hans</forename><surname>Lobel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Pontificia Universidad Cat√≥lica de Chile</orgName>
								<address>
									<addrLine>Av. Vicu√±a Mackena 4860</addrLine>
									<postCode>7820244</postCode>
									<settlement>Macul</settlement>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,412.84,15.42;1,89.29,106.66,406.49,15.42;1,89.29,128.58,43.07,15.43">PUC Chile team at Caption Prediction: ResNet visual encoding and caption classification with Parametric ReLU</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">0506DD6F357F65E7EF9A9E10D17FD8EE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image Captioning</term>
					<term>Medical Artificial Intelligence</term>
					<term>Deep Learning</term>
					<term>Perceptual Similarity</term>
					<term>Convolutional Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article describes PUC Chile team's participation in the Caption Prediction task of ImageCLEFmedical challenge 2021, which resulted in the team winning this task. We first show how a very simple approach based on statistical analysis of captions, without relying on images, results in a competitive baseline score. Then, we describe how to improve the performance of this preliminary submission by encoding the medical images with a ResNet CNN, pre-trained on ImageNet and later fine-tuned with the challenge dataset. Afterwards, we use this visual encoding as the input for a multi-label classification approach for caption prediction. We describe in detail our final approach, and we conclude by discussing some ideas for future work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>ImageCLEF <ref type="bibr" coords="1,144.76,410.45,12.99,10.91" target="#b0">[1]</ref> is an initiative with the aim of advancing the field of image retrieval (IR) as well as enhancing the evaluation of technologies for annotation, indexing and retrieval of visual data. The initiative takes the form of several challenges, and it is especially aware of the changes in the IR field in recent years, which have brought about tasks requiring the use of different types of data such as text, images and other features moving towards multi-modality. ImageCLEF has been running annually since 2003, and since the second version (2004) there are medical images involved in some tasks, such as medical image retrieval. Since those versions, the ImageCLEFmedical challenge group of tasks <ref type="bibr" coords="1,302.74,505.29,12.71,10.91" target="#b1">[2]</ref> has integrated new ones involving medical images, with the medical image captioning task taking place since 2017. It consists of two subtasks: concept prediction and caption detection. Although there have been changes in the data used for the newest versions of the challenge, the goal of this task is the same: help physicians reduce the burden of manually translating visual medical information (such as radiology images) into textual descriptions. In particular, the caption prediction task within the ImageCLEFmedical challenge 2021 aims at supporting clinicians in their responsibility to provide clinical diagnoses by composing coherent captions for the entirety of a medical image.</p><p>In this document we describe the participation of our team from the HAIVis group<ref type="foot" coords="2,469.65,85.21,3.71,7.97" target="#foot_0">1</ref> within the artificial intelligence laboratory<ref type="foot" coords="2,250.87,98.76,3.71,7.97" target="#foot_1">2</ref> at Pontificia Universidad Catolica de Chile (PUC Chile team) in the image captioning task at MedicalImageCLEF 2021 <ref type="bibr" coords="2,370.69,114.06,11.45,10.91" target="#b1">[2]</ref>. Our team earned 1st place in this challenge, and our best submission was a combination of deep learning techniques to visually encode the medical images, followed by a traditional classification of captions that were re-ranked by statistical information obtained from the training dataset.</p><p>The rest of the paper is structured as follows: section 2 describes our data analysis, while in section 3 we provide details of our proposed methods and experiments for model training and validation. Later, in section 4 we provide details of our results, and finally in section 5 we conclude our article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Data Analysis</head><p>The dataset provided for this challenge consists of two sets of 2,756 and 500 image-caption pairs for training and validation, respectively. Each caption consists of a natural language text, which is a highly technical annotation made by physicians about abnormalities and medical objects in the image it corresponds to. Figure <ref type="figure" coords="3,130.61,86.97,4.97,10.91" target="#fig_1">2</ref> shows the distribution words per number of appearances in the dataset, for example, 28% of words have only one occurrence. Figure <ref type="figure" coords="3,301.81,100.52,5.07,10.91" target="#fig_2">3</ref> shows the distribution of caption lengths.  Figure <ref type="figure" coords="3,132.19,540.11,5.17,10.91" target="#fig_3">4</ref> shows the most common words in the dataset and their number of appearances, showing that some words are very common in the dataset, appearing in about 40% of all training captions. From a semantic analysis, these words seem to have broader and more descriptive meanings of the different elements in the images. This may be a direct cause of the fact that simpler and naive methods, that are based on more statistical approaches, can outperform more complex models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method and Experimentation</head><p>While addressing the task we tried three main approaches: a pure statistical method, a multi-label classification approach (MLC) and a perceptual similarity based model (Sim).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Statistical approach</head><p>Our initial approach to the challenge tried to leverage the statistics related to the composition of each caption. This first model was a naive algorithm that randomly selected a caption length from the training set, created a list of this length with the most popular words in the dataset, and shuffled them to get a random order. This simple method obtained a mean BLEU score of 0.357 on the validation set and, when submitted, scored 0.378 points in the test set.</p><p>This first approach helped us gain an intuition about how the BLEU score varied and how susceptible it was to different components of the caption. Our initial hypothesis was that more relevant than the order of words in the caption, the correctness of them was the most significant element for the metric. To test this assumption, we explore the alternative of a multi-label classification approach that, given an image, predicted the most relevant words in the caption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-label classification approach (MLC)</head><p>In this approach we consider each word as a class, and trained a Convolutional Neural Network (CNN) to predict the words of a caption given an image. Then, the top classified words are selected and ordered by a statistical rule to produce a final caption. Figure <ref type="figure" coords="4,433.18,615.91,5.17,10.91" target="#fig_4">5</ref> shows the full pipeline for caption generation with our approach, and we give more details next<ref type="foot" coords="4,450.76,627.70,3.71,7.97" target="#foot_4">5</ref> :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Preprocessing</head><p>We process each image-caption pair to reduce the number of target classes (words), and to prepare the image to pass it through the network. The following steps were applied:</p><p>1. Caption processing: We processed each caption according to the evaluation methodology described in the previous section, transforming each caption into a list of stemmed words (labels). The vocabulary is composed by all the words in the training data with four appearances or more. We did not perform any special handling for words in the validation set that were not present in the training vocabulary. After filtering, the training vocabulary size was reduced to 1,075 words (1,189 when using training and validation set). 2. Image processing: Each image is transformed to have pixel values within a [0, 1] range (in each RGB channel) and then is normalized by the mean and standard deviation (over each channel), according to torchvision documentation <ref type="foot" coords="5,385.70,265.94,3.71,7.97" target="#foot_5">6</ref> . As a data augmentation method, a crop of 300x300 pixels is taken from the image. For the training set, this crop is selected from a random location, whereas for validation and testing, the central crop of the image is always taken. This is a common training setup and has been used for similar purposes in past versions of the challenge <ref type="bibr" coords="5,305.63,321.89,11.43,10.91" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Classification training</head><p>Several ResNet <ref type="bibr" coords="5,159.68,371.22,12.99,10.91" target="#b4">[5]</ref> and DenseNet <ref type="bibr" coords="5,241.95,371.22,12.98,10.91" target="#b5">[6]</ref> model architectures were tested, with and without fine tuning from ImageNet <ref type="bibr" coords="5,189.24,384.77,12.68,10.91" target="#b6">[7]</ref> pre-trained weights. Fine tuning of a DenseNet121 model pre-trained on the ChestX-ray14 dataset <ref type="bibr" coords="5,220.21,398.32,12.97,10.91" target="#b7">[8]</ref> was also tested. Different layers of the network were frozen during fine-tuning, as a measure to avoid over-fitting.</p><p>In addition, the last layer of the network was replaced with a fully connected layer that matched the dimensionality of the training vocabulary size. Furthermore, we added a dropout layer and passed the resulting values by a Parametric ReLU (PReLU) <ref type="bibr" coords="5,402.87,452.52,12.99,10.91" target="#b8">[9]</ref> activation function. With this, the output of our model was a vector of dimension vocabulary size and unbounded range.</p><p>In training, we sought to minimize the Binary Cross Entropy loss between the vector predicted by the model and the one-hot encoded ground truth, calculated as:</p><formula xml:id="formula_0" coords="5,172.76,530.52,249.75,33.17">ùêø = ùê∂ ‚àëÔ∏Å ùëê -ùë§ ùëê [ùë¶ ùëê ‚Ä¢ log ùúé(ùë• ùëê ) + (1 -ùë¶ ùëê ) ‚Ä¢ log(1 -ùúé(ùë• ùëê ))]</formula><p>where ùê∂ is the number of labels to classify. In code, this loss was calculated with BCEWithLogits<ref type="foot" coords="5,508.67,574.13,3.71,7.97" target="#foot_6">7</ref> function from pytorch. As optimizer, we used Adam <ref type="bibr" coords="5,323.33,589.43,22.21,10.91" target="#b9">[10]</ref> with no weight decay and an initial learning rate of 5e-4, after epoch 15 this last hyper-parameter was reduced to 1e-4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Captioning</head><p>Once the classification output is obtained from our visual model, it needs to be translated into a caption. We define N as the length of the output caption, a hyper-parameter of the model and choose the N highest scoring words. Then, we used a statistical approach to order the words in a logical sentence: for each word, we define its position as the most common one it has across all training captions.</p><p>Two output examples from our model are shown next, with good (Fig. <ref type="figure" coords="6,424.29,439.29,4.25,10.91">6</ref>) and bad (Fig. <ref type="figure" coords="6,498.15,439.29,4.25,10.91">7</ref>) performance: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Similarity-based approach (Sim)</head><p>Another method that we used and resulted in a fairly good experimental performance was a similarity-based approach. For each test image, we ranked the most similar images in the training set using the Learned Perceptual Image Path Similarity (LPIPS)<ref type="foot" coords="7,366.76,303.03,3.71,7.97" target="#foot_7">8</ref>  <ref type="bibr" coords="7,370.96,304.78,16.09,10.91" target="#b10">[11]</ref>, a learned metric based on the similarity between deep features from several neural network layers, in our experiments, an AlexNet <ref type="bibr" coords="7,136.99,331.88,19.56,10.91" target="#b11">[12]</ref> model. Then, the caption from the closest training image is assigned to the test image.</p><p>This approach resulted in a very good test performance and helped us to reach and maintain the top 3 in the leaderboard. Furthermore, we tested this approach for the concept detection task where we also achieved better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>To evaluate our model we measured the BLEU score <ref type="bibr" coords="7,320.74,444.70,17.80,10.91" target="#b12">[13]</ref> for each caption generated against its ground truth, following the challenge evaluation procedure <ref type="foot" coords="7,351.64,456.50,3.71,7.97" target="#foot_8">9</ref> . It is important to emphasize that this metric must be calculated with version v3.2.2 of the NLTK library since new updates change the results considerably. Table <ref type="table" coords="7,261.28,485.35,5.07,10.91" target="#tab_0">1</ref> shows our methods' scores in the validation set. Additionally, we measured word recall as a metric for the classification method. Since BLEU is a precision-based metric, including a recall-based metric should help evaluate the performance The best result was achieved with the multi-label classification approach, using a ResNet34 <ref type="bibr" coords="8,89.29,327.44,12.69,10.91" target="#b4">[5]</ref> model pre-trained on ImageNet and fine-tuned for 15 epochs, only with the last 5 layers with learnable parameters, whilst the other layers were frozen. The training scheme mentioned above was followed. For word selection we set N=26, value that was inferred from the distribution in Figure <ref type="figure" coords="8,119.84,368.09,4.98,10.91" target="#fig_2">3</ref> and validated with experimental results. Figure <ref type="figure" coords="8,337.04,368.09,4.98,10.91" target="#fig_6">8</ref> shows the development of BLEU and word recall during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">CrowdAI Runs</head><p>Four submissions were made to crowdai.org using the methods described, the details and results are shown in Table <ref type="table" coords="8,176.61,444.91,3.74,10.91" target="#tab_1">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this article we have provided details of the participation of the PUC Chile team, winners of the caption prediction task within the ImageCLEFmedical challenge 2021. In the process of building our final submission, we tested several approaches, detailed in this paper. Our final submission was based on a ResNet34 architecture to visually encode the input medical image, followed by predicting captions as a multi-label word classification task, and finally re-ranking the word order based on statistical information from the training dataset. In future work, we plan at testing other CNN architectures, perform further experiments exploiting perceptual similarity, and test other techniques for neural language modeling.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,534.89,114.69,8.93;2,140.13,330.90,115.01,196.57"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Dataset examples</figDesc><graphic coords="2,140.13,330.90,115.01,196.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,89.29,300.19,291.57,8.93;3,120.54,123.23,354.19,164.39"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Distribution of number of words per number of appearances.</figDesc><graphic coords="3,120.54,123.23,354.19,164.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,89.29,510.46,156.82,8.93;3,120.54,339.00,354.19,158.90"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Caption length distribution.</figDesc><graphic coords="3,120.54,339.00,354.19,158.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,89.29,279.99,189.99,8.93;4,120.54,84.19,354.19,183.24"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Most common words in the dataset.</figDesc><graphic coords="4,120.54,84.19,354.19,183.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,89.29,313.15,343.36,9.15;6,89.29,84.19,416.69,216.62"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Model diagram. Top ùëÅ = 23 classified words are selected for the caption.</figDesc><graphic coords="6,89.29,84.19,416.69,216.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,89.29,620.10,355.07,9.15;6,151.80,475.55,291.67,132.20"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Example of caption prediction with good performance, BLEU= 0.850 (N=23)</figDesc><graphic coords="6,151.80,475.55,291.67,132.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="8,89.29,262.74,251.62,8.93;8,141.38,84.19,312.52,165.99"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: BLEU and Recall Score during training with (N=26)</figDesc><graphic coords="8,141.38,84.19,312.52,165.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,88.99,514.36,303.05,72.75"><head>Table 1</head><label>1</label><figDesc>Results in the validation set.</figDesc><table coords="7,200.74,543.75,191.31,43.35"><row><cell>Method</cell><cell>BLEU</cell></row><row><cell>Sim: LPIPS Similarity (from AlexNet)</cell><cell>0.459</cell></row><row><cell>MLC: ResNet</cell><cell>0.544</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,88.99,471.47,369.82,137.11"><head>Table 2</head><label>2</label><figDesc>Submission results.</figDesc><table coords="8,136.47,497.34,322.34,111.24"><row><cell></cell><cell>Method</cell><cell>BLEU</cell></row><row><cell>Subm1</cell><cell>Statistical: random length + most common words + random order</cell><cell>0.378</cell></row><row><cell>Subm3</cell><cell>MLC: ResNet50, random length + fixed order</cell><cell>0.351</cell></row><row><cell>Subm4</cell><cell>Sim: LPIPS similarity approach</cell><cell>0.442</cell></row><row><cell>Subm6</cell><cell>MLC: ResNet34 and most common index for ordering, trained for 20 epochs</cell><cell>0.509</cell></row><row><cell>Subm7</cell><cell>MLC: ResNet34 and most common index for ordering, trained for 15 epochs</cell><cell>0.510</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.93,638.14,84.83,8.97"><p>http://haivis.ing.puc.cl/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,108.93,649.10,80.33,8.97"><p>http://ialab.ing.puc.cl/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,108.93,660.06,80.21,8.97"><p>https://www.nltk.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,108.93,671.01,280.73,8.97"><p>Evaluation Methodology at https://www.imageclef.org/2021/medical/caption</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="4,108.93,671.00,344.71,8.97"><p>For all our implementations we used PyTorch as our main DL framework: https://pytorch.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="5,108.93,660.00,236.10,8.97"><p>Documentation @ https://pytorch.org/vision/stable/models.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="5,108.93,670.96,280.28,8.97"><p>https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="7,108.93,660.08,250.48,8.97"><p>Code available @ https://github.com/richzhang/PerceptualSimilarity</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="7,108.93,671.03,318.64,8.97"><p>Refer to "Evaluation methodology" @ https://www.imageclef.org/2021/medical/caption</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was partially funded by <rs type="programName">ANID -Millennium Science Initiative Program</rs> -Code <rs type="grantNumber">ICN17_002</rs> and by <rs type="funder">ANID</rs>, <rs type="funder">FONDECYT</rs> grant <rs type="grantNumber">1191791</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_UAHz34H">
					<idno type="grant-number">ICN17_002</idno>
					<orgName type="program" subtype="full">ANID -Millennium Science Initiative Program</orgName>
				</org>
				<org type="funding" xml:id="_kyKc2ee">
					<idno type="grant-number">1191791</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,336.93,395.01,10.91;9,112.66,350.47,394.53,10.91;9,112.48,364.02,395.18,10.91;9,112.66,377.57,394.53,10.91;9,112.28,391.12,393.70,10.91;9,112.66,404.67,393.33,10.91;9,112.66,418.22,393.33,10.91;9,112.66,431.77,393.53,10.91;9,112.66,445.32,197.61,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,264.32,391.12,241.66,10.91;9,112.66,404.67,261.75,10.91">Overview of the ImageCLEF 2021: Multimedia retrieval in medical, nature, internet and social media applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>P√©teri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tauteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Stefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Moustahfid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,401.27,404.67,104.72,10.91;9,112.66,418.22,393.33,10.91;9,112.66,431.77,201.15,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 12th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="9,348.61,431.77,157.57,10.91;9,112.66,445.32,31.10,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,458.87,394.53,10.91;9,112.66,472.42,394.53,10.91;9,112.66,485.97,394.53,10.91;9,112.66,499.52,67.18,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,167.43,472.42,335.54,10.91">Overview of the ImageCLEFmed 2021 concept &amp; caption prediction task</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garc√≠a Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,127.66,485.97,114.61,10.91">CLEF2021 Working Notes</title>
		<title level="s" coord="9,249.86,485.97,179.10,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,513.06,393.33,10.91;9,112.14,526.61,393.85,10.91;9,112.66,540.16,393.33,10.91;9,112.66,553.71,397.48,10.91;9,112.66,569.70,121.09,7.90" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,192.14,513.06,168.08,10.91">NLTK: The Natural Language Toolkit</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<idno type="DOI">10.3115/1118108.1118117</idno>
		<ptr target="https://doi.org/10.3115/1118108.1118117.doi:10.3115/1118108.1118117" />
	</analytic>
	<monogr>
		<title level="m" coord="9,384.14,513.06,121.84,10.91;9,112.14,526.61,393.85,10.91;9,112.66,540.16,141.53,10.91;9,303.17,540.16,61.04,10.91">Proceedings of the ACL-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics</title>
		<meeting>the ACL-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="63" to="70" />
		</imprint>
	</monogr>
	<note>ETMTNLP &apos;02</note>
</biblStruct>

<biblStruct coords="9,112.66,580.81,393.33,10.91;9,112.66,594.36,201.01,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,245.61,580.81,260.38,10.91;9,112.66,594.36,44.86,10.91">Neural Captioning for the ImageCLEF 2017 Medical Image Challenges</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lyndon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,185.05,594.36,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,607.91,393.32,10.91;9,112.66,621.46,395.00,10.91;9,112.66,635.01,137.64,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,254.59,607.91,207.56,10.91">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,112.66,621.46,307.90,10.91">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,648.56,393.33,10.91;10,112.66,86.97,394.53,10.91;10,112.66,100.52,237.02,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,356.26,648.56,149.73,10.91;10,112.66,86.97,39.90,10.91">Densely Connected Convolutional Networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.243</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,197.71,86.97,304.54,10.91">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,114.06,393.33,10.91;10,112.66,127.61,394.53,10.91;10,112.66,141.16,250.61,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,345.70,114.06,160.29,10.91;10,112.66,127.61,66.03,10.91">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2009.5206848</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,224.88,127.61,277.63,10.91">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,154.71,395.17,10.91;10,112.66,168.26,393.33,10.91;10,112.66,181.81,393.33,10.91;10,112.66,195.36,250.71,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,400.77,154.71,107.07,10.91;10,112.66,168.26,393.33,10.91;10,112.66,181.81,188.31,10.91">ChestX-ray8: Hospital-Scale Chest X-Ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,326.34,181.81,179.65,10.91;10,112.66,195.36,220.26,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,208.91,395.17,10.91;10,112.66,222.46,393.33,10.91;10,112.66,236.01,152.05,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,249.19,208.91,258.64,10.91;10,112.66,222.46,158.00,10.91">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,291.75,222.46,214.24,10.91;10,112.66,236.01,121.87,10.91">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,249.56,393.32,10.91;10,112.33,263.11,394.86,10.91;10,112.66,276.66,394.03,10.91;10,112.41,290.20,45.38,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,232.65,249.56,164.51,10.91">A Method for Stochastic Optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m" coord="10,142.34,263.11,309.71,10.91">3rd International Conference on Learning Representations, ICLR 2015</title>
		<title level="s" coord="10,224.87,276.66,134.38,10.91">Conference Track Proceedings</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">May 7-9, 2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,303.75,393.33,10.91;10,112.66,317.30,393.53,10.91;10,112.30,330.85,202.98,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,355.56,303.75,150.43,10.91;10,112.66,317.30,156.90,10.91">The Unreasonable Effectiveness of Deep Features as a Perceptual Metric</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,291.13,317.30,215.06,10.91;10,112.30,330.85,172.53,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,344.40,395.17,10.91;10,112.66,357.95,393.33,10.91;10,112.66,371.50,394.52,10.91;10,112.66,385.05,129.08,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,303.91,344.40,203.92,10.91;10,112.66,357.95,102.36,10.91">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,238.72,357.95,267.27,10.91;10,112.66,371.50,150.80,10.91;10,313.37,371.50,33.13,10.91">Proceedings of the 25th International Conference on Neural Information Processing Systems</title>
		<meeting>the 25th International Conference on Neural Information Processing Systems<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
	<note>NIPS&apos;12</note>
</biblStruct>

<biblStruct coords="10,112.66,398.60,393.33,10.91;10,112.66,412.15,393.33,10.91;10,112.66,425.70,394.52,10.91;10,112.66,439.25,397.48,10.91;10,112.36,455.24,43.94,7.90" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,310.87,398.60,195.12,10.91;10,112.66,412.15,105.50,10.91">BLEU: A Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
		<ptr target="https://doi.org/10.3115/1073083.1073135.doi:10.3115/1073083.1073135" />
	</analytic>
	<monogr>
		<title level="m" coord="10,246.51,412.15,259.48,10.91;10,112.66,425.70,172.61,10.91">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
