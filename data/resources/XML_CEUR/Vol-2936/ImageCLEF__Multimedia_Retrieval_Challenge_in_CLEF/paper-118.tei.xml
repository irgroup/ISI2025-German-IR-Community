<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.03,75.53,450.99,17.00;1,72.03,96.28,450.96,17.00;1,72.03,117.05,207.45,17.00">ImageSem Group at ImageCLEFmed Caption 2021 Task: Exploring the Clinical Significance of the Textual Descriptions Derived from Medical Images</title>
				<funder ref="#_fgdBsUe">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_Pc7myEv">
					<orgName type="full">Beijing Natural Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.03,149.60,66.26,10.80"><forename type="first">Xuwen</forename><surname>Wang</surname></persName>
							<email>wang.xuwen@imicams.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Medical Information and Library</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Medical Sciences</orgName>
								<orgName type="institution" key="instit2">Peking Union Medical College</orgName>
								<address>
									<postCode>100020</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,151.33,149.60,48.25,10.80"><forename type="first">Zhen</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Medical Information and Library</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Medical Sciences</orgName>
								<orgName type="institution" key="instit2">Peking Union Medical College</orgName>
								<address>
									<postCode>100020</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,212.60,149.60,66.99,10.80"><forename type="first">Chunyuan</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Life Science</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<postCode>100081</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,292.63,149.60,71.26,10.80"><forename type="first">Lianglong</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Medical Information and Library</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Medical Sciences</orgName>
								<orgName type="institution" key="instit2">Peking Union Medical College</orgName>
								<address>
									<postCode>100020</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,393.17,149.60,32.84,10.80"><forename type="first">Jiao</forename><surname>Li</surname></persName>
							<email>li.jiao@imicams.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Medical Information and Library</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Medical Sciences</orgName>
								<orgName type="institution" key="instit2">Peking Union Medical College</orgName>
								<address>
									<postCode>100020</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.03,75.53,450.99,17.00;1,72.03,96.28,450.96,17.00;1,72.03,117.05,207.45,17.00">ImageSem Group at ImageCLEFmed Caption 2021 Task: Exploring the Clinical Significance of the Textual Descriptions Derived from Medical Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AB5FB0C8D4ED467727A3759A222BB599</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Concept detection</term>
					<term>caption prediction</term>
					<term>multi-label classification</term>
					<term>fine-grained semantic labelling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the work of ImageSem group in the ImageCLEFmed Caption 2021 task. In the concept detection subtask, we employed the transfer learning-based multi-label classification model as our baseline. We also trained multiple fine-grained MLC models based on manually annotated semantic categories, such as Imaging Type, Anatomic Structure, and Findings, which may reveal clinical insights of radiology images. We submitted 9 runs to the concept detection subtask, and achieved the F1 Score of 0.419, which ranked 3 rd in the leader board. In the caption prediction subtask, our first method simply combines detected concepts according to the sentence patterns. The second method used a dual path CNN model for matching images and captions. We submitted 4 runs to the caption prediction subtask, and achieved the BLEU score of 0.257, which ranked 6 th among the participating teams.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.5"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.5"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.5"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.5"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.5"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.5"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.5"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The medical track of ImageCLEF <ref type="bibr" coords="1,212.76,440.61,16.44,9.90" target="#b0">[1]</ref>aims at promoting the research of computer-aided radiology image analysis and interpretation. ImageCLEFmed Caption 2021 <ref type="bibr" coords="1,330.84,453.39,14.00,9.90" target="#b1">[2]</ref>is one of the ImageCLEFmedical tasks, which focus on mapping visual information of radiology images to textual descriptions. It consists of two subtasks, namely Concept Detection and Caption Prediction. On behalf of the Institute of Medical Information and Library, Chinese Academy of Medical Sciences, our Image Semantics group (ImageSem) participated in both of the two subtasks.</p><p>The concept detection subtask aims to identify the UMLS <ref type="bibr" coords="1,318.13,522.64,14.87,9.90" target="#b2">[3]</ref>Concept Unique Identifiers (CUIs) for a given radiology image. Following our previous work on ImageCLEF 2019 <ref type="bibr" coords="1,373.65,535.41,11.63,9.90" target="#b3">[4]</ref>, we employed transfer learningbased multi-label classification (MLC) <ref type="bibr" coords="1,244.60,547.91,12.17,9.90" target="#b4">[5]</ref>, <ref type="bibr" coords="1,260.83,547.91,12.17,9.90" target="#b5">[6]</ref> as our first method for modeling all the concepts in the training set. In order to annotate each image with more meaningful concepts, we manually classified the concepts into three categories according to their UMLS semantic types, namely Imaging Type, Anatomical Structure, and Findings. Then we trained MLC sub models separately for different concept categories as our second method.</p><p>The caption prediction subtask asks participants to generate coherent captions for the entirety of an image, which requires higher accuracy and semantic interpretability of expression. We also employed two methods for caption prediction. The first method was the pattern-based combination of concepts identified in the previous task. The second method was based on the dual path CNN model <ref type="bibr" coords="1,423.42,655.19,11.63,9.90" target="#b6">[7]</ref>, which is commonly used in the image-text retrieval field to match images and captions for instance-level retrieval. This paper is organized as follows. Section 2 describes the data set of the ImageCLEFmed Caption 2021 task. Section 3 presents the methods for concept detection and caption prediction. Section 4 lists all of our submitted runs. Section 5 makes a brief summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Dataset</head><p>The ImageCLEFmed Caption 2021 task is in its 5th edition this year. Compared with previous years, the released images were strictly limited to radiology, and the number of images and associated UMLS concepts were reduced. There were 222,314 images with 111,156 concepts in 2018 <ref type="bibr" coords="2,437.18,203.07,11.64,9.90" target="#b7">[8]</ref>, 70,786 radiology images with 5,528 concepts in 2019 <ref type="bibr" coords="2,229.85,215.57,11.44,9.90" target="#b8">[9]</ref>, 80,747 radiology images with 3,047 concepts in 2020 <ref type="bibr" coords="2,490.95,215.57,16.82,9.90" target="#b9">[10]</ref>, and 3,256 radiology images with 1,586 concepts and 3,256 captions in 2021. Another improvement of the dataset is that the validation set and test set include real radiology images annotated by medical doctors, which increased the medical context relevance of the UMLS concepts. For one thing, the reduction of concept scope and size lowered the difficulty of concept identification. For another thing, the reduction of image size is not conducive to training large-scale neural networks.</p><p>The organizers provided UMLS concepts along with their imaging modality information, for training purposes. We observed that most images were assigned with concepts indicating the diagnostic procedure or medical device, and some images were accompanied by concepts indicating the body part, organ or clinical findings. As shown in Table <ref type="table" coords="2,232.85,335.59,4.13,9.90" target="#tab_0">1</ref>, the high-frequency concepts are concentrated in several specific semantic types. For our experiments, we utilized this feature and manually classified three concept categories for building fine-grained multi-label classification models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>This section describes methods we used in two subtasks. Fig. <ref type="figure" coords="3,343.15,104.52,5.50,9.90" target="#fig_0">1</ref> shows the workflow and submissions of ImageSem in ImageCLEFmed Caption 2021. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Concept detection</head><p>In the concept detection subtask, for one thing, we employed the transfer learning-based multi-label classification model to identify overall concepts; for another thing, we paid more attention to the distinction of labels with different semantic types, and focus on three major categories of concepts, which may reveal clinical insights of radiology images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Transfer learning-based multi-label classification</head><p>In our previous work, we employed a transfer learning-based multi-label classification model to assign multiple CUIs to a specific medical image. This is a classic approach under the condition of limited tag size and high frequency concepts. In our first method, for modeling overall concepts, we applied the Inception-V3 <ref type="bibr" coords="3,123.17,509.14,14.29,9.90" target="#b4">[5]</ref> and DenseNet 201 <ref type="bibr" coords="3,223.97,509.14,15.23,9.90" target="#b5">[6]</ref>which were pre-trained on the ImageNet datasets <ref type="bibr" coords="3,464.45,509.14,16.60,9.90" target="#b10">[11]</ref>. The fully connected layer before the last softmax layer was replaced and the parameters of the pre-trained CNN model were transferred as the initial parameters of our MLC model.</p><p>During the training process, we collected 1,586 CUIs from both of training set and validation set as our labels. Then we fine-tuned the models on the validation set. For a given test image, concepts of high probabilities above the threshold were selected as the prediction labels. Empirically, we adjusted the threshold gradually from 0.1 to 0.7 on the basis of the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Fine-grained multi-label classification</head><p>In this method, according to the UMLS semantic types, we go further to divide ImageCLEF concepts into four semantic categories, namely Imaging Type (IT), Anatomic Structure (AS), Findings (FDs) and others.</p><p>Based on the official training set and validation set, we reprocessed the images and associated concepts via our medical image annotation platform.</p><p>As shown in Figure <ref type="figure" coords="3,157.19,710.97,4.13,9.90" target="#fig_1">2</ref>, for a given radiology image, there are three sources of related concepts. The first one is ImageCLEF concepts annotated by concept extraction tools and medical doctors. These concepts are semantically related, but often incomplete, since many images having only one concept. The second source of concepts are automatically annotated from the given image captions, using the Metamap tool <ref type="bibr" coords="3,66.03,761.71,18.14,9.90" target="#b11">[12]</ref>together with UMLS 2020ab. These concepts are more comprehensive, but also introduce noise words. The third source is the expanding concepts that we summarize manually based on the high-frequency ImageCLEF concepts, for labelling convenience purpose.</p><p>We invited graduate students majoring in medical imaging to label images with reference to visual information, caption descriptions and the above three sources of concepts. The labeling protocol is that each radiology image was assigned with at least one IT label, zero or more AS labels, and zero or more FDs labels. Specifically, ImageCLEF concepts that are difficult to be classified to the above categories, can be assigned to the 'Others'.</p><p>Then we build three image-concept sub collections for training fine-grained MLC models. These collections have same training and validation images, but differentiate in related concepts. Table <ref type="table" coords="4,495.07,152.54,5.50,9.90" target="#tab_1">2</ref> shows the distribution of different concept categories.</p><p>We verified our MLC models based on the re-annotated validation set. The experimental results showed that our model performs well on the prediction of Imaging Type labels, with F1 score of 0.9273. However, the predictions for the other two kinds of labels are far from satisfactory. One possible reason is that there are few images but too many labels for training. It is intuitively understandable that images of the same or similar cases would have a similar anatomic structure or medical findings label. Whereas the data characteristics of this subtask are obviously not suitable for specific diseases, which raised the difficulty to predict accurate body part, organ, or findings.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Caption Prediction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Pattern-based caption generation</head><p>For generating reasonable image captions, the first method was the pattern-based combination of concepts identified in the previous task. We designed a simple sentence pattern based on the characteristic of captions in the training and validation set, see Table <ref type="table" coords="5,370.98,74.27,4.13,9.90" target="#tab_2">3</ref>. Obviously, the accuracy of concept detection results would directly determine the quality of sentence generation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Image matching for caption prediction</head><p>In this method, we employed the algorithm commonly used in the image-text retrieval field to match images and captions for instance-level retrieval. It is based on an unsupervised assumption that every image/test group can be viewed as one class, so each category is equivalent to 1+m (1 image vs m descriptions) samples.</p><p>We use the model proposed by Zheng <ref type="bibr" coords="5,226.09,319.09,14.45,9.90" target="#b6">[7]</ref>, which contains two convolutional neural networks to learn visual and textual representations simultaneously. When testing, we first extract the image feature by image CNN and the text feature by text CNN, and then use the cosine distance to evaluate the similarity between the image and candidate sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Data Preparation In this field, most existing works use two generic retrieval datasets (Flickr30k and MSCOCO), which have more than 30,000 images. Each image in these datasets is annotated with around five sentences. So we expanded the caption from 1 to 5 sentences per image. Specifically, we first translate the caption into Chinese, Japanese, German, French and then translate back to English. We use GoogleNews-vectors word2vec model trained by Google, which contains 2,000,000 words to get our dictionary. Our dictionary ultimately have 6039 words, each has a 1*300 vector corresponding to it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Train Given a sentence, we convert it into code T of size n * d, where n is the length of the sentence, and d denotes the size of the dictionary. T is used as the input for the text CNN. Given an image, we resize it to 224 × 224 pixels, which are randomly cropped.</p><p>The training process includes two stages: in the first stage, we use the instance loss to learn fine-grained differences between intra-modal samples with similar semantics. in the second stage, we use the ranking loss to focus on the distance between the two modalities to build the relationship between the image and text.</p><p>• Test In this experiment, we use 16,280 sentences from training set and validation set as candidate captions, each sentence is corresponding to its text feature extracted by text CNN. For each test image, we first extract the image feature by image CNN, and then use the cosine distance to evaluate the similarity between the image and candidate sentences.</p><p>When we use the model trained on ImageCLEF datasets, we get the almost same top 10 sentences from 16,280 candidate captions, because the features learned by text CNN between each captions is not discriminative. However, when we test it on the model trained by MSCOCO datasets, each query image can get different sentences, but they do not match either.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Submitted runs</head><p>Table <ref type="table" coords="5,93.76,754.46,5.50,9.90" target="#tab_3">4</ref> presents the 9 runs we submitted to the concept detection subtask, along with the official rankings. We take the Inception-V3 model trained on overall concepts as a baseline. We tried to submit concepts of the three semantic categories predicted by sub MLC models. The submissions were either by categories or by combining with the baseline results. To our surprise, the concepts of Imaging Types achieved the best F1 score of 0.419, indicating the high precision and coverage of this kind of concepts in radiology images. As to the concepts from other types and baseline results, they introduce more unmentioned words and reduce the overall score. However, in view of our experience on manual labeling, we believe that some unmentioned words may also be helpful in interpreting the given image. Figure <ref type="figure" coords="6,130.00,121.29,5.50,9.90" target="#fig_2">3</ref> shows two examples of our method on the validation set.</p><p>Table <ref type="table" coords="6,95.00,140.04,5.50,9.90" target="#tab_4">5</ref> shows the 4 runs we submitted to the caption prediction subtask. We take the Dual path CNN model as our baseline, which achieved a BLEU score of 0.137. The pattern-based method achieved a BLEU score of 0.257, still far from satisfactory descriptions that are readable and interpretable for doctors.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>This paper presents the participation of the ImageSem Group at the ImageCLEFmed Caption 2021 task. We tried different strategies for both subtasks. In the concept detection subtask, we used the transfer learning-based MLC model to detect overall 1,586 concepts. We also trained multiple fine-grained MLC models based on manually annotated semantic categories. One of the lessons is that we have become much clearer about which concepts are clinically relevant to radiology images, and in order to obtain better predictions, the semantic labels of images should be more focused and specific. Furthermore, how to generate a readable description based on clear and clinically meaningful concepts, is still worth exploring.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,133.05,302.10,328.43,11.00;3,72.00,133.12,453.07,166.60"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Workflow of ImageSem in the ImageCLEFmed Caption 2021 task</figDesc><graphic coords="3,72.00,133.12,453.07,166.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,87.28,543.17,420.10,11.00;4,72.00,288.73,446.70,250.78"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Process of manual re-annotation and fine-grained MLC model training and validation</figDesc><graphic coords="4,72.00,288.73,446.70,250.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,97.55,737.23,399.79,11.00;6,72.00,448.30,444.29,283.25"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Examples of concepts and captions predicted by ImageSem on the validation set</figDesc><graphic coords="6,72.00,448.30,444.29,283.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,72.03,391.62,453.92,349.61"><head>Table 1</head><label>1</label><figDesc>High-frequency concepts in the training and validation set of ImageCLEFmed caption 2021 task.</figDesc><table coords="2,77.78,406.87,440.29,334.36"><row><cell>CUI</cell><cell cols="2">#Num Term String</cell><cell>TUI</cell><cell cols="2">Semantic Type</cell></row><row><cell cols="3">C0040398 1400 Tomography, Emission-Computed</cell><cell cols="3">T060 Diagnostic Procedure</cell></row><row><cell cols="2">C0024485 796</cell><cell>Magnetic Resonance Imaging</cell><cell cols="3">T060 Diagnostic Procedure</cell></row><row><cell cols="2">C1306645 627</cell><cell>Plain x-ray</cell><cell cols="3">T060 Diagnostic Procedure</cell></row><row><cell cols="2">C0041618 373</cell><cell>Ultrasonography</cell><cell cols="3">T060 Diagnostic Procedure</cell></row><row><cell cols="2">C0009924 283</cell><cell>Contrast Media</cell><cell cols="2">T130 Indicator,</cell><cell>Reagent,</cell><cell>or</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Diagnostic Aid</cell></row><row><cell cols="2">C0577559 274</cell><cell>Mass of body structure</cell><cell cols="2">T033 Finding</cell></row><row><cell cols="2">C0002978 119</cell><cell>angiogram</cell><cell cols="3">T060 Diagnostic Procedure</cell></row><row><cell cols="2">C0221198 108</cell><cell>Lesion</cell><cell cols="2">T033 Finding</cell></row><row><cell cols="2">C1322687 107</cell><cell>Endoscopes, Gastrointestinal Tract,</cell><cell cols="3">T074 Medical Device</cell></row><row><cell></cell><cell></cell><cell>Upper Tract</cell><cell></cell><cell></cell></row><row><cell cols="2">C0205400 92</cell><cell>Thickened</cell><cell cols="2">T033 Finding</cell></row><row><cell cols="2">C1881358 78</cell><cell>Large Mass</cell><cell cols="2">T033 Finding</cell></row><row><cell cols="2">C0202823 60</cell><cell>Chest CT</cell><cell cols="3">T060 Diagnostic Procedure</cell></row><row><cell cols="2">C0005910 59</cell><cell>Body Weight</cell><cell cols="3">T032 Organism Attribute</cell></row><row><cell cols="2">C0150312 55</cell><cell>Present</cell><cell cols="2">T033 Finding</cell></row><row><cell cols="2">C0180459 53</cell><cell>Disks (device)</cell><cell cols="3">T073 Manufactured Object</cell></row><row><cell cols="2">C0003617 52</cell><cell>Appendix</cell><cell cols="3">T023 Body Part, Organ, or Organ</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Component</cell></row><row><cell cols="2">C0228134 50</cell><cell>Spinal epidural space</cell><cell cols="3">T030 Body Space or Junction</cell></row><row><cell cols="2">C0016658 47</cell><cell>Fracture</cell><cell cols="3">T037 Injury or Poisoning</cell></row><row><cell cols="2">C0005889 47</cell><cell>Body Fluids</cell><cell cols="3">T031 Body Substance</cell></row><row><cell cols="2">C0227613 47</cell><cell>Right kidney</cell><cell cols="3">T023 Body Part, Organ, or Organ</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Component</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,72.03,569.17,398.63,72.03"><head>Table 2</head><label>2</label><figDesc>Distribution of concepts from different semantic categories</figDesc><table coords="4,77.78,584.42,392.88,56.78"><row><cell>Category</cell><cell>#Concepts</cell><cell>Concept Sample</cell></row><row><cell>Imaging Types</cell><cell>99</cell><cell>C0040398 Tomography Emission-Computed</cell></row><row><cell>Anatomic Structure</cell><cell>786</cell><cell>C0228134 Spinal epidural space</cell></row><row><cell>Findings</cell><cell>854</cell><cell>C0577559 Mass of body structure</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,72.03,112.03,444.61,86.82"><head>Table 3</head><label>3</label><figDesc>Sentence pattern for caption generation</figDesc><table coords="5,77.78,127.30,438.86,71.55"><row><cell>Pattern</cell><cell>Sample</cell></row><row><cell>&lt;image&gt; of &lt;body&gt; demonstrate / show</cell><cell>synpic24243: Sagittal T1-weighted image of the</cell></row><row><cell>/suggest &lt;findings&gt;</cell><cell>cervical spine demonstrates cord expansion.</cell></row><row><cell>&lt;image&gt; demonstrate / show / suggest</cell><cell>synpic19193: Lateral radiograph of the skull shows</cell></row><row><cell>&lt;findings&gt; in/of/within &lt;body&gt;</cell><cell>lytic lesions in the temporoparietal region.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,72.03,196.05,437.84,149.12"><head>Table 4</head><label>4</label><figDesc>Submissions of ImageSem in the concept detection subtask</figDesc><table coords="6,86.03,211.33,423.84,133.84"><row><cell>Approach</cell><cell cols="2">F1 Score Ranking</cell></row><row><cell>03ImagingTypes</cell><cell>0.419</cell><cell>14</cell></row><row><cell>02Comb_ImagingTypes_Baseline</cell><cell>0.400</cell><cell>16</cell></row><row><cell>07Intersect_06_baseline</cell><cell>0.396</cell><cell>17</cell></row><row><cell>04Comb_ImagingTypes_AnatomicStructure</cell><cell>0.370</cell><cell>19</cell></row><row><cell>05Comb_ImagingTypes_MedicalFindings</cell><cell>0.355</cell><cell>22</cell></row><row><cell>06Comb_ImagingTypes_AnatomicStructure_Findings</cell><cell>0.327</cell><cell>24</cell></row><row><cell>08AnatomicStructure</cell><cell>0.037</cell><cell>28</cell></row><row><cell>09Findings</cell><cell>0.019</cell><cell>29</cell></row><row><cell>01baseline</cell><cell>0.380</cell><cell>18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,72.03,350.85,423.96,81.59"><head>Table 5</head><label>5</label><figDesc>Submissions of ImageSem in the caption prediction subtask</figDesc><table coords="6,86.03,366.12,409.96,66.32"><row><cell>Approach</cell><cell>BLEU</cell></row><row><cell>04pattern1+ImagingTypes_AnatomicStructure_Findings</cell><cell>0.203</cell></row><row><cell>05pattern2+ImagingTypes_AnatomicStructure_Findings</cell><cell>0.181</cell></row><row><cell>06pattern3+ImagingTypes_AnatomicStructure_Findings</cell><cell>0.257</cell></row><row><cell>03baseline_Dual_Path_CNN Model</cell><cell>0.137</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6.">Acknowledgements</head><p>This work has been supported by the <rs type="funder">National Natural Science Foundation of China</rs> (Grant No. <rs type="grantNumber">61906214</rs>), the <rs type="funder">Beijing Natural Science Foundation</rs> (Grant No. <rs type="grantNumber">Z200016</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_fgdBsUe">
					<idno type="grant-number">61906214</idno>
				</org>
				<org type="funding" xml:id="_Pc7myEv">
					<idno type="grant-number">Z200016</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,88.05,339.59,435.46,9.90;7,88.05,352.34,435.12,9.90;7,88.05,364.86,435.34,9.90;7,88.05,377.61,438.15,9.90;7,88.05,390.36,57.75,9.90" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,422.57,364.86,100.81,9.90;7,88.05,377.61,196.33,9.90">The 2021 ImageCLEF Benchmark: Multimedia Retrieval in Medical</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Peteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Moustahfid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,291.25,377.61,203.67,9.90">Nature, Internet and Social Media Applications</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="616" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.05,402.86,435.51,9.90;7,88.05,415.50,435.59,10.01;7,88.05,428.11,158.94,9.90" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,304.61,402.86,218.94,9.90;7,88.05,415.61,100.38,9.90">Overview of the ImageCLEFmed 2021 concept &amp; caption prediction task</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,211.00,415.61,118.63,9.90;7,347.26,415.50,176.38,10.01;7,88.05,428.11,31.69,9.90">CEUR&apos; Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>CLEF2021 Working Notes</note>
</biblStruct>

<biblStruct coords="7,88.05,440.86,435.65,9.90;7,88.05,453.39,195.80,9.90" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,162.00,440.86,357.02,9.90">The unified medical language system (umls): integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,88.05,453.39,105.94,9.90">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="267" to="270" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.05,466.14,435.53,9.90;7,88.05,478.89,222.02,9.90" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<title level="m" coord="7,254.63,466.14,268.94,9.90;7,88.05,478.89,151.34,9.90">Imagesem at imageclefmed caption 2019 task: a two-stage medical concept detection strategy</title>
		<meeting><address><addrLine>Lugano</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.05,491.39,435.09,9.90;7,88.05,504.14,216.58,9.90" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="7,345.82,491.39,177.32,9.90;7,88.05,504.14,69.77,9.90">Rethinking the inception architecturefor computer vision</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.05,516.64,435.28,9.90;7,88.05,529.39,136.71,9.90" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="7,316.60,516.64,188.47,9.90">Densely connected convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Laurens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.05,542.16,435.73,9.90;7,88.05,554.66,435.59,9.90;7,88.05,567.41,294.53,9.90" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,366.09,542.16,157.69,9.90;7,88.05,554.66,127.04,9.90">Dual-path convolutional image-text embedding with instance loss</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-D</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3383184</idno>
		<ptr target="https://doi.org/10.1145/3383184" />
	</analytic>
	<monogr>
		<title level="j" coord="7,221.04,554.66,302.60,9.90;7,88.05,567.41,56.27,9.90">ACM Transactions on Multimedia Computing, Communications, and Applications</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.05,579.92,435.66,9.90;7,88.05,592.66,308.56,9.90" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,245.90,579.92,277.81,9.90;7,88.05,592.66,70.90,9.90">Imagesem at imageclef 2018 caption task: Image retrieval and transfer learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,180.26,592.66,105.03,9.90">Clef2018 working notes</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.05,605.41,435.70,9.90;7,88.05,617.94,338.47,9.90" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Androutsopoulos</forename></persName>
		</author>
		<title level="m" coord="7,294.93,605.41,208.57,9.90;7,88.05,617.94,175.96,9.90">Aueb nlp group at imageclefmed caption 2019</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note>Clef2019 working notes, CEUR-WS.org</note>
</biblStruct>

<biblStruct coords="7,88.05,630.69,435.43,9.90;7,88.05,643.19,435.56,9.90;7,88.05,655.94,69.19,9.90" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,320.55,630.69,202.93,9.90;7,88.05,643.19,299.38,9.90">Overview of the imageclef 2020: Multimedia retrieval in medical, lifelogging, nature, and internet applications</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">B</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="7,396.68,643.19,126.93,9.90;7,88.05,655.94,34.35,9.90">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.05,668.69,435.28,9.90;7,88.05,681.19,435.47,9.90;7,88.05,693.94,89.03,9.90" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,88.05,681.19,272.14,9.90">Bernstein, Imagenet large scale visual recognition challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,368.92,681.19,154.60,9.90;7,88.05,693.94,29.24,9.90">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.05,706.47,435.65,9.90;7,88.05,719.21,117.28,9.90" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="7,162.97,706.47,360.73,9.90;7,88.05,719.21,35.20,9.90">Effective mapping of biomedical text to the umls metathesaurus: the metamap program</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Aronson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="17" to="21" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
