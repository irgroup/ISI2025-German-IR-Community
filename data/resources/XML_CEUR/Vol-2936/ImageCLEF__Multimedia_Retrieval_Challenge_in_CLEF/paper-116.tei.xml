<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,105.69,347.53,15.42;1,89.29,127.60,389.57,15.42;1,89.29,149.52,115.77,15.43">Kdelab at ImageCLEF 2021: Medical Caption Prediction with Effective Data Pre-processing and Deep Learning</title>
				<funder ref="#_9KgauP6">
					<orgName type="full">Education and Research in Toyohashi University of Technology</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,177.84,66.70,11.96"><forename type="first">Riku</forename><surname>Tsuneda</surname></persName>
							<email>tsuneda.riku.am@kde.cs.tut.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Toyohashi University of Technology</orgName>
								<address>
									<settlement>Aichi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,168.63,177.84,85.40,11.96"><forename type="first">Tetsuya</forename><surname>Asakawa</surname></persName>
							<email>asakawa@kde.cs.tut.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Toyohashi University of Technology</orgName>
								<address>
									<settlement>Aichi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,285.03,177.84,64.67,11.96"><forename type="first">Masaki</forename><surname>Aono</surname></persName>
							<email>aono@tut.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Toyohashi University of Technology</orgName>
								<address>
									<settlement>Aichi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,105.69,347.53,15.42;1,89.29,127.60,389.57,15.42;1,89.29,149.52,115.77,15.43">Kdelab at ImageCLEF 2021: Medical Caption Prediction with Effective Data Pre-processing and Deep Learning</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">E08839BB184EEE2EA8448ADE63DCD533</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image Captioning</term>
					<term>Deep Learning</term>
					<term>Medical Images</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ImageCLEF 2021 Caption Prediction Task is an example of a challenging research problem in the field of image captioning. The goal of this research is to automatically generate accurate captions describing a given medical image. We describe our approach to captioning medical images and illustrate the text and image pre-processing that is effective for our task dataset. In this paper, we have applied sentence-ending period removal as text pre-processing and histogram normalization of luminance as image pre-processing. Furthermore, we present the effectiveness of our text data augmentation approach. Submission of our kdelab team on the task test dataset achieved a BLEU evaluating of 0.362.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, multimodal processing of images and natural language has attracted much attention in the field of machine learning. Image Captioning is one of these representative tasks, which aims at proper captioning of input images. As these accuracies improve, it is expected that computers will not only be able to detect objects in images, but also to understand the relationships and behaviors between objects.</p><p>Image captioning is also effective in the medical field. For example, interpreting and summarizing possible disease symptoms from a large number of radiology images (e.g. X-ray images and CT images) is a time-consuming task that can only be understood by highly knowledgeable specialists. If computers could understand medical images and generate accurate captions, it would help solve the world's growing shortage of medical doctors. However, there is still the bottleneck problem that few physicians are able to give accurate annotations.</p><p>In this paper, we describe our approach to general Image Captioning task in medical domain at Image Captioning such as Fig. <ref type="figure" coords="1,237.48,574.08,3.97,10.91" target="#fig_0">1</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(right).</head><p>The nature of medical images are quite different from general images such as MS-COCO [1] in many aspects. In the following, we first describe related work on Image Captioning task and Medical Image Captioning in Section 2, followed by the description of the dataset provided for Image-CLEF2021 <ref type="bibr" coords="2,135.21,413.78,12.68,10.91" target="#b1">[2]</ref> Medical Image Captioning <ref type="bibr" coords="2,267.13,413.78,13.07,10.91" target="#b2">[3]</ref>dataset in Section 3. In Section 4, we describe details of the method we have applied, and then of our experiments we have conducted in Section 5. We finally conclude this paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In the field of image recognition, convolutional neural networks (CNN), including VGG <ref type="bibr" coords="2,474.75,499.51,13.39,10.91" target="#b3">[4]</ref>,and ResNet <ref type="bibr" coords="2,123.82,513.06,11.51,10.91" target="#b4">[5]</ref>, have been widely used. In the field of natural language processing for text understanding, encoder-decoder models (seq2seq) <ref type="bibr" coords="2,286.41,526.60,12.78,10.91" target="#b5">[6]</ref> have been the mainstream, but in recent years Transformers <ref type="bibr" coords="2,156.63,540.15,12.99,10.91" target="#b6">[7]</ref> such as BERT <ref type="bibr" coords="2,238.35,540.15,13.00,10.91" target="#b7">[8]</ref> have become common. The Image Captioning task is a fusion of image recognition and sentence generation, and lies in the middle of these two.</p><p>For example, Oriol Vinyals et al. proposed caption generation using an encoder-decoder model <ref type="bibr" coords="2,120.06,580.80,11.58,10.91" target="#b8">[9]</ref>, and Kelvin Xu et al. proposed Show, Attend and Tell, which adds visual attention to the encoder-decoder model <ref type="bibr" coords="2,231.61,594.35,16.41,10.91" target="#b9">[10]</ref>. Recently, P. Anderson et al. presented a model using Bottom-Up Attention obtained by pre-training a Faster-R-CNN used for object detection <ref type="bibr" coords="2,485.01,607.90,16.25,10.91" target="#b10">[11]</ref>.</p><p>In addition, the Caption Prediction Task is the first time of its kind to be held at an Im-ageCLEF conference. However, a similar task, the VQA-Med task <ref type="bibr" coords="2,385.54,635.00,16.41,10.91" target="#b11">[12]</ref>, has been contested at ImageCLEF2018, 2019, and 2020.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset of ImageCLEF 2021 Caption Prediction</head><p>For the ImageCLEF 2021 Medical Caption Prediction task, organizers have provided us with a training set of 2,756 radiology images with the same number of captions, a validation set of 500 radiology images with the same number of captions, and a test set of 444 radiology images with the same number of captions. We are supposed to use these as our datasets. Most of the images in the dataset are non-colored, and they potentially include non-essential logos and texts. The task participants have to generate automatic caption based on radiology image data.</p><p>According to our analysis, the top word frequencies were dominated by prepositions and words such as right and left that indicate position. The word cloud of case-insensitive words and the top 14 ranking words in terms of word frequency are summarized in Figure <ref type="figure" coords="3,456.02,219.67,4.97,10.91" target="#fig_1">2</ref> and Table <ref type="table" coords="3,89.04,233.22,3.74,10.91" target="#tab_0">1</ref>, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodlogy</head><p>The overview of our Medical Image Captioning methodology is divided into three main parts, as shown in Figure <ref type="figure" coords="3,176.26,648.19,3.74,10.91" target="#fig_2">3</ref>.</p><p>The first is the image and text pre-processing. As preliminaries, we propose a method for pre-processing the images and text in the dataset. The second is the encoder part. In the encoder part, the features of the image are extracted. The third is the decoder part. In the decoder part, words are predicted recursively using LSTM <ref type="bibr" coords="4,288.49,252.62,17.91,10.91" target="#b12">[13]</ref> and attention mechanism.</p><p>We have adopted Show, Attend and Tell as the base model. This model is known to have high accuracy among Image Captioning models that do not use object detection such Faster R-CNN <ref type="bibr" coords="4,124.41,293.27,16.25,10.91" target="#b13">[14]</ref>. The image processing consists of two steps. In the first step, we normalize images using histogram smoothing based on the luminance of the image. In the second step, we resize all images to a size of 256 × 256.</p><p>We have tried two ways to normalize the luminance distribution of an image. The first is histogram flattening. Histogram flattening is a method of smoothing the luminance distribution of the entire image. When flattened, the contrast of the image is enhanced and the image becomes clearer. The second is adaptive histogram flattening. This method performs the histogram flattening described in the first method on a small area of the image. In general, this technique can reduce the occurrence of tone jumps. A comparison of the raw image and the pre-processed image is shown in Figure <ref type="figure" coords="5,268.51,141.16,3.74,10.91" target="#fig_3">4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Text Pre-processing</head><p>We preprocess the text by removing and lowercasing periods in the captions of the training data. In general, the MS-COCO captioning task is not case-sensitive, and it is well known that symbols such as periods had better be removed. If there are multiple captions for a single image, only the period in the last caption is should be removed. As a contribution to these, the period is recognized as one of the words in the sentence, since the period is present only in the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Caption Data Expanding using EDA</head><p>We tried EDA (Easy Data Augmentation) <ref type="bibr" coords="5,280.05,616.95,18.07,10.91" target="#b14">[15]</ref> as an extension of our text dataset. EDA is a text classification task in natural language processing, and is an effective method that works well when the dataset is small. In a typical captioning task using MS-COCO, five captions are provided for one image. However, in the ImageCLEF2021 dataset, only one caption per image is provided. We have tested the effectiveness of this approach using various data expansion methods in EDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Neural network model</head><p>As a base neural network model for caption generation, we have adopted "Show, Attend and Tell" model <ref type="bibr" coords="6,147.33,163.79,16.41,10.91" target="#b9">[10]</ref>. This model is capable of highly accurate captioning without using object detection. The architecture of the models is almost the same, but our model differs in that we employ ResNet-101 <ref type="bibr" coords="6,180.94,190.89,12.84,10.91" target="#b4">[5]</ref> instead of VGG16 <ref type="bibr" coords="6,280.08,190.89,12.84,10.91" target="#b3">[4]</ref> as the CNN encoder .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Setting up hyper-parameters and performing pre-processing with validation data</head><p>We experimented with hyper-parameter adjustment and image pre-processing using training and validation data. As noted in 4.1, all characters in the train caption data are lowercased.</p><p>We have setup the following hyper-parameters as follows; batch size as 32, optimization function as "Adam" with a decoder learning rate of 0.001 , and the number of epochs 200. For the implementation, we employ PyTorch1.7.1 <ref type="bibr" coords="6,298.81,338.63,18.07,10.91" target="#b15">[16]</ref> as our deep learning framework. For the evaluation of captioning , we utilize BLEU4 <ref type="bibr" coords="6,280.28,352.18,16.09,10.91" target="#b16">[17]</ref>. Table <ref type="table" coords="6,329.72,352.18,4.97,10.91">2</ref> shows the results. Here we compare in terms of BLEU for data pre-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Validation BLEU-4 of two data pre-processing ("val" in the table means validation. )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Pre-processing val BLEU-4 None 0.432 Xu et al. <ref type="bibr" coords="6,199.04,447.13,16.46,8.87" target="#b9">[10]</ref> Histogram Normalization 0.437 Adaptive Histogram Normalization 0.436</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">The results with test data</head><p>The test dataset consists of the test images distributed as described in 4.1. The test image consists of 444 medical images, without the correct answer captions. In contrast to the text pre-processing in 5.1, the captions used in the training have been all lowercased and the periods at the end of sentences were deleted. Table <ref type="table" coords="6,127.83,574.73,5.17,10.91" target="#tab_1">3</ref> shows the BLEU results for the test data. In the experiments on the test data, the BLEU evaluation was the highest when Histogram Normalization was used. Example of our seemingly successful caption generation results are shown in Fig <ref type="figure" coords="6,380.78,601.83,3.74,10.91" target="#fig_4">5</ref>.</p><p>Table <ref type="table" coords="6,126.39,615.38,4.97,10.91" target="#tab_2">4</ref> shows the BLEU ratings for the EDA attempts. The pre-processing of the dataset uses the method that achieved the highest BLEU rating in Table <ref type="table" coords="6,344.13,628.93,3.66,10.91" target="#tab_1">3</ref>. Using EDA's synonym substitution and other methods, we compare the case of adding one caption, two captions, and four captions. In all cases where data expansion has been performed using EDA, the BLEU rating has dropped.   The results of the submissions of the participants with the highest BLEU values are shown in Table <ref type="table" coords="7,115.79,597.24,3.74,10.91" target="#tab_3">5</ref>. Our rank turns out to be 4th of participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We have described our system with which we submitted to the ImageCLEF2021 Caption Prediction task. In our system, we have done our own data pre-processing, and have attempted to </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,337.11,416.70,8.93;2,89.29,349.12,203.07,8.87;2,89.29,84.19,416.69,234.39"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of general (left) and medical (right) Caption Prediction data&amp; left image : via MS-COCO, [CC BY 4.0](https://cocodataset.org/).</figDesc><graphic coords="2,89.29,84.19,416.69,234.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,89.29,559.91,187.88,8.93;3,89.29,255.94,416.69,279.46"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Word Cloud of caption descriptions</figDesc><graphic coords="3,89.29,255.94,416.69,279.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,89.29,507.80,219.61,8.93;4,89.29,315.50,416.69,167.78"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Over view of the our captioning framework</figDesc><graphic coords="4,89.29,315.50,416.69,167.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,89.29,447.60,200.47,8.93;5,89.29,163.88,416.69,259.20"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Raw images and normalization images</figDesc><graphic coords="5,89.29,163.88,416.69,259.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,89.29,438.61,164.41,8.93;7,89.29,179.71,416.69,234.39"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Example of generated caption</figDesc><graphic coords="7,89.29,179.71,416.69,234.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.81,90.49,334.83,111.08"><head>Table 1</head><label>1</label><figDesc>Word frequrncy Ranking in Dataset</figDesc><table coords="4,171.63,118.18,252.02,83.39"><row><cell>Rank</cell><cell>Word</cell><cell>Freq</cell><cell>Rank</cell><cell>Word</cell><cell>Freq</cell></row><row><cell>1</cell><cell>right</cell><cell>824</cell><cell>7</cell><cell>axial</cell><cell>372</cell></row><row><cell>2</cell><cell>left</cell><cell>672</cell><cell>8</cell><cell>images</cell><cell>327</cell></row><row><cell>3</cell><cell>mass</cell><cell>616</cell><cell>9</cell><cell>image</cell><cell>326</cell></row><row><cell>4</cell><cell>ct</cell><cell>534</cell><cell>10</cell><cell>within</cell><cell>272</cell></row><row><cell>5</cell><cell cols="2">demonstrates 442</cell><cell>11</cell><cell>lesion</cell><cell>246</cell></row><row><cell>6</cell><cell>contrast</cell><cell>373</cell><cell>12</cell><cell cols="2">demonstrate 244</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,88.99,90.49,414.81,75.61"><head>Table 3</head><label>3</label><figDesc>The results of experiment for Image pre-processing for test data ("val" in the table means validation. )</figDesc><table coords="7,140.41,118.58,314.45,47.52"><row><cell>Model</cell><cell>Image Pre-processing</cell><cell cols="2">val BLEU test BLEU</cell></row><row><cell></cell><cell>None</cell><cell>0.436</cell><cell>0.332</cell></row><row><cell>Xu et al. [10]</cell><cell>Histogram Normalization</cell><cell>0.451</cell><cell>0.362</cell></row><row><cell></cell><cell>Adaptive Histogram Normalization</cell><cell>0.443</cell><cell>0.352</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,88.99,471.76,367.97,87.57"><head>Table 4</head><label>4</label><figDesc>The Results of using EDA to extend training data.</figDesc><table coords="7,138.32,499.85,318.64,59.48"><row><cell>Image Pre-processing</cell><cell cols="3">Added Caption by EDA val BLEU test BLEU</cell></row><row><cell></cell><cell>None</cell><cell>0.451</cell><cell>0.362</cell></row><row><cell>Histogram Normalization</cell><cell>one caption two captions</cell><cell>0.417 0.397</cell><cell>0.339 0.291</cell></row><row><cell></cell><cell>four captions</cell><cell>0.384</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,88.99,90.49,417.38,224.85"><head>Table 5</head><label>5</label><figDesc>The best participants' runs submitted for the Caption Prediction task add data augmentation with EDA. In addition, two types of luminance smoothing and period removal were applied to image and text pre-processing. The results demonstrate that these processes have improved the caption prediction accuracy of the neural network model. EDA turns out to be ineffective in this task. Finally, from organizer's evaluation, we have achieved a BLEU score of 0.362 in the ImageCLEF2021Caption Prediction task, placing us 4th.</figDesc><table coords="8,218.65,118.58,157.97,107.30"><row><cell>Group Name</cell><cell cols="2">Rank BLEU</cell></row><row><cell>IALab_PLC</cell><cell>1</cell><cell>0.510</cell></row><row><cell>AUEB_NLP_GROUP</cell><cell>2</cell><cell>0.461</cell></row><row><cell>AEHRC-CSIRO</cell><cell>3</cell><cell>0.432</cell></row><row><cell>kdelab</cell><cell>4</cell><cell>0.362</cell></row><row><cell>jeanbenoit_delbrouck</cell><cell>5</cell><cell>0.285</cell></row><row><cell>ImageSem</cell><cell>6</cell><cell>0.257</cell></row><row><cell>RomiBed</cell><cell>7</cell><cell>0.243</cell></row><row><cell>ayushnanda14</cell><cell>8</cell><cell>0.103</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>A part of this research was carried out with the support of <rs type="grantName">Grant for</rs> <rs type="funder">Education and Research in Toyohashi University of Technology</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_9KgauP6">
					<orgName type="grant-name">Grant for</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,112.66,435.23,394.53,10.91;8,112.66,448.78,393.33,10.91;8,112.33,462.33,285.48,10.91" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="8,215.55,448.78,194.68,10.91">Microsoft COCO: common objects in context</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1405.0312" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,475.87,395.01,10.91;8,112.66,489.42,394.53,10.91;8,112.48,502.97,395.18,10.91;8,112.66,516.52,394.53,10.91;8,112.28,530.07,393.70,10.91;8,112.66,543.62,393.33,10.91;8,112.66,557.17,393.33,10.91;8,112.66,570.72,393.53,10.91;8,112.66,584.27,197.61,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,264.32,530.07,241.66,10.91;8,112.66,543.62,261.75,10.91">Overview of the ImageCLEF 2021: Multimedia retrieval in medical, nature, internet and social media applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tauteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Moustahfid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,401.27,543.62,104.72,10.91;8,112.66,557.17,393.33,10.91;8,112.66,570.72,201.15,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 12th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="8,348.61,570.72,157.57,10.91;8,112.66,584.27,31.10,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,597.82,394.53,10.91;8,112.66,611.37,394.62,10.91;8,112.66,624.92,393.33,10.91;8,112.41,638.46,393.57,10.91;8,112.66,652.01,256.93,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,161.95,611.37,325.47,10.91">Overview of the ImageCLEFmed 2021 concept &amp; caption prediction task</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,112.66,624.92,393.33,10.91;8,112.41,638.46,272.58,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 12th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="8,416.21,638.46,89.77,10.91;8,112.66,652.01,90.42,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,86.97,393.33,10.91;9,112.66,100.52,181.11,10.91" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="9,242.31,86.97,263.68,10.91;9,112.66,100.52,51.39,10.91">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>CoRR abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,114.06,393.32,10.91;9,112.66,127.61,395.00,10.91;9,112.66,141.16,137.64,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,254.59,114.06,207.56,10.91">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,112.66,127.61,307.90,10.91">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,154.71,394.53,10.91;9,112.66,168.26,393.32,10.91;9,112.66,181.81,374.72,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,261.17,154.71,240.91,10.91">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,127.45,168.26,378.53,10.91;9,112.66,181.81,42.77,10.91;9,204.99,181.81,32.84,10.91">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
	<note>NIPS&apos;14</note>
</biblStruct>

<biblStruct coords="9,112.66,195.36,394.53,10.91;9,112.66,208.91,394.62,10.91;9,112.31,222.46,218.89,10.91" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="9,186.71,208.91,120.59,10.91">Attention Is All You Need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1706.03762" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,236.01,393.33,10.91;9,112.33,249.56,395.33,10.91;9,112.66,263.11,187.21,10.91" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="9,318.80,236.01,187.18,10.91;9,112.33,249.56,184.43,10.91">BERT Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1810.04805" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,276.66,394.53,10.91;9,112.66,290.20,395.01,10.91;9,112.66,303.75,219.00,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,296.39,276.66,206.33,10.91">Show and tell: A neural image caption generator</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298935</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,127.24,290.20,333.84,10.91">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,317.30,394.53,10.91;9,112.66,330.85,393.33,10.91;9,112.66,344.40,365.41,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1502.03044" />
		<title level="m" coord="9,112.66,330.85,358.91,10.91">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,357.95,393.33,10.91;9,112.66,371.50,394.62,10.91;9,112.66,385.05,239.89,10.91" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="9,439.42,357.95,66.56,10.91;9,112.66,371.50,228.30,10.91">Bottom-up and top-down attention for image captioning and VQA</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1707.07998" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,398.60,393.73,10.91;9,112.66,412.15,393.33,10.91;9,112.66,425.70,394.53,10.91;9,112.66,439.25,184.68,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,461.99,398.60,44.40,10.91;9,112.66,412.15,393.33,10.91;9,112.66,425.70,102.25,10.91">Overview of the VQA-Med Task at ImageCLEF 2021: Visual Question Answering and Generation in the Medical Domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,242.16,425.70,118.67,10.91">CLEF 2021 Working Notes</title>
		<title level="s" coord="9,368.95,425.70,138.23,10.91;9,112.66,439.25,38.13,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,452.79,393.98,10.91;9,112.41,466.34,48.96,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,250.98,452.79,114.99,10.91">Long Short-Term Memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,375.74,452.79,93.13,10.91">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,479.89,393.33,10.91;9,112.66,493.44,395.01,10.91;9,112.66,506.99,127.84,10.91" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="9,268.81,479.89,237.18,10.91;9,112.66,493.44,114.22,10.91">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1506.01497" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,520.54,393.33,10.91;9,112.66,534.09,395.00,10.91;9,112.66,550.08,97.35,7.90" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="9,194.19,520.54,311.80,10.91;9,112.66,534.09,99.02,10.91">EDA: easy data augmentation techniques for boosting performance on text classification tasks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zou</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1901.11196" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,561.19,394.53,10.91;9,112.66,574.74,395.17,10.91;9,112.66,588.29,395.17,10.91;9,112.66,601.84,394.62,10.91;9,112.66,615.39,239.89,10.91" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="9,409.41,588.29,98.42,10.91;9,112.66,601.84,227.95,10.91">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Köpf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1912.01703" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,628.93,393.33,10.91;9,112.66,642.48,393.33,10.91;9,112.66,656.03,393.33,10.91;9,112.66,669.58,395.01,10.91;10,112.66,86.97,268.32,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="9,438.91,628.93,67.08,10.91;9,112.66,642.48,218.96,10.91">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
		<ptr target="https://www.aclweb.org/anthology/P02-1040.doi:10.3115/1073083.1073135" />
	</analytic>
	<monogr>
		<title level="m" coord="9,359.51,642.48,146.48,10.91;9,112.66,656.03,251.14,10.91">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
