<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,331.40,15.42;1,89.29,106.66,324.22,15.42;1,89.29,128.58,106.29,15.43">Lijie at ImageCLEFmed Tuberculosis 2021: EfficientNet Simplified Tuberculosis Case Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.10,156.89,24.97,11.96"><forename type="first">Jie</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Yunnan University</orgName>
								<address>
									<postCode>650091</postCode>
									<settlement>Kunming</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,126.72,156.89,36.62,11.96"><forename type="first">Li</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">North Minzu University</orgName>
								<address>
									<postCode>750021</postCode>
									<settlement>Ningxia</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,194.33,156.89,42.80,11.96"><forename type="first">Yang</forename><surname>Bai</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Yunnan University</orgName>
								<address>
									<postCode>650091</postCode>
									<settlement>Kunming</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,331.40,15.42;1,89.29,106.66,324.22,15.42;1,89.29,128.58,106.29,15.43">Lijie at ImageCLEFmed Tuberculosis 2021: EfficientNet Simplified Tuberculosis Case Classification</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">8BE5D29A51C688C2D04E5DAB81251B35</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>EfficientNet</term>
					<term>Transfer learning</term>
					<term>Tuberculosis classification</term>
					<term>Ghost</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Tuberculosis has been a bacterial infectious disease known to humans for a long time, and different types of tuberculosis cases have different treatment methods. Therefore, rapid and accurate positioning of its classification is essential for the treatment of tuberculosis. This paper describes our experiment in ImageCLEF 2021 Tuberculosis-TBT classification. We first use two versions of automatically extracted masks of the lungs which are provided by the organizer to process all the CT images. Then the image is transformed into a 2-dimensional through data preprocessing that is projection, and data enhancement is used in the training process. Through the modified version of the EfficientNet network pre-trained on Image Net, and the introduction of the ghost module, and fine-tuning the model after migration learning, the one classification task with 5 labels required by this challenge was completed. The final kappa score of our team in ImageCLEF 2021 Tuberculosis-TBT classification is 0.015, and the acc score is 0.380.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Tuberculosis is a chronic, late-onset, highly morbid, and dangerous respiratory infectious disease. It is a bacterial disease, an airborne disease that uses droplets released when a patient coughs to attack the respiratory system. According to statistics, about 1.5 million people died of chronic tuberculosis in 2018, and the number of people infected with tuberculosis is about 6 times more than the number of people who die from tuberculosis. However, with the advancement of medicine, most patients can be cured clinically if they can be diagnosed in time, treated rationally, improved resistance, or reduced cell-mediated allergic reactions. Therefore, it is very important to detect and diagnose tuberculosis cases quickly and accurately.</p><p>At present, the clinical diagnosis of tuberculosis is mainly a comprehensive analysis of the patient's clinical manifestations, imaging data, and laboratory test results. Among them, the use of CT with higher resolution and more detailed evaluation is more common. When tuberculosis affects the lungs, several visual patterns can be seen in CT images. These models can better reflect the characteristics of the type of tuberculosis, and using these characteristics to determine the classification of the specific type of tuberculosis. Due to the large delay between the onset of disease symptoms and the clinical manifestations or diagnosis of tuberculosis, it is important to find a quick and convenient early diagnosis method within this delay. In this way, the use of antibiotics, a general treatment method that harms the body, can be reduced, and the efficiency of early diagnosis of tuberculosis can be improved.</p><p>ImageCLEF <ref type="bibr" coords="2,155.30,181.81,13.00,10.91" target="#b0">[1]</ref> began to organize image classification and retrieval challenges in 2003 <ref type="bibr" coords="2,492.22,181.81,11.59,10.91" target="#b1">[2]</ref>. Among them, the tuberculosis task only started in 2007 and the tasks vary from year to year. The previous tasks include multidrug resistance (MDR) tuberculosis detection, tuberculosis type (TBT) classification and severity score (SVR), etc. The other difference lies in the changes in the data set. Due to the difficulty of each task, the official plan is to solve them one by one. Therefore, the task of 2021 <ref type="bibr" coords="2,210.36,249.56,12.84,10.91" target="#b2">[3]</ref> was focused on the classification of tuberculosis types (TBT).</p><p>The rest of this article is organized as follows: In the second part, we briefly introduce the literature review on ImageCLEF Tuberculosis. The third part introduces the specific tasks of this competition and analyses the data set in detail. Then, in the fourth part, we introduce the network model and transfer learning method. The fifth part introduces the relevant details of the experiment. In the sixth part, we report the results of our experiment. Finally, in the seventh part, we summarize and discuss the current and future development directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Image recognition is considered to be the earliest application field of machine learning. In recent years, deep learning <ref type="bibr" coords="2,208.99,403.03,12.68,10.91" target="#b3">[4]</ref> has achieved amazing results in the field of medical imaging, and machine learning and deep learning have gradually appeared in tuberculosis diagnosis. Among them, deep learning is also frequently used to diagnose CT scan images of lung diseases. With the rapid development and improvement of the mathematics and computing power of human computers, CNN is considered to be an advanced feedforward neural network containing convolution calculation and its deep structure <ref type="bibr" coords="2,274.57,470.77,11.28,10.91" target="#b4">[5]</ref>. It also has good image classification performance and can be used for various image classification tasks. Therefore, CNN is also widely used in the diagnosis of tuberculosis.</p><p>Since the appearance of the tuberculosis task in the ImageCLEF competition in 2017, the approximate methods are to manually select meaningful axial CT slices first. Then using the semantic features extracted by two-dimensional CNN to describe these slices. Finally, the neural network sets the classifier to make the final category classification representation, such as the method of the MostaganemFSEI group <ref type="bibr" coords="2,258.23,565.62,11.28,10.91" target="#b5">[6]</ref>. Or by dividing the lung field into several sub-regions, and treating these sub-regions as nodes of a graph, the lungs can be modeled as a graph. Then, they defined the weighted edges between adjacent sub-regions, where the weighted edges encode the distance between the 3D texture descriptors obtained in each sub-region (node). To compare the obtained pictures, they converted these pictures into a lung description vector and used a support vector machine to classify them, such as MedGIFT <ref type="bibr" coords="2,384.22,633.36,11.43,10.91" target="#b6">[7]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Task and Dataset</head><p>The tuberculosis task in ImageCLEF 2021 is based on 1,338 chest 3D CT image data of TB patients. There are 5 labels: Infiltrative, Focal, Tuberculoma, Miliary, Fibro-cavernous". Those types are originally labeled manually, and the specific images of each category are shown in Figure <ref type="figure" coords="3,500.04,505.25,3.81,10.91" target="#fig_0">1</ref>. Compared with 2017 and 2018, the data set has been updated. The training set provides metadata items including LeftLungAffected, RightLungAffected, LungCapacityDecrease, Calculation, Pleurisy, Caverns.</p><p>The tuberculosis classification task in 2021 is actually one of the 2017 and 2018 subtasks. The original file stores the original voxel intensity and the corresponding image metadata. Among them, there are 917 images in the training set and 421 images in the test set. In addition, the organizer provides two different segmentation methods for us to use the data. The mask#1 <ref type="bibr" coords="3,493.19,600.09,12.80,10.91" target="#b7">[8]</ref> provided by the first segmentation method is more accurate but the most severe TB cases will miss larger lung abnormal areas. The second mask#2 <ref type="bibr" coords="3,355.88,627.19,13.79,10.91" target="#b8">[9]</ref>inaccurately provides rougher boundaries, but it is more stable in terms of including the lesion area. In the experiment, we conducted experiments on the data processed by the two methods and also proved that the effect of the second method is better than the first one. Table <ref type="table" coords="3,368.20,667.84,5.17,10.91" target="#tab_0">1</ref>   information, and Table <ref type="table" coords="4,193.61,543.09,5.07,10.91">2</ref> lists the number of classified tag information. The ImageCLEF2021 tuberculosis task initially provided a data set in the form of NIfTI 3D, and we processed the data according to the two officially provided masks before starting the experiment. Then project on each coordinate axis to transform into a two-dimensional image. In this way, we can better invest in the neural network for training. Figure <ref type="figure" coords="4,441.27,597.29,5.17,10.91" target="#fig_1">2</ref> is the image projected on the x, y, z-axis with mask#1, and Figure <ref type="figure" coords="4,322.52,610.84,4.99,10.91" target="#fig_2">3</ref> is the image projected on the x, y, z-axis with mask#2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>lists the number of metadata</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Model</head><p>In recent years, when dealing with image classification problems, we tend to choose networks with a large number of calculations for processing. Facts have proved that network structures like ResNet combined with DenseNet and Attention tend to show good results in the extraction of image features. But those have some disadvantage such as the amount of calculation is too large, and the network transmission speed will be much slower than that of a lightweight network. Therefore, to meet the requirements of better feature extraction and improved speed, we use the EfficientNet+Ghost module.</p><p>Convolutional neural networks are generally developed under limited hardware resources and budgets. If more resources are available in the experiment, which can help us get better accuracy by expanding the scale of the model. The expanded model is generally divided into three aspects: model width, depth, and input image resolution. Most of the existing models adjust these three aspects manually, but such a combination space is too large to exhaust manpower, so the EfficientNet model came into being. It mainly uses a composite coefficient to simultaneously enlarge the network from the three dimensions of model depth, width, and resolution. Based on neural structure search technology, an optimal set of parameters (composite coefficients) can be obtained <ref type="bibr" coords="5,219.33,534.03,16.32,10.91" target="#b9">[10]</ref>. Figure <ref type="figure" coords="5,274.23,534.03,5.12,10.91" target="#fig_3">4</ref> is the basic model, and Figure <ref type="figure" coords="5,417.71,534.03,5.12,10.91" target="#fig_4">5</ref> is the EfficientNet model. We can see that the three dimensions have been expanded, and how much is expanded is obtained through the compound model expansion method combined with neural structure search technology.</p><p>Since the existing CNN extracts a large number of redundant features for image cognition, Han <ref type="bibr" coords="5,111.81,601.77,17.70,10.91" target="#b10">[11]</ref>et al. proposed GhostNet, which effectively reduces the computational cost. They found that there are many similar feature maps in the traditional convolutional layer, and these feature maps are called ghosts. The main contributions are as follows: They divide the traditional convolutional layer into two parts. In the first part, there are fewer convolution kernels directly used for feature extraction, which is the same as the original convolution. Then, these features are linearly transformed to obtain multiple feature maps, as shown in Figure <ref type="figure" coords="5,500.12,669.52,3.77,10.91" target="#fig_4">5</ref>. They proved that the Ghost module is also applicable to other CNN models. Ghost is different from 1*1 convolution in that it can control the size of the convolution kernel without restriction and use the same method of parallel mapping and linear transformation. We can use the Ghost module as a simple and convenient and effective component to improve the convolutional neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Output</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General convolution</head><p>Input Output Conv ... </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ghost Module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Transfer Learning</head><p>Due to the complexity of 3D CT image data and the diversity of data representation, including changes in the size and shape of CT image voxels, many image analysis algorithms are difficult to apply correctly, and the complexity and high cost of manually labeling 3D CT images, resulting in a lack of a large amount of high-quality labeling data. So transfer learning can transform existing data to acquire basic knowledge to assist future learning. Transfer Learning <ref type="bibr" coords="6,456.62,656.03,19.08,10.91" target="#b11">[12]</ref>aims to use the knowledge gained from the current context as the main means to complete learning in another context. It is also a method of machine learning. The parameters of the training model developed for task A can be used as the initial point and transferred to the model developed for task B to help train the new model. Initializing the network with migration parameters can improve generalization performance. Even migrating from tasks that are not particularly similar is better than using random filters (or random parameters). Because deep learning models require a lot of training time and training data to get good results, and transfer learning can alleviate this necessity. This method has shown remarkable performance in a variety of medical image classification frameworks <ref type="bibr" coords="7,237.97,181.81,16.43,10.91" target="#b12">[13,</ref><ref type="bibr" coords="7,257.13,181.81,12.32,10.91" target="#b13">14]</ref>.</p><p>In the experiment, we transfer the feature extraction capabilities learned by the neural network on the ImageNet dataset as prior knowledge to the feature extraction of CT image projection. After retaining the weights of EfficientNet on its data set, first, we choose to freeze the weights of the first k layers in the pre-trained model. Then we train the remaining n-k layers. Finally, it needs to be adjusted according to the corresponding output and format to make it meet the output of the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Data preprocessing</head><p>In the experiment, because the official data set is the 3D image, and more computational overhead is required. In the case of limited GPU memory, heavy memory and computational requirements need to be overcome. Therefore, it is necessary for us to first convert the image into the 2D image in the image preprocessing, and it is also necessary to preserve the information of each dimension of the 3D image as much as possible. The method we use is to first process the data according to the two officially provided mask methods. Then the image is projected on the three coordinate axes, namely sagittal, frontal and axial projection, which makes the image easier to process and also increases the amount of data, which makes the training information richer. In addition, some data enhancement methods are used to prevent overfitting and refine image details, such as image enhancement by rotation, cropping, or whitening techniques. Since there is no verification set, we will separate the training set from the training set for verification at a ratio of 3:1.</p><p>The main steps that need to be performed to generate a CT projection image are: the voxel intensity value of the CT image is increased by 1024 Hounsfield Units (HU) to ensure that it is only a positive intensity value. To eliminate the effect of image segmentation defects on the edge of the lung mask, an ellipsoid structure element with a radius of 10 is used to corrode the lung membrane along the XY plane. The radius along the Z-axis is calculated as the inter-layer distance of a specific CT image. The image intensity value outside the corroded lung mask is set to zero. The intensity threshold can be used to filter low-intensity noise voxels. The advantage of this method over traditional slice representation is that each projection obtained by averaging the value of voxel intensity contains information about all slices in the image. On the one hand, this method greatly reduces the complexity of input and training data transmission and greatly reduces the task of learning on limited training data. On the other hand, generating projections along different axes (X, Y, Z) provides additional "native" enhancements to the input data.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Perspectives For Future Work</head><p>In this task, before experimenting with images, the way of preprocessing the data needs to be improved. For example, the model used can only process 2D images, so that a lot of important feature information is lost when the 3D projection is changed to 2D. In addition, the pre-training before transfer learning is performed on the ImageNet dataset. However, general image data sets and medical data sets are still quite different. Therefore, there is still much room for improvement in these two specific aspects of data preprocessing.</p><p>In this competition, since we are only limited to the implementation of deep learning knowledge. However, due to our lack of medical knowledge and in-depth understanding of CT images, the importance of medical image features or label information has been neglected, resulting in poor results. In addition, we should be aware of the latest developments in deep learning, and we should not only limit the design of models to CNN. Recently, many excellent teams have begun to study the work of visual Transformer. The Transformer module has achieved amazing results in the field of natural language processing and has become a standard configuration in the field of NLP, but in the field of imagery, CNN (such as ResNet, DenseNet, etc.) occupies most of the SOTA results. In future work, we will apply the Transformer in the NLP field to the image field as much as possible. I believe that shortly, Transformer can also achieve the same excellent results in the image field as the NLP field.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,416.24,206.46,8.93;3,89.29,84.19,416.68,307.53"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The specific images of each TB category</figDesc><graphic coords="3,89.29,84.19,416.68,307.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.29,505.49,252.80,8.93;4,89.29,352.13,416.71,128.84"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The image projected on the x, y, z axis with mask#1</figDesc><graphic coords="4,89.29,352.13,416.71,128.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,89.29,247.90,252.80,8.93;5,89.29,84.19,416.70,139.20"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The image projected on the x, y, z axis with mask#2</figDesc><graphic coords="5,89.29,84.19,416.70,139.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,89.29,331.41,223.57,8.93"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison of basic model and EfficientNet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,89.29,534.84,199.46,8.93"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of basic model and Ghost</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="9,89.29,265.06,202.25,8.93;9,89.29,176.09,416.69,64.45"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The final official score of this challenge</figDesc><graphic coords="9,89.29,176.09,416.69,64.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.99,90.49,298.59,180.43"><head>Table 1</head><label>1</label><figDesc>Metadata the amount of information in the training set</figDesc><table coords="4,88.99,122.10,298.59,148.82"><row><cell>Filename</cell><cell cols="2">In Training set</cell></row><row><cell>RightLungAffected</cell><cell></cell><cell>197</cell></row><row><cell>LeftLungAffected</cell><cell></cell><cell>181</cell></row><row><cell cols="2">LungCapacityDecrease</cell><cell>69</cell></row><row><cell>Calcification</cell><cell></cell><cell>28</cell></row><row><cell>Pleurisy</cell><cell></cell><cell>13</cell></row><row><cell>Caverns</cell><cell></cell><cell>104</cell></row><row><cell>Table 2</cell><cell></cell></row><row><cell cols="2">The number of classified label information in the training set</cell></row><row><cell>Filename</cell><cell cols="2">In Training set</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,88.99,90.49,260.93,69.87"><head>Table 3</head><label>3</label><figDesc>Results achieved on local validation set for two mask types</figDesc><table coords="9,245.35,122.10,104.57,38.25"><row><cell cols="2">ValidationSet Accuracy</cell></row><row><cell>MASK#1</cell><cell>0.416</cell></row><row><cell>MASK#2</cell><cell>0.432</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experimental model and analysis</head><p>The experimental process is shown in figure <ref type="figure" coords="8,296.07,351.24,3.81,10.91">6</ref>. We first convert 3D images into 2D images through preprocessing methods such as projection. Then through the EfficientNet network, and the network uses transfer learning, and transfers the feature extraction capabilities learned on the Image Net dataset as prior knowledge to the feature extraction of CT image projection.</p><p>Here we simplify the structure of the original EfficientNet, remove the final fully connected layer, and use the new linear layer to implement the one classification task with 5 labels of this year's task. Subsequently, we unfroze the final convolutional layer of EfficientNet and iterated normally. This prevents the network from overfitting during training and allows us to train the model for a longer time for better generalization. In addition, we turn the 1*1 convolution in EfficientNet into a ghost module to effectively reduce costs. Due to the official provision of two masks, we use the processed data of the two masks to train separately, and the final results show that mask2 performs better. The specific model structure of the experiment is shown in Figure <ref type="figure" coords="8,120.36,513.83,3.74,10.91">6</ref>. Among them, the EfficientNet module has been streamlined and improved.</p><p>In our experiment, a total of 100 epochs were trained. The Adam used by the optimizer has a learning rate of 1e-05 and a batch size of 32. During the training process, it stabilized at about 60 epochs, and then we fine-tuned the model, and finally performed best at 100 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head><p>After all the CT images have been processed by the two masks, then the training set is divided into the verification set according to 3:1, and the accuracy rates of the final verification sets of the two mask data are shown in Table <ref type="table" coords="8,259.38,640.20,3.70,10.91">3</ref>. The evaluation methods used in this ImageCLEF 2021 Tuberculosis-TBT classification task are kappa and acc. Our final kappa score is 0.015 and our acc score is 0.380, as shown in Figure <ref type="figure" coords="8,257.06,667.30,3.74,10.91">7</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,601.84,395.01,10.91;9,112.66,615.39,394.53,10.91;9,112.48,628.93,395.18,10.91;9,112.66,642.48,394.53,10.91;9,112.28,656.03,393.70,10.91;9,112.66,669.58,393.33,10.91;10,112.66,86.97,393.33,10.91;10,112.66,100.52,393.53,10.91;10,112.66,114.06,197.61,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,264.32,656.03,241.66,10.91;9,112.66,669.58,261.75,10.91">Overview of the ImageCLEF 2021: Multimedia retrieval in medical, nature, internet and social media applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Peteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tauteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Moustahfid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,401.27,669.58,104.72,10.91;10,112.66,86.97,393.33,10.91;10,112.66,100.52,201.15,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 12th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="10,348.61,100.52,157.57,10.91;10,112.66,114.06,31.10,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,127.61,393.33,10.91;10,112.39,141.16,362.72,10.91" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="10,324.80,127.61,181.19,10.91;10,112.39,141.16,120.79,10.91">ImageCLEF: experimental evaluation in visual information retrieval</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,154.71,395.17,10.91;10,112.66,168.26,394.53,10.91;10,112.66,181.81,322.05,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,393.45,154.71,114.38,10.91;10,112.66,168.26,257.62,10.91">Overview of ImageCLEFtuberculosis 2021 -CT-based tuberculosis type classification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,392.48,168.26,110.11,10.91">CLEF2021 Working Notes</title>
		<title level="s" coord="10,112.66,181.81,175.50,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,195.36,393.33,10.91;10,112.66,208.91,393.33,10.91;10,112.66,222.46,201.34,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,264.29,195.36,241.70,10.91;10,112.66,208.91,120.44,10.91">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,256.22,208.91,249.77,10.91;10,112.66,222.46,103.42,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,236.01,394.52,10.91;10,112.66,249.56,393.98,10.91;10,112.66,263.11,38.81,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,140.56,249.56,224.49,10.91">Recent advances in convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,373.97,249.56,89.89,10.91">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="354" to="377" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,276.66,394.62,10.91;10,112.66,290.20,366.18,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,437.07,276.66,70.20,10.91;10,112.66,290.20,210.88,10.91">Imageclef 2019: Deep learning for tuberculosis ct image analysis</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hamadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">B</forename><surname>Cheikh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zouatine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M B</forename><surname>Menad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Djebbara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,350.23,290.20,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,303.75,393.33,10.91;10,112.66,317.30,394.97,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,205.78,303.75,300.21,10.91;10,112.66,317.30,240.14,10.91">Lung graph-model classification with svm and cnn for tuberculosis severity assessment and automatic ct report generation</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,379.99,317.30,97.96,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,330.85,393.60,10.91;10,112.66,344.40,393.33,10.91;10,112.33,357.95,393.94,10.91;10,112.66,371.50,393.33,10.91;10,112.66,385.05,173.47,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,424.12,330.85,82.14,10.91;10,112.66,344.40,232.90,10.91">Efficient and fully automatic segmentation of the lungs in ct volumes</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">Dicente</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">A</forename><surname>Jiménez Del Toro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Depeursinge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="10,330.31,357.95,175.96,10.91;10,112.66,371.50,178.39,10.91">Proceedings of the VISCERAL Anatomy Grand Challenge at the 2015 IEEE ISBI</title>
		<title level="s" coord="10,298.87,371.50,166.23,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">O</forename><surname>Goksel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Jiménez Del Toro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Foncubierta-Rodríguez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<meeting>the VISCERAL Anatomy Grand Challenge at the 2015 IEEE ISBI</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,398.60,393.33,10.91;10,112.66,412.15,394.53,10.91;10,112.66,425.70,263.63,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,223.08,398.60,282.91,10.91;10,112.66,412.15,100.42,10.91">Imageclef 2017: Supervoxels and co-occurrence for tuberculosis ct image classification</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="10,242.30,412.15,116.63,10.91">CLEF2017 Working Notes</title>
		<title level="s" coord="10,367.79,412.15,139.40,10.91;10,112.66,425.70,45.79,10.91">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,439.25,394.53,10.91;10,112.66,452.79,352.61,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,178.42,439.25,323.86,10.91">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,127.29,452.79,207.49,10.91">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,466.34,393.33,10.91;10,112.66,479.89,393.32,10.91;10,112.66,493.44,149.51,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,338.09,466.34,167.90,10.91;10,112.66,479.89,44.35,10.91">Ghostnet: More features from cheap operations</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,179.82,479.89,326.16,10.91;10,112.66,493.44,51.39,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1580" to="1589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,506.99,393.33,10.91;10,112.66,520.54,395.01,10.91" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="10,208.09,506.99,77.17,10.91;10,309.07,506.99,196.92,10.91;10,112.66,520.54,263.15,10.91">Handbook of research on machine learning applications and trends: algorithms, methods, and techniques</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Torrey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shavlik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>IGI global</publisher>
			<biblScope unit="page" from="242" to="264" />
		</imprint>
	</monogr>
	<note>Transfer learning</note>
</biblStruct>

<biblStruct coords="10,112.66,534.09,394.53,10.91;10,112.66,547.64,393.61,10.91;10,112.66,561.19,242.33,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,252.74,547.64,253.52,10.91;10,112.66,561.19,117.19,10.91">Identifying medical diagnoses and treatable diseases by image-based deep learning</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">S</forename><surname>Kermany</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Goldbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Valentim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">L</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,238.16,561.19,17.68,10.91">Cell</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="1122" to="1131" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,574.74,393.33,10.91;10,112.66,588.29,269.43,10.91" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="10,307.51,574.74,198.48,10.91;10,112.66,588.29,86.87,10.91">Transfusion: Understanding transfer learning for medical imaging</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07208</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
