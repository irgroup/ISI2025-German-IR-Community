<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,382.04,15.42;1,89.29,106.66,188.37,15.42;8,89.29,101.55,359.67,9.96;8,184.35,121.17,26.14,9.96">Overview of the ImageCLEFmed 2021 Concept &amp; Caption Prediction Task</title>
				<funder>
					<orgName type="full">University of Essex GCRF QR Engagement Fund</orgName>
				</funder>
				<funder ref="#_eQ5PHRN">
					<orgName type="full">Research England</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,68.00,11.96"><forename type="first">Obioma</forename><surname>Pelka</surname></persName>
							<email>obioma.pelka@fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Diagnostic and Interventional Radiology and Neuroradiology</orgName>
								<orgName type="institution">University Hospital Essen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,175.22,134.97,89.25,11.96"><forename type="first">Asma</forename><forename type="middle">Ben</forename><surname>Abacha</surname></persName>
							<email>asma.benabacha@nih.gov</email>
							<affiliation key="aff2">
								<orgName type="department">National Library of Medicine</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,277.11,134.97,117.48,11.96"><forename type="first">Alba</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Essex</orgName>
								<address>
									<addrLine>Wivenhoe Park</addrLine>
									<postCode>CO4 3SQ</postCode>
									<settlement>Colchester</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.10,148.92,106.65,11.96"><forename type="first">Janadhip</forename><surname>Jacutprakart</surname></persName>
							<email>j.jacutprakart@essex.ac.uk</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Essex</orgName>
								<address>
									<addrLine>Wivenhoe Park</addrLine>
									<postCode>CO4 3SQ</postCode>
									<settlement>Colchester</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,208.40,148.92,111.78,11.96"><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
							<email>christoph.friedrich@fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Institute for Medical Informatics, Biometry and Epidemiology (IMIBE)</orgName>
								<orgName type="institution">University Hospital Essen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,356.47,148.92,78.20,11.96"><forename type="first">Henning</forename><surname>Müller</surname></persName>
							<email>henning.mueller@hevs.ch</email>
							<affiliation key="aff5">
								<orgName type="institution">University of Applied Sciences Western Switzerland (HES-SO)</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution">University of Geneva</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,382.04,15.42;1,89.29,106.66,188.37,15.42;8,89.29,101.55,359.67,9.96;8,184.35,121.17,26.14,9.96">Overview of the ImageCLEFmed 2021 Concept &amp; Caption Prediction Task</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">8990F46814F5B0403187362BF82B304D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Concept Detection</term>
					<term>Computer Vision</term>
					<term>ImageCLEF 2021</term>
					<term>Image Understanding</term>
					<term>Image Modality</term>
					<term>Radiology</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The 2021 ImageCLEF concept detection and caption prediction task follows similar challenges that were already run from 2017-2020. The objective is to extract UMLS-concept annotations and/or captions from the image data that are then compared against the original text captions of the images. The used images are clinically relevant radiology images and the describing captions were created by medical experts. In the caption prediction task, lexical similarity with the original image captions is evaluated with the BLEU-score. In the concept detection task, UMLS (Unified Medical Language System) terms are extracted from the original text captions and compared against the predicted concepts in a multilabel way. The F1-score was used to assess the performance. The 2021 task has been conducted in collaboration with the Visual Question Answering task and used the same images. The task attracted a strong participation with 25 registered teams. In the end 10 teams submitted 75 runs for the two sub tasks. Results show that there is a variety of used techniques that can lead to good prediction results for the two tasks. In comparison to earlier competitions, more modern deep learning architectures like EfficientNets and Transformer-based architectures for text or images were used.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper sets forth the approaches for the caption prediction task: automated cross-referencing of medical images and captions into predicted coherent captions implying Unified Medical Language System ® (UMLS) concept detection in radiology images as a first step. This task is a part of the ImageCLEF benchmarking campaign, which has proposed medical image understanding tasks since 2003; a new suite of tasks is generated each subsequent year. Further information on the other proposed tasks at ImageCLEF 2021 can be found in Ionescu et al. <ref type="bibr" coords="2,436.99,114.06,11.43,10.91" target="#b0">[1]</ref>. This is the 5th edition of the ImageCLEFcaption task. Although in 2020 the format of the task was the single task of concept detection, this year the task has expanded to include both concept detection sub task and bring back a caption prediction sub task, as the caption prediction sub task was included in the ImageCLEFmed Caption task in 2016 <ref type="bibr" coords="2,363.93,168.26,12.75,10.91" target="#b1">[2]</ref> (as a pilot sub task), 2017 <ref type="bibr" coords="2,492.07,168.26,11.34,10.91" target="#b2">[3]</ref>, and 2018 <ref type="bibr" coords="2,131.77,181.81,11.50,10.91" target="#b3">[4]</ref>. In this edition, ImageCLEF 2021 uses actual radiology images annotated by real doctors, which means that the results achieved are highly relevant within a medical context.</p><p>Manual generation of the knowledge of medical images is a time-consuming process prone to human error. As this process is yet necessary assisting for the better and easier diagnoses of diseases that are susceptible to radiology screening, it is important that we better understand and refine automatic systems that aid in the broad task of radiology-image metadata generation. The purpose of the ImageCLEFmed 2021 concept detection and caption prediction tasks is the continued evaluation of such systems. Concept detection and caption prediction information is applicable for unlabelled and unstructured data sets and medical data sets that do not have textual metadata. The ImageCLEF caption task focuses on the medical image understanding in the biomedical literature and specifically on concept extraction and caption prediction based on the visual perception of the medical images and medical text data such as medical caption or UMLS® Unique Identifiers (CUIs) paired with each image (see Figure <ref type="figure" coords="2,398.50,344.40,3.57,10.91" target="#fig_0">1</ref>).</p><p>For the development data, the same data set used in the ImageCLEFVQA task <ref type="bibr" coords="2,461.36,357.95,12.99,10.91" target="#b4">[5]</ref> where radiology images were selected from the MedPix<ref type="foot" coords="2,311.00,369.74,3.71,7.97" target="#foot_0">1</ref> database. To make the task more realistic and linked to the real-world data, the curated annotated data was used in contrast to earlier years where images were extracted from medical publications. The test set used for the official evaluation was obtained from the same source as proposed in <ref type="bibr" coords="2,365.95,412.15,11.43,10.91" target="#b0">[1]</ref>.</p><p>This paper presents an overview of the ImageCLEF caption task 2021 including the task and participation in Section 2, the data creation in Section 3, and the evaluation methodology in Section 4. The results are described in Section 5, followed by conclusion in Sections 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task and Participation</head><p>In 2021, the ImageCLEFcaption task consisted of two sub tasks: concept detection and caption prediction.</p><p>The concept detection sub task follows the same format proposed since the start of the task in 2017. Participants are asked to predict a set of concepts defined by the Unified Medical Language System ® (UMLS) Concept Unique Identifiers (CUIs) <ref type="bibr" coords="2,373.15,565.62,12.99,10.91" target="#b5">[6]</ref> (UMLS-CUI) based on the visual information provided by the radiology images.</p><p>The caption prediction sub task follows the original format of the sub task used between 2017 and 2018. The task was run again because of participant demand. This sub task aims to define automatic captions for the radiology images provided.</p><p>In 2021, 25 teams registered and signed the End-User-Agreement that is needed to download the development data. 10 teams submitted 75 runs for evaluation (8 teams submitted working notes) attracting more attention than in 2020. Each of the groups was allowed a maximum of 10 graded runs per sub task.</p><p>Table <ref type="table" coords="3,127.27,114.06,5.12,10.91">1</ref> shows all the teams who participated in the task and their submitted runs. 5 teams participated in the concept detection sub task this year, two of those teams participated also in 2020. 8 teams submitted runs to the caption prediction sub task. However, two teams decided not to submit working notes describing the used techniques.  In the previous editions, the data set distributed for the task originates from biomedical articles of the PMC Open Access<ref type="foot" coords="3,238.48,541.92,3.71,7.97" target="#foot_1">2</ref>  <ref type="bibr" coords="3,245.66,543.68,16.41,10.91" target="#b14">[15]</ref>. To make the task more realistic, in this fifth edition, the collection contains real radiology images annotated by medical doctors. The data set used is the same as the ImageCLEFVQA task <ref type="bibr" coords="3,273.17,570.77,13.00,10.91" target="#b4">[5]</ref> where radiology images were selected from the MedPix<ref type="foot" coords="3,122.61,582.57,3.71,7.97" target="#foot_2">3</ref> database. Only cases where the diagnosis was made based on the image were selected and their annotations were used as a basis for the extraction of the concepts and captions. A semi-automatic text pre-processing was applied to improve the quality of the data and to extract the concepts (UMLS-CUI) using the captions, location, and diagnosis as filters. The curated data</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data Creation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Participating groups in the ImageCLEF 2021 caption task and their runs submitted to both sub tasks: T1-Concept Detection and T2-Caption prediction. Teams with previous participation in 2020 are marked with an asterisk.</p><p>Team Institution Runs T1 Runs T2 AEHRC-CSIRO <ref type="bibr" coords="4,160.92,151.28,11.83,9.96" target="#b6">[7]</ref> Australian included radiology images categorised into seven sub-classes indicating the image acquisition technique with a corresponding set of concepts.</p><p>We have also validated all the captions manually and checked the coherence of the generated concepts in the training, validation, and test sets.</p><p>The following subsets were distributed to the participants where each image has one caption and multiple concepts (UMLS-CUI):</p><p>• Training set including 2,756 images and associated captions and concepts.</p><p>• Validation set including 500 images and associated captions and concepts.</p><p>• Test set including 444 images (and associated reference captions and concepts).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation Methodology</head><p>The performance evaluation follows the approach used in the previous edition in evaluating both sub tasks separately. For the concept detection sub task, the balanced precision and recall trade-off were measured in terms of F1 scores. Caption prediction performance is assessed on the basis of BLEU-scores <ref type="bibr" coords="5,184.35,249.99,16.20,10.91" target="#b15">[16]</ref>. Candidate captions are lower cased, stripped of all punctuation and English stop words. Finally, to increase coverage, Snowball stemming was applied. BLEU-scores are computed per reference image, treating each entire caption as a sentence, even though it may contain multiple natural sentences. Average BLEU-scores across all test images was reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>For the concept detection and caption prediction sub tasks, Tables <ref type="table" coords="5,386.60,362.61,4.99,10.91" target="#tab_1">2</ref> and<ref type="table" coords="5,413.22,362.61,4.99,10.91">3</ref> show all the results of the participating team. The results will be discussed in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Results for the Concept Detection sub task</head><p>In 2021, five teams participated in the the concept prediction sub task submitting 29 runs. Table <ref type="table" coords="5,501.01,425.70,4.97,10.91" target="#tab_1">2</ref> presents the results achieved in the submissions.</p><p>The AUEB NLP Group from Athens University of Economics (Greece) submitted the best performing result with an F1-score of 0.505 <ref type="bibr" coords="5,279.85,466.34,11.28,10.91" target="#b7">[8]</ref>. They submitted the best results also in previous years and extended their earlier work. They used Ensembles of classifiers based on DenseNet-121 <ref type="bibr" coords="5,107.19,493.44,18.01,10.91" target="#b16">[17]</ref> and in this year added Networks that have been trained with supervised contrastive learning <ref type="bibr" coords="5,130.13,506.99,16.42,10.91" target="#b17">[18]</ref>. These are followed by a feed-forward Neural Network (FFNN), which acts as the classifier layer on the top. Other submissions are more information retrieval oriented and use CNN encoders of recent architectures like EfficientNet-B0 <ref type="bibr" coords="5,368.92,534.09,17.96,10.91" target="#b18">[19]</ref> and create an ensemble of image embeddings. The networks were first pre-trained on the ImageNet data set <ref type="bibr" coords="5,447.06,547.64,17.76,10.91" target="#b20">[20]</ref> and then fine-tuned using the ImageCLEF 2021 concept detection data set. Several aggregation methods such as the intersection, majority voting, and union of predicted concepts were experimented. The system with the majority voting of concepts from image embeddings achieved the overall highest F1-Score.</p><p>The second best system was proposed by NLIP-Essex-ITESM, a joint team from University of Essex (UK) and ITESM (Mexico). They reached an F1-score of 0.469 and a detailed description of their work is presented in <ref type="bibr" coords="5,206.43,642.48,16.22,10.91" target="#b12">[13]</ref>. They also proposed two routes, an information retrieval based approach and a multi-label classification system. For the information retrieval approach image embeddings from ImageNet <ref type="bibr" coords="5,213.80,669.58,17.80,10.91" target="#b20">[20]</ref> pretrained DenseNet-121 <ref type="bibr" coords="5,346.86,669.58,17.80,10.91" target="#b16">[17]</ref> and EfficientNet <ref type="bibr" coords="5,441.10,669.58,17.80,10.91" target="#b18">[19]</ref> have been  <ref type="bibr" coords="6,408.75,550.50,16.19,10.91" target="#b9">[10]</ref>. They used multilabel classification with DenseNet <ref type="bibr" coords="6,248.07,564.05,18.07,10.91" target="#b16">[17]</ref> and Inception-V3 <ref type="bibr" coords="6,351.52,564.05,18.07,10.91" target="#b21">[21]</ref> networks. Interestingly, they submitted models for subgroups of concepts and reached the best results by predicting only the Imaging Types. The subgroup of Imaging Types contains 99 of the 1,586 concepts from the dataset.</p><p>In the concept prediction sub task the IALab PUC from Pontificia Universidad Católica de Chile reached an F1-score of 0.360. The best submission of this group, described in <ref type="bibr" coords="6,465.63,631.80,18.07,10.91" target="#b10">[11]</ref> uses image embeddings with Learned Perceptual Image Path Similarity (LPIPS) <ref type="bibr" coords="6,421.67,645.35,17.91,10.91" target="#b22">[22]</ref> based on VGG <ref type="bibr" coords="6,89.29,658.90,17.91,10.91" target="#b23">[23]</ref> models.</p><p>The RomiBed group from University of Oulu (Finland) reached an F1-score of 0.143 and described their approach in <ref type="bibr" coords="7,199.32,100.52,16.09,10.91" target="#b13">[14]</ref>. They used an image embedding from a MobileNet-v2 architecture and added a GRU layer for the prediction.</p><p>To summarize, in the concept detection sub task, the groups typically used deep learning models trained as multi-label classificators or more Information Retrieval oriented solutions. For the IR solutions, image embeddings from deep learning models are typically used. In this year, more modern deep learning architectures like EfficientNets <ref type="bibr" coords="7,376.42,168.26,17.83,10.91" target="#b18">[19]</ref> and Visual Transformers (ViT) <ref type="bibr" coords="7,117.50,181.81,17.91,10.91" target="#b24">[24]</ref> were proposed for the solutions. This year's models for concept detection show again increased F1-scores in comparison to earlier years. This could partly be explained by a smaller number of potential concepts in the images. More modern architectures have been used and show improvements. Transformerbased architectures and solutions arrived at both sub tasks. This year, machine learning-based methods and information retrieval oriented solutions were used more equally by all groups. In former years the majority of proposed solutions used multi-label approaches. A few participants noticed that less complex solutions showed the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results for the Caption prediction task sub task</head><p>In this fifth edition, the caption prediction sub task attracted 8 teams which submitted 40 runs. Table <ref type="table" coords="9,142.41,121.08,5.15,10.91">3</ref> presents the results of the submissions. Two groups, jeanbenoit_delbrouck and ayushnanda14 decided not to submit working notes and therefore no description about the approaches is available.</p><p>The best model for the caption prediction sub task was presented by IALab PUC from Pontificia Universidad Católica de Chile. They reached a BLEU-score of 0.510 and described the methods in <ref type="bibr" coords="9,141.24,188.83,11.35,10.91" target="#b8">[9]</ref>. Three methods were tested, a statistical oriented method, a similarity based on LPIPS <ref type="bibr" coords="9,117.38,202.38,17.76,10.91" target="#b22">[22]</ref> and a multi-label classification (MLC) approach. The MLC approach used a ResNet34 <ref type="bibr" coords="9,89.29,215.93,18.07,10.91" target="#b25">[25]</ref> network and ordered the predicted caption words based on statistical analysis from the training set.</p><p>The AUEB NLP Group from Athens University of Economics (Greece) submitted the second best performing result with a BLEU-score of 0.461 <ref type="bibr" coords="9,312.20,256.58,11.34,10.91" target="#b7">[8]</ref>. Two different approaches were tested, a Show Attend and Tell <ref type="bibr" coords="9,187.75,270.13,17.85,10.91" target="#b26">[26]</ref> approach and an ensemble of different image embeddings. The best result came from the ensemble with embeddings from CNN architectures like DenseNet <ref type="bibr" coords="9,487.24,283.68,16.34,10.91" target="#b16">[17]</ref>. Interestingly, the method used general language models like GPT-2 <ref type="bibr" coords="9,390.47,297.22,16.25,10.91" target="#b27">[27]</ref>.</p><p>A group from the Australian e-Health Research Centre (AEHRC-CSIRO) reached a BLEUscore of 0.432 <ref type="bibr" coords="9,150.60,324.32,11.27,10.91" target="#b6">[7]</ref>. The group used modern network architectures like Visual Transformers (ViT) <ref type="bibr" coords="9,89.29,337.87,17.91,10.91" target="#b24">[24]</ref> and tested different pre-trainings on medical datasets like ROCO <ref type="bibr" coords="9,400.25,337.87,17.91,10.91" target="#b28">[28]</ref> and CheXpert. The best model had the simplest configuration and no pre-training on medical datasets.</p><p>The kdelab group from Toyohashi University of Technology (Japan) reached a BLEU-score of 0.362 <ref type="bibr" coords="9,114.88,378.52,16.30,10.91" target="#b11">[12]</ref>. They used a standard Show Attend and Tell <ref type="bibr" coords="9,336.84,378.52,17.96,10.91" target="#b26">[26]</ref> model and focused their work on image pre-processing like Histogram Normalizations. This improved the result to 0.362 from a baseline model reaching a BLEU-score of 0.339.</p><p>The ImageSem Group from Chinese Academy of Medical Sciences and Peking Union Medical College (China) reached a BLEU-score of 0.257 and details are provided in <ref type="bibr" coords="9,422.32,432.72,16.31,10.91" target="#b9">[10]</ref>. They used an approach based on sentence patterns for the caption prediction. For this, they used the results from the first sub task on concept detection and inserted the found concepts in caption patterns like: &lt;image&gt; of &lt;body&gt; demonstrate &lt;findings&gt;.</p><p>The RomiBed group from University of Oulu (Finland) reached a BLEU-score of 0.243 and described their approach in <ref type="bibr" coords="9,213.28,500.46,16.21,10.91" target="#b13">[14]</ref>. They used an attention based encoder-decoder model for the caption prediction.</p><p>To summarize, in the caption prediction task, several teams used variations of the Show, Attend and Tell model <ref type="bibr" coords="9,187.16,541.11,16.08,10.91" target="#b26">[26]</ref>. New approaches were used such as Transformer-based architectures and general language models like GPT-2 <ref type="bibr" coords="9,278.80,554.66,16.41,10.91" target="#b27">[27]</ref>. Transfer Learning has frequently been used and some teams in both sub tasks tried to pretrain with more medically oriented datasets like ROCO <ref type="bibr" coords="9,120.73,581.76,17.89,10.91" target="#b28">[28]</ref> or CheXpert. Interestingly, pre-training with medical oriented datasets seem to be not helpful in this task and many groups found that the most simple architectures provided the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This year's caption task of ImageCLEF included several changes in comparison to earlier years. The task was divided into two sub tasks, the concept detection sub task which is comparable to the years before and the re-introduced caption prediction sub task. Another difference was the choice of images, which no longer come from publications but from original radiology images and the captions were produced by clinicians. This change was appreciated by the participants to be more realistic. As a result more teams took part in one or both tasks. A few teams saw the concept detection as a prerequisite to the caption prediction task and provided interesting caption template-based solutions for the caption prediction from detected concepts. Others use variations of Show, Attend and Tell for the caption prediction and participated only in the caption prediction. For the concept detection, mainly multi-label classification or more information retrieval oriented solutions based on image embeddings were proposed. In this year more modern neural network architectures like EfficientNets and ViT were used for the images, and Transformers and General Language models used for the texts. Several participants found that the variation of caption texts was lower compared to earlier years. As a result, more simple solutions produced the best results. In consequence, we seek to increase the number of images and concepts for later competitions and try to increase the variation of the caption texts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,213.34,294.62,10.91"><head>Figure 1</head><label>1</label><figDesc>Figure 1 shows an example from the data set provided by the task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,89.29,487.64,416.69,9.96;3,89.29,499.59,158.90,9.96;3,89.29,236.05,416.71,233.93"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of a radiology image with the corresponding UMLS®CUIs and captions extracted from the ImageCLEFcaption 2021 task.</figDesc><graphic coords="3,89.29,236.05,416.71,233.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,88.99,90.49,417.00,470.93"><head>Table 2</head><label>2</label><figDesc>Performance of the participating teams in the ImageCLEF 2021 Concept Detection Task The multi-label classifier was based on DenseNet-121. The best submission came from the retrieval technique based on DenseNet-121 with cosine similarity. The ImageSem Group from Chinese Academy of Medical Sciences and Peking Union Medical College (China) reached an F1-score of 0.419 and details are provided in</figDesc><table coords="6,89.29,121.17,312.04,399.60"><row><cell>Group Name</cell><cell cols="2">Submission Run F1-Score</cell></row><row><cell>AUEBs_NLP_Group</cell><cell>136458</cell><cell>0.505</cell></row><row><cell>AUEBs_NLP_Group</cell><cell>136455</cell><cell>0.495</cell></row><row><cell>AUEBs_NLP_Group</cell><cell>135963</cell><cell>0.493</cell></row><row><cell>AUEBs_NLP_Group</cell><cell>136052</cell><cell>0.493</cell></row><row><cell>AUEBs_NLP_Group</cell><cell>135847</cell><cell>0.490</cell></row><row><cell>NLIP-Essex-ITESM</cell><cell>132945</cell><cell>0.469</cell></row><row><cell>AUEBs_NLP_Group</cell><cell>135870</cell><cell>0.466</cell></row><row><cell>AUEBs_NLP_Group</cell><cell>135862</cell><cell>0.459</cell></row><row><cell>AUEBs_NLP_Group</cell><cell>136307</cell><cell>0.456</cell></row><row><cell>NLIP-Essex-ITESM</cell><cell>136429</cell><cell>0.451</cell></row><row><cell>AUEBs_NLP_Group</cell><cell>135989</cell><cell>0.451</cell></row><row><cell>NLIP-Essex-ITESM</cell><cell>136404</cell><cell>0.440</cell></row><row><cell>NLIP-Essex-ITESM</cell><cell>136400</cell><cell>0.423</cell></row><row><cell>ImageSem</cell><cell>135873</cell><cell>0.419</cell></row><row><cell>NLIP-Essex-ITESM</cell><cell>133912</cell><cell>0.412</cell></row><row><cell>ImageSem</cell><cell>135871</cell><cell>0.400</cell></row><row><cell>ImageSem</cell><cell>136142</cell><cell>0.396</cell></row><row><cell>ImageSem</cell><cell>135858</cell><cell>0.380</cell></row><row><cell>ImageSem</cell><cell>136129</cell><cell>0.370</cell></row><row><cell>IALab_PUC</cell><cell>135810</cell><cell>0.360</cell></row><row><cell>NLIP-Essex-ITESM</cell><cell>136379</cell><cell>0.355</cell></row><row><cell>ImageSem</cell><cell>136140</cell><cell>0.355</cell></row><row><cell>AUEBs_NLP_Group</cell><cell>136371</cell><cell>0.348</cell></row><row><cell>ImageSem</cell><cell>136141</cell><cell>0.327</cell></row><row><cell>RomiBed</cell><cell>136011</cell><cell>0.143</cell></row><row><cell>IALab_PUC</cell><cell>135197</cell><cell>0.141</cell></row><row><cell>RomiBed</cell><cell>136025</cell><cell>0.137</cell></row><row><cell>ImageSem</cell><cell>136143</cell><cell>0.037</cell></row><row><cell>ImageSem</cell><cell>136144</cell><cell>0.019</cell></row><row><cell>tested.</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.93,671.03,122.78,8.97"><p>https://medpix.nlm.nih.gov/home</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,108.93,660.05,284.39,8.97"><p>https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/[last accessed: 27.06.2021]</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,108.93,671.01,122.78,8.97"><p>https://medpix.nlm.nih.gov/home</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was partially supported by the <rs type="funder">University of Essex GCRF QR Engagement Fund</rs> provided by <rs type="funder">Research England</rs> (grant number <rs type="grantNumber">G026</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_eQ5PHRN">
					<idno type="grant-number">G026</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,112.66,445.32,395.01,10.91;10,112.66,458.87,394.53,10.91;10,112.48,472.42,395.18,10.91;10,112.66,485.97,394.53,10.91;10,112.28,499.52,393.70,10.91;10,112.66,513.06,393.33,10.91;10,112.66,526.61,393.33,10.91;10,112.66,540.16,393.53,10.91;10,112.66,553.71,197.61,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,264.32,499.52,241.66,10.91;10,112.66,513.06,261.75,10.91">Overview of the ImageCLEF 2021: Multimedia retrieval in medical, nature, internet and social media applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Peteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tauteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Moustahfid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,401.27,513.06,104.72,10.91;10,112.66,526.61,393.33,10.91;10,112.66,540.16,201.15,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 12th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="10,348.61,540.16,157.57,10.91;10,112.66,553.71,31.10,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,567.26,393.59,10.91;10,112.66,580.81,394.53,10.91;10,112.66,594.36,22.69,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,380.21,567.26,126.04,10.91;10,112.66,580.81,77.03,10.91">Overview of the ImageCLEF 2016 medical task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schaer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,212.47,580.81,289.72,10.91">Working Notes of CLEF 2016 (Cross Language Evaluation Forum)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,607.91,395.16,10.91;10,112.66,621.46,393.33,10.91;10,112.14,635.01,130.59,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,381.92,607.91,125.91,10.91;10,112.66,621.46,357.76,10.91">Overview of ImageCLEFcaption 2017-image caption prediction and concept detection for biomedical images</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia Seco De</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,479.17,621.46,26.82,10.91;10,112.14,635.01,100.46,10.91">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,648.56,393.33,10.91;10,112.66,662.11,350.14,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,430.40,648.56,75.59,10.91;10,112.66,662.11,182.00,10.91">Overview of the ImageCLEF 2018 caption prediction tasks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia Seco De</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Eickhof</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,302.74,662.11,129.93,10.91">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,86.97,393.33,10.91;11,112.39,100.52,393.60,10.91;11,112.66,114.06,394.53,10.91;11,112.66,127.61,116.58,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,435.39,86.97,70.60,10.91;11,112.39,100.52,393.60,10.91;11,112.66,114.06,31.38,10.91">Overview of the vqa-med task at imageclef 2021: Visual question answering and generation in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,171.48,114.06,118.94,10.91">CLEF 2021 Working Notes</title>
		<title level="s" coord="11,298.64,114.06,180.76,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,141.16,393.32,10.91;11,112.66,154.71,377.26,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,186.64,141.16,319.34,10.91;11,112.66,154.71,52.45,10.91">The Unified Medical Language System (UMLS): integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
		<idno type="DOI">10.1093/nar/gkh061</idno>
	</analytic>
	<monogr>
		<title level="j" coord="11,173.80,154.71,103.68,10.91">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="267" to="270" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,168.26,394.53,10.91;11,112.66,181.81,394.53,10.91;11,112.66,195.36,67.18,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,286.80,168.26,194.42,10.91">AEHRC CSIRO in ImageCLEFmed Caption</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nicolson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dowling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Koopman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,127.66,181.81,114.61,10.91">CLEF2021 Working Notes</title>
		<title level="s" coord="11,249.86,181.81,179.10,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,208.91,393.33,10.91;11,112.66,222.46,393.33,10.91;11,112.14,236.01,292.58,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,478.16,208.91,27.83,10.91;11,112.66,222.46,201.88,10.91">AUEB NLP Group at ImageCLEFmed Caption Tasks</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Charalampakos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,358.70,222.46,112.95,10.91">CLEF2021 Working Notes</title>
		<title level="s" coord="11,479.09,222.46,26.90,10.91;11,112.14,236.01,146.03,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,249.56,393.33,10.91;11,112.66,263.11,394.53,10.91;11,112.66,276.66,322.05,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,275.48,249.56,230.51,10.91;11,112.66,263.11,253.99,10.91">PUC Chile team at Caption Prediction: ResNet visual encoding and caption classification with Parametric ReLU</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,390.52,263.11,112.00,10.91">CLEF2021 Working Notes</title>
		<title level="s" coord="11,112.66,276.66,175.50,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,290.20,393.58,10.91;11,112.33,303.75,393.65,10.91;11,112.66,317.30,394.53,10.91;11,112.66,330.85,116.58,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,280.63,290.20,225.61,10.91;11,112.33,303.75,393.65,10.91;11,112.66,317.30,29.33,10.91">ImageSem Group at ImageCLEFmed Caption 2021 Task: exploring the clinical significance of the textual descriptions derived from medical images</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,171.80,317.30,116.54,10.91">CLEF2021 Working Notes</title>
		<title level="s" coord="11,297.15,317.30,182.25,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,344.40,394.53,10.91;11,112.66,357.95,394.53,10.91;11,112.66,371.50,67.18,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,330.05,344.40,172.46,10.91">PUC Chile Team at Concept Detection</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Schuit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,127.66,357.95,114.61,10.91">CLEF2021 Working Notes</title>
		<title level="s" coord="11,249.86,357.95,179.10,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,385.05,393.33,10.91;11,112.26,398.60,394.93,10.91;11,112.66,412.15,322.05,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,264.77,385.05,241.22,10.91;11,112.26,398.60,247.60,10.91">Kdelab at ImageCLEF 2021: Medical Caption Prediction with Effective Data Pre-processing and Deep Learning</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tsuneda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Asakawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,386.81,398.60,115.60,10.91">CLEF2021 Working Notes</title>
		<title level="s" coord="11,112.66,412.15,175.50,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,425.70,394.53,10.91;11,112.66,439.25,393.33,10.91;11,112.66,452.79,394.52,10.91;11,112.66,466.34,394.53,10.91;11,112.66,479.89,67.18,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,112.66,439.25,393.33,10.91;11,112.66,452.79,389.71,10.91">NLIP-Essex-ITESM at ImageCLEFcaption 2021 task: deep learning-based information retrieval and multi-label classification towards improving medical image understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">P</forename><surname>Andrade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Cuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Compean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Papanastasiou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,127.66,466.34,114.61,10.91">CLEF2021 Working Notes</title>
		<title level="s" coord="11,249.86,466.34,179.10,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,493.44,395.17,10.91;11,112.66,506.99,393.33,10.91;11,112.14,520.54,292.58,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,307.50,493.44,200.34,10.91;11,112.66,506.99,223.86,10.91">Attention-based CNN-GRU model for automatic medical images captioning: ImageCLEF 2021</title>
		<author>
			<persName coords=""><forename type="first">D.-R</forename><surname>Beddiar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Oussalah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Seppänen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,359.64,506.99,112.22,10.91">CLEF2021 Working Notes</title>
		<title level="s" coord="11,479.27,506.99,26.71,10.91;11,112.14,520.54,146.03,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,534.09,393.33,10.91;11,112.66,547.64,395.01,10.91;11,112.66,561.19,143.58,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,176.64,534.09,264.55,10.91">PubMed Central: The GenBank of the published literature</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Roberts</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.98.2.381</idno>
	</analytic>
	<monogr>
		<title level="j" coord="11,450.78,534.09,55.20,10.91;11,112.66,547.64,309.87,10.91">Proceedings of the National Academy of Sciences of the United States of America</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="381" to="382" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,574.74,393.32,10.91;11,112.66,588.29,393.53,10.91;11,112.66,601.84,203.57,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,308.17,574.74,197.81,10.91;11,112.66,588.29,88.86,10.91">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,224.89,588.29,281.30,10.91;11,112.66,601.84,116.04,10.91">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,615.39,393.33,10.91;11,112.66,628.93,393.33,10.91;11,112.66,642.48,394.51,10.91;11,112.66,658.47,79.55,7.90" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,356.36,615.39,149.63,10.91;11,112.66,628.93,41.53,10.91">Densely Connected Convolutional Networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.243</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,183.61,628.93,322.38,10.91;11,112.66,642.48,51.24,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Honolulu, USA</addrLine></address></meeting>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2017">July 22-25, 2017, 2017</date>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,669.58,395.17,10.91;12,112.66,86.97,395.01,10.91;12,112.66,100.52,394.53,10.91;12,112.66,114.06,394.04,10.91;12,112.66,127.61,250.52,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,149.38,86.97,141.09,10.91">Supervised contrastive learning</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kr-Ishnan</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="s" coord="12,211.67,100.52,237.66,10.91">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18661" to="18673" />
			<date type="published" when="2020">2020</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,141.16,394.53,10.91;12,112.66,154.71,393.33,10.91;12,112.66,168.26,219.41,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,176.85,141.16,325.35,10.91">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,293.89,154.71,212.10,10.91;12,112.66,168.26,125.93,10.91">Proceedings of the 36th International Conference on Machine Learning (ICML</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning (ICML</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,334.80,168.26,172.38,10.91;12,112.66,181.81,394.04,10.91;12,112.66,195.36,55.09,10.91" xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Long</forename><surname>Beach</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/tan19a.html" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
			<pubPlace>California, US; Long Beach, California, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,208.91,394.53,10.91;12,112.28,222.46,395.55,10.91;12,112.66,236.01,394.51,10.91;12,112.66,252.00,104.78,7.90" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="12,329.30,222.46,178.53,10.91;12,112.66,236.01,65.47,10.91">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
	</analytic>
	<monogr>
		<title level="j" coord="12,192.14,236.01,193.12,10.91">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,263.11,393.33,10.91;12,112.66,276.66,393.33,10.91;12,112.33,290.20,275.01,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="12,344.36,263.11,161.63,10.91;12,112.66,276.66,82.87,10.91">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.308</idno>
	</analytic>
	<monogr>
		<title level="m" coord="12,216.12,276.66,289.87,10.91;12,112.33,290.20,30.22,10.91">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,303.75,393.33,10.91;12,112.66,317.30,393.32,10.91;12,112.66,330.85,352.03,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="12,357.13,303.75,148.86,10.91;12,112.66,317.30,157.10,10.91">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00068</idno>
	</analytic>
	<monogr>
		<title level="m" coord="12,315.58,317.30,190.40,10.91;12,112.66,330.85,105.89,10.91">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,344.40,393.33,10.91;12,112.66,357.95,162.24,10.91" xml:id="b23">
	<monogr>
		<title level="m" type="main" coord="12,242.31,344.40,263.68,10.91;12,112.66,357.95,51.39,10.91">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>arXiv 1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,371.50,395.16,10.91;12,112.66,385.05,393.33,10.91;12,112.41,398.60,393.57,10.91;12,112.66,412.15,393.33,10.91;12,112.66,425.70,290.61,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="12,419.65,385.05,86.34,10.91;12,112.41,398.60,266.70,10.91">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=YicbFdNTTy" />
	</analytic>
	<monogr>
		<title level="m" coord="12,403.64,398.60,102.34,10.91;12,112.66,412.15,268.01,10.91">Proceedings of the 9th International Conference on Learning Representations (ICLR</title>
		<meeting>the 9th International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021. 2021</date>
			<biblScope unit="page" from="3" to="07" />
		</imprint>
	</monogr>
	<note>Online Event</note>
</biblStruct>

<biblStruct coords="12,112.66,439.25,394.62,10.91;12,112.66,452.79,394.53,10.91;12,112.66,466.34,393.08,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="12,267.98,439.25,214.89,10.91">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m" coord="12,112.66,452.79,358.65,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Las Vegas, USA</addrLine></address></meeting>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2016-07-01">June 26 -July 1, 2016, 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,479.89,394.53,10.91;12,112.66,493.44,395.01,10.91;12,112.66,506.99,393.60,10.91;12,112.66,520.54,394.52,10.91;12,112.48,534.09,364.33,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="12,481.24,479.89,25.95,10.91;12,112.66,493.44,302.25,10.91">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v37/xuc15.html" />
	</analytic>
	<monogr>
		<title level="m" coord="12,161.68,506.99,344.58,10.91;12,112.66,520.54,17.79,10.91;12,314.90,521.55,187.89,9.72">Proceedings of the 32nd International Conference on Machine Learning, ICML 2015</title>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</editor>
		<meeting>the 32nd International Conference on Machine Learning, ICML 2015<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-11">6-11 July 2015. 2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct coords="12,112.66,547.64,395.17,10.91;12,112.66,561.19,273.10,10.91" xml:id="b27">
	<monogr>
		<title level="m" type="main" coord="12,387.26,547.64,120.56,10.91;12,112.66,561.19,119.41,10.91">Languagemodels are unsupervised multitask learners</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>Open-AI</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="12,112.66,574.74,393.33,10.91;12,112.33,588.29,393.65,10.91;12,112.66,601.84,395.17,10.91;12,112.41,615.39,394.78,10.91;12,112.66,628.93,394.53,10.91;12,112.66,642.48,395.01,10.91;12,112.66,656.03,193.04,10.91" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="12,370.24,574.74,135.74,10.91;12,112.33,588.29,393.65,10.91;12,112.66,601.84,395.17,10.91">Radiology Objects in COntext (ROCO): A Multimodal Image Dataset, in: Intravascular Imaging and Computer Assisted Stenting -and -Large-Scale Annotation of Biomedical Data and Expert Label Synthesis</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01364-6_20</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01364-6_20.doi:10.1007/978-3-030-01364-6\_20" />
	</analytic>
	<monogr>
		<title level="m" coord="12,112.41,615.39,389.37,10.91;12,179.72,628.93,180.34,10.91">7th Joint International Workshop, CVII-STENT 2018 and Third International Workshop</title>
		<meeting><address><addrLine>LABELS; Granada, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09-16">2018. September 16, 2018. 2018</date>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
	<note>Held in Conjunction with MICCAI 2018</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
