<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.03,75.53,451.44,17.00;1,72.03,96.28,383.96,17.00">SSN MLRG at VQA-MED 2021: An Approach for VQA to Solve Abnormality Related Queries using Improved Datasets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.03,129.35,76.21,10.80"><forename type="first">Noor</forename><surname>Mohamed</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of CSE</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<postCode>-603110</postCode>
									<settlement>Kalavakkam</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,151.33,129.35,67.54,10.80"><forename type="first">Sheerin</forename><surname>Sitara</surname></persName>
							<email>sheerinsitaran@ssn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of CSE</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<postCode>-603110</postCode>
									<settlement>Kalavakkam</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,248.10,129.35,91.83,10.80"><forename type="first">Srinivasan</forename><surname>Kavitha</surname></persName>
							<email>kavithas@ssn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of CSE</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<postCode>-603110</postCode>
									<settlement>Kalavakkam</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.03,75.53,451.44,17.00;1,72.03,96.28,383.96,17.00">SSN MLRG at VQA-MED 2021: An Approach for VQA to Solve Abnormality Related Queries using Improved Datasets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">59CCFB03E7315A88C6D597F56837F16D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visual Question Answering</term>
					<term>VGGNet</term>
					<term>Long Short Term Memory</term>
					<term>medical domain</term>
					<term>VQA dataset</term>
					<term>augmented dataset</term>
					<term>reduced dataset</term>
					<term>ImageCLEF</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Visual Question Answering (VQA) in the medical domain attains tremendous advancement in last few years. To improvise the VQA research, ImageCLEF forum is organizing the fourth edition of VQA task in medical domain. This year, the abnormality related VQA queries are to be answered for the given set of radiology images. In the proposed system, VGGNet based on transfer learning approach and LSTM is used to extract image and text features respectively. The extracted three dimensional (embedding, image, text) feature vectors are concatenated into sequence of vectors by LSTM for predicting the answer. The purpose of selecting VGGNet and LSTM are: VGGNet, outperforms complex recognition tasks and also addresses vanishing gradient and exploding gradient problem and LSTM, solves complex sequence learning problems and overcomes long term dependency problems. In addition, the hyper parameters are chosen appropriately and four improved datasets are used to analyze the performance of the proposed model. These four datasets are build by collecting the samples from previous ImageCLEF VQA -MED tasks. The proposed model resulted in an accuracy of 0.196 and a BLEU score of 0.227 for one of the dataset, which is ranked tenth among all participating groups in ImageCLEF 2021 VQA-MED task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The recent studies of 2020 reveals that the 90% of data are unlabelled and 40 -50 % of data is in the form of images <ref type="bibr" coords="1,157.23,492.89,16.68,9.90">[12]</ref>. Hence an Artificial Intelligent (AI) approach is required to analyze both image and text. Now-a-days, the advantage of AI approach is extended to different applications like text summarization, machine translation, sentiment analysis, image captioning, and Visual Question Answering (VQA). Among which, VQA comprises both image and text for real world dataset <ref type="bibr" coords="1,507.48,530.89,11.43,9.90" target="#b0">[1]</ref>, abstract dataset <ref type="bibr" coords="1,141.73,543.64,12.76,9.90" target="#b1">[2]</ref> and medical dataset <ref type="bibr" coords="1,245.35,543.64,12.66,9.90" target="#b2">[3]</ref> are evolved in this decade. For medical dataset, ImageCLEF organizes medical related image captioning and VQA task since 2018 <ref type="bibr" coords="1,393.13,556.16,11.47,9.90" target="#b2">[3]</ref>. From 2020, ImageCLEF concentrates on solving abnormality related VQA questions <ref type="bibr" coords="1,340.90,568.92,11.43,9.90" target="#b3">[4]</ref>.</p><p>The Visual Question Answering system of medical domain takes one or more abnormality related natural language questions with respective radiology images as input and predicts the appropriate answer as output. Some of the applications of medical VQA are: (i). Helps partially visually sighted people (ii). Helps in clinical support and decision. To answer the medical VQA queries, the visual information of the radiology image is extracted based on the significant textual content of the question. In other words, image features are extracted based on the text features and finally both feature vectors are concatenated to answer the respective questions. The different image processing techniques are, Convolutional Neural Network, pre-trained models like VGGNet, ResNet and DenseNet and, text processing techniques are Long Short Term Memory (LSTM), Gated Recurrent Unit (GRU), Bidirectional Encoder Representation (BERT).</p><p>The overview of ImageCLEF VQA -MED tasks (2018, 2019 and 2020) are summarized and given in Table <ref type="table" coords="2,111.75,115.27,4.15,9.90" target="#tab_0">1</ref>. From the results, the observations are: (i). ImageCLEF VQA -MED 2019 achieved better performance than ImageCLEF VQA -MED 2018, because of the increased number of samples for each class (ii). In ImageCLEF VQA -MED 2019 task, abnormality type VQA questions achieved less performance as compared with organ, plane and modality type questions (iii). Based on the ImageCLEF VQA -MED 2019 task outcome, ImageCLEF begin to concentrates on abnormality type questions since 2020 but the performance is reduced.</p><p>From the inference of previous tasks (especially ImageCLEF VQA -MED 2020 task) as tabulated in Table <ref type="table" coords="2,110.27,209.82,4.14,9.90" target="#tab_0">1</ref>, VGGNet and LSTM (modification of RNN) are used in the proposed model for VQA system development. In addition, VGGNet and LSTM have some advantages, such as (i). VGGNet -Outperforms complex recognition tasks, addresses vanishing gradient and exploding gradient problem <ref type="bibr" coords="2,113.25,250.32,12.71,9.90" target="#b4">[5]</ref> (ii). LSTM -Solves complex sequence learning problems and overcomes long term dependency problems <ref type="bibr" coords="2,171.27,263.82,11.48,9.90" target="#b5">[6]</ref>. The research contributions of ImageCLEF VQA -MED 2021 task using the proposed model are: (i). For training the model, the dataset is augmented from ImageCLEF VQA-MED 2018, 2019 and 2020 (test set) datasets. From 2018 and 2019 datasets, 126 samples associated with abnormality related queries are collected and augmented. The ImageCLEF VQA-MED 2020 test set consists of 500 radiology images with respective 500 question-answer pairs are also used for augmenting the dataset. (ii). In terms of implementation, VGGNet followed by LSTM are used for answering the medical questions related to radiology images. (iii). For building the model, the hyper parameters like learning rate, number of epochs, batch size, momentum, dropout, etc., are selected and the values are fixed based on the performance measures.</p><p>The remaining part of the paper spans across following subsections. In Sect. 2, ImageCLEF VQA-MED 2021 task and its dataset are discussed and, compared with 2020 task. In Sect. 3, the design of the proposed VQA model and its implementation are explained. A brief summary about the results obtained and the performance evaluation are given in Sect. 4 with a conclusion at the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task and Dataset Description</head><p>In this section, ImageCLEF VQA -MED 2021 task and given dataset are discussed with three types of improved datasets, which are build from the previous VQA datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">ImageCLEF VQA -MED 2021 task</head><p>ImageCLEF, a part of Conference and Labs of the Evaluation Forum is conducting tasks related to the medical domain since 2018. ImageCLEF VQA -MED 2021 task concentrates on abnormality type questions for different organs, planes and modalities. In this task, 33 participants were registered and 13 teams were participated with 75 successful runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">ImageCLEF VQA -MED 2021 dataset</head><p>The ImageCLEF VQA-MED 2021 dataset <ref type="bibr" coords="3,288.26,366.09,18.28,9.90" target="#b9">[10]</ref> is given as four subsets namely, training set, validation set, new validation set and test set. The first two subsets are equivalent to ImageCLEF VQA-MED 2020 dataset and it is used for training. This set consists of 4500 radiology images and 4500 question-answer pairs, among which the validation set consists of 500 radiology images with respective 500 question-answer pairs. The new validation set consists of 500 question-answer pairs associated with 500 radiology images. Finally, the test set includes 500 radiology images and 500 questions about abnormality.</p><p>The datasets used for training the proposed model is given in Table <ref type="table" coords="3,380.37,454.61,4.14,9.90" target="#tab_1">2</ref>. The acronyms, GD, GTD, AD and ARD represents Given Dataset, Given dataset along with Test dataset from ImageCLEF VQA-MED 2020, Augmented Dataset and Augmented Reduced Dataset. The Augmented Dataset consists of GTD along with the augmented samples from ImageCLEF VQA-MED 2018 and 2019. The Augmented Reduced Dataset is a modification of AD dataset, in which some of the samples are removed by two ways, (i). Least contributing samples, (ii). Identify and reduce the number of samples of similar cases where the count value deviates from the remaining classes. The advancement in ImageCLEF VQA-MED 2021 task when compared to 2020 task are: (i). The number of VQA samples are increased (ii). Number of classes of abnormality type questions are increased</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Methodology</head><p>The proposed VQA model comprises of VGGNet (used as Transfer Learning approach) and LSTM to answer the VQA queries related to radiology images. This VQA model is further tuned by hyperparameter selection as tabulated in Table <ref type="table" coords="4,277.32,131.04,5.50,9.90" target="#tab_2">3</ref> and supported by three improved VQA -MED dataset (as discussed in Section 2). VGGNet and LSTM are used to obtain the image features and text features respectively. These features are then combined using elementwise multiplication and used for model creation. The output of the model is the sequence of words for all possible answer classes. VGGNet, a pre-trained model, is used as a transfer learning approach. The transfer learning approach is adapted because of three factors namely, (i). Higher start -Model with transfer learning approach outperforms the model without transfer learning approach (ii). Higher slope -Performance rate gradually increases in the training phase (iii). Higher asymptote -Training rate converges smoothly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>The proposed model is executed on four datasets (as discussed in Section 2) and the performance is analyzed using five different runs as given in Table <ref type="table" coords="4,308.28,689.44,4.17,9.90" target="#tab_3">4</ref>, are: (i). VGG16 concatenated with LSTM by excluding the last layer with less number of epochs for the given dataset (ii). Same as (i) but number of epochs is increased (iii). Similar to (ii) for Given dataset along with Test dataset from ImageCLEF VQA-MED 2020 (GTD). (iv). Same as (ii) for Augmented Dataset (v). Same as (ii) for Augmented Reduced Dataset. The performance of the model depends on suitable hyper parameters and the appropriate values as given in Table <ref type="table" coords="5,138.54,337.34,4.13,9.90" target="#tab_2">3</ref>. The result of the proposed model are analysed using suitable quantitative metrics for different runs. The quantitative metrics includes, mean square error for training and validation set and, accuracy and BLEU score for test set as given in Table <ref type="table" coords="5,314.60,362.59,4.13,9.90" target="#tab_3">4</ref>. The overall inferences are: (i). Training error is lesser than validation error because most of the samples are learned in the training phase and followed early stopping (ii). For the third run, both training and validation error is minimum than other runs which leads to better prediction rate (iii). Among the five runs, the third run achieved a better accuracy of 0.196 and the BLEU score of 0.227 for GTD dataset. The final result of the leaderboard is given in Table <ref type="table" coords="5,324.72,645.19,5.50,9.90" target="#tab_4">5</ref> where our team achieved 10 th place in the listed ranks. Our proposed model achieved improved accuracy of 0.196 and BLEU score of 0.227 due to the usage of timestamps during the training phase. The timestamps play a major role in relearning the appropriate answers of the sample based on the previously predicted answer. It also helps the proposed model to learn temporal patterns from a sequence of question-answer pairs based on radiology images.</p><p>The overall experience from the VQA-MED 2021 task is based on the dataset only. It concentrates on abnormality type questions which can be answered more easily than the questions related to organ, plane and modality type. However, the accuracy is reduced by 11.4% as compared with previous year because of two reasons such as: large number of samples and the number of classes are also increased in VQA-MED 2021 task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper describes an approach to solve Visual Question Answering on medical domain for ImageCLEF VQA -MED 2021 dataset. The ImageCLEF concentrates on abnormality related VQA dataset from previous year onwards. When compared with previous year, the number of samples, abnormality type and difficulty level are increased. For the VQA dataset, image features and text features are extracted using VGGNet and LSTM, finally both features are concatenated using LSTM to predict the answer. In this VQA model, word embedding is used which allows the model to focus on the part of the image which is relevant to both the image and the keyword in the question. As irrelevant parts of the radiology image are not taken into consideration and thus the classification accuracy is improved by reducing the chances of predicting wrong answers. To validate the model, four datasets namely, Given Dataset (GD), Given dataset along with Test dataset from ImageCLEF VQA-MED 2020 (GTD), Augmented Dataset (AD) and Augmented Reduced Dataset (ARD) are used in five different runs. Among the five runs of the proposed model the better result is achieved for Given dataset along with Test dataset from ImageCLEF VQA-MED 2020 (GTD) with an accuracy score of 0.196 and BLEU score of 0.227. Even though the 2021 dataset is complex, the appropriate parameter selection and improved datasets helps to maintain the performance of the proposed VQA system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" coords="4,86.20,394.38,434.40,234.60"><head></head><label></label><figDesc></figDesc><graphic coords="4,86.20,394.38,434.40,234.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,72.03,290.10,445.66,337.07"><head>Table 1 ImageCLEF VQA -MED task Overview Task Widespread Techniques Remarkable Techniques Category Remarka ble Accuracy Remark able BLEU score Images Texts Images Texts ImageCLEF VQA - MED 2018 [7]</head><label>1</label><figDesc></figDesc><table coords="2,79.28,387.87,427.25,239.30"><row><cell></cell><cell>Convoluti</cell><cell cols="2">Recurrent</cell><cell>ResNet</cell><cell>LSTM</cell><cell>Organ,</cell><cell>-</cell><cell>0.162</cell></row><row><cell></cell><cell>onal</cell><cell>Neural</cell><cell></cell><cell></cell><cell></cell><cell>plane,</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Neural</cell><cell>Network</cell><cell></cell><cell></cell><cell></cell><cell>modality</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Network</cell><cell>(RNN)</cell><cell></cell><cell></cell><cell></cell><cell>and</cell><cell></cell><cell></cell></row><row><cell></cell><cell>(CNN)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>abnormali</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ty</cell><cell></cell><cell></cell></row><row><cell>ImageCLEF</cell><cell>VGGNet</cell><cell cols="2">Bidirection</cell><cell>CNN</cell><cell>BERT</cell><cell>Organ,</cell><cell>0.624</cell><cell>0.644</cell></row><row><cell>VQA -</cell><cell>or ResNet</cell><cell cols="2">al Encoder</cell><cell></cell><cell></cell><cell>plane,</cell><cell></cell><cell></cell></row><row><cell>MED 2019</cell><cell></cell><cell cols="2">Representa</cell><cell></cell><cell></cell><cell>modality</cell><cell></cell><cell></cell></row><row><cell>[8]</cell><cell></cell><cell cols="2">tion (BERT)</cell><cell></cell><cell></cell><cell>and</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>or RNN</cell><cell></cell><cell></cell><cell></cell><cell>abnormali</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ty</cell><cell></cell><cell></cell></row><row><cell>ImageCLEF</cell><cell>CNN,</cell><cell>BERT</cell><cell>or</cell><cell>DenseN</cell><cell>Skeleton</cell><cell>Abnormal</cell><cell>0.496</cell><cell>0.542</cell></row><row><cell>VQA -</cell><cell>VGGNet</cell><cell cols="2">modificatio</cell><cell>et and</cell><cell>based</cell><cell>ity</cell><cell></cell><cell></cell></row><row><cell>MED 2020</cell><cell>or ResNet</cell><cell>n of RNN</cell><cell></cell><cell>ResNet</cell><cell>Sentence</cell><cell></cell><cell></cell><cell></cell></row><row><cell>[9]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Mapping</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,72.03,555.42,450.50,139.28"><head>Table 2</head><label>2</label><figDesc>Dataset Description</figDesc><table coords="3,99.30,597.92,423.22,96.78"><row><cell>Datasets</cell><cell>Training Set</cell><cell></cell><cell>Classes</cell><cell>Description</cell></row><row><cell></cell><cell>Images</cell><cell>QA</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">pairs</cell><cell></cell><cell></cell></row><row><cell>GD</cell><cell>4500</cell><cell>4500</cell><cell>330</cell><cell>Different abnormality</cell></row><row><cell>GTD</cell><cell>5000</cell><cell>5000</cell><cell>366</cell><cell>related medical images</cell></row><row><cell>AD</cell><cell>5126</cell><cell>5126</cell><cell>366</cell><cell>along with associated</cell></row><row><cell>ARD</cell><cell>4848</cell><cell>4848</cell><cell>352</cell><cell>question answer pairs</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,72.03,193.80,371.40,122.30"><head>Table 3</head><label>3</label><figDesc>Hyper parameter selection and its respective values</figDesc><table coords="4,154.83,236.33,288.60,79.77"><row><cell>Hyper Parameters</cell><cell>Value</cell></row><row><cell>Number of epochs</cell><cell>800</cell></row><row><cell>Batch Size</cell><cell>256</cell></row><row><cell>Momentum</cell><cell>0.9</cell></row><row><cell>Dropout</cell><cell>0.3</cell></row><row><cell>Learning rate</cell><cell>0.001</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,72.03,74.28,447.50,231.82"><head>Table 4</head><label>4</label><figDesc>Brief Description about each run with performance score</figDesc><table coords="5,78.78,116.78,440.75,189.32"><row><cell>Run</cell><cell>Dataset</cell><cell>Number</cell><cell>Training</cell><cell>Validation</cell><cell cols="2">Test Set</cell></row><row><cell>number</cell><cell></cell><cell>of</cell><cell>Error</cell><cell>Error</cell><cell cols="2">(Performance</cell></row><row><cell></cell><cell></cell><cell>Epochs</cell><cell></cell><cell></cell><cell cols="2">metrics)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell><cell>BLEU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Score</cell></row><row><cell>1</cell><cell>Given Dataset (GD)</cell><cell>31</cell><cell>0.158</cell><cell>0.231</cell><cell>0.020</cell><cell>0.049</cell></row><row><cell>2</cell><cell>Given Dataset (GD)</cell><cell>401</cell><cell>0.130</cell><cell>0.219</cell><cell>0.172</cell><cell>0.213</cell></row><row><cell>3</cell><cell>Given dataset along with</cell><cell>800</cell><cell>0.114</cell><cell>0.183</cell><cell>0.196</cell><cell>0.227</cell></row><row><cell></cell><cell>Test dataset from</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>ImageCLEF VQA-MED</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2020 (GTD)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4</cell><cell>Augmented Dataset (AD)</cell><cell>800</cell><cell>0.134</cell><cell>0.225</cell><cell>0.172</cell><cell>0.211</cell></row><row><cell>5</cell><cell>Augmented Reduced</cell><cell>800</cell><cell>0.132</cell><cell>0.221</cell><cell>0.170</cell><cell>0.208</cell></row><row><cell></cell><cell>Dataset (ARD)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,72.03,438.12,445.70,188.55"><head>Table 5</head><label>5</label><figDesc>Top 10 ranking of ImageCLEF 2021 VQA-MED</figDesc><table coords="5,96.30,479.90,421.42,146.77"><row><cell>Rank</cell><cell>Team name</cell><cell>Accuracy</cell><cell>BLEU score</cell><cell>No. of runs submitted</cell></row><row><cell>1</cell><cell>duadua</cell><cell>0.382</cell><cell>0.416</cell><cell>10</cell></row><row><cell>2</cell><cell>Zhao_Ling_Ling_</cell><cell>0.362</cell><cell>0.402</cell><cell>10</cell></row><row><cell>3</cell><cell>TeamS</cell><cell>0.348</cell><cell>0.391</cell><cell>11</cell></row><row><cell>4</cell><cell>Jeanbenoit_delbrouck</cell><cell>0.348</cell><cell>0.384</cell><cell>13</cell></row><row><cell>5</cell><cell>riven</cell><cell>0.332</cell><cell>0.361</cell><cell>1</cell></row><row><cell>6</cell><cell>Zhao_Shi_</cell><cell>0.316</cell><cell>0.352</cell><cell>4</cell></row><row><cell>7</cell><cell>IALab_PUC</cell><cell>0.236</cell><cell>0.276</cell><cell>7</cell></row><row><cell>8</cell><cell>Li_Yong_</cell><cell>0.222</cell><cell>0.255</cell><cell>10</cell></row><row><cell>9</cell><cell>silencec</cell><cell>0.220</cell><cell>0.235</cell><cell>2</cell></row><row><cell>10</cell><cell>sheerin</cell><cell>0.196</cell><cell>0.227</cell><cell>5</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6.">Acknowledgements</head><p>Our profound gratitude to <rs type="institution">Sri Sivasubramaniya Nadar College</rs> <rs type="affiliation">of Engineering, Department of CSE</rs>, for allowing us to utilize the <rs type="institution">High Performance Computing Laboratory</rs> and GPU Server for the execution of this challenge successfully.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="6,107.80,463.14,415.26,9.90;6,107.80,475.53,400.65,10.01" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,464.55,463.14,58.51,9.90;6,107.80,475.64,88.58,9.90">VQA: Visual Question Answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,204.33,475.64,179.97,9.90">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="31" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.80,488.39,415.26,9.90;6,107.80,500.89,415.25,9.90;6,107.80,513.64,182.58,9.90" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,107.80,500.89,153.10,9.90">VQA: Visual question answering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,271.44,500.89,251.61,9.90;6,107.80,513.64,74.16,9.90">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.80,526.39,415.56,9.90;6,107.80,538.89,415.18,9.90;6,107.80,551.66,415.23,9.90;6,107.80,564.16,415.23,9.90;6,107.80,576.91,415.54,9.90;6,107.80,589.66,415.12,9.90;6,107.80,602.17,415.21,9.90;6,107.80,614.91,415.58,9.90;6,107.80,627.42,252.10,9.90" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,366.88,576.91,156.46,9.90;6,107.80,589.66,391.82,9.90">Overview of the ImageCLEF 2021: Multimedia Retrieval in Medical, Nature, Internet and Social Media Applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Peteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia Secode Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tauteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Daniel Stefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gabriel Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Moustahfid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,107.80,602.17,415.21,9.90;6,107.80,614.91,299.67,9.90">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 12th International Conference of the CLEF Association (CLEF 2021)</title>
		<title level="s" coord="6,124.30,627.42,155.30,9.90">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Romania</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">September 21-24. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.80,640.19,415.32,9.90;6,107.80,652.94,415.29,9.90;6,107.80,665.44,206.56,9.90" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,244.23,640.19,278.88,9.90;6,107.80,652.94,180.29,9.90">ImageCLEF 2020: An approach for Visual Question Answering using VGG-LSTM for different datasets</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Sheerin Sitara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kavitha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,312.10,652.94,210.99,9.90;6,107.80,665.44,52.02,9.90">CLEF 2020 Working Notes, CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 (2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.80,678.19,415.50,9.90;6,107.80,690.69,415.67,9.90;6,107.80,703.44,32.23,9.90" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,250.21,678.19,273.09,9.90;6,107.80,690.69,51.79,9.90">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,184.77,690.69,242.82,9.90">International Conference on Learning Representations</title>
		<meeting><address><addrLine>Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.80,716.19,415.49,9.90;6,107.80,728.72,415.34,9.90;6,107.80,741.46,88.23,9.90" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,447.09,716.19,76.20,9.90;6,107.80,728.72,64.62,9.90">LSTM: A Search Space Odyssey</title>
		<author>
			<persName coords=""><forename type="middle">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Koutnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">B R</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,195.01,728.72,272.03,9.90">IEEE Transcations on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2222" to="2232" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,107.80,74.77,415.40,9.90;7,107.80,87.27,415.36,9.90;7,107.80,100.02,195.57,9.90" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,386.95,74.77,136.25,9.90;7,107.80,87.27,230.37,9.90">Overview of ImageCLEF 2018 Medical Domain Visual Question Answering Task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lungren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,364.15,87.27,159.01,9.90;7,107.80,100.02,100.54,9.90">CLEF 2018 Working Notes,CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,107.80,112.52,415.33,9.90;7,107.80,125.29,415.11,9.90;7,107.80,138.04,325.12,9.90" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,475.87,112.52,47.26,9.90;7,107.80,125.29,362.84,9.90">VQAMed: Overview of the Medical Visual Question Answering Task at ImageCLEF 2019</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,496.06,125.29,26.85,9.90;7,107.80,138.04,230.26,9.90">CLEF 2019 Working Notes. CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,107.80,150.54,415.64,9.90;7,107.80,163.29,415.49,9.90;7,107.80,175.79,414.82,9.90;7,107.80,188.54,415.10,9.90;7,107.80,201.29,415.44,9.90;7,107.80,213.82,415.13,9.90;7,107.80,226.57,409.61,9.90;7,517.73,225.30,5.50,6.30;7,107.80,239.07,415.68,9.90;7,107.80,251.82,266.08,9.90" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,361.80,201.29,161.45,9.90;7,107.80,213.82,390.82,9.90">Overview of the ImageCLEF 2020: Multimedia Retrieval in Lifelogging, Medical, Nature and Internet Applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Peteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S D</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Stefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,107.80,226.57,409.61,9.90;7,517.73,225.30,5.50,6.30;7,107.80,239.07,289.40,9.90">Experimental IR Meets Multilinguality, Multimodality and Interaction, Proceedings of the 11 th International Conference of the CLEF Association (CLEF 2020)</title>
		<title level="s" coord="7,107.80,251.82,186.02,9.90">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">September 22-25. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,144.05,264.57,379.32,9.90;7,107.80,277.06,415.34,9.90;7,107.80,289.81,415.24,9.90;7,107.80,302.34,215.06,9.90" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,107.80,277.06,415.34,9.90;7,107.80,289.81,163.93,9.90">Overview of the VQA-Med Task at ImageCLEF 2021: Visual Question Answering and Generation in the Medical Domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,301.16,289.81,221.88,9.90;7,107.80,302.34,52.02,9.90">CLEF 2021 Working Notes, CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">September 21-24 (2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,144.05,315.09,379.03,9.90;7,107.80,327.84,415.56,9.90;7,107.80,340.34,377.85,9.90" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,468.36,315.09,54.73,9.90;7,107.80,327.84,258.27,9.90">Overcoming Data Limitation in Medical Visual Question Answering</title>
		<author>
			<persName coords=""><forename type="first">.</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">X</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Tjiputra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,394.43,327.84,128.93,9.90;7,107.80,340.34,279.79,9.90">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="522" to="530" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
