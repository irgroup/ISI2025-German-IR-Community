<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,327.78,15.42;1,88.78,106.66,350.52,15.42">PUC Chile team at TBT Task: Diagnosis of Tuberculosis Type using segmented CT scans</title>
				<funder ref="#_P9Nzpmc">
					<orgName type="full">FONDECYT</orgName>
				</funder>
				<funder ref="#_UvzG7D5">
					<orgName type="full">ANID</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.10,134.97,105.60,11.96"><forename type="first">Jos茅</forename><forename type="middle">Miguel</forename><surname>Quintana</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Engineering</orgName>
								<orgName type="institution">Pontificia Universidad Cat贸lica de Chile</orgName>
								<address>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,207.34,134.97,65.34,11.96"><forename type="first">Daniel</forename><surname>Florea</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Engineering</orgName>
								<orgName type="institution">Pontificia Universidad Cat贸lica de Chile</orgName>
								<address>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,285.32,134.97,49.72,11.96"><forename type="first">Ria</forename><surname>Deane</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Engineering</orgName>
								<orgName type="institution">Pontificia Universidad Cat贸lica de Chile</orgName>
								<address>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,347.69,134.97,57.39,11.96"><forename type="first">Denis</forename><surname>Parra</surname></persName>
							<email>dparra@ing.puc.cl</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Engineering</orgName>
								<orgName type="institution">Pontificia Universidad Cat贸lica de Chile</orgName>
								<address>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,417.72,134.97,52.21,11.96"><forename type="first">Pablo</forename><surname>Pino</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Engineering</orgName>
								<orgName type="institution">Pontificia Universidad Cat贸lica de Chile</orgName>
								<address>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,148.92,69.88,11.96"><forename type="first">Pablo</forename><surname>Messina</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Engineering</orgName>
								<orgName type="institution">Pontificia Universidad Cat贸lica de Chile</orgName>
								<address>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,190.17,148.92,55.17,11.96"><forename type="first">Hans</forename><surname>L枚bel</surname></persName>
							<email>halobel@ing.puc.cl</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Engineering</orgName>
								<orgName type="institution">Pontificia Universidad Cat贸lica de Chile</orgName>
								<address>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,327.78,15.42;1,88.78,106.66,350.52,15.42">PUC Chile team at TBT Task: Diagnosis of Tuberculosis Type using segmented CT scans</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">145F71702DD2351F92CBC7615922AFA2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article describes the participation and results of the PUC Chile team in the Turberculosis task in the context of ImageCLEFmedical challenge 2021. We were ranked 7th based on the kappa metric and 4th in terms of accuracy. We describe three approaches we tried in order to address the task. Our best approach used 2D images visually encoded with a DenseNet neural network, which representations were concatenated to finally output the classification with a softmax layer. We describe in detail this and other two approaches, and we conclude by discussing some ideas for future work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>ImageCLEF <ref type="bibr" coords="1,144.76,347.01,12.99,10.91" target="#b0">[1]</ref> is an initiative with the aim of advancing the field of image retrieval (IR) as well as enhancing the evaluation in various fields of IR. The initiative takes the form of several challenges, and it is specially aware of the changes in the IR field in recent years, which have brought about tasks requiring the use of different types of data such as text, images and other features moving towards multi-modality. ImageCLEF has been running annually since 2003, and since the second version (2004) there are medical images involved in some tasks, such as medical image retrieval. Since then, new tasks involving medical images have been integrated into the ImageCLEFmedical challenge group of tasks <ref type="bibr" coords="1,308.30,441.86,11.52,10.91" target="#b1">[2]</ref>, and that is how the task of Tuberculosis type classification has been taking place since 2017. Although there has been changes in the data used for the newest versions of the challenge, the goal of this task is the same: automatic detection of tuberculosis (TB) types using Computer Tomography (CT) volumes as input data.</p><p>In this document we describe the participation of our team from HAIVis group 1 within the artificial intelligence laboratory 2 at PUC Chile (PUC Chile team) in the TB classification task at MedicalImageCLEF 2021 <ref type="bibr" coords="1,210.19,523.15,11.28,10.91" target="#b1">[2]</ref>. Our team earned the 7th place in terms of kappa metric and the fourth place in terms of accuracy in the challenge. Our best submission was a combination of deep learning techniques for two 2D views of each input CT volume, followed by a traditional multi-class classification via softmax layer.</p><p>The rest of the paper is structured as follows: Section 2 describes our data analysis, and in section 3 we provide details of our proposed approaches, including data augmentation. In Therefore this task corresponds to a multi-class classification problem. To rank submissions, each result is evaluated using the unweighted Cohen's Kappa as a primary metric and accuracy as a secondary metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dataset Analysis</head><p>Before carrying out our different approaches to classify each CT scan into a type of tuberculosis, we studied the training dataset. Figure <ref type="figure" coords="2,263.23,596.30,5.07,10.91" target="#fig_0">1</ref> shows the prevalence of each class.</p><p>Table <ref type="table" coords="2,127.83,609.85,5.17,10.91">1</ref> summarizes this information and presents the relative prevalence of each type of Tuberculosis with respect to the total amount of images in the training dataset. It is important to note the clear class imbalance present, where nearly half of all images have Infiltrative Tuberculosis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Human-Inspired 2D Approach</head><p>After consulting medical advice on how they would personally address the challenge, it was theorized that a model could perform better if it trained and predicted with the same data as reviewed by doctors when analyzing CT scans, this means a top view of the image from top to bottom.</p><p>The main reason for taking this approach was that it has proven to be helpful in different computer vision tasks <ref type="bibr" coords="3,191.55,605.14,12.99,10.91" target="#b2">[3]</ref> and there was no found documentation about the subject. Further investigation about the subject is aimed to be performed in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Preprocessing</head><p>First, segmentation masks provided by ImageCLEF were applied to each 3D CT-scan, in order to separate each lung of the patient into two separate inputs. This practice was validated by medical professional, as diseases such as Tuberculosis tend to manifest on both lungs at a time. Second, each 3D matrix was then split into 2D images viewed from the z-axis, this practice is the one that gives the approach's name, as radiologists only use top-down images to review CT scans, the main reason for not using other views is that noise from other parts of the internal structure of the lungs can mislead the professional on it's diagnosis. This same thought process was used in order to train the model. It was also commented by the medical professionals that tuberculosis often concentrates on the upper part of the lungs, based around this observation 20% of images on the bottom of the scan were discarded.</p><p>After this first pre-treatment, each image was normalized and later concatenated in order to produce RGB images, task performed by the dataset, which also was in charge of performing augmentations on each item when loaded and after which, transforming them into tensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Augmentations</head><p>Each image was cropped and later re-centered and randomly scaled both on the x and y axis. After this, an angle was applied to the image, as well as shear and a random horizontal flip. This last practice was performed in order to avoid biases between left and right lungs.</p><p>It was considered to flip all images only to one side, this idea was later rejected due to the fact that human professionals are skilled enough to detect the presence of tuberculosis in a lung regardless of his orientation or which one of the two is the scan of.</p><p>As for the parameters used, the cropping center for each axis was decided by selecting the original center of the image and displacing it by a random amount of pixels between the values of 0 and 32. As for the angle of the rotation and the shear, a random float between 0 and 6.0 and another one between 0 and 4.0 were used respectively. Finally, for the scaling, each axis was multiplied by 2  , with  being a random float between the values of 0 and 0.15. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Model</head><p>The model used was lightly tweaked DenseNet121 pretrained on ImageNet, using an Adam optimizer in order for training to be faster, this due to the high amount of images used during the training process. As for the loss, a weighted cross entropy was used in order to decrease class imbalance. Finally, learning rate was reduced on plateau of the validation set's loss.</p><p>The model trained on each of the images, receiving both an image and a label for the input and failed to converge on a good solution for the test set even after many epochs of training. It was suspected that this was due to the grand amount of images not containing any information which were given as inputs. This was later proven right when inspecting the output predictions for images belonging to a same CT scan; clusters of correct predictions where found in the middle of many different predictions, later revealed by a medical experts to be the exact layers to have the disease present in them.</p><p>Due to a lack of time, no solution to the issue was available to be implemented and is left to be further developed in future works, due to the uncertainty in the effectiveness of this approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">A Simple 2D Approach</head><p>This approach was a modified version of one proposed in <ref type="bibr" coords="5,337.22,319.85,11.28,10.91" target="#b3">[4]</ref>, which tried to represent 3D images by squeezing the volumetric data to 2D projections. Unfortunately, it did not work as expected due to an exploding gradient problem in its execution which, because of lack of time, was not able to be resolved.</p><p>We believe it is still useful to explain this approach and study the reasons it did not work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Preprocessing</head><p>As mentioned in 3.1, this approach was a modified version of one proposed in <ref type="bibr" coords="5,436.74,423.78,11.41,10.91" target="#b3">[4]</ref>. In this case, instead of calculating the mean, maximum and standard deviation in each dimension, we only calculated the maximum for each axis and created a single 2D, three channel image from the three matrices that appeared, which can be interpreted as an RGB image.</p><p>First of all, we applied the segmentation mask provided by ImageCLEF to each 3D CT-scan, dropped the first 10% and last 20% of the image to eliminate unnecessary information, and then calculated the maximum values across each dimension. This produced three 2D matrices, which were subsequently concatenated to produce one single RGB image. The reason we only calculated the maximum was because we believe that, due to the nature of tuberculosis being various small nodules present across the lungs, calculating the average and the standard deviation across dimensions were not accurate measures to determine if there is tuberculosis in the lungs. This is mainly because most of the lung is empty space, so the average and standard deviations would be close to zero. On the other hand, the maximum would show if there were higher values present in the lung, an indication of tuberculosis, which we hypothesize could also help determine the type of tuberculosis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Augmentations</head><p>The augmentations applied in this approach were resizing, to have three equally sized channels, random horizontal flips, and normalization of the final image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Model</head><p>The model used was one network, composed of a fine-tuned ResNet50 pretrained on ImageNet, available from the Torchvision library in PyTorch<ref type="foot" coords="6,312.86,182.61,3.71,7.97" target="#foot_0">3</ref> , using Cross Entropy as the loss function, due to the nature of the challenge being a multi-class classification problem, and Adam as an optimization algorithm. To add regularization to the model, we implemented a drop rate of 0.3. Additionally, we set class weights in our loss function to resolve the imbalanced dataset problem.</p><p>As mentioned earlier, this approach presented exploding gradient problems. When these started to appear, we implemented gradient clipping and went varying the learning rate. The learning rate that performed best, before presenting exploding gradients, had a value of 0.0001. Unfortunately, this was not enough to resolve the problem. We believe this is due to using only the maximum that, although normalized, still caused all images to present high values and prevented the network to learn correctly.</p><p>In the following subsection, we present a different approach which did not present exploding gradient issues but also implemented a modification of the volumetric squeezing approach presented in <ref type="bibr" coords="6,147.09,360.50,11.43,10.91" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The 2D Approach Scoring the Best</head><p>In this particular approach, the first step was to apply the segmentation masks provided by ImageCLEF, particularly using the method proposed in <ref type="bibr" coords="6,333.06,423.78,11.32,10.91" target="#b4">[5]</ref>, to separate the lungs of each image and delete the non-important parts of the CT scan. After that, we divided the segmented image in half, obtaining two 3D matrices, one for each lung.</p><p>Each 3D CT scan can be reduced to a 2D representation, by computing different statistics across each dimension of the image. We used the procedure suggested in <ref type="bibr" coords="6,424.33,477.97,11.58,10.91" target="#b3">[4]</ref>, that consisted in computing the mean, maximum and standard deviation over the 3D images, but applying it only over the 1st and 2nd axis (lateral and superior). Using this, we obtained 2 images per matrix, which can be interpreted as an RGB image. After that we applied the preprocessing steps implemented in <ref type="bibr" coords="6,163.08,532.17,11.56,10.91" target="#b3">[4]</ref>, which consisted in increasing the voxels intensity in the CT by 1024HU, dividing the maximum values by 1500, and dividing the mean values and standard deviation values by their maximum. Additionally, we resized the images to 256 x 256 pixels. In the end, we end up with 2 RGB images for each lung, which represent the lateral and superior views of them. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Augmentations</head><p>The images pass through multiple augmentations, each one of them with a varying probability of being applied. In summary, the augmentations implemented were image rotation, with a rotation range of 25 degrees, width and height shift, with a shift range of 15% of the image, zoom, with a range of 20% of the image, and horizontal and vertical flipping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Model</head><p>With the 2 RGB images for each lung obtained from 3.2, we further applied the augmentations in 3.2.1 to obtain the images to feed the model. We trained one network for each axis, using a fine-tuned DenseNet121 pre-trained on ImageNet, with average pooling in the output layer, and added some layers at the end to reduce dimensions. Subsequently, we concatenated the output of the two networks and added a softmax layer on top to get the final prediction. Using no regularization, we obtained a Cohen Kappa value on training and validation of approximately 0.11, meanwhile with L2 regularization we got 0.236 with 0.511 accuracy on the training set, and 0.186 with 0.467 accuracy on our validation set. On the test set we got 0.120 Cohen's Kappa with 0.401 of accuracy.</p><p>In an effort to reduce the impact of the most represented classes in the dataset, we tried weighting the loss of each label, according to it麓s representation in the dataset. Regarding this, we tried with multiple configurations of fine tuning of the network, achieving the best results with the last twenty layers unfrozen. Using that configuration, we scored a Cohen's Kappa of 0.206 on training set and 0.105 on validation set.</p><p>We further tried using shared weights along the networks of the axes, in order to reduce the quantity of parameters and over fitting. This didn't improve the results, and got a best Cohen Kappa score of 0.141 on training and 0.098 on the validation set.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,301.30,213.53,8.93;2,155.91,84.19,283.48,204.55"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: CT-Scans of different type of Tuberculosis</figDesc><graphic coords="2,155.91,84.19,283.48,204.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,89.29,323.53,152.48,8.93;3,162.37,84.18,270.54,226.78"><head>Figure 2 : 1</head><label>21</label><figDesc>Figure 2: Number of images by class</figDesc><graphic coords="3,162.37,84.18,270.54,226.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,89.29,632.36,268.56,8.93;4,226.77,478.06,141.73,141.73"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Multiple augmentations on the same crop of a CT Scan</figDesc><graphic coords="4,226.77,478.06,141.73,141.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,89.29,274.18,152.18,8.93;7,212.60,84.19,170.08,177.43"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Images after preprocessing</figDesc><graphic coords="7,212.60,84.19,170.08,177.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="8,113.39,84.19,368.50,189.20"><head></head><label></label><figDesc></figDesc><graphic coords="8,113.39,84.19,368.50,189.20" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="6,108.93,671.03,166.60,8.97"><p>https://pytorch.org/vision/stable/models.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was partially funded by <rs type="programName">ANID -Millennium Science Initiative Program</rs> -Code <rs type="grantNumber">ICN17_002</rs> and by <rs type="funder">ANID</rs>, <rs type="funder">FONDECYT</rs> grant <rs type="grantNumber">1191791</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_UvzG7D5">
					<idno type="grant-number">ICN17_002</idno>
					<orgName type="program" subtype="full">ANID -Millennium Science Initiative Program</orgName>
				</org>
				<org type="funding" xml:id="_P9Nzpmc">
					<idno type="grant-number">1191791</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Evaluation of the Model</head><p>In order to evaluate the model, we preprocessed the data as explained in 3.3, resulting in 2 images per lung. Then, we got the predictions for each side of the lung and sum the softmax results, keeping the highest value as the final prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>In this section we present the results of our approaches at our own developing servers and also at the crowdai.org platform.</p><p>Table <ref type="table" coords="8,126.96,456.95,5.06,10.91">2</ref> shows the results obtained while training, including a fourth approach that was not submitted.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this article we have provided details of the participation of the PUC Chile team, for the Tuberculosis type (TBT) classification task within the ImageCLEFmedical challenge 2021. In the process of building our final submission, we tested several approaches. Our final submission was based on a DenseNet architecture for visually encoding the input medical volumes represented as 2D images, followed by a softmax classification layer. In future work, we plan to address the task based on the process that actual radiologists follow when classifying CT scans for Tuberculosis, which we described in section 3.1. Another idea we plan to further investigate is using perceptual image similarity <ref type="bibr" coords="9,241.62,308.02,12.87,10.91" target="#b5">[6]</ref> to leverage approaches based on K-NN, which have had interesting results in previous challenges. Finally, we plan at using methods which can directly deal with volumes (3D images) rather than 2D images.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,107.59,111.28,400.08,10.91;10,107.59,124.83,399.60,10.91;10,107.41,138.38,400.26,10.91;10,107.59,151.93,399.59,10.91;10,107.20,165.48,398.78,10.91;10,107.59,179.03,400.24,10.91;10,107.59,192.57,398.40,10.91;10,107.59,206.12,399.60,10.91;10,107.59,219.67,116.58,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,261.21,165.48,244.78,10.91;10,107.59,179.03,247.89,10.91">Overview of the ImageCLEF 2021: Multimedia retrieval in medical, nature, internet and social media applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M眉ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>P茅teri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tauteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>tefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Moustahfid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,378.03,179.03,129.79,10.91;10,107.59,192.57,398.40,10.91;10,107.59,206.12,137.45,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 12th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="10,276.45,206.12,183.53,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,107.59,233.22,400.24,10.91;10,107.59,246.77,399.60,10.91;10,107.59,260.32,322.05,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,387.91,233.22,119.91,10.91;10,107.59,246.77,258.95,10.91">Overview of ImageCLEFtuberculosis 2021 -CT-based tuberculosis type classification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M眉ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,389.21,246.77,113.25,10.91">CLEF2021 Working Notes</title>
		<title level="s" coord="10,107.59,260.32,175.50,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,107.59,273.87,400.24,10.91;10,107.59,287.42,398.40,10.91;10,107.59,300.97,175.57,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,340.06,273.87,167.77,10.91;10,107.59,287.42,285.09,10.91">Sedar: Reading floorplans like a human-using deep learning to enable human-inspired localisation</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Mendez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hadfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Pugeault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,400.71,287.42,105.28,10.91;10,107.59,300.97,76.42,10.91">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="1286" to="1310" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,107.59,314.52,398.40,10.91;10,107.59,328.07,398.40,10.91;10,107.59,341.62,107.17,10.91" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="10,251.88,314.52,254.11,10.91;10,107.59,328.07,320.30,10.91">Revealing lung affections from cts. a comparative analysis of various deep learning approaches for dealing with volumetric data</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Miron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Moisii</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Breaban</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.04160</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,107.59,355.17,398.40,10.91;10,107.59,368.71,400.24,10.91;10,106.53,382.26,233.74,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,216.50,355.17,289.49,10.91;10,107.59,368.71,86.60,10.91">Imageclef 2017: Supervoxels and co-occurrence for tuberculosis ct image classification</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="10,216.98,368.71,113.62,10.91">CLEF2017 Working Notes</title>
		<title level="s" coord="10,338.08,368.71,169.75,10.91;10,106.53,382.26,16.54,10.91">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,107.59,395.81,398.40,10.91;10,107.59,409.36,287.88,10.91" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.03924</idno>
		<title level="m" coord="10,354.61,395.81,151.38,10.91;10,107.59,409.36,157.95,10.91">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
