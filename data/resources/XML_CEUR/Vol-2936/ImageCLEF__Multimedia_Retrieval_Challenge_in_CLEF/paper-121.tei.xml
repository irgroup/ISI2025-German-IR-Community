<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.71,84.74,379.60,15.42;1,89.29,106.66,409.64,15.42;1,89.29,128.58,68.59,15.43">ViPTT-Net: Video pretraining of spatio-temporal model for tuberculosis type classification from chest CT scans</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,156.89,64.17,11.96"><forename type="first">Hasib</forename><surname>Zunair</surname></persName>
							<email>hasibzunair@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Concordia University</orgName>
								<address>
									<settlement>Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,166.11,156.89,76.79,11.96"><forename type="first">Aimon</forename><surname>Rahman</surname></persName>
							<email>aimon.rahman@northsouth.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">North South University</orgName>
								<address>
									<settlement>Dhaka</settlement>
									<country key="BD">Bangladesh</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,273.90,156.89,95.07,11.96"><forename type="first">Nabeel</forename><surname>Mohammed</surname></persName>
							<email>nabeel.mohammed@northsouth.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">North South University</orgName>
								<address>
									<settlement>Dhaka</settlement>
									<country key="BD">Bangladesh</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">viptt-net CLEF</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.71,84.74,379.60,15.42;1,89.29,106.66,409.64,15.42;1,89.29,128.58,68.59,15.43">ViPTT-Net: Video pretraining of spatio-temporal model for tuberculosis type classification from chest CT scans</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">1BA317548424CC32F8EEB8729B8A1E4B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Tuberculosis</term>
					<term>3D image classification</term>
					<term>Human Action Recognition</term>
					<term>Spatial-Temporal Information</term>
					<term>Pretraining</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pretraining has sparked groundswell of interest in deep learning workflows to learn from limited data and improve generalization. While this is common for 2D image classification tasks, its application to 3D medical imaging tasks like chest CT interpretation is limited. We explore the idea of whether pretraining a model on realistic videos could improve performance rather than training the model from scratch, intended for tuberculosis type classification from chest CT scans. To incorporate both spatial and temporal features, we develop a hybrid convolutional neural network (CNN) and recurrent neural network (RNN) model, where the features are extracted from each axial slice of the CT scan by a CNN, these sequences of image features are input to a RNN for classification of the CT scan. Our model termed as ViPTT-Net, was trained on over 1300 video clips with labels of human activities, and then fine-tuned on chest CT scans with labels of tuberculosis type. We find that pretraining the model on videos lead to better representations and significantly improved model validation performance from a kappa score of 0.17 to 0.35, especially for under-represented class samples. Our best method achieved 2nd place in the ImageCLEF 2021 Tuberculosis -TBT classification task with a kappa score of 0.20 on the final test set with only image information (without using clinical meta-data). All codes and models are made available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Tuberculosis (TB) is a potentially fatal disease that generally affects the lungs. The disease spreads through cough, spit or sneeze and can remain latent within the human body. Although X-rays and microscopic analysis of bodily fluid are generally used to diagnose the disease, Computed Tomography (CT) provides detailed information about the infection. Deep learning models demonstrate promising results in diagnosing TB from both X-rays and CT scans <ref type="bibr" coords="1,464.69,556.32,11.23,10.91" target="#b0">[1,</ref><ref type="bibr" coords="1,477.81,556.32,7.42,10.91" target="#b1">2,</ref><ref type="bibr" coords="1,487.12,556.32,7.42,10.91" target="#b2">3,</ref><ref type="bibr" coords="1,496.43,556.32,7.49,10.91" target="#b3">4]</ref>. These methods are also proven to be effective for severity scoring of the infection as well <ref type="bibr" coords="1,473.31,569.87,11.23,10.91" target="#b4">[5,</ref><ref type="bibr" coords="1,486.78,569.87,7.42,10.91" target="#b5">6,</ref><ref type="bibr" coords="1,496.43,569.87,7.49,10.91" target="#b6">7]</ref>. However, the types of TB can vary and that may require a different course of treatment, making the identification of TB type an important real-life problem. The deep learning models are yet to provide high accuracy in this particular task.</p><p>The most challenging part of working with CT scan data is, it is three-dimensional (3D), meaning along with height and width, each data point contains depth information. Processing 3D data can be very computationally expensive and may require pre-processing before training a model. These pre-processing steps may include choosing selective slices or transforming slices into uniform sizes. Both of these techniques may contribute to losing some depth information. One of the most effective method of processing 3D data is to resize it into fixed dimension using spline interpolation along x, y and z-axis which was demonstrated in <ref type="bibr" coords="2,428.39,195.36,11.42,10.91" target="#b4">[5]</ref>. Although the method showed promising result in TB severity scoring, might not perform well on other more sophisticated tasks such as classifying TB types.</p><p>An alternative to directly processing 3D data is to decompose each scan into individual slices and afterward feeding them 2D CNN <ref type="bibr" coords="2,251.28,249.56,11.23,10.91" target="#b7">[8,</ref><ref type="bibr" coords="2,264.86,249.56,7.42,10.91" target="#b8">9,</ref><ref type="bibr" coords="2,274.63,249.56,12.50,10.91" target="#b9">10,</ref><ref type="bibr" coords="2,289.48,249.56,12.23,10.91" target="#b10">11]</ref>. The problem with this method is, as the slices are considered independent to each other, the spatial information along the z-axis is missing during training. Moreover, in this method, the label for the whole volume is assigned to each individual slice, which might not be the case when it comes to CT scans. More specifically, a CT scan of infected lungs may contain uninfected 2D slices.</p><p>In this work, we build a model to predict multiple tuberculosis types from chest CT scans. We pretrain a hybrid convolutional neural network (CNN) and recurrent neural network (RNN) model on human action recognition task, and then fine-tune the model for the tuberculosis type classification task. Pretraining significantly improves performance, especially for minority classes. The method is evaluated on the Image-CLEF 2021 Tuberculosis -TBT classification task which achieves 2nd place<ref type="foot" coords="2,204.63,383.29,3.71,7.97" target="#foot_0">1</ref> with a kappa score of 0.20 and accuracy of 0.42.</p><p>We summarize our contributions as follows:</p><p>1. We pretrain a hybrid CNN-RNN model, termed ViPTT-Net, on human action recognition task, and fine-tune the model on a small dataset of CT scans with labels indicating tuberculosis types. 2. We show pretraining ViPTT-Net on realistic videos improve performance for tuberculosis type classification, especially for under-represented class samples. 3. We evaluate our best method on the Image-CLEF 2021 Tuberculosis -TBT classification task which achieves 2nd place overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methodology</head><p>We start by problem formulation followed by presenting the main building blocks of our proposed method for multi-class CT image classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Problem Formulation</head><p>Given the labels of a set of CT scans, the objective is to predict the unknown labels of the new CT scans <ref type="foot" coords="2,116.36,636.53,3.71,7.97" target="#foot_1">2</ref> . More specifically, our goal is to learn a discriminative function 𝑓 (X) ∈ {1, 2, 3, 4, 5},</p><p>where the numbers represent the tuberculosis type: Infiltrative, Focal, Tuberculoma, Miliary and Fibro-cavernous repsectively. X represents a CT scan volume of size 𝐷 × 𝑊 × 𝐻, where D, W, and H represent the depth, width, and height of the volume respectively. The task is considered as a multi-class volumetric image classification problem. A 3D volumetric scan X can also be viewed as a time-series of 2D slices {𝑋 1 , . . . , 𝑋 𝐷 }. Therefore, we can also frame the task as a time-series sequence classification problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">ViPTT-Net Model</head><p>While convolutional neural networks (CNNs) have shown promising results at processing image data <ref type="bibr" coords="3,112.18,217.99,16.56,10.91" target="#b11">[12,</ref><ref type="bibr" coords="3,132.31,217.99,12.59,10.91" target="#b12">13,</ref><ref type="bibr" coords="3,148.48,217.99,12.42,10.91" target="#b13">14]</ref>, the same can be said for recurrent neural networks (RNN) for sequential data <ref type="bibr" coords="3,111.45,231.54,16.56,10.91" target="#b14">[15,</ref><ref type="bibr" coords="3,130.85,231.54,12.59,10.91" target="#b15">16,</ref><ref type="bibr" coords="3,146.28,231.54,12.42,10.91" target="#b16">17]</ref>. We developed a hybrid convolutional neural network (CNN) and recurrent neural network (RNN) model termed ViPTT-Net which is capable of incorporating both spatial and temporal features in the learning process. ViPTT-Net takes as input a CT scan and outputs probability predictions which indicates the type of tuberculosis in that CT scan. To deal with arbitrary volumes, we first resize the CT scan to a fixed size 70 × 224 × 224 using spline interpolated zoom (SIZ) <ref type="bibr" coords="3,195.95,299.28,12.71,10.91" target="#b4">[5]</ref> which exploits the full geometry of the 3D volume by interpolating over the z-axis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning spatial features using CNN.</head><p>To learn spatial features, ViPTT-Net consists of a VGG-16 model as the feature extractor which is pretrained on ImageNet <ref type="bibr" coords="3,400.03,345.91,16.08,10.91" target="#b17">[18]</ref>. The VGG-16 model has 16 layers with learnable weights: 13 convolutional layers, and 3 fully connected layers <ref type="bibr" coords="3,487.51,359.46,16.13,10.91" target="#b18">[19]</ref>. We extract features from the last convolutional layer which results in a 512 dimensional feature vector for a 2D input image. Since a CT scan consists of multiple 2D axial slices (70 in our case), we wrap the VGG-16 model in a time-distributed layer which is then applied to every axial slice (temporal frame) of the CT scan independently. The final output is a sequence of image features where the sequence length is 70 and each of the sequences are a 512 dimensional feature vector.</p><p>As the VGG-16 feature extractor accepts inputs of 3-channels, we map the 1-channel axial slices of the CT scan slices to 3-channel using a convolutional layer with 3 filters and kernel size of 1 × 1 × 1 before input to the feature extractor. We use this instead of converting to pseudo-color image by just duplicating intensity to all channels to mimic 3-channel because it become more computationally expensive to train.</p><p>Learning temporal features using RNN. Following the above, the sequence of image features are aggregated using an RNN. RNNs are a type of neural network which transforms a sequence of inputs into a sequence of outputs. Using an RNN, the temporal order of the axial slices is preserved. We use the long short-term memory (LSTM) <ref type="bibr" coords="3,402.07,555.12,17.98,10.91" target="#b19">[20]</ref> with 256 units and tanh activation. It is important to mention that since we are interested in classifying the full sequence, the output from the LSTM results in a 256 dimensional aggregated feature vector from the last time step coming from the full sequence of image features, rather than producing sequences of outputs. We also experimented with gated recurrent unit (GRU) but did not get good results on the validation set.</p><p>Finally, the aggregated 256 dimensional feature vector is passed to a dense layer with 1024 units with rectified linear hidden units (ReLUs) activation and a dense layer with a softmax function (i.e. a dense softmax layer of 5 units for the multi-class classification case) that yields </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Pretraining on Human Action Recognition Task</head><p>ViPTT-Net was first pretrained on a subset of the UCF50 dataset <ref type="bibr" coords="4,378.47,477.53,16.27,10.91" target="#b20">[21]</ref>. UCF50 is human action recognition dataset with 50 action categories, consisting of realistic videos taken from youtube. Similar to pretraining for 2D image problems <ref type="bibr" coords="4,304.26,599.47,16.44,10.91" target="#b21">[22,</ref><ref type="bibr" coords="4,323.43,599.47,12.33,10.91" target="#b22">23]</ref>, after training ViPTT-Net on a subset of UCF50 dataset, the network was fine-tuned on the tuberculosis type classification task by replacing the final fully connected 10-way softmax layer with a 5-way softmax. During training ViPTT-Net on UCF50, the weights of the VGG-16 feature extractor was frozen. And while fine-tuning on the tuberculosis type classification task, all the network layers were trained.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Weighted Loss and Data Augmentation</head><p>Due to heavy class imbalance, we assign weights in the loss function for each tuberculosis type with the goal to reduce biasness towards the over-represented class samples. Prior to training using weighted loss, the weights are computed over the training set.</p><p>It is a standard practice to perform data augmentation to improve generalization, especially when there are limited number of training data. Data augmentation basically creates modified versions of the input data in a dataset through random transformations such as horizontal and vertical flip, zoom augmentation, horizontal and vertical shift, etc. While training, the 3D CT scans are rotated with degree of rotations picked randomly from [-20, -10, -5, 0, 5, 10, 20] as a form of data augmentation. We use a wider range to cover a larger distribution of augmented images. Notice that we added 0 in the range which means that the model looks at both augmented and non-augmented data. We experimented without adding 0 in the range and led to poor results on the validation set. We also tried blur and random shifts but did not get good results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiment Setup</head><p>We describe experimental details, i.e., dataset, implementation details, evaluaton metrics, etc., and present quantitative results, comparing different training strategies using ViPTT-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets and Preprocessing</head><p>The dataset is provided by ImageCLEF Tuberculosis Type 2021 Challenge <ref type="bibr" coords="5,407.15,593.83,16.30,10.91" target="#b23">[24,</ref><ref type="bibr" coords="5,425.72,593.83,12.23,10.91" target="#b24">25]</ref>. It contains CT scans of a total of 1338 TB patients, 917 of them have been used for training and 421 for the test set. Each scan has a label that indicates one of the TB types -Infiltrative, Focal, Tuberculoma, Miliary and Fibro-cavernous. Each CT scan belongs to only one patient. The images have dimensions of 512 x 512 pixels with varying depth sizes. In addition to labels, some scans have additional meta-data. All the scans have auto-generated lungs mask, although some of these masks are missing in largely affected areas or have rough bounds, majority of the scans are quite accuracte. The original data is in NIFTI format, storing the raw voxel intensity in Hounsfield units (HU). An instance of a CT scan with Infiltrative tuberculosis type is shown in Fig 2b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Implementation Details</head><p>All experiments are performed on a Linux workstation running 4.8Hz and 64GB RAM with and RTX 3080 GPU. Experiments are conducted using Python programming language <ref type="bibr" coords="6,487.15,177.34,16.41,10.91" target="#b25">[26]</ref>. ViPTT-Net is implemented in Keras <ref type="bibr" coords="6,250.46,190.89,16.25,10.91" target="#b26">[27]</ref>, with TensorFlow backend <ref type="bibr" coords="6,391.33,190.89,16.25,10.91" target="#b27">[28]</ref>.</p><p>ViPTT-Net is trained end-to-end using stochastic gradient descent (SGD) optimization algorithm to minimize the categorical cross-entropy loss function with an initial learning rate of 0.001 and batch size of 2. A factor of 0.1 is used to reduce the learning rate once the loss stagnates. Training is continued until the validation loss stagnates using an early stopping mechanism, and then the best weights are retained. To keep data proportions same and ensure reproducibility, we perform a stratified train and validation split with a ratio of 80/20 (732/184 CT scans) on the training data provided by ImageCLEF 2021 -TBT.</p><p>Similar steps are followed while training on a subset of the UCF50 dataset, except we do not use any data augmentation in this case. Also, the pretrained VGG-16 feature extractor was kept frozen during training since both ImageNet and UCF50 samples are natural images/videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Evaluation Metrics</head><p>According to challenge rules, the task is evaluated as a multi-class classification problem. The main evaluation metric is the kappa score. Kappa measures the inter-rater reliability for categorical items.</p><formula xml:id="formula_0" coords="6,269.29,424.85,236.69,25.50">𝜅 ≡ 𝑝 0 -𝑝 𝑒 1 -𝑝 𝑒<label>(1)</label></formula><p>Here, 𝑝 0 indicates the relative observed agreement among raters and 𝑝 𝑒 is the probability of agreement by chance. If raters are in complete agreement then 𝜅 = 1 and no agreement other than by chance will result in 𝜅 = 0. The value would be negative if there is no effective agreement among the raters or the agreement is worse than random. Additionally we also show accuracy (ACC) and per-class F1 scores. For all metrics, the higher the better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Results</head><p>We study the effect of training ViPTT-Net for the task of tuberculosis type classification using different training strategies:</p><p>No PT. ViPTT-Net is trained from scratch on the 732 CT scans annotated for tuberculosis types PT. ViPTT-Net is first pretrained on a subset of the UCF50 dataset which around 1300 video clips annotated for human activities. Then the last layer is replaced with a five unit softmax and fine-tuned on 732 CT scans annotated for tuberculosis types.  <ref type="table" coords="7,127.46,406.66,5.16,10.91" target="#tab_1">1</ref> summarizes the results which show that pretraining ViPTT-Net on a subset of the UCF50 dataset followed by fine-tuning for tuberculosis type classification (PT) improves kappa score by 0.18 on the validation set compared to training ViPTT-Net from scratch (No PT). In both configurations (No PT and PT), F1 score is the lowest for Tuberculoma class with scores of 0 and 0.09 for No PT and PT respectively. This is improved by using weighted loss (PT+CW), where the model achieves Tuberculoma F1 score 0.37. The F1 score of PT+CW also improves for FibC class by a large margin compared to No PT, although there is a slight drop in overall kappa score. PT+CW with data augmentation (PT+CW+AUG) further improves performance, more specifically for Miliary class by a large margin.</p><p>We also report results on the final test set by by ImageCLEF 2021 -TBT classification task in Table <ref type="table" coords="7,116.21,542.15,3.80,10.91" target="#tab_2">2</ref>. Using weighted loss results in similar performance improvements. Our best method PT+CW+AUG in which ViPTT-Net is trained to minimize the weighted cross entropy loss and data augmentation performs the best with a kappa score of 0.20 and accuracy of 42.30%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion and Conclusion</head><p>We address the problem of predicting tuberculosis types from 3D chest CT scans. We develop a hybrid CNN-RNN model, termed ViPTT-Net, which is capable of learning both spatial and temporal features of the CT scan. Our experiments demonstrate that pretraining on human action recognition task significantly improves performance of tuberculosis type classification rather than training the model from scratch. This is most significant for Miliary and Fibro-cavernous types without specifically aiming to improve performance for those classes. Interestingly, these classes along with Tuberculoma are the tuberculosis types which have the least amount of samples in the dataset. To further deal with class imbalance, we use a weighted loss function with weights computed over the training set that improves performance of under-represented classes significantly. Data augmentation also improved performance for few tuberculosis types on the validation set. On the test set, the highest kappa score was observed when pretraining ViPTT-Net on videos, using weighted loss function and data augmentation. This method achieved 2nd place in the ImageCLEF 2021 Tuberculosis -TBT classification task which operates on the CT image alone without using the additional patient meta-data and the lung segmentation masks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,315.28,416.94,8.93;4,89.29,327.29,416.70,8.87;4,89.29,339.24,418.23,8.87;4,89.02,351.20,416.96,8.87;4,89.29,363.15,321.73,8.87"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Schematic layout of the hybrid CNN-RNN model ViPTT-Net.Given a 3D CT scan of arbitrary size, uniform resizing is performed across all dimensions using SIZ<ref type="bibr" coords="4,363.71,327.29,10.51,8.87" target="#b4">[5]</ref>. Features are extracted from all the axial slices of the processed CT scan to output a sequence of image features using a VGG-16 model. These sequence of image features are input to an LSTM layer followed by dense layers of 1024 neurons and finally 5 with softmax activation for the multi-class classification problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,105.75,222.32,187.80,9.96;5,118.91,234.28,90.07,9.96;5,301.42,222.32,187.80,9.96;5,314.81,234.28,102.65,9.96"><head></head><label></label><figDesc>(a) Sequence of images from a video of a man playing Jumping Jack.(b) Sequence of axial images from a CT scan with the label Infiltrative.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,89.29,252.76,416.94,8.93;5,89.29,264.77,248.80,8.87"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of image sequences of a video and a CT scan from the UCF50 and ImageCLEF 2021 -TBT datasets. Both samples are resized to depth of 70.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.96,504.63,418.22,92.21"><head></head><label></label><figDesc>The classes of different action are Baseball Pitch, Jumping Jack, Kayaking etc. Due to compute constraints, we use 1366 videos from 10 classes selected randomly: Mixing, Tennis Swing, Horse Riding, Jump Rope, Jumping Jack, Baseball Pitch, Rowing, SkateBoarding, Walking With Dog, Skijet. Video clips are resized to 70 × 224 × 224, where 70 is the number of temporal frames (image sequences), and 224 is width and height of the temporal frame. It is also important to mention that each RGB frame of the video is converted to grayscale (single channel) since the CT scans also have single channel values. An instance of an action video is shown in Fig 2a.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,88.99,90.49,417.00,115.48"><head>Table 1</head><label>1</label><figDesc>Overall Kappa score and per-class F1 score achieved by different methods on the validation dataset of tuberculosis type classification. FibC denotes Fibro-cavernous. For all methods, the ViPTT-Net model is used.</figDesc><table coords="7,135.71,143.74,321.37,62.22"><row><cell>Method</cell><cell cols="6">Kappa Infiltrative Focal Tuberculoma Miliary FibC</cell></row><row><cell>No PT</cell><cell>0.17</cell><cell>0.61</cell><cell>0.46</cell><cell>0.0</cell><cell>0.2</cell><cell>0.14</cell></row><row><cell>PT</cell><cell>0.35</cell><cell>0.68</cell><cell>0.56</cell><cell>0.09</cell><cell>0.4</cell><cell>0.48</cell></row><row><cell>PT+CW</cell><cell>0.30</cell><cell>0.59</cell><cell>0.41</cell><cell>0.37</cell><cell>0.33</cell><cell>0.65</cell></row><row><cell cols="2">PT+CW+AUG 0.33</cell><cell>0.59</cell><cell>0.47</cell><cell>0.27</cell><cell>0.54</cell><cell>0.61</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,88.99,228.21,417.22,189.35"><head>Table 2</head><label>2</label><figDesc>Overall Kappa and Accuracy score achieved by different methods on the test set by ImageCLEF 2021 Tuberculosis -TBT classification.</figDesc><table coords="7,224.75,269.51,143.28,50.27"><row><cell>Method</cell><cell cols="2">Kappa Accuracy</cell></row><row><cell>PT</cell><cell>0.13</cell><cell>42.30</cell></row><row><cell>PT+CW</cell><cell>0.14</cell><cell>38.50</cell></row><row><cell cols="2">PT+CW+AUG 0.20</cell><cell>42.30</cell></row></table><note coords="7,89.29,346.48,416.69,10.91;7,89.29,360.03,269.96,10.91;7,89.29,379.56,416.69,10.91;7,89.29,393.11,38.08,10.91;7,100.20,406.66,24.54,10.91"><p><p>PT+CW. Same as PT, but additionally weighted loss is used with weights computed over the training set of 732 CT scans annotated for tuberculosis types PT+CW+AUG. Same as PT+CW, but additionally the 3D volumes are randomly rotated during training.</p>Table</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.93,662.09,287.79,6.49"><p>imageclef-2021-tuberculosis-tbt-classification/leaderboards</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,108.93,673.05,248.77,6.49"><p>https://www.imageclef.org/2021/medical/tuberculosis</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,112.66,281.08,393.33,10.91;8,112.66,294.63,393.98,10.91;8,112.66,308.18,38.81,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,226.58,281.08,279.41,10.91;8,112.66,294.63,290.09,10.91">Deep learning at chest radiography: automated classification of pulmonary tuberculosis by using convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lakhani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sundaram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,412.29,294.63,45.71,10.91">Radiology</title>
		<imprint>
			<biblScope unit="volume">284</biblScope>
			<biblScope unit="page" from="574" to="582" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,321.73,394.53,10.91;8,112.66,335.28,393.33,10.91;8,112.66,348.83,393.32,10.91;8,112.33,362.38,48.04,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,285.94,335.28,220.04,10.91;8,112.66,348.83,285.05,10.91">CheXaid: deep learning assistance for physician diagnosis of tuberculosis using chest x-rays in patients with hiv</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>O'connell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schechter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Asnani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kiani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mendelson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Maartens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Van Hoving</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,405.73,348.83,92.38,10.91">NPJ digital medicine</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,375.93,394.53,10.91;8,112.66,389.48,393.53,10.91;8,112.66,403.03,393.33,10.91;8,112.41,416.58,65.99,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,252.43,389.48,253.76,10.91;8,112.66,403.03,312.82,10.91">Deep learning, computer-aided radiography reading for tuberculosis: a diagnostic accuracy study from a tertiary hospital in india</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kadavigere</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Andrade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">A</forename><surname>Sukumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">P</forename><surname>Shenoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Huddart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Saravu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,432.53,403.03,73.46,10.91">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,430.13,393.33,10.91;8,112.66,443.67,393.98,10.91;8,112.41,457.22,23.60,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,329.27,430.13,176.71,10.91;8,112.66,443.67,270.34,10.91">A deep learning system that generates quantitative CT reports for diagnosing pulmonary tuberculosis</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,390.41,443.67,87.71,10.91">Applied Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,470.77,393.65,10.91;8,112.66,484.32,393.33,10.91;8,112.66,497.87,237.05,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,335.75,470.77,170.56,10.91;8,112.66,484.32,204.87,10.91">Uniformizing techniques to process CT scans with 3D CNNs for tuberculosis prediction</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zunair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,339.49,484.32,166.50,10.91;8,112.66,497.87,106.07,10.91">International Workshop on PRedictive Intelligence In MEdicine</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="156" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,511.42,393.32,10.91;8,112.66,524.97,394.93,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,286.10,511.42,219.88,10.91;8,112.66,524.97,241.26,10.91">Estimating severity from CT scans of tuberculosis patients using 3D convolutional nets and slice selection</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zunair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Mohammed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,380.30,524.97,97.69,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,538.52,393.33,10.91;8,112.66,552.07,313.51,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,170.06,538.52,335.93,10.91;8,112.66,552.07,111.55,10.91">ImageCLEF 2019: Projection-based CT image analysis for TB severity scoring and CT report generation</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,232.71,552.07,114.59,10.91">CLEF2019 Working Notes</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2380</biblScope>
			<biblScope unit="page" from="9" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,565.62,393.33,10.91;8,112.66,579.17,345.76,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,243.02,565.62,262.97,10.91;8,112.66,579.17,39.20,10.91">Classification of CT brain images based on deep learning networks</title>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,160.68,579.17,218.88,10.91">Computer methods and programs in biomedicine</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="49" to="56" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,592.72,393.61,10.91;8,112.66,606.27,393.33,10.91;8,112.66,619.81,320.78,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,352.86,592.72,153.40,10.91;8,112.66,606.27,248.23,10.91">RADnet: Radiologist level accuracy using deep learning for hemorrhage detection in CT scans</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Varadarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,404.51,606.27,101.48,10.91;8,112.66,619.81,206.56,10.91">IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="281" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,633.36,393.33,10.91;8,112.66,646.91,394.33,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,166.14,633.36,339.85,10.91;8,112.66,646.91,192.09,10.91">ImageCLEF2019: Tuberculosis-severity scoring and CT report with neural networks, transfer learning and ensembling</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gentili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,313.54,646.91,114.59,10.91">CLEF2019 Working Notes</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2380</biblScope>
			<biblScope unit="page" from="9" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,660.46,394.62,10.91;9,112.66,86.97,393.98,10.91;9,112.66,100.52,23.60,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,429.73,660.46,77.55,10.91;9,112.66,86.97,218.27,10.91">ImageCLEF 2019: Deep learning for tuberculosis CT image analysis</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hamadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">B</forename><surname>Cheikh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zouatine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M B</forename><surname>Menad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Djebbara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,339.16,86.97,114.83,10.91">CLEF2019 Working Notes</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2380</biblScope>
			<biblScope unit="page" from="9" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,114.06,393.33,10.91;9,112.66,127.61,305.62,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,345.40,114.06,160.59,10.91;9,112.66,127.61,96.30,10.91">Gradient-based learning applied to document recognition</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,217.35,127.61,106.84,10.91">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,141.16,393.32,10.91;9,112.66,154.71,395.01,10.91;9,112.41,168.26,48.96,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,295.19,141.16,210.79,10.91;9,112.66,154.71,72.94,10.91">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,216.17,154.71,241.18,10.91">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,181.81,395.17,10.91;9,112.66,195.36,395.01,10.91;9,112.41,208.91,38.81,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,259.74,181.81,203.38,10.91">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,488.38,181.81,19.45,10.91;9,112.66,195.36,347.24,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,222.46,393.33,10.91;9,112.66,236.01,393.33,10.91;9,112.66,249.56,253.93,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,389.97,222.46,116.02,10.91;9,112.66,236.01,149.15,10.91">Recurrent neural networks for emotion recognition in video</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,290.35,236.01,215.64,10.91;9,112.66,249.56,166.37,10.91">Proceedings of the 2015 ACM on international conference on multimodal interaction</title>
		<meeting>the 2015 ACM on international conference on multimodal interaction</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="467" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,263.11,393.33,10.91;9,112.66,276.66,253.05,10.91" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="9,372.30,263.11,133.68,10.91;9,112.66,276.66,70.43,10.91">Recent advances in recurrent neural networks</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Salehinejad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Barfett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Colak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Valaee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01078</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,290.20,394.52,10.91;9,112.66,303.75,393.33,10.91;9,112.66,317.30,393.33,10.91;9,112.33,330.85,29.19,10.91" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="9,319.98,303.75,186.01,10.91;9,112.66,317.30,239.23,10.91">Deep sequential learning for cervical spine fracture detection on computed tomography imaging</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Salehinejad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Crivellaro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Samorodova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Arciniegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Merali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Suthiphosuwan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bharatha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yeom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.13336</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,344.40,393.33,10.91;9,112.66,357.95,394.53,10.91;9,112.66,371.50,103.61,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="9,345.70,344.40,160.29,10.91;9,112.66,357.95,67.28,10.91">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,228.08,357.95,274.55,10.91">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,385.05,393.33,10.91;9,112.66,398.60,342.71,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="9,247.99,385.05,258.00,10.91;9,112.66,398.60,49.16,10.91">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,184.84,398.60,240.50,10.91">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,412.15,393.33,10.91;9,112.66,425.70,393.98,10.91;9,112.66,439.25,48.96,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="9,434.53,412.15,71.45,10.91;9,112.66,425.70,61.83,10.91">LSTM: A search space odyssey</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">R</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,186.22,425.70,275.72,10.91">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2222" to="2232" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,452.79,393.32,10.91;9,112.39,466.34,186.68,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="9,213.35,452.79,245.14,10.91">Recognizing 50 human action categories of web videos</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,466.95,452.79,39.04,10.91;9,112.39,466.34,102.74,10.91">Machine vision and applications</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="971" to="981" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,479.89,393.61,10.91;9,112.66,493.44,280.91,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="9,398.38,479.89,107.88,10.91;9,112.66,493.44,86.89,10.91">A comprehensive survey on transfer learning</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,207.86,493.44,106.84,10.91">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="43" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,506.99,393.53,10.91;9,112.66,520.54,257.76,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="9,221.05,506.99,285.14,10.91;9,112.66,520.54,35.05,10.91">Melanoma detection using adversarial training and deep transfer learning</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zunair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Hamza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,156.02,520.54,136.44,10.91">Physics in Medicine &amp; Biology</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">135005</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,534.09,395.01,10.91;9,112.66,547.64,394.53,10.91;9,112.48,561.19,395.18,10.91;9,112.66,574.74,394.53,10.91;9,112.28,588.29,393.70,10.91;9,112.66,601.84,393.33,10.91;9,112.66,615.39,393.33,10.91;9,112.66,628.93,393.53,10.91;9,112.66,642.48,197.61,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="9,264.32,588.29,241.66,10.91;9,112.66,601.84,261.75,10.91">Overview of the ImageCLEF 2021: Multimedia retrieval in medical, nature, internet and social media applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Peteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tauteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Moustahfid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,401.27,601.84,104.72,10.91;9,112.66,615.39,393.33,10.91;9,112.66,628.93,201.15,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 12th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="9,348.61,628.93,157.57,10.91;9,112.66,642.48,31.10,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,656.03,395.17,10.91;9,112.66,669.58,394.53,10.91;10,112.66,86.97,394.53,10.91;10,112.66,100.52,22.69,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="9,393.45,656.03,114.38,10.91;9,112.66,669.58,257.62,10.91">Overview of ImageCLEFtuberculosis 2021 -CT-based tuberculosis type classification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="9,392.48,669.58,110.11,10.91">CLEF2021 Working Notes</title>
		<title level="s" coord="10,112.66,86.97,181.84,10.91">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,114.06,393.33,10.91;10,112.66,127.61,153.31,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="10,219.53,114.06,138.71,10.91">Python programming language</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Van Rossum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,389.73,114.06,116.26,10.91;10,112.66,127.61,46.30,10.91">USENIX annual technical conference</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,141.16,198.99,10.91" xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,154.71,394.53,10.91;10,112.66,168.26,393.33,10.91;10,112.66,181.81,394.52,10.91;10,112.66,195.36,17.62,10.91" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="10,179.63,168.26,235.77,10.91">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,437.57,168.26,68.42,10.91;10,112.66,181.81,325.54,10.91">12th {USENIX} symposium on operating systems design and implementation ({OSDI} 16)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
