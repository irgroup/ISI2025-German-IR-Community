<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,374.18,15.42;1,89.29,106.66,368.63,15.42;1,89.29,128.58,150.43,15.43">Overview of the 2021 ImageCLEFdrawnUI Task: Detection and Recognition of Hand Drawn and Digital Website UIs</title>
				<funder ref="#_JJB9qMR #_PtycmVA">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,156.89,54.95,11.96"><forename type="first">Raul</forename><surname>Berari</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">teleportHQ</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,156.88,156.89,83.13,11.96"><forename type="first">Andrei</forename><surname>TƒÉuteanu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">teleportHQ</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,252.65,156.89,71.72,11.96"><forename type="first">Dimitri</forename><surname>Fichou</surname></persName>
							<email>dimitri.fichou@teleporthq.io</email>
							<affiliation key="aff0">
								<orgName type="department">teleportHQ</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,337.01,156.89,44.49,11.96"><forename type="first">Paul</forename><surname>Brie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">teleportHQ</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,394.14,156.89,71.28,11.96"><forename type="first">Mihai</forename><surname>Dogariu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Politehnica University of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,170.84,93.69,11.96"><forename type="first">Liviu</forename><forename type="middle">Daniel</forename><surname>≈ûtefan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Politehnica University of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,195.63,170.84,124.23,11.96"><forename type="first">Mihai</forename><forename type="middle">Gabriel</forename><surname>Constantin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Politehnica University of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,350.85,170.84,77.54,11.96"><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
							<email>bogdan.ionescu@upb.ro</email>
							<affiliation key="aff1">
								<orgName type="institution">Politehnica University of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,374.18,15.42;1,89.29,106.66,368.63,15.42;1,89.29,128.58,150.43,15.43">Overview of the 2021 ImageCLEFdrawnUI Task: Detection and Recognition of Hand Drawn and Digital Website UIs</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">F2F9F7B375C0392DFC3B68AC1F3CDBB5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Object Detection</term>
					<term>User Interface</term>
					<term>Machine Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>An appealing web-page is a must have for most companies nowadays. The creation of such user interfaces is a complex process involving various actors such as project managers, designers and developers. Facilitating this process can democratize access to the web to non-experts. The second edition Image-CLEFdrawnUI 2021 addresses this issue by fostering systems that are capable of automatically generating a web-page from a sketch. Participants were challenged to develop machine learning solutions to analyze images of user interfaces and extract the position and type of its different elements, such as images, buttons and text. The task is separated into two subtasks, the wireframe subtask with hand drawn images and the screenshot subtask with digital images. In this article, we overview the task requirements and data as well as the participants results. For the wireframe subtask, three teams submitted 21 runs and two of the teams outperformed the baseline, with the best run scoring 0.9 compared to a baseline of 0.747 in terms of mAP@0.5 IoU. For the screenshot subtask, one team submitted 7 runs and all runs scored better than the baseline in terms of mAP 0.5@IoU, the best run obtaining 0.628 against 0.329 for the baseline.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, the use of machine learning techniques with the aim of automatizing the creation of User Interfaces (UI) gained interest. Several data sets have been made available to help this effort. In 2017, the RICO data set was released by Deka et al. <ref type="bibr" coords="1,325.16,510.29,12.69,10.91" target="#b0">[1]</ref> and consisted of 72,219 screenshots of mobile applications along with the associated position and tree structure of their UI elements present on screen. This data set had later been used as template to create the SWIRE data set in which 3,802 wireframes were drawn by designers and used to demonstrate the retrieval of similar UI after deep learning embedding and nearest neighbour search <ref type="bibr" coords="1,405.22,564.49,11.33,10.91" target="#b1">[2]</ref>. As the positions of the UI elements in the RICO data set were extracted automatically, the bounding boxes were not always overlapping with the elements in the images. The VINS data set was collected to address those drawbacks by manually annotating 4,543 images and use them in two ways, to retrieve similar UIs to a query screenshot and for object detection <ref type="bibr" coords="1,346.80,618.68,11.41,10.91" target="#b2">[3]</ref>. Similarly to this last example of object detection, several end-to-end approaches were explored. Pix2code <ref type="bibr" coords="2,418.80,308.97,12.92,10.91" target="#b3">[4]</ref> and UI2code <ref type="bibr" coords="2,493.06,308.97,12.92,10.91" target="#b4">[5]</ref> were introduced to analyze screenshots and translate them into domain specific languages via Convolutional Neural Network encoding followed by Recurrent Neural Network decoding.</p><p>This paper presents the second edition of the ImageCLEFdrawnUI task as part of the Image-CLEF<ref type="foot" coords="2,113.27,361.42,3.71,7.97" target="#foot_0">1</ref> benchmarking campaign <ref type="bibr" coords="2,232.47,363.17,12.77,10.91" target="#b5">[6]</ref> which is itself part of CLEF<ref type="foot" coords="2,368.56,361.42,3.71,7.97" target="#foot_1">2</ref> (CROSS Language Evaluation Forum). As in the previous edition <ref type="bibr" coords="2,240.49,376.72,11.23,10.91" target="#b6">[7,</ref><ref type="bibr" coords="2,254.06,376.72,7.42,10.91" target="#b7">8,</ref><ref type="bibr" coords="2,263.81,376.72,7.42,10.91" target="#b8">9,</ref><ref type="bibr" coords="2,273.57,376.72,12.23,10.91" target="#b9">10]</ref>, the participants must develop a machine learning system able to detect the position and type of different UI elements in images. The task is separated in two subtasks, one focused on hand drawn UI like last year and a new one taking as input digital screenshots (see an example in Figure <ref type="figure" coords="2,317.52,417.37,3.57,10.91" target="#fig_0">1</ref>).</p><p>The next sections are organized as follows. The data sets are presented in Section 2. The evaluation methodology is described in Section 3. In Section 4, the task results are presented and finally, Section 5 discusses the results and the future work for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Data set</head><p>Two separate data sets have been created for the subtasks of the challenge. The former, corresponding to the wireframe subtask, is an improved version of the 2020 ImageCLEFdrawnUI task and consists of pictures of hand-drawn website representations. The wireframes are highly abstract, a fact which encouraged the creation of a standard for defining 21 clearly differentiated classes of UI elements. The annotation process took place manually.</p><p>The latter, representing the screenshot sub-task, is a novel data set obtained through the recursive parsing and screen capturing of a large number of websites. After applying several filters to eliminate most low quality data points, the resulting collection consists of both partial and full representations of websites as they are found on the internet. To account for the high variance between instances from the same class of UI elements, the set of possible categories numbered 6 classes. Annotation was a predominantly automated process, with manual intervention for about 30% of the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Wireframe Subtask</head><p>Wireframes represent abstract, paper-drawn drawings of websites, created with the intent of reducing an interface to its simplest parts. For example, a web page could be depicted through an arrangement of abstract shapes corresponding to the images, text, and buttons visible to the user. Such depictions are used by designers in their work with either clients, developers or business owners as a simplified way of portraying interfaces and user interactions. To reduce the ambiguity present when 'translating' a website into its wireframe form, a convention was created, establishing strict rules for differentiating between the classes of elements (see Figure <ref type="figure" coords="4,181.24,268.72,3.51,10.91" target="#fig_1">2</ref>). A single data point represents a picture of such a wireframe, annotated according to the convention. The data set consists of 3,218 images used for training the object detection model and 1,073 used in testing it. Figure <ref type="figure" coords="4,319.04,295.82,5.07,10.91" target="#fig_2">3</ref> illustrates three random data points.</p><p>Similar to the version from last year, the data points were created by taking pictures under different lightning and scanning wireframe drawings. Although the UI elements respect the convention mentioned above, the color of the contour and the lighting conditions differ throughout the data set. Annotation was provided manually, using the open-source Microsoft tool, VoTT<ref type="foot" coords="4,501.07,348.26,3.71,7.97" target="#foot_2">3</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Changes from Last Year</head><p>The wireframe data set from the ImageCLEFdrawnUI task in 2020 lacked the proper representation of certain UI elements, such as the stepper input, video, list and table classes. Additionally, the split between the training and testing sets did not take into account this fact, which consequently resulted in an unequal proportion of the UI classes between the two. To account for this issue, we have supplanted the data set with an additional 1,200 images, prioritizing the ones which contain one or more instances of the rare classes (table, stepper input, list and video). Splitting the data set also received special attention, with the resulting subsets denoting a better representation (see Figure <ref type="figure" coords="4,207.67,494.19,3.57,10.91" target="#fig_4">5</ref>).</p><p>Another problem that arose after analysing the 2020 submissions and working notes was that similar-looking data points were present in both the training and the testing sets. This may have caused the models to overfit on some examples, lowering the difficulty of the challenge. In 2021 the data set split included all versions of a wireframe (which differ by lighting or type of capture) in only one of the subsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Data set Analysis</head><p>As Figure <ref type="figure" coords="4,136.13,611.26,5.17,10.91" target="#fig_3">4</ref> shows, the density of UI elements per image has increased on average, with 54 compared to last year's 28. While the minimum number of elements per image has remained the same (4), the maximum number of elements per image has increased to 175, from a maximum  of 131 in 2020. This is the result of supplanting with a significant number of dense data points, for the most part wireframes representing desktop versions of websites.</p><p>Figure <ref type="figure" coords="5,132.19,478.89,5.17,10.91" target="#fig_4">5</ref> illustrates that given the nature of the data, which reflects patterns found in the majority of websites, some UI classes will inherently be more common than others. Combined with the fact that wireframes seldom include only a few elements, manipulating the data set split to create a more proportional representation in the rare classes will essentially skew other classes into non-proportional ratios. For example, wireframes containing a table, which is a rare class, will naturally have a higher number of elements, firstly because tables are more common on desktop web pages, and secondly because tables themselves contain a number of other elements. As an example, taken by the number of data points, the train-test ratio is approximately 3:1, whereas tables are relatively over-represented in the test set (train-test ratio is less than 2:1). This could have resulted in discrepancies affecting other classes of elements by under-representing them in the training set.</p><p>Although under-representation as a by-product skews the proportion of more common elements towards the test set, we regarded the number of instances of under-represented items to be too high to pose any problems to the model performance. Since all under-represented classes number thousands of instances, the impact would be minimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Screenshot Subtask</head><p>The second data set consists of a collection of sections and full-page screenshots, extracted from a set of selected websites. The data points were acquired using a multi-step process, which will be described in the following section. The data set is split into 6,555 images used for training, 1,818 for testing, and 903 for validation.</p><p>Considering that working with websites imply a large number of diverse HTML tags, analysing a large amount webpages using the default tags would not be a suitable approach. As an initial solution, a mapping of each tag into 10 classes -each encapsulating the use case of that respective tag on the web page -had been adopted as follows: text, image, svg, video, link, input, list item, table, button and line break. After analysing the data set and testing an object detection model on it, it could be noticed that a number of classes created a level of ambiguity which reduced its performance. Consequently, the list item, video, table and line break classes have been removed, the image and svg classes merged, and the text class split into text and heading.</p><p>The following is a set of short descriptions of the 6 classes that can be found throughout the data set:</p><p>‚Ä¢ Text: One or multiple lines of text, generally in a smaller font size.</p><p>‚Ä¢ Heading: Usually one line of text, written in a different color or a bigger font size.</p><p>‚Ä¢ Link: Text usually present inside the header and footer. If inside the page, it is often written in a different color, or underlined. ‚Ä¢ Image: A simple image or icon.</p><p>‚Ä¢ Input: Text inputs present in forms, usually having a placeholder.</p><p>‚Ä¢ Button: A rectangular element that contains text and generally has a different background color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Acquisition</head><p>Acquiring the data set implied a multi-step process. Firstly, an in-house web parser was built in order to allow data gathering and screenshots by automatically processing a list of popular websites. However, a significant amount of data points were not fit to be used for object detection, as they contained issues such as bounding box overlapping or 404 pages. To discard most of those data points, a machine learning-based classifier was created to predict the usability of a website. The pipeline was further enhanced to be capable of also parsing the links found in websites of high-probability, therefore creating a recursive loop, and exponentially increasing the number of data points.</p><p>Then, an algorithm that detects and score sections inside the websites was created. A section is a point of interest, inside a website, consisting of a collection of elements, such as a header, a footer, a navbar, a form or a Call-To-Action.</p><p>Using both the full-page screenshots and section data sets, a selection of the highest quality data points was made, in order to be used in the final data set. The initial selection has been done solely based on the probabilities returned by the scoring algorithms for both types of data. Afterwards, using an Interquartile Range built upon different properties such as page height, page width or number of elements, everything that was bellow or above a given threshold, has been removed in order to remove the outliers. Further into the data processing, the most optimal 10,000 data points -consisting of both full-page screenshots, and sections -have been selected. From the 10,000, a last clean-up and a split has been done, resulting in the three sets mentioned in the previous section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Annotation</head><p>Image annotation was provided by a single member of our team, using the desktop application VoTT (see Figure <ref type="figure" coords="7,169.27,475.97,3.65,10.91" target="#fig_5">6</ref>). Each element was annotated using a rectangle shape, which covers the object in its entirety, regardless of potential overlap with other elements. Only the test and validation sets were manually annotated, in order to provide an accurate verification. The train set is using the position of the elements that has been retrieved from the websites, so some inaccuracy might be present.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3.">Data set Analysis</head><p>As the sections are much smaller than the full-page screenshots, the difference between the number of elements of the two types of data points is visible, which can be observed in Figure <ref type="figure" coords="7,89.04,606.59,3.81,10.91" target="#fig_6">7</ref>. Given the reduced number of elements inside sections, the detection of elements inside them may have a higher accuracy than on full-page screenshots. Because the data points were acquired from a list of popular websites, the number of elements per class mirrors their frequency in a web page design (see Figure <ref type="figure" coords="7,283.44,647.24,3.57,10.91" target="#fig_7">8</ref>).</p><p>The most common class is the text, which is the main way to provide information. This category includes HTML tags, such as paragraphs, labels or spans. Initially, the headings were included in the text class, but were then put in their own category to take into account their different usage. While the text is used to provide specific information, the headings are used to draw and direct the attention to a specific section or content. They are also designed in bigger fonts or different colors, and can be easily distinguished from the regular text.</p><p>The links are the second most common class, mainly because they are treated individually, and not as a group inside a given section. For example, a navbar can contain up to 10 links, while a footer can contain more than 20. The links are usually distinguishable by their position in the page, their font or their decorations. Moreover, if a link has been accessed before, the browser usually displays that link in a different style.</p><p>As images in a web page increase the number of views by up to 90%, it means that they are present in a consistent amount in the data set. As the other classes consist mainly of text in different styles or positions, the images can be easily differentiated from the rest, so certain ambiguities are diminished. The last 2 classes are the inputs and buttons, which are used in specific cases like in Forms or Call-To-Action sections of the websites.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Evaluation Methodology</head><p>Two methods are used to compute the scores for each run: mean average precision (mAP) and recall. Both require a minimum of 0.5 IoU overlap between the ground truth and the detection boxes to be taken into account by the evaluator. The algorithm used in this process is the Python API offered by COCO<ref type="foot" coords="9,185.39,357.61,3.71,7.97" target="#foot_3">4</ref> :</p><p>‚Ä¢ mAP@0.5 IoU: The localised mean average precision for each submission.</p><p>‚Ä¢ Recall@0.5 IoU: The localised mean recall for each submission,</p><p>The mean average precision is used as the primary metric for deciding the leader, while the recall provides a secondary metric in the case where two participants score equally in the former.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>Three teams submitted a total of 21 runs for the wireframe subtask and one team submitted 7 runs for the screenshot subtask. Each subtask had a submission limit of 10 runs per team. Table <ref type="table" coords="9,115.09,538.10,4.97,10.91">1</ref> and Table <ref type="table" coords="9,167.32,538.10,4.97,10.91" target="#tab_1">2</ref> display the mean average precision and recall at 0.5 IoU for each run of each task.</p><p>The baseline score was obtained by training a Faster R-CNN <ref type="bibr" coords="9,376.56,565.20,18.07,10.91" target="#b10">[11]</ref> model (with a resnet101 backbone), using the detectron2 API <ref type="bibr" coords="9,261.65,578.75,18.07,10.91" target="#b11">[12]</ref> on an Amazon Web Services EC2 instance. The instance was equipped with an nVidia K80 GPU, CUDA 10.0 and Python 3.6. The batch size was set to 2 and the number of steps to 100,000. Apart from the learning rate being set at 0.00025, all the other default options were kept.</p><p>For the wireframe task, team pwc started with a baseline score of 0.649 and enhanced it to 0.836 mAP@0.5 IoU using data augmentation and parameter optimization, placing them on the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Wireframe Subtask results: ùëÄ ùê¥ùëÉ 0.5ùêºùëúùëà and ùëÖ0.5ùêºùëúùëà for each run. The baselines and best values for each metric are in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team</head><p>Run id Method mAP@0.5 R@0.  second place. They improved the visual qualities of the data set using histogram equalisation, followed by conversion to black and white. Several object detection and segmentation models were considered, but were discarded based on their complexity (U-Net) or poor performance in detecting small objects (Mask-RCNN). Given its speed and flexibility, the authors chose YOLOv5 for their experiments. Their first five runs checked for the influence of pre-trained weights, model size, learning rate and layer freeze on the results, concluding that the best score was achived with the largest model (YOLOv5x) using pre-trained COCO weights and a learning rate scheduler. Finally, post-processing implied multi-pass inference and confidence cutoff variation. The former did not improve the results, while the latter offered marginal improvements. Team vyskocj increased their wireframe score from the 0.794 baseline to 0.900, while the screenshot one was improved from 0.594 to 0.628, placing the team on the first place in both challenges. DETR was considered as an object detector, but given the training time required, the authors opted for a Faster R-CNN architecture supplemented by a Feature Pyramid Network. For the screenshot task, a filtering algorithm was implemented to remove noisy data based on color similarity between bounding boxes. It offered marginal improvements. Augmentation via resizing was employed, the authors opting for Random Relative Resize, a data processor that prevents the more aggressive resizing which reduces small object detection accuracy. Cutout augmentation was also used by removing a number of objects at random from the training images. Anchor box aspect-ratio was modified in accordance with their frequency in the training set. For the backbone architecture, the smaller ResNet-50 converged faster and achieved better results in the Screenshot task. The larger variant, ResNeXt-101, showed better performance on the Wireframe task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and Conclusions</head><p>Compared to the highest score of the 2020 edition, the wireframe subtask mAP@0.5 IoU score improved from 0.79 to 0.90, indicating a significant improvement in model performance. The participants used state-of-the-art model architectures and diversified their modifications, showcasing unique ways in which the task came to be resolved. These changes brought this specific challenge closer to its full completion.</p><p>For the screenshot subtask, although the participation rate was very low, the best results are still higher than expected when compared to the baseline score. The 0.62 mAP score (the best run from team vyskocj) shows space for further improvement on this much harder task. The lower scoring derives from the increased level of complexity inherent in screenshots when compared to wireframes, as well as the relative "pollution" of the data set, which was left in as a challenge for the contestants. To tackle this, vyskocj produced a filtering algorithm for removing the noisy data points based on color similarity. They also demonstrated that the smaller architecture variant, ResNet50, converged faster and obtained better results for the same number of epochs compared to the bigger model, ResNeXt101.</p><p>Despite the low number of competitors, the results indicate an interest in bridging the gap between the visual representation of a website and its code, by using object detection tools. For further editions of the task, our aim is to continue developing the current data sets by making them more challenging from a technical, object detection perspective, as well as more attractive to designers or web-developers interested in tackling machine learning problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,258.76,271.33,9.96;2,126.31,84.18,340.15,162.90"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Website screenshot (left) and its bounding boxes (right).</figDesc><graphic coords="2,126.31,84.18,340.15,162.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,89.29,612.42,303.74,9.96;3,161.75,227.14,269.30,373.60"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Ground truth: the 21 visual representations for the UI elements.</figDesc><graphic coords="3,161.75,227.14,269.30,373.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,89.29,218.28,222.49,9.96;4,112.14,84.18,368.50,122.42"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Three data points of the wireframe data set.</figDesc><graphic coords="4,112.14,84.18,368.50,122.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,89.29,248.94,260.53,9.96;5,168.83,84.19,255.11,153.07"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Distribution of the number of UI elements per image.</figDesc><graphic coords="5,168.83,84.19,255.11,153.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,89.29,401.35,350.71,9.96;5,126.31,276.67,340.15,113.00"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The proportion of each class of UI elements in the training and testing sets.</figDesc><graphic coords="5,126.31,276.67,340.15,113.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="7,116.56,308.45,208.17,9.96;7,126.31,84.19,340.13,212.58"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Screenshot of an annotation using VoTT.</figDesc><graphic coords="7,126.31,84.19,340.13,212.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="8,89.29,367.99,306.94,9.96;8,126.31,84.19,340.16,272.13"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Distribution of the number of UI elements per screenshot image.</figDesc><graphic coords="8,126.31,84.19,340.16,272.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="9,89.29,307.33,300.58,9.96;9,126.31,84.19,340.15,211.46"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Tree map with the number of UI elements per screenshot class.</figDesc><graphic coords="9,126.31,84.19,340.15,211.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,88.99,362.81,416.99,120.32"><head>Table 2</head><label>2</label><figDesc>Screenshot Subtask results: ùëÄ ùê¥ùëÉ 0.5ùêºùëúùëà , and ùëÖ0.5ùêºùëúùëà for each run. The baselines and best values for each metric are in bold.</figDesc><table coords="10,133.19,399.76,328.89,83.38"><row><cell>Team</cell><cell>Run id</cell><cell>Method</cell><cell cols="2">mAP@0.5 Recall@0.5</cell></row><row><cell>vyskocj</cell><cell>134224</cell><cell>ResNet-50 (train+val, RGB)</cell><cell>0.628</cell><cell>0.830</cell></row><row><cell>vyskocj</cell><cell>134716</cell><cell>ResNet-50 (train+val, RGB, 2√ó epochs)</cell><cell>0.621</cell><cell>0.821</cell></row><row><cell>vyskocj</cell><cell>134215</cell><cell>ResNet-50 (anchor settings, RGB)</cell><cell>0.609</cell><cell>0.834</cell></row><row><cell>vyskocj</cell><cell>134214</cell><cell>ResNet-50 (augmentations, RGB)</cell><cell>0.602</cell><cell>0.822</cell></row><row><cell>vyskocj</cell><cell>134217</cell><cell>ResNet-50 (anchor settings, greyscale)</cell><cell>0.601</cell><cell>0.827</cell></row><row><cell>vyskocj</cell><cell>134207</cell><cell>ResNet-50 (baseline, RGB)</cell><cell>0.594</cell><cell>0.815</cell></row><row><cell>vyskocj</cell><cell>134603</cell><cell>ResNeXt-101 (RGB)</cell><cell>0.590</cell><cell>0.807</cell></row><row><cell>dimitri.fichou</cell><cell></cell><cell>Faster RCNN with resnet 101 backbone</cell><cell>0.329</cell><cell>0.408</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.93,660.08,151.01,8.97"><p>https://www.imageclef.org/2021/drawnui</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,108.93,671.04,114.13,8.97"><p>http://www.clef-campaign.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,108.93,671.02,131.39,8.97"><p>https://github.com/microsoft/VoTT/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="9,108.93,671.03,144.03,8.97"><p>https://github.com/cocodataset/cocoapi</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p><rs type="person">Mihai Dogariu</rs>, <rs type="person">Liviu-Daniel Stefan</rs>, <rs type="person">Mihai Gabriel Constantin</rs> and <rs type="person">Bogdan Ionescu</rs>'s contribution is supported under project <rs type="projectName">AI4Media</rs>, <rs type="programName">A European Excellence Centre for Media, Society and Democracy</rs>, <rs type="grantNumber">H2020 ICT-48-2020</rs>, grant #<rs type="grantNumber">951911</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_JJB9qMR">
					<idno type="grant-number">H2020 ICT-48-2020</idno>
					<orgName type="project" subtype="full">AI4Media</orgName>
					<orgName type="program" subtype="full">A European Excellence Centre for Media, Society and Democracy</orgName>
				</org>
				<org type="funding" xml:id="_PtycmVA">
					<idno type="grant-number">951911</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="11,112.66,655.77,393.70,10.91;11,112.66,669.32,393.33,10.91;12,112.66,86.97,394.52,10.91;12,112.66,100.52,213.33,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,474.40,655.77,31.97,10.91;11,112.66,669.32,268.32,10.91">Rico: A mobile app dataset for building data-driven design applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Franzen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hibschman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Afergan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="DOI">10.1145/3126594.3126651</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,401.85,669.32,104.14,10.91;12,112.66,86.97,363.70,10.91">UIST 2017 -Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="845" to="854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,114.06,393.33,10.91;12,111.79,127.61,395.49,10.91;12,112.66,141.16,337.56,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,271.68,114.06,203.86,10.91">Swire: Sketch-based User Interface Retrieval</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Canny</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Nichols</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300334</idno>
		<ptr target="https://doi.org/10.1145/3290605.3300334.doi:10.1145/3290605.3300334" />
	</analytic>
	<monogr>
		<title level="m" coord="12,487.37,114.06,18.62,10.91;12,111.79,127.61,11.83,10.91">CHI &apos;19</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,154.71,393.33,10.91;12,112.66,168.26,393.33,10.91;12,112.66,181.81,206.49,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,415.80,154.71,90.19,10.91;12,112.66,168.26,141.80,10.91">VINS: Visual Search for Mobile User Interface Design</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bunian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Jemmali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Harteveld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Seif El-Nasr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,277.62,168.26,228.37,10.91;12,112.66,181.81,133.56,10.91">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2021 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,195.36,394.53,10.91;12,112.66,208.91,393.33,10.91;12,112.66,222.46,234.69,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,180.02,195.36,322.42,10.91">pix2code : Generating Code from a Graphical User Interface Screenshot</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Beltramelli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07962v2</idno>
	</analytic>
	<monogr>
		<title level="m" coord="12,112.66,208.91,393.33,10.91;12,112.66,222.46,36.36,10.91">Proceedings of the ACM SIGCHI Symposium on Engineering Interactive Computing Systems</title>
		<meeting>the ACM SIGCHI Symposium on Engineering Interactive Computing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,236.01,393.33,10.91;12,112.66,249.56,393.33,10.91;12,112.66,263.11,150.71,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,288.97,236.01,217.01,10.91;12,112.66,249.56,272.00,10.91">From UI Design Image to GUI Skeleton : A Neural Machine Translator to Bootstrap Mobile GUI Implementation</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,393.48,249.56,112.51,10.91;12,112.66,263.11,110.99,10.91">International Conference on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,276.66,395.01,10.91;12,112.66,290.20,394.53,10.91;12,112.48,303.75,395.18,10.91;12,112.66,317.30,394.53,10.91;12,112.28,330.85,393.70,10.91;12,112.66,344.40,393.33,10.91;12,112.66,357.95,393.33,10.91;12,112.66,371.50,393.53,10.91;12,112.66,385.05,197.61,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,264.32,330.85,241.66,10.91;12,112.66,344.40,261.75,10.91">Overview of the ImageCLEF 2021: Multimedia retrieval in medical, nature, internet and social media applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Peteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tauteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>≈ûtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Moustahfid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,401.27,344.40,104.72,10.91;12,112.66,357.95,393.33,10.91;12,112.66,371.50,201.15,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 12th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="12,348.61,371.50,157.57,10.91;12,112.66,385.05,31.10,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,398.60,393.33,10.91;12,112.66,412.15,393.33,10.91;12,112.66,425.70,394.53,10.91;12,112.66,439.25,168.96,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,218.25,398.60,287.74,10.91;12,112.66,412.15,299.92,10.91">HTML Atomic UI Elements Extraction from Hand-Drawn Website Images using Mask-RCNN and novel Multi-Pass Inference Technique</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mohapatra</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="12,421.34,412.15,84.65,10.91;12,112.66,425.70,23.30,10.91">CLEF2020 Working Notes</title>
		<title level="s" coord="12,143.36,425.70,178.48,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,452.79,394.62,10.91;12,112.66,466.34,393.93,10.91;12,112.66,479.89,325.43,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,320.86,452.79,186.42,10.91;12,112.66,466.34,61.24,10.91">Deep Learning for UI Element Detection: DrawnUI 2020</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">N A</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Jaganathan</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="12,182.27,466.34,110.77,10.91">CLEF2020 Working Notes</title>
		<title level="s" coord="12,300.37,466.34,176.87,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020) (2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,493.44,393.33,10.91;12,112.66,506.99,395.01,10.91;12,112.66,520.54,345.06,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,221.57,493.44,284.42,10.91;12,112.66,506.99,55.32,10.91">Sketch2Code: Automatic hand-drawn UI Elements Detection with Faster-RCNN</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>R√≠ha</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="12,196.81,506.99,304.74,10.91">CLEF2020 Working Notes. CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020), 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,534.09,394.53,10.91;12,112.66,547.64,393.33,10.91;12,112.26,561.19,395.41,10.91;12,112.66,574.74,345.06,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,112.66,547.64,393.33,10.91;12,112.26,561.19,70.04,10.91">Overview of ImageCLEFdrawnUI 2020: the detection and recognition of hand drawn website UIs task</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>≈ûtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="12,204.90,561.19,296.82,10.91">CLEF2020 Working Notes. CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25 2020), 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,588.29,393.33,10.91;12,112.66,601.84,395.01,10.91;12,112.66,617.83,109.22,7.90" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,257.53,588.29,248.46,10.91;12,112.66,601.84,114.23,10.91">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01497v3</idno>
	</analytic>
	<monogr>
		<title level="m" coord="12,250.27,601.84,227.58,10.91">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,628.93,394.04,10.91;12,112.66,642.48,156.24,10.91" xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
	</analytic>
	<monogr>
		<title level="j" coord="12,361.15,628.93,48.12,10.91">Detectron</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
