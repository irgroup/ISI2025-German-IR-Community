<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,391.77,15.42;1,89.29,106.66,312.80,15.42;1,89.29,128.58,249.90,15.43">Classification of Tuberculosis Type on CT Scans of Lungs using a fusion of 2D and 3D Deep Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,156.89,100.20,11.96"><forename type="first">Emad</forename><surname>Aghajanzadeh</surname></persName>
							<email>emad.aghajanzadeh@mail.um.ac.ir</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Engineering Department</orgName>
								<orgName type="institution">Ferdowsi University of Mashhad</orgName>
								<address>
									<settlement>Mashhad</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,202.13,156.89,77.94,11.96"><forename type="first">Behzad</forename><surname>Shomali</surname></persName>
							<email>behzad.shomali@mail.um.ac.ir</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Engineering Department</orgName>
								<orgName type="institution">Ferdowsi University of Mashhad</orgName>
								<address>
									<settlement>Mashhad</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,292.71,156.89,88.96,11.96"><forename type="first">Diba</forename><surname>Aminshahidi</surname></persName>
							<email>d.aminshahidi@mail.um.ac.ir</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Engineering Department</orgName>
								<orgName type="institution">Ferdowsi University of Mashhad</orgName>
								<address>
									<settlement>Mashhad</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,412.67,156.89,79.48,11.96"><forename type="first">Navid</forename><surname>Ghassemi</surname></persName>
							<email>navidghassemi@mail.um.ac.ir</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Engineering Department</orgName>
								<orgName type="institution">Ferdowsi University of Mashhad</orgName>
								<address>
									<settlement>Mashhad</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,391.77,15.42;1,89.29,106.66,312.80,15.42;1,89.29,128.58,249.90,15.43">Classification of Tuberculosis Type on CT Scans of Lungs using a fusion of 2D and 3D Deep Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">9FA416FA0CFBFC400515C15D1979CCD0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Learning</term>
					<term>Information Fusion</term>
					<term>Tuberculosis</term>
					<term>CT Scan</term>
					<term>Diagnosis</term>
					<term>Volumetric Data</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a novel deep-learning-based method to deal with volumetric data like CT scans. The method ensembles a 2-dimensional convolutional neural network (2D-CNN) with a 3D-CNN followed by a recurrent neural network (RNN). We used this approach and its constituent to solve the task of categorizing tuberculosis type in the context of ImageCLEF 2021. Our best run ranked 4th based on the Kappa metric by reaching a value of 0.181 and 3rd based on the accuracy of 0.404. Also, it is worthy of mentioning that our obtained results were very similar to that of the third team with a Kappa of 0.190; and we had a big gap with the fifth team with a Kappa of 0.140.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Tuberculosis (TB) is an airborne disease that usually affects the lungs and causes severe coughing, chest pains, and fever. The disease is still one of the main health concerns worldwide, being second in causing high mortality rates <ref type="bibr" coords="1,258.73,417.64,11.28,10.91" target="#b0">[1]</ref>. Approximately 10.0 million people around the world caught TB in 2019 in line with WHO[World Health Organization. Global tuberculosis report 2020. Geneva, Switzerland: World Health Organization; 2020]. A CT Scan or Computerized Tomography Scan is a versatile medical imaging modality that uses computers and rotating X-ray machines to create cross-sectional images of the patient's body. In other words, the number of detector rows in the z-axis is increased. This allows us to image the whole organ, which reduces image capturing time. It also has several advantages, including improving the quality of images, reducing radiation exposure, and illustrating the soft tissues, blood vessels, and bones of the patient's body <ref type="bibr" coords="1,231.84,526.03,11.36,10.91" target="#b1">[2,</ref><ref type="bibr" coords="1,245.92,526.03,7.57,10.91" target="#b2">3]</ref>. Despite all the advantages, CT scan-based diagnosing approaches have some challenges in terms of the variety of images, their corresponding size, and the complexities there exist in the diagnosing process itself. Moreover, there exist some factors, namely, eye exhaustion and the great number of visitors, which lead to human mistakes <ref type="bibr" coords="1,361.79,580.23,11.58,10.91" target="#b3">[4]</ref>. These challenges motivated researchers to use Artificial Intelligence(AI) in order to create automated diagnosis systems for increasing the accuracy of medical diagnosis on CT-Scan <ref type="bibr" coords="2,341.99,86.97,11.29,10.91" target="#b4">[5,</ref><ref type="bibr" coords="2,356.00,86.97,7.52,10.91" target="#b5">6]</ref>. In recent years, Deep Learning (DL), a sub-field of AI, has shown encouraging results in medical diagnosis <ref type="bibr" coords="2,424.59,100.52,11.36,10.91" target="#b6">[7,</ref><ref type="bibr" coords="2,438.68,100.52,7.57,10.91" target="#b7">8]</ref>.</p><p>In this paper, we presented a strategy based on deep learning approaches to detect the type of TB disease, in the context of the ImageCLEF tuberculosis task <ref type="bibr" coords="2,392.35,127.61,11.48,10.91" target="#b8">[9,</ref><ref type="bibr" coords="2,407.39,127.61,12.42,10.91" target="#b9">10]</ref>. ImageCLEF 2021 is an evaluation campaign that is being organized as part of the CLEF initiative labs. The campaign offers several research tasks that welcome participation from teams around the world. In 2021, there were three medical subtasks, one of which was Tuberculosis CT analysis that we participated in. The task is to classify the CT scans into five classes based on their TB type. Besides the dataset, the organizers also provided two versions of extracted masks per each lung <ref type="bibr" coords="2,111.94,208.91,16.40,10.91" target="#b10">[11,</ref><ref type="bibr" coords="2,131.06,208.91,12.30,10.91" target="#b11">12]</ref>. We analyzed the effectiveness of three main approaches based on Convolutional Neural Network (CNN) <ref type="bibr" coords="2,194.52,222.46,17.84,10.91" target="#b12">[13]</ref> for this task. The first one is to use a 2 Dimensional convolutional Neural Network (2D-CNN) to learn slice-level features and then obtain the final prediction label through several strategies, such as majority voting or the most certain prediction. The second approach is to utilize a 3 Dimensional convolutional Neural Network (3D-CNN) to capture the spatial features, which are not extracted by the 2D-CNN. Finally, the last approach was to combine the 2D-CNN and 3D-CNN models to reach a model that benefits from both the slice-level and the inter-slice-level features.</p><p>The rest of the paper is organized as follows. Section 2 is devoted to describing the competition. Section 3 introduces the preprocessing steps that were used. Section 4 explains the proposed method, which obtained the results demonstrated in section 5. Finally, section 6 concludes the paper with future work directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">ImageClef Tuberculosis: task, data, evaluation</head><p>The tuberculosis task of ImageCLEF 2021 Challenge was categorizing each TB case based on its type into five categories: Infiltrative, Focal, Tuberculoma, Miliary, and Fibro-cavernous. Figure <ref type="figure" coords="2,501.27,430.13,4.97,10.91">1</ref> illustrates one example for each TB type <ref type="foot" coords="2,271.90,441.92,3.71,7.97" target="#foot_0">1</ref> . The dataset contains 1338 CT images stored in the NIfTI (Neuroimaging Informatics Technology Initiative) format with the resolution of 512 × 512 pixels and around 100 slices per scan. The file format stores raw voxel intensities in Hounsfield Units (HU). The training dataset consists of 917 CTs, each of which belongs to only one of the five classes. Hence, the task is a multi-class classification (see Table <ref type="table" coords="2,391.20,497.87,3.57,10.91" target="#tab_0">1</ref>). The results are evaluated using unweighted Cohen's Kappa <ref type="bibr" coords="3,370.02,332.12,18.01,10.91" target="#b13">[14]</ref> and accuracy metrics, but the primary ranking is done based on only the Kappa metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preprocessing</head><p>As the dataset files were provided in the NIfTI format with the extension .nii, we used the Nibabel package<ref type="foot" coords="3,164.20,415.93,3.71,7.97" target="#foot_1">2</ref> in Python<ref type="foot" coords="3,216.53,415.93,3.71,7.97" target="#foot_2">3</ref> to load the dataset. Following this, a threshold between -1000 and 400 is used to normalize the CT scans, HU values are scaled to be between 0 and 1. One of the major outcomes of this normalization is reducing the existing contrast among data (See Figure <ref type="figure" coords="3,121.25,458.33,3.65,10.91" target="#fig_0">2</ref>). The volumes are then rotated by 90 degrees so that their orientation is fixed. We did not use the masks provided by the task organizers. In most cases, the first and last slices do not contain beneficial features for a model to consider <ref type="bibr" coords="3,350.38,485.43,16.40,10.91" target="#b14">[15]</ref>, therefore we only selected 50 middle slices and removed the rest of them; this number was chosen empirically by testing a few alternatives. It is worth mentioning, this also saves the power of computational resources; thus helping us to search through different models and settings more efficiently. Moreover, for the same reason, we resized each slice to 100*100, so finally, we had a set of 100*100*50 CT scans. Figure <ref type="figure" coords="3,150.02,553.18,5.07,10.91" target="#fig_1">3</ref> shows the slices of a CT after the preprocessing phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Method</head><p>In recent years, Deep Neural Networks (DNN) have shown great performance in various tasks, and medical diagnosis has not been an exception <ref type="bibr" coords="3,317.04,625.20,16.56,10.91" target="#b15">[16,</ref><ref type="bibr" coords="3,336.82,625.20,12.42,10.91" target="#b16">17]</ref>. More specifically, Convolutional Neural Networks (CNN), a well-known deep learning architecture inspired by the mechanism  of visual perception of creatures, have been used to solve many image processing tasks. It takes its name from Convolution, a mathematical operation, which performs mapping on input data and processes them into a new space. The main advantage of using CNN is that the kernel can automatically extract the important features from the input data such as detecting edges and distribution of colors in an image which other networks are unable to do, thus making these networks very robust in some processes like image classification. However, despite all aforementioned advantages, CNN models are data-hungry <ref type="bibr" coords="4,371.20,659.56,16.41,10.91" target="#b17">[18]</ref>, making them less useful, when there are not enough data available like in medical tasks. Moreover, there are even more challenges to face to train these models properly; such as unbalanced <ref type="bibr" coords="5,399.85,100.52,16.25,10.91" target="#b18">[19]</ref>.</p><p>In our attempt to use the CNN to categorize the CT images, we remedied the mentioned problems as follows:</p><p>• Small Dataset: We used data duplication and also data augmentation techniques to increase our training data. For augmenting the training data, a degree between -5 to 5 was randomly chosen to apply rotation on each data. Later, we zoomed each data by a ratio of 1.25 and then resized it to its original size; by putting resizing at the last step, we ensured that image quality is kept during data augmentation. • Imbalanced Data: As shown in Table <ref type="table" coords="5,281.34,216.08,3.71,10.91" target="#tab_0">1</ref>, the number of samples in different classes varies dramatically. To overcome this issue, two distinct approaches were used: the first one is to consider variable penalties for different classes, that is, the multiplier error in classes with fewer data becomes larger. The second approach is to remove the samples from the classes with more data. For this purpose, we removed plenty of samples from classes 0 and 1 from the dataset.</p><p>By doing these steps, the training dataset became larger and more balanced (see Table <ref type="table" coords="5,467.71,304.29,3.50,10.91" target="#tab_1">2</ref>). In the proposed method, we mainly used two different approaches, 2D-CNN-based and 3D-CNN-based, where their training settings are as follows:</p><p>• Optimizer: Adam optimizer with default parameters (alpha=0.001, beta1=0.9, beta2=0.999, epsilon=1e-7) <ref type="bibr" coords="5,180.76,504.70,18.07,10.91" target="#b19">[20]</ref> was used for all epochs of 2D models and the first 30 epochs of 3D models. For the rest epochs of 3D models, we decayed the learning rate by a 1/2 ratio. • Train/validation split: The dataset was split into train and validation partitions with a ratio of 0.2. • Batch size: Due to facing some problems with memory, we had to keep the batch size small; thus we set the batch sizes 32 and 8 for 2D and 3D models, respectively. • Loss function: A combination of cross-entropy and weighted Kappa <ref type="bibr" coords="5,415.28,587.99,17.76,10.91" target="#b20">[21]</ref> with multipliers of 0.7 and 0.3 was used for all epochs of 2D models and 30 initial epochs of 3D models. The contribution ratio of losses was then changed into 0.85 and 0.15 for the 10 last epochs of 3D models. The cross-entropy loss is defined as:</p><formula xml:id="formula_0" coords="5,259.18,649.98,246.80,33.71">𝐿 𝐶𝐸 = - 𝐶 ∑︁ 𝑖=1 𝑡 𝑖 log(𝑝 𝑖 )<label>(1)</label></formula><p>Where C is the number of classes, 𝑡 𝑖 is the ground truth, and 𝑝 𝑖 is the probability for the 𝑖-th class. The formula of weighted kappa with the matrix of observed scores 𝑂, the matrix of expected scores based on chance agreement 𝐸, and the weight matrix 𝜔 is defined as follows:</p><formula xml:id="formula_1" coords="6,218.05,147.44,287.94,29.90">𝜅 = 1 - ∑︀ 𝑖,𝑗 𝜔 𝑖,𝑗 𝑂 𝑖,𝑗 ∑︀ 𝑖,𝑗 𝜔 𝑖,𝑗 𝐸 𝑖,𝑗 ∀𝑖, 𝑗 ∈ {1, 2, ..., 𝐶}<label>(2)</label></formula><p>where 𝑂 𝑖,𝑗 is the number of observations that are predicted to be in class 𝑖, but their true classes were 𝑗. 𝐸 𝑖,𝑗 also denotes the outer product between the vectors of prediction and true value. Finally, 𝜔 𝑖,𝑗 represents the weight penalization for every pair 𝑖, 𝑗.</p><p>It is worthy of mentioning that all of our experiments were done using Google Colab <ref type="bibr" coords="6,470.19,238.38,16.25,10.91" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">2D</head><p>In this method, we examined each slice of a single CT individually. To be more specific, in the training phase, we assigned the label of each CT to all of its corresponding slices and fed each slice to the 2D-CNN separately. In this case, we have a vector of 50, the number of slices, predicted labels as output for each CT. To obtain the final prediction for each CT in the testing phase, we have used two different approaches:</p><p>• Pick the label which appeared the most (majority voting)</p><p>• Pick the label whose corresponding probability was the highest, i.e., where the model is highly certain about that label.</p><p>To configure hyper-parameters, we examined various settings such as using skip connection, changing the number of neurons of the last hidden layer, using different activation functions, and differing kernel size of convolution layers and selected our final network empirically. Finally, we got the best result from the model shown in Figure <ref type="figure" coords="6,333.66,456.44,3.74,10.91" target="#fig_2">4</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">2D + RNN</head><p>In this method, we used the best-trained 2D-CNN that we found in the previous section, but in order to have a more accurate classifier, we implemented a simple Recurrent Neural Network (RNN). As illustrated in Figure <ref type="figure" coords="7,225.98,385.54,3.70,10.91">6</ref>, we used the extracted features of the 2D-CNN as input of the RNN. To be more clear, we tried two different approaches to fulfill this:</p><p>• Feeding the output of the 2D CNN (vector of 50 labels) to the RNN (see 6a)</p><p>• Feeding the features extracted by the last hidden layer of 2D CNN to the RNN (see 6b)</p><p>We tried different settings for the RNN such as trying different architectures, including long short-term memory (LSTM) <ref type="bibr" coords="7,213.49,470.94,17.76,10.91" target="#b22">[23]</ref> and gated recurrent unit (GRU) <ref type="bibr" coords="7,372.85,470.94,16.09,10.91" target="#b23">[24]</ref>, besides, different number of units. Despite all of the efforts, the accuracy obtained in this method was almost in the same range of 2D-CNN and neither was superior to another one. Therefore, we decided not to submit the result of this approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">3D</head><p>Generally, 2D-CNN are unable to catch information that exists among slices, i.e., spatial information. This is because they take a single slice as input and the learning process is applied on each slice individually, thus some of the spatial information may be lost in the process. However, the input of 3D-CNN is a 3D matrix with dimensions of height, width, and depth and the kernel slides over these three dimensions. This property of 3D-CNNs enables them to capture the spatial information between slices. For this reason, we used a 3D model consists of 5 convolution layers as shown in Figure <ref type="figure" coords="7,268.61,642.48,5.05,10.91" target="#fig_6">7</ref> and its corresponding learning curve is displayed in Figure <ref type="figure" coords="7,120.36,656.03,3.74,10.91" target="#fig_7">8</ref>.</p><p>The evaluation result of the model is also listed in Table    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">3D + Transfer learning</head><p>In order to make use of pre-trained networks, we designed a model that transforms the input images into three-channel images using convolution layers and then followed by the pre-trained models. In this experiment, we used ResNet <ref type="bibr" coords="9,291.66,185.84,16.41,10.91" target="#b24">[25]</ref>, VGG16 <ref type="bibr" coords="9,350.48,185.84,18.06,10.91" target="#b25">[26]</ref> and EfficientNet <ref type="bibr" coords="9,447.95,185.84,16.41,10.91" target="#b26">[27]</ref>, none of which obtained better result than what was obtained through the 3D model itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Fusion of 2D and 3D with RNN</head><p>3D CNN can capture the spatial information among CT slices, while 2D CNN can better extract 2D features in each slice. We assumed that ensembling these two models can result in a model with both advantages. Therefore, we first put the features of all slices together and then concatenate them with the features obtained by the 3D CNN model. This forms a feature vector for each CT image, which is then passed to the RNN model as Figure <ref type="figure" coords="9,397.06,303.31,3.73,10.91" target="#fig_8">9</ref>. The result was similar to that of the 3D model, which implies that the contribution of the 3D model in the learning process is more dominant than the other model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Fusion of 2D and 3D to use the best of both</head><p>After investigating the confusion matrices obtained from the 3D and 2D models, we noticed that the 3D model can better separate the last 3 classes, while can not properly categorize the first two ones. However, this pattern was completely reversed for the 2D model. Therefore, we decided to select the final prediction of classes 1 and 2 on test data manually from the prediction of both models. The result was similar to the 3D model which shows that the 2D model did not help the 3D model in the prediction of classes 1 and 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Comparative results</head><p>Table <ref type="table" coords="10,116.30,111.28,5.17,10.91" target="#tab_4">5</ref> shows the results obtained on the test data of the competition. As it can be seen, the 2D-CNN model obtained 0.036 and 0.373 scores based on Kappa and accuracy score, which is the worst score on Kappa and second-best on accuracy metric among all our submissions. Then, the Kappa is increased by 0.02 when the RNN module is added on top of the 2D-CNN; meanwhile, the accuracy score is decreased by 0.031. Furthermore, the 3D-CNN model obtained 0.181 and 0.404 on Kappa and accuracy, respectively, which is our best score on both metrics. This result is then decreased to 0.136 and 0.371 by adding the 2D features and manually selecting the final prediction when 2 models had conflicts in the first two classes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Works</head><p>In this paper, we have described our proposed method for the tuberculosis task of ImageCLEF Tuberculosis 2021. We proposed three different approaches and analyzed their corresponding results. The results demonstrate that the 2D-CNN didn't work well, while we believe that it can be significantly improved by applying a smarter voting mechanism for outputting the final label. These improvements can be such as applying Gaussian Distribution Normalization or defining a window with a fixed size, k, to move through the vector of 50 labels and pick the final label. By having a better 2D-CNN in hand, we can expect some enhancements while ensembling 2D and 3D-CNN models. Moreover, comparing the obtained results on validation data, Table <ref type="table" coords="10,482.13,474.41,4.99,10.91" target="#tab_2">3</ref> and Table <ref type="table" coords="10,114.93,487.96,3.66,10.91" target="#tab_3">4</ref>, with the results on test data, Table <ref type="table" coords="10,275.50,487.96,3.66,10.91" target="#tab_4">5</ref>, shows that there exists a considerable gap between them, which can be caused by the existence of different distributions between validation and test datasets. To resolve this issue, we suggest exchanging the order of duplicating and splitting the data. This guarantees that there is no overlap between training and validation data. We also plan to employ more complicated approaches for data augmentation; in this case, the models can learn and generalize more robustly. During our experiments, we also figured out that the main part of the incorrect predictions of models caused by predicting the first two classes interchangeably. By having this in mind, we can fix this problem by training a separate binary classification on those classes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,274.49,257.62,8.93;4,199.84,181.52,195.60,80.40"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Examples of slices with absolutely different contrast.</figDesc><graphic coords="4,199.84,181.52,195.60,80.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.29,540.66,308.22,8.93;4,89.31,301.34,416.66,226.76"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of 50 selected slices of a CT scan, after preprocessing.</figDesc><graphic coords="4,89.31,301.34,416.66,226.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,89.29,605.10,140.29,8.93;6,89.29,479.15,416.70,113.39"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of 2D model</figDesc><graphic coords="6,89.29,479.15,416.70,113.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,89.29,210.14,182.67,8.93;7,212.60,84.19,170.07,113.38"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Learning curve of 2D-CNN model.</figDesc><graphic coords="7,212.60,84.19,170.07,113.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,351.71,669.58,5.07,10.91"><head>4</head><label>4</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="8,89.29,346.06,268.33,8.93;8,139.66,221.57,315.96,102.12"><head>( a )Figure 6 :</head><label>a6</label><figDesc>Figure 6: Two types of employing RNN model after the 2D CNN.</figDesc><graphic coords="8,139.66,221.57,315.96,102.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="8,89.29,442.18,180.50,8.93;8,89.29,372.92,416.70,56.69"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Illustration of proposed 3D model</figDesc><graphic coords="8,89.29,372.92,416.70,56.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="8,89.29,594.98,182.67,8.93;8,212.60,469.03,170.07,113.38"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Learning curve of 3D-CNN model.</figDesc><graphic coords="8,212.60,469.03,170.07,113.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="9,89.29,521.60,200.28,8.93;9,89.30,353.12,416.69,155.91"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Illustration of fusion 2D and 3D model</figDesc><graphic coords="9,89.30,353.12,416.69,155.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="3,160.14,84.18,274.99,198.43"><head></head><label></label><figDesc></figDesc><graphic coords="3,160.14,84.18,274.99,198.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,88.99,526.51,272.82,109.48"><head>Table 1</head><label>1</label><figDesc>The number of training samples for each of the five TB types. Examples of the five types of TB.</figDesc><table coords="2,233.46,554.60,128.36,81.39"><row><cell>Type</cell><cell># of samples</cell></row><row><cell>Infiltrative</cell><cell>419</cell></row><row><cell>Focal</cell><cell>226</cell></row><row><cell>Tuberculoma</cell><cell>101</cell></row><row><cell>Miliary</cell><cell>101</cell></row><row><cell cols="2">Fibro-cavernous 70</cell></row><row><cell>total</cell><cell>917</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,88.99,332.48,272.82,109.48"><head>Table 2</head><label>2</label><figDesc>The number of training samples after the preprocessing step.</figDesc><table coords="5,233.46,360.57,128.36,81.39"><row><cell>Type</cell><cell># of samples</cell></row><row><cell>Infiltrative</cell><cell>376</cell></row><row><cell>Focal</cell><cell>262</cell></row><row><cell>Tuberculoma</cell><cell>342</cell></row><row><cell>Miliary</cell><cell>250</cell></row><row><cell cols="2">Fibro-cavernous 250</cell></row><row><cell>total</cell><cell>1480</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,88.99,243.03,244.60,71.00"><head>Table 3</head><label>3</label><figDesc>Evaluation results on 2D-CNN model.</figDesc><table coords="7,261.68,268.90,71.91,45.13"><row><cell>Criteria</cell><cell>Score</cell></row><row><cell cols="2">Accuracy 0.614</cell></row><row><cell>Kappa</cell><cell>0.509</cell></row><row><cell>F1-score</cell><cell>0.618</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,88.99,90.49,244.60,71.00"><head>Table 4</head><label>4</label><figDesc>Evaluation results on 3D-CNN model.</figDesc><table coords="9,261.68,116.35,71.91,45.13"><row><cell>Criteria</cell><cell>Score</cell></row><row><cell cols="2">Accuracy 0.646</cell></row><row><cell>Kappa</cell><cell>0.547</cell></row><row><cell>F1-score</cell><cell>0.656</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,88.99,235.13,281.78,82.95"><head>Table 5</head><label>5</label><figDesc>Results obtained on the test data.</figDesc><table coords="10,224.50,261.00,146.27,57.09"><row><cell>Method</cell><cell>Kappa</cell><cell>Acc</cell></row><row><cell>2D</cell><cell>0.036</cell><cell>0.373</cell></row><row><cell>2D + RNN</cell><cell>0.056</cell><cell>0.342</cell></row><row><cell>3D</cell><cell>0.181</cell><cell>0.404</cell></row><row><cell cols="2">3D + 2D + Manual 0.136</cell><cell>0.371</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.93,670.99,195.90,8.97"><p>https://www.imageclef.org/2021/medical/tuberculosis</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,108.93,660.08,116.52,8.97"><p>https://github.com/nipy/nibabel</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,108.93,671.04,96.68,8.97"><p>https://github.com/python</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,112.66,654.98,381.53,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,155.74,654.98,189.37,10.91">Tuberculosis: a disease without boundaries</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Fogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,353.74,654.98,56.52,10.91">Tuberculosis</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="527" to="531" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,86.97,393.33,10.91;11,112.66,100.52,393.61,10.91;11,112.66,114.06,167.85,10.91" xml:id="b1">
	<analytic>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-D</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,482.67,86.97,23.32,10.91;11,112.66,100.52,393.61,10.91;11,112.66,114.06,87.99,10.91">Deep learning-based recognizing covid-19 and other common infectious diseases of the lung by chest ct scan images</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,127.61,393.33,10.91;11,112.66,141.16,393.32,10.91;11,112.33,154.71,68.33,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,373.42,127.61,132.57,10.91;11,112.66,141.16,316.35,10.91">Ct angiography with volume rendering: advantages and applications in splanchnic vascular imaging</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">T</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Heath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">S</forename><surname>Kuszyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">K</forename><surname>Fishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,442.21,141.16,45.58,10.91">Radiology</title>
		<imprint>
			<biblScope unit="volume">200</biblScope>
			<biblScope unit="page" from="564" to="568" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,168.26,394.53,10.91;11,112.66,181.81,393.33,10.91;11,112.66,195.36,236.18,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,140.06,181.81,365.92,10.91;11,112.66,195.36,63.79,10.91">Weakly supervised deep learning for covid-19 infection detection and classification from ct images</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Menpes-Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,185.17,195.36,54.37,10.91">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="118869" to="118883" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,208.91,393.33,10.91;11,112.66,222.46,227.76,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,250.38,208.91,214.00,10.91">Current methods in medical image segmentation</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">L</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,473.08,208.91,32.90,10.91;11,112.66,222.46,148.90,10.91">Annual review of biomedical engineering</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="315" to="337" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,236.01,395.17,10.91;11,112.66,249.56,394.53,10.91;11,112.66,263.11,249.69,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,491.49,236.01,16.33,10.91;11,112.66,249.56,389.79,10.91">Computer-aided diagnosis systems for lung cancer: challenges and methodologies</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>El-Baz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Beache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gimel'farb</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Okada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Elnakib</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Soliman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,112.66,263.11,194.75,10.91">International journal of biomedical imaging</title>
		<imprint>
			<biblScope unit="page">2013</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Abdollahi</note>
</biblStruct>

<biblStruct coords="11,112.66,276.66,394.53,10.91;11,112.66,290.20,393.33,10.91;11,112.66,303.75,393.32,10.91;11,112.66,317.30,119.73,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,352.04,290.20,153.95,10.91;11,112.66,303.75,296.09,10.91">Deep-learning framework to detect lung abnormality-a study with chest x-ray and lung ct scan images</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bhandary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">A</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Rajinikanth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">P</forename><surname>Thanaraj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">C</forename><surname>Satapathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Shasky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M R</forename><surname>Tavares</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">S M</forename><surname>Raja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,417.47,303.75,88.51,10.91;11,112.66,317.30,30.72,10.91">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="271" to="278" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,330.85,393.33,10.91;11,112.66,344.40,393.33,10.91;11,112.66,357.95,238.97,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,248.48,330.85,257.51,10.91;11,112.66,344.40,360.45,10.91">Combining generative and discriminative representation learning for lung ct analysis with convolutional restricted boltzmann machines</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Van Tulder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,484.09,344.40,21.90,10.91;11,112.66,357.95,144.89,10.91">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1262" to="1272" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,371.50,395.17,10.91;11,112.66,385.05,394.53,10.91;11,112.66,398.60,394.53,10.91;11,112.66,412.15,22.69,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,393.45,371.50,114.38,10.91;11,112.66,385.05,257.62,10.91">Overview of ImageCLEFtuberculosis 2021 -CT-based tuberculosis type classification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="11,392.48,385.05,110.11,10.91">CLEF2021 Working Notes</title>
		<title level="s" coord="11,112.66,398.60,181.84,10.91">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,425.70,395.01,10.91;11,112.66,439.25,394.53,10.91;11,112.48,452.79,395.18,10.91;11,112.66,466.34,394.53,10.91;11,112.28,479.89,393.70,10.91;11,112.66,493.44,393.33,10.91;11,112.66,506.99,393.33,10.91;11,112.66,520.54,393.53,10.91;11,112.66,534.09,197.61,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,264.32,479.89,241.66,10.91;11,112.66,493.44,261.75,10.91">Overview of the ImageCLEF 2021: Multimedia retrieval in medical, nature, internet and social media applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Peteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tauteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Moustahfid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,401.27,493.44,104.72,10.91;11,112.66,506.99,393.33,10.91;11,112.66,520.54,201.15,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 12th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="11,348.61,520.54,157.57,10.91;11,112.66,534.09,31.10,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,547.64,393.33,10.91;11,112.66,561.19,394.53,10.91;11,112.66,574.74,263.63,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,223.08,547.64,282.91,10.91;11,112.66,561.19,100.42,10.91">Imageclef 2017: Supervoxels and co-occurrence for tuberculosis ct image classification</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="11,242.30,561.19,116.63,10.91">CLEF2017 Working Notes</title>
		<title level="s" coord="11,367.79,561.19,139.40,10.91;11,112.66,574.74,45.79,10.91">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,588.29,393.60,10.91;11,112.66,601.84,393.33,10.91;11,112.33,615.39,393.94,10.91;11,112.66,628.93,393.33,10.91;11,112.66,642.48,173.47,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,424.12,588.29,82.14,10.91;11,112.66,601.84,232.90,10.91">Efficient and fully automatic segmentation of the lungs in ct volumes</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">Dicente</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">A</forename><surname>Jiménez Del Toro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Depeursinge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="11,330.31,615.39,175.96,10.91;11,112.66,628.93,178.39,10.91">Proceedings of the VISCERAL Anatomy Grand Challenge at the 2015 IEEE ISBI</title>
		<title level="s" coord="11,298.87,628.93,166.23,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">O</forename><surname>Goksel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Jiménez Del Toro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Foncubierta-Rodríguez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<meeting>the VISCERAL Anatomy Grand Challenge at the 2015 IEEE ISBI</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,656.03,393.33,10.91;11,112.66,669.58,395.01,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,295.99,656.03,209.99,10.91;11,112.66,669.58,68.98,10.91">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,190.20,669.58,225.40,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,86.97,393.32,10.91;12,112.66,100.52,134.02,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,157.43,86.97,202.95,10.91">A coefficient of agreement for nominal scales</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,368.40,86.97,137.58,10.91;12,112.66,100.52,60.23,10.91">Educational and psychological measurement</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="37" to="46" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,114.06,393.32,10.91;12,112.66,127.61,394.53,10.91;12,112.66,141.16,171.98,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,269.32,114.06,236.66,10.91;12,112.66,127.61,390.56,10.91">Statistical analysis for obtaining optimum number of ct scanners in patient dose surveys for determining national diagnostic reference levels</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sohrabi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Parsi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Tabrizi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,112.66,141.16,88.05,10.91">European radiology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="168" to="175" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,154.71,394.53,10.91;12,112.66,168.26,393.33,10.91;12,112.66,181.81,393.33,10.91;12,112.33,195.36,29.19,10.91" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Khodatars</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shoeibi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khadem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Moridian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Alizadehsani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zare</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01285</idno>
		<title level="m" coord="12,295.68,168.26,210.31,10.91;12,112.66,181.81,245.88,10.91">Deep learning for neuroimaging-based diagnosis and rehabilitation of autism spectrum disorder: A review</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,208.91,394.53,10.91;12,112.66,222.46,393.32,10.91;12,112.66,236.01,364.46,10.91" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shoeibi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Khodatars</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Alizadehsani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Moridian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khadem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zare</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10785</idno>
		<title level="m" coord="12,285.33,222.46,220.65,10.91;12,112.66,236.01,182.19,10.91">Automated detection and forecasting of covid-19 using deep learning techniques: A review</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,249.56,384.62,10.91" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Marcus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00631</idno>
		<title level="m" coord="12,164.41,249.56,150.87,10.91">Deep learning: A critical appraisal</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,263.11,393.33,10.91;12,112.66,276.66,330.05,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,255.97,263.11,185.21,10.91">Classification of imbalanced data: A review</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Kamel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,449.04,263.11,56.95,10.91;12,112.66,276.66,246.12,10.91">International journal of pattern recognition and artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="687" to="719" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,290.20,393.33,10.91;12,112.66,303.75,102.10,10.91" xml:id="b19">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m" coord="12,251.96,290.20,172.98,10.91">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,317.30,393.33,10.91;12,112.66,330.85,357.89,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="12,252.16,317.30,253.82,10.91;12,112.66,330.85,138.11,10.91">Weighted kappa loss function for multi-class classification of ordinal data in deep learning</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Valls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,259.08,330.85,122.47,10.91">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="144" to="154" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,344.40,393.33,10.91;12,112.66,357.95,235.30,10.91" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="12,159.13,344.40,86.58,10.91;12,267.87,344.40,238.12,10.91;12,112.66,357.95,114.80,10.91">Building Machine Learning and Deep Learning Models on Google Cloud Platform</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Bisong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="59" to="64" />
		</imprint>
	</monogr>
	<note>Google colaboratory</note>
</biblStruct>

<biblStruct coords="12,112.66,371.50,393.98,10.91;12,112.41,385.05,48.96,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="12,253.72,371.50,112.17,10.91">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,376.65,371.50,91.46,10.91">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,398.60,395.16,10.91;12,112.66,412.15,393.32,10.91;12,112.66,425.70,223.38,10.91" xml:id="b23">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<title level="m" coord="12,133.03,412.15,372.95,10.91;12,112.66,425.70,46.51,10.91">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,439.25,395.17,10.91;12,112.66,452.79,395.01,10.91;12,112.41,466.34,38.81,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="12,259.74,439.25,203.38,10.91">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,488.38,439.25,19.45,10.91;12,112.66,452.79,347.24,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,479.89,393.33,10.91;12,112.66,493.44,226.28,10.91" xml:id="b25">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m" coord="12,247.99,479.89,258.00,10.91;12,112.66,493.44,49.16,10.91">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,506.99,394.53,10.91;12,112.66,520.54,352.61,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="12,178.42,506.99,323.86,10.91">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,127.29,520.54,207.49,10.91">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
