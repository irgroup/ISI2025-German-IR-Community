<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,388.72,15.42;1,89.29,106.66,333.53,15.42;1,89.29,128.58,112.98,15.43">Lijie at ImageCLEFmed VQA-Med 2021: Attention Model-based Efficient Interaction between Multimodality</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.10,156.89,24.97,11.96"><forename type="first">Jie</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Yunnan University</orgName>
								<address>
									<postCode>650091</postCode>
									<settlement>Kunming</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,126.72,156.89,67.02,11.96"><forename type="first">Shengyan</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">CSIC</orgName>
								<address>
									<addrLine>750 proving ground</addrLine>
									<postCode>650216</postCode>
									<settlement>Yunnan Province, Kunming</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,388.72,15.42;1,89.29,106.66,333.53,15.42;1,89.29,128.58,112.98,15.43">Lijie at ImageCLEFmed VQA-Med 2021: Attention Model-based Efficient Interaction between Multimodality</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">3FDC77753F060EF6BD151AE6A3B88195</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multi-modal Factorized High-order Pooling</term>
					<term>BioBERT</term>
					<term>Co-attention</term>
					<term>Visual Question Answering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe the visual question answering (VQA medicine) task in the medical domain that we submitted on the ImageCLEF 2021 challenge. In terms of semantic feature extraction of question text, we use a more efficient method than BERT, which is processed through the pre-trained BioBERT model on the biomedical data set. Then the image and text features are merged and effectively interacted between multimodality through a more efficient MFH (High-order pooling) and co-attention than MFB (Multimodal factorized bilinear pooling), then we concatenate the various image features from the problem attention. Finally, the text features after multimodal interaction are mapped to the image feature vector space for the second fusion. In this way, the result is obtained by sending it to the fully connected layer and Softmax layer output after two effective fusions. In the ImageCLEF 2021 task, the overall_accuracy of our model is 0.316 and the BLEU is 0.352, ranking sixth among all participating teams this time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, artificial intelligence technology (AI) <ref type="bibr" coords="1,331.36,447.33,12.89,10.91" target="#b0">[1]</ref> has become more and more mature, especially the rapid development of CV (computer vision) and NLP (natural language processing), so that some difficult tasks have been mentioned again, and it has also collided with all walks of life and produced fierce sparks, and gradually penetrated our daily lives. With the advancement of deep learning <ref type="bibr" coords="1,166.65,501.52,12.78,10.91" target="#b1">[2]</ref> algorithms and big data computing power, a medical revolution triggered by artificial intelligence has come quietly. VQA-Med (Visual Question Answering in Medical Domain) is one of the most attractive tasks. That is to say, for many diseases, viewing and analyzing medical images (CT, MRI, Ultrasound) will undoubtedly allow the doctor to inquire about the patient's physical condition clearly and intuitively than asking the patient's feelings. The same is true for the intelligent diagnosis and treatment system. If the questions raised by the patient and the medical images provided by the patient can be combined, it can answer the questions that the patient wants to know more accurately, and even answer some more complex medical questions.</p><p>For clinicians, the medical visual question answering system can enhance their confidence in diagnosing the patient's condition. For patients, the medical visual question answering system can save them a lot of time and money. Instead of having to search for some unverified information on the Internet to understand their condition, patients can learn more accurately what they want to know.</p><p>The concept of Visual Question Answering (VQA) first appeared in 2014. VQA also began to develop gradually, then the 2018 ImageCLEF competition first proposed the VQA-Med task <ref type="bibr" coords="2,492.06,181.81,11.34,10.91" target="#b2">[3]</ref>, and the VQA-Med task of the ImageCLEF competition is still open to university research teams every year and provides corresponding data sets, attracting the participation of a large number of researchers. In 2018, the task was mainly to answer questions about abnormal medical images. There were not many groups that participated at that time, so there were only 5 groups and the method was relatively simple. In addition, the 2018 VQA-Med task is automatically generated from the image caption before being manually checked by a human annotator. The questions and basic answers are variable-length and free-form, which increases the difficulty of answer generation. In VQA-Med in 2019, the classification task is more clarified, using only radiographic images and asking questions from four aspects: image modalities, imaging planes, visual organ systems, and abnormalities that can be detected in images. In the 2020 task, the data set only contains questions about whether or not and what kind of questions. It continues until this year's 2021 VQA-Med competition has not changed to try to get better results.</p><p>For the VQA-Med task in ImageCLEF in 2021 <ref type="bibr" coords="2,312.48,357.95,11.58,10.91" target="#b3">[4]</ref>, Our model is modified by referring to the combined model of MFB and Co-attention structure proposed by Zhou <ref type="bibr" coords="2,430.26,371.50,12.99,10.91" target="#b4">[5]</ref> and others in ICCV 2017 which applied to general VQA tasks. Specific steps are as follows: 1. For question text extraction, we use Bio-BERT's pre-trained model on the medical data set to process. 2. For image processing, we use vgg8 <ref type="bibr" coords="2,249.57,412.15,13.00,10.91" target="#b5">[6]</ref> for processing, but there is no such complex network as ResNet152 to avoid problems such as excessive training parameters, running delays, and overfitting. 3. MFH is used for efficient fusion during fusion, and the co-attention mechanism is introduced during fusion to improve the effect.</p><p>The other parts of this paper are organized as follows. The second section briefly describes the literature review on VQA and VQA-Med. The third section introduces the VQA-Med task and the detailed analysis of its data set. In the fourth part, we introduce the specific methods and principles we used. In the fifth section, we introduce the model and specific steps we used in the experiment. The sixth part introduces the results we submitted. Finally, the paper summarizes and prospects in the seventh part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>For the development of VQA tasks in the general field, in 2014, Malinowski et al. <ref type="bibr" coords="2,455.39,592.72,12.98,10.91" target="#b6">[7]</ref> initially proposed the concept of "open-world" for visual question and answer, and designed a Bayesian model frame model that combines image semantic scene segmentation and text symbol reasoning two methods to realize automatic question and answer of natural language questions. Also using the Bayesian structure model framework is the view put forward by Kushal et al. <ref type="bibr" coords="2,491.74,646.91,11.58,10.91" target="#b7">[8]</ref>, which transforms open questions into multi-classification problems, such as converting the question "What color is this cat?" into the type of color recognition problem. In fact, most of the initial approach is to use the CNN-RNN framework to process image and text features separately, and almost all images are processed by CNN convolution, and text is processed by RNN, and then the fusion between multi-modality uses the factorization of bilinear pooling, such as MCB (multimodal compact bilinear pooling) <ref type="bibr" coords="3,317.45,141.16,12.69,10.91" target="#b8">[9]</ref> and MLB (Multimodal low-rank bilinear pools) <ref type="bibr" coords="3,119.54,154.71,16.35,10.91" target="#b9">[10]</ref>, or more advanced MFB (Multimodal factorized bilinear pooling) and MFH (Highorder pooling) <ref type="bibr" coords="3,156.91,168.26,16.38,10.91" target="#b10">[11]</ref>. Of course, there are other fusion methods, such as those that map to the same vector space. The subsequent development is the introduction of the latest models and the tuning process of various algorithms.</p><p>For the medical VQA, the development is much slower. The main reason is that the data set in the medical field is much more difficult to obtain than in the general field. Because this requires labeling by professional medical staff and a lot of time to carefully select the quality of the data set, such as the sharpness of the image. Because of the scarcity of data sets, the VQA task has not been rapidly developed in the medical field. The ImageCLEF competition is one of the few organizers that provide data sets in the medical field of VQA. In 2018, Peng et al. <ref type="bibr" coords="3,115.26,290.20,18.07,10.91" target="#b11">[12]</ref> proposed a model based on collaborative attention mechanism and MFB feature fusion, and their experimental results achieved first place in the ImageCLEF2018 VQA-Med task. Zhou et al. <ref type="bibr" coords="3,141.34,317.30,18.07,10.91" target="#b12">[13]</ref> proposed a model based on Inception-Resnet-v2 <ref type="bibr" coords="3,383.96,317.30,18.07,10.91" target="#b13">[14]</ref> and Bi-LSTM <ref type="bibr" coords="3,468.21,317.30,18.07,10.91" target="#b14">[15]</ref> and won the second place in the competition. Zachary et al. <ref type="bibr" coords="3,343.73,330.85,18.07,10.91" target="#b15">[16]</ref> proposed a model of SAN (twolayer Attention mechanism layer stacking) structure, which ranked third in the competition. In the second year of the ImageCLEF VQA-Med mission, the Zhejiang University team <ref type="bibr" coords="3,487.91,357.95,18.07,10.91" target="#b16">[17]</ref> proposed a combination of Bert <ref type="bibr" coords="3,230.94,371.50,17.82,10.91" target="#b17">[18]</ref> and MFB. In particular, this model extracts image features from the middle layer of ImageNet pre-trained VGG16, using Bert performs word embedding on the text to extract features and then uses MFB to perform feature fusion, and the results of the experiment won first place in the ImageCLEF2019 VQA-Med <ref type="bibr" coords="3,398.20,412.15,18.07,10.91" target="#b18">[19]</ref> and MFB task. The ImageCLEF2020 VQA-Med <ref type="bibr" coords="3,211.43,425.70,17.95,10.91" target="#b19">[20]</ref> task competition has just come to an end. It can be seen from the literature that researchers have made certain innovations to the traditional VQA depth model. The University of Adelaide team <ref type="bibr" coords="3,265.92,452.79,17.76,10.91" target="#b20">[21]</ref> proposed Skeleton-based Sentence Mapping (SSM) combined with a knowledge reasoning model and won first place in the competition. They combined the knowledge reasoning method into VQA-Med for the first time. Medical VQA is equivalent to start development based on the general VQA field, and the model draws on the methods used in the general VQA development process, but its limitation is that the lack and difference of data sets have led to many method limitations.</p><p>In the 2021 ImageCLEF VQA-Med competition and drawing on the methods used in previous competitions, we also made improvements and innovations. In data processing, image enhancement methods such as ZCA (whitening technology image enhancement model) <ref type="bibr" coords="3,450.46,561.19,18.05,10.91" target="#b21">[22]</ref> are also introduced to make feature extraction richer to get better output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Task and Dataset</head><p>Compared with ImageCLEF VQA-Med 2020, the data set is not much different. Last year's data set consisted of 4,000 medical images and 4000 question-answer (QA) pairs in the training set, 500 medical images and 500 QA pairs in the validation set, and 500 questions and 500 medical </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Image extraction feature</head><p>We first preprocessed the image and used the whitening technology image enhancement model and the adaptive histogram equalization method to limit the contrast to effectively enhance the details of the medical image. While subtly enhancing the contrast of medical images, it also plays a role in suppressing noise. Then a simplified version of the VGG8 model based on the VGG16 model pre-trained on the ImageNet data set <ref type="bibr" coords="4,288.06,553.20,18.00,10.91" target="#b23">[24]</ref> was used to extract image features. Because networks such as VGG16 or ResNet50 are too large and the amount of calculation is too large, and the use of such large networks to extract image feature extraction is too redundant and wasteful of resources. The actual experiment also proved that after the previous preprocessing, as long as the small network can achieve the same extraction effect as these large network models, it can effectively avoid overfitting and shorten the training time. So we reduced the original 13-layer convolutional layer of VGG16 to 5 layers and reduced the number of nodes in the following 3 layers of fully connected layers to 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Feature extraction and coding aspects of question text</head><p>We used BioBERT <ref type="bibr" coords="5,169.06,107.54,17.75,10.91" target="#b24">[25]</ref> which is better than BERT to extract the semantic features of the problem. Since BERT performed well in the ImageCLEF VQA-Med competitions in previous years, we also continued to use the pre-trained model for semantic feature extraction. BioBERT is pretrained in biomedical text. The network structure is the same as BERT. It inherits almost all the advantages of BERT, and its performance in various biomedical text mining tasks is much better than BERT and previous advanced models. We only need to modify the last layer to make it average to more effectively represent the text features of the question sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Feature fusion</head><p>Feature fusion is the same as image feature extraction and question text feature extraction, which is the key point of whether the VQA task can perform well. To make the interaction between different modalities more effective, we use the MFH which is more efficient than the previous MFB. Because in the dimensionality reduction operation before multiplying between multimodal matrices, MFH can be converted to a more suitable dimension for more effective fusion. At the same time, co-attention is introduced to achieve the characteristics of the problem text and pay more attention to the feature area of the image to improve the effect. Using question text features to capture and attention image-specific area features, and a total of two effective MFH fusions have achieved more accurate regional feature extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head><p>In the ImageCLEF 2021 VQA competition, the model we used is shown in Figure <ref type="figure" coords="5,460.46,405.58,3.81,10.91" target="#fig_1">2</ref>. Among them, in terms of extracting image features, we use VGG8 which is a simplified VGG16 network. Because with limited resources, through the number of image data sets after data enhancement, VGG8 is effective enough for extracting image features. Compared with large-scale networks (such as ResNet50), the effect gap is not too huge but the speed is greatly improved. In addition to improving speed, it also prevents overfitting. The text features are pre-trained on BioBERT. After the combined model of MFB and Co-attention fusion is performed, the image is weighted, and then concatenating is performed. The next step is to re-extract features after performing attention operations on the original image. Here we set 4 sets of attention values, because too many extraction groups will ignore the relationship between the information in the image, and too little will make it impossible to better extract the important features of the image So, in the end, we chose 4 groups which is the best grouping.</p><p>After having a better interaction between image features and question text features and giving important features greater weight, once again, the two features are fused and output, here we no longer use the MFH module for fusion. Because the previous 4 sets of features and multiple pieces of training have made the interaction between the modalities sufficient, the image feature information is mapped to the text feature information vector space for fusion, which can effectively reduce the amount of calculation and save resources. Finally, through FC Layer and Softmax layer output.  In the experiment, the loss function we used is the binary cross-entropy loss function, the optimizer is Adam, and the learning rate is 1e-5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results And Summary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Results</head><p>In the ImageCLEFmed VQA-Med 2021 competition, the overall _accuracy and BLEU are used as the evaluation indicators for the final submission results ranking display. That is the proportion of correct predictions, the similarity between the real answer and the predicted answer. Figure <ref type="figure" coords="6,89.29,436.25,5.12,10.91" target="#fig_2">3</ref> shows the change curve of accuracy and loss during our training. The final results after we submitted were 0.316 and 0.352 respectively, ranking 6th among valid submitters. Figure <ref type="figure" coords="6,501.06,449.80,5.17,10.91" target="#fig_4">4</ref> shows the ranking page of this ImageCLEF VQA-Med 2021 medical competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Summary</head><p>Due to the limitations of hardware resources and other conditions, the main idea of this experiment is to obtain the best results with the least resource cost, so the smallest possible modified version of VGG8 is used to extract image features. The corresponding remedy is to use image enhancement. And set an appropriate number of extraction groups in the subsequent collaborative attention mechanism to improve the accuracy. After experimenting with the VGG8 model, we also tried to use larger networks such as VGG16 and ResNet50 to extract features. The accuracy rate has indeed improved, but the time has been much longer and the space overhead has also been much higher, so in the end, I chose a more cost-effective small network, which I originally designed to achieve. If you only start with high-precision considerations, it is better to use large-scale network training when there is ample time. Table <ref type="table" coords="6,388.52,635.02,5.00,10.91">1</ref> shows the comparison of the accuracy results of the tried several models on the validation set.   In addition, although we used the attention mechanism to align the question text with the corresponding area of the image, after all, there is no refined feature mark for reference, and the result is inevitably bad. In future work, we plan to use VisualBert <ref type="bibr" coords="8,395.88,410.48,16.14,10.91" target="#b25">[26]</ref>, ImageBert <ref type="bibr" coords="8,466.88,410.48,16.14,10.91" target="#b26">[27]</ref>, and the Transformer structure model to achieve better performances. Try to migrate from a data set marked with position coordinates, and introduce the method of target detection to make the alignment of text and image more accurate, making the effect better.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,246.31,251.82,8.93;4,89.29,84.19,416.71,137.61"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Three forms in ImageCLEF VQA-Med 2021 data set</figDesc><graphic coords="4,89.29,84.19,416.71,137.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,89.29,264.86,331.88,8.93"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The model we used in the ImageCLEFmed VQA-Med 2021 competition</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,89.29,387.21,260.04,8.93;7,89.29,84.19,416.69,278.50"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The change curve of accuray and loss during training</figDesc><graphic coords="7,89.29,84.19,416.69,278.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,88.99,420.36,32.19,8.93;7,89.29,432.37,209.80,8.87;7,219.35,451.98,26.18,8.87;7,333.82,451.98,118.44,8.87;7,151.14,470.71,162.59,7.21;7,385.00,469.41,16.09,8.87;7,143.01,482.66,178.85,7.21;7,385.00,481.37,16.09,8.87;7,143.01,494.62,178.85,7.21;7,385.00,493.32,16.09,8.87;7,89.29,527.60,209.37,12.85;7,88.93,553.21,418.25,10.91;7,89.29,566.76,416.69,10.91;7,89.29,580.31,417.89,10.91;7,89.29,593.86,416.70,10.91;7,89.29,607.41,416.69,10.91;7,89.29,620.96,416.69,10.91;7,89.29,634.51,416.69,10.91;7,89.29,648.06,416.70,10.91;7,89.29,661.61,416.69,10.91"><head>7 .</head><label>7</label><figDesc>Perspectives For Future workVQA technology mainly includes the solution of three problems: the extraction of image features, the extraction of problem text features, and the effective fusion of multi-modal features. The effectiveness of these three areas directly affects the quality of the results. In this experiment, for the extraction and characterization of text features, we used Bio-Bert's pre-training weights on the biomedical data set, used the VGG8 model for image feature extraction, and used efficient MFH for fusion. Considering the limited resources, in the image feature extraction, VGG8 with a small number of layers is used, and in the second fusion, a faster mapping method is used instead of matrix multiplication MFH and other methods. In other words, that is to reduce the final result score to save more resources and time. In addition, the image pre-training model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,89.29,305.13,155.55,8.93;8,89.29,84.19,416.67,196.42"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: 2021 VQA-Med leaderboard</figDesc><graphic coords="8,89.29,84.19,416.67,196.42" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,112.66,509.76,393.33,10.91;8,112.66,523.31,101.25,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,170.24,509.76,193.95,10.91">An ai view of the treatment of uncertainty</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Saffiotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,375.00,509.76,130.99,10.91;8,112.66,523.31,32.53,10.91">The Knowledge Engineering Review</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="75" to="97" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,536.85,393.33,10.91;8,112.66,550.40,76.60,10.91" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m" coord="8,340.48,536.85,62.74,10.91">Deep learning</title>
		<imprint>
			<publisher>MIT press Cambridge</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,563.95,393.33,10.91;8,112.66,577.50,362.94,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,384.40,563.95,121.59,10.91;8,112.66,577.50,208.08,10.91">Overview of imageclef 2018 medical domain visual question answering task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,346.99,577.50,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,591.05,395.01,10.91;8,112.66,604.60,394.53,10.91;8,112.48,618.15,395.18,10.91;8,112.66,631.70,394.53,10.91;8,112.28,645.25,393.70,10.91;8,112.66,658.80,393.33,10.91;9,112.66,86.97,393.33,10.91;9,112.66,100.52,393.53,10.91;9,112.66,114.06,197.61,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,264.32,645.25,241.66,10.91;8,112.66,658.80,261.75,10.91">Overview of the ImageCLEF 2021: Multimedia retrieval in medical, nature, internet and social media applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Peteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tauteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Moustahfid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,401.27,658.80,104.72,10.91;9,112.66,86.97,393.33,10.91;9,112.66,100.52,201.15,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 12th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="9,348.61,100.52,157.57,10.91;9,112.66,114.06,31.10,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,127.61,395.17,10.91;9,112.66,141.16,393.32,10.91;9,112.66,154.71,182.33,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,228.76,127.61,279.07,10.91;9,112.66,141.16,148.34,10.91">Multi-modal factorized bilinear pooling with co-attention learning for visual question answering</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,284.70,141.16,221.29,10.91;9,112.66,154.71,84.69,10.91">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1821" to="1830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,168.26,393.33,10.91;9,112.66,181.81,260.26,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,173.06,168.26,332.93,10.91;9,112.66,181.81,104.29,10.91">Imageclef 2019: Projection-based ct image analysis for tb severity scoring and ct report generation</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,244.30,181.81,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,195.36,393.32,10.91;9,112.66,208.91,317.68,10.91" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0210</idno>
		<title level="m" coord="9,223.67,195.36,282.32,10.91;9,112.66,208.91,140.67,10.91">A multi-world approach to question answering about real-world scenes based on uncertain input</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,222.46,393.33,10.91;9,112.66,236.01,394.65,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,197.00,222.46,233.25,10.91">Answer-type prediction for visual question answering</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,452.94,222.46,53.05,10.91;9,112.66,236.01,296.73,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4976" to="4984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,249.56,393.33,10.91;9,112.66,263.11,393.33,10.91;9,112.66,276.66,107.17,10.91" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="9,414.89,249.56,91.09,10.91;9,112.66,263.11,314.26,10.91">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01847</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,290.20,393.61,10.91;9,112.66,303.75,250.86,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K.-W</forename><surname>On</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.04325</idno>
		<title level="m" coord="9,366.63,290.20,139.63,10.91;9,112.66,303.75,68.59,10.91">Hadamard product for low-rank bilinear pooling</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,317.30,393.33,10.91;9,112.66,330.85,393.32,10.91;9,112.66,344.40,188.31,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,275.64,317.30,230.35,10.91;9,112.66,330.85,216.45,10.91">Beyond bilinear: Generalized multimodal factorized high-order pooling for visual question answering</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,337.93,330.85,168.06,10.91;9,112.66,344.40,94.23,10.91">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5947" to="5959" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,357.95,393.33,10.91;9,112.33,371.50,241.08,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,252.45,357.95,253.53,10.91;9,112.33,371.50,86.21,10.91">Umass at imageclef medical visual question answering (med-vqa) 2018 task</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Rosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,224.79,371.50,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,385.05,393.33,10.91;9,112.39,398.60,268.42,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,226.33,385.05,279.66,10.91;9,112.39,398.60,111.92,10.91">Employing inception-resnet-v2 and bi-lstm for medical domain visual question answering</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,252.19,398.60,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,412.15,393.33,10.91;9,112.66,425.70,393.33,10.91;9,112.66,439.25,130.69,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,307.29,412.15,198.70,10.91;9,112.66,425.70,152.62,10.91">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,287.99,425.70,217.99,10.91;9,112.66,439.25,50.10,10.91">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,452.79,393.32,10.91;9,112.66,466.34,172.11,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,232.02,452.79,173.19,10.91">Bidirectional recurrent neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,414.04,452.79,91.95,10.91;9,112.66,466.34,78.03,10.91">IEEE transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,479.89,393.33,10.91;9,112.66,493.44,393.48,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,428.58,479.89,77.40,10.91;9,112.66,493.44,236.73,10.91">Nlm at imageclef 2018 visual question answering in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gayen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rajaraman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,377.52,493.44,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,506.99,393.33,10.91;9,112.66,520.54,300.49,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="9,268.54,506.99,237.44,10.91;9,112.66,520.54,143.74,10.91">Zhejiang university at imageclef 2019 visual question answering in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,284.53,520.54,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,534.09,393.33,10.91;9,112.66,547.64,363.59,10.91" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="9,353.43,534.09,152.55,10.91;9,112.66,547.64,181.08,10.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,561.19,394.62,10.91;9,112.66,574.74,393.58,10.91;9,112.33,588.29,101.72,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="9,464.35,561.19,42.93,10.91;9,112.66,574.74,317.49,10.91">Vqa-med: Overview of the medical visual question answering task at imageclef</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,481.58,574.74,24.66,10.91;9,112.33,588.29,71.82,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,601.84,393.32,10.91;9,112.39,615.39,393.60,10.91;9,112.66,628.93,218.05,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="9,433.15,601.84,72.83,10.91;9,112.39,615.39,393.60,10.91;9,112.66,628.93,30.76,10.91">Overview of the vqa-med task at imageclef 2020: Visual question answering and generation in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,152.48,628.93,117.32,10.91">CLEF 2020 Working Notes</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="22" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,642.48,393.33,10.91;9,112.66,656.03,393.33,10.91;9,112.66,669.58,145.48,10.91" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="9,354.56,642.48,151.43,10.91;9,112.66,656.03,393.33,10.91;9,112.66,669.58,84.94,10.91">Aiml at vqa-med 2020: Knowledge inference via a skeleton-based sentence mapping approach for medical domain visual question answering</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Verjans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
		<respStmt>
			<orgName>CLEF</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,86.97,393.32,10.91;10,112.66,100.52,244.55,10.91" xml:id="b21">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">K</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Sindhu</forename><surname>Ramachandran</surname></persName>
		</author>
		<title level="m" coord="10,286.71,86.97,219.27,10.91;10,112.66,100.52,212.63,10.91">Harendrakv at vqa-med 2020: Sequential vqa with attention for medical visual question answering</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,114.06,393.33,10.91;10,112.39,127.61,393.60,10.91;10,112.66,141.16,394.53,10.91;10,112.66,154.71,116.58,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="10,435.39,114.06,70.60,10.91;10,112.39,127.61,393.60,10.91;10,112.66,141.16,31.38,10.91">Overview of the vqa-med task at imageclef 2021: Visual question answering and generation in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,171.48,141.16,118.94,10.91">CLEF 2021 Working Notes</title>
		<title level="s" coord="10,298.64,141.16,180.76,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,168.26,393.33,10.91;10,112.66,181.81,394.53,10.91;10,112.66,195.36,103.61,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="10,346.64,168.26,159.35,10.91;10,112.66,181.81,67.28,10.91">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,228.08,181.81,274.55,10.91">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,208.91,393.33,10.91;10,112.66,222.46,393.98,10.91;10,112.41,236.01,48.96,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="10,361.64,208.91,144.35,10.91;10,112.66,222.46,268.25,10.91">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,394.29,222.46,66.92,10.91">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,249.56,393.33,10.91;10,112.66,263.11,323.74,10.91" xml:id="b25">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m" coord="10,345.60,249.56,160.39,10.91;10,112.66,263.11,141.28,10.91">Visualbert: A simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,276.66,393.33,10.91;10,112.26,290.20,395.42,10.91" xml:id="b26">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sacheti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07966</idno>
		<title level="m" coord="10,338.43,276.66,167.55,10.91;10,112.26,290.20,215.86,10.91">Imagebert: Cross-modal pre-training with large-scale weak-supervised image-text data</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
