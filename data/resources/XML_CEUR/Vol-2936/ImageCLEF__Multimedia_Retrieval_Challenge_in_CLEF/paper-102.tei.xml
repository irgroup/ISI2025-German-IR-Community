<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,327.18,15.42;1,89.29,106.66,106.29,15.43">UAIC2021: Lung Analysis for Tuberculosis Classification</title>
				<funder ref="#_z9swW8u #_XQHbKnZ #_HrxS9CR">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,88.87,134.97,95.54,11.96"><forename type="first">Alexandra</forename><surname>Hanganu</surname></persName>
							<email>alexandra.hanganu@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">Cuza&quot; University</orgName>
								<address>
									<settlement>Iasi</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,196.40,134.97,95.49,11.96"><forename type="first">Cristian</forename><surname>Simionescu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">Cuza&quot; University</orgName>
								<address>
									<settlement>Iasi</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,303.22,134.97,108.64,11.96"><forename type="first">Lucia-Georgiana</forename><surname>Coca</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">Cuza&quot; University</orgName>
								<address>
									<settlement>Iasi</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,439.32,134.97,65.04,11.96"><forename type="first">Adrian</forename><surname>Iftene</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">Cuza&quot; University</orgName>
								<address>
									<settlement>Iasi</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,93.29,160.96,53.75,7.99"><forename type="first">Alexandru</forename><surname>Ioan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">Cuza&quot; University</orgName>
								<address>
									<settlement>Iasi</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,327.18,15.42;1,89.29,106.66,106.29,15.43">UAIC2021: Lung Analysis for Tuberculosis Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4A60CFE2894E47D0C3E3671262C43178</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Computed Tomography</term>
					<term>Tuberculosis</term>
					<term>Deep Learning</term>
					<term>2D Projections</term>
					<term>k-Means</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article presents a methodology for chest CT scan analysis that enables the automatic categorization of pulmonary tuberculosis cases into one of the following five types: Infiltrative, Focal, Tuberculoma, Miliary, Fibro-cavernous. We showcase several deep learning methods for classifying tuberculosis in CT scans from the ImageCLEF 2021 Tuberculosis -TBT classification challenge. Furthermore, it explores the use of pre-trained models, as well as training from scratch on volumetric data and 2D projections.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>According to the NIH (National Institute of Allergy and Infectious Diseases), tuberculosis (TB) is the leading infectious cause of death worldwide. Despite the development of technology, in 2017, 10 million people became ill with TB, and 1.6 million people died of TB disease, including 230,000 children, according to the World Health Organization. Over the past 200 years, TB has claimed the lives of more than one billion people-more deaths than those caused by malaria, influenza, smallpox, HIV/AIDS, cholera, and plague combined. Although tuberculosis treatment exists and further research is ongoing, drug resistance poses a continued threat.</p><p>ImageCLEF <ref type="bibr" coords="1,152.72,441.55,12.68,10.91" target="#b0">[1]</ref> is an evaluation campaign that is being organized as part of the CLEF initiative labs1. The ImageCLEFmedical 2021 challenge has a task <ref type="bibr" coords="1,346.75,455.10,13.00,10.91" target="#b1">[2]</ref> which consists of automatically categorizing each TB case into one of the following five types: (1) Infiltrative, (2) Focal, (3) Tuberculoma, (4) Miliary, (5) Fibro-cavernous.</p><p>The organizers provided to participants two types of masks. The first version of segmentation provides more accurate masks <ref type="bibr" coords="1,227.73,509.30,11.50,10.91" target="#b2">[3]</ref>, but it tends to miss large abnormal regions of lungs in the most severe TB cases. The second segmentation on the contrary provides more rough bounds of masks <ref type="bibr" coords="1,130.23,536.39,11.28,10.91" target="#b3">[4]</ref>, but behaves more stable in terms of including lesion areas. In case the participants use the provided masks in their experiments.</p><p>This paper was organized as follows: section 2 describes the related work, section 3 presents our methods, section 4 evaluates the methods proposed and, in the end, we draw conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In previous editions, different approaches were proposed for the tuberculosis task. In 2017, there were two important tasks: MDR Detection which consisted of assigning each TB patient the probability of having a resistant form of tuberculosis based on the analysis of chest CT scan; Detection of TB Type task which automatically categorize each TB case into one of the following five types: Infiltrative, Focal, Tuberculoma, Miliary, Fibro-cavernous.</p><p>In 2018, the valuable result was of team UIIP-BioMed that used a Deep CNN <ref type="bibr" coords="2,439.28,179.03,11.38,10.91" target="#b4">[5]</ref>, folowed by MedGift that used an SVM with and RBF kernel <ref type="bibr" coords="2,304.58,192.57,11.43,10.91" target="#b5">[6]</ref>.</p><p>In 2019, UIIP-BioMed <ref type="bibr" coords="2,202.20,206.12,12.99,10.91" target="#b6">[7]</ref> was the leader again, followed by CompEle-cEngCU <ref type="bibr" coords="2,463.24,206.12,11.58,10.91" target="#b7">[8]</ref>. UIIP-BioMed used a 2D CNN whilst CompEle-cEngCU used a 2D CNN based on AlexNet <ref type="bibr" coords="2,469.84,219.67,11.56,10.91" target="#b8">[9]</ref>. The solution developed by our group uses stage-wise boosting in low-resource environments <ref type="bibr" coords="2,486.66,233.22,16.25,10.91" target="#b9">[10]</ref>.</p><p>In 2020, the majority of the participants used some variations of the projection-based approach and created 2D CNNs. As a result, only four groups tried 3D CNNs for a direct analysis of the CT volumetric data <ref type="bibr" coords="2,181.26,273.87,16.41,10.91" target="#b10">[11]</ref>. SenticLab.UAIC <ref type="bibr" coords="2,281.06,273.87,18.06,10.91" target="#b11">[12]</ref> was the winner of the task using 2D and 3D CNNs. The SDVA-UCSD was ranked on second place using a 3D CNN with a convolutional block attention module (CBMA) and a customized loss function <ref type="bibr" coords="2,372.92,300.97,16.21,10.91" target="#b12">[13]</ref>. Our team was ranked on 7th place, using SVMs and CNNs for lung-wise processing <ref type="bibr" coords="2,350.98,314.52,16.25,10.91" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>We propose an extension to the 2D projection methods which were successfully applied in previous years by replacing the arithmetic mean aggregation function with k-means. We made this choice since it can be considered that the average is a specific case for k-means when k=1. With this, we hope to capture more relevant characteristics from the data. This gives us the advantage of working with smaller dimensionality data.</p><p>Transforming 3D CT scans into 2D images does have the downside of losing spacial proximity information along the third dimension and introduces the probability of losing important features. Due to this, we also experimented working with the 3D samples directly in order to have a reference point of how the two modalities (2D and 3D) performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>Based on the metadata file presented by the organizers, we concluded that the data at hand is highly imbalanced. Since this could have negatively influenced our models, we have decided to use weighted loss, in order to counteract this phenomenon.</p><p>Table <ref type="table" coords="2,126.50,571.91,4.97,10.91" target="#tab_0">1</ref> provides an insight for the distribution of each affection in the training set, as well as the weight used for the loss function. To calculate the weight, we have divided the lowest total number of patients affected by one of the types (in our case, this would be 70) by the number of patients affected for each type. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data Pre-Processing</head><p>Given that the masks provided were not able to return the best segmentation possible, a decision was made to combine the two of them and compute a new mask that would be closer to what we needed. However, this happened to be quite hard, as some of the masks did not have a proper lung segmentation, in accordance to the anatomical position given by the midsagittal line in the upper thoracic area. Furthermore, some issues were not related to the segmentations, but by the CT Scans, since some of them were not able to be opened or were considered faulty. Table <ref type="table" coords="3,501.01,307.46,4.97,10.91" target="#tab_1">2</ref> presents what issues we have encountered when processing the data and the proposed solution for said problems. The most difficult case to treat was the test CT Scan for patient TST_247, because we were not able to load it properly and it needed to be processed and labeled. Since it had a similar issue to the scan of patient TRN_360, we have assumed that both files have a similar origin and that the problem was either in an underlying anatomical condition or in the acquisition of a similar device fault. Thus, we manually assigned the same TBT class that TRN_360 was assigned in the training metadata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Data Normalization</head><p>When normalizing the images, we have opted for a similar approach to past high-ranking participants <ref type="bibr" coords="3,141.42,478.73,13.03,10.91" target="#b6">[7]</ref>. The erosion radius adopted was of 10 and the voxel intensity values were increased by 1024 Hounsfield Units (HU) with the threshold set to -1200, clipped to 600.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Segmentation Pre-Processing</head><p>The two types of lung segmentations provided by the organizers had different issues. One of them has accurate edges, but it tends to overlook or miss entirely some of the large cavities or other lesions in the lungs that would be relevant for the model. However, the other performs better in covering the entire lung, but it is generally more inaccurate, especially when referring to the edges of the lung lobes. The solution proposed for this issue was to generate a mean of the two masks. Combining them and retrieving the entire information, helped ensure that no particularity of the affections is overlooked, while also highlighting the increased importance of the sections where the segmentation methods overlap and hence agree upon. To complete this task, we looked at the first type of mask, the fully automatic multistage one, taking into account only the non-zero values, added them to the second type of mask and then computed the mean of the two, multiplying it by 255 in the end. Therefore, we computed a new image that had the opacity adjusted for the parts of the lungs where the two segmentations did not overlap. The resulting masks were applied to the raw CT scans by multiplying the two tensors. However, when completing this step, there was a threshold set so that it could help with eliminating the non-lung pixels present in the resulting image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Data Augmentation</head><p>Multiple types of data augmentation techniques were attempted to be used with a view to prevent an early over-fitting of the models. To this end, we have employed Randaugment <ref type="bibr" coords="4,89.29,567.92,16.23,10.91" target="#b14">[15]</ref>, only disabling the "Invert" and "Solarize" augmentations and setting ùëÅ = 2 and ùëÄ = 14. Additionally, we used Random Erasing <ref type="bibr" coords="4,268.76,581.47,18.07,10.91" target="#b15">[16]</ref> which is a technique that randomly crops out a patch of size between 5% and 15% from the original image. Nonetheless, there were other approaches to augmenting the dataset that we have tried to apply, such as Mixup <ref type="bibr" coords="4,467.85,608.57,18.07,10.91" target="#b16">[17]</ref> and Cutmix <ref type="bibr" coords="4,125.58,622.12,16.34,10.91" target="#b17">[18]</ref>, but these methods proved to produce images which were making it too difficult for the model to learn on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Volumetric Classification</head><p>When working with the 3D volumetric data, we have looked for a model that had been pretrained, preferably on medical data. Thus, we came across MedicalNet <ref type="bibr" coords="5,411.41,121.08,16.41,10.91" target="#b18">[19]</ref>, which provided some interesting models that had been trained previously. We have worked with these in order to obtain better results that could not be achieved when training the network from scratch. However, due to the large dimension of the samples and the hardware limitation imposed by using a single Nvidia RTX 2080Ti GPU, we had to resize all the images to 256x256x32. Taking these facts into consideration, we managed to fit into the GPU memory only the MedicalNet10 variant, which is essentially a pre-trained 3D-ResNet10 network.</p><p>One technique adopted when training was gradient accumulation. Therefore, since we could only use a maximum batch size of 2, we accumulated 16 gradient backpropagation steps before updating the weights, effectively allowing us to simulate a batch size of 32.</p><p>We loaded the 23-dataset pre-trained MedicalNet10 weights, replacing its segmentation head with a newly initialized classification head, where a linear layer outputs the probabilities for each of the 5 classes.</p><p>As for the hyper parameter tuning step, we attempted multiple variations of optimizers and data augmentations, but unfortunately our attempts were consistently faced with an early overfitting of the model. This means that it would happen after the first 40 epochs of training, when very little to no data augmentations were applied. One other issue was that frequently we would encounter an unstable training process, caused by exploding gradients, which would then be followed by a model collapse, usually occurring in the first 5 epochs when stronger augmentations were used.</p><p>Our best submitted run is a MedicalNet10 model trained for 44 epochs using the AdamP optimizer <ref type="bibr" coords="5,134.08,405.62,17.80,10.91" target="#b19">[20]</ref> with the default parameters, cross entropy loss and with no data augmentations. Attempts to train 3D-ResNet variants from scratch also suffered from the same issues but the overfitting would start sooner and as such the models generated poorer results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Projection Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1.">Projection Generation</head><p>In previous years, the computation and usage of 2D projections has proven to be one of the most successful ways of pre-processing a 3D volume. This happened mostly due to its effective reduction in dimensionality without the loss of relevant information, so, this year, we attempted to build upon this approach. We had managed to adapt the projection method to use the k-means algorithm <ref type="bibr" coords="5,134.91,557.21,16.83,10.91" target="#b20">[21]</ref> <ref type="bibr" coords="5,151.74,557.21,16.83,10.91" target="#b21">[22]</ref>. As described in <ref type="bibr" coords="5,245.78,557.21,12.68,10.91" target="#b6">[7]</ref> and <ref type="bibr" coords="5,280.02,557.21,16.09,10.91" target="#b11">[12]</ref>, generating 2D projections from the X, Y and Z axis, by aggregating along the a given direction with the max(), average() and std() functions and stacking these results as channels of the same image, proved to conserve enough relevant information, while simultaneously drastically reducing the size. We propose the extension of the family of aggregation functions used for this purpose with k-means. We take the centroid values from k-means as the output which will construct multiple image channels. Consequently, when looking at the issue this way, the average() function from the original projection method becomes a particular case of k-means, that is when k=1. Using k-means as an aggregation function gives us more flexibility with the number of potentially relevant features that we extract as centroids from the 3D volume. Since the function outputs a number of centroids equal to the value of k (not just 1, as in the case of average()), each of these centroids is mapped to a different channel in the resulting projection. The centroids are initialized randomly, this factor did not seem to be a big influence on the results since the algorithm always and very rapidly finds the final centroids, in very few steps, as it is a very simple 1D k-means calculation. The mapping is done by sorting the centroid values, the smallest valued centroids will be mapped to the first channel, the second smallest to the second channel and so on. Our final projection algorithm still used max() and std() as aggregation functions and replaced average() with k-means() where k=5. The resulting images consist of 7 channel projections for each of the axis directions X, Y and Z. The usage of max() could be replaced by using higher values for k since the centroid on the higher end of the interval will probably be very close in value to the max value.</p><p>An ablation study would be needed to identify an optimal value for k, as a starting point, we arbitrarily picked k=5. Since a stable training procedure was not obtained, we did not get to fine-tune this parameter.</p><p>The Y projection had to be treated separately, since projecting from that direction would overlap the two lungs. In order to prevent losing relevant information at the lung level, we first separated the lungs with the help of the lung-wise segmentation and then calculated the projection separately for each one. Consequently, we concatenated the two images such that the two lungs would be positioned next to each other. The resulting X, Y and Z projections can be observed in Fig. <ref type="figure" coords="7,173.78,445.85,4.00,10.91" target="#fig_0">1</ref>. Due to having more than 3 channels, the figures here were obtained by resizing the number of channels to 3, for RGB. However, there exists less information, because some was lost in the process of resizing the channels for visualization purposes.</p><p>After computing the X, Y and Z projections, each with 7 channels, we have decided to stack them along the channel dimension. Given that the anatomical positioning is the same, overlapping the lobes would not be considered an issue, especially since there would be 21 individual channels for the model to look at and learn the particularities of each tuberculosis type. An approximated look of the resulting images is presented in Fig. <ref type="figure" coords="7,409.77,540.70,3.99,10.91" target="#fig_1">2</ref>. This was favorable especially because the algorithm would only use one image for the training and testing process, meaning that the data would be fed all at once. A fixed random seed was used for to generate the k-means projections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2.">Models</head><p>The considerably smaller 2D image size allowed us to use larger models and batch sizes. To do so, we tried resizing the images, while preserving the number of channels. As for the CNN models, we experimented with PreResNet56 <ref type="bibr" coords="7,283.61,657.77,17.76,10.91" target="#b22">[23]</ref> and the Swin Transformer <ref type="bibr" coords="7,421.14,657.77,17.75,10.91" target="#b23">[24]</ref> large, base and tiny variants.</p><p>For the Swin Transformer models, we also tried to use the weights trained on ImageNet-21k <ref type="bibr" coords="8,108.95,114.06,18.07,10.91" target="#b24">[25]</ref> and to apply various transfer learning techniques in order to adapt it to the TBT classification task. One of the steps implemented to do this was to reinitialize the PatchEmbed layer and the final normalization, pooling and linear layers, so that they can match our 5 classes. The Swin Transformer model runs were not able to noticeably reduce the loss when trained from scratch or when loading the pre-trained weights, but we believe this might have been due to an implementation error on our part or from a faulty normalization technique.</p><p>The two models that delivered the best results were both based on PreResNet, with a depth of 56. Most of the experiments with this type of model were based on hyper-parameter tuning, as the base was a standard PreResNet56 that had been trained from scratch on data for the task.</p><p>For the hyper-parameters, we used the RAdam optimizer <ref type="bibr" coords="8,351.75,236.01,18.57,10.91" target="#b25">[26]</ref> with the default parameters, a cross entropy loss function and a batch size of 16 with 4 gradient accumulation steps, resulting in a simulated batch size of 64. Both models use lookahead <ref type="bibr" coords="8,345.84,263.11,18.79,10.91" target="#b26">[27]</ref>. Further information about the difference between the parameters of the two models is presented in Table .3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head><p>When evaluating our experiments, the main reference was the kappa score, defined on the interval [-1,1]. This score is not differentiable, so we could not use it as a loss function directly for our models. Thence, the training process was also monitored by following the cross entropy loss function. As a means to track the progress of the training process, we randomly split the initial train dataset into train and validation splits, while also preserving the same class imbalanced for both splits. Hence, in order for the results to be consistent between different runs, all runs had the same data split. The experiments were performed using only flips and rotations as data augmentations, this caused the model to overfit very early on and would not recover with more training. This phenomenon convinced us to apply stronger augmentation techniques, such as Randaugment and Random Erasing, in order to populate the latent space. However, when these stronger augmentations were being used, the models suffered from training instability since our loss values would explode and hence the gradients would too, resulting in numeric overflow. Finally, once the loss exploded, the model collapsed and did not learn anything else. This happened very fast, usually in the first 5 epochs of training.</p><p>The four submissions made by our team are listed in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Future Work</head><p>Further work needs to be invested into adapting state-of-the-art data augmentations from the general computer vision domain to medical images. Since the size of the datasets usually encountered in the field is small, stable data augmentation strategies are required, as frequently acquiring additional data is not feasible.</p><p>As a retrospective analysis of our methods, we need to carefully investigate the exact causes of training instability. We suspect that, because we did not normalize the data after the final step in the pre-processing/augmentation pipeline, the resulting images might have very different value distributions between different samples, which could even escape our intended value interval of [0.0, 1.0]. Another potential cause of our issues could come from the large size of the linear classification layer which could be the cause of the exploding gradients we noticed, we have to investigate if using a pooling layer before the classification one helps solve our issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper we present ways of detecting different types of tuberculosis in lungs. The methods that have proven to work best for us are focused on pre-trained models for classifying 3D volumetric data and models trained from scratch for 2D projections.</p><p>The best result from our submissions can be improved, but in doing so, we would have to investigate different normalization techniques and types of data augmentation that would suit the task better. However, in order to work with volumetric data properly, we would also need to improve the current hardware with the aim of being able to apply larger models, as well as greater batch sizes, on medical datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,89.29,389.44,223.34,8.93;6,223.47,231.03,145.84,145.84"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: X, Y and Z Axis Projections Approximations.</figDesc><graphic coords="6,223.47,231.03,145.84,145.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,89.29,367.61,182.91,8.93;7,160.97,84.19,270.85,270.85"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Stacked Projection Approximation</figDesc><graphic coords="7,160.97,84.19,270.85,270.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.99,90.49,365.99,100.72"><head>Table 1</head><label>1</label><figDesc>Patients Affected in the Training Set and Weighted Loss Used.</figDesc><table coords="3,140.29,122.11,314.70,69.09"><row><cell>Tuberculosis Type</cell><cell>Patients Affected (out of 915)</cell><cell>Weighted Loss</cell></row><row><cell>(1) Infiltrative</cell><cell>418</cell><cell>0.1674</cell></row><row><cell>(2) Focal</cell><cell>226</cell><cell>0.3097</cell></row><row><cell>(3) Tuberculoma</cell><cell>101</cell><cell>0.6930</cell></row><row><cell>(4) Miliary</cell><cell>100</cell><cell>0.6930</cell></row><row><cell>(5) Fibro-cavernous</cell><cell>70</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,88.99,90.49,388.95,318.87"><head>Table 2</head><label>2</label><figDesc>Dataset Issues and Solutions</figDesc><table coords="4,117.33,119.89,360.62,289.46"><row><cell>Data Type</cell><cell>Patient ID</cell><cell>Issue</cell><cell>Solution</cell></row><row><cell>Train CT Scan</cell><cell>TRN_305</cell><cell>Corrupt file</cell><cell>Left out</cell></row><row><cell></cell><cell></cell><cell>Initial CT Scan consisted</cell><cell></cell></row><row><cell>Train CT Scan</cell><cell>TRN_360</cell><cell>of only some white pixels</cell><cell>Left out</cell></row><row><cell></cell><cell></cell><cell>on a black background</cell><cell></cell></row><row><cell></cell><cell></cell><cell>and could not be used</cell><cell></cell></row><row><cell></cell><cell>TRN_366</cell><cell></cell><cell></cell></row><row><cell></cell><cell>TRN_433</cell><cell></cell><cell>Recolored the mask image</cell></row><row><cell></cell><cell>TRN_867</cell><cell></cell><cell>according to the initial</cell></row><row><cell>Train Mask 1</cell><cell>TRN_867</cell><cell>Wrong lung</cell><cell>colour scheme and</cell></row><row><cell></cell><cell>TRN_887</cell><cell>segmentation</cell><cell>removed the trachea as</cell></row><row><cell></cell><cell>TRN_891</cell><cell></cell><cell>it was also selected as</cell></row><row><cell></cell><cell>TRN_894</cell><cell></cell><cell>part of the lungs</cell></row><row><cell></cell><cell>TRN_897</cell><cell></cell><cell></cell></row><row><cell></cell><cell>TST_051</cell><cell></cell><cell>Recolored the mask image</cell></row><row><cell></cell><cell>TST_093</cell><cell></cell><cell>according to the initial</cell></row><row><cell>Test Mask 1</cell><cell>TST_124</cell><cell>Wrong lung</cell><cell>colour scheme and</cell></row><row><cell></cell><cell>TST_269</cell><cell>segmentation</cell><cell>removed the trachea as</cell></row><row><cell></cell><cell>TST_362</cell><cell></cell><cell>it was also selected as</cell></row><row><cell></cell><cell></cell><cell></cell><cell>part of the lungs</cell></row><row><cell></cell><cell></cell><cell>Initial CT Scan consisted</cell><cell>Assumed it was similar</cell></row><row><cell>Test CT Scan</cell><cell>TST_247</cell><cell>of only some white pixels</cell><cell>to patient TRN_360</cell></row><row><cell></cell><cell></cell><cell>on a black background</cell><cell>and assigned the</cell></row><row><cell></cell><cell></cell><cell>and could not be used</cell><cell>same label (Type 1)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,88.99,305.66,288.15,92.35"><head>Table 3</head><label>3</label><figDesc>Comparison of the Two PreResNet56 Models</figDesc><table coords="8,218.13,337.29,159.01,60.72"><row><cell>Parameter</cell><cell cols="2">#135756 #135798</cell></row><row><cell>Epochs</cell><cell>55</cell><cell>28</cell></row><row><cell>Image Size</cell><cell>384x384</cell><cell>512x512</cell></row><row><cell>Learning Rate</cell><cell>0.0001</cell><cell>1.0e-05</cell></row><row><cell>Lookahead K</cell><cell>8</cell><cell>16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,333.13,666.32,174.06,10.91"><head></head><label></label><figDesc>Table.4, detailing what model was used, whether or not the model had pre-trained weights, the local validation kappa score and the test kappa score. Since training with strong data augmentations would result in an unstable training, which stopped the model from learning in the first few epochs, no submissions were made with them. All of the runs we submitted suffered from relatively fast overfitting, this occurring in the first approximately 50 epochs.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,88.99,170.17,383.73,90.13"><head>Table 4</head><label>4</label><figDesc></figDesc><table coords="9,89.29,182.18,383.43,78.12"><row><cell cols="3">Submission Runs -Official Evaluation Results</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ID</cell><cell>Model</cell><cell cols="4">Pre-trained Modality Val. Kappa Test Kappa</cell></row><row><cell cols="2">135708 MedicalNet10</cell><cell>Yes</cell><cell>3D</cell><cell>0.232</cell><cell>0.129</cell></row><row><cell>135756</cell><cell>ResNet56</cell><cell>No</cell><cell>2D</cell><cell>0.209</cell><cell>-0.003</cell></row><row><cell>135750</cell><cell>3D-ResNet20</cell><cell>No</cell><cell>3D</cell><cell>0.117</cell><cell>-0.019</cell></row><row><cell>135798</cell><cell>ResNet56</cell><cell>No</cell><cell>2D</cell><cell>0.134</cell><cell>0.016</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7.">Acknowledgements</head><p>This work was supported by project <rs type="projectName">REVERT</rs> (<rs type="projectName">taRgeted thErapy for adVanced colorEctal canceR paTients</rs>), Grant Agreement number: <rs type="grantNumber">848098</rs>, <rs type="grantNumber">H2020-SC1-BHC-2018-2020/H2020-SC1-2019-Two-Stage-RTD</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_z9swW8u">
					<orgName type="project" subtype="full">REVERT</orgName>
				</org>
				<org type="funded-project" xml:id="_XQHbKnZ">
					<idno type="grant-number">848098</idno>
					<orgName type="project" subtype="full">taRgeted thErapy for adVanced colorEctal canceR paTients</orgName>
				</org>
				<org type="funding" xml:id="_HrxS9CR">
					<idno type="grant-number">H2020-SC1-BHC-2018-2020/H2020-SC1-2019-Two-Stage-RTD</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,112.66,197.00,395.01,10.91;10,112.66,210.55,394.53,10.91;10,112.48,224.10,395.18,10.91;10,112.66,237.65,394.53,10.91;10,112.28,251.20,393.70,10.91;10,112.66,264.75,393.33,10.91;10,112.66,278.30,393.33,10.91;10,112.66,291.85,393.53,10.91;10,112.66,305.40,197.61,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,264.32,251.20,241.66,10.91;10,112.66,264.75,261.75,10.91">Overview of the ImageCLEF 2021: Multimedia retrieval in medical, nature, internet and social media applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Peteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tauteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>≈ûtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Moustahfid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,401.27,264.75,104.72,10.91;10,112.66,278.30,393.33,10.91;10,112.66,291.85,201.15,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 12th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="10,348.61,291.85,157.57,10.91;10,112.66,305.40,31.10,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,318.95,395.17,10.91;10,112.66,332.50,394.53,10.91;10,112.66,346.05,394.53,10.91;10,112.66,359.59,22.69,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,393.45,318.95,114.38,10.91;10,112.66,332.50,257.62,10.91">Overview of ImageCLEFtuberculosis 2021 -CT-based tuberculosis type classification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="10,392.48,332.50,110.11,10.91">CLEF2021 Working Notes</title>
		<title level="s" coord="10,112.66,346.05,181.84,10.91">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,373.14,393.60,10.91;10,112.66,386.69,393.33,10.91;10,112.33,400.24,393.94,10.91;10,112.66,413.79,393.33,10.91;10,112.66,427.34,173.47,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,424.12,373.14,82.14,10.91;10,112.66,386.69,232.90,10.91">Efficient and fully automatic segmentation of the lungs in ct volumes</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">Dicente</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">A</forename><surname>Jim√©nez Del Toro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Depeursinge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<ptr target="-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="10,330.31,400.24,175.96,10.91;10,112.66,413.79,178.39,10.91">Proceedings of the VISCERAL Anatomy Grand Challenge at the 2015 IEEE ISBI</title>
		<title level="s" coord="10,298.87,413.79,166.23,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">O</forename><surname>Goksel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Jim√©nez Del Toro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Foncubierta-Rodr√≠guez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</editor>
		<meeting>the VISCERAL Anatomy Grand Challenge at the 2015 IEEE ISBI</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,440.89,393.33,10.91;10,112.66,454.44,394.53,10.91;10,112.66,467.99,263.63,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,220.70,440.89,285.29,10.91;10,112.66,454.44,100.42,10.91">ImageCLEF 2017: Supervoxels and co-occurrence for tuberculosis ct image classification</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="10,242.30,454.44,116.63,10.91">CLEF2017 Working Notes</title>
		<title level="s" coord="10,367.79,454.44,139.40,10.91;10,112.66,467.99,45.79,10.91">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,481.54,395.17,10.91;10,112.66,495.09,295.26,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,347.00,481.54,160.83,10.91;10,112.66,495.09,137.85,10.91">ImageCLEF 2018: Lesion-based tbdescriptor for ct image analysis</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Snezhko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,258.69,495.09,117.32,10.91">CLEF 2018 Working Notes</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,508.64,395.17,10.91;10,112.66,522.18,393.33,10.91;10,112.66,535.73,253.46,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,229.11,508.64,278.73,10.91;10,112.66,522.18,393.33,10.91;10,112.66,535.73,96.16,10.91">Texture-based graph model of the lungs for drug resistance detection, tuberculosis type classification, and severity scoring: Participation in the imageclef 2018 tuberculosis task</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">Dicente</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,216.88,535.73,117.32,10.91">CLEF 2018 Working Notes</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,549.28,393.33,10.91;10,112.66,562.83,284.14,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,173.06,549.28,332.93,10.91;10,112.66,562.83,106.10,10.91">Imageclef 2019: Projection-based ct image analysis for tb severity scoring and ct report generation</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,227.27,562.83,114.59,10.91">CLEF2019 Working Notes</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">2380</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,576.38,393.32,10.91;10,112.66,589.93,382.77,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,236.46,576.38,269.52,10.91;10,112.66,589.93,204.51,10.91">Multi-view cnn with mlp for diagnosing tuberculosis patients using ct scans and clinically relevant metadata</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mossa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yibre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Evik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,325.90,589.93,114.59,10.91">CLEF2019 Working Notes</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">2380</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,603.48,393.33,10.91;10,112.66,617.03,298.18,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,295.99,603.48,209.99,10.91;10,112.66,617.03,70.43,10.91">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,191.91,617.03,130.83,10.91">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,630.58,393.33,10.91;10,112.66,644.13,395.16,10.91;10,112.66,657.68,97.69,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,259.66,630.58,246.32,10.91;10,112.66,644.13,312.26,10.91">ImageCLEFmed Tuberculosis 2019: Predicting ct scans severity scores using stage-wise boosting in low-resource environments</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tabarcea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Rosca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,433.70,644.13,74.12,10.91;10,112.66,657.68,42.75,10.91">CLEF2019 Working Notes</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">2380</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,86.97,393.33,10.91;11,112.66,100.52,393.32,10.91;11,112.66,114.06,57.62,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,450.85,86.97,55.13,10.91;11,112.66,100.52,297.53,10.91">Overview of imagecleftuberculosis 2020 -automatic ct-based report generation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,418.74,100.52,87.25,10.91;11,112.66,114.06,25.70,10.91">CLEF2020 Working Notes</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,127.61,393.33,10.91;11,112.66,141.16,393.32,10.91;11,112.66,154.71,57.62,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,254.74,127.61,251.25,10.91;11,112.66,141.16,300.65,10.91">Revealing lung affections from cts. a comparative analysis of various deep learning approaches for dealing with volumetric data</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Miron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Moisii</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Breaban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,421.43,141.16,84.55,10.91;11,112.66,154.71,25.70,10.91">CLEF2020 Working Notes</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,168.26,393.33,10.91;11,112.66,181.81,393.32,10.91;11,112.66,195.36,293.73,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,335.73,168.26,170.26,10.91;11,112.66,181.81,393.32,10.91;11,112.66,195.36,139.01,10.91">ImageCLEF2020: Laterality-reduction three-dimensional cbam-resnet with balanced sampler for multi-binary classification of tuberculosis and ct auto reports</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gentili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,259.88,195.36,114.59,10.91">CLEF2020 Working Notes</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,208.91,393.33,10.91;11,112.66,222.46,194.12,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,320.68,208.91,185.31,10.91;11,112.66,222.46,39.31,10.91">UAIC2020: Lung analysis for tuberculosis detection</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Coca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanganu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cusmuliuc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,160.27,222.46,114.59,10.91">CLEF2020 Working Notes</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,236.01,395.17,10.91;11,112.66,249.56,393.32,10.91;11,112.66,263.11,325.92,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,302.02,236.01,205.81,10.91;11,112.66,249.56,171.71,10.91">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,307.43,249.56,198.55,10.91;11,112.66,263.11,237.36,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,276.66,394.62,10.91;11,112.66,290.20,394.53,10.91;11,112.41,303.75,27.76,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,322.30,276.66,161.24,10.91">Random erasing data augmentation</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,112.66,290.20,267.21,10.91">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,317.30,395.17,10.91;11,112.66,330.85,197.93,10.91" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m" coord="11,331.02,317.30,176.81,10.91;11,112.66,330.85,16.17,10.91">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,344.40,393.32,10.91;11,112.66,357.95,393.32,10.91;11,112.66,371.50,240.15,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="11,329.08,344.40,176.90,10.91;11,112.66,357.95,181.87,10.91">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,317.15,357.95,188.84,10.91;11,112.66,371.50,142.26,10.91">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,385.05,393.60,10.91;11,112.66,398.60,146.44,10.91" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00625</idno>
		<title level="m" coord="11,226.41,385.05,247.55,10.91">Med3D: Transfer learning for 3D Medical Image Analysis</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,412.15,393.33,10.91;11,112.66,425.70,393.32,10.91;11,112.66,439.25,391.93,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="11,403.92,412.15,102.07,10.91;11,112.66,425.70,303.48,10.91">Adamp: Slowing down the slowdown for momentum optimizers on scale-invariant weights</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,439.52,425.70,66.46,10.91;11,112.66,439.25,289.23,10.91">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,452.79,395.01,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="11,164.69,452.79,146.97,10.91">Least squares quantization in pcm</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">P</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,320.87,452.79,103.87,10.91">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="129" to="136" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,466.34,395.17,10.91;11,112.39,479.89,393.60,10.91;11,112.66,493.44,267.34,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="11,203.89,466.34,303.94,10.91;11,112.39,479.89,30.70,10.91">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,166.16,479.89,339.83,10.91;11,112.66,493.44,46.28,10.91">Proceedings of the fifth Berkeley symposium on mathematical statistics and probability</title>
		<meeting>the fifth Berkeley symposium on mathematical statistics and probability<address><addrLine>Oakland, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1967">1967</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,506.99,395.17,10.91;11,112.66,520.54,395.01,10.91;11,112.41,534.09,38.81,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="11,259.74,506.99,203.38,10.91">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,488.38,506.99,19.45,10.91;11,112.66,520.54,347.24,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,547.64,393.33,10.91;11,112.39,561.19,366.29,10.91" xml:id="b23">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m" coord="11,370.89,547.64,135.10,10.91;11,112.39,561.19,183.23,10.91">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,574.74,393.33,10.91;11,112.66,588.29,394.53,10.91;11,112.66,601.84,103.61,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="11,346.64,574.74,159.35,10.91;11,112.66,588.29,67.28,10.91">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,228.08,588.29,274.55,10.91">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,615.39,393.33,10.91;11,112.66,628.93,395.01,10.91;11,112.66,642.48,127.84,10.91" xml:id="b25">
	<monogr>
		<title level="m" type="main" coord="11,362.14,615.39,143.84,10.91;11,112.66,628.93,112.27,10.91">On the variance of the adaptive learning rate and beyond</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1908.03265" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,656.03,394.53,10.91;11,112.66,669.58,393.49,10.91" xml:id="b26">
	<monogr>
		<title level="m" type="main" coord="11,289.22,656.03,213.46,10.91">Lookahead optimizer: k steps forward, 1 step back</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1907.08610" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
