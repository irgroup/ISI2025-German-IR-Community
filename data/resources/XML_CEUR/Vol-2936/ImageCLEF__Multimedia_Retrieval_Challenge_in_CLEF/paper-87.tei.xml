<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,405.59,15.42;1,88.71,106.66,389.29,15.42;1,89.29,128.58,129.60,15.43">Overview of the VQA-Med Task at ImageCLEF 2021: Visual Question Answering and Generation in the Medical Domain</title>
				<funder>
					<orgName type="full">U.S. National Library of Medicine, National Institutes of Health</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.87,156.89,89.25,11.96"><forename type="first">Asma</forename><forename type="middle">Ben</forename><surname>Abacha</surname></persName>
							<email>asma.benabacha@nih.gov</email>
							<affiliation key="aff0">
								<orgName type="institution">National Library of Medicine</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,190.76,156.89,80.81,11.96"><forename type="first">Mourad</forename><surname>Sarrouti</surname></persName>
							<email>mourad.sarrouti@nih.gov</email>
							<affiliation key="aff0">
								<orgName type="institution">National Library of Medicine</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,284.21,156.89,114.66,11.96"><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Library of Medicine</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,411.52,156.89,74.31,11.96"><forename type="first">Sadid</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
							<email>sadidhasan@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">CVS Health</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,113.26,170.84,78.20,11.96"><forename type="first">Henning</forename><surname>Müller</surname></persName>
							<email>henning.mueller@hevs.ch</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Applied Sciences Western Switzerland (HES-SO)</orgName>
								<address>
									<settlement>Sierre</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,405.59,15.42;1,88.71,106.66,389.29,15.42;1,89.29,128.58,129.60,15.43">Overview of the VQA-Med Task at ImageCLEF 2021: Visual Question Answering and Generation in the Medical Domain</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">A8E7BB1BA0E0B875D877ED0F08086324</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visual Question Answering</term>
					<term>Visual Question Generation</term>
					<term>Data Creation</term>
					<term>Radiology Images</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents an overview of the fourth edition of the Medical Visual Question Answering (VQA-Med) task at ImageCLEF 2021. VQA-Med 2021 includes a task on Visual Question Answering (VQA), where participants are tasked with answering questions from the visual content of radiology images, and a second task on Visual Question Generation (VQG), consisting of generating relevant questions about radiology images. Thirteen teams participated in VQA-Med 2021 and submitted a total of 75 runs. The best teams achieved a BLEU score of 0.416 in the VQA task and 0.383 in the VQG task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual Question Answering is a challenging and promising problem that combines natural language processing (NLP) and computer vision (CV) techniques. With the increasing interest in artificial intelligence (AI) technologies to support clinical decision making and improve patient engagement, opportunities to generate and leverage algorithms for automated medical image interpretation are being explored at a faster pace. To offer more training data and evaluation benchmarks, we organized the first visual question answering (VQA) task in the medical domain in 2018 <ref type="bibr" coords="1,123.91,500.73,11.43,10.91" target="#b0">[1]</ref>, and continued the task in 2019 <ref type="bibr" coords="1,280.94,500.73,12.84,10.91" target="#b1">[2]</ref> and 2020 <ref type="bibr" coords="1,338.67,500.73,11.43,10.91" target="#b2">[3]</ref>.</p><p>Following the strong engagement from the research community in the previous editions of VQA in the medical domain (VQA-Med), we continued the task this year within the scope of ImageCLEF 2021 <ref type="bibr" coords="1,168.34,541.37,11.58,10.91" target="#b3">[4]</ref>, with a focus on answering questions about abnormalities in radiology images. In this edition, we also organized a second task on visual question generation (VQG), consisting of generating relevant natural language questions about radiology images based on their visual content 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task Description</head><p>The two VQA-Med tasks can be described more precisely as follows:</p><p>• Visual question answering (VQA) <ref type="foot" coords="2,277.72,132.73,4.10,7.13" target="#foot_0">2</ref> : given a radiology image accompanied by a relevant question, participating systems in VQA-Med 2021 were tasked with answering the question based on the visual image content. • Visual question generation (VQG) <ref type="foot" coords="2,283.31,174.73,4.10,7.13" target="#foot_1">3</ref> : given a radiology image, participating systems were tasked with generating relevant natural language questions about the abnormality present in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data Creation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">VQA Data</head><p>For the visual question answering task, we automatically constructed the training, validation, and test sets by: (i) applying several filters to select relevant images and associated annotations, and, (ii) creating patterns to generate the questions and their answers. We selected relevant medical images from the MedPix<ref type="foot" coords="2,236.86,321.38,3.71,7.97" target="#foot_2">4</ref> database with filters based on their captions, localities and diagnosis methods. We selected only the cases where the diagnosis was made based on the image. Finally, we considered the most frequent abnormality question categories to create the data set, which included a training set of 4,500 radiology images with 4,500 question-answer (QA) pairs (the same dataset used in 2020), a new validation set of 500 radiology images with 500 QA pairs, and a new test set of 500 radiology images with 500 questions about Abnormality.</p><p>To further ensure the quality of the data, the reference answers of the test set were manually validated by a medical doctor. Figure <ref type="figure" coords="2,262.39,417.98,5.17,10.91" target="#fig_0">1</ref> presents examples from the VQA 2021 test set. The participants were also encouraged to utilize the VQA-Med 2019 and 2020 datasets as additional training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">VQG Data</head><p>For the visual question generation task, we constructed the validation and test sets semiautomatically. First, we generated questions automatically from the images and their captions using two different approaches. In a first approach, we used only the image and a variational autoencoder model called VQGR <ref type="bibr" coords="2,236.33,535.45,12.80,10.91" target="#b4">[5]</ref> trained on the VQA-RAD dataset <ref type="bibr" coords="2,400.27,535.45,12.79,10.91" target="#b5">[6]</ref> (A CNN was used to encode the images and an LSTM to decode the questions). The second approach used a T5-based model fine-tuned on the SQuAD and MS MARCO datasets to generate questions from the image captions. Then, a medical doctor curated the list of automatically created questions. The final curated corpus for the VQG task was comprised of 85 radiology images with 200 questions for validation and 100 radiology images with 302 reference questions for the test set. Figure <ref type="figure" coords="2,500.89,603.20,5.09,10.91" target="#fig_2">2</ref> presents examples from the VQG 2021 test set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Submitted Runs</head><p>Out of 48 online registrations, 33 participants submitted signed end user agreement forms, and 13 teams submitted a total of 75 successful runs; including 68 runs for the VQA task and 7 runs for the VQG task. Table <ref type="table" coords="3,219.55,627.60,5.06,10.91">1</ref> gives an overview of all participating teams and the number of submitted runs (only 10 runs were allowed per team).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Participating groups in the VQA-Med 2021 tasks.</p><p>Team Institution # Valid Runs Yunnan <ref type="bibr" coords="4,156.84,130.53,11.83,8.87" target="#b6">[7]</ref> Yunnan University (China) 10 SYSU-HCP <ref type="bibr" coords="4,170.82,142.49,11.83,8.87" target="#b7">[8]</ref> School of Computer Science and Engineering, Sun Yat-sen University (China) 10 TAM <ref type="bibr" coords="4,145.17,166.40,11.83,8.87" target="#b8">[9]</ref> South China Normal University (China) 10 TeamS <ref type="bibr" coords="4,152.32,178.35,16.46,8.87" target="#b9">[10]</ref> D4L data4life gGmbH&amp;Hasso Plattner Institute (Germany) 10 jeanbenoit_delbrouck Stanford University (USA) 10 sheerin <ref type="bibr" coords="4,155.19,214.22,16.46,8.87" target="#b10">[11]</ref> Siva Subramaniya Nadar College of Engineering (India) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>Maximum Average BLEU Scores for the VQG Task (out of each team's submitted runs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team</head><p>Average BLEU Chabbiimen 0.383 Baseline 0.274</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>Similar to the evaluation setup of the VQA-Med 2020 challenge <ref type="bibr" coords="5,389.13,111.28,11.58,10.91" target="#b2">[3]</ref>, the evaluation of the participant systems for the VQA task in VQA-Med 2021 is also conducted based on two primary metrics: accuracy and BLEU. We used an adapted version of accuracy from VQA in the open domain <ref type="foot" coords="5,122.11,150.17,3.71,7.97" target="#foot_3">5</ref> that relies on an exact matching between a participant provided answer and the ground truth answer. To compensate for the strictness of the accuracy metric, BLEU <ref type="bibr" coords="5,441.51,165.48,18.07,10.91" target="#b14">[15]</ref> is used to capture the word overlap-based similarity between a system-generated answer and the ground truth answer. The overall methodology and resources for the BLEU metric are essentially similar to last year's VQA task. The BLEU metric is also used to evaluate the submissions for the VQG task to compute an overlap-based average similarity score between the system-generated questions and the ground truth question for each given test image <ref type="foot" coords="5,383.43,231.47,3.71,7.97" target="#foot_4">6</ref> . We prepared three baseline systems for the VQA and VQG tasks. Our VQA baselines are based on a multi-class image classification approach using ResNet50 (baseline 1) and a variational autoencoder model (baseline 2) trained on the VQA-Med data <ref type="bibr" coords="5,366.97,273.87,16.30,10.91" target="#b15">[16]</ref>. Our VQG baseline system relies on a variational autoencoder model trained on the VQA-RAD and VQA-Med datasets <ref type="bibr" coords="5,492.47,287.42,11.35,10.91" target="#b4">[5]</ref>.</p><p>The overall results of the participating systems and our baselines are presented in Table <ref type="table" coords="5,500.82,300.97,5.17,10.91" target="#tab_0">2</ref> and Table <ref type="table" coords="5,135.26,314.52,5.07,10.91">3</ref> in descending accuracy order and average BLEU scores, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>The results in Table <ref type="table" coords="5,180.54,373.14,5.13,10.91" target="#tab_0">2</ref> show that participating systems performed relatively well for the VQA task, in comparison with the VQG results, presented in Table <ref type="table" coords="5,365.52,386.69,3.77,10.91">3</ref>, and suggesting that the VQG task was more challenging. However, the participating systems achieved better BLEU scores compared to last year's VQG results <ref type="bibr" coords="5,254.75,413.79,11.43,10.91" target="#b2">[3]</ref>.</p><p>The participants' approaches relied on state-of-the-art deep learning techniques for the VQA and VQG tasks. Most systems used Convolutional Neural Networks (CNNs) for visual feature extraction such as VGGNet, ResNet, and DenseNet. Long-short-term memory (LSTM) networks and Transformer-based models (e.g. BERT, BioBERT) were used to extract question features. Several pooling strategies were explored such as multimodal factorized bilinear "MFB" pooling or multi-modal factorized high-order "MFH" pooling to combine image and question features and generate the answer (e.g. <ref type="bibr" coords="5,222.72,508.64,11.36,10.91" target="#b6">[7,</ref><ref type="bibr" coords="5,236.80,508.64,7.30,10.91" target="#b8">9]</ref>).</p><p>Participating teams also applied various attention mechanisms and ensemble methods. For instance, the SYSU-HCP team <ref type="bibr" coords="5,226.61,535.73,12.99,10.91" target="#b7">[8]</ref> designed a hierarchical feature extraction structure to capture multi-scale features of radiology images and replaced the fully-connected layers with hierarchical adaptive global average pooling layers. For training, they used three techniques: data augmentation, curriculum learning, and label smoothing. Their final system relied on a multi-architecture ensemble combining the output of eight models and achieving the best accuracy of 0.382 and BLEU score of 0.416.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we presented the ImageCLEF VQA-Med 2021 tasks and official results. We created new datasets for the visual question generation and visual question answering tasks with a more pronounced focus on questions about abnormality. For the VQG task, we explored the use of Deep Learning and Transformer-based models for semi-automatic question generation from the images and their captions. The VQA-Med task attracted high participation in ImageCLEF 2021. The best VQA team achieved 0.416 BLEU score and 0.382 accuracy. For the VQG task, the best BLEU score is 0.383, outperforming the results achieved last year. We hope that these VQA and VQG datasets will encourage further research efforts in multimodal architectures and approaches for radiology image understanding.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,538.59,241.65,8.93;3,287.76,336.16,153.07,143.89"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples from the test set of the VQA 2021 Task</figDesc><graphic coords="3,287.76,336.16,153.07,143.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,146.02,274.92,144.49,9.07;8,157.86,287.01,132.90,8.93;8,157.86,299.02,132.65,8.87;8,157.86,310.92,134.31,8.93;8,157.86,322.93,132.65,8.87;8,157.86,334.83,134.31,8.93;8,157.86,346.79,132.65,8.93;8,157.86,358.79,134.32,8.87;8,157.86,370.75,26.35,8.87;8,310.17,274.92,138.82,9.07;8,322.55,287.01,126.44,8.93;8,322.55,299.02,126.44,8.87;8,322.55,310.92,126.44,8.93;8,322.55,322.88,127.54,8.93;8,322.55,334.88,126.43,8.87;8,322.55,346.84,126.43,8.87;8,322.55,358.74,126.44,8.93;8,322.55,370.70,126.43,8.93;8,322.55,382.70,128.10,8.87;8,322.55,394.66,78.41,8.87;8,144.60,595.78,144.49,9.07;8,156.14,607.87,132.96,8.93;8,156.14,619.87,132.95,8.87;8,156.14,631.78,132.95,8.93;8,156.14,643.74,132.95,8.93;8,156.14,655.69,132.95,8.93;8,156.14,667.70,109.43,8.87;8,308.75,595.78,143.32,9.07;8,320.99,607.87,131.08,8.93;8,320.99,619.82,129.41,8.93;8,320.99,631.83,95.42,8.87"><head></head><label></label><figDesc>(a) Q1: What lesion is seen in the mediastinum? Q2: Are there any calcifications in the mediastinal mass? Q3: Where is the hypodensity consistent with necrosis seen? Q4: Are there any enlarged lymph nodes? Q5: Where is an enlarged lymph node located? (b) Q1: Where are the exophytic lesions located? Q2: What lesions affect the femur and tibia? Q3: Do the lesions involve the knee joint? Q4: Do the lesions demonstrate medullary continuity with the bone of origin? Q5: Is the fibula deformed? Q6: For what disorder are these multiple exostoses diagnostic? (c) Q1: What causes proptosis of the right eye? Q2: What kind of lesion is present in the right orbit? Q3: Are the optic nerves and muscles involved? Q4: Is the mass homogeneous? Q5: What is the lesion suggestive of? (d) Q1: Where is the thrombus located? Q2: Where is collateralization demonstrated? Q3: Is the thecal sac effected?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,89.29,685.24,245.28,8.93"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Examples from the Test Set of the VQG 2021 Task</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.99,214.22,418.53,341.10"><head>Table 2</head><label>2</label><figDesc>Maximum Accuracy and Maximum BLEU Scores for the VQA Task (out of each team's submitted runs).</figDesc><table coords="4,425.86,214.22,4.63,8.87"><row><cell>5</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,108.93,649.10,246.78,8.97"><p>https://www.aicrowd.com/challenges/imageclef-2021-vqa-med-vqa</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="2,108.93,660.06,247.17,8.97"><p>https://www.aicrowd.com/challenges/imageclef-2021-vqa-med-vqg</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="2,108.93,671.02,102.35,8.97"><p>https://medpix.nlm.nih.gov/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="5,108.93,660.01,131.42,8.97"><p>https://visualqa.org/evaluation.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="5,108.93,670.96,259.81,8.97"><p>https://github.com/abachaa/VQA-Med-2021/tree/main/EvaluationCode</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was partially supported by the intramural research program at the <rs type="funder">U.S. National Library of Medicine, National Institutes of Health</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,112.66,350.47,393.33,10.91;6,112.66,364.02,395.17,10.91;6,112.66,377.57,394.53,10.91;6,112.66,391.12,22.69,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,401.26,350.47,104.73,10.91;6,112.66,364.02,235.24,10.91">Overview of imageclef 2018 medical domain visual question answering task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lungren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,370.80,364.02,137.03,10.91;6,112.66,377.57,203.04,10.91">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14, 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,404.67,394.61,10.91;6,112.66,418.22,393.32,10.91;6,112.66,431.77,394.52,10.91;6,112.66,445.32,389.87,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,465.37,404.67,41.90,10.91;6,112.66,418.22,309.52,10.91">Vqa-med: Overview of the medical visual question answering task at imageclef</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="6,466.54,418.22,39.45,10.91;6,112.66,431.77,294.32,10.91">Working Notes of CLEF 2019 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="6,282.38,445.32,151.15,10.91">CEUR Workshop Proceedings, CEUR</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09-09">2019. September 9-12, 2019. 2019</date>
			<biblScope unit="volume">2380</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,458.87,393.33,10.91;6,112.39,472.42,393.60,10.91;6,112.66,485.97,394.53,10.91;6,112.33,499.52,120.27,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,434.73,458.87,71.26,10.91;6,112.39,472.42,393.60,10.91;6,112.66,485.97,31.38,10.91">Overview of the vqa-med task at imageclef 2020: Visual question answering and generation in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,171.48,485.97,118.94,10.91">CLEF 2020 Working Notes</title>
		<title level="s" coord="6,298.64,485.97,180.76,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,513.06,395.01,10.91;6,112.66,526.61,394.53,10.91;6,112.48,540.16,395.18,10.91;6,112.66,553.71,394.53,10.91;6,112.28,567.26,393.70,10.91;6,112.66,580.81,393.33,10.91;6,112.66,594.36,393.33,10.91;6,112.66,607.91,393.53,10.91;6,112.66,621.46,197.61,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,264.32,567.26,241.66,10.91;6,112.66,580.81,261.75,10.91">Overview of the ImageCLEF 2021: Multimedia retrieval in medical, nature, internet and social media applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Peteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tauteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Moustahfid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,401.27,580.81,104.72,10.91;6,112.66,594.36,393.33,10.91;6,112.66,607.91,201.15,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 12th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="6,348.61,607.91,157.57,10.91;6,112.66,621.46,31.10,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,635.01,393.33,10.91;6,112.66,648.56,393.33,10.91;7,112.30,86.97,394.97,10.91;7,112.66,100.52,218.26,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,355.67,635.01,150.32,10.91;6,112.66,648.56,75.01,10.91">Visual question generation from radiology images</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.alvr-1.3" />
	</analytic>
	<monogr>
		<title level="m" coord="6,211.28,648.56,294.70,10.91;7,112.30,86.97,259.39,10.91">Proceedings of the First Workshop on Advances in Language and Vision Research, Association for Computational Linguistics</title>
		<meeting>the First Workshop on Advances in Language and Vision Research, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="12" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,114.06,393.32,10.91;7,112.39,127.61,394.88,10.91;7,112.66,141.16,209.17,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,363.98,114.06,142.01,10.91;7,112.39,127.61,245.61,10.91">A dataset of clinically generated visual questions and answers about radiology images</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gayen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<ptr target="https://www.nature.com/articles/sdata2018251" />
	</analytic>
	<monogr>
		<title level="j" coord="7,371.26,127.61,67.06,10.91">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,154.71,393.33,10.91;7,112.66,168.26,378.26,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,266.44,154.71,239.55,10.91;7,112.66,168.26,203.15,10.91">Yunnan university at vqa-med 2021: Pretrained biobert for medical domain visual question answering</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,339.25,168.26,105.91,10.91">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">201</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,181.81,393.33,10.91;7,112.66,195.36,393.33,10.91;7,112.66,208.91,81.54,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,269.50,181.81,236.49,10.91;7,112.66,195.36,303.14,10.91">Sysu-hcp at vqa-med 2021: A data-centric model with efficient training methodology for medical visual question answering</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,439.11,195.36,66.88,10.91;7,112.66,208.91,35.78,10.91">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">201</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,222.46,393.32,10.91;7,112.66,236.01,372.57,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,211.65,222.46,71.91,10.91;7,312.28,222.46,193.71,10.91;7,112.66,236.01,197.45,10.91">A hybrid model with feature extraction and fusion for medical visual question answering</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,333.56,236.01,105.91,10.91">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="volume">201</biblScope>
		</imprint>
	</monogr>
	<note>Tam at vqa-med</note>
</biblStruct>

<biblStruct coords="7,112.66,249.56,393.32,10.91;7,112.66,263.11,326.38,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,262.57,249.56,243.41,10.91;7,112.66,263.11,151.27,10.91">Teams at vqa-med 2021: Bbn-orchestra for long-tailed medical visual question answering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Meinel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,287.37,263.11,105.91,10.91">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">201</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,276.66,393.61,10.91;7,112.66,290.20,349.05,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,256.01,276.66,250.25,10.91;7,112.66,290.20,174.58,10.91">Imageclef 2021: An approach for vqa to solve abnormality related queries using improved datasets</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S N</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,310.04,290.20,105.91,10.91">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">201</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,303.75,393.33,10.91;7,112.39,317.30,393.86,10.91;7,112.66,330.85,43.04,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,305.07,303.75,200.92,10.91;7,112.39,317.30,261.71,10.91">Puc chile team at vqa-med 2021: approaching vqa as a classfication task via fine-tuning a pretrained cnn</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schilling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Messina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,398.15,317.30,108.10,10.91">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">201</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,344.40,393.32,10.91;7,112.66,357.95,393.33,10.91;7,112.66,371.50,109.97,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,249.54,344.40,256.44,10.91;7,112.66,357.95,329.57,10.91">Regim lab at vqa-med 2021: Visual generation of relevant natural language questions from radiology images for anomaly detection</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Chebbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Feki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B</forename><surname>Amar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,466.22,357.95,39.77,10.91;7,112.66,371.50,64.21,10.91">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">201</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,385.05,393.33,10.91;7,112.66,398.60,327.06,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="7,172.97,385.05,141.56,10.91;7,346.18,385.05,159.81,10.91;7,112.66,398.60,152.19,10.91">Attention model based on efficient interaction between multimodality</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,288.06,398.60,105.91,10.91">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="volume">201</biblScope>
		</imprint>
	</monogr>
	<note>Lijie at imageclefmed vqa-med</note>
</biblStruct>

<biblStruct coords="7,112.66,412.15,393.33,10.91;7,112.66,425.70,393.53,10.91;7,112.66,439.25,391.67,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="7,315.61,412.15,190.38,10.91;7,112.66,425.70,101.36,10.91">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,239.38,425.70,266.81,10.91;7,112.66,439.25,111.02,10.91">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,452.79,393.33,10.91;7,112.66,466.34,128.86,10.91" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="7,174.28,452.79,74.56,10.91;7,280.55,452.79,225.44,10.91;7,112.66,466.34,68.07,10.91">Visual question answering and generation in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
	<note>Nlm at vqa-med</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
