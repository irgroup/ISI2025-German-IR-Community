<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,376.86,15.42;1,89.29,106.66,154.73,15.42">Improving web user interface element detection using Faster R-CNN</title>
				<funder ref="#_tfFFXQe">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_9nk4NXY">
					<orgName type="full">University of West Bohemia</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.10,134.97,59.45,11.96"><forename type="first">Jiří</forename><surname>Vyskočil</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Cybernetics</orgName>
								<orgName type="department" key="dep2">Faculty of Applied Sciences</orgName>
								<orgName type="institution">University of West Bohemia</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,161.20,134.97,58.19,11.96"><forename type="first">Lukáš</forename><surname>Picek</surname></persName>
							<email>picekl@ntis.zcu.cz</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Cybernetics</orgName>
								<orgName type="department" key="dep2">Faculty of Applied Sciences</orgName>
								<orgName type="institution">University of West Bohemia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,376.86,15.42;1,89.29,106.66,154.73,15.42">Improving web user interface element detection using Faster R-CNN</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">04BA0675B9995776C84F5E4C2EB66548</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Object Detection</term>
					<term>Machine Learning</term>
					<term>Edge Detection</term>
					<term>Faster R-CNN</term>
					<term>FPN</term>
					<term>CNN</term>
					<term>User Interface</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Several challenges may arise when designing new user interfaces (UIs), e.g., because of communication between designers and developers, to which the detection of UI elements can help. The ImageCLEF DrawnUI 2021 challenge builds on the detection of such elements in two contest tasks: a Screenshot task that contains the website screenshot images with lots of noisy data, and a Wireframe task for detecting UI elements from hand-drawn proposals. This paper describes a simple algorithm based on the edge detection to filter noisy data from the website screenshots, and machine learning method which scored the first place in both tasks while having 0.628 and 0.900 mAP at 0.5 IoU in the Screenshot and Wireframe tasks. This method is based on the Faster R-CNN with a Feature Pyramid Network (FPN) that uses selected aspect ratios of anchor boxes according to the occurrences from the datasets. The code is available at https://github.com/vyskocj/ImageCLEFdrawnUI2021</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The ImageCLEF DrawnUI challenge <ref type="bibr" coords="1,254.60,401.50,12.99,10.91" target="#b0">[1]</ref> was organized as part of the ImageCLEF 2021 workshop <ref type="bibr" coords="1,112.81,415.05,13.00,10.91" target="#b1">[2]</ref> at the CLEF conference. The main goal for the two proposed tasks -Screenshots &amp; Wireframes -was to create a system capable of automatic detection and recognition of individual user interface (UI) elements on given images. The Screenshot task focused on the website screenshot images, and the Wireframe task targeted on hand-drawn UI drawings. The motivation for both tasks is to simplify and speed up the Web development process by giving the designers a tool that can visualize the website immediately based on their hand-drawn sketches.</p><p>The machine learning techniques have already been applied to the hand-drawn UI elements detection in the last years. Gupta et al. <ref type="bibr" coords="1,271.77,509.89,12.99,10.91" target="#b2">[3]</ref> used Mask R-CNN <ref type="bibr" coords="1,375.21,509.89,13.00,10.91" target="#b3">[4]</ref> and Multi-Pass Inference technique to boost the viability of the model by passing the input image (without the already detected objects) to the model several times. Narayanan et al. <ref type="bibr" coords="1,363.29,536.99,12.82,10.91" target="#b4">[5]</ref> explored Cascade R-CNN <ref type="bibr" coords="1,493.17,536.99,12.82,10.91" target="#b5">[6]</ref> and YOLOv4 <ref type="bibr" coords="1,146.58,550.54,12.68,10.91" target="#b6">[7]</ref> architectures, and Zita et al. <ref type="bibr" coords="1,285.03,550.54,12.69,10.91" target="#b7">[8]</ref> used regular Faster R-CNN <ref type="bibr" coords="1,419.50,550.54,12.68,10.91" target="#b8">[9]</ref> architecture and advanced regularization techniques for training the model. In this work, we utilize the Faster R-CNN extended by the Feature Pyramid Network (FPN) <ref type="bibr" coords="1,346.53,577.64,18.02,10.91" target="#b9">[10]</ref> that builds high-level semantic feature maps at all selected scales and makes the predictions more accurate. The models were implemented and fine-tuned using the Detectron2 API <ref type="bibr" coords="1,329.53,604.74,17.79,10.91" target="#b10">[11]</ref> from publicly available checkpoints pre-trained on the COCO dataset <ref type="bibr" coords="2,241.68,86.97,16.41,10.91" target="#b11">[12]</ref>. Additionally, we improved the performance by using various augmentations, i.e., Relative Random Resize, Cutout <ref type="bibr" coords="2,371.76,100.52,16.41,10.91" target="#b12">[13]</ref>, brightness and contrast adjustment, and by selecting bounding box proposals. In case of the Screenshot task, we utilized the use of data filtering algorithm based on the edge detection <ref type="bibr" coords="2,370.47,127.61,16.50,10.91" target="#b13">[14,</ref><ref type="bibr" coords="2,389.69,127.61,12.38,10.91" target="#b14">15]</ref>. The improvements of our method, which won in both contest tasks, are shown by comparing with the others in the benchmark of the DrawnUI challenge.</p><p>Besides, we experimented with novel methods <ref type="bibr" coords="2,308.65,168.26,16.44,10.91" target="#b15">[16,</ref><ref type="bibr" coords="2,327.82,168.26,14.04,10.91" target="#b16">17]</ref> based on the Detection Transformer. These approaches remove the need for hand-designed components, e.g., non-maxima suppression (NMS), but requires much more training time to convergence than previous detectors. Given this training issue of the Detection Transformer, we decided to keep the NMS in our model. Using the Transformers on the provided data in the contest tasks led to significantly worse detection performance even with 7.5× more training steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Challenge datasets</head><p>Wireframe task. Provided dataset is a combination of 4,291 hand-drawn high-resolution image templates. The data is divided into 3,218 images for the development and 1,073 images for the testing. For each image in the development set, we have manual annotations with the bounding boxes and their corresponding labels from pre-defined 21 classes. The development set includes all images from last year's challenge and additional images to re-balance the class distribution. As there is no official training/validation split provided, we did a random 85%/15% split. The detailed statistics covering the class distribution, dataset split, and absolute/relative box number are presented in Table <ref type="table" coords="2,246.02,389.48,3.74,10.91" target="#tab_0">1</ref>. Screenshot task. In the Screenshot task, the provided dataset includes 9,630 full-page screenshots of websites in several languages. The data comes with labeled bounding boxes of the UI elements. A total of 6 classes is defined, the distribution of ground truth boxes can be found in Table <ref type="table" coords="2,127.38,443.67,3.80,10.91" target="#tab_1">2</ref>. The development set contains 6,840 training images, and 930 manually annotated validation images. The training set includes noisy data: blank images and bounding boxes with shifted positions. The testing set contained a total of 1,860 samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we cover the noise data filtering algorithm and training the Faster R-CNN <ref type="bibr" coords="2,493.04,529.40,12.95,10.91" target="#b8">[9]</ref> detection network based on the ResNet-50 <ref type="bibr" coords="2,275.99,542.95,17.76,10.91" target="#b17">[18]</ref> backbone. We also use the FPN <ref type="bibr" coords="2,434.50,542.95,17.75,10.91" target="#b9">[10]</ref> extractor to combine semantically strong features thanks to a top-down pathway and lateral connections from the same spatial size. We use SGD optimizer with momentum of 0.9 <ref type="bibr" coords="2,424.22,570.05,16.41,10.91" target="#b18">[19]</ref>, learning rate warm up of the first epoch reached the value of 0.0025 and a smooth L 1 <ref type="bibr" coords="2,423.91,583.60,18.07,10.91" target="#b19">[20]</ref> is applied for regression loss. The detector is implemented and fine-tuned in the Detectron2 API <ref type="bibr" coords="2,463.41,597.15,18.04,10.91" target="#b10">[11]</ref> from publicly available pre-trained weights on the COCO dataset <ref type="bibr" coords="2,362.47,610.69,16.41,10.91" target="#b11">[12]</ref>. For more details about the hyperparameter settings, see Table <ref type="table" coords="2,250.27,624.24,3.81,10.91" target="#tab_2">3</ref>, and advanced augmentations are listed in Table <ref type="table" coords="2,480.54,624.24,3.81,10.91" target="#tab_3">4</ref>. All experiments are evaluated at mean average precision (mAP) and mean average recall (mAR) with Intersection over Union (IoU) in range of 0.5 to 0.95 with the increment of 0.05, and mean average precision with IoU greater than 0.5 (denoted as mAP0.5). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Baseline experiment</head><p>For the baseline experiment, the Random Relative Resize augmentation is applied to resize an image to 70-90% of its size and crop it to a maximum of 1,400 px to limit the memory usage. The resize augmentations are deeply examined in Section 3.3.1. The hyperparameters settings are described in Table <ref type="table" coords="3,190.91,591.96,5.17,10.91" target="#tab_2">3</ref> and advanced augmentations in Table <ref type="table" coords="3,373.42,591.96,3.81,10.91" target="#tab_3">4</ref>. In the Screenshot task, the baseline model reached 0.592 mAP0.5, 0.404 mAP, and 0.603 mAR, while in the Wireframe task, a model with the same settings have 0.969 mAP0.5, 0.703 mAP, and 0.763 mAR. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Filtering noisy data in the Screenshot task</head><p>Even though the noisy data can be effective for the training <ref type="bibr" coords="4,350.13,359.66,16.08,10.91" target="#b20">[21]</ref>, we decided to analyze filtering of blank images and wrongly annotated bounding boxes from the Screenshot task dataset.</p><p>The aim is to remove images or ground truth boxes that contain constant color intensity. For this reason, the data filtering (shown in Algorithm 1) is based on an edge detector <ref type="bibr" coords="4,448.52,400.30,16.33,10.91" target="#b13">[14,</ref><ref type="bibr" coords="4,467.57,400.30,13.97,10.91" target="#b14">15]</ref> to be independent of the intensity of the pixels in the input image. To verify the efficiency of data filtering, we manually selected appropriate thresholds for images (see Table <ref type="table" coords="4,171.56,669.58,4.25,10.91" target="#tab_5">5</ref>) and a set of fixed thresholds from 0.2 to 1.8 for bounding boxes. Then we trained the network with filtered annotations in the Screenshot dataset using the same settings as in Section 3.1. For the results of this experiment, see Table <ref type="table" coords="5,400.43,100.52,3.76,10.91" target="#tab_6">6</ref>. One can observe that filtering the homogeneous images increases the mAP0.5 by 0.012, mAP by 0.008, and mAR by 0.009 compared to the case of the original set. Filtering homogeneous images and bounding boxes also increases the detection performance but it detects less precisely in all tested cases of bounding box thresholds than filtering only the images. This behaviour can be caused by eliminating the training data, which is in fact an object, not noise. Therefore, we determined a new baseline for the Screenshot task by filtering only the images from the training set (this model was submitted as a baseline for the Screenshot task of DrawnUI challenge <ref type="bibr" coords="5,472.22,195.36,11.09,10.91" target="#b0">[1]</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Augmentations</head><p>Image resizing, Cutout <ref type="bibr" coords="5,195.00,525.46,18.07,10.91" target="#b12">[13]</ref> augmentation, and color spaces are tested to improve detection performance. The improvement is evaluated as a comparison with baseline models defined in the previous sections, i.e., Section 3.1 is relevant for the Wireframe task and for the Screenshot task, a new baseline was established in the Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Image resize</head><p>The basic approach to dealing with various sizes of the input images is to resize it to the desired constant value so that the original aspect ratio is kept. However, various sizes can help the learning algorithm to detect objects at different stages of the network. For example, imagine that we only have small boxes available for the category button in the training set. In the test stage, this network will not expect a large button at the input and will most likely fail to detect it. In order to use different input image sizes, the backbone network must not contain any fully connected layer. Two types of resizing images are compared in Table <ref type="table" coords="6,337.79,114.06,3.81,10.91" target="#tab_7">7</ref>. The first one, Resize Shortest Edge (default for Detectron2 <ref type="bibr" coords="6,189.45,127.61,17.76,10.91" target="#b10">[11]</ref> software) has a defined set of shortest edge lengths of the image from 640 to 800 px with the increment of 32, which are selected randomly during the training. If the longer edge is larger than 1,333 px, the shorter edge is underscaled so that the longer edge does not exceed this maximum size. We proposed the second type of resizing as a Random Relative Resize. It defines an interval for which the image is randomly resized, and a maximum length of the edges for cropping image during the training due to memory requirements. The particular aim of this augmentation is to keep the small boxes so that they do not disappear when the image size is reduced, and the network is able to detect them. In the test stage, the image is resized only by the middle value of the specified interval and no image cropping is applied. This augmentation proved to be most suitable for an image enlarging by a random value in the interval [0.6, 1.0] for both tasks, where mAP0.5 and mAP metrics are roughly ranging from 0.034 to 0.045 higher than when using the Resize Shortest Edge. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Cutout augmentation</head><p>To increase the performance of the network, in addition to brightness and resize augmentations, we also used Cutout <ref type="bibr" coords="6,178.14,509.87,17.76,10.91" target="#b12">[13]</ref> from Albumentations library <ref type="bibr" coords="6,325.33,509.87,17.76,10.91" target="#b21">[22]</ref> to randomly cuts boxes (denoted also as holes) from the image. This augmentation expects the number of maximum holes and their maximum spatial size as the input. In our experiments, we define the maximum size of holes in the percentage of the image. The results (see Table <ref type="table" coords="6,309.97,550.51,4.08,10.91" target="#tab_8">8</ref>) show that it can increase mAP0.5 by 0.008, and mAP by 0.004 for the Screenshot task when using 4 holes with max size of 5% of the image, while in the Wireframe task, the detection performance was slightly reduced in all settings of the Cutout augmentation. Even so, we applied this augmentation in our further research (see Section 3.4 and Section 4) to keep the experiments for both contest tasks comparable, and in the Screenshot task, the augmentation shows meaningful improvements. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.">Color space</head><p>In the next step, converting images to the greyscale, such as in the previous works of this challenge <ref type="bibr" coords="7,132.66,286.12,11.23,10.91" target="#b7">[8,</ref><ref type="bibr" coords="7,146.59,286.12,7.49,10.91" target="#b2">3]</ref>, is applied. It results in no improvement against the RGB images (see Table <ref type="table" coords="7,486.45,286.12,4.08,10.91" target="#tab_9">9</ref>) in the Screenshot task. On the other hand, for the Wireframe task, converting the data to grayscale yields up to approximately 0.005 greater mAP0.5, mAP, and mAR. Therefore, both RGB and grayscale images are used for the remaining experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Anchor box proposals</head><p>We followed up on previous experiments that examines augmentations (see Section 3.3) and we trained new models (parameters for new augmentations are summarized in Table <ref type="table" coords="7,444.55,520.54,7.74,10.91" target="#tab_10">10</ref>). After that we analyzed which aspect ratios of ground truth boxes are included in the datasets. Occurrence of such aspect ratios are visualized in Figure <ref type="figure" coords="7,284.89,547.64,4.98,10.91" target="#fig_0">1</ref> for both the Screenshot and the Wireframe tasks. One can observe that the horizontal boxes are far more frequent than the vertical ones. As a result, the appropriate aspect ratios were selected to generate the box proposals. We added one horizontal aspect ratio of 0.2 to the default ones (i.e., to the set of 0.5, 1, and 2). Then we selected aspect ratios of 0.1, 0.5, 1, and 1.5 according to the distribution from the Figure <ref type="figure" coords="7,445.99,601.84,3.80,10.91" target="#fig_0">1</ref>. Eventually, we also reduced the size of the anchors 2× for each output layer from the Feature Pyramid Network <ref type="bibr" coords="7,130.38,628.93,17.94,10.91" target="#b9">[10]</ref> of ResNet <ref type="bibr" coords="7,196.06,628.93,17.93,10.91" target="#b17">[18]</ref> backbone, i.e., reduced size for semantic feature maps from 𝑃 2 to 𝑃 6 , see Table <ref type="table" coords="7,148.89,642.48,10.15,10.91" target="#tab_11">11</ref> for a summary of these settings. An experiment examining the use of different aspect ratios for anchor box proposals (see Table <ref type="table" coords="7,115.37,669.58,9.11,10.91" target="#tab_12">12</ref>) shows that selecting aspect ratios by frequency in the dataset increases detection performance in most cases. Only for the Wireframe task with RGB images, the default aspect ratios achieved slightly greater mAP and mAR than the ones se lected from statistics. The value of mAP0.5 is greater for aspect ratios selected according to statistics with smaller anchor sizes, and this setting proved to be better performing for greyscale images roughly ranging from 0.002 to 0.003 for all measured metrics. Therefore, we selected this setting for comparison with the other backbone models in the Wireframe task. For the Screenshot task, the same aspect ratios were selected but with default sizes of anchor boxes, because it performed better with mAP0.5 and mAP up to 0.006 than the default anchor settings.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Backbones comparison</head><p>As a last step, several backbone architectures were compared for UI element detection on greyscale and RGB images. We used base parameters and augmentations from Table <ref type="table" coords="9,480.72,461.61,5.17,10.91" target="#tab_2">3</ref> and Table <ref type="table" coords="9,115.37,475.16,3.81,10.91" target="#tab_3">4</ref>, and additional augmentations described in Table <ref type="table" coords="9,350.40,475.16,8.53,10.91" target="#tab_10">10</ref>. In the Wireframe task, statistical + smaller sizes variant of the anchor generator was used, and the statistical variant was used for the Screenshot task (for these settings of anchor box proposals see Table <ref type="table" coords="9,428.24,502.26,7.90,10.91" target="#tab_11">11</ref>).</p><p>In the comparison of the backbone architectures (see Table <ref type="table" coords="9,373.71,515.81,8.06,10.91" target="#tab_13">13</ref>), the reader can recognise that only for the Wireframe task, the most complex compared architecture achieved better performance for measured metrics in both cases of selected color space. Although we expected better performance with the more complex ResNeXt-101 backbone, superior results were achieved with the ResNet-50 in the Screenshot task. The model with a complex backbone converges slower than ResNet-50, hence more epochs should be ran for better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Submissions</head><p>In the DrawnUI challenge <ref type="bibr" coords="9,209.53,642.18,11.59,10.91" target="#b0">[1]</ref>, we have created up to 9 submissions using the configuration listed bellow. The configuration is the same for both the Screenshot and the Wireframe tasks, any additional configurations relevant for any of the tasks are also specified. Results on the test   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Our method, including data filtering, Cutout augmentation and statistical aspect ratios for anchor box proposals, ended in the first place in both contest tasks of DrawnUI challenge: Screenshot task -ResNet-50 backbone trained on whole development set with 0.628 mAP at 0.5 IoU on the test set, and Wireframe task -ResNeXt-101 backbone trained with split development set for training and validation, this model achieved 0.900 mAP at 0.5 IoU on the test set. Besides, we explored the State-of-the-Art object detectors based on the transformers, such as a DETR. The DETR did not achieve satisfactory results even after 300 epochs compared with the Faster R-CNN trained up to 40 epochs. Due to time constraints, we will consider the use of transformer in the upcoming research projects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,89.29,601.29,416.94,8.93;8,89.29,613.30,36.23,8.87"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Occurrence of the aspect ratios of the bounding boxes in the Screenshot and Wireframe task datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="10,89.29,365.69,416.70,10.91;10,89.29,379.24,418.53,10.91;10,89.29,392.79,154.97,10.91;10,89.29,406.34,326.27,10.91;10,89.29,419.89,402.22,10.91;10,89.29,433.44,416.69,10.91;10,89.29,446.99,416.69,10.91;10,89.29,460.54,233.68,10.91"><head># 4 : 5 : 6 : 7 : 8 : 9 :</head><label>456789</label><figDesc>ResNet-50 (anchor settings, greyscale) -same as submission #3 but w/ greyscale images. #ResNet-50 (train+val, RGB) -same as submission #3 but trained on the whole development set (w/o any validation data). #ResNeXt-101 (RGB) -trained w/ the same settings as submission #3. #ResNet-50 (train+val, RGB, 2× epochs) -submission #5 trained for 2× more epochs. #ResNet-50 (train+val, greyscale) -same as submission #5 but trained w/ greyscale images. #ResNeXt-101 (RGB, train+val, +5 epochs) -submission #6 fine-tuned w/ 5 more epochs on whole development set (w/o any validation data).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.99,90.49,324.33,266.91"><head>Table 1</head><label>1</label><figDesc>Distribution of training and validation set categories of the Wireframe task.</figDesc><table coords="3,179.96,121.67,233.36,235.73"><row><cell></cell><cell></cell><cell>Wireframe task</cell><cell></cell><cell></cell></row><row><cell>Category</cell><cell cols="2">Training set # of boxes fraction [%]</cell><cell cols="2">Validation set # of boxes fraction [%]</cell></row><row><cell>button</cell><cell>21,787</cell><cell>19.33</cell><cell>3,657</cell><cell>17.80</cell></row><row><cell>label</cell><cell>17,348</cell><cell>15.39</cell><cell>3,462</cell><cell>16.85</cell></row><row><cell>paragraph</cell><cell>13,884</cell><cell>12.32</cell><cell>2,474</cell><cell>12.04</cell></row><row><cell>image</cell><cell>10,328</cell><cell>9.16</cell><cell>1,752</cell><cell>8.53</cell></row><row><cell>link</cell><cell>6,359</cell><cell>5.64</cell><cell>1,133</cell><cell>5.52</cell></row><row><cell>linebreak</cell><cell>6,208</cell><cell>5.51</cell><cell>1,118</cell><cell>5.44</cell></row><row><cell>container</cell><cell>5,425</cell><cell>4.81</cell><cell>953</cell><cell>4.64</cell></row><row><cell>header</cell><cell>4,356</cell><cell>3.86</cell><cell>739</cell><cell>3.60</cell></row><row><cell>textinput</cell><cell>4,192</cell><cell>3.72</cell><cell>825</cell><cell>4.02</cell></row><row><cell>checkbox</cell><cell>3,426</cell><cell>3.04</cell><cell>677</cell><cell>3.30</cell></row><row><cell>radiobutton</cell><cell>3,302</cell><cell>2.93</cell><cell>719</cell><cell>3.50</cell></row><row><cell>toggle</cell><cell>2,785</cell><cell>2.47</cell><cell>574</cell><cell>2.79</cell></row><row><cell>slider</cell><cell>2,668</cell><cell>2.37</cell><cell>524</cell><cell>2.55</cell></row><row><cell>datepicker</cell><cell>2,606</cell><cell>2.31</cell><cell>473</cell><cell>2.30</cell></row><row><cell>textarea</cell><cell>2,449</cell><cell>2.17</cell><cell>452</cell><cell>2.20</cell></row><row><cell>rating</cell><cell>2,372</cell><cell>2.10</cell><cell>437</cell><cell>2.13</cell></row><row><cell>dropdown</cell><cell>1,453</cell><cell>1.29</cell><cell>250</cell><cell>1.22</cell></row><row><cell>video</cell><cell>810</cell><cell>0.72</cell><cell>155</cell><cell>0.75</cell></row><row><cell>list</cell><cell>788</cell><cell>0.70</cell><cell>125</cell><cell>0.61</cell></row><row><cell>stepperinput</cell><cell>137</cell><cell>0.12</cell><cell>22</cell><cell>0.11</cell></row><row><cell>table</cell><cell>55</cell><cell>0.05</cell><cell>19</cell><cell>0.09</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,88.99,379.37,318.66,124.94"><head>Table 2</head><label>2</label><figDesc>Distribution of training and validation set categories of the Screenshot task.</figDesc><table coords="3,185.63,410.55,222.02,93.76"><row><cell></cell><cell></cell><cell>Screensthot task</cell><cell></cell><cell></cell></row><row><cell>Category</cell><cell cols="2">Training set # of boxes fraction [%]</cell><cell cols="2">Validation set # of boxes fraction [%]</cell></row><row><cell>link</cell><cell>106,457</cell><cell>36.72</cell><cell>10,910</cell><cell>30.46</cell></row><row><cell>text</cell><cell>82,642</cell><cell>28.51</cell><cell>9,400</cell><cell>26.24</cell></row><row><cell>image</cell><cell>52,672</cell><cell>18.17</cell><cell>7,462</cell><cell>20.83</cell></row><row><cell>heading</cell><cell>39,330</cell><cell>13.57</cell><cell>5,620</cell><cell>15.69</cell></row><row><cell>input</cell><cell>4,448</cell><cell>1.53</cell><cell>605</cell><cell>1.69</cell></row><row><cell>button</cell><cell>4,353</cell><cell>1.50</cell><cell>1,823</cell><cell>5.09</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,88.99,90.49,409.24,117.69"><head>Table 3</head><label>3</label><figDesc>Base parameters for training the models.</figDesc><table coords="4,95.27,122.10,402.96,86.07"><row><cell>Parameter</cell><cell>Value Parameter</cell><cell>Value</cell></row><row><cell>Checkpoint</cell><cell>COCO Batch size</cell><cell>1</cell></row><row><cell>Optimizer</cell><cell>SGD w/ moment. of 0.9 Accumulated grad.</cell><cell>4</cell></row><row><cell>Loss Base and min lr</cell><cell>smooth L 1 Epochs 0.0025 -0.000625</cell><cell>20 (Screenshot) 40 (Wireframe)</cell></row><row><cell>Decay factor Warm up</cell><cell>0.5 Decay in ... epoch 1 epoch</cell><cell>[10, 15] (Screenshot) [20, 30] (Wireframe)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,88.99,230.81,400.90,81.83"><head>Table 4</head><label>4</label><figDesc>Base augmentations for training the models.</figDesc><table coords="4,95.27,262.42,394.62,50.21"><row><cell>Augmentation</cell><cell>Intensity</cell><cell>Probability [%]</cell></row><row><cell>Random Brightness</cell><cell>0.5 -1.5</cell><cell>50</cell></row><row><cell>Random Contrast</cell><cell>0.5 -1.5</cell><cell>50</cell></row><row><cell>Random Saturation (RGB only)</cell><cell>0.5 -1.5</cell><cell>50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,88.89,438.26,403.58,201.47"><head></head><label></label><figDesc>Apply the 𝑇 𝑏𝑜𝑥 threshold in the same way as 𝑇 𝑖𝑚𝑔 in the image end for if all bounding boxes are discarded from the annotations of the image then Discard this image from the set and continue with the next one</figDesc><table coords="4,107.84,602.87,61.41,36.86"><row><cell>end if</cell></row><row><cell>end if</cell></row><row><cell>end for</cell></row></table><note coords="4,88.89,438.26,306.51,10.91;4,107.84,452.89,128.78,11.36;4,240.13,452.89,38.00,11.50;4,107.84,466.44,82.55,10.91;4,124.20,479.99,294.00,11.36;4,421.71,479.99,70.76,10.91;4,124.20,494.27,94.45,10.63;4,140.56,507.09,286.19,10.91;4,124.20,521.58,18.87,9.76;4,140.56,534.19,175.32,10.91"><p>Algorithm 1 Filtering homogeneous image elements from a dataset. Define threshold values 𝑇 𝑖𝑚𝑔 and 𝑇 𝑏𝑜𝑥 for each image do Apply edge detector to the image and compute a mean value 𝜇 𝑖𝑚𝑔 from the output if 𝜇 𝑖𝑚𝑔 ≤ 𝑇 𝑖𝑚𝑔 then Discard this image from the set and continue with the next one else for each bounding box of the image do</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="5,88.99,224.25,412.09,57.92"><head>Table 5</head><label>5</label><figDesc>Manually designed thresholds for discarding images from a training set of the Screenshot task.</figDesc><table coords="5,91.78,255.60,409.30,26.57"><row><cell>Img size</cell><cell>≤ 500 × 500</cell><cell>≤ 600 × 600</cell><cell>≤ 900 × 900</cell><cell cols="2">≤ 1200×1200 ≥ 1200×1200</cell></row><row><cell>Threshold</cell><cell>2.5</cell><cell>3.5</cell><cell>2.8</cell><cell>2.5</cell><cell>0.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="5,88.99,317.14,418.66,153.56"><head>Table 6</head><label>6</label><figDesc>Results on the validation set while discarding noisy images (for threshold values see Table5) and bounding boxes (BBoxes) from the training set of the Screenshot task.</figDesc><table coords="5,181.61,360.72,225.37,109.99"><row><cell cols="2">Threshold Image BBox</cell><cell>mAP0.5</cell><cell>mAP</cell><cell>mAR</cell></row><row><cell>-</cell><cell>-</cell><cell>0.592</cell><cell>0.404</cell><cell>0.603</cell></row><row><cell>Tab. 5</cell><cell>-</cell><cell>0.604</cell><cell>0.412</cell><cell>0.612</cell></row><row><cell>Tab. 5</cell><cell>0.2</cell><cell>0.599</cell><cell>0.409</cell><cell>0.610</cell></row><row><cell>Tab. 5</cell><cell>0.6</cell><cell>0.599</cell><cell>0.409</cell><cell>0.610</cell></row><row><cell>Tab. 5</cell><cell>1.0</cell><cell>0.601</cell><cell>0.408</cell><cell>0.610</cell></row><row><cell>Tab. 5</cell><cell>1.4</cell><cell>0.601</cell><cell>0.411</cell><cell>0.612</cell></row><row><cell>Tab. 5</cell><cell>1.8</cell><cell>0.593</cell><cell>0.397</cell><cell>0.606</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="6,88.99,305.66,418.66,135.13"><head>Table 7</head><label>7</label><figDesc>Comparison of two types of resizing augmentation on the validation set for the Screenshot and Wireframe tasks. Resize Shortest Edge (RSE) selects sizes from 640 to 800 px with a step of 32. Random Relative Resize (RRR) defines an interval of [min, max] relative sizes as factor for resizing the image.</figDesc><table coords="6,95.27,361.19,396.53,79.61"><row><cell></cell><cell></cell><cell>Screenshot task</cell><cell></cell><cell></cell><cell>Wireframe task</cell><cell></cell></row><row><cell>Resize type</cell><cell>mAP0.5</cell><cell>mAP</cell><cell>mAR</cell><cell>mAP0.5</cell><cell>mAP</cell><cell>mAR</cell></row><row><cell>RSE</cell><cell>0.563</cell><cell>0.372</cell><cell>0.549</cell><cell>0.929</cell><cell>0.672</cell><cell>0.732</cell></row><row><cell>RRR [0.7, 0.9]</cell><cell>0.604</cell><cell>0.412</cell><cell>0.612</cell><cell>0.969</cell><cell>0.703</cell><cell>0.763</cell></row><row><cell>RRR [0.6, 1.0]</cell><cell>0.608</cell><cell>0.417</cell><cell>0.619</cell><cell>0.972</cell><cell>0.706</cell><cell>0.765</cell></row><row><cell>RRR [0.4, 1.0]</cell><cell>0.608</cell><cell>0.414</cell><cell>0.608</cell><cell>0.954</cell><cell>0.689</cell><cell>0.751</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="7,88.99,90.49,416.99,135.22"><head>Table 8</head><label>8</label><figDesc>Comparison of using Cutout augmentation on the validation set for the Screenshot and Wireframe tasks. The max size is given as a percentage of the image size.</figDesc><table coords="7,95.47,134.06,399.95,91.65"><row><cell></cell><cell></cell><cell></cell><cell>Screenshot task</cell><cell></cell><cell></cell><cell>Wireframe task</cell><cell></cell></row><row><cell>max holes</cell><cell>max size</cell><cell>mAP0.5</cell><cell>mAP</cell><cell>mAR</cell><cell>mAP0.5</cell><cell>mAP</cell><cell>mAR</cell></row><row><cell>-</cell><cell>-</cell><cell>0.604</cell><cell>0.412</cell><cell>0.612</cell><cell>0.969</cell><cell>0.703</cell><cell>0.763</cell></row><row><cell>4</cell><cell>5.0%</cell><cell>0.612</cell><cell>0.416</cell><cell>0.610</cell><cell>0.967</cell><cell>0.699</cell><cell>0.759</cell></row><row><cell>8</cell><cell>5.0%</cell><cell>0.603</cell><cell>0.411</cell><cell>0.606</cell><cell>0.968</cell><cell>0.701</cell><cell>0.760</cell></row><row><cell>16</cell><cell>5.0%</cell><cell>0.596</cell><cell>0.405</cell><cell>0.608</cell><cell>0.967</cell><cell>0.698</cell><cell>0.759</cell></row><row><cell>16</cell><cell>2.5%</cell><cell>0.603</cell><cell>0.410</cell><cell>0.607</cell><cell>0.964</cell><cell>0.697</cell><cell>0.758</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="7,88.99,355.07,416.99,97.05"><head>Table 9</head><label>9</label><figDesc>Comparison of using RGB and greyscale images on the validation set for the Screenshot and Wireframe tasks.</figDesc><table coords="7,95.27,396.42,400.27,55.70"><row><cell></cell><cell></cell><cell>Screenshot task</cell><cell></cell><cell></cell><cell>Wireframe task</cell><cell></cell></row><row><cell>Color space</cell><cell>mAP0.5</cell><cell>mAP</cell><cell>mAR</cell><cell>mAP0.5</cell><cell>mAP</cell><cell>mAR</cell></row><row><cell>RGB</cell><cell>0.604</cell><cell>0.412</cell><cell>0.612</cell><cell>0.969</cell><cell>0.703</cell><cell>0.763</cell></row><row><cell>Greyscale</cell><cell>0.593</cell><cell>0.403</cell><cell>0.604</cell><cell>0.974</cell><cell>0.707</cell><cell>0.767</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="8,88.98,90.49,417.00,81.83"><head>Table 10</head><label>10</label><figDesc>Additional augmentations to the training the models (see Table3and Table4for base parameters and common augmentations used for training).</figDesc><table coords="8,95.27,134.06,369.01,38.25"><row><cell>Augmentation</cell><cell></cell><cell>Parameters</cell></row><row><cell>Random Relative Resize</cell><cell>resize interval = [0.6, 1.0]</cell><cell>crop = 1400 px</cell></row><row><cell>Cutout</cell><cell>max holes = 4</cell><cell>max size = 5% of image</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="9,88.99,90.49,393.09,93.78"><head>Table 11</head><label>11</label><figDesc>Settings of the selected aspect ratios and sizes for anchor box proposals.</figDesc><table coords="9,99.18,122.10,382.91,62.16"><row><cell>#</cell><cell>Anchor generator settings</cell><cell>Aspect ratios</cell><cell>Anchor sizes</cell></row><row><cell>1</cell><cell>default</cell><cell>[0.5, 1.0, 2.0]</cell><cell>[32, 64, 128, 256, 512]</cell></row><row><cell>2</cell><cell>default + horizontal</cell><cell>[0.2, 0.5, 1.0, 2.0]</cell><cell>[32, 64, 128, 256, 512]</cell></row><row><cell>3</cell><cell>statistical</cell><cell>[0.1, 0.5, 1.0, 1.5]</cell><cell>[32, 64, 128, 256, 512]</cell></row><row><cell>4</cell><cell>statistical + smaller sizes</cell><cell>[0.1, 0.5, 1.0, 1.5]</cell><cell>[16, 32, 64, 128, 256]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="9,88.99,207.52,418.66,188.53"><head>Table 12</head><label>12</label><figDesc>Comparison of using different settings of aspect ratios and sizes for anchor box proposals on the validation set. Comparison is performed on the RGB and greyscale images for the Screenshot and Wireframe tasks. Anchor settings are specified in Table11.</figDesc><table coords="9,95.27,263.05,399.11,133.00"><row><cell></cell><cell></cell><cell cols="2">Screenshot task</cell><cell></cell><cell cols="2">Wireframe task</cell><cell></cell></row><row><cell cols="2">Anchor setting Color space</cell><cell>mAP0.5</cell><cell>mAP</cell><cell>mAR</cell><cell>mAP0.5</cell><cell>mAP</cell><cell>mAR</cell></row><row><cell>#1</cell><cell>RGB</cell><cell>0.617</cell><cell>0.421</cell><cell>0.615</cell><cell>0.973</cell><cell>0.705</cell><cell>0.765</cell></row><row><cell>#2</cell><cell>RGB</cell><cell>0.621</cell><cell>0.424</cell><cell>0.625</cell><cell>0.974</cell><cell>0.704</cell><cell>0.762</cell></row><row><cell>#3</cell><cell>RGB</cell><cell>0.623</cell><cell>0.426</cell><cell>0.627</cell><cell>0.970</cell><cell>0.704</cell><cell>0.763</cell></row><row><cell>#4</cell><cell>RGB</cell><cell>0.611</cell><cell>0.414</cell><cell>0.627</cell><cell>0.975</cell><cell>0.700</cell><cell>0.762</cell></row><row><cell>#1</cell><cell>Greyscale</cell><cell>0.612</cell><cell>0.417</cell><cell>0.613</cell><cell>0.972</cell><cell>0.702</cell><cell>0.762</cell></row><row><cell>#2</cell><cell>Greyscale</cell><cell>0.610</cell><cell>0.416</cell><cell>0.617</cell><cell>0.974</cell><cell>0.703</cell><cell>0.762</cell></row><row><cell>#3</cell><cell>Greyscale</cell><cell>0.612</cell><cell>0.419</cell><cell>0.620</cell><cell>0.970</cell><cell>0.703</cell><cell>0.762</cell></row><row><cell>#4</cell><cell>Greyscale</cell><cell>0.606</cell><cell>0.410</cell><cell>0.622</cell><cell>0.975</cell><cell>0.704</cell><cell>0.764</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="10,88.99,90.49,418.84,272.57"><head>Table 13</head><label>13</label><figDesc>Comparison of different backbone architectures on the validation set. Comparison is performed on the RGB and greyscale images for the Screenshot and Wireframe tasks. model trained according to Table3and Table4w/ the Random Relative Resize augmentation using image resize interval [0.7, 0.9]. In the Screenshot task, only the images were filtered using thresholds described in Table5.</figDesc><table coords="10,89.29,134.06,405.09,161.04"><row><cell></cell><cell></cell><cell cols="2">Screenshot task</cell><cell></cell><cell cols="2">Wireframe task</cell><cell></cell></row><row><cell>Backbone</cell><cell>Color space</cell><cell>mAP0.5</cell><cell>mAP</cell><cell>mAR</cell><cell>mAP0.5</cell><cell>mAP</cell><cell>mAR</cell></row><row><cell>ResNet-50</cell><cell>RGB</cell><cell>0.623</cell><cell>0.426</cell><cell>0.627</cell><cell>0.975</cell><cell>0.700</cell><cell>0.762</cell></row><row><cell>ResNet-101</cell><cell>RGB</cell><cell>0.603</cell><cell>0.410</cell><cell>0.614</cell><cell>0.970</cell><cell>0.699</cell><cell>0.758</cell></row><row><cell>ResNeXt-101</cell><cell>RGB</cell><cell>0.601</cell><cell>0.408</cell><cell>0.608</cell><cell>0.977</cell><cell>0.705</cell><cell>0.765</cell></row><row><cell>ResNet-50</cell><cell>Greyscale</cell><cell>0.612</cell><cell>0.419</cell><cell>0.620</cell><cell>0.975</cell><cell>0.704</cell><cell>0.764</cell></row><row><cell>ResNet-101</cell><cell>Greyscale</cell><cell>0.604</cell><cell>0.413</cell><cell>0.612</cell><cell>0.970</cell><cell>0.699</cell><cell>0.760</cell></row><row><cell>ResNeXt-101</cell><cell>Greyscale</cell><cell>0.598</cell><cell>0.408</cell><cell>0.605</cell><cell>0.976</cell><cell>0.706</cell><cell>0.766</cell></row><row><cell cols="2">set can be found in Table 14:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">#1: ResNet-50 (baseline, RGB) -</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note coords="10,89.29,325.04,404.62,10.91;10,89.29,338.59,418.54,10.91;10,89.29,352.14,416.33,10.91"><p><p><p>#2: ResNet-50 (augmentations, RGB) -baseline trained w/ augmentations from Table</p>10</p>. #3: ResNet-50 (anchor settings, RGB) -same as submission #2 w/ anchor settings from Table 11: statistical for the Screenshot task, and statistical + smaller sizes for the Wireframe task.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="10,88.99,489.54,407.48,168.77"><head>Table 14</head><label>14</label><figDesc>Test results obtained from the submissions.</figDesc><table coords="10,99.03,518.94,397.44,139.37"><row><cell></cell><cell></cell><cell>Screenshot task</cell><cell></cell><cell></cell><cell>Wireframe task</cell><cell></cell></row><row><cell>#</cell><cell>Run ID</cell><cell>mAP0.5</cell><cell>mAR0.5</cell><cell>Run ID</cell><cell>mAP0.5</cell><cell>mAR0.5</cell></row><row><cell>1</cell><cell>134207</cell><cell>0.594</cell><cell>0.815</cell><cell>134095</cell><cell>0.794</cell><cell>0.832</cell></row><row><cell>2</cell><cell>134214</cell><cell>0.602</cell><cell>0.822</cell><cell>134175</cell><cell>0.830</cell><cell>0.863</cell></row><row><cell>3</cell><cell>134215</cell><cell>0.609</cell><cell>0.834</cell><cell>134180</cell><cell>0.882</cell><cell>0.918</cell></row><row><cell>4</cell><cell>134217</cell><cell>0.601</cell><cell>0.827</cell><cell>134181</cell><cell>0.889</cell><cell>0.923</cell></row><row><cell>5</cell><cell>134224</cell><cell>0.628</cell><cell>0.830</cell><cell>134225</cell><cell>0.888</cell><cell>0.925</cell></row><row><cell>6</cell><cell>134603</cell><cell>0.590</cell><cell>0.807</cell><cell>134548</cell><cell>0.900</cell><cell>0.934</cell></row><row><cell>7</cell><cell>134716</cell><cell>0.621</cell><cell>0.821</cell><cell>134723</cell><cell>0.894</cell><cell>0.928</cell></row><row><cell>8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>134728</cell><cell>0.895</cell><cell>0.927</cell></row><row><cell>9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>134829</cell><cell>0.900</cell><cell>0.933</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The work has been supported by the grant of the <rs type="funder">University of West Bohemia</rs>, project No. <rs type="grantNumber">SGS-2019-027</rs>. Computational resources were supplied by the project "e<rs type="projectName">-Infrastruktura CZ</rs>" (e-<rs type="grantNumber">INFRA LM2018140</rs>) provided within the program Projects of <rs type="projectName">Large Research, Development and Innovations Infrastructures</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_9nk4NXY">
					<idno type="grant-number">SGS-2019-027</idno>
					<orgName type="project" subtype="full">-Infrastruktura CZ</orgName>
				</org>
				<org type="funded-project" xml:id="_tfFFXQe">
					<idno type="grant-number">INFRA LM2018140</idno>
					<orgName type="project" subtype="full">Large Research, Development and Innovations Infrastructures</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="11,112.66,377.57,394.53,10.91;11,112.66,391.12,393.33,10.91;11,112.66,404.67,393.33,10.91;11,112.66,418.22,344.57,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,168.01,391.12,337.98,10.91;11,112.66,404.67,181.21,10.91">Overview of ImageCLEFdrawnUI 2021: The Detection and Recognition of Hand Drawn and Digital Website UIs Task</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tauteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<ptr target=".org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="11,315.97,404.67,109.43,10.91">CLEF2021 Working Notes</title>
		<title level="s" coord="11,432.42,404.67,73.56,10.91;11,112.66,418.22,99.32,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,431.77,395.01,10.91;11,112.66,445.32,394.53,10.91;11,112.48,458.87,395.18,10.91;11,112.66,472.42,394.53,10.91;11,112.28,485.97,393.70,10.91;11,112.66,499.52,393.33,10.91;11,112.66,513.06,393.33,10.91;11,112.66,526.61,393.53,10.91;11,112.66,540.16,197.61,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,264.32,485.97,241.66,10.91;11,112.66,499.52,261.75,10.91">Overview of the ImageCLEF 2021: Multimedia retrieval in medical, nature, internet and social media applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tauteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Moustahfid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,401.27,499.52,104.72,10.91;11,112.66,513.06,393.33,10.91;11,112.66,526.61,201.15,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 12th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="11,348.61,526.61,157.57,10.91;11,112.66,540.16,31.10,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,553.71,393.33,10.91;11,112.66,567.26,393.33,10.91;11,112.66,580.81,394.53,10.91;11,112.66,594.36,58.60,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,224.20,553.71,281.79,10.91;11,112.66,567.26,286.36,10.91">Html atomic ui elements extraction from hand-drawn website images using mask-rcnn and novel multi-pass inference technique</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mohapatra</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="11,421.63,567.26,84.36,10.91;11,112.66,580.81,23.32,10.91">CLEF2020 Working Notes</title>
		<title level="s" coord="11,143.40,580.81,182.51,10.91">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,607.91,393.33,10.91;11,112.66,621.46,294.26,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,313.56,607.91,50.90,10.91">Mask r-cnn</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,394.00,607.91,111.98,10.91;11,112.66,621.46,196.61,10.91">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,635.01,394.62,10.91;11,112.66,648.56,393.33,10.91;11,112.66,662.11,223.31,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,326.47,635.01,180.81,10.91;11,112.66,648.56,58.93,10.91">Deep learning for ui element detection: Drawnui 2020</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">N A</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Jaganathan</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="11,193.72,648.56,109.56,10.91">CLEF2020 Working Notes</title>
		<title level="s" coord="11,310.36,648.56,178.81,10.91">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,86.97,395.17,10.91;12,112.66,100.52,393.98,10.91;12,112.41,114.06,224.93,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,221.42,86.97,286.42,10.91;12,112.66,100.52,44.11,10.91">Cascade r-cnn: High quality object detection and instance segmentation</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2019.2956516</idno>
	</analytic>
	<monogr>
		<title level="j" coord="12,167.25,100.52,295.42,10.91">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1483" to="1498" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,127.61,393.33,10.91;12,112.66,141.16,221.40,10.91" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-Y</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10934</idno>
		<title level="m" coord="12,304.83,127.61,201.15,10.91;12,112.66,141.16,39.31,10.91">Yolov4: Optimal speed and accuracy of object detection</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,154.71,393.33,10.91;12,112.66,168.26,393.33,10.91;12,112.66,181.81,223.31,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,225.43,154.71,280.56,10.91;12,112.66,168.26,49.98,10.91">Sketch2code: Automatic hand-drawn ui elements detection with faster r-cnn</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Říha</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="12,185.64,168.26,112.41,10.91">CLEF2020 Working Notes</title>
		<title level="s" coord="12,305.46,168.26,183.33,10.91">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,195.36,393.33,10.91;12,112.66,208.91,393.33,10.91;12,112.33,222.46,394.85,10.91;12,112.66,236.01,70.43,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,264.19,195.36,241.80,10.91;12,112.66,208.91,108.44,10.91">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="12,142.90,222.46,238.48,10.91">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015">2015</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,249.56,393.33,10.91;12,112.66,263.11,393.33,10.91;12,112.33,276.66,264.86,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,391.19,249.56,114.80,10.91;12,112.66,263.11,81.53,10.91">Feature pyramid networks for object detection</title>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.106</idno>
	</analytic>
	<monogr>
		<title level="m" coord="12,215.20,263.11,290.79,10.91;12,112.33,276.66,30.22,10.91">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,290.20,394.04,10.91;12,112.66,303.75,156.24,10.91" xml:id="b10">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
	</analytic>
	<monogr>
		<title level="j" coord="12,361.15,290.20,48.12,10.91">Detectron</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,317.30,394.53,10.91;12,112.66,330.85,394.53,10.91;12,112.66,344.40,123.33,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,112.66,330.85,189.06,10.91">Microsoft coco: Common objects in context</title>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,324.47,330.85,178.59,10.91">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,357.95,393.33,10.91;12,112.66,371.50,208.77,10.91" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="12,224.87,357.95,281.12,10.91;12,112.66,371.50,26.61,10.91">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,385.05,393.32,10.91;12,112.66,398.60,353.56,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,157.03,385.05,175.86,10.91">Laplacian operator-based edge detectors</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2007.1027</idno>
	</analytic>
	<monogr>
		<title level="j" coord="12,341.11,385.05,164.87,10.91;12,112.66,398.60,111.46,10.91">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="886" to="890" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,412.15,393.33,10.91;12,112.66,425.70,389.60,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,226.34,412.15,279.65,10.91;12,112.66,425.70,221.36,10.91">Edge detection techniques-an overview, Pattern Recognition and Image Analysis C/C of Raspoznavaniye Obrazov I</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ziou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tabbone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,336.74,425.70,86.65,10.91">Analiz Izobrazhenii</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="537" to="559" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,439.25,393.32,10.91;12,112.66,452.79,394.53,10.91;12,112.66,466.34,80.57,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,427.66,439.25,78.33,10.91;12,112.66,452.79,125.74,10.91">End-to-end object detection with transformers</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,264.96,452.79,193.48,10.91">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,479.89,393.53,10.91;12,112.66,493.44,302.71,10.91" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<title level="m" coord="12,303.15,479.89,203.04,10.91;12,112.66,493.44,120.62,10.91">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,506.99,393.33,10.91;12,112.66,520.54,395.00,10.91;12,112.66,534.09,137.64,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,259.74,506.99,203.37,10.91">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m" coord="12,488.38,506.99,17.60,10.91;12,112.66,520.54,307.90,10.91">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,547.64,393.33,10.91;12,112.41,561.19,81.21,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,151.09,547.64,274.76,10.91">On the momentum term in gradient descent learning algorithms</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,433.34,547.64,72.64,10.91">Neural networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,574.74,393.54,10.91;12,112.39,588.29,269.15,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="12,167.48,574.74,42.22,10.91">Fast r-cnn</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.169</idno>
	</analytic>
	<monogr>
		<title level="m" coord="12,232.40,574.74,273.80,10.91;12,112.39,588.29,25.20,10.91">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,601.84,393.33,10.91;12,112.66,615.39,393.33,10.91;12,112.66,628.93,272.76,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="12,488.38,601.84,17.60,10.91;12,112.66,615.39,318.59,10.91">The unreasonable effectiveness of noisy data for fine-grained recognition</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,462.28,615.39,43.71,10.91;12,112.66,628.93,142.26,10.91">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="301" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,642.48,394.53,10.91;12,112.28,656.03,395.00,10.91;12,112.66,669.58,334.84,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="12,112.28,656.03,254.97,10.91">Albumentations: Fast and flexible image augmentations</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Buslaev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">I</forename><surname>Iglovikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Khvedchenya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Parinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Druzhinin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Kalinin</surname></persName>
		</author>
		<idno type="DOI">10.3390/info11020125</idno>
		<ptr target="https://www.mdpi.com/2078-2489/11/2/125.doi:10.3390/info11020125" />
	</analytic>
	<monogr>
		<title level="j" coord="12,379.45,656.03,54.58,10.91">Information</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
