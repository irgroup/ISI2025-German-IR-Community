<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,335.87,15.42;1,88.71,106.66,229.22,15.42">Overview of the Cross-Domain Authorship Verification Task at PAN 2021</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,138.55,79.96,5.42"><forename type="first">Mike</forename><surname>Kestemont</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Antwerp</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,181.89,138.55,98.86,5.42"><forename type="first">Enrique</forename><surname>Manjavacas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Antwerp</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,293.39,138.55,56.34,5.42"><forename type="first">Ilia</forename><surname>Markov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Antwerp</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,362.38,138.55,83.69,5.42"><forename type="first">Janek</forename><surname>Bevendorff</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,152.50,83.08,5.42"><forename type="first">Matti</forename><surname>Wiegmann</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,185.01,152.50,105.05,5.42"><forename type="first">Efstathios</forename><surname>Stamatatos</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of the Aegean</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,302.71,152.50,58.99,5.42"><forename type="first">Benno</forename><surname>Stein</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,392.69,152.50,76.81,5.42"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Leipzig University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,335.87,15.42;1,88.71,106.66,229.22,15.42">Overview of the Cross-Domain Authorship Verification Task at PAN 2021</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">1E34A2E77F457FFF6F7EB54E4FA44C6F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Idiosyncrasies in human writing styles make it difficult to develop systems for authorship identification that scale well across individuals. In this year's edition of PAN, the authorship identification track focused on open-set authorship verification, so that systems are applied to unknown documents by previously unseen authors in a new domain. As in the previous year, the sizable materials for this campaign were sampled from English-language fanfiction. The calibration materials handed out to the participants were the same as last year, but a new test set was compiled with authors and fandom domains not present in any of the previous datasets. The general setup of the task did not change, i.e., systems still had to estimate the probability of a pair of documents being authored by the same person. We attracted 13 submissions by 10 international teams, which were compared to three complementary baselines, using five diverse evaluation metrics. Post-hoc analyses show that systems benefitted from the abundant calibration materials and were well-equipped to handle the open-set scenario: Both the top-performing approach and the highly competitive cohort of runner-ups presented surprisingly strong verifiers. We conclude that, at least within this specific text variety, (large-scale) open-set authorship verification is not necessarily or inherently more difficult than a closed-set setup, which offers encouraging perspectives for the future of the field.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper provides a full-length description of the authorship verification shared task at PAN 2021. This edition was the second task installment in a renewed three-year program on the PAN authorship track (2020-2022), in which the scope, the difficulty and, the realism of the tasks are gradually increased each year. After last year's edition focused on providing participants with the largest pool of calibration material by far of any previous authorship shared task at PAN-a technical challenge in its own right-, we sought to improve the difficulty this year by sampling a fully disjunct test set. This is different to last year's edition where the overall task difficulty was kept in check by means of resorting to a closed-set evaluation scenario in which the test set was restricted to only authors and fandom domains also included in the calibration set (hence a clever participant could re-cast the task as an attribution task). This year's test set, on the other hand, comes with document pairs of exclusively unseen authors writing in unseen fandom domains, which results in an open-set or "true" authorship verification scenario, which is conventionally considered a much more demanding problem than attribution. For the next year, we planned a consecutive and final "surprise task", on which more details will be released in due time.</p><p>In the following, we first contextualize and motivate the design choices outlined above. Next, we shall formalize the task, describe the composition of this year's test set, and detail the employed evaluation metrics as well as the three generic baseline systems that were applied as a point of reference. In the sections following that, we shall briefly discuss the participating systems through a summary of their respective notebooks and the results of the task in tandem with a statistical analysis to assess whether pairs of systems in fact produced significantly different outcomes. In our discussion, we present post hoc analyses-including a comparison with last year's results regarding the distribution of scores-, the effect of non-answers, and the relationship of stylistic and topical similarities. Finally, we assess the contributions of this year's edition regarding the closed-set vs. open-set debate and offer an outlook into the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Motivation and Design Rationale</head><p>Much of the research in present-day computational authorship identification is implicitly underpinned by a basic assumption that could be summarized as the "Stylome Hypothesis". This hypothesis, seminally formulated by van Halteren et al. <ref type="bibr" coords="2,340.31,356.74,11.48,4.94" target="#b0">[1]</ref>, states that all writing individuals would leave a unique stylistic and linguistic "fingerprint" in their work, i.e., a set of stable empirical characteristics that can be extracted from and identified in a large-enough writing sample. In the analogy of the human genome, the assumption is that this fingerprint is a sufficient means to identifying the author of any given writing sample, provided it is long enough. The Stylome Hypothesis is an attractive working hypothesis, but remains hard to demonstrate, let alone prove. Experimental studies in the past decades have enabled scholars to close in on the experimental conditions that must be met for an authorship identification: we know that texts have to be long enough to be analyzed in the first place and verification across different text varieties has proven to be very challenging, not to mention issues of collaborative authorship or copy editors who inject additional stylistic noise. Cases where a reliable set of candidate authors is already available, are easier to solve than those where such a list cannot be established.</p><p>One general property of human authorship that has emerged in various studies appears to be its ad-hoc nature: Even within a single genre, textual features that work well to differentiate author A from a set of peers, might fail to separate author B from the same set of peers. Due to the many idiosyncracies that occur in an individual's writing style, this makes it challenging to develop systems that can be robustly scaled across many different individuals. Modeling authorial writing style requires bespoke models that are tailored to the characteristics of a single author or a specific set of authors. These observations tie in with two important scenarios that are commonly distinguished in the field: closed-set and open-set authorship verification. The former term describes the situation in which a system is applied to a set of texts by authors who are already known to the system (as they were seen during the training or calibration phase). The latter term describes the scenario in which a system is applied to texts whose authors are (potentially) unknown. This open-set scenario is supposedly much more challenging, since one would expect verification systems to overfit on textual properties that are significant for distinguishing this author from their known peers, but which may eventually turn out not to be a general characteristic of their style and hence not distinguish them from other, unknown authors.</p><p>This state of affairs has clearly motivated and shaped the shared task in authorship identification at PAN over the years. In particular, three factors have informed the design of the tasks:</p><p>(1) issues of scale, (2) methodological developments, and (3) the ad-hoc nature of authorship. First of all, to reliably assess the plausibility of the Stylome Hypothesis, much larger corpora are required than were previously available. It is only in recent years, in fact, that larger datasets for authorship attribution have become more widespread. This concern relating to scale is closely related to methodological developments in the field. In the 2018 task overview paper, the organizers voiced serious concerns about a noticeable lack of diversity in the submitted systems. Save a few exceptions, most of the systems then took the form of a simple classifier (typically a linear SVM or decision tree) that was applied to a bag-of-words representation of documents on the basis of character n-grams and other conventional feature sets. This methodological dearth was remarkable, since deep (neural) representation learning had already been shaping the landscape of NLP for several years. Such late adoption of deep neural models for authorship identification was very likely an immediate result of a lack of sufficient training resources as are typically required for representation learning (in particular for the data-hungry pre-training and finetuning of sentence-or document-level embeddings).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Authorship Verification</head><p>The most central element of authorship analysis is the identification of the document's author(s) <ref type="bibr" coords="3,122.97,433.39,11.48,4.94" target="#b1">[2,</ref><ref type="bibr" coords="3,138.05,433.39,7.52,4.94" target="#b2">3,</ref><ref type="bibr" coords="3,149.16,433.39,7.65,4.94" target="#b3">4]</ref>. In various fields, scholars have been studying how stylistic and linguistic properties of documents can be harnessed for the achievement of this goal. Because of the variety in authorial styles, including diachronic and synchronic shifts, progress in the field of style-based document authentication is hard to monitor, as it requires extensive, transparent, and repeated benchmarking initiatives <ref type="bibr" coords="3,261.34,487.58,11.33,4.94" target="#b4">[5]</ref>. The long-running authorship identification track at PAN hopes to contribute in this area and has organized tasks on authorship identification in various guises. The following section offers an overview of the central concepts as an update on a previously published survey <ref type="bibr" coords="3,239.47,528.23,11.56,4.94" target="#b5">[6]</ref>:</p><p>• Authorship Attribution: Given a document and a set of candidate authors, determine who wrote the document (PAN 2011-2012, 2016-2020); • Authorship Verification: Given a pair (or collection) of documents, determine whether they are written by the same author (PAN 2013-2015, 2021); • Authorship Obfuscation: Given a set of documents by the same author, paraphrase one or all of them so that its author cannot be identified anymore (PAN 2016-2018); • Obfuscation Evaluation: Devise and implement performance measures that quantify safeness, soundness, and / or sensibleness of an obfuscation software (PAN 2016-2018).</p><p>The formal goal of authorship verification is to approximate the target function 𝜑 : (𝐷 𝑘 , 𝑑 𝑢 ) → {𝑇, 𝐹 }, where 𝐷 𝑘 is a set of documents of known authorship by the same author and 𝑑 𝑢 is a document of unknown or questioned authorship. <ref type="foot" coords="4,313.32,114.69,3.71,3.61" target="#foot_0">1</ref> If 𝜑(𝐷 𝑘 , 𝑑 𝑢 ) = 𝑇 , then the author of 𝐷 𝑘 is also the author of 𝑑 𝑢 and if 𝜑(𝐷 𝑘 , 𝑑 𝑢 ) = 𝐹 , then the author of 𝐷 𝑘 is not the same as the author of 𝑑 𝑢 . In the case of cross-domain verification, 𝐷 𝑘 and 𝑑 𝑢 stem from a different text variety or encompass considerably different content (e.g. topics or themes, genres, registers, etc.). For the present task, we considered the simplest (and most challenging) formulation of the verification task, i.e., we only considered cases where 𝐷 𝑘 is a singleton, thus only pairs of two documents are examined. Given a training set of such problems, the verification systems of the participating teams had to be trained and calibrated to analyze the authorship of the unseen text pairs (from the test set). We shall distinguish between same-author text pairs (SA: 𝜑(𝐷 𝑘 , 𝑑 𝑢 ) = 𝑇 ) and different-author (DA: 𝜑(𝐷 𝑘 , 𝑑 𝑢 ) = 𝐹 ) text pairs. In terms of setup, the novelty this year was that (1) the authors and (2) the stories' fandom domains in the test set were not part of any of the provided calibration materials, which, theoretically speaking, should make this year's task more challenging than last year's.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Datasets</head><p>Given our aim to benchmark authorship identification systems at a much larger scale, our tasks in recent years <ref type="bibr" coords="4,157.97,352.09,11.36,4.94" target="#b7">[8,</ref><ref type="bibr" coords="4,172.05,352.09,8.96,4.94" target="#b8">9]</ref> focused on transformative literature, or so-called "fanfiction" <ref type="bibr" coords="4,458.15,352.09,16.27,4.94" target="#b9">[10]</ref>, a text variety that is nowadays abundantly available on the internet <ref type="bibr" coords="4,377.02,365.64,18.07,4.94" target="#b10">[11]</ref> with rich metadata and in many languages. Additionally, fanfiction is an excellent source of material for studies of cross-domain scenarios, since users often publish "fics" ranging over multiple topical domains ("fandoms"), such as Harry Potter, Twilight, or Marvel comics. The datasets we provided for our tasks at PAN 2020 and PAN 2021 were crawled from the long-established fanfiction community fanfiction.net. Access to the data can be requested on Zenodo. <ref type="foot" coords="4,389.66,430.75,3.71,3.61" target="#foot_1">2</ref> The 2021 edition of the authorship verification task built upon last year's <ref type="bibr" coords="4,316.57,446.94,12.99,4.94" target="#b6">[7]</ref> with the same general task layout and training data, but with a conceptually different test set. We retained the overall cross-domain setting, in which the texts in a pair stem from different fandoms, but we replaced the closed-set setting with an open-set setting, where both the authors and the fandoms in the test set are entirely "new" and do not occur in the training set.</p><p>The training resources were identical to those from last year and came in a "small" and a "large" variant. The large dataset contains 148,000 same-author and 128,000 different-author pairs across 1,600 fandoms. Each single author has written in at least two, but not more than six fandoms. The small training set is a subset of the large training set with 28,000 same-author and 25,000 different-author pairs from the same 1,600 fandoms. The new test was sampled with the same general strategy (19,999 text pairs in total), but in a way so as to fulfill the previously described open-set constraints to make the task-at least in theory-more difficult.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation Framework</head><p>For each of the 19,999 problems (or document pairs) in the test set, the systems had to produce a scalar score 𝑎 𝑖 in the range [0, 1] indicating the (scaled) probability that the pair was written by the same author (𝑎 𝑖 &gt; 0.5) or different authors (𝑎 𝑖 &lt; 0.5). Systems could choose to leave problems they deemed too difficult to decide unanswered by submitting a score of precisely 𝑎 𝑖 = 0.5. Such a non-answer is rewarded by some of the metrics over a wrong answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Performance Measures</head><p>Similar to year, we adopted a diverse mix of evaluation metrics that focused on different aspects of the verification task at hand. We reused the four evaluation metrics from the 2020 edition, but also included the (complement of the) Brier score <ref type="bibr" coords="5,363.09,245.56,17.86,4.94" target="#b11">[12]</ref> as an additional fifth metric (following discussions with participants and audience from the 2020 workshop<ref type="foot" coords="5,431.90,256.48,3.71,3.61" target="#foot_2">3</ref> ). The following performance measures were used:</p><p>• AUC: the ROC area-under-the-curve score, • c@1: a variant of the conventional accuracy measure, which rewards systems that leave difficult problems unanswered <ref type="bibr" coords="5,254.43,326.62,16.25,4.94" target="#b12">[13]</ref>,</p><p>• F 1 : the well-known F 1 performance measure (not taking into account non-answers), • F 0.5𝑢 : a newly-proposed F 0.5 -based measure that emphasizes correctly-answered sameauthor cases and rewards non-answers <ref type="bibr" coords="5,292.38,369.98,16.25,4.94" target="#b13">[14]</ref>, • Brier: the Brier score (more precisely: the complement of the Brier score loss function <ref type="bibr" coords="5,488.22,384.88,17.76,4.94" target="#b11">[12]</ref> as implemented in sklearn <ref type="bibr" coords="5,245.52,398.43,15.89,4.94" target="#b14">[15]</ref>), a straightforward, strictly proper scoring rule that measures the accuracy of probabilistic predictions.</p><p>The inclusion of the Brier score was an addition which was meant to measure the probabilistic confidence of the verifiers in a more fine-grained manner. This metric rewards verifiers that produce bolder but correct scores (i.e., 𝑎 𝑖 close to 0.0 or 1.0). Conversely, the metric would indirectly penalize less committal solutions, such as non-answers (𝑎 𝑖 = 0.5).</p><p>To produce a final ranking for a system, we used the mean score across all individual measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Baselines</head><p>In total, we provided three baseline systems (calibrated on the small training set) for comparison, of which the first two were also employed during last year's competition. These were a compression-based approach <ref type="bibr" coords="5,253.13,568.51,17.93,4.94" target="#b15">[16]</ref> and a naive distance-based, first-order bag-of-words model <ref type="bibr" coords="5,120.51,582.06,16.41,4.94" target="#b16">[17]</ref>. Both were made available to participants at the start. The third baseline was a post-hoc addition for this overview paper and consisted of a short-text variant of Koppel and Schler's unmasking <ref type="bibr" coords="5,179.04,609.15,16.43,4.94" target="#b17">[18,</ref><ref type="bibr" coords="5,198.20,609.15,12.32,4.94" target="#b18">19]</ref>, which had yielded good empirical results in the recent past.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Survey of Submissions</head><p>The authorship verification task received 13 submissions from 10 participating teams. In this section, we provide a short and concise overview of the submitted systems. For further details (including bibliographic references), we refer the interested reader to the full versions of these notebooks. Teams were allowed to hand in exactly one submission per training dataset (large and small). Three teams submitted two systems, the other teams either deliberately chose to submit only a single variant or were unable to produce a valid run in time. The systems listed below are described in the order in which the notebooks were initially submitted.</p><p>1. ikae21 <ref type="bibr" coords="6,153.44,219.15,18.07,4.94" target="#b19">[20]</ref> used a hard majority-voting ensemble that incorporated five different machine-learning classifiers (i.e., linear discriminant analysis, gradient boosting, extra trees, support vector machines, and stochastic gradient descent). The features used were top-800 TF-IDF-weighted word unigrams. 2. menta21 <ref type="bibr" coords="6,161.85,274.70,17.94,4.94" target="#b20">[21]</ref> exploited two types of stylometric features, character n-grams and punctuation marks, to train a neural network on each type of feature separately. The outputs were concatenated and fed into another neural network in order to obtain the predictions. 3. liaozhihao21 <ref type="bibr" coords="6,182.31,316.70,17.76,4.94" target="#b21">[22]</ref> used four retrieval models from the Lucene framework. Each retrieval model assigned a probability to a piece of text that it was written by the corresponding author. Later on, a weighted average of the probabilities was calculated to get the final score. The approach assumes that both texts were written by the same author if the highest final score corresponds to the same author. 4. weerasinghe21 <ref type="bibr" coords="6,193.24,385.80,18.07,4.94" target="#b22">[23]</ref> extracted stylometric features from each text pair and used the absolute differences between the feature vectors as input to the logistic regression classifier. The features included character and POS n-grams, special characters, function words, vocabulary richness, POS-tag chunks, and unique spellings. 5. boenninghoff21 <ref type="bibr" coords="6,198.47,441.36,18.07,4.94" target="#b23">[24]</ref> presented a hybrid neural-probabilistic end-to-end framework, which included neural feature extraction and deep metric learning, deep Bayes factor scoring, uncertainty modeling and adaptation, a combined loss function, and additionally an out-of-distribution detector for defining non-responses. In the final step, the model was extended to a majority-voting ensemble. 6. peng21 <ref type="bibr" coords="6,154.66,510.46,17.87,4.94" target="#b24">[25]</ref> proposed an approach that split the texts into fragments and used BERT to extract feature vectors from each fragment, which were then concatenated and fed into a neural network for the final predictions. 7. futrzynski21 <ref type="bibr" coords="6,182.46,552.46,17.90,4.94" target="#b25">[26]</ref> proposed an approach based on the cosine similarities of output representations extracted from BERT. These similarities were compared to several thresholds and were rescaled in order to classify a text pair. The BERT model was trained on the following tasks: masked language modeling, author classification, fandom classification, and author-fandom separation. In addition, the authors proposed a method for decreasing the computational costs by combining embeddings of many short text sequences. 8. embarcaderoruiz21 <ref type="bibr" coords="6,213.24,635.11,17.76,4.94" target="#b26">[27]</ref> proposed a novel approach consisting of a graph representation to represent the texts, which served as input to a Siamese network. The feature extraction network consisted of node embedding layers to obtain vector representations for each</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>System rankings for all PAN 2021 submissions across five evaluation metrics: AUC, c@1, F 1 , F 0.5𝑢 , Brier, and an overall mean score (as the final ranking criterion). The dataset column indicates which calibration dataset was used. Bold digits reflect the per-column maximum. Horizontal lines indicate the range of scores yielded by the baselines (in italics). node in the graph as well as a global pooling. The authors also incorporated stylometric features, combining them with the graph components to an ensemble. 9. tyo21 <ref type="bibr" coords="7,146.43,381.06,17.97,4.94" target="#b27">[28]</ref> used BERT within a Siamese network. The embedding space was optimized so that texts written by the same author are adjacent in that space, while texts written by different authors are farther apart. At inference time, the distance between embeddings was compared to a threshold (selected based on a grid search) to make the predictions. 10. rabinovits21 <ref type="bibr" coords="7,181.41,436.61,18.07,4.94" target="#b28">[29]</ref> relied on regression models. The authors incorporated the cosine distance for a set of vector-based features (word-, and character frequencies, POS tags, POS chunk n-grams, punctuation, stopwords) and absolute differences for scalar features (vocabulary richness, average sentence length, Flesch reading ease score) as measures of text-pair similarity. The concatenated similarity scores were used as input to a random forest model (adapted as a regressor).</p><formula xml:id="formula_0" coords="7,90.35,153.98,310.29,7.71">Team Dataset AUC c@1 F 1 F 0.5𝑢</formula><p>Overall, we observe a healthy diversity of methods, with several novel approaches, for instance from representation learning with neural networks, appearing among more established methods from text classification or information retrieval. Multiple teams employed a so-called "Siamese" neural network approach <ref type="bibr" coords="7,202.04,568.32,16.09,4.94" target="#b29">[30]</ref>, which seems to be a natural choice for the analysis of text pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Evaluation Results</head><p>Table <ref type="table" coords="7,117.14,626.94,5.17,4.94">1</ref> offers a tabular representation of the final results of the submitted systems on the PAN 2021 test set. The overall ranking is based on the mean performance of the five evaluation metrics (last column). The dataset column indicates whether a system was calibrated on the "large" or "small" dataset. In the following, we refer to these as "large" and "small" systems or</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Significance of pairwise differences in F 1 scores between submissions. Notation: '=' (not significant: 𝑝 ≥ 0.05), '*' (significant with 𝑝 &lt; 0.05), '**' (significant with 𝑝 &lt; 0.01), '***' (significant with 𝑝 &lt; 0.001). submissions. In Table <ref type="table" coords="8,184.96,414.31,3.66,4.94">2</ref>, we show a pairwise comparison of all combinations of systems to assess whether their solutions are significantly different from each other (based on their F 1 scores).</p><p>The statistical procedure we applied for this is the approximate randomization test <ref type="bibr" coords="8,469.43,441.41,16.41,4.94" target="#b30">[31]</ref>, for 10,000 bootstraps per comparison. The top-performing system this year was contributed by the participant who submitted last year's strongest system. Team boenninghof21 achieved an exceptionally solid and robust performance, including the overall highest score across all evaluation metrics. The team in first place is followed by a tight cohort of strong runner-ups (embarcaderoruiz21, weerasinghe21, menta21, and peng21) who all achieved similar scores in the same ballpark. With the exception of three systems (tyo21, futrzynski21, liaozhihao21), most approaches significantly outperformed the three (unoptimized) baselines. The baselines themselves all yielded surprisingly similar performances, with unmasking21 being the best-performing baseline with a slight edge. Somewhat surprisingly, the system by tyo21 turned out to not be significantly different from the unmasking baseline, although it was based on a completely different verification approach.</p><p>For most systems, the pairwise F 1 scores significantly differ (Table <ref type="table" coords="8,402.47,604.00,3.61,4.94">2</ref>), though in the upper echelons we see a few exceptions. This is to be expected with such exceptional (and hence necessarily similar) performances. The top-performing approach did, in fact, produce a significantly different solution from the runner-up, though the same is not true for all systems in the next cohort, which indicates that their particular ranking order does not necessarily indicate their quality, but incorporates a certain amount of chance. Some participants did well for some scoring metrics, but showed a more pronounced drop in others. The system by ikae21 for instance, achieved a more than respectable AUC in the lower nineties, but an F 0.5𝑢 only in the lower seventies (which should primarily be attributed to the different treatment of same-author pairs by this metric). Overall, the non-responses played an important part in the rankings, primarily affecting the c@1 and F 0.5𝑢 scores. Systems such as liaozhihao21-small, that delivered binary answers without any non-responses were at a clear disadvantage in this regard.</p><p>Of particular importance is the observation that if teams submitted separate systems for the large and the small dataset, they invariably yielded significantly different solutions. Most importantly, the "large" variant always outperformed the "small" one. It should be emphasized that last year, the stronger performance of the large systems might could have been attributed to the closed-set scenario, in which a sufficiently complex model could have fully memorized each author's individual characteristics. This effect cannot serve as an explanation in this year's edition, because none of the test set authors or fandoms were present in the calibration materials. The performance improvements this year must therefore be attributed to the mere scope or size of the dataset or other characteristics not pertaining directly to the individual authors. This serves as additional evidence that systems were generally able to benefit from the increased training dataset size and could capitalize on accessing more abundant and more diverse material by more authors, even in an open-set verification scenario. It also signals clearly that the supposed ad-hoc nature of authorship identification should not be over-estimated. At least within a single textual domain, the results demonstrate the feasibility of modeling authorship quite reliably and at a large scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head><p>In this section, we provide a more in-depth analysis of the submitted approaches and their evaluation results, also in comparison with last year's task. First, we take a look at the distribution of the submitted verification scores, including a meta classifier. We go on to inspect the effect of non-responses, and finally try to analyze how topic similarities between texts in a pair might have affected the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Comparison 2020-2021</head><p>Due to the intricate similarities and differences between the 2020 and 2021 editions of the task, a more detailed comparison is worthwhile. A clear advantage of the software submission procedure through tira.io is that we were able to rerun the systems from one year on the test dataset of another year in most cases. This way we were able to perform a cross-evaluation of quite a few systems with some exceptions due to unresolvable failures when running systems on datasets which they were not designed for. These were mostly a result of hard-coded assumptions that were violated by the new data. For example, several 2020 systems assumed all fandoms in the test set to be known, which was in clear violation with the 2021 dataset design. In Table <ref type="table" coords="9,161.47,632.16,3.67,4.94">3</ref>, we present the performance of these system and data combinations in terms of c@1. This comparison is necessarily incomplete but allows us to glean some interesting trends. Across systems, the scores for the 2020 dataset are consistently lower than for 2021 in all instances but one (ikae). We must therefore draw the counter-intuitive conclusion that Table <ref type="table" coords="10,116.06,82.74,5.12,8.93">3</ref> Cross-comparison of the performances (in terms of c@1) across different combinations of submissions (2020 vs. 2021) and test datasets (also 2020 vs. 2021). Some combinations could not be evaluated due to failures when running the system on a dataset it was not designed for. the open-set formulation with unseen authors and topical domains was, in fact, easier to solve than the closed setting. On the other hand, the new 2021 systems tended to underperform on the 2020 dataset in comparison with the original 2020 submission by the same team-at least in the preciously rare cases in which we were able to make this comparison (i.e., boeninghoff, weerasinghe, ikea).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1.">Distributions</head><p>Figure <ref type="figure" coords="10,120.78,485.99,4.06,4.94">7</ref>.1.1 (left) visualizes the overall distribution of the submitted answers for the systems that outperformed the baselines (best-performing system per team). We see a clear trimodal distribution with peaks around 0, 0.5 and 1, respectively. We noticed that systems submitted "bolder" answers than last year, i.e., only few answers lie in between the three peaks. The middle peak around 0.5 leads to the assumption that some systems deliberately optimized for non-responses. This assumption is further supported by Figure <ref type="figure" coords="10,374.04,553.73,4.03,4.94">7</ref>.1.1 (right), which shows the same observation, but broken down by individual systems.</p><p>In Figure <ref type="figure" coords="10,140.69,580.83,3.66,4.94">2</ref>, we plot the precision-recall curves for the above-mentioned submissions, including that of a naive meta classifier that predicts the mean score over all systems (dotted line). Whereas in previous years, the meta-classifier often suffered from a lack of methodological diversity in participant systems, this year, the mean verification score outperforms most individual systems. Nevertheless, while the meta classifier can compete with boenninghoff21 in terms of precision, it clearly falls short with regards to recall. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2.">Non-answers</head><p>Non-answers were an integral aspect of the evaluation procedure. In the submitted scores, but also in the participants' notebooks, we observed that particularly returning participants, such as boenninghoff21 and ikae21 took greater care to fine-tune this aspect of their systems (and were indeed successful in doing so). The different systems used non-responses to varying degrees. In Figure <ref type="figure" coords="12,175.25,486.93,3.81,4.94" target="#fig_2">3</ref>, we plot the c@1 performance as a function of the absolute number of non-responses per system. We see that futrzynski21 returned overall the most non-responses, though at the cost of a below-baseline performance. The three baselines, too, gave non-answers in comparably many cases, but were convincingly outperformed by most participant systems. The top-performing systems (boenninghoff21, embarcaderoruiz21) refused to answer cases to a more moderate degree, resulting in an overall very good performance. Many of the other high-ranking systems, such as weerasinghe21, menta21, or peng21, appeared as if they did not pay particular attention to optimizing for this aspect of the task and submitted only very few non-answers, if any. We performed a paired, non-parametric Wilcoxon signed-rank test (𝑛 = 16) to assess whether the number of non-responses of a system (including the baselines) correlated positively with its c@1 score. The result (𝑊 = 28.0; 𝑝 = 0.019) offers some ground to accept this positive correlation and thus supports the hypothesis that it generally paid off for systems to submit non-answers for difficult cases. Like last year, these observations raise the question as to which extent boenninghoff21's</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4</head><p>Evaluation results for top-performing systems (one per team), excluding any test problems for which boenninghoff21-large submitted a non-response (𝑎 𝑖 = 0.5). competitive edge can be attributed to the system's ability to correctly identify such difficult cases in order to leave them unanswered. Table <ref type="table" coords="13,312.06,242.39,5.17,4.94">4</ref> summarizes the performances of the top systems (one per participant) on all cases on which boenninghoff21 submitted a score of 𝑎 𝑖 ̸ = 0.5. Interestingly, the differences in performance stay the same, as well as the ranking, which indicates that the treatment of difficult cases is not the only magic ingredient (we should emphasize boenninghoff21's exceptional F 0.5𝑢 score on this subset; indicating that they primarily backed off for different-author document pairs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.3.">The Influence of Topic (continued)</head><p>In last year's overview paper, we applied a generic topic model to analyze the test problems from a semantic perspective. To avoid repetition, we will not reintroduce this model (nonnegative matrix factorization with 150 dimensions applied to a TF-IDF-normalized bag-of-words representation of content words) at length, but it remains an interesting challenge to analyze this year's test data from the same topical perspective. We applied the same pipeline to this year's test data for assessing topic similarities between the document pairs, in which we calculated the cosine similarity between the L1-normalized topic vectors for each document. Overall, the topical distances over all the document pairs in both the 2020 and 2021 test sets show a very similar distribution (2020: 𝜇 = 0.656, 𝜎 = 0.147; 2021: 𝜇 = 0.641, 𝜎 = 0.153). This is reassuring, as it show that while both datasets are cross-fandom, the open-set vs. closed-set reformulation did not introduce any obvious topical artifacts.</p><p>Generally speaking, all of the trends reported last year also hold on this year's test set:</p><p>1. Same-author pairs displayed a higher topical similarity then different-author pairs, indicating that authors do have an inclination to write about the same topics (see Figure <ref type="figure" coords="13,501.06,545.37,5.17,4.94" target="#fig_3">4</ref> (left)). A non-parametric (one-sided, but unpaired) Mann-Whitney 𝑈 test (𝑛 1 = 10, 000, 𝑛 2 = 9, 999) lends support to this view (𝑈 &gt; 68, 687𝐾, 𝑝 &lt; 0.001).</p><p>2. There is a mild but real correlation between the topical similarity of a document pair in a test problem and the average verification score submitted by systems. 3. Results for the standard linear regression model reported last year were: 𝛽 = 0.16, 𝑅 2 = 0.15. When limited to the correctly answered cases of the meta classifier, the resulting model this year is comparable (𝛽 = 0.16, 𝑅 2 = 0.15), but for the incorrect predictions, the coefficients markedly drop (𝛽 = 0.09, 𝑅 2 = 0.01). All in all, we can hypothesize that this year again, the models were generally susceptible to a misleading influence of topic similarity, as indicated by Figure <ref type="figure" coords="14,389.90,315.04,5.12,4.94" target="#fig_3">4</ref> (right): Correctly solved different-author pairs tended to be of lower topical similarity than those answered incorrectly. Same as last year, this relationship was reversed for the same-author pairs. Thus, topical information can be very useful for authorship verification, but it cannot necessarily be taken at face value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Outlook</head><p>Last year's edition proved to be a turning point in the history of the authorship identification track at PAN: Through the release of large-scale calibration materials, the performance of authorship models could, for the first time, be benchmarked on a scale sufficient for deep representation learning. This stimulated the adoption of new neural models which produced competitive and, in some cases, outstanding results. Interestingly, this size increase did attract new participants, while at the same time, some of the regular participants from previous years found the sudden increase in data size rather intimidating and struggled in the adaptation of their pre-existing systems to the new data. To counteract this effect and to maximize the inclusivity of our initiative, the separate submission of systems trained on the small and on the large dataset variant was introduced.</p><p>Another critical change compared to previous installments was the fact that the new dataset was limited to English-language documents only, a mere result of the availability of the source material. While we assume that most systems would also generalize to other (at least European) languages, we are aware that this might be a potential source of bias and it remains to be seen to which extent exactly the results reported here will be reproducible in other (more heavily inflected) languages. Also, the effect of (potentially very many) non-native speakers of English that appear as authors in the data is hard to quantify at this time. To the best of our knowledge, very few studies have looked at authorship identification across different writing languages. One might hypothesize that authors, when active in their native language, will demonstrate greater mastery and diversity of style, while in a second language, less refined writing and typical errors might increase their identifiability. Another deserving field for future studies is the comparison of fanfiction material that was exclusively written by authors who self-identify as (non-professional) "fans"-hence received very little (if any) moderation or editing-to writing samples by professional authors.</p><p>In spite of these critical remarks, the central take-away message from this year's shared task remains positive: Modern, large-scale authorship verification systems can perform extremely well within the fanfiction domain. Contrary to our expectations, recasting last year's task as an open-set setup did not degrade, but in fact improve their performance. Most systems were more than capable to accurately answer the cases, even though none of the authors and fandoms were seen in the training data. This is highly encouraging, though it remains to be seen whether this holds true for other textual domains outside of transformative fiction. In light of the outstanding results, we should certainly raise the uncomfortable question of whether cross-domain authorship verification in the fanfiction domain is simply too easy. Perhaps the variance between different fandoms is limited (e.g., due to a focus on erotic and pornographic content <ref type="bibr" coords="15,189.15,293.47,16.96,4.94" target="#b31">[32]</ref>) and should thus not be taken as a proxy for domain differences in other text varieties. Nevertheless, the findings demonstrate that the issue of the ad-hoc nature of authorship identification can be overcome, at least within a single textual domain, which is certainly a positive and encouraging message.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,194.12,148.73,7.98,91.48;8,215.73,163.01,29.59,77.20;8,258.95,187.11,7.98,53.11;8,280.56,191.15,7.98,49.06;8,303.17,147.40,7.98,92.81;8,324.78,185.78,7.98,54.44;8,346.39,172.60,7.98,67.62;8,368.00,194.05,7.98,46.16;8,388.61,168.37,7.98,71.84;8,410.22,198.60,7.98,41.61;8,432.83,189.66,7.98,50.55;8,453.44,167.07,7.98,73.14;8,475.05,171.18,30.59,69.03;8,89.29,248.68,78.85,7.98;8,192.06,248.68,314.19,7.98;8,89.29,258.54,91.48,7.98;8,217.56,258.54,3.88,7.98;8,238.74,258.54,267.51,7.98;8,89.29,268.40,75.87,7.98;8,235.28,268.40,270.97,7.98;8,89.29,278.27,77.20,7.98;8,256.89,278.27,249.37,7.98;8,89.29,288.13,53.11,7.98;8,278.51,288.13,227.75,7.98;8,89.29,297.99,49.06,7.98;8,300.12,297.99,206.14,7.98;8,89.29,307.86,92.81,7.98;8,321.73,307.86,184.53,7.98;8,89.29,317.72,54.44,7.98;8,343.34,317.72,162.92,7.98;8,89.29,327.58,67.62,7.98;8,364.95,327.58,141.31,7.98;8,89.29,337.44,46.16,7.98;8,386.55,337.44,119.70,7.98;8,89.29,347.31,71.84,7.98;8,408.16,347.31,98.09,7.98;8,89.29,357.17,41.61,7.98;8,429.77,357.17,76.47,7.98;8,89.29,367.03,50.55,7.98;8,454.85,367.03,51.40,7.98;8,89.29,376.90,73.14,7.98;8,472.99,376.90,33.26,7.98;8,89.29,386.76,66.97,7.98;8,494.60,386.76,11.65,7.98"><head></head><label></label><figDesc>*** *** *** *** *** *** *** *** *** *** *** *** *** *** embarcaderoruiz21-large * = *** ** *** *** *** *** *** *** *** *** *** *** weerasinghe21-large *** *** = *** *** *** *** *** *** *** *** *** *** weerasinghe21-small *** *** *** *** *** *** *** *** *** *** *** *** menta21-large *** *** *** *** *** *** *** *** *** *** *** peng21-small *** *** *** *** *** *** *** *** *** *** embarcaderoruiz21-small *** *** *** *** *** *** *** *** *** menta21-small *** *** *** *** *** *** *** *** rabinovits21-small *** *** *** *** *** *** *** ikae21-small *** * *** *** *** *** unmasking21-small *** *** *** *** **</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="10,89.29,485.99,416.69,4.94;10,89.29,499.53,416.69,4.94;10,89.29,513.08,416.70,4.94;10,88.07,526.63,417.92,4.94;10,89.29,540.18,416.90,4.94;10,89.29,553.73,416.69,4.94;10,89.29,567.28,261.14,4.94;10,100.20,580.83,405.78,4.94;10,89.29,594.38,416.69,4.94;10,89.29,607.93,416.69,4.94;10,89.29,621.48,418.38,4.94;10,89.29,635.03,417.89,4.94;10,89.29,645.94,189.82,7.58"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 7.1.1 (left)  visualizes the overall distribution of the submitted answers for the systems that outperformed the baselines (best-performing system per team). We see a clear trimodal distribution with peaks around 0, 0.5 and 1, respectively. We noticed that systems submitted "bolder" answers than last year, i.e., only few answers lie in between the three peaks. The middle peak around 0.5 leads to the assumption that some systems deliberately optimized for non-responses. This assumption is further supported by Figure7.1.1 (right), which shows the same observation, but broken down by individual systems.In Figure2, we plot the precision-recall curves for the above-mentioned submissions, including that of a naive meta classifier that predicts the mean score over all systems (dotted line). Whereas in previous years, the meta-classifier often suffered from a lack of methodological diversity in participant systems, this year, the mean verification score outperforms most individual systems. Nevertheless, while the meta classifier can compete with boenninghoff21 in terms of precision, it clearly falls short with regards to recall.4   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="12,89.29,422.95,374.27,8.93"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3:The c@1 scores per system as a function of the absolute number of non-answers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="14,89.29,240.70,418.22,8.95;14,89.29,252.67,416.69,8.94;14,89.29,264.66,391.96,8.87"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Left: Distribution of topical similarity, separate for same-author and different-author pairs. Right: The distribution of topical similarity within document pairs in the test set for same-author and different-author pairs broken down by whether the meta-classifier answered the pairs correctly.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,108.93,662.66,367.20,4.06"><p>This paragraph is based on last year's overview paper<ref type="bibr" coords="4,308.05,662.66,10.55,4.06" target="#b6">[7]</ref> and included for the sake of completeness.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,108.93,671.58,172.60,8.30"><p>https://zenodo.org/record/3716403</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,108.93,673.63,334.87,4.06"><p>Thanks to Fabrizio Sebastiani (Consiglio Nazionale delle Ricerche, Italy) for this suggestion.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="10,108.93,671.63,382.96,7.86"><p>Meta classifier performance: AUC: 0.917, c@1: 0.917, F1: 0.916, F0.5𝑢: 0.919, Brier: 0.917, Overall: 0.917.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>As in previous years, this initiative would not have been possible without the generous contributions of the participating teams, whose patience and enthusiasm we wish to acknowledge in what has been an unusually trying edition. Our thanks also go to the CLEF organizers for the continuation of their hard annual work. Finally, we would like to extend our appreciation to <rs type="person">Sebastian Bischoff</rs>, <rs type="person">Niklas Deckers</rs>, <rs type="person">Marcel Schliebs</rs>, and <rs type="person">Ben Thies</rs> for assembling the fanfiction.net corpus.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="15,111.10,505.90,390.44,4.51;15,111.11,517.85,374.38,4.51;15,111.11,529.81,181.47,4.51" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="15,372.01,505.90,129.53,4.51;15,111.11,517.85,187.24,4.51">New machine learning methods demonstrate the existence of a human stylome</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Van Halteren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Baayen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Tweedie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Haverkort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Neijt</surname></persName>
		</author>
		<idno type="DOI">10.1080/09296170500055350</idno>
	</analytic>
	<monogr>
		<title level="j" coord="15,306.21,517.85,140.56,4.51">Journal of Quantitative Linguistics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="65" to="77" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,111.10,541.76,383.59,4.51;15,111.11,553.72,267.74,4.51" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="15,171.83,541.76,209.29,4.51">A survey of modern authorship attribution methods</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<idno type="DOI">10.1002/asi.21001</idno>
		<ptr target="https://doi.org/10.1002/asi.21001.doi:10.1002/asi.21001" />
	</analytic>
	<monogr>
		<title level="j" coord="15,389.34,541.76,28.70,4.51">JASIST</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="538" to="556" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,111.11,565.67,368.05,4.51;15,111.11,577.63,35.45,4.51" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="15,147.42,565.67,90.81,4.51">Authorship attribution</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Juola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,245.64,565.67,199.43,4.51">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="233" to="334" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,111.11,589.58,385.32,4.51;15,111.11,601.54,316.96,4.51" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="15,248.47,589.58,200.14,4.51">Computational methods in authorship attribution</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Argamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,456.02,589.58,40.41,4.51;15,111.11,601.54,254.20,4.51">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="9" to="26" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,111.11,613.49,369.15,4.51;15,111.11,625.45,355.29,4.51;15,111.11,637.40,360.88,4.51;15,111.11,649.36,360.97,4.51;15,111.11,661.31,367.10,4.51;15,110.76,673.27,388.26,4.51" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="15,345.56,637.40,126.43,4.51;15,111.11,649.36,305.02,4.51">Who wrote the web? revisiting influential author identification research applicable to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Buz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Duffhauss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Gülzow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Köhler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Lötzsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paßmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Reinke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Rettenmeier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rometsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sommer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Träger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,110.76,673.27,138.50,4.51">Advances in Information Retrieval</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Crestani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M.-F</forename><surname>Moens</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Mothe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Silvestri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Di Nunzio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Hauff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Silvello</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="393" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,111.10,90.65,381.70,4.51;16,111.11,102.61,355.58,4.51;16,111.11,114.56,394.00,4.51;16,111.11,126.52,378.44,4.51;16,111.11,138.48,102.48,4.51" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="16,402.78,102.61,63.91,4.51;16,111.11,114.56,110.79,4.51">Shared tasks on authorship analysis at PAN</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Giachanou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Manjavacas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Specht</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Zangerle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,206.26,126.52,138.50,4.51">Advances in Information Retrieval</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Magalhães</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Castells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Silva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Martins</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="508" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,111.11,150.43,395.48,4.51;16,111.11,162.39,343.15,4.51;16,111.11,174.34,388.36,4.51;16,111.11,186.30,388.58,4.51;16,111.11,196.20,223.58,8.88;16,111.11,210.21,177.07,4.51" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="16,147.96,162.39,269.05,4.51">Overview of the cross-domain authorship verification task at PAN</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Manjavacas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Markov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_264.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="16,326.79,174.34,172.68,4.51;16,111.11,186.30,135.02,4.51">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="16,111.11,196.20,138.03,8.88">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-09-22">2020. September 22-25, 2020. 2696. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,111.11,222.16,378.98,4.51;16,111.11,234.12,394.70,4.51;16,111.11,246.07,381.98,4.51;16,111.11,258.03,312.78,4.51" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="16,111.11,234.12,394.70,4.51;16,111.11,246.07,89.14,4.51">Overview of the author identification task at PAN-2018: cross-domain authorship attribution and style change detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tschuggnall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Specht</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,221.18,246.07,228.33,4.51">Working Notes Papers of the CLEF 2018 Evaluation Labs</title>
		<editor>et al.</editor>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14, 2018/Cappellato. 2018</date>
			<biblScope unit="page" from="1" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,111.11,269.98,384.90,4.51;16,111.11,281.94,356.32,4.51;16,111.11,293.89,381.77,4.51;16,111.11,305.85,164.54,4.51" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="16,445.65,269.98,50.35,4.51;16,111.11,281.94,241.49,4.51">Overview of the Cross-domain Authorship Attribution Task at PAN 2019</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Manjavacas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380/" />
	</analytic>
	<monogr>
		<title level="m" coord="16,224.28,293.89,127.39,4.51">CLEF 2019 Labs and Workshops</title>
		<title level="s" coord="16,358.96,293.89,94.11,4.51">Notebook Papers, CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Losada</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,111.11,317.80,375.52,4.51" xml:id="b9">
	<monogr>
		<title level="m" coord="16,231.85,317.80,123.34,4.51">The Fan Fiction Studies Reader</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Hellekson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Busse</surname></persName>
		</editor>
		<imprint>
			<publisher>University of Iowa Press</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,111.11,329.76,392.61,4.51;16,111.11,341.71,91.72,4.51" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="16,161.30,329.76,288.58,4.51">Fanfiction and the Author. How FanFic Changes Popular Cultural Texts</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fathallah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Amsterdam University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,111.11,353.67,382.70,4.51;16,111.11,365.62,85.61,4.51" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="16,185.66,353.67,230.15,4.51">Verification of forecasts expressed in terms of probability</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">W</forename><surname>Brier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,423.24,353.67,70.57,4.51;16,111.11,365.62,27.49,4.51">Monthly weather review</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="1950">1950</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,111.11,377.58,372.50,4.51;16,110.76,389.53,344.30,4.51;16,110.81,401.49,372.34,4.51;16,110.88,413.44,44.71,4.51" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="16,200.75,377.58,165.78,4.51">A simple measure to assess non-response</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rodrigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,387.87,377.58,95.74,4.51;16,110.76,389.53,344.30,4.51;16,110.81,401.49,59.42,4.51;16,215.48,401.49,31.57,4.51">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1415" to="1424" />
		</imprint>
	</monogr>
	<note>HLT &apos;11</note>
</biblStruct>

<biblStruct coords="16,111.11,425.40,365.50,4.51;16,110.95,437.35,355.52,4.51;16,110.76,449.31,353.23,4.51;16,110.81,461.26,379.08,4.51;16,111.11,473.22,332.13,4.51;16,111.11,485.18,279.96,4.51" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="16,300.83,425.40,158.14,4.51">Generalizing unmasking for short texts</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1068</idno>
		<ptr target="https://doi.org/10.18653/v1/n19-1068.doi:10.18653/v1/n19-1068" />
	</analytic>
	<monogr>
		<title level="m" coord="16,268.13,437.35,198.33,4.51;16,110.76,449.31,353.23,4.51;16,110.81,461.26,131.33,4.51">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Burstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Doran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Solorio</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">June 2-7, 2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="654" to="659" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="16,111.11,497.13,345.36,4.51;16,111.11,509.09,369.31,4.51;16,111.11,521.04,392.57,4.51;16,111.11,533.00,122.36,4.51" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="16,214.92,521.04,163.72,4.51">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,386.74,521.04,116.93,4.51;16,111.11,533.00,36.44,4.51">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,111.11,544.95,393.08,4.51;16,111.11,556.91,381.50,4.51;16,111.11,568.86,382.38,4.51;16,110.86,578.76,287.83,8.88;16,111.11,592.77,172.43,4.51" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="16,203.01,544.95,301.18,4.51;16,111.11,556.91,72.52,4.51">Cross-domain authorship attribution based on compression: Notebook for PAN at CLEF 2018</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Halvani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Graner</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2125/paper_90.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="16,395.89,556.91,96.72,4.51;16,111.11,568.86,210.98,4.51">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="16,175.10,578.76,138.04,8.88">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Nie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Soulier</surname></persName>
		</editor>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14, 2018. 2125. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,111.11,604.73,394.88,4.51;16,111.11,616.68,283.40,4.51;16,111.11,628.64,330.99,4.51" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="16,382.55,604.73,123.44,4.51;16,111.11,616.68,48.15,4.51">Authenticating the writings of julius caesar</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Stover</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Karsdorp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2016.06.029</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2016.06.029.doi:10.1016/j.eswa.2016.06.029" />
	</analytic>
	<monogr>
		<title level="j" coord="16,166.71,616.68,137.87,4.51">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="86" to="96" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,111.11,640.59,366.98,4.51;16,111.11,652.55,369.62,4.51;16,110.81,662.45,385.82,8.88;17,111.11,88.60,263.25,8.88" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="16,195.82,640.59,241.99,4.51">Authorship verification as a one-class classification problem</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schler</surname></persName>
		</author>
		<idno type="DOI">10.1145/1015330.1015448</idno>
	</analytic>
	<monogr>
		<title level="m" coord="16,168.76,652.55,311.97,4.51;16,110.81,664.50,48.22,4.51">Machine Learning, Proceedings of the Twenty-first International Conference (ICML 2004)</title>
		<title level="s" coord="16,376.34,662.45,120.29,8.88;17,111.11,88.60,66.02,8.88">ACM International Conference Proceeding Series</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</editor>
		<meeting><address><addrLine>Banff, Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">July 4-8, 2004. 2004</date>
			<biblScope unit="volume">69</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,111.11,102.61,369.91,4.51;17,110.95,114.56,380.52,4.51;17,110.76,126.52,369.36,4.51;17,110.76,138.48,272.70,4.51;17,111.11,150.43,186.10,4.51" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="17,300.83,102.61,162.19,4.51">Generalizing Unmasking for Short Texts</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/N19-1068" />
	</analytic>
	<monogr>
		<title level="m" coord="17,288.40,114.56,203.07,4.51;17,110.76,126.52,365.41,4.51">Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL 2019)</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Burstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Doran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Solorio</surname></persName>
		</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="654" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,111.11,162.39,356.94,4.51;17,111.11,174.34,376.00,4.51;17,111.11,186.30,20.72,4.51" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="17,144.97,162.39,186.74,4.51">UniNE at PAN-CLEF 2021: Author verification</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ikae</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="17,218.51,174.34,127.39,4.51">CLEF 2021 Labs and Workshops</title>
		<title level="s" coord="17,353.20,174.34,94.11,4.51">Notebook Papers, CEUR</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,111.11,198.25,369.53,4.51;17,111.11,210.21,376.22,4.51;17,111.11,222.16,246.22,4.51" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="17,232.15,198.25,248.49,4.51;17,111.11,210.21,86.37,4.51">Authorship verification with neural networks via stylometric feature concatenation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Menta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia-Serrano</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="17,444.22,210.21,43.10,4.51;17,111.11,222.16,81.80,4.51">CLEF 2021 Labs and Workshops</title>
		<title level="s" coord="17,200.20,222.16,94.11,4.51">Notebook Papers, CEUR</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,111.11,234.12,388.31,4.51;17,111.11,246.07,381.17,4.51;17,111.11,258.03,246.22,4.51" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="17,297.51,234.12,201.91,4.51;17,111.11,246.07,91.64,4.51">Authorship verification of language models based on Lucene architecture</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="17,449.18,246.07,43.10,4.51;17,111.11,258.03,81.80,4.51">CLEF 2021 Labs and Workshops</title>
		<title level="s" coord="17,200.20,258.03,94.11,4.51">Notebook Papers, CEUR</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,111.11,269.98,386.60,4.51;17,111.11,281.94,382.55,4.51;17,111.11,293.89,246.22,4.51" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="17,275.67,269.98,222.04,4.51;17,111.11,281.94,93.20,4.51">Feature vector difference based authorship verification for open world settings</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Weerasinghe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Greenstadt</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="17,450.56,281.94,43.10,4.51;17,111.11,293.89,81.80,4.51">CLEF 2021 Labs and Workshops</title>
		<title level="s" coord="17,200.20,293.89,94.11,4.51">Notebook Papers, CEUR</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,111.11,305.85,359.22,4.51;17,111.11,317.80,392.99,4.51;17,110.81,329.76,319.09,4.51" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="17,284.46,305.85,185.87,4.51;17,111.11,317.80,176.54,4.51">O2D2: Out-of-distribution detector to capture undecidable trials in authorship verification</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Boenninghoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kolossa</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="17,138.09,329.76,127.39,4.51">CLEF 2021 Labs and Workshops</title>
		<title level="s" coord="17,272.78,329.76,94.11,4.51">Notebook Papers, CEUR</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,111.11,341.71,388.81,4.51;17,111.11,353.67,380.87,4.51;17,111.11,365.62,246.22,4.51" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="17,289.75,341.71,210.17,4.51;17,111.11,353.67,91.53,4.51">Encoding text information by pre-trained model for authorship verification</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="17,448.88,353.67,43.10,4.51;17,111.11,365.62,81.80,4.51">CLEF 2021 Labs and Workshops</title>
		<title level="s" coord="17,200.20,365.62,94.11,4.51">Notebook Papers, CEUR</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,111.11,377.58,369.89,4.51;17,111.11,389.53,357.75,4.51;17,111.11,401.49,157.12,4.51" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="17,170.83,377.58,292.39,4.51">Author classification as pre-training for pairwise authorship verification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Futrzynski</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="17,336.66,389.53,127.39,4.51">CLEF 2021 Labs and Workshops</title>
		<title level="s" coord="17,111.11,401.49,94.11,4.51">Notebook Papers, CEUR</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,111.11,413.44,385.20,4.51;17,111.11,425.40,368.49,4.51;17,111.11,437.35,376.00,4.51;17,111.11,449.31,20.72,4.51" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="17,111.11,425.40,232.15,4.51">Graph-based Siamese network for authorship verification</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Embarcadero-Ruiz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Gómez-Adorno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Reyes-Hernández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Embarcadero-Ruiz</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="17,218.51,437.35,127.39,4.51">CLEF 2021 Labs and Workshops</title>
		<title level="s" coord="17,353.20,437.35,94.11,4.51">Notebook Papers, CEUR</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,111.11,461.26,385.46,4.51;17,110.76,473.22,345.50,4.51;17,111.11,485.18,82.91,4.51" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="17,231.20,461.26,160.74,4.51">Siamese Bert for authorship verification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tyo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lipton</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="17,249.85,473.22,127.39,4.51">CLEF 2021 Labs and Workshops</title>
		<title level="s" coord="17,384.54,473.22,71.72,4.51;17,111.11,485.18,19.90,4.51">Notebook Papers, CEUR</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,111.11,497.13,393.18,4.51;17,110.86,509.09,372.77,4.51;17,110.63,521.04,207.63,4.51" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="17,269.17,497.13,235.13,4.51;17,110.86,509.09,44.85,4.51">Feature similarity-based regression models for authorship verification</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pinzhakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Yagel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rabinovits</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="17,401.94,509.09,81.68,4.51;17,110.63,521.04,43.22,4.51">CLEF 2021 Labs and Workshops</title>
		<title level="s" coord="17,161.14,521.04,94.11,4.51">Notebook Papers, CEUR</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,111.11,533.00,392.73,4.51;17,111.11,544.95,372.59,4.51" xml:id="b29">
	<monogr>
		<title level="m" type="main" coord="17,155.11,533.00,161.79,4.51">Siamese Neural Networks: An Overview</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chicco</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-0716-0826-5_3</idno>
		<ptr target="https://doi.org/10.1007/978-1-0716-0826-5_3.doi:10.1007/978-1-0716-0826-5_3" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Springer US</publisher>
			<biblScope unit="page" from="73" to="94" />
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,111.11,556.91,357.53,4.51;17,110.63,568.86,150.46,4.51" xml:id="b30">
	<monogr>
		<title level="m" type="main" coord="17,169.29,556.91,285.83,4.51">Computer-Intensive Methods for Testing Hypotheses: An Introduction</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">W</forename><surname>Noreen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>A Wiley-Interscience publication</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,111.11,580.82,382.85,4.51;17,111.11,592.77,345.21,4.51;17,111.11,604.73,159.71,4.51" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="17,213.21,580.82,276.93,4.51">A transfer learning approach to cross-domain authorship attribution</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Barlas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12530-021-09377-2</idno>
		<ptr target="https://link.springer.com/10.1007/s12530-021-09377-2.doi:10.1007/s12530-021-09377-2" />
	</analytic>
	<monogr>
		<title level="j" coord="17,111.11,592.77,71.80,4.51">Evolving Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
