<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.32,386.66,16.17;1,89.29,108.89,160.62,10.37">Detection of Hate Speech Spreaders with BERT Notebook for PAN at CLEF 2021</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,89.29,134.79,61.19,10.37"><forename type="first">David</forename><surname>Dukiƒá</surname></persName>
							<email>david.dukic@fer.hr</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering and Computing</orgName>
								<orgName type="institution">University of Zagreb</orgName>
								<address>
									<addrLine>Unska 3</addrLine>
									<postCode>10000</postCode>
									<settlement>Zagreb</settlement>
									<country key="HR">Croatia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,163.93,134.79,79.50,10.37"><forename type="first">Ana</forename><forename type="middle">Soviƒá</forename><surname>Kr≈æiƒá</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering and Computing</orgName>
								<orgName type="institution">University of Zagreb</orgName>
								<address>
									<addrLine>Unska 3</addrLine>
									<postCode>10000</postCode>
									<settlement>Zagreb</settlement>
									<country key="HR">Croatia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.32,386.66,16.17;1,89.29,108.89,160.62,10.37">Detection of Hate Speech Spreaders with BERT Notebook for PAN at CLEF 2021</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">7D99F837DDB14589B8450244BC48D7CB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>BERT</term>
					<term>fine-tuning</term>
					<term>indicators</term>
					<term>logistic regression</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As social media grows, more and more users are disseminating hate speech through their posts. This often comes as a consequence of feeling a false security and anonymity in virtual environment. To stop hate speech spreaders, researchers started developing machine learning systems that automatically detect spreaders of hate speech based on the contents of their posts. This paper describes one such system which was trained on a corpus of English Twitter posts with a goal to predict if author of the given posts spreads hate speech or not. The features were crafted using fine-tuned BERT contextualized embeddings summed over the last 12 hidden states corresponding to the classification token, concatenated with the three binary variables called indicators. Binary variables were indicating whether hashtag, retweet or url were present in author's tweet posts, respectively. Feature vectors were then fed into a Logistic Regression classifier. Described model achieved 75% of accuracy score on the test set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the last decade, social networks started to attract a vast number of newcomers. Users can now share everything they desire with their followers and express their opinions in an instant. While the freedom of speech is important and generally should not be restricted, not all kinds of free speech should be tolerated. This brings us to the subject of hate speech. Freedom of speech in virtual environment gave users a false sense of security, a sense that even hate speech could be shared without repercussions. Since the network is huge, it is almost impossible to stop hate speech spreaders using only human resources. Therefore, numerous computational methods are being developed to enable automated detection of hate spreaders on social networks.</p><p>To support and encourage creation of machine learning systems equipped to detect users that spread hate speech, PAN 1 organized a shared task called Profiling Hate Speech Spreaders on Twitter 2 <ref type="bibr" coords="1,143.47,538.55,11.73,9.46" target="#b0">[1]</ref>. This task was one of the shared tasks in this year's PAN at CLEF 2021 <ref type="bibr" coords="1,492.25,538.55,11.73,9.46" target="#b1">[2]</ref>. Developed models were deployed on the TIRA platform <ref type="bibr" coords="1,337.79,552.10,11.59,9.46" target="#b2">[3]</ref>. This paper describes how given task can be solved using a combination of fine-tuned Bidirectional Encoder Representations from Transformers (BERT) <ref type="bibr" coords="2,364.41,101.60,11.60,9.46" target="#b3">[4]</ref>, binary variables (hashtag, retweet (rt), and url occurrence indicators), and Logistic Regression model. We give a brief related work overview in Section 2. Further, a data set description is presented in Section 3 followed by a detailed description of used features and models in Section 4. Implementation details can be located in Section 5 while evaluation results are displayed in Section 6 alongside an ablation study of different types of features. Lastly, we conclude the paper in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The task of automatic hate speech detection from an author profiling viewpoint is relatively new. Therefore, there were not many proposed solutions.</p><p>In the past few years PAN organized multiple author profiling shared tasks including bots and gender profiling in Twitter <ref type="bibr" coords="2,205.52,269.94,11.46,9.46" target="#b4">[5]</ref>, profiling fake news spreaders on Twitter <ref type="bibr" coords="2,399.01,269.94,11.47,9.46" target="#b5">[6]</ref>, gender and language variety identification in Twitter <ref type="bibr" coords="2,229.65,283.49,11.73,9.46" target="#b6">[7]</ref>, and multimodal gender identification in Twitter <ref type="bibr" coords="2,461.05,283.49,11.73,9.46" target="#b7">[8]</ref>. There were also tasks closely related to the task solved in this paper. Some refer to the hate speech directed towards specific groups of people. Examples are two tasks that deal with misogyny identification <ref type="bibr" coords="2,149.60,324.14,11.81,9.46" target="#b8">[9,</ref><ref type="bibr" coords="2,164.14,324.14,12.95,9.46" target="#b9">10]</ref>, and hate speech task against immigrants and women in Twitter <ref type="bibr" coords="2,460.64,324.14,16.72,9.46" target="#b10">[11]</ref>.</p><p>The most similar task to the task at hand is hate speech detection task <ref type="bibr" coords="2,416.85,337.69,18.32,9.46" target="#b11">[12]</ref> given in Italian language. Two data sets were used. One was obtained from Facebook comments and the other from Twitter posts. Highest performing system had 82.88% of macro F1-score on the Facebook data set and 79.93% on the Twitter data set. Some authors approached hate speech detection problem from text using deep learning <ref type="bibr" coords="2,471.19,391.89,17.14,9.46" target="#b12">[13,</ref><ref type="bibr" coords="2,490.75,391.89,12.86,9.46" target="#b13">14]</ref>. Others conducted detailed research of existing methods for automatic detection of hate speech <ref type="bibr" coords="2,89.29,418.99,17.40,9.46" target="#b14">[15,</ref><ref type="bibr" coords="2,109.93,418.99,13.05,9.46" target="#b15">16]</ref>. These surveys show that researchers also used methods from the traditional natural language processing toolbox.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data Set</head><p>The data set for this task was given in English and Spanish language. We only used the English part of the corpus and did not develop model for Spanish language. English language corpus had data from 200 authors in the train set and 100 authors in the test set. Half of all the authors in both sets were labeled with 1 indicating that the author spreads hate speech, while the other half in both sets was labeled with 0 to indicate that the author does not spread hate speech. Data was collected from users that posted on the Twitter social network. Each of the total 300 authors had 200 unique tweet posts. This sums up to 60000 tweet posts which seems as a reasonable number. However, if we consider that the task is to predict on author-level rather than on tweet-level, and also that we cannot use test set for creating the model, we end up with only 200 training examples. Moreover, it is possible that not all posts, from an author that spreads hate speech, are genuine hate speech. Hence, this brings additional noise into the data set and aggravates prediction on author-level.</p><p>In order to tackle the small data set problem and also the noisy data set problem, we incorporated two approaches. To enlarge the training corpus, we concatenated every 20 tweet posts of each author into a separate training example. Thus, we got 10 training examples for each author and labeled them with original label that was assigned to author whose tweets we concatenated. This produced 2000 training examples and enabled creation of the model that took 20 concatenated tweets, as a single data point, at its input. Test set was likewise restructured by concatenating every 20 tweet posts grouped by author into a new training example. However, that did not result in larger test corpus since we still predicted on author-level. More details on our prediction idea comes in Section 6. It is worth mentioning that we tried concatenating other number of tweet posts e.g. 5, 10, 25, but 20 worked the best for our final model performance. To mitigate the noise effect, we conducted an analysis of what indicators we could use to better discriminate between hate speech spreaders and the non hate speech spreaders. The indicators are variables that occur in most tweet posts and their presence or absence could indicate to which class author belongs.</p><p>Furthermore, some tweet posts had the following html entities: &amp;amp;, &amp;quot;, &amp;apos;, &amp;gt;, and &amp;lt;. They were replaced with appropriate punctuation that they represent. Hashtag, mention, and url occurrences were already substituted with #hashtag#, #user#, and #url# respectively. Afterwards, three distinct cleaning techniques were applied:</p><p>1. All the text in tweet post was lowercased, unicode errors were fixed, weird characters were transliterated to the closest ASCII representation, and emojis were removed 2. The same as previous cleaning method with additional removal of all the punctuation 3. The same as previous cleaning method with additional removal of the lowercased hashtag, user, rt, and url occurrences.</p><p>It is important to note that all three cleaning techniques were applied separately and tested separately. Surprisingly, the first cleaning method always gave best results independent of the model we tried when we used BERT for feature extraction. It seems that BERT works better with punctuation, as if interpunction provides extra context. Hence, we used data cleaned with first cleaning method for the models that we describe in this paper.</p><p>Indicators analysis is presented in Figure <ref type="figure" coords="3,280.64,443.04,4.07,9.46" target="#fig_0">1</ref>. Analysis was done on separate tweet posts where each tweet was labeled with hate/no hate depending on the corresponding author's class. We estimated the probabilities ùëÉ (Class|Indicator ) for each of the five indicators: hashtag, user, rt, url, and emoji using the maximum likelihood estimation (MLE) with Laplacian smoothing:</p><formula xml:id="formula_0" coords="3,199.84,506.06,194.39,24.43">ùëÉ (Class = ùëê|Indicator = ùëñ) = ùê∂(c, i ) + 1 ùê∂(i ) + 2</formula><p>where ùê∂ denotes count value i.e. frequency. Probabilities sum to 1.0 inside each indicator variable by classes. We can observe that there exists significant difference between the two classes for indicators hashtag, rt, url, and emoji. Indicator user did not turn out as indicating for discrimination between the classes and therefore was not used for feature creation. On the other hand, all the other indicators were utilized for crafting the features. hashtag, rt, and url were used as binary variables (verbatim indicators) while emoji were utilized with the help of emoji2vec embeddings <ref type="bibr" coords="3,144.74,620.22,18.17,9.46" target="#b16">[17]</ref> for some alternative models we developed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Model and Feature Engineering</head><p>Models that achieved best results on the test set were essentially Logistic Regression classifiers with specific features. Generally speaking, feature vector for each training example was created using a concatenation of the following three different types of feature vectors:</p><p>1. Fine-tuned BERT embedding 2. emoji2vec embedding 3. Three binary indicator variables.</p><p>Some models used all three feature vector types, while others used only a concatenated subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Fine-tuned BERT Embeddings</head><p>For the first part, BERT BASE (uncased) model was used. To be specific, a Hugging Face Fine-tuning details can be found in Section 5. It is important to note that a part of training set was held out as a validation set for hyperparameters optimization when we fine-tuned the model on the training set. The model with highest accuracy score on validation set was used as a final fine-tuned BERT model.</p><p>From the fine-tuned BertForSequenceClassification, the 768-dimensional embeddings corresponding to the [CLS] token were extracted. We experimented with different embeddings from different hidden states. Since BERT BASE is composed of 12 stacked encoders, it is possible to extract embeddings from hidden states corresponding to each of the 12 encoders. Thus, we experimented with the following embedding combination options (numbers in brackets refer to the length of each feature vector):</p><p>1. Only embedding from the last hidden state (768) 2. Sum of embedding vectors from the 12 hidden states (768) 3. Average over the embedding vectors from the 12 hidden states (768) 4. Sum of embedding vector from the last 4 hidden states (768) 5. Average over the embedding vectors from the last 4 hidden states (768) 6. Concatenated embeddings from the last 4 hidden states (3072).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">emoji2vec Embedding</head><p>Regarding the second part, 300-dimensional emoji2vec embeddings were exploited. As mentioned before, every 20 tweet posts were concatenated into a new training example. Ergo, the emojis from these posts were concatenated. Although each BERT embedding was extracted from the training example that was stripped of emojis, they were not discarded. Only the unique emojis from the concatenated string were used (if some emoji occured more than once, that information was ignored). Each of the emojis had unique emoji2vec embedding and was summed cumulatively with starting 300 dimensional embedding of zeros. Consequently, if the training example before cleaning contained no emojis, its emoji embedding was a 300-dimensional vector of zeros.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Binary Indicator Variables</head><p>Lastly, the three binary indicator variables were indicating whether the training example contained at least one hashtag, rt or url occurrence. Therefore, each of the three dimensions was either 1 if the occurrence was present or 0 otherwise.</p><p>The model that achieved highest accuracy score on the test set used a sum of fine-tuned BERT embeddings from the 12 hidden states concatenated with three indicator variables in combination with Logistic Regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Implementation Details</head><p>To be consistent and train all models one the same part of the train set, tune hyperparameters on the same validation set, and rate prediction success on the same test set, 50 authors and their tweet posts were held out from the train set to act as a validation set. Therefore, train set consisted of tweet posts from 150 authors, validation set of 50 authors, and test set of 100 authors. After tuning the hyperparameters on the validation set, each model was retrained on the original train set and then the predictions were obtained on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Fine-tuning BERT</head><p>BertForSequenceClassification model was fine-tuned on the tweet posts of 150 authors from the train set. Number of epochs was set a priori to 200. Moreover, early stopping method was implemented that stopped the fine-tuning process if there was no improvement in terms of accuracy score on the validation set for more than 20 epochs. Because of that, the fine-tuning process never lasted longer than 50 epochs. During the fine-tuning we employed Adam optimizer, cross-entropy loss as a loss function, and a linear learning rate scheduler with zero warmup steps. Model was fine-tuned using mini batches with size equal to 5 data points in a single mini batch. Prediction on validation set was done on author-level using a threshold method that we introduce in the next section. Best learning rate was 1 √ó 10 -7 , while best threshold was the one equal to 50%. Mentioned hyperparameters were optimized with randomized search. This fine-tuned model was trained only on 75% of the train set and achieved 72% of accuracy score on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Hyperparameter Optimization for Traditional Models</head><p>We tried three different traditional classifiers: Logistic Regression (LR), Support Vector Machine (SVM) with radial basis function (RBF) kernel, and Random Forest (RF) classifier with no bootstrap samples used for building decision trees.</p><p>Hyperparameters for these models were optimized using a grid search method with same fixed validation set for each classifier. Values of hyperparameters optimized for each of the classifiers are presented in Table <ref type="table" coords="6,188.74,420.01,4.14,9.46">1</ref>. Hyperparameter ùê∂ represents inverse of regularization strength while ùõæ refers to the RBF kernel coefficient. For RF, max_depth defines the maximum depth of each decision tree. ùëÅ ùëúùëõùëí value specifies that nodes are being expanded while it is possible to make splits. n_estimators is the parameter that determines how many decision tree classifiers are in the random forest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Different traditional machine learning classifiers and their hyperparameters which were optimized using a grid search method in order to maximize the prediction success on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classifier</head><p>Hyperparameters Values </p><formula xml:id="formula_1" coords="6,140.78,566.99,322.78,48.32">LR ùê∂ {2 -6 , 2 -5 , ..., 2 0 , 2 1 , 3, 4, 5, 6, 7, 2 3 , 2 4 , 2 5 , 2 6 } SVM ùê∂ {2 -8 , 2 -7 , ..., 2 7 , 2 8 } ùõæ {2 -8 , 2 -7 , ...,<label>2</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Evaluation</head><p>Before presenting our evaluation results and the ablation study, it is necessary to clarify the concept behind model prediction. Namely, the idea was to predict with a threshold. The prediction for single author happens in three steps:</p><p>1. 200 tweet posts for each author are split into 10 tweet posts by concatenating every 20 tweet posts into a single data point 2. For each of the created 10 data points the prediction is obtained (either 0 or 1) 3. If we set the threshold to e.g. 50% and less than 50% of predictions are zeros, then the final prediction for that author is 1, videlicet author spreads hate speech. Otherwise, the final prediction for that author is 0 which means that he does not in fact share hate speech.</p><p>We experimented with prediction thresholds set to 30%, 50%, and 70%. Surprisingly enough, the threshold of 50% always gave the best results and was tuned on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Results on the Test Set</head><p>Table <ref type="table" coords="7,117.12,315.45,5.56,9.46">2</ref> displays evaluation results on the test set. Moreover, careful reader will notice that this table also shows a simple ablation study, that is what happens when we remove some type of features from the model and how does the removal affect model performance?. Although the specified evaluation metric for the shared task was accuracy, we present results through three additional standard evaluation metrics: precision, recall, and F1-score. First column shows which combinations of features and classifier were tried out. Regarding the features, fine-tuned BERT embeddings were tested solely or in combination with emoji2vec and indicator features. Additionally, we experimented with the 6 BERT embedding combination options. Three traditional classifiers were used: LR, SVM, and RF classifier. We can observe how, for most feature combinations and evaluation metrics, Logistic Regression outperformed the other two classifiers. The model that achieved highest accuracy and F1-score was the one that used fine-tuned BERT embeddings summed over the last 12 hidden states combined with indicator features and LR classifier. High recall of 86% indicates minimization of false negatives. This is useful from the interpretation perspective because only in 14% of cases the hate speech spreaders detection system would fail to detect them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Evaluation results on the test set for all tried combinations of features and traditional classifiers. Four evaluation metrics were used: precision (P), recall (R), F1-score (F1), and accuracy (Acc). BERT refers to BERT embedding extracted after the fine-tuning phase was finished. Superscripts denote specific embedding combination option introduced in Section 4. emoji represents emoji2vec features while indicators describe the three aforementioned binary variables. Bold scores denote the features and classifier combination that gave the proposed model with highest F1-score and accuracy on the test set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>Automated hate speech detection is already an important area of research among computer scientists. This paper's contribution to the community lies in the development of a machine learning model that successfully detects English hate speech spreading authors with 75% of accuracy score. Various models were tried out. The one that worked best used a combination of fine-tuned BERT embeddings, indicator binary variables, and LR classifier.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,263.18,416.69,9.35;4,89.29,275.23,353.16,9.22;4,168.04,84.19,259.20,172.80"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Estimated probability distributions for each possible indicator on the entire train set on a tweet-level. Probabilities were estimated using MLE with Laplacian smoothing.</figDesc><graphic coords="4,168.04,84.19,259.20,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,89.29,485.46,416.88,160.55"><head></head><label></label><figDesc>Special classification token [CLS] was added at the beginning of the each training example, while special separation token [SEP] was added at the end of each training example. Longer tweets than 300 tokens were cut off, and shorter tweets were padded with zeros until there were 300 tokens present in the training example.</figDesc><table /><note coords="4,501.50,485.46,3.99,6.91;4,89.29,501.06,416.70,9.46;4,89.29,514.61,416.69,9.46;4,89.29,528.16,416.69,9.46;4,89.29,541.71,416.69,9.46;4,89.29,554.91,416.69,9.81;4,89.29,568.80,416.88,9.46;4,89.29,582.35,319.45,9.46"><p><p>3  </p>implementation called BertForSequenceClassification was used which is in fact BERT model with a linear layer on top of the model to enable classification and fine-tuning. Since the data set was enlarged by merging every 20 tweets into a new training example, the average length of each train example (after split over whitespaces was applied) came to be around 250 tokens and 75% train examples had length less than or equal to 270 tokens. Hence, maximum number of tokens for BERT model was set to 300. Tokens can be either words, characters or subwords. Training examples were tokenized using BertTokenizer.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,144.66,605.67,278.31,48.50"><head></head><label></label><figDesc>7 , 2 8 }</figDesc><table coords="6,144.66,628.77,278.31,25.40"><row><cell>RF</cell><cell>max_depth</cell><cell>{10, 50, 100, 200, ùëÅ ùëúùëõùëí}</cell></row><row><cell></cell><cell>n_estimators</cell><cell>{128, 512, 1024, 2048}</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,99.53,193.85,401.43,314.36"><head></head><label></label><figDesc>, 71.186 2 , 64.4073 , 67.6924 , 65.5175 , 69.355 6 78.000 1 , 84.000 2 , 75.0003 , 88.0004 , 75.0005 , 86.000 6 BERT + emoji + LR 67.213 1 , 72.414 2 , 63.793 3 , 67.188 4 , 62.500 5 , 69.841 6 82.000 1 , 84.000 2 , 74.000 3 , 86.000 4 , 70.000 5 , 88.000 6 BERT + indicators + LR 66.129 1 , 71.667 2 , 65.517 3 , 67.742 4 , 63.158 5 , 69.841 6 82.000 1 , 86.000 2 , 75.000 3 , 84.000 4 , 72.000 5 , 88.000 6 BERT + LR 65.079 1 , 69.355 2 , 63.158 3 , 67.742 4 , 63.158 5 , 68.750 6 82.000 1 , 86.000 2 , 72.000 3 , 84.000 4 , 72.000 5 , 88.000 6 BERT + emoji + indicators + SVM 58.182 1 , 64.706 2 , 65.000 3 , 61.404 4 , 59.259 5 , 63.043 6 64.000 1 , 66.000 2 , 78.000 3 , 70.000 4 , 64.000 5 , 58.000 6 BERT + emoji + SVM 58.182 1 , 64.706 2 , 68.333 3 , 61.404 4 , 58.929 5 , 63.043 6 64.000 1 , 66.000 2 , 82.000 3 , 70.000 4 , 66.000 5 , 58.000 6 BERT + indicators + SVM 62.745 1 , 64.815 2 , 66.038 3 , 63.793 4 , 63.158 5 , 62.963 6 64.000 1 , 70.000 2 , 70.000 3 , 74.000 4 , 72.000 5 , 68.000 6 BERT + SVM 62.745 1 , 64.815 2 , 63.462 3 , 63.793 4 , 62.264 5 , 62.963 6 64.000 1 , 70.000 2 , 66.000 3 , 74.000 4 , 66.000 5 , 68.000 6 BERT + emoji + indicators + RF 60.714 1 , 62.005 2 , 61.404 3 , 60.345 4 , 60.714 5 , 60.000 6 68.000 1 , 70.000 2 , 70.000 3 , 70.000 4 , 68.000 5 , 72.000 6 BERT + emoji + RF 63.793 1 , 63.636 2 , 60.714 3 , 61.017 4 , 63.158 5 , 68.000 6 74.000 1 , 70.000 2 , 68.000 3 , 72.000 4 , 72.000 5 , 68.000 6 BERT + indicators + RF 61.111 1 , 63.158 2 , 63.636 3 , 62.264 4 , 60.345 5 , 65.517 6 66.000 1 , 72.000 2 , 70.000 3 , 66.000 4 , 70.000 5 , 75.000 6 BERT + RF 63.793 1 , 62.069 2 , 59.615 3 , 60.000 4 , 60.714 5 , 60.000 6 74.000 1 , 72.000 2 , 62.000 3 , 66.000 4 , 68.000 5 , 72.000 6 , 77.064 2 , 69.725 3 , 76.522 4 , 70.370 5 , 76.786 6 69.000 1 , 75.000 2 , 67.000 3 , 73.000 4 , 68.000 5 , 74.000 6 BERT + emoji + LR 73.874 1 , 77.778 2 , 68.519 3 , 75.439 4 , 66.038 5 , 77.876 6 71.000 1 , 75.000 2 , 66.000 3 , 72.000 4 , 64.000 5 , 75.000 6 BERT + indicators + LR 73.214 1 , 78.182 2 , 70.370 3 , 75.000 4 , 67.290 5 , 77.876 6 70.000 1 , 75.000 2 , 68.000 3 , 72.000 4 , 65.000 5 , 75.000 6 BERT + LR 72.566 1 , 76.786 2 , 67.290 3 , 75.000 4 , 67.290 5 , 77.193 6 69.000 1 , 74.000 2 , 65.000 3 , 72.000 4 , 65.000 5 , 74.000 6 BERT + emoji + indicators + SVM 60.952 1 , 65.347 2 , 70.909 3 , 65.421 4 , 61.538 5 , 60.417 6 59.000 1 , 65.000 2 , 68.000 3 , 63.000 4 , 60.000 5 , 62.000 6 BERT + emoji + SVM 60.952 1 , 65.347 2 , 74.545 3 , 65.421 4 , 62.264 5 , 60.417 6 59.000 1 , 65.000 2 , 72.000 3 , 63.000 4 , 60.000 5 , 62.000 6 BERT + indicators + SVM 63.366 1 , 67.308 2 , 67.961 3 , 68.519 4 , 67.290 5 , 65.385 6 63.000 1 , 66.000 2 , 67.000 3 , 66.000 4 , 65.000 5 , 64.000 6 BERT + SVM 63.366 1 , 67.308 2 , 64.706 3 , 68.519 4 , 64.078 5 , 65.385 6 63.000 1 , 66.000 2 , 64.000 3 , 66.000 4 , 63.000 5 , 64.000 6 BERT + emoji + indicators + RF 64.151 1 , 66.038 2 , 65.421 3 , 64.815 4 , 64.151 5 , 65.455 6 62.000 1 , 64.000 2 , 63.000 3 , 62.000 4 , 62.000 5 , 62.000 6 BERT + emoji + RF 68.519 1 , 66.667 2 , 64.151 3 , 66.055 4 , 67.290 5 , 68.000 6 66.000 1 , 65.000 2 , 62.000 3 , 63.000 4 , 65.000 5 , 68.000 6 BERT + indicators + RF 63.462 1 , 67.290 2 , 66.667 3 , 64.078 4 , 64.815 5 , 70.370 6 62.000 1 , 65.000 2 , 65.000 3 , 63.000 4 , 62.000 5 , 68.000 6 BERT + RF 68.519 1 , 66.667 2 , 60.784 3 , 62.857 4 , 64.151 5 , 65.455 6 66.000 1 , 64.000 2 , 60.000 3 , 61.000 4 , 62.000 5 , 62.000 6</figDesc><table coords="8,102.30,193.85,334.39,184.20"><row><cell>Features + Classifier</cell><cell></cell><cell>P</cell><cell>R</cell></row><row><cell cols="2">BERT + emoji + indicators + LR 66.102 1 Features + Classifier</cell><cell>F1</cell><cell>Acc</cell></row><row><cell>BERT + emoji + indicators + LR</cell><cell>71.560 1</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="4,108.93,671.90,130.16,7.77"><p>https://huggingface.co/transformers/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,112.92,112.89,393.07,9.46;9,112.92,126.44,393.34,9.46;9,112.92,139.99,130.54,9.46" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,408.63,112.89,97.36,9.46;9,112.92,126.44,150.05,9.46">Profiling Hate Speech Spreaders on Twitter Task at PAN</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">L D L P</forename><surname>Sarrac√©n</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fersini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chulvi</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="9,309.68,126.44,144.89,9.46">CLEF 2021 Labs and Workshops</title>
		<title level="s" coord="9,462.54,126.44,43.71,9.46;9,112.92,139.99,57.34,9.46">Notebook Papers, CEUR</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.92,153.53,394.43,9.46;9,112.92,167.08,394.43,9.46;9,112.92,180.63,393.06,9.46;9,112.92,194.18,393.06,9.46;9,112.92,207.73,242.59,9.46" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,229.32,180.63,276.66,9.46;9,112.92,194.18,242.16,9.46">Overview of PAN 2021: Authorship Verification,Profiling Hate Speech Spreaders on Twitter,and Style Change Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chulvi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">L D L P</forename><surname>Sarrac√©n</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Manjavacas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Markov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mayerl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wolska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Zangerle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,377.83,194.18,128.15,9.46;9,112.92,207.73,168.03,9.46">12th International Conference of the CLEF Association (CLEF 2021)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.92,221.28,394.43,9.46;9,112.92,234.83,393.07,9.46;9,112.92,248.38,394.37,9.46;9,112.92,262.95,135.63,7.68" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,329.13,221.28,173.72,9.46">TIRA Integrated Research Architecture</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-22948-1_5</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,244.71,234.83,261.27,9.46;9,112.92,248.38,120.48,9.46">Information Retrieval Evaluation in a Changing World, The Information Retrieval Series</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.92,275.48,393.06,9.46;9,112.92,289.03,394.98,9.46;9,112.92,302.58,199.68,9.46" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="9,321.69,275.48,184.29,9.46;9,112.92,289.03,173.84,9.46">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1810.04805" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.92,316.12,393.06,9.46;9,112.92,329.67,393.37,9.46;9,112.92,343.22,394.58,9.46;9,112.62,356.77,107.64,9.46" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,215.05,316.12,290.93,9.46;9,112.92,329.67,93.18,9.46">Overview of the 7th Author Profiling Task at PAN 2019: Bots and Gender Profiling</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380/" />
	</analytic>
	<monogr>
		<title level="m" coord="9,479.09,329.67,27.21,9.46;9,112.92,343.22,114.26,9.46">CLEF 2019 Labs and Workshops</title>
		<title level="s" coord="9,235.10,343.22,121.32,9.46">Notebook Papers, CEUR-WS</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Losada</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.92,370.32,393.33,9.46;9,112.92,383.87,394.43,9.46;9,112.92,397.42,394.87,9.46;9,112.41,410.97,265.03,9.46" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,323.04,370.32,183.21,9.46;9,112.92,383.87,249.25,9.46">Overview of the 8th Author Profiling Task at PAN 2020: Profiling Fake News Spreaders on Twitter</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Giachanou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/" />
	</analytic>
	<monogr>
		<title level="m" coord="9,237.95,397.42,146.17,9.46">CLEF 2020 Labs and Workshops</title>
		<title level="s" coord="9,392.35,397.42,115.45,9.46;9,112.41,410.97,10.33,9.46">Notebook Papers, CEUR-WS</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>N√©v√©ol</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.92,424.52,393.34,9.46;9,112.92,438.07,394.43,9.46;9,112.92,451.62,394.70,9.46;9,112.41,465.17,395.49,9.46;9,112.92,478.71,155.22,9.46" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,312.94,424.52,193.31,9.46;9,112.92,438.07,305.88,9.46">Overview of the 5th Author Profiling Task at PAN 2017: Gender and Language Variety Identification in Twitter</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-1866/" />
	</analytic>
	<monogr>
		<title level="m" coord="9,298.13,451.62,209.48,9.46;9,112.41,465.17,96.06,9.46">CLEF 2017 Evaluation Labs and Workshop -Working Notes Papers</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Mandl</surname></persName>
		</editor>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-14">11-14 September. 2017</date>
			<biblScope unit="page" from="1" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.92,492.26,393.06,9.46;9,112.58,505.81,394.76,9.46;9,112.92,519.36,393.06,9.46;9,112.41,532.91,394.94,9.46;9,112.92,546.46,226.13,9.46" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,348.55,492.26,157.43,9.46;9,112.58,505.81,390.16,9.46">Overview of the 6th Author Profiling Task at PAN 2018: Cross-domain Authorship Attribution and Style Change Detection</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y-G√≥mez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2125/" />
	</analytic>
	<monogr>
		<title level="m" coord="9,360.50,519.36,145.48,9.46;9,112.41,532.91,155.18,9.46">CLEF 2018 Evaluation Labs and Workshop -Working Notes Papers</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Soulier</surname></persName>
		</editor>
		<meeting><address><addrLine>Avignon, France, CEUR-WS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09-14">10-14 September. 2018</date>
			<biblScope unit="page" from="1" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.92,560.01,393.44,9.46;9,112.92,573.56,393.80,9.46;9,112.92,587.11,13.64,9.46" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,252.36,560.01,254.00,9.46;9,112.92,573.56,82.83,9.46">Overview of the evalita 2018 task on automatic misogyny identification (ami)</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fersini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nozza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,204.13,573.56,256.84,9.46">EVALITA Evaluation of NLP and Speech Tools for Italian</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">59</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.92,600.66,394.88,9.46;9,112.92,614.21,282.22,9.46" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,266.85,600.66,240.95,9.46;9,112.92,614.21,85.81,9.46">Overview of the task on automatic misogyny identification at ibereval 2018</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fersini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Anzovino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,211.75,614.21,83.40,9.46">IberEval@ SEPLN</title>
		<imprint>
			<biblScope unit="volume">2150</biblScope>
			<biblScope unit="page" from="214" to="228" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.92,627.76,394.43,9.46;9,112.92,641.31,393.06,9.46;9,112.92,654.85,394.43,9.46;9,112.53,668.40,395.36,9.46;10,112.92,88.05,151.11,9.46" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,112.92,641.31,393.06,9.46;9,112.92,654.85,42.40,9.46">SemEval-2019 task 5: Multilingual detection of hate speech against immigrants and women in Twitter</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bosco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fersini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nozza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Patti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanguinetti</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S19-2007</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,180.44,654.85,326.91,9.46;9,112.53,668.40,180.45,9.46">Proceedings of the 13th International Workshop on Semantic Evaluation, Association for Computational Linguistics</title>
		<meeting>the 13th International Workshop on Semantic Evaluation, Association for Computational Linguistics<address><addrLine>Minneapolis, Minnesota, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="54" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.92,101.60,393.06,9.46;10,112.92,115.14,393.06,9.46;10,112.92,128.69,383.72,9.46" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,379.51,101.60,126.47,9.46;10,112.92,115.14,116.67,9.46">Overview of the evalita 2018 hate speech detection task</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bosco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Poletto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanguinetti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Maurizio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,257.00,115.14,248.98,9.46;10,112.92,128.69,217.38,9.46">EVALITA 2018-Sixth Evaluation Campaign of Natural Language Processing and Speech Tools for Italian</title>
		<imprint>
			<publisher>CEUR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2263</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.92,142.24,393.06,9.46;10,112.92,155.79,393.07,9.46;10,112.92,169.34,45.45,9.46" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,435.37,142.24,70.61,9.46;10,112.92,155.79,277.17,9.46">Data driven and psycholinguistics motivated approaches to hate speech detection</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Caetano Da Silva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Castro</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Silva Ramos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Paraboni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,398.23,155.79,107.75,9.46">Computaci√≥n y Sistemas</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.92,182.89,393.06,9.46;10,112.92,196.44,393.07,9.46;10,112.92,209.99,243.64,9.46" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,306.95,182.89,199.03,9.46;10,112.92,196.44,80.65,9.46">Improving hate speech detection with deep learning ensembles</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zimmerman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Kruschwitz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,216.97,196.44,289.02,9.46;10,112.92,209.99,173.49,9.46">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.92,223.54,393.06,9.46;10,112.92,237.09,197.86,9.46" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,218.31,223.54,250.81,9.46">A survey on automatic detection of hate speech in text</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Fortuna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nunes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,481.08,223.54,24.90,9.46;10,112.92,237.09,125.14,9.46">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.92,250.64,393.06,9.46;10,112.92,264.19,393.80,9.46;10,112.10,277.74,24.55,9.46" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,356.73,250.64,149.26,9.46;10,112.92,264.19,196.54,9.46">Resources and benchmark corpora for hate speech detection: a systematic review</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Poletto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanguinetti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bosco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Patti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,317.78,264.19,157.40,9.46">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.92,291.28,393.06,9.46;10,112.92,304.83,394.58,9.46;10,112.62,318.38,229.91,9.46" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="10,411.58,291.28,94.40,9.46;10,112.92,304.83,195.29,9.46">emoji2vec: Learning emoji representations from their description</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rockt√§schel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bosnjak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1609.08359" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
