<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,377.86,15.42;1,88.78,106.66,372.54,15.42;1,89.29,128.58,80.95,15.43;1,89.29,150.91,157.29,11.96">Identify Hate Speech Spreaders on Twitter using Transformer Embeddings Features and AutoML Classifiers Notebook for PAN at CLEF 2021</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,88.93,176.82,63.68,11.96"><forename type="first">Talha</forename><surname>Anwar</surname></persName>
							<email>chtalhaanwar@gmail.com</email>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,377.86,15.42;1,88.78,106.66,372.54,15.42;1,89.29,128.58,80.95,15.43;1,89.29,150.91,157.29,11.96">Identify Hate Speech Spreaders on Twitter using Transformer Embeddings Features and AutoML Classifiers Notebook for PAN at CLEF 2021</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">B5DD52F5ECDAD5418E3A47CAF835214F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>hate speech</term>
					<term>transformers</term>
					<term>AutoML</term>
					<term>Twitter</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hate speech against other communities, religions and countries is getting more common on social media. There is a need to control the spread of hate and offensive language on social media. Most studies identify whether a sentence is a hatred or not. This paper deals with the identification of whether a user spreads hate on Twitter or not by analyzing hundreds of tweets from the user. Feature embeddings of hundreds of tweets of a user are extracted using different transformers techniques such as BERT, BERTTweet, and RoBERTa for the English language and BETO for the Spanish language. An AutoML classifier is used to classify these embedding features. An accuracy of 75% and 85% is achieved using five-fold crossvalidation and Accuracy of 72% and 82% is obtained for gold standard test data for English and Spanish, respectively. This paper secured 4th position in PAN competition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Hate speech and offensive language against ethnicity, race, color, nationality, gender, sexual orientation, religion, or other characteristics are common on social media, particularly on Twitter. If a particular group raises hate Twitter trend against one specific group, the supporter of that group reply with a more intensive and offensive hate trend. This is more common among political party supporters. Recent studies also show that not only among political parties, this behavior is also observed among countries such as China received a lot of hate comments based on COVID spread from USA <ref type="bibr" coords="1,216.36,513.68,11.30,10.91" target="#b0">[1]</ref>. A lot of studies are conducted to identify the hatred, offensive tweets, and fake news <ref type="bibr" coords="1,189.62,527.23,12.81,10.91" target="#b1">[2]</ref> in different languages <ref type="bibr" coords="1,304.33,527.23,11.33,10.91" target="#b2">[3,</ref><ref type="bibr" coords="1,318.39,527.23,8.94,10.91" target="#b3">4]</ref> on twitter as well as youtube <ref type="bibr" coords="1,461.32,527.23,11.40,10.91" target="#b4">[5]</ref>. These studies focused on the identification of hatred tweets instead of hate spreaders. Profiling hate speech spreaders on Twitter is current challenge to identify hate spreaders on user base instead of a single tweet <ref type="bibr" coords="1,163.68,567.88,11.23,10.91" target="#b5">[6,</ref><ref type="bibr" coords="1,177.59,567.88,7.49,10.91" target="#b6">7]</ref>. This competition is organized at TIRA Integrated Research Architecture to maintain the essence of reproduce-ability of the results <ref type="bibr" coords="1,348.72,581.42,11.43,10.91" target="#b7">[8]</ref>. This paper deals with profiling hate speech spreaders on Twitter using transformers embeddings as features and AutoML as classifiers. Tweet preprocessing, classification based on tweet and user are both studied this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Dataset</head><p>Dataset consists of English and Spanish tweets. For each language, data of 300 users is collected; 200 for training and 100 for testing. Each user data comprised of 200 tweets. The rating as hate speech or not is based on the user instead of the tweet. It meant that 200 tweets for each user are analyzed to label that user as a hate spreader.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Pre-processing of tweets</head><p>Text data is pre-processed to remove any noise. First digits are removed, followed by 'RT', '#USER#', and '#URL#'. Next, tweets are converted to lower case. Stop-words were not removed from the tweet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Tweet level training</head><p>Most NLP algorithms work best on the sentence level. So tweet level model training is used as our baseline approach. A label is assigned to each tweet from the user label. All the tweets of a user are given the label of that user. This way 40,000 tweets are obtained and split into train, validation and test at a ratio of 80:10:10. Train set consists of 32400 tweets, validation and test set comprised of 4000 and 3600 tweets, respectively. This test data is not the same as gold standard test data. This is created for internal evaluation of the system. The aim is that once the model is trained on tweet level, we will predict on tweet level for each user and then average the prediction of all tweets for a particular user and assigned it as user label. For the English baseline model, BERT base uncased model is implemented on tweet level to solve the problem. Ktrain, a lightweight wrapper of deep learning libraries, is used to implement this <ref type="bibr" coords="2,492.59,482.80,11.31,10.91" target="#b8">[9]</ref>. The maximum length of the transformer token is set to 20, batch-size to 16, number of epochs to 5. Fit-one-cycle is used to train the model with a learning rate of 1ùëí -5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">User level training</head><p>In user-level model training, instead of training the BERT model <ref type="bibr" coords="2,373.99,559.63,17.79,10.91" target="#b9">[10]</ref> on our data, we extracted the BERT embeddings from the pre-trained BERT model. The embeddings are extracted from the last hidden layer of the BERT model. Features are also obtained by concatenating the last four layers of BERT model as mentioned in BERT paper. We want to test from which layer the better results can be achieved. These feature embeddings are extracted on tweet level and then averaged at a user level. These user-level features are then fed to autogluon tabular predictor for classification. <ref type="bibr" coords="2,167.46,640.92,16.18,10.91" target="#b10">[11]</ref>. As there are two features set one from the last layer and the other from last 4 layers, so two separate AutoML models are trained. We also used BERTTweet <ref type="bibr" coords="2,468.59,654.47,18.03,10.91" target="#b11">[12]</ref> and RoBerta <ref type="bibr" coords="2,128.72,668.02,18.07,10.91" target="#b12">[13]</ref> model to extract features of English data. In BERTTweet model preprocessing step is changed '#USER#' is replaced with '@USER' and '#URL#' is replaced by 'HTTPURL'. Emojis are converted to text. For Spanish language, BETO (BERTSpanish) model is used <ref type="bibr" coords="3,487.23,100.52,16.35,10.91" target="#b13">[14]</ref>. HuggingFace framework is used to implement all of these transformers models. For submission of results to competition, we used 5-fold cross validation such that gold test data (unlabeled) is evaluated in each fold and finally the prediction is weighted averaged for each model. No train, validation and test splitting as in tweet level training is applied as in user level training data is limited because of averaging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head><p>The baseline approach is based on tweet-level training resulted in over-fitting. The training accuracy achieved for English task using BERT is 85%, whereas validation accuracy is 49% and test accuracy is 53%. As initial results are not good, so further 5-fold cross-validation is not applied. For the Spanish task, train, validation and test accuracy achieved is 92%, 64% and 56% respectively.</p><p>In user-level classification, the average 5 fold accuracy obtained is 75%, 73.5%, and 70% using embeddings from the last hidden layer of BERT, BERTTweet and RoBERTa, respectively. From the last 4 hidden layers of BERT, BERTTweet and RoBERTa, the accuracy of 72.5%, 70% and 73% is achieved. For the Spanish language, BETO resulted in an accuracy of 85% and 85% from embeddings of the last hidden layer and last four hidden layers, respectively. On gold standard test data, the accuracy of 72% and 82% is obtained for English and Spanish, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>Profiling hate speech spreaders on Twitter should be based on user profiles containing multiple tweets. It cannot be identified using a single tweet of a user. So deep learning methods based on sentence/tweet classification failed to classify the user as a hate spreader or not. There needs an approach to consider all tweets of a user at once while training. One way to do is to combine all tweets of user and do documentation classification. This technique produces large text documents which are not feasible to classify as transformers' self-attention mechanism computational expense increase quadratically. One way is to use transformers which have a linear scale able self-attention mechanism such as Longformer <ref type="bibr" coords="4,379.32,100.52,16.41,10.91" target="#b14">[15]</ref>, but the issue is that in document sentence have some connection with each other. In our case, the tweets have no connection with each other, so Longformer is not a feasible option. To resolve this issue, we extracted embeddings of each tweet and average to make one embedding sequence for one user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Identify hate speech spread on Twitter is a need of the day to make social media a peaceful platform. This paper proposed a feature extraction technique using transformers embeddings and AutoML classifiers to classify hate spread users on Twitter. A 5 fold validation accuracy of 75% and 85% is obtained for English and Spanish language, which reduced by 3% on gold standard test data. This paper also discussed the disadvantage of classification techniques based o tweets instead of users and also highlighted the impact of using long sequence transformers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,274.38,256.00,8.93"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Flow diagram of user level hate spread classification</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6.">Acknowledgments</head><p>I would like to express my special thanks to all organizers specially <rs type="person">Francisco Manuel Rangel Pardo</rs>, <rs type="person">Paolo Rosso</rs> and <rs type="person">Elisabetta Fersini</rs></p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="4,112.66,398.34,393.33,10.91;4,112.66,411.89,393.33,10.91;4,112.66,425.43,138.94,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,211.32,398.34,294.66,10.91;4,112.66,411.89,134.48,10.91">Stigmatization in social media: Documenting and analyzing hate speech for covid-19 on twitter</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,255.57,411.89,250.42,10.91;4,112.66,425.43,71.32,10.91">Proceedings of the Association for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page">313</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,112.66,438.98,393.33,10.91;4,112.66,452.53,393.98,10.91;4,112.41,466.08,23.60,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,332.06,438.98,173.93,10.91;4,112.66,452.53,183.43,10.91">Fake reviews classification using deep learning ensemble of shallow convolutions</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Majeed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mujtaba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">O</forename><surname>Beg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,303.97,452.53,174.03,10.91">Journal of Computational Social Science</title>
		<imprint>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,112.66,479.63,394.52,10.91;4,112.66,493.18,393.32,10.91;4,112.26,506.73,393.73,10.91;4,112.66,520.28,209.09,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,140.10,493.18,365.88,10.91;4,112.26,506.73,75.68,10.91">Semeval-2019 task 5: Multilingual detection of hate speech against immigrants and women in twitter</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bosco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fersini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Debora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Patti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M R</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanguinetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,210.56,506.73,235.94,10.91">13th International Workshop on Semantic Evaluation</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="54" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,112.66,533.83,393.33,10.91;4,112.66,547.38,393.33,10.91;4,112.14,560.93,250.08,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,200.34,533.83,305.64,10.91;4,112.66,547.38,221.91,10.91">Tac at semeval-2020 task 12: Ensembling approach for multilingual offensive language identification in social media</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Baig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,365.37,547.38,140.62,10.91;4,112.14,560.93,152.05,10.91">Proceedings of the Fourteenth Workshop on Semantic Evaluation</title>
		<meeting>the Fourteenth Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2177" to="2182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,112.66,574.48,393.33,10.91;4,112.66,588.02,107.17,10.91" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tehreem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10075</idno>
		<title level="m" coord="4,171.51,574.48,259.07,10.91">Sentiment analysis for youtube comments in roman urdu</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="4,112.66,601.57,394.53,10.91;4,112.66,615.12,395.17,10.91;4,112.66,628.67,393.33,10.91;4,112.66,642.22,393.33,10.91;4,112.66,655.77,222.66,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,194.78,628.67,311.21,10.91;4,112.66,642.22,221.30,10.91">Overview of PAN 2021: Authorship Verification,Profiling Hate Speech Spreaders on Twitter,and Style Change Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chulvi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">L D L P</forename><surname>Sarrac√©n</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Manjavacas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Markov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mayerl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wolska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Zangerle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,357.51,642.22,148.47,10.91;4,112.66,655.77,150.17,10.91">12th International Conference of the CLEF Association (CLEF 2021)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,112.66,86.97,393.33,10.91;5,112.66,100.52,393.60,10.91;5,112.66,114.06,125.73,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,406.65,86.97,99.33,10.91;5,112.66,100.52,154.06,10.91">Profiling Hate Speech Spreaders on Twitter Task at PAN</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">L D L P</forename><surname>Sarrac√©n</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chulvi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fersini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="5,311.51,100.52,142.17,10.91">CLEF 2021 Labs and Workshops</title>
		<title level="s" coord="5,461.81,100.52,44.45,10.91;5,112.66,114.06,56.73,10.91">Notebook Papers, CEUR</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,112.66,127.61,394.53,10.91;5,112.66,141.16,393.33,10.91;5,112.66,154.71,394.51,10.91;5,112.66,170.70,123.08,7.90" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="5,327.46,127.61,175.13,10.91">TIRA Integrated Research Architecture</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-22948-1_5</idno>
	</analytic>
	<monogr>
		<title level="m" coord="5,240.99,141.16,264.99,10.91;5,112.66,154.71,123.97,10.91">Information Retrieval Evaluation in a Changing World, The Information Retrieval Series</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,112.66,181.81,393.33,10.91;5,112.66,195.36,207.25,10.91" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="5,169.94,181.81,263.66,10.91">ktrain: A low-code library for augmented machine learning</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Maiya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10703</idno>
		<idno>arXiv:2004.10703</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,112.66,208.91,393.33,10.91;5,112.66,222.46,363.59,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="5,353.43,208.91,152.55,10.91;5,112.66,222.46,181.08,10.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,112.66,236.01,394.61,10.91;5,112.66,249.56,390.16,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Erickson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shirkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Larroy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06505</idno>
		<title level="m" coord="5,423.71,236.01,83.56,10.91;5,112.66,249.56,208.18,10.91">Autogluon-tabular: Robust and accurate automl for structured data</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,112.66,263.11,393.32,10.91;5,112.66,276.66,209.16,10.91" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="5,273.39,263.11,232.59,10.91;5,112.66,276.66,26.95,10.91">Bertweet: A pre-trained language model for english tweets</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10200</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,112.66,290.20,394.53,10.91;5,112.30,303.75,393.68,10.91;5,112.66,317.30,107.17,10.91" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m" coord="5,173.53,303.75,256.77,10.91">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,112.66,330.85,393.32,10.91;5,112.66,344.40,266.40,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="5,395.98,330.85,110.00,10.91;5,112.66,344.40,115.82,10.91">Spanish pre-trained bert model and evaluation data</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ca√±ete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chaperon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-H</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>P√©rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,251.30,344.40,97.80,10.91">PML4DC at ICLR 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,112.66,357.95,393.59,10.91;5,112.66,371.50,146.44,10.91" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m" coord="5,268.64,357.95,203.75,10.91">Longformer: The long-document transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
