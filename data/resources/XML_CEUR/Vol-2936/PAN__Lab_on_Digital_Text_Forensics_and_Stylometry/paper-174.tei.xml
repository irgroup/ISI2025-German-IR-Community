<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,383.18,15.42;1,88.59,106.66,275.24,15.42">Early Detection of Online Hate Speech Spreaders with Learned User Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,58.71,11.96"><forename type="first">Darius</forename><surname>Irani</surname></persName>
							<email>dr.irani21@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,160.65,134.97,68.20,11.96"><forename type="first">Avyakta</forename><surname>Wrat</surname></persName>
							<email>avyaktawrat@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Bombay</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,259.85,134.97,55.83,11.96"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
							<email>s.amir@northeastern.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,383.18,15.42;1,88.59,106.66,275.24,15.42">Early Detection of Online Hate Speech Spreaders with Learned User Representations</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">7A6C8A8CD5D47C2C243BEFD394635403</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>hate speech detection</term>
					<term>social media analysis</term>
					<term>user representation learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We developed and evaluated models for early detection of online hate speech spreaders. We addressed the problem as a social media author profiling task: given a small collection of tweets, the goal is to predict whether the author is likely to spread hate speech in the future (e.g. against women or immigrants). We investigated the impact of augmenting standard lexical representations with learned user-level representations from author-topic models and neural user embeddings. The evaluation was conducted on a dataset created for the social media author profiling shared task at PAN 2021. Our results show that: (i) learned user representations capture latent user aspects that correlate with the propensity to spread hate speech; and (ii) different user representations are complementary and can combined to improve hate speech detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The mass adoption of social media has been accompanied by a rise in abusive and toxic communications such as, cyberbullying and hate speech. Hate speech is commonly defined as any communication that disparages a person or a group on the basis of some characteristic such as race, colour, ethnicity, gender, sexual orientation, nationality or religion <ref type="bibr" coords="1,433.51,457.09,11.58,10.91" target="#b0">[1]</ref>. While most social platforms prohibit such behaviours, manually enforcing such policies is too laborious for the volume and velocity of user generated content. Despite being critical to ensure the safety and well-being of social media users, the problem of automatic hate speech detection has been relatively underappreciated. Most previous work has framed this as a document-level text classification task, i.e. predicting whether a document contains hate speech. However, clues from a single document are often insufficient to detect more nuanced forms of hate speech <ref type="bibr" coords="1,492.50,538.39,11.38,10.91" target="#b1">[2]</ref>. Moreover, operationalizing document-level hate speech detectors may still be too slow or burdensome for large social media platforms: offensive posts may still need to be manually flagged by the users; or every single post must be analyzed before publication.</p><p>In this paper, we address the problem of online hate speech detection as an author-profiling task, i.e. given a small set of social media posts, predict whether the author is likely to spread hate speech in the future (e.g. against women or immigrants). Approaching this problem from an author profiling perspective can help to overcome some of the limitations of document-level methods: first, a set of posts may provide a stronger signal than a single post; second, this opens the door for early detection systems to identify abusive users early on. In turn, this can expedite and reduce the burden of content moderation.</p><p>We investigate the impact of learned user representations on models for early detection of hate speech spreaders. Specifically, we develop classifiers with a combination of BOW features and user representations based on author-topic models estimated with Author-LDA <ref type="bibr" coords="2,454.03,181.81,12.69,10.91" target="#b2">[3]</ref> and user embeddings learned with User2Vec <ref type="bibr" coords="2,250.62,195.36,11.53,10.91" target="#b3">[4]</ref>. We conduct experiments on a dataset created for the 2021 edition of the PAN author profiling shared task: Profiling hate speech Spreaders on Twitter<ref type="foot" coords="2,501.11,207.15,3.71,7.97" target="#foot_0">1</ref> . Our results show that: learned user representations capture relevant latent attributes for hate speech detection; (ii) different user-level representations capture complementary user aspects that can be combined to improve early detection of hate speech spreaders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Social Media Author Profiling with User Representations</head><p>Social media content tends to be noisy and short, which poses challenges to traditional text classifiers. Standard bag-of-words models (BOW) yield very large and sparse vectors which are are often insufficient for inferences over nuanced content. Therefore, previous work on document-level online hate speech detection has sought to improve BOW models with additional features that capture high-level semantic properties of words and documents, such as latent topic models <ref type="bibr" coords="2,148.70,375.93,11.43,10.91" target="#b4">[5]</ref>, word embeddings <ref type="bibr" coords="2,248.73,375.93,12.84,10.91" target="#b5">[6]</ref> and paragraph embeddings <ref type="bibr" coords="2,388.17,375.93,11.43,10.91" target="#b6">[7]</ref>.</p><p>We address the problem of online hate speech detection as an author profiling task (note that we will use the terms user and author interchangeably). Similarly to document-level approaches, we seek to augment BOW models with additional features that capture high-level personal aspects of the authors. We hypothesize that augmenting local features from individual documents with learned user representations can improve online hate speech detection by: first, providing models with a global view of the author; and second, allowing models to discover latent personal aspects that correlate with the propensity to spread hate speech. We compare two approaches to learn user representations: author-topic models induced with Author LDA <ref type="bibr" coords="2,493.30,484.32,12.68,10.91" target="#b2">[3]</ref> and user embeddings induced with User2Vec <ref type="bibr" coords="2,294.93,497.87,11.56,10.91" target="#b3">[4]</ref>. User2Vec embeddings have been shown to capture meaningful latent attributes of social media users e.g., that correlate with political leanings <ref type="bibr" coords="2,130.16,524.97,13.00,10.91" target="#b3">[4]</ref> and mental-health status <ref type="bibr" coords="2,261.78,524.97,11.48,10.91" target="#b7">[8,</ref><ref type="bibr" coords="2,276.15,524.97,7.65,10.91" target="#b8">9]</ref>. Here, we assess whether these embeddings can also capture relevant attributes for hate speech detection.</p><p>Formally, let ğ’ ğ‘˜ = {ğ‘‘ 1 ğ‘˜ , . . . , ğ‘‘ ğ‘€ ğ‘˜ } be a collection of documents authored by user ğ‘¢ ğ‘˜ , where each post ğ‘‘ ğ‘— ğ‘˜ = {ğ‘¤ 1 , . . . , ğ‘¤ ğ‘ } is composed of words ğ‘¤ ğ‘– from a vocabulary ğ’±. We estimate the probability that user ğ‘¢ ğ‘˜ is a hate speech spreader as</p><formula xml:id="formula_0" coords="2,228.57,607.61,277.42,11.51">ğ‘ƒ (ğ‘¦ = 1|ğ¶ ğ‘˜ , ğ‘¢ ğ‘˜ ) = ğ‘“ ([c ğ‘˜ , u ğ‘˜ ])<label>(1)</label></formula><p>where ğ‘“ is a binary classifier, c k âˆˆ 0, 1 |ğ’±| is a BOW representation of ğ’ ğ‘˜ , and u ğ‘˜ âˆˆ R â„ is a learned representation of user ğ‘¢ ğ‘˜ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Author-Topic Model</head><p>Latent Dirichlet Allocation (LDA) is a generative probabilistic model in which documents are represented as mixtures of latent topics, and topics are characterized as distributions over words drawn from a Dirichlet distribution <ref type="bibr" coords="3,246.26,134.63,16.09,10.91" target="#b9">[10]</ref>. LDA defines the following generative process for each document: (i) a distribution over topics ğ‘ is sampled from a Dirichlet distribution; (ii) for each word in the document, a single topic ğ‘§ is chosen according to this distribution; and (iii) each word is sampled from a multinomial distribution for topic ğ‘§.</p><p>Author-topic models are an extension of LDA that includes authorship information by associating each author ğ‘¢ ğ‘˜ with a distribution over topics <ref type="bibr" coords="3,344.32,202.38,11.29,10.91" target="#b2">[3]</ref>. The generative process is similar to LDA, however, the observed words are generated from a topic distribution sampled from an author-specific distribution over topics ğ‘ ğ‘¢ ğ‘˜ . By correlating authorship information with particular topics, author-topic models capture information about the topics that authors typically write about, and are able to represent documents in terms of these topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Neural User Embeddings</head><p>User2Vec aims to capture the relations between users and the content (i.e., the words) they generate, by optimizing the probability of sentences conditioned on their authors. Each user ğ‘¢ ğ‘˜ is associated with a parameter vector u ğ‘˜ , which is then optimized to accurately predict the words from previous posts written by said user</p><formula xml:id="formula_1" coords="3,214.08,370.13,291.90,30.49">ğ‘ƒ (ğ’ ğ‘˜ |ğ‘¢ ğ‘˜ ) âˆ âˆ‘ï¸ ğ‘‘ ğ‘— ğ‘˜ âˆˆğ’ ğ‘˜ âˆ‘ï¸ ğ‘¤ ğ‘– âˆˆğ‘‘ ğ‘— ğ‘˜ log ğ‘ƒ (ğ‘¤ ğ‘– |u ğ‘˜ )<label>(2)</label></formula><p>Since the goal is to learn user representations, the term ğ‘ƒ (ğ‘¤ ğ‘– |u ğ‘˜ ) can be approximated with Negative Sampling <ref type="bibr" coords="3,176.06,425.89,17.91,10.91" target="#b10">[11]</ref> by minimizing the following Hinge-loss objective:</p><formula xml:id="formula_2" coords="3,185.23,449.09,320.75,25.61">â„’(w ğ‘– , u ğ‘˜ ) = âˆ‘ï¸ ğ‘¤ Ëœğ‘— âˆˆğ’± max(0, ğ‘š -w ğ‘– â€¢ u ğ‘˜ + w Ëœğ‘— â€¢ u ğ‘˜ )<label>(3)</label></formula><p>where word ğ‘¤ Ëœğ‘— (and associated embedding, w Ëœğ‘—) is a negative sample, i.e. a word not occurring in the post under consideration (authored by user ğ‘¢ ğ‘˜ ); and ğ‘š is an hyperparameter that controls the loss margin. Note that both words and users are represented as ğ‘‘-dimensional vectorspretrained word vectors w ğ‘– âˆˆ R ğ‘‘ and user vectors u ğ‘˜ âˆˆ R ğ‘‘ to be learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>We investigate the impact of learned user-level representations on the performance of author profiling models for online hate speech detection. As baselines, we consider document-level representations based on latent document-topics and word embeddings. Since the goal is to make predictions with respect to users, all document-level representations must be aggregated into a single user vector. We represent each user ğ‘¢ ğ‘˜ by averaging all their associated document vectors</p><formula xml:id="formula_3" coords="3,123.18,653.32,87.89,16.96">u ğ‘˜ = 1 ğ‘› âˆ‘ï¸€ ğ‘‘ ğ‘– ğ‘˜ âˆˆğ’ ğ‘˜ d ğ‘– ğ‘˜ .</formula><p>We develop models that combine BOW features with the following learned representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Main results in terms of Accuracy. We compare the impact of augmenting BOW models (rows 1-2) with learned document-level representations (rows 3-6) and user-level representations (rows 7-10). The loss margin ğ‘š and learning rate ğ‘™ used to train the User2Vec user embeddings are shown in parenthesis. The last two rows show the results of combining different user-level representations. implement the classifiers. We compare the performance of Support Vector Machines <ref type="bibr" coords="5,468.80,355.98,17.95,10.91" target="#b16">[17]</ref> and Random Forests <ref type="bibr" coords="5,162.17,369.52,17.79,10.91" target="#b17">[18]</ref> with different random seed initializations. We obtain the best results with Random Forests and thus adopt this classifier for the main experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Results</head><p>Table <ref type="table" coords="5,117.31,432.80,5.17,10.91">1</ref> shows our main results in terms of Accuracy. As expected, BOW representations perform rather poorly by themselves. We can alleviate the problem of feature sparsity by including character n-grams, e.g. we obtain an improvement of 2% in Accuracy by adding character trigrams (Char-3). Regarding our main hypothesis, we observe that document-level representations based on document-topics and contextualized word embeddings provide only modest gains compared to a baseline of just lexical features (up to 1.5% in Accuracy) and static word embeddings actually hurt the models performance. Document-level representations are suboptimal for this task, in part because averaging features from multiple documents squashes all the information and thus the resulting vectors are less discriminative.</p><p>In contrast, all user-level representations yield noticeable gains. Indeed, we find that the user-level representations always outperform their document-level counterparts -i.e, replacing document-topic with author-topic representations improves performance by 3.5%, and replacing word embeddings with the corresponding user embeddings yields gains of up to 7.5%. Moreover, we see that User2Vec user embeddings induced with contextualized word embeddings (User2Vec-ELMo and User2Vec-BERT) outperform user embeddings from static word embeddings (User2Vec-FastText). While this is not surprising, it shows that User2Vec user embeddings can directly benefit from improvements on the underlying word representations.</p><p>Finally, we assess whether different user representations capture complementary user aspects that can be combined to improve the models. We find that ELMo user embeddings (User2Vec-ELMo) can be combined with BERT user embeddings (User2Vec-BERT) and with author-topic features (Author-LDA). Overall, these results confirm our hypothesis: learned user-level representations encode personal aspects that correlate with hate speech spreading behaviour. Our best performing model obtains an Accuracy of 72% with a combination of BOW, character tri-grams, ELMo user embeddings and author-topic features. The learned user representations improve the model's absolute performance by 10%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>We developed and evaluated social media author profiling models for the Profiling Hate Speech Spreaders on Twitter shared task at PAN 2021. Specifically, we investigated whether learned userlevel representations capture latent aspects of users that correlate with hate speech spreading behaviour. We compared the impact of enriching BOW models with user-level representations based on latent author-topics and user embeddings, against document-level representations based on latent document-topics and word embeddings. While document-level representations provide some gains, we found that user-level representations yield much larger improvements (up to 10% in Accuracy). We believe that this is because user representations are able to complement local information from single documents with global information about the author.</p><p>Our results also show that User2Vec user embeddings induced with contextualized word embeddings perform better than static word embeddings; and that different embeddings capture complementary user aspects. However, it is not clear why these differences are observed. Moving forward we would like to probe into the learned representations to better understand what kinds of personal aspects are being captured by different models. In this work, we only considered contextualized word representations produced by the last layer of deep language models. In the future, we will investigate the impact of learning user representations using contextualized word representations from the inner layers of these models. This can open the door to more sophisticated user embedding methods, e.g. to induce deep or contextualized user embeddings.</p><p>This work presents an important first step towards early detection of online hate speech spreaders. However, the limited size of the dataset makes it difficult to draw definitive conclusions about the relative performance of each method. Moving this line of research forward may require larger annotated datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,378.97,154.04,37.69,8.87;5,178.62,169.92,23.36,8.87;5,393.96,169.65,22.69,8.74;5,178.62,181.88,36.23,8.87;5,398.95,181.61,17.71,8.74;5,178.62,199.31,25.92,8.87;5,398.95,199.04,17.71,8.74;5,178.62,211.27,60.51,8.87;5,398.95,211.00,17.71,8.74;5,178.62,223.22,50.55,8.87;5,398.95,222.95,17.71,8.74;5,178.62,235.18,49.10,8.87;5,393.96,234.91,22.69,8.74;5,178.62,252.61,57.89,8.87;5,393.96,252.34,22.69,8.74;5,178.62,264.29,152.28,9.14;5,398.95,264.29,17.71,8.74;5,178.62,276.25,147.30,9.14;5,393.96,276.25,22.69,8.74;5,178.62,288.20,133.12,9.14;5,393.96,288.20,22.69,8.74;5,178.62,308.44,147.50,8.87;5,393.96,308.18,22.69,8.74;5,178.62,320.35,185.90,8.93;5,396.70,320.10,19.95,8.77"><head></head><label></label><figDesc>FastText (ğ‘š = 5; ğ‘™ = 0.1) 0.65 + User2Vec-ELMo (ğ‘š = 15; ğ‘™ = 0.1) 0.685 + User2Vec-BERT (ğ‘š = 5; ğ‘™ = 1) 0.695 + User2Vec-ELMo + User2Vec-BERT 0.705 + Char-3 + User2Vec-ELMo + Author-LDA 0.72</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.93,671.02,221.77,8.97"><p>https://pan.webis.de/clef21/pan21-web/author-profiling.html</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Document Topics (LDA): Documents represented as mixtures of latent document-topics. We train a LDA topic model using the lda python package with the default hyperparameters. For each tweet, we use the model to estimate document-topic proportions and compute a feature vector d âˆˆ R â„ , where â„ = 50 is the number of topics.</p><p>Word Embeddings (Avg-*): Documents represented as the average of pretrained embeddings associated with each word. For each tweet, we compute a feature vector d = 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ğ‘›</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>âˆ‘ï¸€</head><p>[w 1 , . . . , w ğ‘› ], where w ğ‘– âˆˆ R ğ‘˜ is the embedding for word ğ‘¤ ğ‘– and â„ is the embedding size. We leverage static embeddings trained with fastText <ref type="bibr" coords="4,339.38,197.02,17.76,10.91" target="#b11">[12]</ref> (â„ = 400) over a collection of 400 Million tweets 2 . We also experiment with contextualized word embeddings produced by deep pretrained language models: we extract ELMo <ref type="bibr" coords="4,302.00,224.12,18.07,10.91" target="#b12">[13]</ref> embeddings (â„ = 256) with a pretrained implementation available on the AllenNLP library 3 ; and BERT <ref type="bibr" coords="4,365.30,237.67,17.76,10.91" target="#b13">[14]</ref> embeddings (â„ = 768) with a pretrained implementation available on the Huggingface library 4 .</p><p>Author Topics (Author-LDA): Users represented as a mixture of latent author-topics. We train an Author-LDA model using the implementation available on the Gensim python library 5  with the default training hyperparameters. For each user, we estimate author-topic distributions and induce a feature vector u âˆˆ R â„ where â„ = 50 is the number of topics.</p><p>User Embeddings (User2Vec-*): Users represented as User2Vec user embeddings. For each user, we estimate an embedding u âˆˆ R â„ , where â„ is the embedding size. We experimented with the same set of fastText, ELMo and BERT word representations. The embeddings were trained by minimizing Eq. 3 with ADAM <ref type="bibr" coords="4,282.37,390.03,18.07,10.91" target="#b14">[15]</ref> for 20 epochs with a fixed learning rate and early-stopping (using 20% of the tweets as a development set). We compare the performance of embeddings trained with different learning rates from the set ğ¿ = {10, 1, 0.1, 0.01} and the margin values from the set ğ‘€ = {1, 5, 10, 15}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Evaluation</head><p>We evaluate our models on a dataset created for the social media author profiling shared task at PAN 2021: Profiling Hate Speech Spreaders on Twitter. The dataset contains 200 users annotated with binary labels indicating if they are hate speech spreaders; each user is also associated with a set of 200 tweets. We pre-process each tweet by lower-casing, removing white spaces and stop words and tokenizing the text with the Twokenize python package 6 .</p><p>Given the small size of the training dataset and the lack of a validation dataset, we adopt a Leave-One-Out cross-validation methodology. We conduct preliminary experiments to select the best performing classifier for this task. We use scikit-learn 7 python library <ref type="bibr" coords="4,475.43,575.25,18.06,10.91" target="#b15">[16]</ref> to 1 https://pypi.org/project/lda/ 2 https://github.com/FredericGodin/TwitterEmbeddings 3 https://github.com/allenai/allennlp 4 https://huggingface.co/ 5 https://radimrehurek.com/gensim/models/atmodel.html 6 https://pypi.org/project/twokenize/ 7 https://scikit-learn.org</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="6,112.66,583.60,393.33,10.91;6,112.66,597.15,192.22,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,216.59,583.60,254.27,10.91">A survey on automatic detection of hate speech in text</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Fortuna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nunes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,482.10,583.60,23.89,10.91;6,112.66,597.15,123.50,10.91">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,610.69,393.33,10.91;6,112.66,624.24,393.33,10.91;6,112.66,637.79,191.54,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,235.80,610.69,270.18,10.91;6,112.66,624.24,46.35,10.91">A survey on hate speech detection using natural language processing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,188.35,624.24,317.64,10.91;6,112.66,637.79,118.55,10.91">Proceedings of the fifth international workshop on natural language processing for social media</title>
		<meeting>the fifth international workshop on natural language processing for social media</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,651.34,393.33,10.91;6,112.66,664.89,223.71,10.91" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rosen-Zvi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.4169</idno>
		<title level="m" coord="6,331.18,651.34,174.81,10.91;6,112.66,664.89,45.96,10.91">The author-topic model for authors and documents</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,112.66,86.97,393.53,10.91;7,112.66,100.52,393.61,10.91;7,112.66,114.06,345.41,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,375.33,86.97,130.86,10.91;7,112.66,100.52,221.87,10.91">Modelling context with user embeddings for sarcasm detection in social media</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,358.31,100.52,147.97,10.91;7,112.66,114.06,257.41,10.91">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="167" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,127.61,394.53,10.91;7,112.66,141.16,394.53,10.91;7,112.39,154.71,141.72,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,112.66,141.16,337.79,10.91">Content-driven detection of cyberbullying on the instagram social network</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Squicciarini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Rajtmajer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Caragea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,479.63,141.16,22.97,10.91">IJCAI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="3952" to="3958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,168.26,393.32,10.91;7,112.66,181.81,393.33,10.91;7,112.26,195.36,103.80,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,369.98,168.26,136.01,10.91;7,112.66,181.81,84.60,10.91">Abusive language detection in online user content</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Nobata</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,220.47,181.81,285.51,10.91;7,112.26,195.36,15.37,10.91">Proceedings of the 25th international conference on world wide web</title>
		<meeting>the 25th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="145" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,208.91,393.32,10.91;7,112.66,222.46,393.33,10.91;7,112.66,236.01,160.77,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,451.08,208.91,54.90,10.91;7,112.66,222.46,158.72,10.91">Hate speech detection with comment embeddings</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Djuric</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grbovic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Radosavljevic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Bhamidipati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,294.27,222.46,211.72,10.91;7,112.66,236.01,82.49,10.91">Proceedings of the 24th international conference on world wide web</title>
		<meeting>the 24th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="29" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,249.56,393.33,10.91;7,112.66,263.11,393.33,10.91;7,112.66,276.66,168.96,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,390.44,249.56,115.55,10.91;7,112.66,263.11,215.13,10.91">Quantifying mental health from social media with neural user embeddings</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Coppersmith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,354.04,263.11,151.94,10.91;7,112.66,276.66,48.47,10.91">Machine Learning for Healthcare Conference</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="306" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,290.20,393.32,10.91;7,112.66,303.75,393.33,10.91;7,112.66,317.30,136.23,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,255.57,290.20,250.41,10.91;7,112.66,303.75,30.61,10.91">Mental health surveillance over social media with digital cohorts</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Ayers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,165.71,303.75,340.28,10.91;7,112.66,317.30,48.12,10.91">Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology</title>
		<meeting>the Sixth Workshop on Computational Linguistics and Clinical Psychology</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="114" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,330.85,393.33,10.91;7,112.66,344.40,164.05,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,272.80,330.85,116.61,10.91">Latent dirichlet allocation</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,400.29,330.85,105.70,10.91;7,112.66,344.40,80.12,10.91">the Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,357.95,393.33,10.91;7,112.66,371.50,102.10,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8251</idno>
		<title level="m" coord="7,154.86,357.95,275.74,10.91">Notes on noise contrastive estimation and negative sampling</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,112.66,385.05,393.33,10.91;7,112.66,398.60,393.98,10.91;7,112.41,412.15,38.81,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,331.70,385.05,174.29,10.91;7,112.66,398.60,52.01,10.91">Enriching word vectors with subword information</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,177.72,398.60,288.70,10.91">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,425.70,393.32,10.91;7,112.66,439.25,283.09,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,482.48,425.70,23.51,10.91;7,112.66,439.25,160.18,10.91">Deep contextualized word representations</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,295.86,439.25,68.21,10.91">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,452.79,393.33,10.91;7,112.66,466.34,363.59,10.91" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="7,353.43,452.79,152.55,10.91;7,112.66,466.34,181.08,10.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,112.66,479.89,393.33,10.91;7,112.66,493.44,102.10,10.91" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m" coord="7,251.96,479.89,172.98,10.91">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,112.66,506.99,394.53,10.91;7,112.66,520.54,393.33,10.91;7,112.48,534.09,261.79,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="7,307.23,520.54,176.03,10.91">Scikit-learn: Machine learning in python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,492.05,520.54,13.94,10.91;7,112.48,534.09,167.70,10.91">the Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,547.64,375.65,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="7,208.31,547.64,109.06,10.91">Support-vector networks</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,326.19,547.64,78.19,10.91">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,561.19,277.53,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="7,167.61,561.19,67.74,10.91">Random forests</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,243.28,561.19,78.19,10.91">Machine learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
