<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,293.29,84.74,176.17,15.42;1,89.29,106.66,223.35,15.42;1,89.29,129.00,157.29,11.96">@ PAN: Profiling Hate Speech Spreaders on Twitter Notebook for PAN at CLEF 2021</title>
				<funder ref="#_xeSUDqg">
					<orgName type="full">Volkswagen Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,154.90,129.32,11.96"><forename type="first">Kwabena</forename><forename type="middle">Odame</forename><surname>Akomeah</surname></persName>
						</author>
						<author>
							<persName coords="1,230.88,154.90,78.61,11.96"><forename type="first">Udo</forename><surname>Kruschwitz</surname></persName>
							<email>udo.kruschwitz@ur.de</email>
						</author>
						<author>
							<persName coords="1,336.93,154.90,69.20,11.96"><forename type="first">Bernd</forename><surname>Ludwig</surname></persName>
							<email>bernd.ludwig@ur.de</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Regensburg</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Regensburg</orgName>
								<address>
									<addrLine>Universitätsstraße 31</addrLine>
									<postCode>93053</postCode>
									<settlement>Regensburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,293.29,84.74,176.17,15.42;1,89.29,106.66,223.35,15.42;1,89.29,129.00,157.29,11.96">@ PAN: Profiling Hate Speech Spreaders on Twitter Notebook for PAN at CLEF 2021</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">6AE75B3B3916F875D657B21C8874F8AC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Hate Speech</term>
					<term>BERT</term>
					<term>Embeddings</term>
					<term>Sentence Encoder</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper reports on our approach to addressing the Shared Task Profiling hate speech spreaders on Twitter for both English and Spanish, organised as part of the PAN@CLEF 2021 Challenge. We submitted one run for each language based on pre-trained language models. For English we fine-tuned a BERTmodel while for Spanish we used a language-agnostic BERT-based sentence embedding model without fine-tuning. The second approach appears to have been a lot more effective than the first one. Given the simplicity of the approaches there is plenty of room for future directions based on the architectures adopted here.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Hate speech is not a new phenomenon but it has become more and more of a problem in recent years and has consequently attracted a lot of attention in the research community making hate speech detection a very active research field, e.g. <ref type="bibr" coords="1,305.53,415.65,11.30,10.91" target="#b0">[1]</ref>. In particular the growing impact of social media on the way people share and access information has demonstrated the need to tackle the problem systematically as issues such as cyber-bullying and other hurtful and anti-social behaviours <ref type="bibr" coords="1,139.98,456.29,11.23,10.91" target="#b1">[2,</ref><ref type="bibr" coords="1,153.67,456.29,7.42,10.91" target="#b0">1,</ref><ref type="bibr" coords="1,163.56,456.29,8.88,10.91" target="#b2">3]</ref> have become a growing cancer that needs to be tackled broadly across many different platforms and applications. We should note that the task of removing hate speech is not as simple as it seems as there is a fine balance between filtering hate speech and the possible restriction to the freedom of speech if a perfectly reasonable opinion is incorrectly flagged as hate speech and subsequently removed <ref type="bibr" coords="1,266.30,510.49,11.43,10.91" target="#b3">[4]</ref>.</p><p>The motivation of this task <ref type="bibr" coords="1,218.46,524.04,12.69,10.91" target="#b4">[5]</ref> is to move from a purely reactive to a more pro-active approach that does not simply identify messages as hate speech but instead identifies social media users as hate speech spreaders thereby allowing the problem to be addressed more effectively (e.g. by suggesting to the owner of the social media platform to ban such users).</p><p>Transformer-based methods have been demonstrated to be highly effective for a wide range of NLP tasks, e.g. <ref type="bibr" coords="1,166.47,591.79,11.28,10.91" target="#b5">[6]</ref>. This is the reason we adopt state-of-the-art pre-trained transformer-based deep neural text embeddings for tackling the PAN sub-task on profiling hate speech spreaders on Twitter. In this report, we will provide an overview of the steps taken and the models used in our experiment. We will start by briefly describing the dataset, looking at the pre-processing steps and models used before we report our results obtained for the two submissions as compared to the baselines <ref type="bibr" coords="2,149.31,141.16,12.84,10.91" target="#b6">[7]</ref> submitted by the organizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PAN Task 3: Profiling Hate Speech Spreaders on Twitter</head><p>The third task <ref type="bibr" coords="2,154.17,199.37,12.80,10.91" target="#b4">[5]</ref> of the PAN Challenge at CLEF 2021 <ref type="bibr" coords="2,329.35,199.37,12.80,10.91" target="#b7">[8]</ref> involves the profiling of hate speech spreaders on Twitter towards, for instance, immigrants and women using sampled data from the individual user's timeline.</p><p>The training dataset consists of 40,000 tweets constituting a set of 200 tweets sampled per each of 200 anonymized users in XML format for two languages, English and Spanish. The test set contains tweets from 100 anonymized users per language. The tasks were treated separately for each language and therefore two different models were used for both English and Spanish.</p><p>An small snapshot of the raw training English data is reproduced in Figure <ref type="figure" coords="2,435.87,294.21,3.74,10.91" target="#fig_0">1</ref>.</p><p>Note that the 200 tweets of hate speech spreaders may not all contain hate speech. The aim of the experiment is to discover if frequent hate spreaders can be identified based on their timeline history.</p><p>The systems are ranked using the average of Accuracy achieved on the English and Spanish test sets. Submission and evaluation of this year's tasks were done on TIRA <ref type="bibr" coords="2,430.52,361.96,12.91,10.91" target="#b8">[9]</ref> or sent to the organizers through mail. All codes used in this experiment can be accessed via GitHub.<ref type="foot" coords="2,477.29,373.76,3.71,7.97" target="#foot_0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Preparing the Data using Contextual Embeddings</head><p>In recent years, transformer-based models such as Bidirectional Encoder Representations from Transformers (BERT) have emerged as the dominant paradigm in a broad range of NLP applications ranging from translation to classification, e.g. <ref type="bibr" coords="2,334.01,451.92,11.49,10.91" target="#b5">[6,</ref><ref type="bibr" coords="2,348.29,451.92,12.42,10.91" target="#b9">10]</ref>. Part of the success story is the fact that the expensive task of pre-training is only done once and this pre-trained model can be subsequently fine-tuned to each NLP task at hand with just one additional output layer to create state-of-the-art models. In effect there has been a large number of different BERT-based models that have emerged from this, e.g. <ref type="bibr" coords="2,279.41,506.11,16.55,10.91" target="#b10">[11,</ref><ref type="bibr" coords="2,299.01,506.11,7.52,10.91" target="#b5">6,</ref><ref type="bibr" coords="2,309.58,506.11,12.59,10.91" target="#b11">12,</ref><ref type="bibr" coords="2,325.22,506.11,12.59,10.91" target="#b12">13,</ref><ref type="bibr" coords="2,340.86,506.11,12.42,10.91" target="#b13">14]</ref>. Specifically, we turn the textual representation of the input documents (tweets) into contextual embeddings as follows. The input data (in XML format) has to be turned initially into tensors for input in the Keras Model by first extracting all text for each user using an XML parser and pandas in Python. The dataframes indexed with user-ids are then formatted into tensors ready to be used as input for the transformer model. The training dataset was split for train-test reasons; 160 for training, 32 for validation, 8 for testing and shuffled for every other time it was run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Approach for the English Run</head><p>The experiment for the English task involved the binary classification test set of 100 users similarly parsed in XML format. The training set is composed of 200 users with 200 tweets each. The model architecture used was a dense artificial neural network with a single output with sigmoid activations. With the success of BERT-based models in NLP, we employed ALBERT, a BERT-based model trained on a large corpus of the English language with reduced parameters without a significant effect on the performance benchmark <ref type="bibr" coords="3,368.27,469.63,16.41,10.91" target="#b10">[11]</ref>. Reusing the model only required a fine-tuning of its parameters on the dataset which requires that output from ALBERT is learned as well in the network. The network had a dense layer receiving BERT encoder outputs with a dropout of 0.1 with sigmoid activation on a single output layer. The network run for 10 epochs with 5 steps per epoch and a batch size of 32. The loss function used was binary cross-entropy with an adaptive moment estimation (Adam) optimizer and a learning rate of 1e-6. The metric used for training was binary accuracy in line with accuracy as the specified metric for evaluation of the challenge.</p><p>A checkpoint was implemented for the neural network. The epochs had an average runtime of about 250 seconds and therefore a larger number of epochs would be costly. The checkpoint was to monitor the minimum binary validation loss with a patience of 3 epochs. The validation loss was chosen other than training loss to check overfitting of the training dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Approach for the Spanish Run</head><p>A similar model to the English run was used for the Spanish run in that we used a transformerbased model containing a text input layer, prepocessing layer, an encoding layer, and a single densely-connected layer as output also with a dropout of 0.1. The binary cross-entropy loss function, as well as Adam optimizer were also used for the Spanish run. The preprocessing layer was a multilingual universal sentence encoder preprocessor <ref type="bibr" coords="4,384.63,285.74,16.30,10.91" target="#b9">[10]</ref>. This preprocessor is a companion to the BERT models for preprocsssing plain text inputs into the input format expected by BERT. The model uses a vocabulary for multilingual models extracted from Wikipedia, CommonCrawl, and translation pairs from the Web. It has no trainable parameters and can be used in an input pipeline outside the training loop <ref type="bibr" coords="4,315.59,339.94,16.43,10.91" target="#b9">[10,</ref><ref type="bibr" coords="4,334.75,339.94,12.32,10.91" target="#b14">15]</ref>. The encoder layer used was the language-agnostic BERT sentence embedding model (LaBSE) <ref type="bibr" coords="4,89.29,367.04,16.41,10.91" target="#b15">[16]</ref>. LaBSE supports about 109 languages including Spanish. The language-agnostic BERT sentence embedding encodes sentences into high-dimensional vectors. The model is trained and optimized to produce similar representations solely for bilingual sentence pairs that are translations of each other. Because of its usefulness in sentence translations in a larger multilingual corpus, text classification, semantic similarity, clustering and other natural language tasks <ref type="bibr" coords="4,89.29,434.78,16.43,10.91" target="#b15">[16,</ref><ref type="bibr" coords="4,108.45,434.78,14.03,10.91" target="#b14">15]</ref> we applied it for this classification task.</p><p>The encoder was not fine-tuned because of the large memory requirement. Running on Google Colabs required a RAM of about 12 gigabyte and even for better performance and speed, a GPU and a RAM greater 32 gigabyte is recommended. The model was trained in 10 epochs with callbacks on the binary validation loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results Obtained</head><p>During training the fine-tuned ALBERT model used for the English task peaked at a best binary validation accuracy of 0.65 as illustrated in Table <ref type="table" coords="4,309.59,561.16,3.75,10.91" target="#tab_0">1</ref>. A possible explanation for this rather low figure is that the length of data used for training was length of 200 and longer sentences for each user. Besides each user having 200 tweets, those users profiled as hate speech spreaders may still be quite similar to non-hate spreaders as not all tweets in the history of hate spreaders may contain hate. Therefore training a model to perform classification on such a dataset can be quite a challenge.</p><p>However, with the Spanish model which applied a BERT-based language-agnostic sentence encoder that was not fine-tuned performance peaked at a binary validation accuracy of 0.71 (see Table <ref type="table" coords="4,116.12,669.55,4.22,10.91" target="#tab_1">2</ref>) but (unlike the English system) performed better on the test set. The model attained an accuracy of 0.53 for English and 0.77 for Spanish after evaluation on the test set for the challenge.</p><p>It is not easy to put these numbers in context -other than observing that the performance of the English run was surprisingly low. The language-agnostic BERT-based sentence encoder on the other hand performed better as our Spanish run outperformed three of the baselines <ref type="bibr" coords="5,89.29,430.63,12.87,10.91" target="#b6">[7]</ref> submitted (see Table <ref type="table" coords="5,199.09,430.63,3.59,10.91">3</ref>). Understanding why different approaches perform better or worse on a particular dataset is not easy anyway, in particular when it comes to the explainability and interpretability of neural network-based approaches, e.g. <ref type="bibr" coords="5,366.44,457.73,35.96,10.91">[17][18]</ref> as performance can be affected by many parameters including a particular sample, learning rate, initialized weights among others used in training. What we do however see is a huge performance variation across training, validation and test data as well as across different submissions for this task. We attribute this in part to the small sample making it difficult to draw generalizable conclusions from a single run. We conclude that the approaches need to be tested on a wide range of additional collections to gain a better understanding of strengths and weaknesses as well as variation of results and robustness more generally, something that fits well with the idea of moving away from aiming to train systems that do amazingly well on specific collections but tend to fall over when applying them elsewhere, e.g. <ref type="bibr" coords="5,325.43,579.67,16.25,10.91" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>The use of transformer-based models for NLP tasks has pushed the state of the art in many applications. In this experiment, two different (very simple) BERT-based models were applied in an attempt to classify hate speech spreaders through training on the bulkiness of their timelines.</p><p>We extracted contextual embeddings by using pretrained transformer-based models and then run through a single layered output for classification. What we found in our experiments was that the power of transformer-based approaches varied substantially across runs, and some traditional baselines did in fact perform surprisingly well (at a much lower cost overall). We do however not see this as a reflection of the weakness of more sophisticated methodologies but more of an issues arising from the datasets that are used for training and testing. The aim should be to explore a wide range of datasets to find out which of the methods is most robust, something particularly important when thinking about hate speech. A particularly promising approach, which has been shown to work well in many NLP tasks including hate speech detection <ref type="bibr" coords="6,165.58,208.91,11.34,10.91" target="#b3">[4]</ref>, is to use ensemble classifiers which can tap into the different strengths of individual classifiers, be it transformer-based or traditional ideas.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,391.38,282.02,8.93"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Small sample of the raw XML of the English Training Data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.98,90.49,295.31,93.78"><head>Table 1</head><label>1</label><figDesc>Accuracy Results for English</figDesc><table coords="4,210.98,122.10,173.31,62.16"><row><cell cols="3">English(Albert) Sample Size Accuracy</cell></row><row><cell>Training</cell><cell>160</cell><cell>0.59</cell></row><row><cell>Validation</cell><cell>32</cell><cell>0.65</cell></row><row><cell>Test</cell><cell>8</cell><cell>0.86</cell></row><row><cell>Evaluation Test</cell><cell>100</cell><cell>0.53</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,88.98,90.49,296.33,245.69"><head>Table 2</head><label>2</label><figDesc>Accuracy results for Spanish</figDesc><table coords="5,88.99,122.10,296.32,214.07"><row><cell cols="3">Spanish(LaBSE) Sample Size Accuracy</cell></row><row><cell>Training</cell><cell>160</cell><cell>0.56</cell></row><row><cell>Validation</cell><cell>32</cell><cell>0.71</cell></row><row><cell>Test</cell><cell>8</cell><cell>0.75</cell></row><row><cell>Evaluation Test</cell><cell>100</cell><cell>0.77</cell></row><row><cell>Table 3</cell><cell></cell><cell></cell></row><row><cell cols="2">Baselines comparison of Accuracy results for Spanish test set</cell><cell></cell></row><row><cell>Model</cell><cell cols="2">Accuracy</cell></row><row><cell cols="2">Word nGram+SVM 0.83</cell><cell></cell></row><row><cell>LSDE</cell><cell>0.82</cell><cell></cell></row><row><cell>USE+LSTM</cell><cell>0.79</cell><cell></cell></row><row><cell>LaBSE(ours)</cell><cell>0.77</cell><cell></cell></row><row><cell>MBERT LSTM</cell><cell>0.76</cell><cell></cell></row><row><cell>XLMR-LSTM</cell><cell>0.73</cell><cell></cell></row><row><cell>TFIDF-LSTM</cell><cell>0.51</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.93,671.04,105.86,8.97"><p>https://github.com/kaodamie</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported by the project <rs type="projectName">COURAGE: A Social Media Companion Safeguarding and Educating Students</rs> funded by the <rs type="funder">Volkswagen Foundation</rs>, grant number <rs type="grantNumber">95564</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_xeSUDqg">
					<idno type="grant-number">95564</idno>
					<orgName type="project" subtype="full">COURAGE: A Social Media Companion Safeguarding and Educating Students</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,112.66,353.26,393.33,10.91;6,112.66,366.81,246.80,10.91" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M.-A</forename><surname>Rizoiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ferraro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Suominen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03829</idno>
		<title level="m" coord="6,320.75,353.26,185.24,10.91;6,112.66,366.81,64.17,10.91">Transfer learning for hate speech detection in social media</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,112.66,380.36,393.32,10.91;6,112.66,393.91,308.72,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,229.22,380.36,199.42,10.91">Detecting hate speech on the world wide web</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Warner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hirschberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,452.18,380.36,53.80,10.91;6,112.66,393.91,230.65,10.91">Proceedings of the second workshop on language in social media</title>
		<meeting>the second workshop on language in social media</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="19" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,407.46,394.53,10.91;6,112.66,421.01,393.33,10.91;6,112.66,434.55,393.33,10.91;6,112.66,448.10,316.43,10.91" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ognibene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Taibi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Kruschwitz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>Wilkens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hernandez-Leo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Theophilou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Scifo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Lobo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Lomonaco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Eimler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">U</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Malzahn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04211</idno>
		<title level="m" coord="6,450.67,421.01,55.31,10.91;6,112.66,434.55,393.33,10.91;6,112.66,448.10,133.50,10.91">Challenging social media threats using collective well-being aware recommendation algorithms and an educational virtual companion</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,112.66,461.65,395.17,10.91;6,112.66,475.20,393.33,10.91;6,112.66,488.75,199.78,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,286.04,461.65,221.79,10.91;6,112.66,475.20,61.92,10.91">Improving hate speech detection with deep learning ensembles</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zimmerman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Kruschwitz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,199.59,475.20,306.40,10.91;6,112.66,488.75,170.04,10.91">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,502.30,393.33,10.91;6,112.66,515.85,393.60,10.91;6,112.66,529.40,125.73,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,406.65,502.30,99.33,10.91;6,112.66,515.85,154.06,10.91">Profiling Hate Speech Spreaders on Twitter Task at PAN</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">L D L P</forename><surname>Sarracén</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chulvi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fersini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="6,311.51,515.85,142.17,10.91">CLEF 2021 Labs and Workshops</title>
		<title level="s" coord="6,461.81,515.85,44.45,10.91;6,112.66,529.40,56.73,10.91">Notebook Papers, CEUR</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,542.95,393.33,10.91;6,112.66,556.50,393.33,10.91;6,112.66,570.05,393.32,10.91;6,112.66,583.60,393.33,10.91;6,112.66,597.15,394.03,10.91;6,112.66,610.69,234.20,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,323.15,542.95,182.83,10.91;6,112.66,556.50,186.91,10.91">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://www.aclweb.org/anthology/N19-1423.doi:10.18653/v1/N19-1423" />
	</analytic>
	<monogr>
		<title level="m" coord="6,327.87,556.50,178.11,10.91;6,112.66,570.05,393.32,10.91;6,112.66,583.60,99.97,10.91">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="6,112.66,624.24,395.17,10.91;6,112.66,637.79,393.33,10.91;6,112.66,651.34,265.47,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,295.40,624.24,212.43,10.91;6,112.66,637.79,122.58,10.91">A Low Dimensionality Representation for Language Variety Identification</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Franco-Salvador</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,257.97,637.79,248.02,10.91;6,112.66,651.34,135.18,10.91">International Conference on Intelligent Text Processing and Computational Linguistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="156" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,664.89,394.53,10.91;7,112.66,86.97,395.17,10.91;7,112.66,100.52,393.33,10.91;7,112.66,114.06,393.33,10.91;7,112.66,127.61,222.66,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,194.78,100.52,311.21,10.91;7,112.66,114.06,221.30,10.91">Overview of PAN 2021: Authorship Verification,Profiling Hate Speech Spreaders on Twitter,and Style Change Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chulvi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">L D L P</forename><surname>Sarracén</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Manjavacas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Markov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mayerl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wolska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Zangerle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,357.51,114.06,148.47,10.91;7,112.66,127.61,150.17,10.91">12th International Conference of the CLEF Association (CLEF 2021)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,141.16,394.53,10.91;7,112.66,154.71,393.33,10.91;7,112.66,168.26,394.51,10.91;7,112.66,184.25,117.15,7.90" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,327.46,141.16,175.13,10.91">TIRA Integrated Research Architecture</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-22948-1_5</idno>
	</analytic>
	<monogr>
		<title level="m" coord="7,240.99,154.71,264.99,10.91;7,112.66,168.26,123.97,10.91">Information Retrieval Evaluation in a Changing World, The Information Retrieval Series</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,195.36,395.17,10.91;7,112.66,208.91,162.40,10.91;7,290.38,208.91,132.24,10.91;7,437.93,208.91,68.05,10.91;7,112.66,222.46,107.17,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>-Y. Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11175</idno>
		<title level="m" coord="7,290.38,208.91,127.49,10.91">Universal sentence encoder</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,112.66,236.01,393.53,10.91;7,112.66,249.56,393.32,10.91;7,112.33,263.11,29.19,10.91" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="7,392.89,236.01,113.30,10.91;7,112.66,249.56,241.83,10.91">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,112.66,276.66,394.53,10.91;7,112.66,290.20,295.45,10.91" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<title level="m" coord="7,303.00,276.66,204.19,10.91;7,112.66,290.20,113.82,10.91">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,112.66,303.75,393.33,10.91;7,112.66,317.30,347.38,10.91" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<title level="m" coord="7,335.14,303.75,170.85,10.91;7,112.66,317.30,165.13,10.91">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,112.66,330.85,394.53,10.91;7,112.30,344.40,393.68,10.91;7,112.66,357.95,107.17,10.91" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m" coord="7,173.53,344.40,256.77,10.91">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,112.66,371.50,393.33,10.91;7,112.66,385.05,340.27,10.91" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="7,295.74,371.50,210.25,10.91;7,112.66,385.05,157.57,10.91">Universal sentence representation learning with conditional masked language model</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Darve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.14388</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,112.66,398.60,393.33,10.91;7,112.66,412.15,229.42,10.91" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="7,355.58,398.60,150.41,10.91;7,112.66,412.15,46.53,10.91">Language-agnostic bert sentence embedding</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01852</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,112.66,425.70,393.33,10.91;7,112.66,439.25,393.58,10.91;7,112.33,452.79,29.19,10.91" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Semenova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11251</idno>
		<title level="m" coord="7,405.54,425.70,100.45,10.91;7,112.66,439.25,248.02,10.91">Interpretable machine learning: Fundamental principles and 10 grand challenges</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,112.66,466.34,394.53,10.91;7,112.66,479.89,173.79,10.91" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Tiňo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.14261</idno>
		<title level="m" coord="7,303.94,466.34,199.10,10.91">A survey on neural network interpretability</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,112.66,493.44,395.17,10.91;7,112.66,506.99,395.17,10.91;7,112.33,520.54,393.65,10.91;7,112.66,534.09,395.17,10.91;7,112.66,547.64,395.17,10.91;7,112.41,561.19,394.86,10.91;7,112.31,574.74,230.16,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="7,251.05,493.44,256.78,10.91;7,112.66,506.99,98.63,10.91">What will it take to fix benchmarking in natural language understanding?</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2021.naacl-main.385/" />
	</analytic>
	<monogr>
		<title level="m" coord="7,438.45,520.54,67.53,10.91;7,112.66,534.09,395.17,10.91;7,112.66,547.64,317.96,10.91">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Rumshisky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Hakkani-Tür</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bethard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Cotterell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Chakraborty</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</editor>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">June 6-11, 2021. 2021</date>
			<biblScope unit="page" from="4843" to="4855" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
