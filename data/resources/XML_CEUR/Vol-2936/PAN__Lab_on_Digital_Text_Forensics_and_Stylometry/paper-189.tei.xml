<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,320.03,15.42;1,89.29,106.66,240.77,15.42;1,89.29,129.00,157.29,11.96">Detection of hate speech spreaders using convolutional neural networks Notebook for PAN at CLEF 2021</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,154.90,58.76,11.96"><forename type="first">Marco</forename><surname>Siino</surname></persName>
							<email>marco.siino@unipa.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Ingegneria</orgName>
								<orgName type="institution">Università degli Studi di Palermo</orgName>
								<address>
									<postCode>90128</postCode>
									<settlement>Palermo</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,160.70,154.90,73.30,11.96"><forename type="first">Elisa</forename><forename type="middle">Di</forename><surname>Nuovo</surname></persName>
							<email>elisa.dinuovo@unito.it</email>
							<affiliation key="aff1">
								<orgName type="department">Dipartimento di Lingue e Letterature Straniere e Culture Moderne</orgName>
								<orgName type="institution">Università degli Studi di Torino</orgName>
								<address>
									<postCode>10124</postCode>
									<settlement>Torino</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,246.64,154.90,78.83,11.96"><forename type="first">Ilenia</forename><surname>Tinnirello</surname></persName>
							<email>ilenia.tinnirello@unipa.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Ingegneria</orgName>
								<orgName type="institution">Università degli Studi di Palermo</orgName>
								<address>
									<postCode>90128</postCode>
									<settlement>Palermo</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,356.47,154.90,80.41,11.96"><forename type="first">Marco</forename><forename type="middle">La</forename><surname>Cascia</surname></persName>
							<email>marco.lacascia@unipa.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Ingegneria</orgName>
								<orgName type="institution">Università degli Studi di Palermo</orgName>
								<address>
									<postCode>90128</postCode>
									<settlement>Palermo</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,320.03,15.42;1,89.29,106.66,240.77,15.42;1,89.29,129.00,157.29,11.96">Detection of hate speech spreaders using convolutional neural networks Notebook for PAN at CLEF 2021</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">B0C57346829BA96697E0244AF2C3452A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Profiling</term>
					<term>Hate Speech</term>
					<term>Twitter</term>
					<term>Spanish</term>
					<term>English</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe a deep learning model based on a Convolutional Neural Network (CNN). The model was developed for the Profiling Hate Speech Spreaders (HSSs) task proposed by PAN 2021 organizers and hosted at the 2021 CLEF Conference. Our approach to the task of classifying an author as HSS or not (nHSS) takes advantage of a CNN based on a single convolutional layer. In this binary classification task, on the tests performed using a 5-fold cross validation, the proposed model reaches a maximum accuracy of 0.80 on the multilingual (i.e., English and Spanish) training set, and a minimum loss value of 0.51 on the same set. As announced by the task organizers, the trained model presented is able to reach an overall accuracy of 0.79 on the full test set. This overall accuracy is obtained averaging the accuracy achieved by the model on both languages. In particular, with regard to the Spanish test set, the organizers announced that our model achieves an accuracy of 0.85, while on the English test set the same model achieved -as announced by the organizers too -an accuracy of 0.73. Thanks to the model presented in this paper, our team won the 2021 PAN competition on profiling HSSs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The aim of the PAN 2021 Profiling Hate Speech Spreaders (HSSs) on Twitter task <ref type="bibr" coords="1,451.21,467.25,11.35,10.91" target="#b0">[1,</ref><ref type="bibr" coords="1,465.28,467.25,8.95,10.91" target="#b1">2]</ref> was to investigate whether the author of a given Twitter thread is likely to spread tweets containing hate speech or not. The multilingual dataset, namely English and Spanish datasets provided by the organizers of the task, consisted of 120,000 tweets: 200 tweets per author, 200 authors per each language training set and 100 authors per each language test set <ref type="bibr" coords="1,401.04,521.45,11.45,10.91" target="#b2">[3]</ref>. The model we used to compete for the task consists of a shallow Convolutional Neural Network (CNN). Broadly speaking, our network preprocess each sample in the dataset to build a dictionary 1 where the key is an integer number and the value is an n-gram resulting from our custom preprocessing function. Each integer value (e.g. a key in the dictionary) is then mapped to a single point into a 100-dimensional word embedding space. Then, a 1-dimensional convolution is applied. The output of the convolution layer is then fed into an average pooling and then into a global average pooling layer fully connected to a single dense layer output.</p><p>The remainder of this paper is organized as follows. In Section 2 we present some related works about the usage of deep learning methods for similar text classification tasks. In Section 3 we describe in detail our approach, explaining our choices and the configuration of each layer of the model. In Section 4 we report the results obtained in the final 5-fold cross validation and on the test set. In Section 5 we discuss some interesting future works and in Section 6 we conclude our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>To address the problem of identifying HSSs within the Twitter microblogging platform, we started from the analysis and study of state-of-the-art techniques for text classification <ref type="bibr" coords="2,472.24,294.63,11.27,10.91" target="#b3">[4,</ref><ref type="bibr" coords="2,486.24,294.63,7.44,10.91" target="#b4">5,</ref><ref type="bibr" coords="2,496.40,294.63,7.51,10.91" target="#b5">6]</ref>. Our choice to use a deep learning-based approach was dictated by the notable performances reached in various text classification tasks where deep AI methods outperformed classical techniques used in natural language processing (e.g. Bayes, Decision Tree, K-Neearest Neighbour, Support Vector Machine) as reported in <ref type="bibr" coords="2,267.85,348.83,11.36,10.91" target="#b6">[7,</ref><ref type="bibr" coords="2,281.93,348.83,7.57,10.91" target="#b7">8]</ref>.</p><p>Also hybrid approaches are present in the literature, as in the case of a CNN used to extract textual features and a SVM to carry out the classification and prediction <ref type="bibr" coords="2,412.08,375.93,11.43,10.91" target="#b8">[9]</ref>. However, similar results to that obtained by a CNN can be achieved.</p><p>As a starting point to develop our model we decided to consider the work conducted in <ref type="bibr" coords="2,488.13,403.03,17.85,10.91" target="#b9">[10]</ref> in which, for the first time, a CNN was used to address a text classification task. The CNN obtained promising performances compared to the state of the art. To model text in a vector format, a common representation in most deep learning models is based on word embeddings <ref type="bibr" coords="2,89.29,457.22,16.56,10.91" target="#b10">[11,</ref><ref type="bibr" coords="2,109.10,457.22,12.42,10.91" target="#b11">12]</ref>. And it is thanks to word embedding-based methods that since 2015 <ref type="bibr" coords="2,443.24,457.22,18.07,10.91" target="#b12">[13]</ref> deep and non-deep models have performed very well on several text classification tasks.</p><p>As already mentioned, our work concerning the task of identifying users prone to spread hate speech on Twitter by analysing their last 200 tweets was experimentally tested on the dataset provided by the PAN 2021 competition organizers.</p><p>A similar author profiling task was organized last year <ref type="bibr" coords="2,342.49,524.97,16.12,10.91" target="#b13">[14]</ref>, in which the participants had to identify authors prone to spread fake news based on their last 100 tweets. The winners were <ref type="bibr" coords="2,89.29,552.07,17.97,10.91" target="#b14">[15]</ref> and <ref type="bibr" coords="2,129.22,552.07,16.31,10.91" target="#b15">[16]</ref>. Their models obtained an overall accuracy of 0.77 on the provided test set. The approaches used by the winners were based on a SVM and n-grams and on an ensemble of different machine learning models. As reported in <ref type="bibr" coords="2,313.71,579.17,17.88,10.91" target="#b13">[14]</ref> the only approach based on CNNs was the one presented in <ref type="bibr" coords="2,183.02,592.72,16.25,10.91" target="#b16">[17]</ref>, achieving an overall accuracy of 0.72. some input text) in which keys are token indices for such a list and values are the tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed model</head><p>The architecture of the model is presented in Figure <ref type="figure" coords="3,323.91,111.28,3.76,10.91" target="#fig_0">1</ref>, in which the dimensions of inputs and outputs of each model layer are highlighted. Each layer of the network is chosen taking into account the works presented in Section 2 as well as those cited below and the hints gained from our extensive tests conducted over the training set provided. Indeed, many experiments were attempted to determine the appropriate hyperparameter values and network architecture. In what follows we present each layer of the network and the choosen hyperparameter values. Before discussing the network architecture, it is important to bear in mind that each set of the dataset (training and test per language) is made of XML files-each XML is related to a single author-containing 200 tweets of the author. In addition, a ground truth file containing the labels 0 and 1 matched to each XML file is provided for the training set. For handling these files and before training, our system organizes these XML files into two folders (i.e., 0 and 1) while reading the ground truth file. Then each sample (i.e., a single XML file) is read by the model for training or test, depending on the number of fold validation. These function for reading samples is accomplished by the first layer (namely, InputLayer).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Text vectorization</head><p>The first layer of our model reads the text in the XML files and apply our custom preprocessing function to split n-grams. In what follows, we refer to an n-gram as a sequence of characters. This sequence of characters is determined looking at the space before and after the sequence considered (i.e., the n-grams are splitted from the input text in correspondence of spaces). Then we build a dictionary where the keys are integer numbers and the values are the n-grams from the training set. While applying this tokenization based on spaces to the English dataset, we likely obtain n-grams that correspond to traditional tokens, or syntactic words. It is not the case when applied to the Spanish language. Hence, being n-gram a broader term as defined above, we prefer saying n-grams instead of tokens. Since the classification of HSSs is approached as an author profiling task, we decided to keep punctuation and capitalization to maintain a certain amount of stylistic information in our dictionary entries. Specifically and as an example, when splitting text, we associate two different dictionary entries to the word Hello and to hello or to Hello!. The hyperparameters characterizing this layer are described below.</p><p>• Standardize. It is the preprocessing function applied to the text before proceeding with its vectorization. In our case, this function, in addition to removing tabulations and newline characters, substitutes the occurrences of the CDATA tag with a space followed by a minus than sign, adds a space between the closing of one tag and the next, and then split each n-gram at each space; • Max tokens. This parameter refers to the dictionary size. To get this value we simply count the numbers of different n-grams resulting from our preprocessing step. It is worth noting that our dictionary size is developed scanning both the Spanish and the English training sets; • Output mode. This parameter is the type of token index returned by the vectorization function. We used the INT type, so that every word is mapped to a positive integer number; </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Embedding</head><p>This layer takes as input a tensor of 3,911 integer numbers generated as described in the previous subsection. Each integer value of this tensor is mapped to a 100-dimension word embedding tensor. In this way, each integer from the previous layer is mapped to a single tensor consisting of 100 floating point values. A notable difference with the previous layer is that the 100 coordinate values of each tensor is updated at each optimization step while training the model. More precisely, we trained and tested multiple models as the word embedding space varies from 2 to 800 dimensions, as also discussed in a similar Twitter text classification problem <ref type="bibr" coords="5,89.29,202.38,16.10,10.91" target="#b17">[18]</ref>. The best performances over different tests on a 5-fold cross validation were obtained with a 100-dimension embedding space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Convolution</head><p>In our model a single 1D-convolution layer is implemented. This layer consists of 64 filters of size 36. The layer then performs convolution on 36-ngram windows with stride value of 1 (i.e., after each convolution, the convolutional filter is shifted of one word embedding tensor). For this layer no padding is added and ReLu <ref type="bibr" coords="5,299.23,306.30,16.56,10.91" target="#b18">[19,</ref><ref type="bibr" coords="5,319.39,306.30,14.11,10.91" target="#b19">20]</ref> is used as activation function on the output values. Number of filters and filters size (i.e., the two main parameters of this layer) are of paramount importance for the global performance of the model. Indeed, the filter size determines the size of the windows over the text of the input sample provided. In this way, we observed that a filter of size 36 generally gets n-grams from 3-4 different tweets each time.</p><p>Similarly, the number of filters used (i.e., 64) determines the number of different feature maps relevant for the classification task. Both parameters are determined after extensive experiments conducted over the training set on many 5-fold cross validation runs. To fine-tune these two hyperparameters, we performed a binary search <ref type="bibr" coords="5,307.11,414.70,16.45,10.91" target="#b20">[21,</ref><ref type="bibr" coords="5,326.27,414.70,14.04,10.91" target="#b21">22]</ref> for both, looking in the range values 1-1,024. We discovered that a number of filters greater than 256 increases the overfitting of the model while a filter size greater than 1,024 does not allow the model to reach an accuracy of 1.0 not even on the train fold considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Average and global average pooling</head><p>The average pooling layer <ref type="bibr" coords="5,210.10,505.07,18.06,10.91" target="#b22">[23]</ref> downsamples the input representation by taking the average value over the window defined by a pool size parameter. The window is shifted by strides. As an example, consider a single dimension array X = [1.0, 2.0, 3.0, 4.0, 5.0]. Defining a 1D-average pooling layer having pool size of 2 and stride of 1 and providing X as input to such a layer, the array Y=[1.5, 2.5, 3.5, 4.5] is returned. In this case too, in the attempt of finding the best value for the hyperparameters of this layer, we performed a binary search and found an optimimum value of 8 for the pool size and 1 for the stride. The pool size of 8 represents the number of averaged values outputted from the convolution layer at each step. We suppose that the optimum of 1 as stride value might be maybe due to our tokenization choices. A final 1D-Global Average Pooling layer is similar to the previously described average pooling one. In this case, it is not the average value over a window of the pool size defined that is returned as output but, instead, a global average along the first dimension from the previous layer outputs. Looking at the Figure <ref type="figure" coords="5,249.62,667.66,3.67,10.91" target="#fig_0">1</ref>, the output of AveragePooling1D layer is made of 484x64 elements. The global average value is calculated along the first dimension of size 484, in fact reducing by one the dimension of the input tensor. As an example consider the following matrix X. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑋</head><formula xml:id="formula_0" coords="6,252.47,137.18,122.05,58.56">𝑥 𝑑1 𝑥 𝑑2 𝑥 𝑑3 . . . 𝑥 𝑑𝑛 ⎤ ⎥ ⎥ ⎥ ⎦</formula><p>Providing X as input to a 1D-Global Average Pooling layer returns as output the following array Y, where each 𝑦 i is calculated averaging all the values along the i-th column of the matrix X.</p><formula xml:id="formula_1" coords="6,234.54,256.53,126.20,12.44">𝑌 = [︀ 𝑦 1 𝑦 2 𝑦 3 . . . 𝑦 𝑛 ]︀</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Dense</head><p>The Global Average Pooling 1D layer is fully-connected to the last layer which is a single dense unit output. The layer is followed by a simple linear activation (e.g., 𝑎(𝑥) = 𝑥). The final output is a single float value. Positive values are considered as HSSs and negative ones as nHSSs. A threshold of 0.0 is set to determine the accuracy of the model in predicting the label of the sample provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Model training</head><p>The values assigned to the various hyperparameters were originally set taking into account many of the decisions adopted in the studies conducted in <ref type="bibr" coords="6,345.86,424.82,16.31,10.91" target="#b23">[24,</ref><ref type="bibr" coords="6,364.89,424.82,13.95,10.91" target="#b24">25]</ref> and subsequently fine-tuned to improve the accuracy achieved by our model. To initialize the weights of the model we used a Glorot uniform initializer <ref type="bibr" coords="6,202.19,451.92,16.09,10.91" target="#b25">[26]</ref>. The model is compiled with a binary cross entropy loss function; this function calculates loss with respect to two classes (i.e., 0 and 1) as defined in 1.</p><formula xml:id="formula_2" coords="6,136.56,489.29,369.42,33.58">𝐿𝑜𝑠𝑠 𝐵𝐶𝐸 = - 1 𝑁 𝑁 ∑︁ 𝑛=1 [𝑦 𝑛 × log (ℎ 𝜃 (𝑥 𝑛 )) + (1 -𝑦 𝑛 ) × log (1 -ℎ 𝜃 (𝑥 𝑛 ))]<label>(1)</label></formula><p>where:</p><p>• N is the number of training examples;</p><p>• 𝑦 𝑛 is the target label for the training sample 𝑛;</p><p>• 𝑥 𝑛 is the input sample 𝑛;</p><p>• ℎ 𝜃 is the neural network model with weights 𝜃.</p><p>Optimization is performed with an Adamic optimizer <ref type="bibr" coords="6,334.58,619.10,18.06,10.91" target="#b26">[27]</ref> after giving each batch of data as input. We performed a binary search for finding the optimal batch size. The model achieved the best overall accuracy with a batch size of 2. The model architecture is depicted in Figure <ref type="figure" coords="6,499.75,646.20,3.72,10.91" target="#fig_1">2</ref>, where the number of the various network hyperparameters are provided. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Dataset summary showing the number of samples (i.e., authors) for each set, language and class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental evaluation and results</head><p>In this section we report the results obtained by our model during evaluation on the 5-fold cross validation on the training set. Then, we report the results of the trained model on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiments</head><p>Table <ref type="table" coords="7,115.06,483.93,4.97,10.91">1</ref> reports the number of samples within each set. Each sample in all the sets is an XML file whose name corresponds to the author id. Each XML file contains 200 tweets of the considered author. Considering that we have 200 tweets per author (the number of authors is shown in Table <ref type="table" coords="7,115.79,524.58,3.57,10.91">1</ref>), the whole dataset consists of 120,000 tweets. Task organizers invited participants to deploy their models on TIRA <ref type="bibr" coords="7,398.51,538.13,19.53,10.91" target="#b27">[28]</ref>. As communicated by email by the task organizers, TIRA has been experiencing technical issues. Therefore, our model was developed and tested as a Jupyter Notebook in Google Colab using TensorFlow. The complete source code is publicly available and reusable. <ref type="foot" coords="7,336.73,577.02,3.71,7.97" target="#foot_0">2</ref>To validate our model and fine-tune its hyperparameters, we ran 5-fold cross validations at each test performed. We considered the full training set made by the union of both language sets, then we shuffled this multilanguage training set and used it for the cross validations. Then we split 80-20 for each of the fold. Specifically, the first fold was made using the first 80% of the samples for training and the remaining 20% for validation. The remaining folds were made as  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>Results achieved by the model on a 5-fold cross validation on the complete multilingual training set (i.e., Spanish and English data). Both loss and accuracy are computed for the validation set used at the fold indicated on the upper row. In the last two columns we report the values of the arithmetic mean and the standard deviation over the 5 folds.</p><p>reported in Table <ref type="table" coords="8,169.18,303.67,5.12,10.91" target="#tab_2">2</ref> with the order of the percentage of samples taken for both sets (train and validation) from the complete training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>In Table <ref type="table" coords="8,127.95,366.95,5.07,10.91">3</ref> the results obtained adopting a 5-fold cross validation on the complete multilingual training dataset are reported. The 5-folds were made as explained in the previous subsection. Table <ref type="table" coords="8,114.73,394.05,4.97,10.91">3</ref> reports accuracy and loss values achieved on the validation set used at each fold, together with the arithmetic mean and standard deviation. For each fold we trained the model for 15 epochs, then we reported the higher accuracy and the related loss over the 15 epochs of training with respect to the validation set used at the fold indicated in the upper row. As can be noted, some splits achieved a better performance and this could be due to a higher level of similarity between the considered train and validation sets. Finally, as reported in the PAN website, our model achieved an accuracy of 0.73 on the English test set and of 0.85 on the Spanish test set. <ref type="foot" coords="8,274.55,487.14,3.71,7.97" target="#foot_1">3</ref> Considering these results, the overall accuracy (i.e., the arithmetic mean of the accuracy achieved per language) is 0.79.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Future works</head><p>In future developments, it would be interesting to analyze the behaviour of our model and the output of each layer. Furthermore, another point deserving to be investigated concerns the reasons behind the fact that the tested models are better at identifying HSSs in Spanish than in English. In fact, in each performed test, our models performed better on the Spanish set, even using a non-deep model (i.e., a Naïve Bayesian model using the same preprocessing function). Perhaps, conducting a qualitative analysis of the dataset tweets could be beneficial for understanding why profiling HSSs in Spanish achieves a higher accuracy than in English. This investigation could help us shed some light on this accuracy difference in the two datasetswhich concerned also the last year PAN author profiling task-but also on how our model works looking at both correct and wrong predictions. Another direction to improve accuracy in profiling HSSs could be to add more complexity to the model, maybe using some additional layers. Given the dimension of the dataset provided some techniques of data augmentation could be also applied. Finally, some investigation on the content of each tweet could guide us in applying some techniques to remove noise (i.e., not relevant features) from the input samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we described the submitted model for the Profiling HSSs on Twitter task at PAN 2021. It consists of a CNN based on a single convolutional layer. To get a more accurate evaluation of the model performance, we run several 5-fold cross validation for each different hyperparameter configuration. In fact, several binary searches are conducted to fine tuning the hyperparameters involved during the training of our proposed model. After finding the model achieving the highest accuracy during our cross validation tests, we trained such a model on the entire training set to submit our predictions on the test set. The model proved to maintain the good accuracy achieved on the cross-validation process also when tested on the test set. Overall, as announced by the organizers, our software-achieving an average accuracy of 0.79 (0.73 on the English test set and 0.85 on the Spanish test set)-ranked first in the PAN 2021 HSSs profiling task. Our model, developed in TensorFlow, is publicly available as a Jupyter Notebook on Google Colab.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,401.40,416.69,9.15;3,89.29,413.62,416.70,8.87;3,89.29,425.58,42.39,8.87;3,180.00,147.54,235.27,246.65"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Model architecture. Numbers in brackets indicate tensor dimensions; 𝑁 𝑜𝑛𝑒 stands for the batch dimension not yet known before running the model. Layers as depicted on our Google Colab Notebook.</figDesc><graphic coords="3,180.00,147.54,235.27,246.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,89.29,228.55,416.69,8.93;7,89.29,240.56,416.70,8.87;7,89.29,252.51,115.17,8.87;7,177.17,84.19,240.94,136.94"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Model representation showing the number of parameters involved at each layer. Such a small number of parameters allows low computational load for training and testing. Figure as depicted on our Google Colab notebook.</figDesc><graphic coords="7,177.17,84.19,240.94,136.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,109.95,86.37,28.62,8.93;8,189.21,86.37,28.62,8.93;8,283.33,86.37,28.62,8.93;8,377.44,86.37,28.62,8.93;8,456.70,86.37,28.62,8.93;8,98.24,98.77,398.79,8.87"><head></head><label></label><figDesc>60%T -20%V -20%T 40%T -20%V -40%T 20%T -20%V -60%T 20%V -80%T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,233.68,137.19,133.01,46.25"><head></head><label></label><figDesc>𝑥 11 𝑥 12 𝑥 13 . . . 𝑥 1𝑛 𝑥 21 𝑥 22 𝑥 23 . . . 𝑥 2𝑛</figDesc><table coords="6,233.68,137.19,126.65,46.25"><row><cell></cell><cell>⎡</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>=</cell><cell>⎢ ⎢ ⎢ ⎣</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,88.99,116.59,416.99,96.93"><head>Table 2 5</head><label>2</label><figDesc>-fold splitting applied on the complete multilingual training set. T indicates that this percentage is used for training and V for validation on this fold.</figDesc><table coords="8,135.92,167.59,323.44,45.93"><row><cell></cell><cell></cell><cell></cell><cell>Fold Nr.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>Avg.</cell><cell>Dev.</cell></row><row><cell>Accuracy</cell><cell cols="7">0.6625 0.7000 0.6750 0.8000 0.6875 0.7050 0.0491</cell></row><row><cell>Loss</cell><cell cols="7">0.6097 0.7070 0.7771 0.5074 0.6234 0.6449 0.0916</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="7,108.93,671.03,381.73,8.97"><p>Model notebook: https://colab.research.google.com/drive/1hUwn_uk0YPC6Tpo3MK1gDVGPQxmzPh_E.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="8,108.93,671.02,334.32,8.97"><p>Pan 2021 task results: https://pan.webis.de/clef21/pan21-web/author-profiling.html#results.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank anonymous reviewers for their comments and suggestions that have helped to improve the presentation of the paper.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CRediT Authorship Contribution Statement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Online Resources</head><p>The source code of our model is available via • GitHub, • Google Colab.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,619.55,394.53,10.91;9,112.66,633.10,395.17,10.91;9,112.66,646.65,393.33,10.91;10,112.66,86.97,393.33,10.91;10,112.66,100.52,243.54,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,194.78,646.65,311.21,10.91;10,112.66,86.97,221.30,10.91">Overview of PAN 2021: Authorship Verification,Profiling Hate Speech Spreaders on Twitter,and Style Change Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chulvi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">L D L P</forename><surname>Sarracén</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Manjavacas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Markov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mayerl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wolska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Zangerle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,357.51,86.97,148.47,10.91;10,112.66,100.52,150.17,10.91">12th International Conference of the CLEF Association (CLEF 2021)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,114.06,393.33,10.91;10,112.66,127.61,393.60,10.91;10,112.66,141.16,146.61,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,406.65,114.06,99.33,10.91;10,112.66,127.61,154.06,10.91">Profiling Hate Speech Spreaders on Twitter Task at PAN</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">L D L P</forename><surname>Sarracén</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chulvi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fersini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,311.51,127.61,142.17,10.91">CLEF 2021 Labs and Workshops</title>
		<title level="s" coord="10,461.81,127.61,44.45,10.91;10,112.66,141.16,73.07,10.91">Notebook Papers, CEUR-WS</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,154.71,393.33,10.91;10,112.66,168.26,325.83,10.91" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Chulvi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">L</forename><surname>De La Pena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fersini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<ptr target="https://zenodo.org/record/4603578" />
		<title level="m" coord="10,405.57,154.71,100.41,10.91;10,112.66,168.26,93.27,10.91">Profiling Hate Speech Spreaders on Twitter</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Data set</note>
</biblStruct>

<biblStruct coords="10,112.66,181.81,395.17,10.91;10,112.66,195.36,306.85,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,237.44,181.81,213.24,10.91">Text classification techniques: A literature review</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Thangaraj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sivakami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,463.07,181.81,44.76,10.91;10,112.66,195.36,262.06,10.91">Interdisciplinary Journal of Information, Knowledge &amp; Management</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,208.91,394.53,10.91;10,112.66,222.46,269.51,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,216.97,208.91,285.52,10.91">Semantic text classification: A survey of past and recent advances</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Altınel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">C</forename><surname>Ganiz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,112.66,222.46,175.43,10.91">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="1129" to="1153" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,236.01,393.32,10.91;10,112.66,249.56,221.40,10.91" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Oshikawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00770</idno>
		<title level="m" coord="10,264.44,236.01,241.54,10.91;10,112.66,249.56,39.31,10.91">A survey on natural language processing for fake news detection</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,263.11,395.17,10.91;10,112.66,276.66,283.58,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,223.22,263.11,247.47,10.91">Review of text classification methods on deep learning</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,480.36,263.11,27.47,10.91;10,112.66,276.66,189.50,10.91">CMC-COMPUTERS MATERIALS &amp; CONTINUA</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="1309" to="1321" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,290.20,393.33,10.91;10,112.26,303.75,393.93,10.91;10,112.66,317.30,107.04,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,257.56,290.20,248.43,10.91;10,112.26,303.75,199.87,10.91">Classifying tweets using convolutional neural networks with multi-channel distributed representation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hashida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,320.53,303.75,185.66,10.91;10,112.66,317.30,33.25,10.91">IAENG International Journal of Computer Science</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="68" to="75" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,330.85,393.33,10.91;10,112.66,344.40,394.52,10.91;10,112.66,357.95,117.32,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,185.52,330.85,320.47,10.91;10,112.66,344.40,15.30,10.91">Research on web text classification algorithm based on improved cnn and svm</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,174.22,344.40,328.35,10.91">IEEE 17th International Conference on Communication Technology (ICCT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1958" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,371.50,393.33,10.91;10,112.66,385.05,102.10,10.91" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="10,156.89,371.50,268.11,10.91">Convolutional neural networks for sentence classification</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,398.60,393.32,10.91;10,112.66,412.15,394.53,10.91;10,112.41,425.70,48.64,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,201.44,398.60,215.10,10.91">Learning distributed representations of concepts</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="10,439.82,398.60,66.17,10.91;10,112.66,412.15,275.66,10.91">Proceedings of the eighth annual conference of the cognitive science society</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="1986">1986</date>
			<pubPlace>Amherst, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,439.25,394.53,10.91;10,112.66,452.79,139.40,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,251.87,439.25,250.85,10.91">A survey of word embeddings based on deep learning</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,112.66,452.79,50.39,10.91">Computing</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="717" to="740" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,466.34,393.33,10.91;10,112.66,479.89,393.33,10.91;10,112.66,493.44,274.79,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,235.64,466.34,270.35,10.91;10,112.66,479.89,39.20,10.91">Twitter sentiment analysis with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,175.30,479.89,330.69,10.91;10,112.66,493.44,187.26,10.91">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="959" to="962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,506.99,393.61,10.91;10,112.66,520.54,327.93,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,322.17,506.99,184.10,10.91;10,112.66,520.54,48.74,10.91">Overview of the 8th author profiling task at pan 2020</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Giachanou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,169.16,520.54,176.25,10.91">Profiling fake news spreaders on twitter</title>
		<imprint>
			<publisher>CLEF</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,534.09,386.41,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,159.51,534.09,244.37,10.91">Using n-grams to detect fake news spreaders on twitter</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pizarro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,426.46,534.09,21.05,10.91">CLEF</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,547.64,393.61,10.91;10,112.66,561.19,230.03,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,204.14,547.64,302.13,10.91;10,112.66,561.19,134.84,10.91">An ensemble model using n-grams and statistical features to identify fake news spreaders on twitter</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Bolonyai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,270.08,561.19,21.05,10.91">CLEF</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,574.74,340.78,10.91" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="10,182.00,574.74,176.26,10.91">Profiling fake news spreaders on twitter</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Chilet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>CLEF</publisher>
			<biblScope unit="page">0</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,588.29,394.53,10.91;10,112.66,601.84,215.37,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,257.76,588.29,245.49,10.91">Using word embeddings in twitter election classification</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,112.66,601.84,131.43,10.91">Information Retrieval Journal</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="183" to="207" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,615.39,393.32,10.91;10,112.66,628.93,370.64,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,181.29,615.39,324.69,10.91;10,112.66,628.93,37.46,10.91">Visual feature extraction by a multilayered network of analog threshold elements</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,158.73,628.93,245.71,10.91">IEEE Transactions on Systems Science and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="322" to="333" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,642.48,393.33,10.91;10,112.66,656.03,394.52,10.91;10,112.66,669.58,123.33,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="10,231.22,642.48,274.77,10.91;10,112.66,656.03,176.42,10.91">Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Miyake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,311.89,656.03,191.16,10.91">Competition and cooperation in neural nets</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1982">1982</date>
			<biblScope unit="page" from="267" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,86.97,394.62,10.91;11,112.66,100.52,355.37,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="11,191.56,86.97,294.45,10.91">A modification to the half-interval search (binary search) method</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">F</forename><surname>Williams</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,112.66,100.52,272.52,10.91">Proceedings of the 14th annual Southeast regional conference</title>
		<meeting>the 14th annual Southeast regional conference</meeting>
		<imprint>
			<date type="published" when="1976">1976</date>
			<biblScope unit="page" from="95" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,114.06,395.17,10.91;11,111.60,127.61,59.10,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="11,159.02,114.06,160.74,10.91">The Art Of Computer Programming</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Knuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,358.05,114.06,100.70,10.91">Sorting And Searching</title>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1973">1973</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,141.16,393.86,10.91;11,112.66,154.71,75.95,10.91" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><surname>Tensorflow</surname></persName>
		</author>
		<ptr target="https://keras.io/api/layers/pooling_layers/average_pooling1d/" />
		<title level="m" coord="11,168.97,141.16,105.78,10.91">AveragePooling1D layer</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,168.26,393.33,10.91;11,112.66,181.81,370.57,10.91" xml:id="b23">
	<monogr>
		<title level="m" type="main" coord="11,209.84,168.26,296.15,10.91;11,112.66,181.81,188.85,10.91">A sensitivity analysis of (and practitioners&apos; guide to) convolutional neural networks for sentence classification</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wallace</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.03820</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,195.36,393.53,10.91;11,112.66,208.91,257.46,10.91" xml:id="b24">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jacovi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">S</forename><surname>Shalom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.08037</idno>
		<title level="m" coord="11,280.47,195.36,225.72,10.91;11,112.66,208.91,75.75,10.91">Understanding convolutional neural networks for text classification</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,222.46,339.24,10.91" xml:id="b25">
	<monogr>
		<title level="m" type="main" coord="11,142.69,222.46,106.81,10.91">Layer weight initializers</title>
		<author>
			<persName coords=""><surname>Keras</surname></persName>
		</author>
		<ptr target="https://keras.io/api/layers/initializers/" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,236.01,395.01,10.91" xml:id="b26">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m" coord="11,227.57,236.01,157.91,10.91">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,249.56,394.53,10.91;11,112.66,263.11,393.33,10.91;11,112.66,276.66,397.48,10.91;11,112.36,292.65,152.76,7.90" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="11,327.46,249.56,175.13,10.91">TIRA Integrated Research Architecture</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-22948-1_5</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,240.99,263.11,264.99,10.91;11,112.66,276.66,126.85,10.91">Information Retrieval Evaluation in a Changing World, The Information Retrieval Series</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
