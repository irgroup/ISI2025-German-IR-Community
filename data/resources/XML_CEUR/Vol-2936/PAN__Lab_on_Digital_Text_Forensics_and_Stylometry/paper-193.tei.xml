<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.32,342.05,16.17;1,89.29,108.89,160.62,10.37">Siamese BERT for Authorship Verification Notebook for PAN at CLEF 2021</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.06,134.79,48.51,10.37"><forename type="first">Jacob</forename><surname>Tyo</surname></persName>
							<email>jtyo@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Ave</addrLine>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">US Army Research Laboratory</orgName>
								<address>
									<addrLine>2800 Powder Mill Rd</addrLine>
									<postCode>20783</postCode>
									<settlement>Adelphi</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,157.01,134.79,81.89,10.37"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
							<email>bdhingra@cs.duke.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Duke University</orgName>
								<address>
									<postCode>27708</postCode>
									<settlement>Durham</settlement>
									<region>NC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,269.61,134.79,74.04,10.37"><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
							<email>zlipton@cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Ave</addrLine>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.32,342.05,16.17;1,89.29,108.89,160.62,10.37">Siamese BERT for Authorship Verification Notebook for PAN at CLEF 2021</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">15F2D87DFC7A85ABF96E3A6A5E5FFE02</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The PAN 2021 authorship verification (AV) challenge focuses on determining if two texts are written by the same author or not, specifically when faced with new, unseen, authors. In our approach, we construct a Siamese network initialized with pretrained BERT encoders, employing a learning objective that incentives the model to map texts written by the same author to nearby embeddings while mapping texts written by different authors to comparatively distant embeddings. Additionally, inspired by related work in computer vision, we attempt to incorporate triplet losses but are unable to realize any benefit. Our method results in a slight performance gain of 0.9% overall score over the baseline and an increase of 8% in F1 score.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Authorship verification (AV) is the task of determining if two texts were written by the same person or not. While traditionally, this feat has required the expertise of forensic linguists, recent advances in both natural language processing (NLP) and related matching tasks in computer vision, offer several paths for improving automated methods. The traditional machine learning approach to this problem consists of two steps: feature extraction and model fitting. Feature extraction can include the count of specific words/sub-words/punctuation, misspellings, part-ofspeech tags, etc. More recent methods have paired these hand-engineered features with modern feature extraction methods such as n-grams <ref type="bibr" coords="1,280.41,465.82,11.54,9.46" target="#b0">[1]</ref>, pretrained word embeddings <ref type="bibr" coords="1,425.43,465.82,11.55,9.46" target="#b1">[2]</ref>, and pretrained sentence structure embeddings <ref type="bibr" coords="1,227.59,479.36,11.69,9.46" target="#b2">[3]</ref>. The models leveraged for this task have ranged from latent Dirichlet allocation <ref type="bibr" coords="1,179.10,492.91,12.86,9.46" target="#b3">[4]</ref> and support-vector machines <ref type="bibr" coords="1,327.51,492.91,12.86,9.46" target="#b4">[5]</ref> to convolutional <ref type="bibr" coords="1,419.12,492.91,11.94,9.46" target="#b0">[1,</ref><ref type="bibr" coords="1,434.24,492.91,9.16,9.46" target="#b5">6]</ref> and recurrent neural networks <ref type="bibr" coords="1,160.69,506.46,11.70,9.46" target="#b6">[7,</ref><ref type="bibr" coords="1,175.12,506.46,7.80,9.46" target="#b7">8]</ref>. However, prior work in AV has not yet made extensive use of transformer architectures or pretrained language models. 1  In this work, we apply the pretrained BERT model in a Siamese configuration for the task of AV <ref type="bibr" coords="1,106.29,547.11,16.88,9.46" target="#b11">[12]</ref>. We make use of WordPiece <ref type="bibr" coords="1,256.23,547.11,18.32,9.46" target="#b12">[13]</ref> for tokenization and do not use engineered features. We set out to determine how well modern methods perform on AV, and the feasibility of removing hand-engineered features in favor of deeper models and prior knowledge in the form of pretraining. Furthermore, triplet loss has provided benefits in image processing <ref type="bibr" coords="2,391.72,353.21,16.88,9.46" target="#b13">[14]</ref>, but has not yet been leveraged for AV. We experiment with triplet loss (leveraging multiple sampling strategies), contrastive loss, and a modified version of contrastive loss that has proved beneficial in a previous AV study <ref type="bibr" coords="2,132.57,393.86,16.78,9.46" target="#b14">[15]</ref>. The dataset for this task was obtained from fanfiction.net, where each datapoint consists of pairs of text from two different fanfics (an amateur fictional writing based on an existing work of fiction) <ref type="bibr" coords="2,197.50,420.95,16.72,9.46" target="#b11">[12]</ref>. More on this dataset in Section 2.1.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Siamese BERT for Authorship Verification</head><p>We introduce Siamese BERT for Authorship Verification (SAV) <ref type="foot" coords="2,367.49,478.86,3.99,6.91" target="#foot_0">2</ref> . Our method uses a pretrained BERT model in a Siamese setup as shown in Figure <ref type="figure" coords="2,320.13,494.46,5.51,9.46" target="#fig_0">1</ref> and originally introduced by <ref type="bibr" coords="2,456.21,494.46,16.80,9.46" target="#b15">[16]</ref>. In the AV task, we are given two input texts ùë• 1 and ùë• 2 and the expected output is a score in the interval [0, 1] indicating the likelihood with which they belong to the same author. The maximum input size for the BERT model is 512 tokens, therefore we truncate each text to the first 512 tokens. Separately, for both input texts, they are passed through the BERT model resulting in an output of size ùëõ √ó 768 (where ùëõ is the number of tokens in the input and 768 is the dimension of the BERT output for each token). All ùëõ representations are then averaged into a 768 dimensional vector (the mean pooling layer), and then passed through a fully connected layer to generate the final text embedding (256 dimensional). This gives the final output representation ùë¢ and ùë£ of input texts ùë• 1 and ùë• 2 , respectively. These representations are then compared using a distance metric, which is then used for loss calculation and model optimization.</p><p>During inference, the same procedure is followed. The only difference is that after the distance between embeddings ùë¢ and ùë£ are calculated, it is compared to a threshold. If the corresponding distance is smaller than the threshold, the texts are predicted to have been written by the same author and vice versa. In Section 4 we discuss more detail on finding the thresholds, as well as an alternative approach to truncating each input to 512 tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Data Preprocessing</head><p>The PAN 2021 AV challenge provided two datasets, both obtained from fanfiction.net. Each datapoint consists of a pair of texts from two different fanfics, as well as a tag representing which fandom (the particular fictional series) each text is from. We leverage only the large dataset in this work, which contains 275, 565 text pairs. Roughly 54% of these pairs were written by the same author (i.e. a same-author pair). Approximately 8% of the pairs were texts from the same fandom, but none of the same-author pairs contained texts from the same fandom. In total, the texts were pulled from 1, 600 fandoms and over 278, 000 authors. Instead of using these predefined pairs for training, we elected to split all pairs and store all texts individually. However, we don't want to change the data distribution for the test set. Therefore, we sample 10% of the pairs randomly to form the test set. We then ensure that all authors found in this test set have no texts in the training set. If so, the text pair is moved to the test set. We form a secondary test set by splitting all of the test pairs, and then recombining them randomly (based on author, using the same procedure as is used during training). We will refer to this set as the modified test set, as it has the same data distribution as the training set we have created but not as the original data. During training, we randomly sample text pairs (at roughly 50% same-author 50% different author pair rates). Although this changes the data distribution, it allows us to leverage a much larger set of text pairs (‚âà76 billion possible pairs vs ‚âà275 thousand).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Loss Functions</head><p>Any Siamese model can be trained using a wide range of loss functions. In this work, we explored training our model with the contrastive, modified contrastive, and triplet loss functions.</p><p>The contrastive loss is</p><formula xml:id="formula_0" coords="3,154.49,526.57,351.49,24.43">‚Ñí c (ùë¢, ùë£, ùë¶) = 1 2 (Ô∏Å ùë¶ d(ùë¢, ùë£) 2 + (1 -ùë¶) max{(ùëö -d(ùë¢, ùë£)) 2 , 0} )Ô∏Å ,<label>(1)</label></formula><p>where ùë¢ and ùë£ are text embeddings, ùë¶ ‚àà {0, 1} is the label (1 if ùë¢ and ùë£ were written by the same author, 0 otherwise), d is the distance metric, and ùëö is a margin (no loss is incurred for a different-author pair if their representations are further apart than ùëö).</p><p>The modified contrastive loss, originally introduced by <ref type="bibr" coords="3,342.17,602.23,16.72,9.46" target="#b14">[15]</ref>, is</p><formula xml:id="formula_1" coords="3,104.40,622.89,401.59,24.43">‚Ñí mc (ùë¢, ùë£, ùë¶) = 1 2 (Ô∏Å ùë¶ max{(d(ùë¢, ùë£) -ùëö ùë† ) 2 , 0} + (1 -ùë¶) max{(ùëö ùëë -d(ùë¢, ùë£)) 2 , 0} )Ô∏Å .<label>(2)</label></formula><p>The modification from the aforementioned contrastive loss is that there are now two margins. ùëö ùë† refers to a margin for same-author pairs. If the distance between the embeddings of a same-author pair is smaller than ùëö ùë† , then no loss is incurred. In normal contrastive loss, loss is incurred unless the pair of texts evaluate to identical representations. This modified contrastive loss allows for some variation among the texts of a single author, which should help account for differences in a single author's text such as topic differences, and therefore make the resulting model more robust to non-stylistic difference among authors. The second margin ùëö ùëë refers to a margin for different-author pairs, and performs the same function as ùëö in the original contrastive loss.</p><p>The triplet loss function is</p><formula xml:id="formula_2" coords="4,196.06,193.50,309.92,10.81">‚Ñí t (ùëé, ùëù, ùëõ) = max{ùëë(ùëé, ùëù) -ùëë(ùëé, ùëõ) + ùëö, 0},<label>(3)</label></formula><p>where ùëé represents the embedding of an anchor text, ùëù represents the embedding of a different text than ùëé but from the same author (positive pair), and ùëõ represents the embedding of a text from an author different than that of ùëé (negative pair). ùëö is the margin to separate the positive and negative pairs by (i.e. the negative sample should be further from the anchor than the positive sample by at least ùëö). Note that the triplet loss does not explicitly push same-author pairs together, but instead only forces different-author pairs to be farther apart than same-author pairs. With the contrastive and modified contrastive loss functions, we sample pairs of texts randomly. With triplet loss, it is common to use different sampling techniques. Hermans et al. <ref type="bibr" coords="4,375.15,313.20,18.17,9.46" target="#b13">[14]</ref> describe an efficient way of performing hard negative mining. Given a random batch of samples, the loss is computed (according to the triplet loss function) for all possible, valid triplets. Then the hardest positive and the hardest negative (i.e. the positive that is furthest from the anchor and the negative that is closest to the anchor) are selected, and the loss with respect to these samples is used for updating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3.">Distance Metrics</head><p>We test with the cosine (d cos ) and Euclidean (d euc ) distance measures:</p><formula xml:id="formula_3" coords="4,239.19,436.22,266.79,41.87">d cos (ùë¢, ùë£) = 1 - ùë¢ ‚Ä¢ ùë£ ||ùë¢|| ||ùë£|| (4) d euc (ùë¢, ùë£) = ||ùë¢ -ùë£|| 2<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4.">Resources</head><p>The final model was trained for 3 days on 8 Tesla v100's. This allowed for 16 samples per GPU, for a total batch size of 128. We used the standard learning rate for the hugging face transformer pretrained models (5 √ó 10 -5 ) and anneal it over 4 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Evaluation</head><p>We use the evaluation metrics described in <ref type="bibr" coords="4,274.26,605.10,16.58,9.46" target="#b16">[17]</ref>, as well as the baseline model provided as part of the AV task <ref type="foot" coords="4,139.93,616.60,3.99,6.91" target="#foot_1">3</ref> [18]:</p><p>‚Ä¢ AUC: the conventional area-under-the-curve of the precision-recall curve ‚Ä¢ F1-score: the harmonic mean of the precision and recall <ref type="bibr" coords="5,363.52,102.89,18.17,9.46" target="#b18">[19]</ref> ‚Ä¢ c@1: a variant of the conventional F1-score, which rewards systems that leave difficult problems unanswered (i.e. scores of exactly 0.5) [20] ‚Ä¢ F_0.5u: a newly proposed measure that puts more emphasis on deciding same-author cases correctly <ref type="bibr" coords="5,158.06,159.68,18.17,9.46" target="#b20">[21]</ref> ‚Ä¢ overall: the simple average of all previous metrics For hyperparameter selection, we predefined 17 models that differ in terms of their loss function, distance metric, and margin(s). Each of these models is trained for 3 days on a single Tesla V100 GPU. Table <ref type="table" coords="5,168.93,223.97,5.39,9.46">2</ref> details the performance of each of these models with respect to the modified testing set. The highest performing model with respect to the overall score is one that leverages the modified contrastive loss along with the Euclidean distance metric and an upper and lower margin of 5 and 0.25 respectively. We choose this hyperparameter combination for our final model, which was then evaluated on a hidden test set via the TIRA environment <ref type="bibr" coords="5,448.97,278.17,16.88,9.46" target="#b21">[22]</ref>. Table <ref type="table" coords="5,501.24,278.17,5.56,9.46">1</ref> shows the performance of our model on this hidden test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>The performance of our final model on the hidden test set, evaluated on the TIRA environment for the PAN21 competition <ref type="bibr" coords="5,206.85,342.92,16.62,9.22" target="#b17">[18]</ref> Model AUC F1 c@1 F_0.5u Brier Overall Final Model 0.8275 0.7911 0.7594 0.7257 0.8123 0.7832</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Analysis of the final model</head><p>The final model was trained for 3 days on 8 Tesla V100 GPU's, equating to roughly 4 epochs. All analysis and results are with respect to the test set, not the modified test set as in previous sections. We first examine the score distribution, shown in Figure <ref type="figure" coords="5,377.06,470.84,8.67,9.46" target="#fig_1">2a</ref>. The same author pairs are represented by the blue histogram, and different author pairs are represented by the yellow. There is still significant overlap in the scores of the two different groups.</p><p>We optimize our thresholds on the test set via grid search, which is visualized in Figure <ref type="figure" coords="5,472.70,511.49,8.91,9.46" target="#fig_1">2b</ref>. The z-axis represents the overall performance of the final model as the thresholds are varied (x and y axes). The optimal thresholds are 0.470 and 0.553 respectively, giving an overall performance on the test set of 0.701. The other performance metrics, along with the performance of the baseline on test set is shown in Table <ref type="table" coords="5,214.51,565.68,5.45,9.46" target="#tab_1">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Adding more context</head><p>Our model leverages only the first 512 tokens of each text (the maximum tokens that will fit in a single pass through the BERT model). Here, using the final model, we investigate chunking each text longer than 512 tokens into sets, and then combining the final representation of each chunk before passing the text representation to the distance metric. We combine the representations of each individual chunk via averaging, resulting in a fixed-length vector that encodes information</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>The performance of all models during the hyperparameter search. All models were trained for 3 days on a single Tesla V100 GPU, and were evaluated on the modified test set. The "Final Model" entry details the performance of the best performing model on the modified test set after completing the final training phase <ref type="bibr" coords="6,245.96,137.95,4.52,9.22">(</ref> from the entire input text regardless of length. The score distribution for the final model using chunking is shown in Figure <ref type="figure" coords="6,222.22,467.33,4.17,9.46" target="#fig_2">3</ref>. We compare these two score distributions by looking at the percentage of overlap, and find that both the chunking and non-chunking procedures produce roughly 55% overlap. Furthermore, this chunking behavior results in slightly worse overall performance on the test set, 0.701 vs 0.715 of the final model without chunking. Lastly, chunking is computationally expensive. During inference, on an 8-core CPU, the final model takes about 1.5 seconds to process an input pair. On the same machine, the chunking model takes about 9.8 seconds to process that same input pair. Because of this large cost increase along with the indistinguishable performance, we use only the non-chunking model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we construct a Siamese network initialized with pretrained BERT encoders, employing a learning objective that incentives the model to map texts written by the same author to nearby embeddings while mapping texts written by different authors to comparatively distant embeddings. Our method results in a slight performance gain over a baseline of 0.9% overall score, and an increase of 8% in F1 score. We explore the effectiveness of different loss functions, distance metrics, and margins and our results indicate the need for either hand engineered features or more training time and data. This work represents the first steps in understanding the ability of modern language models and tokenizers to perform authorship verification, without any of the common hand engineered features. Some interesting future work includes broadening the training data (incorporating many AV datasets during training) and lengthening the training time, further investigating sampling strategies (we expect approaches such as hard-negative mining to provide improvements vs random sampling), and explore different methods of embedding text longer than the input size of the BERT model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,304.19,416.69,9.35;2,89.29,316.24,41.89,9.22;2,193.47,84.19,208.35,212.76"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The Siamese BERT for Authorship Verification (SAV) model structure n the givand data flow.</figDesc><graphic coords="2,193.47,84.19,208.35,212.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,89.29,301.62,416.69,9.35;7,88.96,313.68,380.41,9.22;7,313.72,94.15,187.50,185.42"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) The score distribution of the final model on the test set. (b) The overall performance (z-axis) with respect to the lower and upper thresholds (x-axis and y-axis respectively).</figDesc><graphic coords="7,313.72,94.15,187.50,185.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,89.29,649.21,383.24,9.35;7,193.47,488.15,208.35,148.90"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The score distribution of the final model on the test set when using chunking.</figDesc><graphic coords="7,193.47,488.15,208.35,148.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,89.29,137.95,416.70,290.59"><head></head><label></label><figDesc>3 days on 8 Tesla V100's -i.e. 8x larger batch size for the same number of training iterations).</figDesc><table coords="6,103.26,165.47,388.75,263.06"><row><cell>Loss Function</cell><cell>Distance Metric</cell><cell>Upper Margin</cell><cell>Lower Margin</cell><cell>AUC</cell><cell>F1</cell><cell cols="2">c@1 F_0.5u Overall</cell></row><row><cell></cell><cell></cell><cell>1000</cell><cell>-</cell><cell cols="3">0.552 0.206 0.522</cell><cell>0.341</cell><cell>0.405</cell></row><row><cell>Triplet</cell><cell>Euclidean</cell><cell>100</cell><cell>-</cell><cell cols="3">0.561 0.361 0.537</cell><cell>0.468</cell><cell>0.482</cell></row><row><cell></cell><cell></cell><cell>10</cell><cell>-</cell><cell cols="3">0.529 0.457 0.519</cell><cell>0.495</cell><cell>0.500</cell></row><row><cell></cell><cell></cell><cell>0.01</cell><cell>-</cell><cell cols="3">0.878 0.692 0.556</cell><cell>0.584</cell><cell>0.678</cell></row><row><cell>Contrastive</cell><cell>Cosine</cell><cell>0.1 0.5</cell><cell>--</cell><cell cols="3">0.904 0.696 0.563 0.874 0.795 0.773</cell><cell>0.588 0.751</cell><cell>0.688 0.798</cell></row><row><cell></cell><cell></cell><cell>0.9</cell><cell>-</cell><cell cols="3">0.826 0.646 0.712</cell><cell>0.749</cell><cell>0.733</cell></row><row><cell></cell><cell></cell><cell>0.1</cell><cell>-</cell><cell cols="3">0.839 0.765 0.738</cell><cell>0.72</cell><cell>0.766</cell></row><row><cell cols="2">Contrastive Euclidean</cell><cell>1</cell><cell>-</cell><cell cols="3">0.805 0.737 0.722</cell><cell>0.712</cell><cell>0.744</cell></row><row><cell></cell><cell></cell><cell>10</cell><cell>-</cell><cell cols="3">0.782 0.663 0.699</cell><cell>0.715</cell><cell>0.715</cell></row><row><cell></cell><cell></cell><cell>0.5</cell><cell>2.5</cell><cell cols="3">0.813 0.743 0.723</cell><cell>0.712</cell><cell>0.748</cell></row><row><cell></cell><cell></cell><cell>0.25</cell><cell>5</cell><cell cols="3">0.871 0.809 0.776</cell><cell>0.757</cell><cell>0.803</cell></row><row><cell>Modified Contrastive</cell><cell>Euclidean</cell><cell>0.5 0.25</cell><cell>5 1</cell><cell cols="3">0.789 0.722 0.705 0.881 0.789 0.754</cell><cell>0.698 0.688</cell><cell>0.729 0.778</cell></row><row><cell></cell><cell></cell><cell>0.1</cell><cell>1</cell><cell cols="3">0.858 0.778 0.756</cell><cell>0.713</cell><cell>0.776</cell></row><row><cell></cell><cell></cell><cell>0.75</cell><cell>1</cell><cell cols="3">0.815 0.627 0.704</cell><cell>0.698</cell><cell>0.711</cell></row><row><cell>Modified Contrastive</cell><cell>Cosine</cell><cell>0.1</cell><cell>0.5</cell><cell cols="3">0.811 0.719 0.729</cell><cell>0.735</cell><cell>0.749</cell></row><row><cell>Baseline</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">0.831 0.769 0.78</cell><cell>0.764</cell><cell>0.786</cell></row><row><cell>Final Model</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">0.892 0.811 0.796</cell><cell>0.777</cell><cell>0.819</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,88.98,597.40,332.78,61.03"><head>Table 3</head><label>3</label><figDesc>The performance for the baseline and final model on the test set.</figDesc><table coords="6,173.51,624.90,248.25,33.52"><row><cell>Model</cell><cell>AUC</cell><cell>F1</cell><cell cols="2">c@1 F_0.5u Overall</cell></row><row><cell>Baseline</cell><cell cols="3">0.779 0.659 0.759</cell><cell>0.628</cell><cell>0.706</cell></row><row><cell cols="4">Final Model 0.780 0.739 0.731</cell><cell>0.611</cell><cell>0.715</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,108.93,671.91,306.25,7.77"><p>All code for this model can be found here: https://github.com/JacobTyo/PAN21_SAV</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="4,108.93,649.99,397.05,7.77;4,89.01,660.94,416.97,7.77;4,89.29,671.90,335.57,7.77"><p>As described in<ref type="bibr" coords="4,168.08,649.99,13.87,7.77" target="#b16">[17]</ref>, the provided baseline is a simple method that calculates the cosine similarities between TF-IDF-normalized, bag-of-character-tetragrams representations of the texts in a pair. The resulting scores are then shifted using a simple grid search, to arrive at an optimal performance on the calibration data.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,112.92,215.75,393.07,9.46;8,112.92,229.30,387.03,9.46" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="8,268.09,215.75,237.90,9.46;8,112.92,229.30,200.59,9.46">Character-level and multi-channel convolutional neural networks for large-scale authorship attribution</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ghaffari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Breslin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.06686</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,112.92,242.85,363.31,9.46;8,112.92,256.39,283.48,9.46" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Boenninghoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kolossa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.10105</idno>
		<title level="m" coord="8,347.29,242.85,128.94,9.46;8,112.92,256.39,97.02,9.46">Deep bayes factor scoring for authorship verification</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,112.92,269.94,362.85,9.46;8,112.92,283.49,335.54,9.46" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Jafariakinabad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">A</forename><surname>Hua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.06786</idno>
		<title level="m" coord="8,244.02,269.94,231.75,9.46;8,112.92,283.49,149.10,9.46">A self-supervised representation learning of sentence structure for authorship attribution</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,112.92,297.04,358.79,9.46;8,112.92,310.59,207.25,9.46" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,155.29,297.04,255.38,9.46">Authorship attribution based on a probabilistic topic model</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,419.60,297.04,52.11,9.46;8,112.92,310.59,118.17,9.46">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="341" to="354" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.92,324.14,375.45,9.46;8,112.92,337.69,357.95,9.46" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,458.06,324.14,30.31,9.46;8,112.92,337.69,208.18,9.46">Cic-gil approach to cross-domain authorship attribution</title>
		<author>
			<persName coords=""><forename type="first">C. Mart√≠n</forename><surname>Del Campo-Rodr√≠guez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>G√≥mez-Adorno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sidorov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Batyrshin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,329.03,337.69,107.30,9.46">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.92,351.24,374.63,9.46;8,112.92,364.79,366.48,9.46;8,112.92,378.34,379.34,9.46;8,112.53,391.89,192.67,9.46" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,425.77,351.24,61.78,9.46;8,112.92,364.79,240.47,9.46">Convolutional neural networks for authorship attribution of short texts</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sierra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">A</forename><surname>Gonz√°lez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Solorio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,376.08,364.79,103.32,9.46;8,112.92,378.34,375.05,9.46">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<title level="s" coord="8,159.91,391.89,53.49,9.46">Short Papers</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="669" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.92,405.44,377.72,9.46;8,112.92,418.99,151.22,9.46" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="8,166.84,405.44,290.56,9.46">Author identification using multi-headed recurrent neural networks</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bagnall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04891</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,112.92,432.53,363.98,9.46;8,112.92,446.08,279.20,9.46" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="8,310.40,432.53,166.51,9.46;8,112.92,446.08,92.76,9.46">Syntactic recurrent neural network for authorship attribution</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Jafariakinabad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tarnpradab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">A</forename><surname>Hua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09723</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,112.92,459.63,390.34,9.46;8,112.92,473.18,370.90,9.46;8,112.92,486.73,183.67,9.46" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,226.23,459.63,277.03,9.46;8,112.92,473.18,29.35,9.46">Cross-domain authorship attribution using pre-trained language models</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Barlas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,165.99,473.18,317.83,9.46;8,112.92,486.73,49.32,9.46">IFIP International Conference on Artificial Intelligence Applications and Innovations</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="255" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.92,500.28,361.30,9.46;8,112.92,513.83,379.46,9.46;8,112.92,527.38,183.52,9.46" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,354.49,500.28,119.73,9.46;8,112.92,513.83,92.76,9.46">Bertaa: Bert fine-tuning for authorship attribution</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fabien</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Villatoro-Tello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Parida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,228.50,513.83,263.88,9.46;8,112.92,527.38,150.22,9.46">Proceedings of the 17th International Conference on Natural Language Processing, CONF, ACL</title>
		<meeting>the 17th International Conference on Natural Language Processing, CONF, ACL</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.92,540.93,387.98,9.46;8,112.92,554.48,308.53,9.46" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,310.93,540.93,189.97,9.46;8,112.92,554.48,43.97,9.46">Language models and fusion for authorship attribution</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Fourkioti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Symeonidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,164.81,554.48,173.01,9.46">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page">102061</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.92,568.03,394.30,9.46;8,112.92,581.58,368.54,9.46;8,112.92,595.12,391.92,9.46;8,112.92,608.67,393.06,9.46;8,112.92,622.22,242.59,9.46" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,228.98,595.12,275.86,9.46;8,112.92,608.67,242.16,9.46">Overview of PAN 2021: Authorship Verification,Profiling Hate Speech Spreaders on Twitter,and Style Change Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chulvi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">L D L P</forename><surname>Sarrac√©n</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Manjavacas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Markov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mayerl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wolska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Zangerle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,377.83,608.67,128.15,9.46;8,112.92,622.22,168.03,9.46">12th International Conference of the CLEF Association (CLEF 2021)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.92,635.77,376.19,9.46;8,112.92,649.32,392.95,9.46;8,112.92,662.87,360.89,9.46" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="8,238.06,649.32,267.81,9.46;8,112.92,662.87,174.39,9.46">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.92,88.05,389.31,9.46;9,112.92,101.60,178.52,9.46" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m" coord="9,258.34,88.05,239.87,9.46">In defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.92,115.14,384.52,9.46;9,112.65,128.69,378.32,9.46;9,112.53,142.24,356.04,9.46" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,351.09,115.14,146.36,9.46;9,112.65,128.69,116.14,9.46">Similarity learning for authorship verification in social media</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Boenninghoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kolossa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,252.51,128.69,238.45,9.46;9,112.53,142.24,223.56,9.46">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2457" to="2461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.92,155.79,338.49,9.46;9,112.92,169.34,387.74,9.46;9,112.92,182.89,341.80,9.46;9,112.92,196.44,143.98,9.46" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,226.55,155.79,224.86,9.46;9,112.92,169.34,58.69,9.46">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1908.10084" />
	</analytic>
	<monogr>
		<title level="m" coord="9,194.96,169.34,305.71,9.46;9,112.92,182.89,282.68,9.46">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.92,209.99,379.61,9.46;9,112.92,223.54,379.96,9.46;9,112.92,237.09,99.17,9.46" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="9,210.50,223.54,282.38,9.46">Overview of the cross-domain authorship verification task at pan</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Manjavacas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Markov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.92,250.64,360.02,9.46;9,112.92,264.19,372.84,9.46;9,112.41,277.74,230.56,9.46" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="9,112.92,264.19,233.11,9.46">Overview of the Authorship Verification Task at PAN</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Manjavacas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="9,392.13,264.19,93.63,9.46;9,112.41,277.74,46.39,9.46">CLEF 2021 Labs and Workshops</title>
		<title level="s" coord="9,166.68,277.74,103.09,9.46">Notebook Papers, CEUR</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.92,291.28,383.27,9.46;9,112.92,304.83,392.97,9.46;9,112.71,318.38,264.79,9.46" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="9,308.90,304.83,175.14,9.46">Scikit-learn: Machine learning in python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,492.64,304.83,13.25,9.46;9,112.71,318.38,164.79,9.46">the Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.92,331.93,314.79,9.46" xml:id="b19">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pe√±as</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rodrigo</surname></persName>
		</author>
		<title level="m" coord="9,212.90,331.93,180.27,9.46">A simple measure to assess non-response</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.92,345.48,394.58,9.46;9,112.92,359.03,393.20,9.46;9,112.92,372.58,382.70,9.46;9,112.92,386.13,122.55,9.46" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="9,321.36,345.48,167.04,9.46">Generalizing unmasking for short texts</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,112.92,359.03,393.20,9.46;9,112.92,372.58,259.14,9.46">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="654" to="659" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="9,112.92,399.68,394.58,9.46;9,112.92,413.23,376.44,9.46;9,112.92,426.78,326.99,9.46;9,112.92,440.33,204.97,9.46" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="9,321.04,399.68,167.07,9.46">TIRA Integrated Research Architecture</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-22948-1_5</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,229.27,413.23,260.09,9.46;9,112.92,426.78,122.20,9.46">Information Retrieval Evaluation in a Changing World, The Information Retrieval Series</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
