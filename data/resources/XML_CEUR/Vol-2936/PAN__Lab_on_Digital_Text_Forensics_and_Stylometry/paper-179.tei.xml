<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.00,77.76,374.05,14.99">Authorship Verification based on Lucene architecture</title>
				<funder ref="#_eMV5JsK">
					<orgName type="full">Social Science Foundation of Guangdong Province</orgName>
				</funder>
				<funder ref="#_WJwxREg #_yVAMSqP">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.00,108.66,62.73,10.54"><forename type="first">Zhihao</forename><surname>Liao</surname></persName>
							<email>liaozhihaoqwq@outlook.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Foshan University</orgName>
								<address>
									<settlement>Foshan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,148.08,108.66,51.46,10.54"><forename type="first">Yong</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Foshan University</orgName>
								<address>
									<settlement>Foshan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,215.73,108.66,60.65,10.54"><forename type="first">Leilei</forename><surname>Kong</surname></persName>
							<email>kongleilei@fosu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Foshan University</orgName>
								<address>
									<settlement>Foshan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,288.98,108.66,81.32,10.54"><forename type="first">Zhuopeng</forename><surname>Hong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Foshan University</orgName>
								<address>
									<settlement>Foshan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,382.90,108.66,45.32,10.54"><forename type="first">Zijian</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Foshan University</orgName>
								<address>
									<settlement>Foshan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,440.82,108.66,75.32,10.54"><forename type="first">Guiyuan</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Foshan University</orgName>
								<address>
									<settlement>Foshan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,72.00,122.46,61.65,10.54"><forename type="first">Zhenwei</forename><surname>Mo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Foshan University</orgName>
								<address>
									<settlement>Foshan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,143.25,122.46,50.99,10.54"><forename type="first">Zhixian</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Foshan University</orgName>
								<address>
									<settlement>Foshan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,203.84,122.46,77.65,10.54"><forename type="first">Zhongyuan</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Foshan University</orgName>
								<address>
									<settlement>Foshan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.00,77.76,374.05,14.99">Authorship Verification based on Lucene architecture</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">979B3BB7A34A99267DE392467C1E4222</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Lucene</term>
					<term>indexing</term>
					<term>Retrieval 1</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Authorship verification is the task of deciding whether two texts have been written by the same author based on comparing the texts' writing styles. We regard this task as a retrieval problem and a model based on information retrieval is proposed for verifying the authorship. The vector space model is used to estimate the simiarity between documents. To consider the different features of documents, we build four kinds of indexes for each document. Then a weighted score is computed to decide whether two documents come from the same author. Using this simple-minded approach, we get achieved 0.3032 on the overall score.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper proposes an implementation method of author authentication shared task in PAN 2021. The goal of the task was to create a way to predict whether two given documents were written by the same person <ref type="bibr" coords="1,130.89,398.99,11.68,9.66" target="#b1">[2]</ref>. In the task, we used vector space modal to build indexes focused on different views. The vector space model is implemented by Lucene. In addition, we consider the different characteristics of the document, such as the influence of stoppage words on the discrimination, the influence of capitalization on the discrimination, the influence of nouns on the discrimination, and the influence of adjectives on the discrimination. Different characteristics are used to judge the similarity of the text. Then we retrieve the documents in the test set, and we get a ranking list of scores for each document. Because each document is indexed from four views, there are four ranking lists of scores for each document. We weighted the scores for each document to get a new ranking list of scores. Rank the new list of scores from highest to lowest. When we determine whether two papers were written by the same person, we compare whether the first place in the weighted score ranking list of the two papers is the same. If they are the same, they are written by the same person, otherwise they are not written by the same person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Datasets</head><p>The data set provided for this task is from the English documentation of fanfiction.net. Each line in the training set gives two passages, which may be written by the same author or by different authors. In the Truth file, each line records whether the two documents were written by the same author. In this paper, I only used the Small training set, which has 52601 rows of data and a total of 52655 authors. In the test set provided by the task, the authors included in the training set did not appear, which made the task more difficult. The test set is given by the authorship verification shared task at PAN21.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>This document describes the model we built to verify authorship. In the pre-processing, we do four kinds of pre-processing for each document, which are converted to lowercase letters, deleted stop words, retained nouns, and retained adjectives. Then we set up four indexes for the processed documents. We retrieve documents for the test set. A ranking list of scores retrieved by weighted summation. When determining whether two documents were written by the same person, if their highest score in the weighted ranking list is the same, then they were written by the same author; otherwise they are not.</p><p>When comparing whether document1 and document2 were written by the same author, we retrieved document1 and document2 respectively. For document1 and document2, we can get four score ranking lists respectively. We weight each of the four score ranking lists and then add them together to get a final score ranking list. Finally, we compare the final score ranking list for document1 and document2. If the author at the top of the final ranking list for document1 and document2 is the same, we assume that document1 and document2 were written by the same person; otherwise, they were not by the same person. For all the documents in the training set, we know which author wrote the document, so we first classified the documents in the training set according to their authors before establishing the index. For example, if text1 was written by author1, we would write text1 to file 1. If text2 is also written by author1, text2 will also be written to file 1. If text3 is written by author2, text3 will be written to file 3.</p><p>After we have classified the document, we can build the index. We first use the Lucene framework to index documents converted to lowercase letters <ref type="bibr" coords="2,305.02,498.60,11.68,9.66" target="#b0">[1]</ref>. Similarly, we index documents that remove stops, documents that retain nouns, and documents that retain adjectives.</p><p>After retrieving all the documents in the test set with each of the four indexes, we can get four score ranking lists for each document. We weighted the score ranking list of the four groups of authors to get the final score ranking list. In the final score ranking list, if the highest score for document1 and document2 corresponds to author1, then we can assume that both document1 and document2 were written by author1 and that the value we write in the result file is 1. If not written by the same author, the result file has a value of 0.</p><p>The final scoring formula can be expressed as:</p><formula xml:id="formula_0" coords="2,86.25,612.68,420.50,45.47">𝑓𝑖𝑛𝑎𝑙_𝑠𝑐𝑜𝑟𝑒(𝑎𝑢𝑡ℎ𝑜𝑟) = 𝑠𝑐𝑜𝑟𝑒_𝑙𝑜𝑤𝑒𝑟(𝑎𝑢𝑡ℎ𝑜𝑟) * ω 1 + 𝑠𝑐𝑜𝑟𝑒_𝑛𝑜𝑠𝑡𝑜𝑝𝑤𝑜𝑟𝑑(𝑎𝑢𝑡ℎ𝑜𝑟) * ω 2 + 𝑠𝑐𝑜𝑟𝑒_𝑛𝑜𝑟𝑛(𝑎𝑢𝑡ℎ𝑜𝑟) * ω 3 𝑠𝑐𝑜𝑟𝑒_𝑎𝑑𝑗(𝑎𝑢𝑡ℎ𝑜𝑟) * ω 4 (1)</formula><p>The final_score(author) in the formula represents the probability score that an document was written by an author. Score_lower(author) represents the probability score that the lowercase model considers to be written by the author after an document is converted to lowercase. Score_nostopword(author) represents the probability score that the delete stop model thinks was written by the author after the stopword is deleted from an document. Score_norn(author) represents the probability score that the retention noun model considers to be written by the author after an document has retained a noun. Score_adj(author) represents the probability score that the retained adjective model considers to be written by the author after an document has retained an adjective.</p><p>, , and are the weights set by the four models respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>We deployed these program into the TIRA evaluation system provided by the PAN 2020 organizer, where the models were evaluated with unpublished data [6]. The system will compare and rank based on five supplementary indicators, namely AUC, F1-score, C@1, F_0.5U and BRIER.</p><p>Here are the results when we use the training set.  Here are the results released by PAN on the test set <ref type="bibr" coords="3,313.25,414.80,11.68,9.66" target="#b1">[2]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Authorship verification is the task of deciding whether two texts have been written by the same author based on comparing the texts' writing styles. In order to complete the task of author authentication, this paper proposes to use vector space modal to establish indexes to classify author attribution. The vector space model is implemented by Lucene. To consider the different features of documents, we build four kinds of indexes for each document. This method has been experimented on the test set provided by the Authors Verification Shared Task of PAN21, and the result is F1 = 0.0067, auc = 0.4962, c@1 = 0.4962, f_05_u = 0.0161, Brier = 0.4962, overall = 0.3032.</p><p>In addition, we can see from the result, we just choose the characteristics of the nouns and adjectives as a writer is not very accurate, the follow-up work, we should use a more effective method to extract the characteristics of each document, compare the characteristics of the two documents are similar, so as to determine whether two documents written by the same authors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,72.00,409.69,194.45,10.03;2,86.25,422.71,436.44,9.66;2,72.00,435.36,450.74,9.66;2,72.00,448.00,450.98,9.66;2,72.00,460.65,451.15,9.66;2,72.00,473.30,8.25,9.66;2,86.25,485.95,436.75,9.66;2,72.00,498.60,450.58,9.66;2,72.00,511.25,319.92,9.66;2,86.25,523.90,436.84,9.66;2,72.00,536.55,450.69,9.66;2,72.00,549.20,450.53,9.66;2,72.00,561.85,450.85,9.66;2,72.00,574.49,450.93,9.66;2,72.00,587.14,132.47,9.66;2,86.25,599.79,204.81,9.66;2,86.25,612.68,99.25,9.78;2,155.19,625.88,128.13,9.78;2,283.31,633.42,4.31,6.91;2,290.91,625.88,159.13,9.78;2,73.50,297.41,450.00,106.50"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Retrieval result judgment diagramFor all the documents in the training set, we know which author wrote the document, so we first classified the documents in the training set according to their authors before establishing the index. For example, if text1 was written by author1, we would write text1 to file 1. If text2 is also written by author1, text2 will also be written to file 1. If text3 is written by author2, text3 will be written to file 3.After we have classified the document, we can build the index. We first use the Lucene framework to index documents converted to lowercase letters<ref type="bibr" coords="2,305.02,498.60,11.68,9.66" target="#b0">[1]</ref>. Similarly, we index documents that remove stops, documents that retain nouns, and documents that retain adjectives.After retrieving all the documents in the test set with each of the four indexes, we can get four score ranking lists for each document. We weighted the score ranking list of the four groups of authors to get the final score ranking list. In the final score ranking list, if the highest score for document1 and document2 corresponds to author1, then we can assume that both document1 and document2 were written by author1 and that the value we write in the result file is 1. If not written by the same author, the result file has a value of 0.The final scoring formula can be expressed as: 𝑓𝑖𝑛𝑎𝑙_𝑠𝑐𝑜𝑟𝑒(𝑎𝑢𝑡ℎ𝑜𝑟) = 𝑠𝑐𝑜𝑟𝑒_𝑙𝑜𝑤𝑒𝑟(𝑎𝑢𝑡ℎ𝑜𝑟) * ω 1 + 𝑠𝑐𝑜𝑟𝑒_𝑛𝑜𝑠𝑡𝑜𝑝𝑤𝑜𝑟𝑑(𝑎𝑢𝑡ℎ𝑜𝑟) * ω</figDesc><graphic coords="2,73.50,297.41,450.00,106.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,117.00,349.97,2.75,9.66;3,167.50,349.97,2.75,9.66;3,86.25,364.21,436.43,9.66;3,72.00,376.85,451.16,9.66;3,72.00,389.50,451.17,9.66;3,72.00,402.15,161.46,9.66"><head></head><label></label><figDesc>, . Baseline is a simple method based on text compression that given a pair of texts calculates the cross-entropy of text2 using the Prediction by Partial Matching model of text1 and vice-versa. Then, the mean and absolute difference of the two cross-entropies are used by a logistic regression model to estimate a verification score in [0,1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,72.00,221.92,450.99,114.25"><head>Table 1</head><label>1</label><figDesc>Results of training set experiments</figDesc><table coords="3,83.25,250.28,439.74,85.89"><row><cell>Method</cell><cell>F1</cell><cell></cell><cell>AUC</cell><cell></cell><cell>c@1</cell><cell>f_05_u</cell><cell>overall</cell></row><row><cell>baseline</cell><cell>0.785</cell><cell></cell><cell>0.808</cell><cell></cell><cell>0.743</cell><cell>0.71</cell><cell>0.762</cell></row><row><cell>M1-train</cell><cell>0.884</cell><cell></cell><cell>0.878</cell><cell></cell><cell>0.943</cell><cell>0.869</cell><cell>0.894</cell></row><row><cell>M2-train</cell><cell>0.885</cell><cell></cell><cell>0.897</cell><cell></cell><cell>0.891</cell><cell>0.951</cell><cell>0.906</cell></row><row><cell cols="2">M1-train represents</cell><cell>,</cell><cell>,</cell><cell>,</cell><cell cols="2">. M2-train represents</cell><cell>,</cell><cell>,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,72.00,440.84,437.89,102.38"><head>Table 2</head><label>2</label><figDesc>Test set experimental results</figDesc><table coords="3,75.16,469.20,434.73,74.02"><row><cell>Method</cell><cell>F1</cell><cell>AUC</cell><cell>c@1</cell><cell></cell><cell>f_05_u</cell><cell>Brier</cell><cell>overall</cell></row><row><cell>M2-test-sm</cell><cell>0.0067</cell><cell>0.4962</cell><cell>0.4962</cell><cell></cell><cell>0.0161</cell><cell>0.4962</cell><cell>overall</cell></row><row><cell>all</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">M2-test-small represents</cell><cell>,</cell><cell>,</cell><cell>,</cell><cell>.</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6.">Acknowledgments</head><p>This work is supported by the <rs type="funder">National Natural Science Foundation of China</rs> (No.<rs type="grantNumber">61806075</rs> and No.<rs type="grantNumber">61772177</rs>), and the <rs type="funder">Social Science Foundation of Guangdong Province</rs> (No. <rs type="grantNumber">GD20CTS02</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_WJwxREg">
					<idno type="grant-number">61806075</idno>
				</org>
				<org type="funding" xml:id="_yVAMSqP">
					<idno type="grant-number">61772177</idno>
				</org>
				<org type="funding" xml:id="_eMV5JsK">
					<idno type="grant-number">GD20CTS02</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="4,108.00,164.20,414.85,9.66;4,108.00,176.85,222.84,9.66" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,258.75,164.20,264.10,9.66;4,108.00,176.85,30.08,9.66">Design and implementation of web search engine based on Lucene</title>
		<author>
			<persName coords=""><surname>Guan Jian-He</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gan Jian-Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,145.84,176.85,152.94,9.66">Computer Engineering and Design</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,108.00,189.50,414.70,9.66;4,108.00,202.15,414.97,9.66;4,108.00,214.80,368.88,9.66" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,156.47,202.15,251.78,9.66">Overview of the Authorship Verification Task at PAN</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Markov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Manjavacas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="4,441.48,202.15,81.49,9.66;4,108.00,214.80,262.07,9.66">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
