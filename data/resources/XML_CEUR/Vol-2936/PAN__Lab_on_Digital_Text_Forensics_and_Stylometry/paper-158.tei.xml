<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,372.06,15.42;1,89.29,106.66,355.29,15.42;1,89.29,126.87,157.29,14.15;2,230.53,90.94,40.88,8.96;2,238.80,100.84,24.38,8.96;2,234.68,143.93,38.26,8.96;2,238.20,153.28,30.50,8.96;2,290.80,90.31,5.17,7.68;2,362.94,93.45,64.36,8.96;2,141.46,127.01,28.76,8.96;2,307.33,89.30,39.77,8.96;2,286.54,131.93,21.19,8.96;2,335.08,183.05,48.17,8.96;2,229.98,183.35,63.02,8.96;2,232.53,192.38,52.72,8.96">O2D2: Out-Of-Distribution Detector to Capture Undecidable Trials in Authorship Verification Notebook for PAN at CLEF 2021</title>
				<funder ref="#_9UJQJbj">
					<orgName type="full">North Rhine-Westphalia</orgName>
				</funder>
				<funder ref="#_z5EF6Sj">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder ref="#_MpGcj5a">
					<orgName type="full">Deutsche Forschungsgemeinschaft (DFG)</orgName>
				</funder>
				<funder ref="#_52vsmXn">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,152.77,113.01,14.16"><forename type="first">Benedikt</forename><surname>Boenninghoff</surname></persName>
							<email>benedikt.boenninghoff@rub.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Ruhr University Bochum</orgName>
								<address>
									<settlement>Germay</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,214.95,152.77,82.60,14.16"><forename type="first">Robert</forename><forename type="middle">M</forename><surname>Nickel</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Bucknell University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,328.54,152.77,86.16,14.16"><forename type="first">Dorothea</forename><surname>Kolossa</surname></persName>
							<email>dorothea.kolossa@rub.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Ruhr University Bochum</orgName>
								<address>
									<settlement>Germay</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,372.06,15.42;1,89.29,106.66,355.29,15.42;1,89.29,126.87,157.29,14.15;2,230.53,90.94,40.88,8.96;2,238.80,100.84,24.38,8.96;2,234.68,143.93,38.26,8.96;2,238.20,153.28,30.50,8.96;2,290.80,90.31,5.17,7.68;2,362.94,93.45,64.36,8.96;2,141.46,127.01,28.76,8.96;2,307.33,89.30,39.77,8.96;2,286.54,131.93,21.19,8.96;2,335.08,183.05,48.17,8.96;2,229.98,183.35,63.02,8.96;2,232.53,192.38,52.72,8.96">O2D2: Out-Of-Distribution Detector to Capture Undecidable Trials in Authorship Verification Notebook for PAN at CLEF 2021</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">03CD5BCC2031BCF296EEA49AFDD2E496</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Authorship Verification</term>
					<term>Out-Of-Distribution Detection</term>
					<term>Open-Set</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The PAN 2021 authorship verification (AV) challenge is part of a three-year strategy, moving from a cross-topic/closed-set AV task to a cross-topic/open-set AV task over a collection of fanfiction texts. In this work, we present a novel hybrid neural-probabilistic framework that is designed to tackle the challenges of the 2021 task. Our system is based on our 2020 winning submission, with updates to significantly reduce sensitivities to topical variations and to further improve the system's calibration by means of an uncertainty adaptation layer. Our framework additionally includes an out-of-distribution detector (O2D2) for defining non-responses. Our proposed system outperformed all other systems that participated in the PAN 2021 AV task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this paper we are proposing a significant extension to the authorship verification (AV) system presented in <ref type="bibr" coords="1,149.61,395.72,11.58,12.92" target="#b0">[1]</ref>. The work is part of the PAN 2021 AV shared task <ref type="bibr" coords="1,398.28,395.72,12.78,12.92" target="#b1">[2]</ref>, for which the PAN organizers provided the challenge participants with a publicly available dataset of fanfiction.</p><p>Fanfiction texts are fan-written extensions of well-known story lines, in which the so-called fandom topic describes the principal subject of the literary document (e.g. Harry Potter). The use of fanfiction as a genre has three major advantages. Firstly, the abundance of texts written in this genre makes it feasible to collect a large training dataset and, therefore, to build more complex authorship verification (AV) systems based on modern deep learning techniques, which will hopefully boost progress in this research area. Additionally, fanfictional documents also come with meaningful meta-data like topical information, which can be used to investigate the topical interference in authorship analysis. Lastly, although the documents are usually produced by non-professional writers, contrary to social media messages, they usually follow standard grammatical and spelling conventions. This allows participants to incorporate pretrained models for, e.g., part-of-speech tagging, and to reliably extract traditional stylometric features <ref type="bibr" coords="1,474.54,558.31,11.43,12.92" target="#b2">[3]</ref>.</p><p>The previous edition of the PAN AV task dealt with cross-fandom/closed-set AV <ref type="bibr" coords="1,468.81,571.86,11.59,12.92" target="#b3">[4]</ref>. The objective of the cross-fandom AV task is to automatically decide whether two fanfictional documents covering different fandoms belong to the same author. The term closed-set refers to the fact that the test dataset, which is not publicly available, only contains trials from a subset of the authors and fandoms provided in the training data.</p><p>To increase the level of difficulty, the current PAN AV challenge moved from a closed-set task to an open-set task in 2021, while the training dataset is identical to that of the previous year <ref type="bibr" coords="2,111.81,298.09,11.57,12.92" target="#b4">[5]</ref>. In this scenario, the new test data contains only authors and fandoms that were not included in the training data. We thus expect a covariate shift between training and testing data, i.e. the distribution of our neural stylometric representations extracted from the training data is expected to be different from the distribution of the test data representations. It was implicitly shown in <ref type="bibr" coords="2,132.03,352.29,11.28,12.92" target="#b3">[4]</ref>, and our experiments confirm this analysis, that such a covariate shift, due to topic variability, is a major cause of errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">System Overview</head><p>The overall structure of our revised system<ref type="foot" coords="2,279.29,406.23,3.71,9.44" target="#foot_0">1</ref> is shown in Fig. <ref type="figure" coords="2,358.50,407.46,3.71,12.92" target="#fig_0">1</ref>. It expands our winning system from 2020 as follows: Suppose we have a pair of documents 𝒟 1 and 𝒟 2 with an associated ground-truth hypothesis ℋ 𝑎 for 𝑎 ∈ {0, 1}. The value of 𝑎 indicates, whether the two documents were written by the same author (𝑎 = 1) or by different authors (𝑎 = 0). Our task can formally be expressed as a mapping 𝑓:{𝒟 1 , 𝒟 2 } -→ 𝑝 ∈ [0, 1]. The estimated label ̂︀ 𝑎 is obtained from a threshold test applied to the output prediction 𝑝. In our case, we choose ̂︀ 𝑎 = 1 if 𝑝 &gt; 0.5 and ︀ 𝑎 = 0 if 𝑝 &lt; 0.5. The PAN 2020/21 shared tasks also permit the return of a non-response (in addition to ̂︀ 𝑎 = 1 and ̂︀ 𝑎 = 0) in cases of high uncertainty <ref type="bibr" coords="2,343.42,502.30,11.32,12.92" target="#b3">[4]</ref>, e.g. when 𝑝 is close to 0.5. In this work, we therefore define three hypotheses: ℋ 0 : The two documents were written by two different persons, ℋ 1 : The two documents were written by the same person, ℋ 2 : Undecidable, trial does not suffice to establish authorship.</p><p>In <ref type="bibr" coords="2,112.86,586.99,11.59,12.92" target="#b0">[1]</ref>, we introduced the concept of linguistic embedding vectors (LEVs). To obtain these, we perform neural feature extraction followed by deep metric learning (DML) to encode the stylistic characteristics of a pair of documents into a pair of fixed-length and topic-invariant stylometric representations. Given the LEVs, a Bayes factor scoring (BFS) layer computes the posterior probability for a trial. This discriminative two-covariance model was introduced in <ref type="bibr" coords="3,492.63,85.02,11.28,12.92" target="#b5">[6]</ref>.</p><p>As a new component, we propose an uncertainty adaptation layer (UAL). This idea is adopted from <ref type="bibr" coords="3,113.09,112.12,11.29,12.92" target="#b6">[7]</ref>, aiming to find and correct wrongly classified trials of the BFS layer, to model its noise behavior, and to return re-calibrated posteriors.</p><p>For the decision whether to accept ℋ 0 /ℋ 1 , or to return a non-response, i.e. ℋ 2 , it is desirable that the value of the posterior 𝑝 reliably reflects the uncertainty of the decision-making process. We may roughly distinguish two different types of uncertainty <ref type="bibr" coords="3,380.80,166.32,11.71,12.92" target="#b7">[8]</ref>: In AV, aleatoric or data uncertainty is associated with properties of the document pairs. Examples are topical variations or the intra-and inter-author variabilities. Aleatoric uncertainty generally can not be reduced, but it can be addressed (to a certain extent) by returning a non-response (i.e. hypothesis ℋ 2 ) if it is too large to allow for a reliable decision. To accomplish this, and inspired by <ref type="bibr" coords="3,474.11,220.52,11.59,12.92" target="#b8">[9]</ref>, we incorporate a feed-forward network for out-of-distribution detection (O2D2), which is trained on a dataset that is different, i.e. disjoint w.r.t. authors and fandoms, from the training set used to optimize the DML, BFS and UAL components.</p><p>Additionally, epistemic or model uncertainty characterizes uncertainty in the model parameters. Examples are unseen authors or topics. Epistemic uncertainty can be reduced through a substantial increase in the amount of training data, i.e. an increase in the number of training pairs. We capture epistemic uncertainty in our work through the proposed O2D2 approach and also by extending our model to an ensemble. We expect all models to behave similarly for known authors or topics, but the output predictions may be widely dispersed for pairs under covariate shift <ref type="bibr" coords="3,155.48,356.01,16.25,12.92" target="#b9">[10]</ref>.</p><p>The training procedure consists of two stages: In the first stage, we simultaneously train the DML, BFS and UAL components. In the second stage, we learn the parameters of the O2D2 model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset Splits for the PAN 2021 AV Task</head><p>The text preprocessing strategies, including tokenization and pair re-sampling, are comprehensively described in <ref type="bibr" coords="3,176.40,451.82,16.42,12.92" target="#b10">[11]</ref>. The fanfictional dataset for the PAN 2020/21 AV tasks are described in <ref type="bibr" coords="3,100.57,465.37,11.24,12.92" target="#b3">[4,</ref><ref type="bibr" coords="3,114.40,465.37,7.49,12.92" target="#b4">5]</ref>. In the following, we report on the various dataset splits that we employed for our PAN 2021 submission.</p><p>Each document pair is characterized by a tuple (𝑎, 𝑓 ), where 𝑎 ∈ {0, 1} denotes the authorship similarity label and 𝑓 ∈ {0, 1} describes the equivalent for the fandom. We assign each document pair to one of the following author-fandom subsets<ref type="foot" coords="3,374.70,518.34,3.71,9.44" target="#foot_1">2</ref> SA_SF, SA_DF, DA_SF, and DA_DF given its label tuple (𝑎, 𝑓 ). As shown in <ref type="bibr" coords="3,157.76,546.67,16.12,12.92" target="#b10">[11]</ref>, one of the difficulties working with the provided small/large PAN datasets is that each author generally contributes only with a small number of documents. As a result, we observe a high degree of overlap in the re-sampled subsets of same-author trials. We decided to work only with the large dataset this year and split the documents into three disjoint (w.r.t. authorship and fandom) sets. Overlapping documents, where author and fandom belong to different sets, are removed. The splits are summarized in Fig. <ref type="figure" coords="3,387.46,614.41,5.17,12.92">2</ref>  • The purpose of the validation set is to tune the hyper-parameters of the O2D2 stage and to report the final evaluation metrics for all stages in Section 5. • The development set is identical to the evaluation set in <ref type="bibr" coords="4,373.79,390.60,17.90,12.92" target="#b10">[11]</ref> and was used to tune the hyper-parameters during the training of the first stage. This dataset contains pairs from the calibration and validation sets. However, due to the pair re-sampling strategy in <ref type="bibr" coords="4,487.02,417.70,16.13,12.92" target="#b10">[11]</ref>, documents may appear in different subsets and varied document pairs may be sampled. It thus does not represent a union of the calibration and validation sets. • Finally, the PAN 2021 evaluation set, which is not publicly available, has been used to test our submission and to compare it with the proposed frameworks of all other participants.</p><p>Note that both, the validation and development set in Table <ref type="table" coords="4,358.06,506.48,5.10,12.92" target="#tab_0">1</ref> only contain SA_DF and DA_SF pairs, for reasons discussed in Section 5. The pairs of these sets are sampled once and then kept fixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodologies</head><p>In this section, we briefly describe all components of our neural-probabilistic model. Sections 4.1 through 4.4 repeat information that is already provided in <ref type="bibr" coords="4,349.49,588.74,17.91,12.92" target="#b10">[11]</ref> to provide proper context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Neural Feature Extraction and Deep Metric Learning</head><p>Feature extraction and deep metric learning are realized in the form of a Siamese network, feeding both input documents through exactly the same function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Neural Feature Extraction:</head><p>The system passes token and character embeddings into a two-tiered bidirectional LSTM network with attentions,</p><formula xml:id="formula_0" coords="5,203.56,134.84,302.43,14.37">𝑥 𝑖 = NeuralFeatureExtraction 𝜃 (︀ 𝐸 𝑤 𝑖 , 𝐸 𝑐 𝑖 )︀ ,<label>(1)</label></formula><p>where 𝜃 contains all trainable parameters, 𝐸 𝑤 𝑖 represents word embeddings and 𝐸 𝑐 𝑖 represents character embeddings. A comprehensive description is given in <ref type="bibr" coords="5,374.52,167.91,16.25,12.92" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Deep Metric Learning:</head><p>We feed the document embeddings 𝑥 𝑖 in Eq. ( <ref type="formula" coords="5,282.55,208.74,3.86,12.92" target="#formula_0">1</ref>) into a metric learning layer, 𝑦 𝑖 = tanh (︀</p><formula xml:id="formula_1" coords="5,89.29,207.14,425.46,27.39">𝑊 DML 𝑥 𝑖 + 𝑏 DML )︀</formula><p>, which yields the two LEVs 𝑦 1 and 𝑦 2 via the trainable parameters 𝜓 = {𝑊 DML , 𝑏 DML }. We then compute the Euclidean distance between both LEVs, 𝑑(𝑦</p><formula xml:id="formula_2" coords="5,375.70,236.92,96.93,15.24">1 , 𝑦 2 ) = ‖𝑦 1 -𝑦 2 ‖ 2 2 .</formula><p>In <ref type="bibr" coords="5,487.06,237.27,16.10,12.92" target="#b10">[11]</ref>, we introduced a new probabilistic version of the contrastive loss: Given the Euclidean distance of the LEVs, we apply a kernel function</p><formula xml:id="formula_3" coords="5,201.44,282.89,304.55,14.29">𝑝 DML (ℋ 1 |𝑦 1 , 𝑦 2 ) = exp (︀ -𝛾 𝑑(𝑦 1 , 𝑦 2 ) 𝛼 )︀ ,<label>(2)</label></formula><p>where 𝛾 and 𝛼 can be seen as both, hyper-parameters or trainable variables. The loss then is given by</p><formula xml:id="formula_4" coords="5,94.10,332.56,411.88,29.59">ℒ DML 𝜃,𝜓 = 𝑎 • max {︀ 𝜏 𝑠 -𝑝 DML (ℋ 1 |𝑦 1 , 𝑦 2 ), 0 }︀ 2 + (1 -𝑎) • max {𝑝 DML (ℋ 1 |𝑦 1 , 𝑦 2 ) -𝜏 𝑑 , 0} 2 ,<label>(3)</label></formula><p>where we set 𝜏 𝑠 = 0.91 and 𝜏 𝑑 = 0.09.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Deep Bayes Factor Scoring</head><p>We assume that the LEVs stem from a Gaussian generative model that can be decomposed as 𝑦 = 𝑠 + 𝑛, where 𝑛 characterizes a noise term. We assume that the writing characteristics of the author lie in a latent stylistic variable 𝑠. The probability density functions for 𝑠 and 𝑛 are modeled as Gaussian distributions. We outlined in <ref type="bibr" coords="5,346.87,449.80,12.98,12.92" target="#b0">[1]</ref> how to compute the likelihoods for both hypotheses. The verification score for a trial is then given by the log-likelihood ratio:</p><formula xml:id="formula_5" coords="5,89.29,476.89,394.98,15.43">score(𝑦 1 , 𝑦 2 ) = log 𝑝(𝑦 1 , 𝑦 2 |ℋ 1 ) -log 𝑝(𝑦 1 , 𝑦 2 |ℋ 0 ). Assuming 𝑝(ℋ 1 ) = 𝑝(ℋ 0 ) = 1 2</formula><p>, the probability for a same-author trial is calculated as <ref type="bibr" coords="5,313.40,490.44,11.56,12.92" target="#b0">[1]</ref>:</p><formula xml:id="formula_6" coords="5,127.33,509.82,378.66,26.51">𝑝 BFS (ℋ 1 |𝑦 1 , 𝑦 2 ) = 𝑝(𝑦 1 , 𝑦 2 |ℋ 1 ) 𝑝(𝑦 1 , 𝑦 2 |ℋ 1 ) + 𝑝(𝑦 1 , 𝑦 2 |ℋ 0 ) = Sigmoid (︀ score(𝑦 1 , 𝑦 2 ) )︀<label>(4)</label></formula><p>We reduce the dimension of the LEVs via 𝑦 BFS 𝑖 = tanh (︀ 𝑊 BFS 𝑦 𝑖 + 𝑏 BFS )︀ to ensure numerically stable inversions of the matrices <ref type="bibr" coords="5,235.70,555.00,11.43,12.92" target="#b0">[1]</ref>. We rewrite Eq. ( <ref type="formula" coords="5,326.77,555.00,3.86,12.92" target="#formula_6">4</ref>) as</p><formula xml:id="formula_7" coords="5,193.23,571.75,312.76,16.14">𝑝 BFS (ℋ 1 |𝑦 1 , 𝑦 2 ) = Sigmoid (︀ score(𝑦 BFS 1 , 𝑦 BFS 2 ) )︀<label>(5)</label></formula><p>and incorporate Eq. ( <ref type="formula" coords="5,183.41,593.06,3.86,12.92" target="#formula_7">5</ref>) into the binary cross entropy,</p><formula xml:id="formula_8" coords="5,135.70,609.81,370.29,16.31">ℒ BFS 𝜑 = 𝑎 • log {𝑝 BFS (ℋ 1 |𝑦 1 , 𝑦 2 )} + (1 -𝑎) • log {1 -𝑝 BFS (ℋ 1 |𝑦 1 , 𝑦 2 )} ,<label>(6)</label></formula><p>where all trainable parameters are denoted with 𝜑 = {︀ 𝑊 BFS , 𝑏 BFS , 𝑊 , 𝐵, 𝜇 }︀ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Uncertainty Modeling and Adaptation</head><p>Now, we treat the posteriors of the BFS component as noisy outcomes and rewrite Eq. ( <ref type="formula" coords="6,486.07,102.76,3.86,12.92" target="#formula_7">5</ref>) as 𝑝 BFS ( ̂︀ ℋ 1 |𝑦 1 , 𝑦 2 ) to emphasize that this represents an estimated posterior. We firstly have to find a single representation for both LEVs, which is done by</p><formula xml:id="formula_9" coords="6,89.29,129.68,418.81,27.39">𝑦 UAL = tanh (︀ 𝑊 UAL (︀ 𝑦 1 -𝑦 2 )︀ ∘2 + 𝑏 UAL )︀</formula><p>, where (•) ∘2 denotes the element-wise square. Next, we compute a 2 × 2 confusion matrix as follows</p><formula xml:id="formula_10" coords="6,142.83,172.66,363.15,40.36">𝑝(ℋ 𝑗 | ̂︀ ℋ 𝑖 , 𝑦 1 , 𝑦 2 ) = exp (︀ 𝑤 𝑇 𝑗𝑖 𝑦 BFS + 𝑏 𝑗𝑖 )︀ ∑︀ 𝑖 ′ ∈{0,1} exp (︀ 𝑤 𝑇 𝑗𝑖 ′ 𝑦 BFS + 𝑏 𝑗𝑖 ′ )︀ for 𝑖, 𝑗 ∈ {0, 1}.<label>(7)</label></formula><p>The term 𝑝(ℋ 𝑗 | ̂︀ ℋ 𝑖 , 𝑦 1 , 𝑦 2 ) defines the conditional probability of the true hypothesis ℋ 𝑗 given the hypothesis ̂︀ ℋ 𝑖 assigned by the BFS. We can then define the final output predictions as:</p><formula xml:id="formula_11" coords="6,159.99,256.19,345.99,26.14">𝑝 UAL (ℋ 𝑗 |𝑦 1 , 𝑦 2 ) = ∑︁ 𝑖∈{0,1} 𝑝(ℋ 𝑗 | ̂︀ ℋ 𝑖 , 𝑦 1 , 𝑦 2 ) • 𝑝 BFS ( ̂︀ ℋ 𝑖 |𝑦 1 , 𝑦 2 ).<label>(8)</label></formula><p>The loss consists of two terms, the negative log-likelihood of the ground-truth hypothesis and a regularization term,</p><formula xml:id="formula_12" coords="6,97.26,320.15,408.73,42.05">ℒ UAL 𝜆 = -log 𝑝 UAL (ℋ 𝑗 |𝑦 1 , 𝑦 2 ) + 𝛽 ∑︁ 𝑖∈{0,1} ∑︁ 𝑗∈{0,1} 𝑝(ℋ 𝑗 | ̂︀ ℋ 𝑖 , 𝑦 1 , 𝑦 2 ) • log 𝑝(ℋ 𝑗 | ̂︀ ℋ 𝑖 , 𝑦 1 , 𝑦 2 ),<label>(9)</label></formula><p>with trainable parameters denoted by 𝜆 = {︀ 𝑊 UAL , 𝑏𝑓 UAL , 𝑤 𝑗𝑖 , 𝑏 𝑗𝑖 |𝑗, 𝑖 ∈ {0, 1} }︀ . The regularization term, controlled by 𝛽, follows the maximum entropy principle to penalize the confusion matrix for returning over-confident posteriors <ref type="bibr" coords="6,297.62,397.04,16.25,12.92" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Combined Loss Function:</head><p>All components are optimized independently w.r.t. the following combined loss:</p><formula xml:id="formula_13" coords="6,222.49,454.94,283.50,16.31">ℒ 𝜃,𝜓,𝜑,𝜆 = ℒ DML 𝜃,𝜓 + ℒ BFS 𝜑 + ℒ UAL 𝜆 .<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Out-of-Distribution Detector (O2D2)</head><p>Following <ref type="bibr" coords="6,137.09,506.52,11.54,12.92" target="#b8">[9]</ref>, we incorporate a second neural network to detect undecidable trials. We treat the training procedure as a binary verification task. Given the learned DML, BFS and UAL components, the estimated authorship labels are obtained via</p><formula xml:id="formula_14" coords="6,183.86,552.15,322.13,16.67">︀ 𝑎 = arg max [︀ 𝑝 UAL (ℋ 0 |𝑦 1 , 𝑦 2 ), 𝑝 UAL (ℋ 1 |𝑦 1 , 𝑦 2 ) ]︀ .<label>(11)</label></formula><p>Now, we can define the binary O2D2 labels as follows:</p><formula xml:id="formula_15" coords="6,149.11,590.29,356.88,30.54">𝑙 O2D2 = {︃ 1, if 𝑎 ̸ = ̂︀ 𝑎 or 0.5 -𝜖 ≤ 𝑝 UAL (ℋ 1 |𝑦 1 , 𝑦 2 ) ≤ 0.5 + 𝜖, 0, otherwise.<label>(12)</label></formula><p>The  architecture,</p><formula xml:id="formula_16" coords="7,184.61,321.76,321.37,51.83">ℎ 1 = tanh (︀ 𝑊 O2D2 1 𝑦 O2D2 + 𝑏 O2D2 1 )︀ , ℎ 2 = tanh (︀ 𝑊 O2D2 2 ℎ 1 + 𝑏 O2D2 2 )︀ , 𝑝 O2D2 (ℋ 2 |𝑦 1 , 𝑦 2 ) = Sigmoid (︀ 𝑊 O2D2 3 ℎ 2 + 𝑏 O2D2 3 )︀ .<label>(13)</label></formula><p>All trainable parameters are summarized in</p><formula xml:id="formula_17" coords="7,286.83,378.28,156.76,16.12">Γ = {︀ 𝑊 O2D2 𝑖 , 𝑏 O2D2 𝑖 |𝑖 ∈ {1, 2, 3} }︀ .</formula><p>The obtained prediction for hypothesis ℋ 2 is inserted into the cross-entropy loss,</p><formula xml:id="formula_18" coords="7,101.31,410.38,404.67,16.25">ℒ O2D2 Γ = 𝑙 O2D2 • log {𝑝 O2D2 (ℋ 2 |𝑦 1 , 𝑦 2 )} + (1 -𝑙 O2D2 ) • log {1 -𝑝 O2D2 (ℋ 2 |𝑦 1 , 𝑦 2 )} . (14)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Ensemble Inference</head><p>As a last step, an ensemble is constructed from 𝑀 trained models, ℳ 1 , . . . , ℳ 𝑀 , with 𝑀 being an odd number. Since all models are randomly initialized and trained on different re-sampled pairs in each epoch, we expect to obtain a slightly different set of weights/biases, which in turn produces different posteriors, especially for pairs under covariate shift. We propose a majority voting for the non-responses. More precisely, the ensemble returns a non-response, if</p><formula xml:id="formula_19" coords="7,190.37,533.22,315.62,33.58">𝑀 ∑︁ 𝑚=1 1 [︀ 𝑝 O2D2 (ℋ 2 |𝑦 1 , 𝑦 2 , ℳ 𝑚 ) ≥ 0.5 ]︀ &gt; ⌊︂ 𝑀 2 ⌋︂ ,<label>(15)</label></formula><p>where 1[•] denotes the indicator function. Otherwise, we define a subset of confident models,</p><formula xml:id="formula_20" coords="7,89.29,587.56,175.38,11.65">ℳ 𝑐 = {ℳ| 𝑝 O2D2 (ℋ 2 |𝑦 1 , 𝑦 2 , ℳ) &lt; 0.</formula><p>5}, and return the averaged posteriors of its elements,</p><formula xml:id="formula_21" coords="7,168.20,603.33,333.62,29.64">E [︀ 𝑝 UAL (ℋ 1 |𝑦 1 , 𝑦 2 ) ]︀ = 1 |ℳ 𝑐 | ∑︁ ℳ∈ℳ𝑐 𝑝 UAL (ℋ 1 |𝑦 1 , 𝑦 2 , ℳ). (<label>16</label></formula><formula xml:id="formula_22" coords="7,501.82,608.04,4.16,12.92">)</formula><p>Our submitted system consisted of an ensemble with 𝑀 = 21 trained models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>The PAN evaluation metrics and procedure are described in <ref type="bibr" coords="8,350.68,334.07,11.23,12.92" target="#b3">[4,</ref><ref type="bibr" coords="8,364.45,334.07,7.42,12.92" target="#b4">5,</ref><ref type="bibr" coords="8,374.42,334.07,12.23,12.92" target="#b13">14]</ref>. To capture the calibration capacity, we also provide the accuracy (acc), confidence score (conf), expected calibration error (ECE) and maximum calibration error (MCE) <ref type="bibr" coords="8,284.08,361.17,16.40,12.92" target="#b14">[15]</ref>. All confidence values lie within the interval [0.5, 1], since we are solving a binary classification task. Hence, to obtain confidence scores, the posterior values are transformed w.r.t. the estimated authorship label, showing</p><formula xml:id="formula_23" coords="8,89.14,388.27,416.84,30.22">𝑝(ℋ 1 |𝑦 1 , 𝑦 2 ) if ︀ 𝑎 = 1 and 1 -𝑝(ℋ 1 |𝑦 1 , 𝑦 2 ) if ̂︀ 𝑎 = 0.</formula><p>For both metrics, the confidence interval is discretized into a fixed number of bins. The ECE then reflects the average absolute error between confidence and accuracy of all bins, while the MCE returns the maximum absolute error. For acc and conf, we perform weighted macro-averaging w.r.t. the number of trials in each bin.</p><p>Inspired by the promising results in domain-adversarial training of neural networks in <ref type="bibr" coords="8,490.70,456.01,16.48,12.92" target="#b15">[16,</ref><ref type="bibr" coords="8,89.04,469.56,12.42,12.92" target="#b16">17]</ref>, we also experimented with an adversarial fandom verifier: Starting with the document embeddings in Eq. ( <ref type="formula" coords="8,178.25,483.11,3.50,12.92" target="#formula_0">1</ref>), we fed this vector into the author verification system (including DML, BFS and UAL) and into an additional fandom verifier, which is placed parallel to the author verification system. It has the same architecture but includes a gradient reversal layer and different trainable parameters. However, in these experiments, we did not achieve any significant improvements by domain-adversarial training. Therefore, we independently optimized the fandom verifier by stopping the flow of the gradients from the fandom verifier to the authorship verification components, so that the training of the fandom verifier does not affect the target system at all. Fig. <ref type="figure" coords="8,170.02,577.96,5.12,12.92" target="#fig_1">3</ref> shows the obtained epoch-wise accuracies during training. It can be seen that the fandom accuracy stays around 55%, which indicates that the training strategy yields nearly topic-invariant stylometric representations, even without domain-adversarial training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Results on the Calibration Dataset</head><p>We first evaluated the UAL component on the calibration set (without non-responses) and calculated the respective PAN metrics for different combinations of the author-fandom subsets.  Results are shown in Table <ref type="table" coords="9,214.97,330.84,3.81,12.92" target="#tab_2">2</ref>. To guarantee that the calculated metrics are not biased by an imbalanced dataset, we reduced the number of pairs to the smallest number of pairs of all subsets. Thus, all results in Table <ref type="table" coords="9,238.22,357.94,5.05,12.92" target="#tab_2">2</ref> were computed from 2 × 2, 100 pairs. Unsurprisingly, best performance was obtained for the least challenging SA_SF + DA_DF pairs and the worst performance was seen for the most challenging SA_DF + DA_SF pairs. We continued to optimize our system w.r.t this most challenging subset combination in particular, even though we specifically expect to see SA_DF + DA_DF pairs in the PAN 2021 evaluation set. Table <ref type="table" coords="9,126.60,425.68,4.99,12.92" target="#tab_3">3</ref> additionally provides the corresponding calibration metrics. Analogously to the PAN metrics, the ECE consistently increases from the least to the most challenging data scenarios. Interestingly, our system is under-confident for SA_SF pairs, i.e. conf &lt; acc. The predictions then change to be over-confident (conf &gt; acc) for SA_DF pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results on the Validation Dataset</head><p>Next, we separately provide experimental results for all system components on the validation dataset, since O2D2 has been trained on the calibration dataset. The first four rows in Tables <ref type="table" coords="9,501.20,521.10,5.04,12.92" target="#tab_4">4</ref> and<ref type="table" coords="9,108.53,534.65,5.10,12.92" target="#tab_5">5</ref> summarize the PAN metrics and the corresponding calibration measures averaged over all ensembles models.</p><p>The overall score of the UAL component in the third row of Table <ref type="table" coords="9,394.55,561.75,5.06,12.92" target="#tab_4">4</ref> is on par with the DML and BFS components and slightly lower compared to the corresponding UAL score measured on the calibration dataset in Table <ref type="table" coords="9,242.09,588.85,3.70,12.92" target="#tab_2">2</ref>. Nevertheless, we do not observe significant differences in the metrics for both datasets, which shows the robustness and generalization of our system.</p><p>Going from the third to the fourth row in Table <ref type="table" coords="9,315.84,615.95,3.77,12.92" target="#tab_4">4</ref>, it can be observed that the overall score, boosted by c@1 and F1, significantly increases from 92.5 to 93.2. Hence, the model performs better if we take undecidable trials into account. However, the f_05_u score decreases, since it treats non-responses as false negatives. The percentage of undecidable trials generally ranges  from 8% to 11%.</p><p>In Table <ref type="table" coords="10,140.36,403.24,3.81,12.92" target="#tab_5">5</ref>, we see that both, the BFS and UAL components notably improve the ECE and MCE metrics. However, an insertion of non-responses via O2D2 significantly increases the MCE. This can be explained by the posterior histograms in Fig. <ref type="figure" coords="10,354.83,430.34,3.81,12.92" target="#fig_2">4</ref>. The plots (a) and (b) show the histograms for SA_DF and DA_SF pairs without applying O2D2 to define non-responses. In contrast, plots (c) and (d) present the corresponding histograms including the 0.5-values of non-responses. The effect of O2D2 is that most of the trials, whose posteriors fall within the interval [0.3, 0.7], are eventually declared as undecidable. Hence, the system correctly predicts nearly all of the remaining as confidently assigned trials around 0.7/0.8 for same-author pairs or 0.2/0.3 for different-author pairs. As a result, we see a large gap (i.e. conf &lt;&lt; acc) between the confidence score and the averaged accuracy in these bins. The last two rows in Tables <ref type="table" coords="10,223.31,538.73,4.98,12.92" target="#tab_4">4</ref> and<ref type="table" coords="10,249.84,538.73,4.98,12.92" target="#tab_5">5</ref> show the performance of the ensemble, first without and then with non-responses, to show the effect of O2D2. On the validation set, our ensemble with O2D2 returns non-responses in 9% of the test cases. Comparing the last two rows, we obtain the highest overall score with our proposed framework, which ultimately presents our final submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results on the PAN 2021 Evaluation Dataset</head><p>To conclude this section, we present our results on the official PAN 2021 evaluation set. The performance for both, the early-bird and the final submission, can be found in Table <ref type="table" coords="10,461.14,647.70,3.69,12.92" target="#tab_7">6</ref>. We also provide the reported result on the PAN 2020 evaluation set for the predecessor model. Unsurprisingly, the early-bird overall score (single model) on the PAN 2021 evaluation set is slightly higher, since it contains DA_DF pairs instead of DA_SF pairs. The main difference is, unexpectedly, given by the f_05_u score, which increases from 89.3% to 94.6%. In our opinion, this is caused by returning a lower number of non-responses, which would also explain the lower values for c@1 and F1.</p><p>Comparing the early-bird (2 𝑛𝑑 row) with the final submission (4 𝑡ℎ row), we can further significantly increase the overall score by 1.5%. We assume that the ensemble now returns a higher number of non-responses, which results in a slightly lower f_05_u score. Conversely, we can observe improved values for the c@1, F1 and brier scores.</p><p>The last row displays the achieved PAN 2020 results. As can be seen, our final submission ends up with a higher overall score (plus 2%) by significantly improving all single metrics, although the PAN competition moved from a closed-set to open-set shared task, illustrating the efficiency of the proposed extensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we presented O2D2, which captures undecidable trials and supports our hybrid neural-probabilistic end-to-end framework for authorship verification. We made use of the early-bird submission to receive a preliminary assessment of how the framework behaves on the novel open-set evaluation. Finally, based on the presented results, we submitted an O2D2-supported ensemble to the shared task, which clearly outperformed our own system from 2020 as well as the new submissions to the PAN 2021 AV task.</p><p>These results support our hypothesis that modeling aleatoric and epistemic uncertainty and using them for decision support is a beneficial strategy-not just for responsible ML, which needs to be aware of the reliability of its proposed decisions, but also, importantly, for achieving optimal performance in real-life settings, where distributional shift is almost always hard to avoid.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,207.35,416.69,12.92;2,134.73,220.90,166.34,12.92"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Our proposed hybrid neural-probabilistic framework for the PAN 2021 cross-fandom open-set authorship verification task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,89.29,269.77,416.69,12.92;7,134.73,283.32,216.45,12.92"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Averaged accuracy curves (including mean and standard deviation) for the authorship and fandom verification outputs during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="10,171.09,366.70,253.10,12.92"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Posterior histograms on the validation dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,89.29,614.41,417.90,40.02"><head>Table 1 :</head><label>1</label><figDesc>and Table1. Altogether, the following datasets have been involved in the PAN 2021 shared task, to train the model components, tune the hyper-parameter and for testing: Disjoint splits of the large PAN 2020/21 training set. Numbers of (re-)sampled pairs for all datasets. The training set is identical to the one used in<ref type="bibr" coords="4,341.31,251.04,18.07,12.92" target="#b10">[11]</ref> and was employed for the first stage, i.e., to train the DML, BFS and UAL components simultaneously. During training we re-sampled the pairs epoch-wise such that all documents contribute equally to the neural network training in each epoch. The numbers of training pairs provided in Table 1 therefore vary in each epoch. • The calibration set has been used for the second stage, i.e., to train (calibrate) the O2D2 model. During training, we again re-sampled the pairs in each epoch and limited the total number of pairs in the different-authors subsets to partly balance the dataset.</figDesc><table coords="4,107.28,82.11,353.78,181.85"><row><cell>Training Set</cell><cell></cell><cell cols="2">Calibration Set</cell><cell>Development Set</cell></row><row><cell>303,142 docs 200,732 authors</cell><cell></cell><cell cols="2">49,654 docs 39,547 authors</cell><cell>46,373 docs 37,883 authors</cell></row><row><cell>1,200 fandoms</cell><cell></cell><cell cols="2">200 fandoms</cell><cell>200 fandoms</cell></row><row><cell>Figure 2: Dataset</cell><cell cols="4">SA_SF SA_DF DA_SF DA_DF</cell></row><row><cell>Training set</cell><cell cols="4">16,045 28,500 64,300 42,730</cell></row><row><cell>Calibration set</cell><cell>2,100</cell><cell>2,715</cell><cell>4,075</cell><cell>4,075</cell></row><row><cell>Validation set</cell><cell>0</cell><cell>2,280</cell><cell>3075</cell><cell>0</cell></row><row><cell>Development set</cell><cell>0</cell><cell>5,215</cell><cell>7,040</cell><cell>0</cell></row><row><cell>•</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,88.93,83.55,417.26,103.26"><head>Table 2 :</head><label>2</label><figDesc>Averaged results (including mean and standard deviation) of the UAL framework for different subset combinations on the calibration dataset.</figDesc><table coords="8,117.83,120.65,359.62,66.17"><row><cell>Model</cell><cell>AUC</cell><cell>c@1</cell><cell>PAN 2021 Evaluation Metrics f_05_u F1</cell><cell>Brier</cell><cell>overall</cell></row><row><cell cols="6">SA_SF + DA_DF 99.8 ± 0.0 97.5 ± 0.2 97.2 ± 0.3 97.5 ± 0.2 98.1 ± 0.1 98.0 ± 0.2</cell></row><row><cell cols="6">SA_SF + DA_SF 99.6 ± 0.1 95.9 ± 0.4 94.8 ± 0.6 96.0 ± 0.4 97.1 ± 0.2 96.7 ± 0.4</cell></row><row><cell cols="6">SA_DF + DA_DF 98.1 ± 0.1 92.4 ± 0.3 94.8 ± 0.3 92.1 ± 0.3 94.2 ± 0.2 94.3 ± 0.2</cell></row><row><cell cols="6">SA_DF + DA_SF 97.1 ± 0.1 90.9 ± 0.3 92.3 ± 0.6 90.6 ± 0.3 93.1 ± 0.2 92.8 ± 0.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,88.93,199.54,417.34,105.61"><head>Table 3 :</head><label>3</label><figDesc>Averaged calibration results (including mean and standard deviation) of the UAL framework for different subsets on the calibration dataset. DA_DF 98.4 ± 0.3 97.4 ± 1.0 1.1 ± 0.9 5.7 ± 3.6 SA_SF + DA_SF 96.1 ± 0.6 95.6 ± 1.0 1.3 ± 0.8 9.9 ± 4.0 SA_DF + DA_DF 92.4 ± 0.3 93.4 ± 1.2 1.6 ± 0.6 6.2 ± 2.5 SA_DF + DA_SF 90.9 ± 0.3 92.4 ± 1.2 2.0 ± 0.7 7.4 ± 2.9</figDesc><table coords="8,168.09,236.72,248.84,32.86"><row><cell>Model</cell><cell>acc</cell><cell>Calibration Metrics conf ECE</cell><cell>MCE</cell></row><row><cell>SA_SF +</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,117.70,83.55,359.88,111.73"><head>Table 4 :</head><label>4</label><figDesc>Results for PAN 2021 evaluation metrics on the validation datset.</figDesc><table coords="9,117.70,107.06,359.88,88.22"><row><cell></cell><cell>Model</cell><cell>AUC</cell><cell>c@1</cell><cell cols="2">PAN 2021 Evaluation Metrics f_05_u F1</cell><cell>Brier</cell><cell>overall</cell></row><row><cell>single</cell><cell>DML BFS UAL</cell><cell cols="6">97.2 ± 0.1 91.3 ± 0.3 90.5 ± 0.6 89.6 ± 0.4 93.2 ± 0.4 92.4 ± 0.2 97.1 ± 0.1 91.0 ± 0.3 90.7 ± 0.8 89.2 ± 0.5 93.2 ± 0.1 92.3 ± 0.2 97.2 ± 0.1 91.3 ± 0.3 90.7 ± 0.5 89.6 ± 0.4 93.5 ± 0.2 92.5 ± 0.2</cell></row><row><cell></cell><cell>O2D2</cell><cell cols="6">97.1 ± 0.1 93.8 ± 0.2 88.1 ± 0.6 93.5 ± 0.3 93.4 ± 0.1 93.2 ± 0.2</cell></row><row><cell></cell><cell>ensemble</cell><cell>97.8</cell><cell>92.5</cell><cell>92.1</cell><cell>90.9</cell><cell>94.3</cell><cell>93.5</cell></row><row><cell cols="2">ensemble + O2D2</cell><cell>97.7</cell><cell>94.8</cell><cell>90.0</cell><cell>94.5</cell><cell>94.2</cell><cell>94.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,144.31,207.99,306.29,112.64"><head>Table 5 :</head><label>5</label><figDesc>Results for the calibration metrics on the validation dataset.</figDesc><table coords="9,167.77,231.52,259.74,89.11"><row><cell></cell><cell>Model</cell><cell>acc</cell><cell cols="2">Calibration Metrics conf ECE</cell><cell>MCE</cell></row><row><cell>single</cell><cell>DML BFS UAL</cell><cell cols="3">91.3 ± 0.3 87.9 ± 2.7 3.4 ± 2.7 91.0 ± 0.3 90.0 ± 2.3 2.3 ± 1.5 91.3 ± 0.3 92.3 ± 1.2 1.6 ± 0.6</cell><cell>9.0 ± 3.6 6.2 ± 3.0 5.8 ± 2.2</cell></row><row><cell></cell><cell>O2D2</cell><cell cols="4">91.4 ± 0.3 90.9 ± 1.1 2.3 ± 0.5 10.7 ± 2.7</cell></row><row><cell></cell><cell>ensemble</cell><cell>92.5</cell><cell>91.2</cell><cell>1.2</cell><cell>2.9</cell></row><row><cell cols="2">ensemble + O2D2</cell><cell>92.6</cell><cell>91.8</cell><cell>1.5</cell><cell>10.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="11,97.02,83.55,401.24,90.61"><head>Table 6 :</head><label>6</label><figDesc>Results of the early-bird (first two rows) and the final submission runs.</figDesc><table coords="11,97.02,104.66,401.24,69.49"><row><cell cols="2">Dataset Model type</cell><cell>AUC</cell><cell>c@1</cell><cell>f_05_u</cell><cell>F1</cell><cell>brier</cell><cell>overall</cell></row><row><cell>Validation dataset</cell><cell>single-21</cell><cell>97.2</cell><cell>93.6</cell><cell>89.3</cell><cell>92.9</cell><cell>93.3</cell><cell>93.3</cell></row><row><cell>PAN 21 evaluation dataset</cell><cell>single-21</cell><cell>98.3</cell><cell>92.6</cell><cell>94.6</cell><cell>92.1</cell><cell>92.7</cell><cell>94.0</cell></row><row><cell cols="2">Validation dataset ensemble-21</cell><cell>97.7</cell><cell>94.8</cell><cell>90.1</cell><cell>94.4</cell><cell>94.2</cell><cell>94.2</cell></row><row><cell cols="2">PAN 21 evaluation dataset ensemble-21</cell><cell>98.7</cell><cell>95.0</cell><cell>93.8</cell><cell>95.2</cell><cell>94.5</cell><cell>95.5</cell></row><row><cell cols="2">PAN 20 evaluation dataset ensemble-20</cell><cell>96.9</cell><cell>92.8</cell><cell>90.7</cell><cell>93.6</cell><cell>-</cell><cell>93.5</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.93,658.47,397.50,10.62;2,89.07,669.42,41.76,10.62"><p>The source code is accessible online: https://github.com/boenninghoff/pan_2020_2021_authorship_ verification</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,108.93,669.42,291.00,10.62"><p>SA=same author, DA=different authors, SF=same fandom, DF=different fandoms</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was in significant parts performed on an HPC cluster at <rs type="institution">Bucknell University</rs> through the support of the <rs type="funder">National Science Foundation</rs>, Grant Number <rs type="grantNumber">1659397</rs>. Project funding was provided by the state of <rs type="funder">North Rhine-Westphalia</rs> within the <rs type="programName">Research Training Group</rs> "<rs type="projectName">SecHuman -Security for Humans in Cyberspace</rs>" and by the <rs type="funder">Deutsche Forschungsgemeinschaft (DFG)</rs> under <rs type="programName">Germany</rs>'s <rs type="programName">Excellence Strategy</rs> <rs type="grantNumber">-EXC2092CaSa-390781972</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_z5EF6Sj">
					<idno type="grant-number">1659397</idno>
				</org>
				<org type="funded-project" xml:id="_9UJQJbj">
					<orgName type="project" subtype="full">SecHuman -Security for Humans in Cyberspace</orgName>
					<orgName type="program" subtype="full">Research Training Group</orgName>
				</org>
				<org type="funding" xml:id="_MpGcj5a">
					<orgName type="program" subtype="full">Germany</orgName>
				</org>
				<org type="funding" xml:id="_52vsmXn">
					<idno type="grant-number">-EXC2092CaSa-390781972</idno>
					<orgName type="program" subtype="full">Excellence Strategy</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,111.10,107.69,396.57,11.80;12,111.11,119.64,186.94,11.80" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,306.98,107.69,200.69,11.80;12,111.11,119.64,23.38,11.80">Deep Bayes Factor Scoring for Authorship Verification</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Boenninghoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kolossa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,155.33,119.64,41.14,11.80">CLEF 2020</title>
		<title level="s" coord="12,203.11,119.64,67.52,11.80">Notebook Papers</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.11,131.60,395.97,11.80;12,111.11,143.55,395.97,11.80;12,111.11,155.51,394.88,11.80;12,111.11,167.46,395.97,11.80;12,111.11,179.42,59.77,11.80" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,111.11,155.51,394.88,11.80;12,111.11,167.46,94.55,11.80">Overview of PAN 2021: Authorship Verification,Profiling Hate Speech Spreaders on Twitter,and Style Change Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chulvi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">L D L P</forename><surname>Sarracén</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Manjavacas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Markov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mayerl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wolska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Zangerle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,228.34,167.46,274.79,11.80">12th International Conference of the CLEF Association (CLEF 2021)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.11,191.37,394.88,11.80;12,111.11,203.33,272.42,11.80" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,175.16,191.37,219.64,11.80">A Survey of Modern Authorship Attribution Methods</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,405.72,191.37,100.26,11.80;12,111.11,203.33,195.77,11.80">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="538" to="556" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.11,215.28,395.97,11.80;12,111.11,227.24,395.97,11.80;12,111.11,239.19,94.93,11.80" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,147.12,227.24,294.57,11.80">Overview of the Cross-Domain Authorship Verification Task at PAN 2020</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Manjavacas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Markov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,462.11,227.24,40.83,11.80">CLEF 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Notebook Papers</note>
</biblStruct>

<biblStruct coords="12,111.11,251.15,394.88,11.80;12,110.76,263.11,396.32,11.80;12,111.11,275.06,82.91,11.80" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,440.16,251.15,65.82,11.80;12,110.76,263.11,149.02,11.80">Overview of the Authorship Verification Task at PAN</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Manjavacas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="12,300.32,263.11,127.61,11.80">CLEF 2021 Labs and Workshops</title>
		<title level="s" coord="12,435.30,263.11,71.78,11.80;12,111.11,275.06,19.90,11.80">Notebook Papers, CEUR</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.11,287.02,394.88,11.80;12,111.11,298.97,373.86,11.80" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,408.02,287.02,97.96,11.80;12,111.11,298.97,166.49,11.80">Pairwise Discriminative Speaker Verification in the I-Vector Space</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Cumani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Brümmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Laface</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Plchot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vasilakakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,285.43,298.97,72.35,11.80">IEEE Trans. Audio</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Speech, Lang. Process</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.11,310.93,396.56,11.80;12,111.11,322.88,394.88,11.80;12,111.11,334.84,122.60,11.80" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,363.28,310.93,144.39,11.80;12,111.11,322.88,286.34,11.80">Learning with Noise: Enhance Distantly Supervised Relation Extraction with Dynamic Transition Matrix</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,419.25,322.88,86.73,11.80;12,111.11,334.84,41.41,11.80">55th Annual Meeting of the ACL</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="430" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.11,346.79,395.07,11.80;12,110.78,358.75,396.30,11.80;12,111.11,370.70,40.22,11.80" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,194.30,346.79,311.88,11.80;12,110.78,358.75,28.64,11.80">What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="12,162.62,358.75,212.86,11.80">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.11,382.66,394.88,11.80;12,111.11,394.61,157.34,11.80" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<idno>ArXiv abs/2006.08914</idno>
		<title level="m" coord="12,221.41,382.66,284.58,11.80;12,111.11,394.61,32.71,11.80">Calibrating Deep Neural Network Classifiers on Out-of-Distribution Datasets</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.11,406.57,396.56,11.80;12,111.11,418.52,265.34,11.80" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,298.83,406.57,208.84,11.80;12,111.11,418.52,109.13,11.80">Simple and Scalable Predictive Uncertainty Estimation Using Deep Ensembles</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,241.70,418.52,50.10,11.80">31st NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6405" to="6416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.11,430.48,395.07,11.80;12,110.76,442.43,395.47,11.80;12,110.76,454.39,163.33,11.80" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,313.08,430.48,193.10,11.80;12,110.76,442.43,190.14,11.80">Self-Calibrating Neural-Probabilistic Model for Authorship Verification Under Covariate Shift</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Boenninghoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kolossa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><forename type="middle">M</forename><surname>Nickel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,327.77,442.43,178.45,11.80;12,110.76,454.39,97.12,11.80">12th International Conference of the CLEF Association (CLEF 2021)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.11,466.34,394.88,11.80;12,111.11,478.30,395.97,11.80;12,111.11,490.25,64.32,11.80" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,324.80,466.34,181.19,11.80;12,111.11,478.30,190.07,11.80">Explainable Authorship Verification in Social Media via Attention-based Similarity Learning</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Boenninghoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hessler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kolossa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Nickel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,326.50,478.30,176.30,11.80">IEEE International Conference on Big Data</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="36" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.11,502.21,395.14,11.80;12,111.11,514.16,198.31,11.80;12,311.90,518.17,88.91,7.22" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06548</idno>
		<title level="m" coord="12,368.99,502.21,137.25,11.80;12,111.11,514.16,171.12,11.80">Regularizing Neural Networks by Penalizing Confident Output Distributions</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.11,526.12,395.97,11.80;12,111.11,538.07,394.88,11.80;12,111.11,550.03,207.72,11.80" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,298.04,526.12,154.81,11.80">TIRA Integrated Research Architecture</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,176.26,538.07,329.72,11.80;12,111.11,550.03,22.48,11.80">Information Retrieval Evaluation in a Changing World, The Information Retrieval Series</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.11,561.98,394.88,11.80;12,111.11,573.94,355.24,11.80" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,289.89,561.98,176.09,11.80">On Calibration of Modern Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,488.21,561.98,17.77,11.80;12,111.11,573.94,189.49,11.80">34th International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.11,585.89,396.56,11.80;12,111.11,597.85,393.47,11.80" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,475.70,585.89,31.97,11.80;12,111.11,597.85,219.78,11.80">Domain-Adversarial Training of Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,339.12,597.85,79.54,11.80">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Lempitsky</note>
</biblStruct>

<biblStruct coords="12,111.11,609.81,394.88,11.80;12,111.11,621.76,384.73,11.80" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="12,490.23,609.81,15.76,11.80;12,111.11,621.76,261.76,11.80">The Importance of Suppressing Domain Style in Authorship Analysis</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bischoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Deckers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schliebs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<idno>CoRR abs/2005.14714</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
