<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,391.53,15.42;1,89.29,106.66,320.50,15.42">BeaSku at CheckThat! 2021: Fine-Tuning Sentence BERT with Triplet Loss and Limited Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,87.40,11.96"><forename type="first">Beata</forename><surname>Skuczyńska</surname></persName>
							<email>beata.skuczynska@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Artificial Intelligence Department</orgName>
								<orgName type="institution">University of Groningen</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,189.34,134.97,66.16,11.96"><forename type="first">Shaden</forename><surname>Shaar</surname></persName>
							<email>sshaar@hbku.edu.qa</email>
							<affiliation key="aff1">
								<orgName type="department">Qatar Computing Research Institute</orgName>
								<orgName type="institution">HBKU</orgName>
								<address>
									<settlement>Doha</settlement>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,268.14,134.97,87.09,11.96"><forename type="first">Jennifer</forename><surname>Spenader</surname></persName>
							<email>j.spenader@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Artificial Intelligence Department</orgName>
								<orgName type="institution">University of Groningen</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,386.23,134.97,70.20,11.96"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
							<email>pnakov@hbku.edu.qa</email>
							<affiliation key="aff1">
								<orgName type="department">Qatar Computing Research Institute</orgName>
								<orgName type="institution">HBKU</orgName>
								<address>
									<settlement>Doha</settlement>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,391.53,15.42;1,89.29,106.66,320.50,15.42">BeaSku at CheckThat! 2021: Fine-Tuning Sentence BERT with Triplet Loss and Limited Data</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">1F2AD46DAE3FD838FBB9C53BBFB23465</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>previously fact-checked claim retrieval</term>
					<term>sentence BERT</term>
					<term>fake news</term>
					<term>triplet loss</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Misinformation and disinformation are growing problems online. The negative consequences of the proliferation of false claims became especially apparent during the COVID-19 pandemic. Thus, there is a need to detect and to track false claims. However, this is a slow and time-consuming process, especially when done manually. At the same time, the same claims, with some small variations, spread simultaneously across many accounts and even on different platforms. One promising approach is to develop systems for detecting new instances of claims that have been previously fact-checked online, as in the CLEF-2021 CheckThat! Lab Task-2b. Here we describe our system for this task. We finetuned sentence BERT using triplet loss, and we experimented with two types of augmented datasets. We further combined BM25 scores with language model similarity scores as features in a reranker. The official evaluation results have put our BeaSku system at the second place.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As misinformation and disinformation has grown online, the need for reliable fact-checking has also grown. Consider for example the 2016 US Presidential Election, where fake news affected political decisions and opinions <ref type="bibr" coords="1,236.05,440.91,11.58,10.91" target="#b0">[1]</ref>. Since then, multiple initiatives and organisations (such as PolitiFact 1 in USA and EUvsDisinfo<ref type="foot" coords="1,262.13,452.70,3.71,7.97" target="#foot_0">2</ref> in the EU) have put efforts into fact-checking claims. According to the Duke Reporters' Lab, in 2020 the global number of fact-check organisations crossed 300 <ref type="foot" coords="1,141.80,479.80,3.71,7.97" target="#foot_1">3</ref> . These organisations use a variety of approaches, typically relying on manual verification of claims, and less often using partial or full automation. The former approach is accurate but time-consuming, while the latter is resource-intensive; thus, it might be a good idea to combine them.</p><p>One of the first key steps in the fact-checking pipeline is to check whether the claim was previously fact-checked. This step is especially important for efficiency and tracking, as the same claims are often repeated across platforms with little or no change in the content. Thus, before going for full fact-checking, it is useful to check whether the claim was verified before, which can make fact-checkers more efficient and could also help track the spread of false claims in different variations. Thus, verified claim retrieval could increase the impact of fact-checking without increasing the amount of manual work required.</p><p>The CLEF-2021 CheckThat! Lab featured a shared task designed to increase researchers' interest in verified claim retrieval. Task 2B of the lab is dedicated to determining whether a claim from a political debate or a speech was previously fact-checked. The task uses data from PolitiFact, which is one of the biggest fact-checking organizations. After a major political event (mostly in USA), PolitiFact verifies some of the claims that were made during the event. Often, they also link such claims to articles about already checked claims, e.g., when politicians keep repeating the same thing. The dataset thus contains a list of claims (Input) matched with previously fact-checked claims (VerClaim).</p><p>Below, we describe our approach to the task. We focused mainly on experimenting with triplet loss and other data augmentation methods to optimally exploit the provided data, and on fine-tuning sentence BERT. The produced cosine similarity scores between a candidate and previously fact-checked claims. We then used these features, together with BM25 scores, in a ranking model. Our submission was ranked second in the shared task, and our score was very close to that of the winner. The next sections provide more detail.</p><p>In the present work, we experiment with using a triplet loss representation of the data, both with the aim to increase the amount of information we can use to train our models, to deal with data imbalance, and to encourage the model to learn the subtleties of the data. The mathematical formula for triplet loss is presented in Equation 1. A triplet consist of an anchor (𝐴), plus a positive (𝑃 ) and a negative (𝑁 ) example. The training objective of the triplet loss is to increase the distance between an anchor and a negative example, while decreasing the distance between an anchor and a positive example. 𝛼 is a margin between these two distances and it a hyperparameter of the model.</p><formula xml:id="formula_0" coords="3,156.10,208.91,349.88,10.91">ℒ(𝐴, 𝑃, 𝑁 ) = 𝑚𝑎𝑥(‖𝑓 (𝐴) -𝑓 (𝑃 )‖ -‖𝑓 (𝐴) -𝑓 (𝑁 )‖ + 𝛼, 0)<label>(1)</label></formula><p>The triplet loss was first successfully implemented in computer vision for face recognition ( <ref type="bibr" coords="3,92.64,242.48,11.04,10.91" target="#b5">[6]</ref>), and has more recently also been applied in Natural Language Processing tasks <ref type="bibr" coords="3,481.23,242.48,11.48,10.91" target="#b6">[7,</ref><ref type="bibr" coords="3,496.18,242.48,7.65,10.91" target="#b7">8]</ref>. What is particularly interesting about triplet loss is that it is possible to create triplets in an online manner, i.e., after each training round, the next most difficult (as of yet unused) negative example can be chosen to create a triplet for training. In this way, the model can learn more nuanced representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data Preparation</head><p>The PolitiFact dataset for the CLEF-2021 CheckThat! lab task 2B consist of 700 claims (Input) from political debates or speeches, matched against 19,250 previously fact-checked claims (VerClaim). All the data for this subtask was in English (however, the lab featured tasks in five languages). Each VerClaim comes with a Title and a Body of an article discussing the veracity of the claim. The dataset is divided into training and development parts, with 561 input claims for training, and 139 for testing. In order to emulate the desired use of the system, the smaller set (which we treated as a test set) consists of events that happened later in time than the events that were used to produce input claims for the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Triplet Construction</head><p>The dataset is severely imbalanced: for each positive pair of Input-VerClaim there are almost 20,000 negative examples. Thus, the triplet loss can be used as a strategy to compensate for this imbalance when fine-tuning sentence BERT. We converted the training into a collection of triplets, i.e., an anchor, a positive, and a negative claim. The anchor is an Input claim, while the positive and the negative claims are taken from VerClaims. Positive claims matched to the anchor were already provided by the shared task organisers. For the negative claims, we ranked all the previously fact-checked claims against the anchor using the BM25 algorithm <ref type="bibr" coords="3,469.82,581.17,11.56,10.91" target="#b8">[9]</ref>. The highest scoring claim that was not a positive match to the anchor was chosen. The advantage of this method is that the difference between the positive and the negative claims in a triplet is not trivial, and it can really challenge the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dataset Augmentation</head><p>The triplet dataset is quite small: the training set contains just 561 input claims. Thus, we explored two methods for data augmentation. The key problem is the limited number of positive examples, and thus our goal was to extend those.</p><p>Potential sentences that could expand the narrow pool of matched claims were mined from the Title and the Body of the articles.</p><p>Our first method simply created additional triplets, where the positive example was the article title. Examples are shown in Table <ref type="table" coords="4,245.12,188.83,3.74,10.91" target="#tab_0">1</ref>. This doubled the size of the dataset to 1,124 triplets.</p><p>Our second augmentation method added new triplets with positives examples made of sentences in the body of the articles. Table <ref type="table" coords="4,261.29,215.93,4.97,10.91" target="#tab_0">1</ref> also shows examples of such artificial positives for this second approach. From the articles, we chose sentences that had the biggest cosine similarity between their embedding and the Input claim embedding, relying on the stsb-roberta-large model <ref type="foot" coords="4,119.45,254.82,3.71,7.97" target="#foot_3">5</ref> . Augmenting the training set with article sentences almost quadrupled (2,212 triplets) the number of triplets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Our experiments focused on fine-tuning sentence BERT. We used the sentence-transformers package <ref type="foot" coords="4,125.65,340.55,3.71,7.97" target="#foot_4">6</ref> . Following common practice <ref type="bibr" coords="4,262.70,342.30,11.46,10.91" target="#b2">[3]</ref>, we ran experiments using both a smaller language model (245M parameters: distilbert-base-nli-mean-tokens) and a larger one (1.31G parameters: stsb-roberta-large) <ref type="foot" coords="4,253.62,367.64,3.71,7.97" target="#foot_5">7</ref> . We used a training batch size of 30, warm up steps set to 10% of the total, and triplet margin of 5. With the distilled version of sentence-BERT, we trained for 5 epochs, and with RoBERTa, we trained for 8 epochs. For evaluation, fact-checked claims were ranked against input claims based on cosine similarity between their BERT embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Online Mining of Triplets</head><p>By choosing negative claims that are still quite similar to the anchor claim in our triplets, our negative claims are quite challenging. We can also use the learned representation to create additional challenging claims. After each training epoch, the Input claim and the collection of all (VerClaims) are again encoded by currently fine-tuned model. Because the model was trained for one epoch, the encoded representations are different than before that epoch. We then computed the cosine similarity between the Input claim and (VerClaims) again. Then we constructed new triplets, where we chose the most similar fact-checked claim that is not a positive as a new negative. These triplets replaced the ones from the previous epoch. Therefore, as the model learns the dataset better, we iteratively select the most challenging unmatching VerClaims. This is an online mining of triplets, and it yields consistent improvement of around 0.05 MAP points (Table <ref type="table" coords="4,198.97,595.27,3.65,10.91">2</ref>). Unfortunately it also substantially increases training time (from 8 minutes to around 30 minutes using Google Colab GPU<ref type="foot" coords="4,344.43,607.06,3.71,7.97" target="#foot_6">8</ref> and distilBERT). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>You were for a singlepayer system, a Canadian-style system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vclaim</head><p>Says Donald Trump is for a single-payer health care system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Title</head><p>Is Donald Trump still 'for single-payer' health care?</p><p>Top-3 sentences of Text 1. Perry said Trump is "for single-payer health care. ". 2. Fifteen years ago, Trump was decidedly for a universal healthcare system that resembled Canada's system, in which the government pays for care for all citizens. 3. "How can anyone who's a conservative stand up and say I am for single-payer health care?".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Model Robustness</head><p>Our experimental results with augmenting the triplet dataset were mixed. Augmenting with sentences from the articles actually resulted in performance that was worse than the baseline performance (i.e., not fine-tuned) achieved with distilBERT (see Table <ref type="table" coords="5,408.64,601.43,3.65,10.91">3</ref>). This might be due to the article sentences being more context-dependent and having more pronouns rather than named entities. Moreover, our inspection showed that the top-3 sentences did not always say exactly the same thing as the Vclaim. An example is the third sentence for the third input in Table <ref type="table" coords="5,116.93,655.62,3.81,10.91" target="#tab_0">1</ref>, where the augmenting claim is much more general and only vaguely refers to the former President of USA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Official Evaluation Results on the Test Set</head><p>Our official submission used rankSVM and the features described in the previous section. In the bottom half of Table <ref type="table" coords="8,181.24,124.83,5.01,10.91">5</ref> are shown the official evaluation results of our BeaSku team submission.</p><p>Our model was ranked second in the competition, just 0.001 points absolute behind the winning team, based on the official evaluation measure: MAP@5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 5</head><p>Evaluation scores for our official submission for the shared task.</p><p>Official evaluation results on the test set.</p><p>MAP@𝑘 MRR 1 3 5 10 20 1000</p><p>.320 .266 .308 .327 .332 .332 .333</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>We have described the submission of our BeaSku team for the CLEF-2021 CheckThat! Lab Task 2B on detecting previously fact-checked claims. Our main focus was on fine-tuning sentence BERT on the training dataset. For that, we used triplet loss and we augmented the dataset to make the model more robust. We then used scores and reciprocal ranks produced by this model, together with BM25 scores, to train a reranker. We further discussed a number of experiments and improvements introduced in the model such as online mining of triplets, different methods for data augmentation and various extensions of sentence BERT. Our official submission was ranked second in the competition with a MAP@5 score of 0.327. As dataset augmentation yielded only a small performance boost, increasing the dataset size is a good direction for future work. This can be done through adding paraphrases of VerClaims. There are not many satisfactory paraphrase generators, but fine-tuning GPT-2 has shown promising results <ref type="bibr" coords="8,202.74,483.93,16.32,10.91" target="#b9">[10]</ref>. Moreover, the additional features used in the reranker, such as matching against the article sentences or Natural Language Inference metrics, could be included when training the reranker.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,88.99,90.49,416.90,305.85"><head>Table 1</head><label>1</label><figDesc>Positive examples from our augmented dataset. Vclaim Hillary Clinton "agreed with (John McCain) on voting for the war in Iraq. " Obama is right: Clinton and McCain were on the same side in voting for the use of force in Iraq. 2. Clinton voted for the measure, as did McCain. 3. In making his argument, Obama attacked Clinton for voting with Republicans on national security issues.. . Input Governor Romney, I'm glad that you recognize that alQaida's a threat because a few months ago when you were asked, what's the biggest geopolitical threat facing America, you said Russia not alQaida, you said Russia. In the debate, Obama said Romney called Russia "the biggest geopolitical threat facing America. ". 2. He has called Russia the biggest geopolitical foe or enemy for the U.S., but he has said the biggest threat is Iran.. . 3. Blitzer asked Romney if he thought Russia is a bigger foe than Iran, China or North Korea.. .</figDesc><table coords="5,89.56,122.05,416.15,227.83"><row><cell>Input</cell><cell>She voted for the War in Iraq.</cell></row><row><cell>Title</cell><cell>Clinton and McCain had same vote on Iraq war</cell></row><row><cell cols="2">Top-3 sentences of Text 1. Vclaim "A few months ago when you were asked what's the biggest geopolitical threat</cell></row><row><cell></cell><cell>facing America, you said Russia. "</cell></row><row><cell>Title</cell><cell>Obama: Romney called Russia our top geopolitical threat</cell></row><row><cell>Top-3 sentences</cell><cell>1.</cell></row><row><cell>of Text</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="1,108.93,649.12,78.38,8.97"><p>http://euvsdisinfo.eu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="1,108.93,660.08,397.64,8.97;1,89.29,671.04,179.77,8.97"><p>The Duke Reporters' Lab keeps track of media verification initiatives since 2014: http://reporterslab.org/ fact-checking-count-tops-300-for-the-first-time/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2" coords="2,107.37,350.31,89.76,12.85;2,89.29,375.93,416.69,10.91;2,89.29,389.48,416.69,10.91;2,88.93,403.03,417.05,10.91;2,89.29,416.58,417.89,10.91;2,88.89,430.13,411.74,10.91;2,500.63,428.37,3.71,7.97;2,504.83,430.13,2.35,10.91;2,89.29,443.67,416.69,10.91;2,89.29,457.22,416.69,10.91;2,89.29,470.77,416.69,10.91;2,89.29,484.32,416.69,10.91;2,89.29,497.87,113.16,10.91;2,100.20,511.42,405.78,10.91;2,89.29,524.97,416.97,10.91;2,89.29,538.52,416.69,10.91;2,89.29,552.07,416.69,10.91;2,89.29,565.62,416.69,10.91;2,89.29,579.17,416.70,10.91;2,89.29,592.72,416.69,10.91;2,89.29,606.27,416.69,10.91;2,89.29,619.81,416.69,10.91;2,89.29,633.36,314.88,10.91;2,105.65,669.44,2.78,5.98;2,108.93,671.00,81.15,8.97"><p>Related WorkFact-checked claim matching has not received much attention from the NLP community, and has only recently been explored as a separate task. Research done within Shared Task 2 on Verified Claim Retrieval at CLEF 2020 CheckThat! Lab<ref type="bibr" coords="2,340.26,403.03,13.00,10.91" target="#b1">[2]</ref> helps define and refine the scope of claim matching. In this earlier shared task, the organizers provided a dataset from Snopes, where the focus was on tweets rather than on political debates. The baseline used Elasticsearch<ref type="bibr" coords="2,500.63,428.37,3.71,7.97" target="#b3">4</ref> , and achieved a MAP@5 of 0.609. Eight teams submitted systems using diverse approaches, and the top two teams exceeded a score of 0.9. The organizers then created, in a similar fashion another, a more challenging dataset based on data from PolitiFact<ref type="bibr" coords="2,372.02,470.77,11.28,10.91" target="#b2">[3]</ref>, for which the Elasticsearch baseline achieved only around 0.5 MAP. This latter dataset is an earlier version of the one used in this year's shared task.Shaar et al.<ref type="bibr" coords="2,152.13,511.42,12.75,10.91" target="#b2">[3]</ref> achieved their best performance on the Snopes and on the PolitiFact datasets by combining both semantic similarity and classic information retrieval techniques. Many of the high-performing models from the CheckThat! 2020 Lab shared task also developed novel methods to obtain both simple negative examples as well as challenging ones, which imitated positive examples well, so they can be used to make models more sensitive to semantic subtleties. For example, the UNIPI-NLE team<ref type="bibr" coords="2,294.94,579.17,12.98,10.91" target="#b3">[4]</ref> added negative candidates from the pool of Input-VerClaim pairs that reached high scores (but were wrong) in an information retrieval module, which was based on keyword matching. In another innovation, in the process of training, the Buster.AI team<ref type="bibr" coords="2,214.12,619.81,12.73,10.91" target="#b4">[5]</ref> looked for similar claims (but unmatching) as the ones that the model already predicted correctly and added them to the training data.<ref type="bibr" coords="2,105.65,669.44,2.78,5.98" target="#b3">4</ref> http://www.elastic.co/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="4,108.93,638.15,231.42,8.97"><p>http://huggingface.co/sentence-transformers/stsb-roberta-large</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="4,108.93,649.10,118.86,8.97"><p>http://www.sbert.net/index.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5" coords="4,108.93,660.06,298.41,8.97"><p>http://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6" coords="4,108.93,671.02,121.52,8.97"><p>http://colab.research.google.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7" coords="7,108.93,671.02,131.21,8.97"><p>https://pypi.org/project/rank-bm25/</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Comparison of triplet loss training with and without online mining when fine-tuning pre-trained language models of different sizes. The absence/presence of online mining in the model is indicated with no/yes after the model name.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>MAP@𝑘 HasPositives@𝑘 Using the article titles as additional positive examples to double the number of triplets (see Table <ref type="table" coords="6,117.33,475.14,4.25,10.91">3</ref>) did not bring the expected boost in performance on the test set: for some of the evaluation measures, there was a slight improvement, but for other, there was a decrease. This could be because article titles and previously fact-checked claims differ stylistically: titles are usually shorter, contain less quotations, and are much better formulated.</p><p>However, even though models trained only on the dataset provided by the task organisers and models trained on the dataset augmented with article titles both exhibited roughly similar performance, in closer analysis, we noticed that data augmentation did have a positive impact. We constructed a new test set consisting of input claims from the test set matched on the article titles instead of the corresponding fact-checked claims. The comparison of the performance is shown in Table <ref type="table" coords="6,175.65,597.08,3.81,10.91">4</ref>. There we can see that both fine-tuned models improve on both test sets. However the one trained on the augmented dataset performs considerably better. These results suggest that using the dataset extended with titles is better in abstracting from specific formulations of previously checked claims and is therefore more robust. In the final submission, the use of the RankSVM model further improved the results. The features used in training are not only concerned with VerClaims, but also with the Titles of the articles. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">RankSVM</head><p>As shown in <ref type="bibr" coords="7,149.82,455.47,11.58,10.91" target="#b2">[3]</ref>, strong results for the task of detecting previously fact-checked claims are achieved when combining BM25 features with similarity scores obtained from sentence BERT. The former method is good at detecting exact matches of words between compared claims, which is especially useful when the same phrases (e.g., named entities) are used. On the other hand, the latter method provides more semantic matching, grasping similarities of paraphrases.</p><p>In order to get the best of both worlds, we decided to train a rankSVM model with an RBF kernel using a pairwise loss. The features used were scores and reciprocal ranks from BM25 and from sentence BERT for matching input claims against VerClaims and against Titles, i.e., a total of eight features. We obtained the BM25 scores using the BM25Plus algorithm from the rank-bm25 9  Python package. The cosine similarity scores were produced by matching embeddings created by a distilBERT model fine-tuned with online mining on a dataset augmented with Triplets.</p><p>We trained the RankSVM reranker on the original unaugmented training dataset from Politi-Fact. The evaluation results of the reranking are shown in the top half of Table <ref type="table" coords="7,445.41,618.06,3.77,10.91">5</ref>. We can see that using rankSVM improved the scores by around 0.02 points absolute across all evaluation measures.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,112.66,569.65,393.33,10.91;8,112.66,583.20,185.88,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,232.29,569.65,218.03,10.91">Social media and fake news in the 2016 election</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Allcott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gentzkow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,460.42,569.65,45.57,10.91;8,112.66,583.20,101.94,10.91">Journal of Economic Perspectives</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="211" to="236" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,596.75,394.53,10.91;8,112.66,610.30,394.62,10.91;8,112.28,623.85,393.70,10.91;8,112.66,637.40,393.33,10.91;8,112.66,650.95,178.28,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,367.11,610.30,140.17,10.91;8,112.28,623.85,299.85,10.91">Overview of CheckThat! 2020: Automatic identification and verification of claims in social media</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hamdan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,438.55,623.85,67.43,10.91;8,112.66,637.40,393.33,10.91;8,112.66,650.95,91.11,10.91">Proceedings of the International Conference of the Cross-Language Evaluation Forum for European Languages, CLEF &apos;20</title>
		<meeting>the International Conference of the Cross-Language Evaluation Forum for European Languages, CLEF &apos;20</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="215" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,86.97,393.33,10.91;9,112.66,100.52,393.33,10.91;9,112.28,114.06,325.01,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,362.90,86.97,143.09,10.91;9,112.66,100.52,140.34,10.91">That is a known lie: Detecting previously fact-checked claims</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,284.17,100.52,221.81,10.91;9,112.28,114.06,227.70,10.91">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL &apos;20</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL &apos;20</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3607" to="3618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,127.61,395.17,10.91;9,112.66,141.16,394.52,10.91;9,112.66,154.71,121.88,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,319.42,127.61,188.41,10.91;9,112.66,141.16,389.98,10.91">UNIPI-NLE at CheckThat! 2020: Approaching fact checking from a sentence similarity perspective through the lens of transformers</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Passaro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondielli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Marcelloni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,112.66,154.71,73.85,10.91">Cappellato et al</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,168.26,393.64,10.91;9,112.66,181.81,309.13,10.91" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bouziane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Perrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cluzeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mardas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sadeq</surname></persName>
		</author>
		<title level="m" coord="9,371.72,168.26,134.57,10.91;9,112.66,181.81,277.21,10.91">Team Buster.ai at CheckThat! 2020: Insights and recommendations to improve fact-checking</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,195.36,393.33,10.91;9,112.66,208.91,393.33,10.91;9,112.66,222.46,128.87,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,283.05,195.36,222.94,10.91;9,112.66,208.91,61.70,10.91">FaceNet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,197.17,208.91,308.82,10.91;9,112.66,222.46,99.58,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;15</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;15</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,236.01,393.33,10.91;9,112.28,249.56,129.33,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,177.62,236.01,299.96,10.91">Intention detection based on siamese neural network with triplet loss</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,484.95,236.01,21.04,10.91;9,112.28,249.56,30.17,10.91">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="82242" to="82254" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,263.11,393.60,10.91;9,112.66,276.66,394.52,10.91;9,112.66,290.20,70.43,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,320.73,263.11,185.54,10.91;9,112.66,276.66,60.59,10.91">A metric learning approach to misogyny categorization</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Coria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ghannay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bredin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,196.04,276.66,305.86,10.91">Proceedings of the 5th Workshop on Representation Learning for NLP</title>
		<meeting>the 5th Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="89" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,303.75,393.72,10.91;9,112.66,317.30,90.59,10.91" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="9,229.60,303.75,248.66,10.91">The probabilistic relevance framework: BM25 and beyond</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Now Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,330.85,393.33,10.91;9,112.66,344.40,395.01,10.91;9,112.66,357.95,38.81,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,234.93,330.85,182.03,10.91">Paraphrasing with large language models</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Witteveen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Andrews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,440.33,330.85,65.65,10.91;9,112.66,344.40,256.99,10.91">Proceedings of the 3rd Workshop on Neural Generation and Translation</title>
		<meeting>the 3rd Workshop on Neural Generation and Translation<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="215" to="220" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
