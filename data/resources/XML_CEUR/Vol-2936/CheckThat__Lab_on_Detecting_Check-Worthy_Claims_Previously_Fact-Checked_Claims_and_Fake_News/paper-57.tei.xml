<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,402.17,15.42;1,89.29,106.66,383.46,15.42;1,89.29,128.58,225.83,15.43">Fight for 4230 at CheckThat! 2021: Domain-Specific Preprocessing and Pretrained Model for Ranking Claims by Check-Worthiness</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.90,156.89,58.98,11.96"><forename type="first">Xinrui</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<addrLine>Clearwater Bay</addrLine>
									<settlement>Kowloon</settlement>
									<country key="HK">Hong Kong</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,166.61,156.89,52.09,11.96"><forename type="first">Bohuai</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<addrLine>Clearwater Bay</addrLine>
									<settlement>Kowloon</settlement>
									<country key="HK">Hong Kong</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,256.98,156.89,63.27,11.96"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
							<email>pascale@ece.ust.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<addrLine>Clearwater Bay</addrLine>
									<settlement>Kowloon</settlement>
									<country key="HK">Hong Kong</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<country key="TW">Republic of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">CLEF</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,402.17,15.42;1,89.29,106.66,383.46,15.42;1,89.29,128.58,225.83,15.43">Fight for 4230 at CheckThat! 2021: Domain-Specific Preprocessing and Pretrained Model for Ranking Claims by Check-Worthiness</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">76F3187DE977D0906A22BFCA55E49CE6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>check-worthiness</term>
					<term>data preprocessing</term>
					<term>BERTweet</term>
					<term>distilRoBERTa</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The widespread dissemination of false news on social media has brought negative effects to society. In this paper, we describe a model submitted to the CLEF-2021 CheckThat! Task 1 -English to estimate the check-worthiness of tweets and political debates/speeches. Our official submission was ranked 2 ùëõùëë in subtask 1A with a MAP score of 0.195 and ranked 1 ùë†ùë° in subtask 1B with a MAP score of 0.402. The main challenges of the task 1 are the imbalanced data and the not standard texts of tweets. We did thorough data preprocessing and mainly focused on combining different pretrained models with a dropout layer and a dense linear layer. We explored and experimented with many combinations of different data preprocessing techniques and augmentation methods. We also tried extracting additional features from metadata and ensembling the best-performance models to further improve. We have developed a preprocessing procedure for tweets, and our experiments show that domain-specific preprocessing and pretrained models can significantly improve the performance. Finally, we submitted the result produced by the BERTweet model with extra dropout layer and classifier layer with preprocessed data for subtask 1A and RoBERTa model fine-tuned on tweets_hate_speech_detection dataset with extra dropout layer and classifier layer for subtask 1B.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Social media becomes increasingly popular for information seeking out and consumption due to its low cost, easy access and rapid dissemination of information, however, it also facilitates the release and dissemination of rumors and false information <ref type="bibr" coords="1,352.58,505.31,11.46,10.91" target="#b0">[1]</ref>. The detection of fake news on social media presents unique characteristics, and there are huge differences in content, format, and writing style, which makes the existing detection algorithms of traditional news media ineffective or inapplicable, thus posing new challenges to fake news detection <ref type="bibr" coords="1,429.93,545.96,11.27,10.91" target="#b0">[1]</ref>. This problem has become more serious and urgent during the epidemic. As the COVID-19 pandemic spread, social media played an important role in socializing, and also a quick channel to seek and share information about the diseases. This enabled an explosion of unchecked information and the spread of misinformation <ref type="bibr" coords="2,243.04,86.97,11.56,10.91" target="#b1">[2]</ref>. More than 400 people in Iran died from drinking toxic substances due to rumors that high-proof alcohol can cure COVID-19, which have widespread influence on social media <ref type="bibr" coords="2,205.08,114.06,11.45,10.91" target="#b2">[3]</ref>. The World Health Organization claimed that a massive amount of misleading information on social media during the epidemic had brought an 'infodemic' and severely threatened public health <ref type="bibr" coords="2,238.82,141.16,11.37,10.91">[4]</ref>. Thus, fast false claim identification has become a crucial and challenging task, especially in the epidemic period.</p><p>Furthermore, the content of false news related to the epidemic is updated and spread quickly on social media, while manual fact-checking is time-consuming and inefficient. Therefore, it is of great significance to carry out automated false news detection to reduce the human burden.</p><p>However, even with automated detection, we can't detect every single claim on social networks. In 2020, there were over 500 million tweets sent every day on average <ref type="bibr" coords="2,439.79,222.46,11.55,10.91" target="#b3">[5]</ref>. This poses the need to pre-filter and prioritizes what should be passed to following the fact-checking pipeline, which is namely the task of check-worthiness estimation. A leading contribution in check-worthiness estimation has been the CLEF CheckThat! Lab, which has set up the task of heck-worthiness estimation in the four years' editions. In this paper, we present our approaches in tackling subtask 1A and subtask 1B of the CLEF-2021 CheckThat! Lab in English <ref type="bibr" coords="2,457.28,290.20,13.31,10.91" target="#b4">[6]</ref>:</p><p>Subtask 1A -Check-worthiness of tweets: Given a topic and a stream of potentially related tweets, rank the tweets according to their check-worthiness for the topic.</p><p>Subtask 1B -Check-Worthiness of Debates/Speeches: Given a political debate/speech, produce a ranked list of its sentences, ordered by their check-worthiness. This is a ranking task.</p><p>To effectively address this challenge, we mainly focus on the pretrained models with a dropout layer and a dense linear layer, and also explored and experimented with many combinations of different data preprocessing and augmentation methods, with additional features and ensembling methods. The contributions of this paper are mainly from two aspects: we developed a useful automatic preprocessing procedure to effectively process tweets before analysis; Secondly, we show that domain-specific preprocessing and pretrained models can significantly improve the performance to filter out check-worthy claims.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>ClaimBuster system is one of the earliest end-to-end systems for check-worthiness estimation and fact-checking <ref type="bibr" coords="2,171.79,511.42,11.47,10.91" target="#b5">[7]</ref>. The ClaimBuster system is still ongoing and could detect claims worth checking on the live discourses, social media and news. It used various supervised learning methods, including Multinomial Naive Bayes Classifier (NBC), Support Vector Classifier (SVM) and Random Forest Classifier (RFC). Moreover, it used different features, such as wording embedding with Part-of-Speech (POS) tags, entity types, sentiment and length and other 100 most important features.</p><p>Another online system for check-worthiness detection is ClaimRank, which is trained on actual annotations and can work for various kinds of text <ref type="bibr" coords="2,347.38,606.27,11.43,10.91" target="#b6">[8]</ref>.</p><p>In CLEF2020 CheckThat! Lab competition task1, participants investigated more methods and models than the participants in Check That! Lab 2019. There were several models that have been used by the participants, such as BERT <ref type="bibr" coords="2,287.21,646.91,11.31,10.91" target="#b7">[9,</ref><ref type="bibr" coords="2,301.24,646.91,12.53,10.91" target="#b8">10,</ref><ref type="bibr" coords="2,316.49,646.91,12.53,10.91" target="#b9">11,</ref><ref type="bibr" coords="2,331.74,646.91,12.28,10.91" target="#b10">12]</ref>, RoBERTa <ref type="bibr" coords="2,394.99,646.91,16.38,10.91" target="#b11">[13,</ref><ref type="bibr" coords="2,414.08,646.91,12.53,10.91" target="#b12">14,</ref><ref type="bibr" coords="2,429.33,646.91,12.28,10.91" target="#b13">15]</ref>, BiLSTM <ref type="bibr" coords="2,486.95,646.91,16.19,10.91" target="#b14">[16]</ref>, CNN, Random Forest <ref type="bibr" coords="2,188.26,660.46,16.41,10.91" target="#b15">[17]</ref>, Logistic regression and SVM <ref type="bibr" coords="2,343.83,660.46,11.58,10.91" target="#b7">[9]</ref>. Many groups already combined other representations, such as FastText, GloVe, Dependencies, POS and Named entities. To deal with the problems coming from the limited amounts of training data, several groups attempted different external data and graph relations. According to the overviews of the task1 <ref type="bibr" coords="3,468.37,223.90,16.38,10.91" target="#b16">[18]</ref>, the top-ranked team, Accenture, used RoBERTa with extra mean pooling and dropout layer which were more useful than other data preprocessing.</p><p>It is worth mentioning that most of the above systems focused on tweets but we need to deal with both tweets, and speeches and debates. We retry some of the features of these systems, and we focus on the preprocessing techniques to fit the tweets and different combinations of various models and above features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset Analysis and Processing</head><p>For tweets on subtask 1A, to better understand the relationship between different features and corresponding check worthiness, we did the exploratory data analysis on the training data to explore the dataset. After that, we did data preprocessing according to the analyses to remove some useless words and modify some abbreviations. To deal with the limitation problem of the training data, we tried data augmentation techniques, such as back translation, to produce more data.</p><p>Then, we extracted the different features, such as Node2Vec word embeddings, text meta feature and sentence embeddings produced by different models for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Exploratory Data Analysis</head><p>In task 1A, organizers provided datasets of tweets collected from a variety of COVID-19-related topics. The selected tweets are manual annotated and considered as check-worthy if it contains a verifiable factual claim and also needs a professional fact-checker to verify.</p><p>We did an exploratory analysis on tweet metadata, including word_count, unique_word_count, stop_word_count, punctuation_count, china_mention_count, url_count, wuhan_mention_count, mean_word_length, char_count, hashtag_count, @_count, number_count, among which we found that most of the meta-features have information about a target, as have very different distributions for check-worthy and non-check-worthy tweets, such as stop_word_count, mean_word_length. These features might be useful in models. It looks like check-worthy tweets are written more formally with longer words compared to non-check-worthy tweets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data preprocessing</head><p>We applied different techniques to preprocess the raw tweet text and developed many handcrafted domain-specific dictionaries. Our preprocessing procedure includes the following processing rules and orders:</p><p>‚Ä¢ Normalize all punctuations to English ‚Ä¢ Clear entity reference ‚Ä¢ Remove all links ‚Ä¢ Clean punctuations, non_ASCII except emojis: In the BERTweet model, each emoji icon is translated into text strings as a word token using the emoji package <ref type="foot" coords="4,470.67,556.60,3.71,7.97" target="#foot_0">1</ref> . Emoji icons could reveal the sentence sentiment and are relevant to check-worthiness. ‚Ä¢ Expanding shortened quantities: The presence of quantities and numbers can influence the check-worthiness label. We replaced tokens such as 6m, 12k, 8wks with expanded quantities like 6 million, 12 thousand, 8 weeks. ‚Ä¢ Expand contractions: We used a handcrafted dictionary to expand contractions and correct misspelling in contractions, such as y'all, youve.</p><p>‚Ä¢ Unification of all coronavirus synonyms: The dataset contains different forms of hashtags that refer to COVID_19, such as #covid2019, #CoronaVirus, #coronavirus19, we unified all coronavirus synonyms to the term coronavirus, including different spelling such as #korona, #koronawirus. We also expand all word forms and hashtags that contain the word corona, such as replacing CoronaOutbreak with coronavirus outbreak. ‚Ä¢ Transform slang: The tweets dataset contains many slangs that can affect the semantic meaning of the sentences. We developed a handcraft dictionary to transform them into the form that models can recognize, such as transform w/o to without, lmao to laughing, RT to retweet. ‚Ä¢ Expand hashtags: Many hashtags are used directly as subjects or objects in tweets, therefore, expanding them can better help the model understand the semantics. We used a handcraft dictionary to expand them, such as POTUS to the president of the United States. We tried to use the wordninja package<ref type="foot" coords="5,336.77,250.51,3.71,7.97" target="#foot_1">2</ref> which splits hashtags into separate words based on probability. However, it has poor performance on our dataset since it can poorly identify which words need to be processed and could mistakenly split words like "the" to "t" and "he".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Data Augmentation</head><p>The size of training data is small and easily results in overfitting. To increase the amount of data and help reduce overfitting, we employ some data augmentation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Back Translation</head><p>We translate each tweet text to the temporary destination language (French and Dutch) and then translate back the previously translated text into English to produce new sentences with different expressions of the same meanings. After back translation, we have a training data size 3 times larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Synonym Replacement</head><p>We also use WordNet to identify relevant synonyms, and randomly select n words to replace them by their synonyms in the tweet text to produce new sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Features Extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1.">Word2vec Word Embedding (WE)</head><p>Word embeddings are able to catch semantic and syntactic features of words. Thus, we represent each sentence as the average vector of its words. We use word2vec models pretrained on Google news, which provides a vector size of 300.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2.">Text Meta Feature (TMF)</head><p>Metadata of tweets might be an indicator for check-worthy claims. We used the following information of tweets as features: word count, count of a hashtag, presence of a link, punctuation count.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3.">Sentence embedding (SE)</head><p>After getting the word embeddings from the BERT-based pretrained model, we used the original word embedding matrix, average embedding for word embeddings of all words and the concatenated embeddings of all words as the three different sentences embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Models</head><p>In this part, we used the word embeddings and other features with the various models to train the training data and test using the validation dataset. We compared different methods to get a whole performance picture about different natural language processing methods for check worthiness tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Bert-based classification model</head><p>BERT <ref type="bibr" coords="6,118.07,360.06,16.41,10.91" target="#b17">[19]</ref>, RoBERTa <ref type="bibr" coords="6,186.94,360.06,16.42,10.91" target="#b18">[20]</ref>, BERTWWM and BERTweet <ref type="bibr" coords="6,339.78,360.06,18.06,10.91" target="#b19">[21]</ref> models are the principal models that we have used to train the training dataset for subtask1A and the distilRoBERTa <ref type="bibr" coords="6,454.67,373.61,17.75,10.91" target="#b20">[22]</ref> models for subtask1B.</p><p>For BERT, we used the BERT-large pretrained model with 24 transformers, 1024 hidden sizes and 16 self-attention heads with totally 336M parameters on lower-case English texts. Similarly, the RoBERTa and BERTWWM also used the same architecture with the BERT-large pretrained model with 355M parameters and 336M parameters respectively.</p><p>BERTweet is the first public large-scale pretrained language model for tweets with the BERT BASE architecture and RoBERTa training procedure <ref type="bibr" coords="6,359.73,468.45,16.42,10.91" target="#b18">[20]</ref>. BERTweet produces better performance than the previous state-of-the-art models on POS tagging, named-entity recognition and text classification tasks on the English tweets. Therefore, our models mainly used the pretrained BERTweet model released by VinAI. And also, our final model is the BERTweet model with preprocessing for data for subtask 1A.</p><p>The distilRoBERTa is the distilled version of RoBERTa models, which is faster than the original RoBERTa model. And for subtask 1B, we mainly used the distilroberta-finetunedtweets-hate-speech model which is the distilroberta-base model architecture fine-tuned on the tweets-hate-speech dataset released by mrm8488.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ensembling Models</head><p>For ensembling different natural language processing tools, we also tried several ensembling models. The first model is the combination of 4 top models with voting or weights. Second, we fed the sentence embedding got from the BERTweet model into the AdaBoost regressor <ref type="bibr" coords="6,487.92,653.67,18.06,10.91" target="#b21">[23]</ref> or logistic regression <ref type="bibr" coords="6,185.70,667.22,16.30,10.91" target="#b22">[24]</ref>. The third one is that we put prediction values from the Bert-based classification model and metadata as features and combined them with sentence embeddings as the new sentence representation then fed the new embeddings into the AdaBoost regressor, logistic regression or SVM <ref type="bibr" coords="7,209.45,354.11,16.25,10.91" target="#b23">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this part, our projects will present the experiments that have been done for subtask1A. The results will include the measurement of precision@K (K is 3, 5, 10) and Mean Average Precision (MAP) comparison among two original baselines and different models, followed by the analyses for the improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experiments for Subtask 1A</head><p>For subtask1A, our experiments can be divided into 3 parts, including, the comparison among BERT-based models with one dropout and classifier layer, all BERT-based models with KFold algorithm, and different ensembling models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">BERT-based Models</head><p>In this part, we used the original BERT-large model, BERTWWM, RoBERTa-large, and BERTweet models with one extra dropout layer and one classifier layer which is a dense linear layer. Moreover, we also trained the models with preprocessed data and augmented datasets.</p><p>Originally, there are two official baselines, Random baseline, and Ngram baseline which use random guess and ngram prediction respectively. Table <ref type="table" coords="7,342.16,633.74,5.15,10.91" target="#tab_1">2</ref> shows the MAP results of Random Baseline and Ngram Baseline. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experiments for Subtask 1B</head><p>For subtask 1B, according to the experiments for subtask 1A, we simply used the distilrobertafinetuned-tweets-hate-speech model with one dropout layer and one classifier layer. And the Table <ref type="table" coords="9,115.79,346.33,5.07,10.91" target="#tab_3">6</ref> shows the average MAP for 9 different speeches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results and Discussion</head><p>According to Table <ref type="table" coords="9,179.93,404.96,3.81,10.91" target="#tab_2">3</ref>, it cannot be denied that BERT-based models have a strong ability to deal with the classification task. Among BERT, BERTWWM, RoBERTa and BERTweet models, RoBERTa, BERTWWM and BERTweet are more powerful than the basic BERT model. It is due to the development of the masking pattern used by RoBERTa and BERTWWM, and the domain-specific pretraining by the BERTweet model. And also, through the experiments of using augmented data and preprocessed data, both the augmentation and preprocessing can help the models to better understand the training data, especially the preprocessing, however, the Text Meta Features (TMFs) are not very effective. After comparing the Table <ref type="table" coords="9,224.81,513.35,5.17,10.91" target="#tab_2">3</ref> and Table <ref type="table" coords="9,281.04,513.35,3.81,10.91">4</ref>, the Kfold for training can improve most of the BERT-based models, although the improvement for the BERTweet model with preprocessed data is not large. But Kfold can still be used for a limited dataset with feature fine-tuning.</p><p>According to the comparison between Table <ref type="table" coords="9,301.04,554.00,5.11,10.91">5</ref> and Table <ref type="table" coords="9,355.15,554.00,3.77,10.91" target="#tab_2">3</ref>, it shows that one dropout layer with one classifier layer is more useful than a simple adaptive boosting algorithm or logistic regression. The experiments showed that the model with the highest accuracy will dominate other models. Therefore, the ensembling model will have higher accuracy if the weight assigned to the highest model is larger.</p><p>Final models for subtask 1A and subtask 1B submitted are shown in Table <ref type="table" coords="9,431.07,621.74,3.74,10.91">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion and Future Work</head><p>In this paper, we present our models and efforts in Task 1 of CLEF2021 Check That! Lab. For subtask 1A, we used three main methods, BERT-based models with extra dropout layer and classifier layer, KFold algorithm and ensembling models. We adopted various data preprocessing and augmentation techniques to help the system improve the accuracy. The main contributions of this paper are: firstly, the development of a useful automatic preprocessing procedure to effectively process tweets before analysis; Secondly, we show that domain-specific preprocessing and pretrained models can significantly improve the performance to filter out check-worthy claims.</p><p>In the final submission of Subtask 1a, our final system is the BERTweet model with one dropout layer and one classifier layer without KFold algorithm on the preprocessed training dataset, which eventually ranked 2 ùëõùëë (out of 9 groups) based on the official evaluation metric. For subtask 1B, we used the distilRoBERTa-finetuned-tweets-hate-speech model followed by one dropout layer and one classifier layer, which finally ranked 1 ùë†ùë° based on the official evaluation metric.</p><p>In future work, we plan to experiment with more ensembling techniques as well as with more extra features such as sentence sentiments, POS tags, social characteristics on tweets like the number of retweets, likes and so on.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,380.36,417.79,8.93;4,89.29,392.36,191.86,8.87;4,89.29,84.19,416.70,277.63"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Distributions of word_count, unique_word_count, stop_word_count, punctuation_count, china_mention_count and mean_word_length.</figDesc><graphic coords="4,89.29,84.19,416.70,277.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.99,90.49,281.48,79.60"><head>Table 1 Information of Data in Subtask 1A Dataset Total Check-worthy</head><label>1</label><figDesc></figDesc><table coords="3,231.15,137.31,113.82,32.78"><row><cell>Train</cell><cell>822</cell><cell>290</cell></row><row><cell>Dev</cell><cell>140</cell><cell>60</cell></row><row><cell>Test</cell><cell>350</cell><cell>19</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,88.99,90.49,262.95,69.87"><head>Table 2</head><label>2</label><figDesc>Original Baselines</figDesc><table coords="7,243.33,122.05,108.61,38.30"><row><cell>Model</cell><cell>MAP</cell></row><row><cell cols="2">Random Baseline 0.4795</cell></row><row><cell>Ngram Baseline</cell><cell>0.5916</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,88.99,182.61,337.12,117.69"><head>Table 3</head><label>3</label><figDesc>BERT-based Models With One Extra Dropout And Classifier Layer.</figDesc><table coords="7,169.17,214.18,256.95,86.13"><row><cell>Model</cell><cell cols="4">MAP P@3 P@5 P@10</cell></row><row><cell>BERT</cell><cell>0.7074</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>BERTWWM</cell><cell>0.8030</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>RoBERTa</cell><cell>0.7765</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>BERTweet</cell><cell>0.8136</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell cols="2">BERTweet w/ preprocessed 0.8753</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>BERTweet w/ augmented</cell><cell>0.8205</cell><cell>1</cell><cell>1</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,88.99,90.49,418.34,181.48"><head>Table 6</head><label>6</label><figDesc>DistilRoBERTa for Subtask 1B</figDesc><table coords="9,88.99,119.83,418.34,152.13"><row><cell></cell><cell>Model</cell><cell>MAP</cell></row><row><cell></cell><cell cols="2">distilroberta + dropout + classifier 0.1696</cell></row><row><cell>Table 7</cell><cell></cell><cell></cell></row><row><cell cols="2">Final Models for Subtask 1A and 1B</cell><cell></cell></row><row><cell cols="2">Tasks Subtask 1A</cell><cell>Subtask 1B</cell></row><row><cell cols="3">Model BERTweet + Dropout + Classfier w/ preprocessed data distilroberta + dropout + classifier</cell></row><row><cell>MAP</cell><cell>0.195</cell><cell>0.402</cell></row><row><cell>P@3</cell><cell>0.333</cell><cell>0.833</cell></row><row><cell>P@5</cell><cell>0.400</cell><cell>0.750</cell></row><row><cell cols="2">P@10 0.400</cell><cell>0.600</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,108.93,671.02,157.24,8.97"><p>Available at https://pypi.org/project/emoji/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,108.93,670.95,174.40,8.97"><p>Available at https://pypi.org/project/wordninja/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research is partially supported by <rs type="person">Yu Tiezheng</rs>, <rs type="person">Dai Wenliang</rs> and <rs type="person">Ji Ziwei</rs>. And also, <rs type="person">Zhou Zhuorui</rs> supports one of the ensembling models and few ideas about the processing.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In Table <ref type="table" coords="8,140.11,458.52,3.81,10.91">3</ref>, for each model, we trained 3 epochs with 16 batch size and 128 max sequence length. The learning rates we have used are 3e-5 and 5e-5. According to the experiments, the BERTweet model with one dropout layer and one classifier layer achieves the highest accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">KFold BERT-based Models</head><p>According to Table <ref type="table" coords="8,174.88,534.95,3.67,10.91">4</ref>, we used 20 splits of StratifiedKFold to train the same BERT-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3.">Ensembling Models</head><p>Table <ref type="table" coords="8,115.78,584.28,5.07,10.91">5</ref> shows the several methods of ensembling. We used BERTWWM sequence embedding with mean or concatenation method with AdaBoost regressor. Also, we put other text metafeatures. Also node2vec was combined into a logistic regression model. Moreover, we tried to consider the prediction value as a novel feature and fed it into the LinearSVC with the node2vec sequence embedding. Finally, we combined some models with different voting weights.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,112.66,458.87,393.33,10.91;10,112.66,472.42,337.29,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,303.46,458.87,202.53,10.91;10,112.66,472.42,83.30,10.91">Fake news detection on social media: A data mining perspective</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,204.32,472.42,171.84,10.91">ACM SIGKDD explorations newsletter</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="22" to="36" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,485.97,393.33,10.91;10,112.66,499.52,385.34,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,260.86,485.97,245.12,10.91;10,112.66,499.52,171.59,10.91">An exploration of how fake news is taking over social media and putting public health at risk</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">B</forename><surname>Naeem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bhatti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,292.03,499.52,174.05,10.91">Health Information &amp; Libraries Journal</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,513.06,393.33,10.91;10,112.66,526.61,394.04,10.91;10,112.66,540.16,299.55,10.91" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Trew</surname></persName>
		</author>
		<ptr target="https://www.independent.co.uk/news/world/middle-east/iran-coronavirus-methanol-drink-cure-deaths-fake-a9429956.html" />
		<title level="m" coord="10,161.42,513.06,344.57,10.91;10,112.66,526.61,77.78,10.91">Hundreds dead in iran from drinking methanol amid fake reports it cures coronavirus</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,580.81,394.04,10.91;10,112.66,594.36,115.33,10.91" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="10,157.47,580.81,173.57,10.91">The number of tweets per day in 2020</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Sayce</surname></persName>
		</author>
		<ptr target="https://www.dsayce.com/social-media/tweets-day/" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,607.91,393.32,10.91;10,112.66,621.46,394.53,10.91;10,112.66,635.01,393.33,10.91;10,112.66,648.56,393.33,10.91;10,112.66,662.11,310.90,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,112.66,635.01,393.33,10.91;10,112.66,648.56,123.26,10.91">Overview of the CLEF-2021 CheckThat! lab task 1 on check-worthiness estimation in tweets and political debates</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hamdan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">K</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">A</forename><surname>Yavuz Selim Kartal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barr√≥n-Cede√±o</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>M√≠guez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,259.42,648.56,246.57,10.91;10,112.66,662.11,150.24,10.91">Working Notes of CLEF 2021-Conference and Labs of the Evaluation Forum, CLEF &apos;2021</title>
		<meeting><address><addrLine>Bucharest, Romania (online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,86.97,394.53,10.91;11,112.28,100.52,393.70,10.91;11,112.66,114.06,300.45,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,263.56,100.52,242.42,10.91;11,112.66,114.06,28.69,10.91">Claimbuster: The first-ever end-to-end fact-checking system</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Caraballo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gawsane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Nayak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,150.06,114.06,168.97,10.91">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1945" to="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,127.61,393.32,10.91;11,112.66,141.16,368.63,10.91" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Jaradat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gencheva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barr√≥n-Cede√±o</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>M√†rquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07587</idno>
		<title level="m" coord="11,408.35,127.61,97.64,10.91;11,112.66,141.16,186.58,10.91">Claimrank: Detecting check-worthy claims in arabic and english</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,154.71,395.17,10.91;11,112.66,168.26,393.33,10.91;11,112.66,181.81,107.17,10.91" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Cheema</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hakimov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ewerth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10534</idno>
		<title level="m" coord="11,291.22,154.71,216.61,10.91;11,112.66,168.26,315.08,10.91">Check_square at checkthat! 2020: Claim detection in social media via fusion of transformer and syntactic features</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,195.36,394.61,10.91;11,112.66,208.91,393.33,10.91;11,112.66,222.46,231.90,10.91" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Alkhalifa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Yoong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kochkina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zubiaga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Liakata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.13160</idno>
		<title level="m" coord="11,378.78,195.36,128.49,10.91;11,112.66,208.91,393.33,10.91;11,112.66,222.46,49.67,10.91">Qmul-sds at checkthat! 2020: determining covid-19 tweet check-worthiness using an enhanced ct-bert with numeric expressions</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,236.01,393.33,10.91;11,112.66,249.56,248.78,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,211.68,236.01,294.31,10.91;11,112.66,249.56,118.21,10.91">Tobb etu at checkthat! 2020: Prioritizing english and arabic claims based on check-worthiness</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Kartal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,239.56,249.56,73.85,10.91">Cappellato et al</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,263.11,393.33,10.91;11,112.66,276.66,187.22,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,295.54,263.11,210.45,10.91;11,112.66,276.66,57.32,10.91">Uaics at checkthat! 2020: Fact-checking claim prioritization</title>
		<author>
			<persName coords=""><forename type="first">C.-G</forename><surname>Cusmuliuc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-G</forename><surname>Coca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,178.00,276.66,73.85,10.91">Cappellato et al</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,290.20,393.32,10.91;11,112.66,303.75,393.57,10.91;11,112.33,317.30,29.19,10.91" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Novak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.02431</idno>
		<title level="m" coord="11,273.61,290.20,232.37,10.91;11,112.66,303.75,244.33,10.91">Accenture at checkthat! 2020: If you say so: Post-hoc fact-checking of claims using transformer-based models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,330.85,394.62,10.91;11,112.66,344.40,393.58,10.91;11,112.33,357.95,29.19,10.91" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">D S</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Koychev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.02931</idno>
		<title level="m" coord="11,350.37,330.85,156.91,10.91;11,112.66,344.40,248.33,10.91">Team alex at clef checkthat! 2020: Identifying check-worthy tweets with transformer models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,371.50,393.65,10.91;11,112.66,385.05,393.33,10.91;11,112.66,398.60,141.93,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,411.99,371.50,94.31,10.91">Ssn nlp at checkthat!</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sachin Krishan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kayalvizhi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Thenmozhi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Rishi</forename><surname>Vardhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,139.94,385.05,366.04,10.91;11,112.66,398.60,110.01,10.91">Tweet check worthiness using transformers, convolutional neural networks and support vector machines</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,412.15,393.71,10.91;11,112.66,425.70,393.33,10.91;11,112.66,439.25,177.63,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,333.26,412.15,173.11,10.91;11,112.66,425.70,393.33,10.91;11,112.66,439.25,47.16,10.91">Nlp&amp;ir@ uned at checkthat! 2020: A preliminary approach for check-worthiness and claim retrieval tasks using neural networks and graphs</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martinez-Rico</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martinez-Romo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,168.42,439.25,73.85,10.91">Cappellato et al</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,452.79,394.53,10.91;11,112.33,466.34,393.65,10.91;11,112.66,479.89,157.94,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,112.33,466.34,393.65,10.91;11,112.66,479.89,28.12,10.91">The university of sheffield at checkthat! 2020: Claim identification and verification on twitter</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hampson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Leidner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,148.72,479.89,73.85,10.91">Cappellato et al</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,493.44,394.53,10.91;11,112.66,506.99,394.62,10.91;11,112.28,520.54,393.71,10.91;11,112.33,534.09,29.19,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,339.78,506.99,167.50,10.91;11,112.28,520.54,293.90,10.91">Overview of checkthat! 2020 english: Automatic identification and verification of claims in social media</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barr√≥n-Cedeno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Martino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,415.08,520.54,74.62,10.91">Cappellato et al</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,547.64,393.33,10.91;11,112.66,561.19,395.01,10.91;11,112.66,574.74,187.21,10.91" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="11,319.43,547.64,186.56,10.91;11,112.66,561.19,180.57,10.91">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1810.04805" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,588.29,394.53,10.91;11,112.30,601.84,393.68,10.91;11,112.66,615.39,107.17,10.91" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m" coord="11,173.53,601.84,256.77,10.91">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,628.93,393.53,10.91;11,112.66,642.48,395.01,10.91;11,112.66,658.47,97.35,7.90" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="11,296.08,628.93,210.10,10.91;11,112.66,642.48,65.32,10.91">Bertweet: A pre-trained language model for english tweets</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="2005.10200" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,669.58,394.52,10.91;12,112.66,86.97,395.01,10.91;12,112.66,100.52,127.84,10.91" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="11,298.66,669.58,208.52,10.91;12,112.66,86.97,116.06,10.91">Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1910.01108" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,114.06,395.17,10.91;12,112.66,127.61,395.01,10.91;12,112.66,141.16,222.14,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="12,260.57,114.06,247.26,10.91;12,112.66,127.61,18.79,10.91">Adaboost. rt: a boosting algorithm for regression problems</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Solomatine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">L</forename><surname>Shrestha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,180.03,127.61,327.63,10.91;12,112.66,141.16,45.96,10.91">IEEE International Joint Conference on Neural Networks (IEEE Cat. No. 04CH37541</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1163" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,154.71,178.85,10.91" xml:id="b22">
	<monogr>
		<title level="m" type="main" coord="12,173.44,154.71,81.75,10.91">Logistic regression</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,168.26,393.33,10.91;12,112.66,181.81,258.44,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="12,370.31,168.26,106.12,10.91">Support vector machines</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Osuna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,484.95,168.26,21.04,10.91;12,112.66,181.81,184.65,10.91">IEEE Intelligent Systems and their applications</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="18" to="28" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
