<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,411.81,15.42;1,89.29,106.66,254.00,15.42">Nkovachevich at CheckThat! 2021: BERT fine-tuning approach to fake news detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,89.29,134.97,95.47,11.96"><forename type="first">Ninko</forename><surname>Kovachevich</surname></persName>
							<email>n.kovachevich@mail.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Sofia University</orgName>
								<orgName type="institution" key="instit2">&quot;St. Kliment Ohridski&quot;</orgName>
								<address>
									<addrLine>bul. &quot;Tsar Osvoboditel&quot; 15, Center</addrLine>
									<postCode>1504</postCode>
									<settlement>Sofia, Sofia</settlement>
									<country key="BG">Bulgaria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,411.81,15.42;1,89.29,106.66,254.00,15.42">Nkovachevich at CheckThat! 2021: BERT fine-tuning approach to fake news detection</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">5A58AC8186E00501CDE9393683117FEF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CLEF CheckThat</term>
					<term>fake news</term>
					<term>classification</term>
					<term>BERT</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The success of a text classification approach depends to a large extent on the data that it is trained on. The adaptation of a model with thousands of weights, such as BERT, usually requires large amount of data. CLEF 2021 CheckThat! Lab for fake news detection offers a challenging multi-class task with relatively small data set to train on. The experiments, which include BERT fine-tuning, together with a couple of simpler algorithms, produce average results, and hardly overcome model and data size limitations. Nevertheless, they show a potential to be successful if implemented properly.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This year's edition of CLEF CheckThat! Lab <ref type="bibr" coords="1,281.68,356.57,12.69,10.91" target="#b0">[1]</ref> <ref type="bibr" coords="1,294.36,356.57,12.69,10.91" target="#b1">[2]</ref>[3] consists of three tasks, which if connected, could represent a comprehensive approach to the verification of a news article.</p><p>1. Check-Worthiness Estimation 2. Detecting Previously Fact-Checked Claims 3. Consists of two sub-tasks <ref type="bibr" coords="1,227.57,418.73,13.11,10.91" target="#b3">[4]</ref>[5]:</p><p>• Sub-task A -multi-class fake news detection of news articles • Sub-task B -topical domain classification of news articles This paper describes developments in the third of the above tasks. The provided data consists of the headline and text of the news, with separate sets for each of the sub-tasks. The classes in which a news item may fall are the following:</p><p>• Sub-task A -true, false, partially false, other • Sub-task B -health, crime, economy, elections, climate, education</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Early methods for recognizing fake news are based on statistical and linguistic text characteristics. Gravanis et al. <ref type="bibr" coords="1,198.38,605.68,12.95,10.91" target="#b5">[6]</ref> make a comparison of such early methods, with emphasis on the features that are extracted from text. Some of the feature categories are language complexity, expressiveness, affect, bias, etc. These approaches rely on the idea that the writing style differs in fake news. The authors compare various classification algorithms on different fake news data sets in a bid to provide a benchmark for the task. The reported accuracy results show significant dependence on data set size. The best performing algorithms, SVM and AdaBoost, classify over 90% of the news articles successfully if there are at least few thousand samples, while this percentage is between 60 and 70 if the text documents are just few hundreds.</p><p>In modern methods the content of the text is used. Jwa et al. <ref type="bibr" coords="2,358.28,181.81,12.68,10.91" target="#b6">[7]</ref> propose a BERT model, trained on CNN and Daily Mail news articles, and subsequently applied to the FNC-1 challenge <ref type="bibr" coords="2,489.60,195.36,13.55,10.91" target="#b7">[8]</ref>. Although the FNC-1 challenge is not a straight classification and is actually a different task that involves title to body stance detection, the described approach and the data set itself can be leveraged in other fake news detection problems.</p><p>Nasir et al. <ref type="bibr" coords="2,151.54,249.56,12.88,10.91" target="#b8">[9]</ref> apply a combination of convolutional and recurrent neural networks whose inputs are popular word embedding such as GloVe <ref type="bibr" coords="2,314.87,263.11,18.98,10.91" target="#b9">[10]</ref>. Their best performing hybrid model consists of six layers, which include embedding, 1-D convolution, 1-D max pooling and LSTM. They validate their model on two differently sized data sets, reporting F1-score of nearly 99% for the big one and 60% for the small one. One thing to note is that these data sets represent a binary classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Training data and working environment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Sub-task A data</head><p>900 news items are provided, each consisting of a headline and text content, distributed in four classes of misinformation <ref type="bibr" coords="2,224.36,410.45,19.15,10.91" target="#b10">[11]</ref>. About 52 percent of the items are labelled as FALSE, i.e. real news, and about 16 percent are labelled as TRUE. The rest are 24 percent and 8 percent, respectively for PARTIALLY FALSE and OTHER.</p><p>The articles are of different length, with most of them between 100 and 1000 words. Figure <ref type="figure" coords="2,501.24,451.09,5.00,10.91" target="#fig_0">1</ref> shows the relationship between the articles and their length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Sub-task B data</head><p>318 articles that are a subset of those in sub-task A, additionally labelled by topic domain. The largest group is health articles, about 40 percent, and the smallest is education -8 percent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Working environment</head><p>The developments are done using Google Colaboratory Python IDE that provides about 15GB of GPU, which is important for BERT models application. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation details 4.1. Articles title use</head><p>The best option is to determine the stance between the text of the news article and its title and then use this as a starting point or a parameter for the classification. This is not done because a separate classifier must be trained on a specifically labelled set of data, such as FNC-1 <ref type="bibr" coords="3,487.78,416.74,14.91,10.91" target="#b7">[8]</ref>. Therefore, this issue is addressed separately for each of the approaches, usually experimenting with both options -text contents only, or concatenation of article and text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Including additional training data</head><p>Considering that the available training data set is small, additional sources of similar fake news challenges can be included. Unfortunately, with these sources the prediction task is different, usually a binary classification. Given that the evaluation metric is F1-macro, which does not take label imbalance into account, changing the original class distribution during training and thus suppressing the minor class, seriously degrades the evaluation score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training data split</head><p>Although the data set is small, the separation is static without cross-validation. 10 percent is set aside for testing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Developments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Baseline -Naive Bayes</head><p>The text is cleared of everything possible (stop words, numbers, punctuation, etc.) and stemming is applied to the words. In sub-task A, the title is used, while in sub-task B it is not.</p><p>Multinomial Naïve Bayes classification is applied. Before that, the documents are converted to TF-IDF vectors and only the most frequent stems are used for vocabulary. For sub-task A, limiting to the most common 500 stems surprisingly, given the simplicity of the method, leads to some meaningful results. For sub-task B, the optimal value is 200. Only single stems, i.e. uni-grams, are considered.</p><p>The results for sub-task A are shown in Figure <ref type="figure" coords="4,309.87,465.03,3.73,10.91" target="#fig_1">2</ref>. The predictions are not really diverse and the score is quite poor -F1-macro is 0.39. Nevertheless, the news articles declared by the model to be fake are in fact such. In addition, some meaning can be sought in the wrong cases. For example, the four articles classified as partially false, that are truly fake. The opposite case is an article that is classified as fake, but in fact is partially untrue. All this comes as a prove that there is some underlying logic in the text, which if extracted, can be used to judge the authenticity of a news article.</p><p>For sub-task B, the results are much closer to the goal and the value for the F1-macro metric is 0.81, which is good enough, considering how simple the model is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Word embedding with GloVe</head><p>The idea is to get a vector with some dimensionality for each word in the text. The word vectors can then be:</p><p>• Averaged and passed as parameters to a classification algorithm.</p><p>• Used in raw form by specifying a fixed number of words for all text articles. Thus, the long articles are cut and the short ones are complemented with padding. In this form, the vectors are fed to a neural network in which the first layer is an Embedding layer.</p><p>An obvious disadvantage is that information is lost in texts longer than the selected number of words. On the other hand, a higher number means more weights need to be learned by the neural network.</p><p>GloVe <ref type="bibr" coords="5,124.94,175.74,19.80,10.91" target="#b9">[10]</ref> provides pre-trained models that differ in output vector size and training source. The one used in the experiments, glove-wiki-gigaword-100, provides word embedding with size 100 and is trained on Wikipedia and Gigaword text corpus. The experiments produce the following results:</p><p>• When averaging the vectors, XGBoost <ref type="bibr" coords="5,287.02,237.42,21.26,10.91" target="#b11">[12]</ref> with tree-based booster and 200 estimators gives 0.47 and 0.80 for the two sub-tasks, respectively. • By training a neural network for 8 epochs with the vectors of the first 1000 words of each article, F1-macro for sub-task A is 0.51. The network architecture has the Embedding layer and two LSTM layers, consisting of 64 and 32 neurons. For optimizer is used Adam with learning rate of 0.01 and categorical cross-entropy as a loss function.</p><p>For sub-task B none of the tested network architectures returns a good result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">BERT fine-tuning</head><p>The pre-trained BERT models combine word embedding and a multilayer neural network with weights trained on a huge corpus of texts. The fine-tuning of such a model basically means several epochs of additional training on texts for the specific classification task. The authors of BERT recommend this to be done in two, three, or four epochs <ref type="bibr" coords="5,364.82,409.55,18.65,10.91" target="#b12">[13]</ref>.</p><p>The Hugging Face implementation <ref type="bibr" coords="5,250.25,423.10,19.33,10.91" target="#b13">[14]</ref> is used and in particular experiments were performed with:</p><p>• BertForSequenceClassification -implementation of the original BERT <ref type="bibr" coords="5,423.04,457.68,21.71,10.91" target="#b12">[13]</ref> model with a linear classifier on top of it. • RobertaForSequenceClassification -implementation of RoBERTa <ref type="bibr" coords="5,397.42,485.71,21.52,10.91" target="#b14">[15]</ref> with a linear classifier on top of it.</p><p>A major limitation of this approach is that these models can ingest up to 512 tokens as input. In addition, they are so large that the resources in Google Colaboratory are not enough to process more than about 400 for batch size of 10 text documents. However, in case of small number of training documents, even this number of input tokens could be too high to be able to achieve decent classification results.</p><p>One decision that can be made is where to cut those 400 tokens from -beginning of text, the middle part, or the end. Experiments show that cutting from the beginning produces better results than the other two options.</p><p>Applying BertForSequenceClassification to sub-task A produces F1-macro value of 0.46 with 300 tokens as input and training for 4 epochs. It is somewhat surprising that there are almost no mistakes in news longer than 1000 tokens. Prediction results by text length are shown in Figure <ref type="figure" coords="5,120.36,669.58,3.74,10.91" target="#fig_2">3</ref>.  With the difference that it requires more epochs for training, RobertaForSequenceClassification does not show significant improvement.</p><p>For sub-task B, despite the limited amount of sample news articles, the pre-training of the BERT models provides good enough background for classification, and the results are satisfactory. Again, only the first 300 tokens are used, with 3 learning epochs. The F1-macro score is 0.93. Results' visualization is shown in Figure <ref type="figure" coords="6,271.19,660.00,3.74,10.91" target="#fig_3">4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results and conclusion</head><p>Table <ref type="table" coords="7,116.36,294.16,5.17,10.91" target="#tab_0">1</ref> shows a summary of all the described approaches to the task. Although for sub-task A the expectations for decent results using BERT were not met, this model generalized better. Applying it to the labelled test set after the competition ended, the results were close to those obtained during training, while the other methods's score was down by a margin.</p><p>The main direction of further development is the inclusion of article titles as an input. For the purpose, a separate classifier can be trained on FNC-1 data.</p><p>Another development direction is to divide longer texts into paragraphs or sentences thus supplying the BERT model with text in pieces. On the one hand all the information from the text will be used, but on the other hand valuable context can be lost.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,306.75,307.26,8.93;3,155.39,84.19,282.00,210.00"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Number of articles according to their length, measured in words.</figDesc><graphic coords="3,155.39,84.19,282.00,210.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.29,265.62,418.35,8.93;4,89.29,277.62,416.70,8.87;4,89.29,289.58,218.50,8.87;4,196.39,84.19,200.00,174.00"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Confusion matrix for the results of applying Multinomial Naïve Bayes classification on TF-IDF presentation of articles, with a limit of 500 most common stems that occur in the text. The results are for sub-task A over a test set of 10% of all articles.</figDesc><graphic coords="4,196.39,84.19,200.00,174.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,89.29,301.62,416.88,8.93;6,89.29,313.62,416.69,8.87;6,89.29,325.58,416.87,8.87;6,89.29,337.53,71.93,8.87;6,139.14,84.19,314.50,210.00"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Text length of the predicted news articles by BERT model for sub-task A. The largest number of erroneous predictions were made in articles with length of 500 to 1000 tokens. Although only the first 300 tokens of each news item are submitted to the model, there are just few errors in texts longer than 1000 tokens.</figDesc><graphic coords="6,139.14,84.19,314.50,210.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,89.29,542.70,416.69,8.93;6,89.29,554.71,326.54,8.87;6,195.89,362.11,201.00,174.00"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Confusion matrix for the prediction results by BERT for sub-task B over testing set of 10% of all articles. The classification domain is quite common and the model does well.</figDesc><graphic coords="6,195.89,362.11,201.00,174.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,88.99,90.49,265.29,20.87"><head>Table 1</head><label>1</label><figDesc>F1-macro score summary of the results obtained during training.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,112.66,461.18,388.34,10.91" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><surname>Clef</surname></persName>
		</author>
		<ptr target="Https://sites.google.com/view/clef2021-checkthat" />
		<title level="m" coord="7,141.70,461.18,105.85,10.91">Clef2021-checkthat! lab</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,474.73,394.53,10.91;7,112.66,488.28,393.33,10.91;7,112.66,501.83,393.33,10.91;7,112.66,515.38,393.33,10.91;7,112.66,528.92,394.03,10.91;7,112.41,542.47,136.90,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,488.80,488.28,17.19,10.91;7,112.66,501.83,393.33,10.91;7,112.66,515.38,94.69,10.91">The CLEF-2021 CheckThat! lab on detecting check-worthy claims, previously fact-checked claims, and fake news</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Míguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Struß</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mandl</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-72240-1_75</idno>
		<ptr target="https://link.springer.com/chapter/10.1007/978-3-030-72240-1_75" />
	</analytic>
	<monogr>
		<title level="m" coord="7,231.02,515.38,274.97,10.91;7,112.66,528.92,80.27,10.91">Proceedings of the 43rd European Conference on Information Retrieval, ECIR &apos;21</title>
		<meeting>the 43rd European Conference on Information Retrieval, ECIR &apos;21<address><addrLine>Lucca, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="639" to="649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,556.02,394.53,10.91;7,112.66,569.57,394.53,10.91;7,112.66,583.12,393.32,10.91;7,112.66,596.67,393.33,10.91;7,112.66,610.22,393.33,10.91;7,112.66,623.77,393.33,10.91;7,112.33,637.32,62.17,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,261.26,583.12,244.73,10.91;7,112.66,596.67,296.85,10.91">overview of the CLEF-2021 CheckThat! lab on detecting check-worthy claims, previously fact-checked claims, and fake news</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Míguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Struß</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mandl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Modha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kutlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,442.14,596.67,63.85,10.91;7,112.66,610.22,393.33,10.91;7,112.66,623.77,297.52,10.91">Proceedings of the 12th International Conference of the CLEF Association: Information Access Evaluation Meets Multiliguality, Multimodality, and Visualization</title>
		<meeting>the 12th International Conference of the CLEF Association: Information Access Evaluation Meets Multiliguality, Multimodality, and Visualization<address><addrLine>Bucharest, Romania (online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>CLEF &apos;2021</note>
</biblStruct>

<biblStruct coords="7,112.66,650.87,393.33,10.91;8,112.66,86.97,393.33,10.91;8,112.66,100.52,293.98,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,274.26,650.87,231.73,10.91;8,112.66,86.97,103.38,10.91">Overview of the CLEF-2021 CheckThat! lab task 3 on fake news detection</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Struß</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mandl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,240.60,86.97,265.39,10.91;8,112.66,100.52,133.32,10.91">Working Notes of CLEF 2021-Conference and Labs of the Evaluation Forum, CLEF &apos;2021</title>
		<meeting><address><addrLine>Bucharest, Romania (online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,114.06,394.52,10.91;8,112.66,127.61,378.11,10.91" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="8,263.93,114.06,238.32,10.91">Task 3: Fake news detection at CLEF-2021 CheckThat!</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Struß</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mandl</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.4714517</idno>
		<ptr target="https://doi.org/10.5281/zenodo.4714517.doi:10.5281/zenodo.4714517" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,141.16,393.33,10.91;8,112.66,154.71,394.51,10.91;8,112.66,170.70,109.22,7.90" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,351.36,141.16,154.62,10.91;8,112.66,154.71,125.78,10.91">Behind the cues: A benchmarking study for fake news detection</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gravanis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vakali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Diamantaras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Karadais</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2019.03.036</idno>
	</analytic>
	<monogr>
		<title level="j" coord="8,246.02,154.71,146.94,10.91">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,181.81,393.33,10.91;8,112.66,195.36,393.33,10.91;8,112.66,208.91,188.51,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,295.94,181.81,210.05,10.91;8,112.66,195.36,311.74,10.91">exbake: Automatic fake news detection model based on bidirectional encoder representations from transformers (bert)</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jwa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<idno type="DOI">10.3390/app9194062</idno>
	</analytic>
	<monogr>
		<title level="j" coord="8,432.03,195.36,73.96,10.91">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">4062</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,222.46,320.25,10.91" xml:id="b7">
	<monogr>
		<ptr target="Http://www.fakenewschallenge.org/" />
		<title level="m" coord="8,112.66,222.46,123.39,10.91">Fake news challenge</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>FNC-1</note>
</biblStruct>

<biblStruct coords="8,112.66,236.01,393.33,10.91;8,112.66,249.56,395.01,10.91;8,112.66,263.11,185.12,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,243.23,236.01,262.76,10.91;8,112.66,249.56,38.07,10.91">Fake news detection: A hybrid cnn-rnn based deep learning approach</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Nasir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Varlamis</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jjimei.2020.100007</idno>
	</analytic>
	<monogr>
		<title level="j" coord="8,159.08,249.56,277.61,10.91">International Journal of Information Management Data Insights</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">100007</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,276.66,394.61,10.91;8,112.66,290.20,394.62,10.91;8,112.66,303.75,199.51,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,290.62,276.66,197.56,10.91">Glove: Global vectors for word representation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1162" />
	</analytic>
	<monogr>
		<title level="m" coord="8,112.66,290.20,270.61,10.91">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,317.30,393.33,10.91;8,112.66,330.85,283.01,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,292.17,317.30,213.82,10.91;8,112.66,330.85,42.26,10.91">An exploratory study of covid-19 misinformation on twitter</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dirkson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Majchrzak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,162.86,330.85,154.86,10.91">Online Social Networks and Media</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">100104</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,344.40,393.33,10.91;8,112.66,357.95,394.52,10.91;8,112.66,371.50,394.04,10.91;8,112.66,385.05,233.99,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,211.45,344.40,186.67,10.91">XGBoost: A scalable tree boosting system</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939785</idno>
		<ptr target="http://doi.acm.org/10.1145/2939672.2939785.doi:10.1145/2939672.2939785" />
	</analytic>
	<monogr>
		<title level="m" coord="8,421.80,344.40,84.19,10.91;8,112.66,357.95,394.52,10.91;8,112.66,371.50,36.69,10.91">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,398.60,393.33,10.91;8,112.66,412.15,393.33,10.91;8,112.66,425.70,393.32,10.91;8,112.66,439.25,393.33,10.91;8,112.66,452.79,394.03,10.91;8,112.66,466.34,234.20,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,323.15,398.60,182.83,10.91;8,112.66,412.15,186.91,10.91">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://www.aclweb.org/anthology/N19-1423.doi:10.18653/v1/N19-1423" />
	</analytic>
	<monogr>
		<title level="m" coord="8,327.87,412.15,178.11,10.91;8,112.66,425.70,393.32,10.91;8,112.66,439.25,99.97,10.91">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="8,112.66,479.89,394.53,10.91;8,112.66,493.44,393.32,10.91;8,112.66,506.99,347.04,10.91" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Brew</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1910.03771" />
		<title level="m" coord="8,217.10,493.44,288.88,10.91;8,112.66,506.99,47.83,10.91">HuggingFace&apos;s Transformers: State-of-the-art Natural Language Processing</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,520.54,395.17,10.91;8,112.66,534.09,393.32,10.91;8,112.33,547.64,196.41,10.91" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR abs/1907.11692</idno>
		<ptr target="http://arxiv.org/abs/1907.11692" />
		<title level="m" coord="8,140.43,534.09,261.00,10.91">Roberta: A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
