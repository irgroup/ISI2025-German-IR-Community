<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.02,75.44,450.91,17.04;1,72.02,96.20,321.00,17.04">Qword at CheckThat! 2021: An Extreme Gradient Boosting Approach for Multiclass Fake News Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.02,129.10,94.56,10.80"><forename type="first">Rudra</forename><surname>Sarker Utsha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Daffodil International University</orgName>
								<address>
									<settlement>Dhaka</settlement>
									<country key="BD">Bangladesh</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">CLEF</orgName>
								<address>
									<postCode>2021</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,176.66,129.10,103.58,10.80"><forename type="first">Mumenunnessa</forename><surname>Keya</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Daffodil International University</orgName>
								<address>
									<settlement>Dhaka</settlement>
									<country key="BD">Bangladesh</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">CLEF</orgName>
								<address>
									<postCode>2021</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,292.37,129.10,77.52,10.80"><forename type="first">Md</forename><forename type="middle">Arid</forename><surname>Hasan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Daffodil International University</orgName>
								<address>
									<settlement>Dhaka</settlement>
									<country key="BD">Bangladesh</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">CLEF</orgName>
								<address>
									<postCode>2021</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,397.51,129.10,94.27,10.80"><forename type="first">Md</forename><surname>Sanzidul Islam</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Daffodil International University</orgName>
								<address>
									<settlement>Dhaka</settlement>
									<country key="BD">Bangladesh</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">CLEF</orgName>
								<address>
									<postCode>2021</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.02,75.44,450.91,17.04;1,72.02,96.20,321.00,17.04">Qword at CheckThat! 2021: An Extreme Gradient Boosting Approach for Multiclass Fake News Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6D124F5554FBE6C6C8B5C078EBBFD81A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fake news</term>
					<term>XGBoost</term>
					<term>Logistic Regression</term>
					<term>Passive Aggressive</term>
					<term>Random Forest</term>
					<term>Classification</term>
					<term>Machine Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fake news basically means fabricating a story without verifiable information, source or quote of the story. CheckThat! 2021 Task-3 has two subtasks, subtask 3a and 3b. We participated in Subtask 3a, which is a multi-class fake news classification problem. The goal was to determine whether the main claim of a news article is true, partially true, false or other. We were provided with a dataset of news articles by the organizers which consists of news articles, their titles and the rating of the article. We took advantage of TF-IDF vectorization and proposed an Extreme Gradient Boosting algorithm for our best classification model. The approaches were very interpretative with a highest classification accuracy of 0.57 and highest f1-macro score of 0.54 on the given dataset. We also tried other classification models and got varying results which are simple Logistic Regression Classifiers, Passive Aggressive Classifiers and Random Forest Classifiers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The number of social media users has increased so much in a decade that the spread and dissemination of information online is being noticed very fast. Due to this, the number of online abusers increases and it becomes very dangerous, furthermore the result is the dissemination, distribution and reproduction of fake news <ref type="bibr" coords="1,191.23,469.89,17.07,9.94" target="#b11">[12]</ref>. The term fake news has been given different definitions, one of them being, Fake news is fabricated and misinforming news that is often created with the intention to manipulate people's perceptions of reality <ref type="bibr" coords="1,267.05,495.21,11.77,9.94" target="#b0">[1]</ref>. Fake news identification is the ability to verify the accuracy and veracity of information by analyzing various information and the features associated with it. The spread of Fake news poses real-life consequences with serious negative impacts on individuals and society. We have seen in recent years, especially in the 2016 US election how misinformation can impact even the presidential election <ref type="bibr" coords="1,235.23,545.85,11.77,9.94" target="#b3">[4]</ref>. This is causing deep concern and the spread of "fake news" is also reflected in the political arena, strengthening its backbone. On the other hand, the main problem is that people's confidence in government institutions is declining and democracy is constantly being undermined by fake news. Identifying fake news is a significant advance to keep fake news from proliferating through social media. Although fabricated news is not a new phenomenon, the detection of fake news has never been more important than today.</p><p>As a part of CLEF 2021 CheckThat! Task 3 <ref type="bibr" coords="1,291.55,621.72,17.52,9.94" target="#b20">[21,</ref><ref type="bibr" coords="1,312.79,621.72,13.05,9.94" target="#b21">22]</ref>, fake news detection was a subtask, where given the text of a news article, it is required to determine whether the main claim made in the article is true, partially true, false, or other.</p><p>In this paper, we portray the work we performed for this common assignment. For this task a dataset of news articles written in English was prepared manually and shared with the participants by the organizers. We tried different types of machine learning algorithms (i.e XGBoost, Logistic Regression, Passive Aggressive &amp; Random Forest) for this problem and found better results with Gradient boosting algorithms <ref type="bibr" coords="2,122.01,125.32,12.91,9.94" target="#b1">[2]</ref> &amp; RF. We have found the best result with both the XGBoost <ref type="bibr" coords="2,409.04,125.32,12.91,9.94" target="#b2">[3]</ref> classifier model &amp; RF to classify the fake news in four categories. This multi class classification problem and how we tackled the problem with a ML based classification algorithm is structured in the rest of the paper as follows.</p><p>We dedicated section 2 to related work in fake news detection. In section 3 there is an overview of the proposed methodology of our work, section 4 is in the Result &amp; Discussion section and finally the Conclusion is in section 5. The abbreviations we use in our research is given in Table <ref type="table" coords="2,450.70,188.56,4.14,9.94" target="#tab_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The task of detecting fake news is currently receiving a huge response in research, and researchers are focusing on detecting fake news <ref type="bibr" coords="2,233.57,454.53,16.88,9.94" target="#b20">[21]</ref>, bots <ref type="bibr" coords="2,278.63,454.53,18.45,9.94" target="#b12">[13]</ref> and clickbaits <ref type="bibr" coords="2,363.67,454.53,16.88,9.94" target="#b13">[14]</ref>, among many others <ref type="bibr" coords="2,477.73,454.53,16.99,9.94" target="#b14">[15]</ref>. Fake news is misinformed and fabricated news often created to deliberately mislead or deceive readers <ref type="bibr" coords="2,507.40,467.13,11.69,9.94" target="#b0">[1]</ref>. In recent years, we have seen how fake news can even have an impact on the election process <ref type="bibr" coords="2,487.50,479.85,11.68,9.94" target="#b3">[4]</ref>. The detection of fake news by linguistic approaches focusing on text properties, such as the writing style and content <ref type="bibr" coords="2,126.84,505.17,12.91,9.94" target="#b8">[9]</ref> depend on misleading language and leakage cue to foresee misdirection. Considering the dangerous impact fake news can have in our current society, a lot of research has been conducted on this topic in recent years.</p><p>On social media, some features and instances are presented to identify false news that do not work in the same way that they are based. False news is deliberately fabricated and done in such a way that it is not easy to identify them from the subject matter of the text <ref type="bibr" coords="2,355.40,568.41,11.96,9.94" target="#b7">[8]</ref>.</p><p>We can see the significant increase of published papers indexed in the Scopus database regarding fake news. The number of papers on this topic was less than 20 in 2006 to more than 200 in 2018 <ref type="bibr" coords="2,501.87,593.76,16.98,9.94" target="#b9">[10]</ref>. The task of detecting fake news has been approached from various perspectives, such as Natural Language Processing (NLP), Data Mining (DM), Social Media Analysis (SMA). In many cases, the classification was reasoned as a binary problem, either the news is fake or real. However, there are cases where the news can be partially false or others. For this reason, several systems capable of multiclass classification were proposed in <ref type="bibr" coords="2,219.15,657.00,11.77,9.94" target="#b7">[8]</ref>. The Natural Language Processing field has been tackling the problem of detection and classification with techniques such as Machine Learning and Deep Learning <ref type="bibr" coords="2,72.02,682.20,11.77,9.94" target="#b6">[7]</ref>, focusing on content-based features that can be extracted from the text content, like linguistic features. In recent research on a relevant subtopic of fake news spreaders detection showed satisfactory results. Very often fake news spreaders are referred to as bots and can spread fake news in completely automated manners. In <ref type="bibr" coords="2,175.88,720.24,11.69,9.94" target="#b4">[5]</ref>, the author proposed a behavior enhanced deep model (BeDM) that reports an F1-score of 87.32% on a Twitter-related dataset on distinguishing between bots and humans. The proposed model of that research regards user content as temporal text data instead of plain text, fuses content information and behavior information using a deep learning method. Recent research on fake news detection with Bidirectional Encoder Representations from Transformers model (BERT) has shown to perform very well in <ref type="bibr" coords="3,208.30,87.28,11.68,9.94" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview of the Proposed Method</head><p>Our proposed model is pre-processed with given data (e.g., stopword removal, porter steamer and TF-IDF) by importing the NLTK and then using the model when the data is ready to predict fake news. The model is designed using each group of special features. Our proposed model is illustrated in Figure <ref type="figure" coords="3,72.02,183.04,4.14,9.94" target="#fig_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset Description</head><p>For the task, all the participants were given training and a testing dataset by the organizers. The datasets were formatted as csv files containing news articles that were identified manually and the fake news texts were added to the collection with the decision label from the fact checking sites. All of these news articles are written in English language. Training dataset has 4 columns containing 900 total rows of news articles with their public id and title and the end column is the rating column. The test dataset has a total of 364 rows of articles.</p><p>The training dataset overview is in Table <ref type="table" coords="3,255.08,504.09,4.28,9.94" target="#tab_1">2</ref>: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data Preprocessing</head><p>Text data are unstructured and can't be used to train any ML model without some kind of preprocessing. To prepare the dataset for our proposed models, we used several widely used approaches for text data in the industry.</p><p>Punctuation Removing-News articles contain a lot of punctuations, links, digits and special characters that in most cases do not matter in terms of news being fake or true. Moreover, punctuations appear very frequently and lead to high metrics for them while making no impact on the text classification.</p><p>Capitalization-Mix of upper-and lower-case words can be useful for a human being when reading something, but for a computational model it is better to use the same type of register level. It does not matter what type of register level to use since it will all be transformed into digits. In this paper we have used lowercase registers.</p><p>Lemmatization-Lemmatization is used to reduce the number of words carrying similar kinds of meanings. Stemming can also be used to do the task. While lemmatization reduces word to its morphological root, stemming simply remove the affixes from the word to obtain a root. We used stemming, in this paper for our task.</p><p>Removing Stop-words-We can further reduce the number of words from our data while making no impact on how good our model predicts. Stop-words <ref type="bibr" coords="4,310.35,212.56,18.32,9.94" target="#b10">[11]</ref> are the words that appear in text extremely frequently while making no impact on the meaning of the text. This is the last step of cleaning our data before creating our bag of words.</p><p>After cleaning the texts our goal is to make the data trainable as ML models can't really work with text or strings. So, we convert all our text into numbers for it to be trainable. We do so by vectorizing our text data with the TF-IDF vectorization method and making our final trainable data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">ML Algorithms</head><p>The more the machine learning algorithm comes in contact with the data, the better it will perform. The word "learning" in machine learning means that processes change over time as data is processed and at the same time the way people process data changes.</p><p>XGBoost This is highly credited by the machine learning practitioners and popular among ML competitors. XGBoost is an implementation of gradient boosted decision trees that were designed for speed and performance. This framework can be found for all popular data analysis languages and performs considerably well for multiclass classification problems. XGBoost is used for both regression and classification problems thus expected to perform well for our classification task. XGBoost decision tree being a gradient boosted tree, usually outperforms random forest. More about this method is in the original paper, written by Tianqi Chen and Carlos Guestrin <ref type="bibr" coords="4,334.71,470.85,11.77,9.94" target="#b2">[3]</ref>.</p><p>Logistic regression This is a well-known statistical model that in its default form is limited to predict binary classification problems. In our case it can be used with some kind of extension like onevs-rest to allow it to classify multiclass classification problems. However, the classification problem first transformed into multiple binary classification problems. Basically, the idea behind the method is to calculate the probability of a news article to be true vs anything else. And similarly calculating the probability of a news article to be False vs anything else and so on. For the reason that the probability function (1) has a logistic form, the model also got the name Logistic Regression:</p><formula xml:id="formula_0" coords="4,252.05,582.44,114.57,20.04">𝑓(𝑧) = 1 1 + 𝑒 -𝑥 (1)</formula><p>Here, z is a set of model input factors, in our case these are vectors of count vectorized matrix. More about this regression can be found in <ref type="bibr" coords="4,236.56,616.08,18.43,9.94" target="#b16">[17]</ref> written by Cramer.</p><p>Passive Aggressive Another classifier we implemented was a Passive Aggressive classifier. It is an incremental learning algorithm and the concept is that the classifier adjusts its weight vector for every misclassified training sample it receives trying to correct it. The passive part is to leave the model as it is if the prediction is correct and the aggressive part is to make changes to the model when the prediction is incorrect. That is, some changes to the model may correct it. Passive Aggressive classifiers can be used in binary classification, multiclass class classification, and uniclass classification problems. More on this topic can be found in <ref type="bibr" coords="4,200.02,717.24,16.88,9.94" target="#b17">[18]</ref>.</p><p>Random Forest A very well-known and extremely frequently used by machine learning competitors, this is an ensemble learning method used for both classification and regression tasks.</p><p>Random forest constructs an ensemble of decision trees (Forest) to get a stable and more accurate prediction. The idea behind this combination of tree predictors is that as the trees in the forest become large the generalization error for the forest converges to a limit. Detailed information about this method can be found in the paper <ref type="bibr" coords="5,186.33,112.60,18.32,9.94" target="#b19">[20]</ref> by Breiman.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Result and Discussion</head><p>This section will discuss all the accuracy and f1-macro scores and some parts of classification reports of fake news detection. The accuracy and macro-average F1 score we have achieved through the application of XGBoost algorithm and other algorithms are shown in Table <ref type="table" coords="5,435.10,195.64,5.52,9.94" target="#tab_2">3</ref> and the results of classifiers in terms of precision, recall and F-1 standard is presented in Table <ref type="table" coords="5,433.90,208.36,4.14,9.94" target="#tab_3">4</ref>. And with the ID (public_id) that we represented our dataset through preprocessing, we will see the predicted ratings (predicted_rating) of XGBoost, LR, PA &amp; RF of the first 5 texts from the test dataset in Table <ref type="table" coords="5,489.58,233.56,5.52,9.94">5</ref> to see if the model can accurately detect fake news. Table <ref type="table" coords="5,100.70,381.31,5.52,9.94" target="#tab_2">3</ref> shows the classification accuracy score and f1-macro average score of four models. Table <ref type="table" coords="5,517.66,381.31,5.52,9.94" target="#tab_3">4</ref> represents the precision, recall and f1-score. In the first column of the table, we've taken the algorithms and in the second one there we place the state of the dataset which provides for fake news as if the data is False or True or Other or Partially False. We have found out the precision, recall and f1-score. For different classifiers, all of the values sometimes perform well and some are given quite small values.   Previously, working with these same models we tried a different approach to vectorize our data with the Count-Vectorization method and set max_feature to 5000 and got really bad results.The XGBoost classifier were showing a f1-macro score of 0.15 on test data even though it was showing good result on training data. Finally, we vectorized our dataset with TF-IDF vectorization and set max_feature to 18256.We also changed ngram_range from (1,3) to (1,1). These small changes and a different approach in vectorizing the dataset bumped our accuracy from 0.28 to 0.54 and f1-macro measure from 0.15 to a 0.52 on the test dataset.</p><p>Our best performing XGBClassifier has these hyperparameters in Table <ref type="table" coords="7,390.79,74.68,4.14,9.94" target="#tab_4">6</ref>. With the hyperparameters in Table <ref type="table" coords="7,246.53,233.56,4.14,9.94" target="#tab_4">6</ref>, we got a slight improvement of performance, our accuracy went to 0.57 and F1-macro score to 0.54 and these are the final results from our best performing XGB model, in fact any ML model that we have tried. The next best model is Random Forest classifier with an accuracy of 0.53 and F1-macro score of .50. In third place is Passive Aggressive classifier with an accuracy of 0.54 and F1-macro score of 0.49. And at last, as expected the simple Logistic Regression classifier did a poor job of classifying news articles. It had an accuracy of 0.41 and F1-macro score of 0.27.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper showcases our work on fake news detection with various machine learning algorithms and provides the best results which we found. While our models were good at classifying false and partially false and other classes, all of the models were performing really bad at predicting true classes. We suspect it was due to the nature of our dataset, a high number of samples present in false and partially false classes made the training of the model very biased towards these classes. However, the overall results were not unsatisfactory as the model was really good at classifying the false news and partially false news, and that is a step closer towards preventing the spread of false news. We think it is likely that our models could perform better once we find the optimal hyperparameters, but this needs more time and computing power. As per our expectation, Random Forest classifier did well but the XGBoost model did overall better than Random Forest by a thin margin. But as we stated earlier, by tweaking the hyperparameters of both of the models we could get better results. We also observed that the Passive Aggressive Classifier was also very good at text classification tasks. Besides that, in future we might also try making a bigger and more balanced dataset where every class contains a similar number of samples and then perform classification with Deep Learning models such as Transformers based models like BERT, RoBERTa, ALBERT models. We might also try more traditional LSTM and CNN models for comparison. And our belief is that with a bigger dataset, these Deep Learning models would show a better classification performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,86.18,369.05,190.88,11.04;3,136.60,192.98,336.00,159.75"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The Proposed Model of the Work</figDesc><graphic coords="3,136.60,192.98,336.00,159.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,144.02,406.01,80.52,11.04;6,362.83,406.01,126.73,11.04;6,74.75,247.16,216.00,144.00"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: XGBoost Figure 3: Logistic Regression</figDesc><graphic coords="6,74.75,247.16,216.00,144.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,144.02,601.66,124.42,11.04;6,362.83,601.66,110.23,11.04;6,297.30,442.53,215.99,144.25"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Passive Aggressive Figure 5: Random Forest</figDesc><graphic coords="6,297.30,442.53,215.99,144.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,72.02,213.26,424.07,163.35"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="2,72.02,226.70,424.07,149.91"><row><cell>Machine Learning Acronym</cell><cell></cell></row><row><cell>Acronym</cell><cell>Definition</cell></row><row><cell>ML</cell><cell>Machine Learning</cell></row><row><cell>XGBC</cell><cell>Extreme Gradient Boosting Classifier</cell></row><row><cell>LRC</cell><cell>Logistic Regression Classifier</cell></row><row><cell>PAC</cell><cell>Passive Aggressive Classifier</cell></row><row><cell>RFC</cell><cell>Random Forest Classifier</cell></row><row><cell>BERT</cell><cell>Bidirectional Encoder Representations from</cell></row><row><cell></cell><cell>Transformers</cell></row><row><cell>RoBERTa</cell><cell>A Robustly Optimized BERT Pretraining Approach</cell></row><row><cell>ALBERT</cell><cell>A Lite BERT</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,72.02,528.79,315.60,94.23"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="3,72.02,542.23,315.60,80.79"><row><cell>Training dataset overview</cell><cell></cell></row><row><cell>Class</cell><cell>No. of articles</cell></row><row><cell>False</cell><cell>465</cell></row><row><cell>True</cell><cell>142</cell></row><row><cell>Other</cell><cell>76</cell></row><row><cell>Partially false</cell><cell>217</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,72.02,271.13,407.96,94.08"><head>Table 3</head><label>3</label><figDesc></figDesc><table coords="5,72.02,284.45,407.96,80.76"><row><cell>Accuracy Table</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Accuracy</cell><cell>F1-macro Avg</cell></row><row><cell>XGboost</cell><cell>0.571</cell><cell>0.543</cell></row><row><cell>Logistic Regression</cell><cell>0.412</cell><cell>0.272</cell></row><row><cell>Passive Aggressive</cell><cell>0.542</cell><cell>0.489</cell></row><row><cell>Random Forest</cell><cell>0.534</cell><cell>0.502</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,72.02,456.67,421.76,268.71"><head>Table 4</head><label>4</label><figDesc></figDesc><table coords="5,72.02,470.11,421.76,255.27"><row><cell>Performance overview</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Algorithms</cell><cell>State</cell><cell>Precision</cell><cell>Recall</cell><cell>F1 Score</cell></row><row><cell>XGBoost</cell><cell>False</cell><cell>0.49</cell><cell>0.91</cell><cell>0.64</cell></row><row><cell></cell><cell>True</cell><cell>0.42</cell><cell>0.26</cell><cell>0.32</cell></row><row><cell></cell><cell>Other</cell><cell>0.76</cell><cell>0.55</cell><cell>0.64</cell></row><row><cell></cell><cell>Partially False</cell><cell>0.79</cell><cell>0.45</cell><cell>0.57</cell></row><row><cell>Logistic Regression</cell><cell>False</cell><cell>0.36</cell><cell>0.92</cell><cell>0.52</cell></row><row><cell></cell><cell>True</cell><cell>0.35</cell><cell>0.12</cell><cell>0.18</cell></row><row><cell></cell><cell>Other</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell></cell><cell>Partially False</cell><cell>0.75</cell><cell>0.26</cell><cell>0.39</cell></row><row><cell>Passive Aggressive</cell><cell>False</cell><cell>0.56</cell><cell>0.85</cell><cell>0.68</cell></row><row><cell></cell><cell>True</cell><cell>0.35</cell><cell>0.26</cell><cell>0.30</cell></row><row><cell></cell><cell>Other</cell><cell>0.38</cell><cell>0.53</cell><cell>0.44</cell></row><row><cell></cell><cell>Partially False</cell><cell>0.72</cell><cell>0.43</cell><cell>0.54</cell></row><row><cell>Random Forest</cell><cell>False</cell><cell>0.42</cell><cell>0.98</cell><cell>0.59</cell></row><row><cell></cell><cell>True</cell><cell>0.71</cell><cell>0.15</cell><cell>0.25</cell></row><row><cell></cell><cell>Other</cell><cell>0.91</cell><cell>0.53</cell><cell>0.67</cell></row><row><cell></cell><cell>Partially False</cell><cell>0.83</cell><cell>0.36</cell><cell>0.50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,72.02,100.22,322.58,112.68"><head>Table 6</head><label>6</label><figDesc></figDesc><table coords="7,72.02,113.66,322.58,99.24"><row><cell>Hyperparameters</cell><cell></cell></row><row><cell>learning Rate</cell><cell>0.3</cell></row><row><cell>Max_depth</cell><cell>15</cell></row><row><cell>min_child_weight</cell><cell>7</cell></row><row><cell>gamma</cell><cell>0.1</cell></row><row><cell>colsample_bytree</cell><cell>0.7</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6.">Acknowledgements</head><p>This is unbelievable support we've got from some of the faculty members and seniors of <rs type="institution">DIU NLP</rs> and <rs type="institution">ML Research Lab</rs> to continue our whole research flow from the beginning. We acknowledge <rs type="person">Dr. Firoj Alam</rs> for guiding us and informing the secretes of research workshops. Also, we're thankful to <rs type="institution">Daffodil International University</rs> for the workplace support and the academic collaboration in some cases. <rs type="person">Dr. Touhid Bhuiyan</rs> and <rs type="person">Dr. Sheak Rashed Haider Noori</rs> also supported us with guidance, motivation, and advocating in institutional supports. Lastly, for sure we're thankful to our God always for every fruitful work with our given knowledge.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="8,108.02,105.76,414.90,9.94;8,86.18,118.36,436.85,9.94;8,86.18,131.08,84.45,9.94" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,324.12,118.36,106.75,9.94">The science of fake news</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Lazer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Benkler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Berinsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">M</forename><surname>Greenhill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Menczer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Metzger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Nyhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Pennycook</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Rothschild</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,438.11,118.36,34.17,9.94">Science</title>
		<imprint>
			<biblScope unit="volume">359</biblScope>
			<biblScope unit="issue">6380</biblScope>
			<biblScope unit="page" from="1094" to="1096" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.02,143.68,415.11,9.94;8,86.18,156.40,277.80,9.94" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,240.41,143.68,176.51,9.94">Gradient Boosting Machines, A Tutorial</title>
		<author>
			<persName coords=""><forename type="first">Alexey</forename><surname>Natekin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alois</forename><surname>Knoll</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnbot.2013.00021</idno>
		<ptr target="https://doi.org/10.3389/fnbot.2013.00021" />
	</analytic>
	<monogr>
		<title level="j" coord="8,423.94,143.68,80.84,9.94">Front. Neurorobot</title>
		<imprint>
			<date type="published" when="2013-12-04">04 December 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.02,169.00,415.00,9.94;8,86.18,181.72,330.51,9.94" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="8,242.73,169.00,192.68,9.94">XGBoost: A Scalable Tree Boosting System</title>
		<author>
			<persName coords=""><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939785</idno>
		<ptr target="http://dx.doi.org/10.1145/2939672.2939785" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.02,194.32,414.72,9.94;8,86.18,206.92,234.72,9.94" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,223.38,194.32,299.36,9.94;8,86.18,206.92,33.47,9.94">Influence of fake news in twitter during the 2016 US presidential election</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bovet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">A</forename><surname>Makse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,126.60,206.92,105.30,9.94">Nature communications</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.02,219.64,415.18,9.94;8,86.18,232.24,436.97,9.94" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,220.29,219.64,232.50,9.94">Behavior enhanced deep bot detection in social media</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zengi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,474.79,219.64,48.41,9.94;8,86.18,232.24,311.14,9.94">2017 IEEE International Conference on Intelligence and Security Informatics (ISI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="128" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.02,244.99,414.66,9.94;8,86.18,257.59,436.91,9.94;8,86.18,270.19,435.20,9.94" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,86.18,257.59,436.91,9.94;8,86.18,270.19,118.80,9.94">exBAKE: Automatic Fake News Detection Model Based on Bidirectional Encoder Representations from Transformers (BERT)</title>
		<author>
			<persName coords=""><forename type="first">Heejung</forename><surname>Jwa 1</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dongsuk</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kinam</forename><surname>Park 3</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jang</forename><surname>Mook</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kang</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Heuiseok</forename><surname>Lim 1</surname></persName>
		</author>
		<idno type="DOI">10.3390/app9194062</idno>
		<ptr target="https://doi.org/10.3390/app9194062" />
	</analytic>
	<monogr>
		<title level="j" coord="8,213.29,270.19,42.58,9.94">Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page">4062</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.02,282.91,414.76,9.94;8,86.18,295.51,436.81,9.94;8,86.18,308.23,73.41,9.94" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,265.73,282.91,235.65,9.94">Csi: A hybrid deep model for fake news detection</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ruchansky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,86.18,295.51,413.58,9.94">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="797" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.02,320.83,414.75,9.94;8,86.18,333.43,437.00,9.94;8,86.18,346.15,324.86,9.94" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,363.91,320.83,158.87,9.94;8,86.18,333.43,228.92,9.94">Truth of varying shades: Analyzing language in fake news and political fact-checking</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Y</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Volkova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,341.40,333.43,181.78,9.94;8,86.18,346.15,216.62,9.94">Proceedings of the 2017 conference on empirical methods in natural language processing</title>
		<meeting>the 2017 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2931" to="2937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.02,358.75,414.78,9.94;8,86.18,371.47,436.99,9.94;8,86.18,384.07,436.73,9.94;8,86.18,396.67,202.71,9.94" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,370.28,358.75,147.48,9.94">Automatic detection of fake news</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Pérez-Rosas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lefevre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/C18-1287" />
	</analytic>
	<monogr>
		<title level="m" coord="8,100.93,371.47,346.64,9.94">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-08">Aug 2018</date>
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.02,409.41,414.89,9.94;8,86.18,422.01,180.62,9.94" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="8,210.69,409.41,307.88,9.94">Fake news: A survey of research, detection methods, and opportunities</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zafarani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00315</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,110.78,434.73,412.27,9.94;8,86.18,447.33,70.66,9.94;8,192.47,447.33,11.04,9.94;8,239.01,447.33,15.90,9.94;8,290.58,447.33,41.29,9.94;8,367.49,447.33,45.87,9.94;8,448.83,447.33,74.40,9.94;8,86.18,459.93,176.79,9.94" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,171.15,434.73,299.57,9.94">Keyword-in-Context Index for Technical Literature (KWIC Index)</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">P</forename><surname>Luhn</surname></persName>
		</author>
		<idno type="DOI">10.1002/asi.5090110403</idno>
		<idno>10.1.1.468.1425</idno>
		<ptr target="https://doi.org/10.1002/asi.5090110403" />
	</analytic>
	<monogr>
		<title level="j" coord="8,479.63,434.73,43.42,9.94;8,86.18,447.33,65.61,9.94">American Documentation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="288" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.02,472.65,414.84,9.94;8,86.18,485.25,436.65,9.94;8,86.18,497.97,396.80,9.94" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,440.66,472.65,82.19,9.94;8,86.18,485.25,294.55,9.94">Deep Learning for Fake News Detection in a Pairwise Textual Input Schema</title>
		<author>
			<persName coords=""><forename type="first">Despoina</forename><surname>Mouratidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Nefeli Nikiforos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Katia</forename><forename type="middle">Lida</forename><surname>Kermanidis</surname></persName>
		</author>
		<idno type="DOI">10.3390/computation9020020</idno>
		<ptr target="https://www.mdpi.com/journal/computation" />
	</analytic>
	<monogr>
		<title level="j" coord="8,394.27,485.25,57.33,9.94">Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.78,510.57,412.11,9.94;8,86.18,523.29,436.80,9.94;8,86.18,535.89,73.18,9.94" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,322.39,510.57,200.50,9.94;8,86.18,523.29,134.03,9.94">Detecting automation of twitter accounts: Are you a human, bot, or cyborg</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gianvecchio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jajodia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,229.98,523.29,267.23,9.94">IEEE Transactions on Dependable and Secure Computing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="811" to="824" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.02,548.49,414.84,9.94;8,86.18,561.21,436.60,9.94;8,86.18,573.81,237.04,9.94" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,283.63,548.49,239.23,9.94;8,86.18,561.21,173.11,9.94">We used Neural Networks to Detect Clickbaits: You won&apos;t Believe what Happened Next!</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,283.77,561.21,239.02,9.94;8,86.18,573.81,94.16,9.94;8,247.67,573.81,40.66,9.94">Proceedings of the 2017 European Conference on Information Retrieval</title>
		<meeting>the 2017 European Conference on Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="541" to="547" />
		</imprint>
	</monogr>
	<note>ECIR &apos;17</note>
</biblStruct>

<biblStruct coords="8,108.02,586.56,414.86,9.94;8,86.18,599.16,436.59,9.94;8,86.18,611.76,201.29,9.94" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,372.34,586.56,150.54,9.94;8,86.18,599.16,374.04,9.94">Overview of the 8th Author Proling Task at PAN 2020: Proling Fake News Spreaders on Twitter</title>
		<author>
			<persName coords=""><forename type="first">Bilal</forename><surname>Ghanem2</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,108.02,586.56,190.78,9.94;8,467.96,599.16,49.87,9.94">Francisco Rangel1, Anastasia Giachanou2</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-09">September 2020</date>
			<biblScope unit="page" from="22" to="25" />
		</imprint>
	</monogr>
	<note>CLEF 2020</note>
</biblStruct>

<biblStruct coords="8,108.02,624.37,415.03,10.05;8,86.18,637.08,436.76,9.94;8,86.18,649.69,334.91,10.04" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="8,252.73,624.37,270.32,10.05;8,86.18,637.08,436.76,9.94;8,86.18,649.69,53.76,10.04">A modified deep convolutional neural network for detecting COVID-19 and pneumonia from chest X-ray images based on the concatenation of Xception and ResNet50V2</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rahimzadeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Attar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,153.45,649.69,149.55,10.04">Informatics in Medicine Unlocked</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">100360</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.78,662.40,412.44,9.94;8,86.18,675.00,310.37,9.94" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="8,217.13,662.40,156.71,9.94">The origins of logistic regression</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Cramer</surname></persName>
		</author>
		<idno type="DOI">10.2139/ssrn.360300</idno>
		<ptr target="https://doi.org/10.2139/ssrn.360300" />
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="167" to="178" />
		</imprint>
		<respStmt>
			<orgName>Tinbergen Institute</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>PDF</note>
</biblStruct>

<biblStruct coords="8,108.02,687.72,415.06,9.94;8,86.18,700.32,437.12,9.94;8,86.18,713.04,19.32,9.94" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="8,86.18,700.32,168.88,9.94">Online Passive-Aggressive Algorithms</title>
		<author>
			<persName coords=""><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ofer</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joseph</forename><surname>Keshet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,262.07,700.32,167.21,9.94">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">12/1</biblScope>
			<biblScope unit="page" from="551" to="585" />
			<date type="published" when="2006">2006. /2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.02,725.64,414.74,9.94;8,86.18,738.24,436.93,9.94;8,86.18,750.98,149.27,9.94" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="8,320.14,725.64,202.62,9.94;8,86.18,738.24,141.80,9.94">An Ensemble Random Forest Algorithm for Insurance Big Data Analysis</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2017.2738069</idno>
	</analytic>
	<monogr>
		<title level="j" coord="8,262.93,738.24,61.36,9.94">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="16568" to="16575" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,108.02,74.68,415.11,9.94;9,86.18,87.28,74.40,9.94" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="9,165.83,74.68,71.22,9.94">Random Forests</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1010933404324</idno>
		<ptr target="https://doi.org/10.1023/A:1010933404324" />
	</analytic>
	<monogr>
		<title level="j" coord="9,245.09,74.68,81.91,9.94">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,108.02,100.00,415.14,9.94;9,86.18,112.60,436.78,9.94;9,86.18,125.32,436.59,9.94;9,86.18,137.92,432.20,9.94" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="9,209.70,100.00,313.46,9.94;9,86.18,112.60,218.38,9.94">The CLEF-2021 CheckThat! Lab on Detecting Check-Worthy Claims, Previously Fact-Checked Claims, and Fake News</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-72240-1_75?fbclid=IwAR3CyDltYFL14ufESaCURtx5mozASkUXZVOf_qLhUVOvZvh-YHXJOrcsQ1M</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-72240-1_75" />
	</analytic>
	<monogr>
		<title level="m" coord="9,244.57,125.32,207.81,9.94">Advances in Information Retrieval. ECIR 2021</title>
		<title level="s" coord="9,460.44,125.32,62.33,9.94;9,86.18,137.92,90.50,9.94">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Moens</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Mothe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Perego</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Potthast</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12657</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,108.02,150.52,415.02,9.94;9,86.18,163.24,374.72,9.94" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="9,391.15,150.52,131.89,9.94;9,86.18,163.24,115.85,9.94">Task 3: Fake News Detection at CLEF-2021 CheckThat!</title>
		<author>
			<persName coords=""><forename type="first">Kishore</forename><surname>Gautam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julia</forename><forename type="middle">Maria</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Struß</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mandl</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.4714517</idno>
		<ptr target="https://doi.org/10.5281/zenodo.4714517" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">apr</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
