<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,358.08,15.42;1,89.29,106.66,355.41,15.42;1,89.29,128.58,76.47,15.43">University of Regensburg at CheckThat! 2021: Exploring Text Summarization for Fake News Detection</title>
				<funder ref="#_vYxvjzd">
					<orgName type="full">Volkswagen Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,156.89,62.00,11.96"><forename type="first">Philipp</forename><surname>Hartl</surname></persName>
							<email>philipp1.hartl@stud.uni-regensburg.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Regensburg</orgName>
								<address>
									<addrLine>Universit√§tsstra√üe 31</addrLine>
									<postCode>93053</postCode>
									<settlement>Regensburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,162.05,156.89,78.61,11.96"><forename type="first">Udo</forename><surname>Kruschwitz</surname></persName>
							<email>udo.kruschwitz@ur.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Regensburg</orgName>
								<address>
									<addrLine>Universit√§tsstra√üe 31</addrLine>
									<postCode>93053</postCode>
									<settlement>Regensburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,358.08,15.42;1,89.29,106.66,355.41,15.42;1,89.29,128.58,76.47,15.43">University of Regensburg at CheckThat! 2021: Exploring Text Summarization for Fake News Detection</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">530B32823C108FC0A7497C0E09E2C628</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fake News Detection</term>
					<term>Text Summarization</term>
					<term>Abstractive / Extractive Summarization</term>
					<term>CLEF</term>
					<term>BERT</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present our submission to the CLEF 2021 CheckThat! challenge. More specifically, we took part in Task 3a, Multi-class fake news detection of news articles. The conceptual idea of our work is that (a) transformer-based approaches represent a strong foundation for a broad range of NLP tasks including fake news detection, and that (b) compressing the original input documents into some form of automatically generated summary before classifying them is a promising approach. The official results indicate that this is indeed an interesting direction to explore. They also confirm that oversampling to address the class imbalance was effective to further improve the results. We also note that both abstractive and extractive summarization approaches score way better when we do not apply hypertuning of parameters suggesting that the small scale of the test collection leads to overfitting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Fake news, misinformation and disinformation is by no means a recent phenomenon, but instead has been around since classical antiquity when manipulated information was used to discredit political opponents or alter battle courses <ref type="bibr" coords="1,273.90,438.64,11.30,10.91" target="#b0">[1]</ref>. What did change though over time was the scale and extent of the problem, e.g. initially dissemination happened verbally, but the invention of the printing press marked a major milestone as easy access and distribution of information combined with increasing literacy enabled more people to consume and create information. The advent of social media with the freedom to publish marks the birth of a yet another era altogether <ref type="bibr" coords="1,138.47,506.38,11.59,10.91" target="#b1">[2]</ref>. The term fake news has been particularly prevalent in the mainstream media since the 2016 US election, when a large amount of intentionally false news was spread through social media during the campaign <ref type="bibr" coords="1,242.82,533.48,11.45,10.91" target="#b2">[3]</ref>. These platforms operate with a non-restrictive content policy by design and provide various ways for automation which eases the spread of mis-and disinformation. Combined with their enormous user bases (e.g. Facebook with 2.8 billion active users in December 2020 1 ) information is able to reach many people in a very short period of time. In an age of information pollution (irrelevant, redundant, unsolicited and low-value information <ref type="bibr" coords="2,146.96,100.52,12.24,10.91" target="#b3">[4]</ref>) it is therefore important to (semi-) automatically identify such claims and minimize their harm -in particular as humans appear to not be very skilled at identifying disinformation, with typical recognition rates only being slightly better than chance <ref type="bibr" coords="2,466.48,127.61,11.43,10.91" target="#b4">[5]</ref>.</p><p>CheckThat! Lab <ref type="bibr" coords="2,177.17,141.16,12.99,10.91" target="#b5">[6]</ref> is an evaluation campaign which is part of the 2021 Cross-Language Evaluation Forum (CLEF) conference and contains three tasks related to fact-checking or fake news detection with two subtasks each. Our team participated in this year's Task 3a whose goal it is to create a system to identify fake news in a multi-label scenario. We built four models based on fine-tuned BERT <ref type="bibr" coords="2,212.44,195.36,11.58,10.91" target="#b6">[7]</ref>, a highly-popular bidirectional transformer architecture, and abstractive respectively extractive summarization technologies <ref type="bibr" coords="2,365.21,208.91,11.23,10.91" target="#b7">[8,</ref><ref type="bibr" coords="2,378.93,208.91,7.49,10.91" target="#b8">9]</ref>. Our best submitted model (abstractive summarization) was ranked 8th among all 25 participating teams in the lab for this task. Post-hoc runs reveal though that the same runs but without hyperparameter tuning lead to substantially improved results (placing our best run 3rd in the ranked list). In this paper, we describe our participation in Task 3a at CLEF 2021 in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Traditionally, fake news detection is modelled as a classification problem but often with varying class numbers. While datasets like FakeNewsNet <ref type="bibr" coords="2,318.17,335.28,16.41,10.91" target="#b9">[10]</ref>, MM-COVID <ref type="bibr" coords="2,401.74,335.28,18.07,10.91" target="#b10">[11]</ref> or ReCOVery <ref type="bibr" coords="2,487.92,335.28,18.06,10.91" target="#b11">[12]</ref> provide only two labels and hence see fake news detection as a binary classification, there exist also several datasets which got multiple labels such as FEVER <ref type="bibr" coords="2,384.49,362.38,16.09,10.91" target="#b12">[13]</ref>, NELA-GT-2019 <ref type="bibr" coords="2,476.33,362.38,17.76,10.91" target="#b13">[14]</ref> or the dataset provided by the organizers of this task (see Section 3). Unfortunately, generating comprehensive datasets still takes a lot of work as the ground-truth labels often need to be assigned by, e.g., journalists or domain experts. Fake news detection systems typically adopt one of three general approaches or a combination of them. The most commonly used way is based on the news content which can be either linguistic, auditory (e.g., attached voice recordings) or visual (e.g., images or videos) <ref type="bibr" coords="2,293.51,443.67,16.41,10.91" target="#b14">[15]</ref>. This is based on the assumption that real and fantasy statements differ in content style and quality <ref type="bibr" coords="2,358.47,457.22,16.41,10.91" target="#b15">[16]</ref>. Therefore, it is possible to successfully differentiate claims solely on their content with either hand-engineered features <ref type="bibr" coords="2,89.29,484.32,17.76,10.91" target="#b16">[17]</ref> or deep learning methods <ref type="bibr" coords="2,222.51,484.32,16.09,10.91" target="#b17">[18]</ref>. However, approaches which only focus on the news content might miss valuable context information. Hence, feedback-based solutions target secondary information such as user engagements <ref type="bibr" coords="2,259.32,511.42,17.76,10.91" target="#b18">[19]</ref> and dissemination networks <ref type="bibr" coords="2,404.98,511.42,16.08,10.91" target="#b19">[20]</ref>. These approaches are often used in combination with content-based methods to increase performance <ref type="bibr" coords="2,456.32,524.97,16.09,10.91" target="#b20">[21]</ref>. While contextual information can be useful when available, it is often not or only partially available (as reflected by common benchmark collections for fake news detection <ref type="bibr" coords="2,413.37,552.07,16.49,10.91" target="#b21">[22,</ref><ref type="bibr" coords="2,432.58,552.07,11.87,10.91" target="#b12">13]</ref>). While both methods discussed above are limited to a snapshot of features present at the time of training, intervention-based methods try to dynamically interpret real-time dissemination data. These are arguably the least common approaches used at the moment because of their difficult way to evaluate <ref type="bibr" coords="2,140.97,606.27,16.27,10.91" target="#b22">[23]</ref>. When used though, they try to intervene the process of fake news spreading through e.g., injecting of true news into social networks <ref type="bibr" coords="2,344.04,619.81,18.01,10.91" target="#b23">[24]</ref> or user intervention <ref type="bibr" coords="2,457.82,619.81,16.50,10.91" target="#b24">[25,</ref><ref type="bibr" coords="2,477.04,619.81,12.38,10.91" target="#b25">26]</ref>. In this work we use a solely content-based approach simply because the dataset provided for this challenge has no additional context data. Additionally, gathering of some context data was explicitly forbidden as described in Section 4, so we decided to focus on a text-based solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Task Description</head><p>This year there have been a total of three CheckThat! tasks with two subtasks each <ref type="bibr" coords="3,472.12,111.28,11.58,10.91" target="#b5">[6]</ref>. We participated in Task 3a: Multi-class fake news detection of news articles, which is a part of Task 3: Fake News Detection. The goal is to "given the text of a news article, determine whether the main claim made in the article is true, partially true, false, or other". The data used in this task is only available in English. As this task is designed as a four-class classification problem, the official evaluation metric introduced by the organizers is the F1-macro score. The F1-macro score is simply the mean of class-wise F1 scores:</p><formula xml:id="formula_0" coords="3,231.28,210.29,274.71,24.43">ùêπ 1 = 2 * ùëùùëüùëíùëêùëñùë†ùëñùëúùëõ * ùëüùëíùëêùëéùëôùëô ùëùùëüùëíùëêùëñùë†ùëñùëúùëõ + ùëüùëíùëêùëéùëôùëô<label>(1)</label></formula><formula xml:id="formula_1" coords="3,248.37,242.65,257.61,33.71">ùêπ 1 ùëöùëéùëêùëüùëú = 1 ùëõ ùëÅ ‚àëÔ∏Å ùëñ=0 ùêπ 1 ùëñ<label>(2)</label></formula><p>Up to five runs were permitted for each team. We submitted three competitive configurations and one baseline run to compare against our own approaches. Further details on all tasks can be found in the task overview <ref type="bibr" coords="3,224.64,307.38,11.43,10.91" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Dataset</head><p>As this work is part of this year's CLEF CheckThatLab! <ref type="bibr" coords="3,333.33,365.20,12.68,10.91" target="#b5">[6]</ref> Task 3a, we used a modified version of the dataset by Shahi <ref type="bibr" coords="3,197.21,378.75,17.91,10.91" target="#b26">[27]</ref> provided by the organizers. This dataset also got four different classes to predict as defined in <ref type="bibr" coords="3,225.83,392.30,16.18,10.91" target="#b27">[28]</ref>. The distribution of each class in the provided training and test data can be seen in Table <ref type="table" coords="3,222.24,405.85,3.74,10.91" target="#tab_0">1</ref>. The dataset was given in .csv format with four columns:</p><p>‚Ä¢ public_id -unique identifier of the news article ‚Ä¢ title -title/heading of the news article ‚Ä¢ text -text content of the news article ‚Ä¢ our rating -class of the news article (either false, partially false, true or other) The training set contains 950 data points including the 50 sample data points released before both batches of data. The provided test set got 364 data points without labels. We received the ground-truth labels separately after the competition had finished (see Table <ref type="table" coords="3,441.88,596.88,3.54,10.91" target="#tab_0">1</ref>). Each group had to submit a .csv file with their predictions separately on Codalab<ref type="foot" coords="3,398.95,608.68,3.71,7.97" target="#foot_0">2</ref> . Additionally, through a data sharing agreement, it was forbidden to identify individuals and the original entries on the fact-checking websites. Therefore, we refrained from finding this information, although it would have been useful for classification purposes as demonstrated on a similar task <ref type="bibr" coords="3,467.73,651.08,16.25,10.91" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Methodology</head><p>In the following section we provide an overview on how we prepared the data, the models we used as well as the training and evaluation process. Everything has been implemented in Python and is available on Github. <ref type="foot" coords="4,242.66,136.62,3.71,7.97" target="#foot_1">3</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Data preparation</head><p>We started our preprocessing with first converting all labels to numeric values. We used 0 for true, 1 for false, 2 for partially false and 3 for other. As seen in Table <ref type="table" coords="4,423.75,201.55,3.81,10.91" target="#tab_0">1</ref>, the four classes are not equally distributed. We therefore applied random oversampling of all classes except the majority class using the imbalanced-learn package <ref type="bibr" coords="4,346.22,228.65,18.07,10.91" target="#b28">[29]</ref> with the aim to train a better classifier. Additionally, we generated abstractive and extractive summaries (we did this offline as in particular the generation of abstractive summaries was time-consuming). Before sending the text into our models we also tokenized and normalized the texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Model architecture</head><p>All models used are fine-tuned variants of Google's BERT <ref type="bibr" coords="4,356.51,318.92,12.99,10.91" target="#b6">[7]</ref> and use the bert-base-uncased implementation provided by Wolf et al. <ref type="bibr" coords="4,263.28,332.47,17.91,10.91" target="#b29">[30]</ref> in conjunction with a linear layer on top to predict the output. We have chosen BERT because it already has shown good performance in various text classification tasks <ref type="bibr" coords="4,191.65,359.57,17.76,10.91" target="#b30">[31]</ref> as well as in fake news detection <ref type="bibr" coords="4,355.16,359.57,16.09,10.91" target="#b31">[32]</ref>. Due to limited computational resources we could not use a more sophisticated BERT model like RoBERTa <ref type="bibr" coords="4,434.51,373.12,16.40,10.91" target="#b32">[33]</ref>. One of the main drawbacks of BERT-based models is the maximum sequence length each model is able to process which is at a maximum of 512 tokens (word pieces) for BERT. Unfortunately, fake news articles often are a longer than this value <ref type="bibr" coords="4,271.10,413.77,16.08,10.91" target="#b33">[34]</ref>. In the provided dataset the mean token length is 806 with at least 55% of texts exceeding the 512 token limit. As these values are calculated with nltk <ref type="bibr" coords="4,109.58,440.87,18.01,10.91" target="#b34">[35]</ref> and word pieces do not exactly match tokens, the real ratio is even higher (all other token values reported are calculated similarly). By default, BERT-based models simply truncate the text to the desired input length (or apply padding if it is too short). This leads to the loss of potentially important information in the input text. To circumvent this issue we propose three different solutions, all aimed at compressing the original text:</p><p>‚Ä¢ Modified hierarchical transformer representation ‚Ä¢ Extractive summarization ‚Ä¢ Abstractive summarization Hierarchical transformer representations have been introduced by Pappagari et al. <ref type="bibr" coords="4,450.41,568.35,16.21,10.91" target="#b35">[36]</ref>. In their work they suggest splitting the input text into smaller text segments with overlapping parts (stride) to represent the structure of the text. In our model we split the text into parts of 500 tokens with a stride length of 50. After getting the BERT embeddings for each text segment we then calculated the mean representation dimensionally and fed this into BERT. The output of BERT is then used to classify the input text. Mean embeddings have been successfully used before by Mulyar et al. <ref type="bibr" coords="4,193.09,649.65,17.91,10.91" target="#b36">[37]</ref> Another possible solution is to use automatic summarization to get a more condensed text representation. Deep learning models such as BART <ref type="bibr" coords="5,324.89,100.52,11.45,10.91" target="#b7">[8]</ref>, XLNet <ref type="bibr" coords="5,374.55,100.52,17.93,10.91" target="#b37">[38]</ref> or ALBERT <ref type="bibr" coords="5,449.04,100.52,17.93,10.91" target="#b38">[39]</ref> perform exceptionally well on summarization tasks like SQuAD <ref type="bibr" coords="5,346.22,114.06,18.06,10.91" target="#b39">[40]</ref> or ELI5 <ref type="bibr" coords="5,404.52,114.06,18.07,10.91" target="#b40">[41]</ref> -even sometimes surpassing humans. These algorithms are able to reduce the text length by a significant amount if desired, which is ideal for the initial problem with BERT. In our work we use the extractive summarization technology implemented by <ref type="bibr" coords="5,285.49,154.71,16.22,10.91" target="#b41">[42]</ref>. Note that while this method is also based on BERT it has no maximum sequence length. To ensure a better summarization quality while keeping the running time reasonable we activated co-reference handling (better contextualization) and used distilBERT <ref type="bibr" coords="5,205.60,195.36,17.91,10.91" target="#b42">[43]</ref> as the underlying model. In contrast to <ref type="bibr" coords="5,402.40,195.36,12.84,10.91" target="#b8">[9]</ref> we are interested in long sequences and not only the first two sentences for classifying. After manually inspecting different configurations we settled with a summarization ratio of 0.40.</p><p>Apart from an extractive approach we also implemented an abstractive technique based on BART. This model is specifically well suited for text generation, outperforming similar ones on summarization tasks like SQuAD 1.1 <ref type="bibr" coords="5,255.51,263.11,11.45,10.91" target="#b7">[8]</ref>. The Huggingface transformers library <ref type="bibr" coords="5,447.11,263.11,17.93,10.91" target="#b29">[30]</ref> provides an easy way to use BART-models for sequence generation. Because of the repetitive nature of greedy and beam search <ref type="bibr" coords="5,196.08,290.20,16.31,10.91" target="#b43">[44,</ref><ref type="bibr" coords="5,214.71,290.20,13.95,10.91" target="#b44">45]</ref> we used Top-K <ref type="bibr" coords="5,297.72,290.20,17.76,10.91" target="#b45">[46]</ref> and Top-p sampling <ref type="bibr" coords="5,405.41,290.20,17.76,10.91" target="#b46">[47]</ref> for our summaries. The exact model we used is sshleifer/distilbart-cnn-12-6 <ref type="foot" coords="5,328.30,302.00,3.71,7.97" target="#foot_2">4</ref> , which is a smaller BART model trained on a news summarization dataset by Hermann et al. <ref type="bibr" coords="5,327.80,317.30,16.29,10.91" target="#b47">[48]</ref>. In our final configuration we used the 100 (Top-K) most likely words and a probability (Top-p) of 95%. Like BERT, BART has a sequence limit of 1024 tokens. Therefore, if the input text was longer than 1000 tokens we used our first approach to ensure all parts of the text are taken into consideration when getting summarized. We also tried to get a summarization ratio of roughly 40% for better comparability to the extractive approach. However, as both approaches are not deterministic this cannot always be guaranteed (also, as noted, both approaches take quite a while to execute, so we saved the results in files once generated). Additionally, due to the late release of the dataset we could not try out many configurations but instead had to use suggested configurations.</p><p>Finally, the submitted models all use the hierarchical text representation (even when using text summaries). There is one model for each type of input text aka. no summary, extractive summary or abstractive summary. We also submitted a run without oversampling for better comparability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Experimental setup</head><p>For training, we represented each input as [CLS] + title + [SEP] + text, where text is either the original text or one of the two summaries produced and [CLS] is a classification token and [SEP] is a token to indicate a separator between two sentences. For training, we use an 80/20 training/validation split and optimize hyperparameters based on the loss of the validation set. We used the same initial random state and split for all configurations to provide a better comparability. We used a batch size of 8, an initial learning rate of 5e-5, a weight decay of 0.01 with 500 warm-up steps and three training epochs with an AdamW <ref type="bibr" coords="5,389.17,610.92,17.82,10.91" target="#b48">[49]</ref> optimizer. Everything was trained on a single RTX 2080 Ti with 11 GB VRAM using the Huggingface library.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head><p>We report three sets of results -(a) official results for all four of our runs, and for comparison we also present results obtained on (b) the development set as well as (c) the test set without hyperparameter tuning (not submitted to the challenge).</p><p>First of all, in Table <ref type="table" coords="6,193.42,151.93,5.17,10.91" target="#tab_1">2</ref> we present the official results as returned to us by the shared task organizers. We marked the best-performing model for each metric in bold. <ref type="foot" coords="6,442.48,163.72,3.71,7.97" target="#foot_3">5</ref> Recall, that hierarchical transformer representation is applied to the source text in all of our runs, i.e. the term "original texts" refers to text that has been created this way but without subsequently applying abstractive or extractive summarization, respectively. To contextualise the official results better (and also due to the fact that at this point we do not have official baseline results to compare against), we also report the results on the validation set (see Table <ref type="table" coords="6,195.17,360.65,3.50,10.91" target="#tab_2">3</ref>). The configuration is the same as described in section 5.3 but without hyperparameter tuning (using a 80/20 split of the training data). Table <ref type="table" coords="6,127.65,516.20,5.17,10.91" target="#tab_3">4</ref> also follows the same configuration, but has been calculated once the test set was available and does not use hyperparameter tuning either (using all training data and evaluating on the test data).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head><p>First of all we observe that the non-fine-tuned model and the model which has been trained without oversampling the minority classes perform worst in all setups. This is in line with expectations. It however gets more complicated when comparing the other models. The official runs suggest that BERT w/ abstractive summaries wins overall by a tiny bit, but is on par with BERT w/ original texts (i.e. the original articles hierarchically transformed but without applying summarization). Given that this makes it into 8th place of 25 submissions and the fact that abstractive summarization is becoming more and more competitive, we see this as a clear signal that our general conceptual idea is a promising one.</p><p>When taking a look at the official results for BERT w/ extractive summaries and BERT w/o oversampling, both models are still reasonably well-placed in the rankings. They would have ranked 16th and 18th respectively showing how well a vanilla BERT is pre-trained already.</p><p>Looking beyond the official results, we observe some wide variation of scores though. While BERT w/ extractive summaries performs better than other approaches when not using hyperparameter tuning (see Table <ref type="table" coords="7,202.48,362.46,3.50,10.91" target="#tab_3">4</ref>), it scores way worse when hyperparameter tuning is in place (Table <ref type="table" coords="7,89.29,376.01,3.63,10.91" target="#tab_1">2</ref>). In fact, not applying hyperparameter tuning would rank the system in 3rd position of the ranked list of 25 runs with an F1-macro of 0.508. This seems to be an indication of overfitting happening internally. The validation set in general seems to be not well suited to learn with, as all tuned models perform better when applying them to the test dataset directly (this is also the case, when the training set is exactly the same). All this raises some concerns about the size, robustness and generalisability of the test collection. This is by no means a novel finding, and some researchers go as far as to call the current (commonly applied) NLP evaluation approach to be broken <ref type="bibr" coords="7,148.32,470.85,16.30,10.91" target="#b49">[50]</ref>. We conclude that we will have to test our methodology on a wide range of additional collections to gain a better understanding of its strengths and weaknesses.</p><p>One last point to note, there seems to be only little difference in performance when using BERT w/ original texts or BERT w/ abstractive summaries. Interestingly, the respective models achieve very similar performance independently of the dataset and experimental setup used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Limitiatons</head><p>Due to the nature of such challenges there was not much time to try different experimental setups. Especially abstractive summarization generation has a lot of different parameters to work with. Unfortunately, one iteration for those alone takes about half a day of computing time on our system. While we always tried to use the recommended configurations when possible, we could only use BERT with a maximum batch size of 8. It would have been interesting to see, whether batch sizes of 16 or greater make a significant difference in performance. Previous work on parameter tuning of BERT suggests this <ref type="bibr" coords="7,275.29,656.03,16.09,10.91" target="#b50">[51]</ref>. While BERT itself is a very sophisticated system, an approach using an even better system like RoBERTa <ref type="bibr" coords="7,334.66,669.58,17.81,10.91" target="#b32">[33]</ref> or XLNet <ref type="bibr" coords="7,398.63,669.58,17.81,10.91" target="#b37">[38]</ref> could outperform it. This has already been proven in their respective papers on other NLP tasks. The substantial difference in performance between the official results (Table <ref type="table" coords="8,362.28,100.52,4.21,10.91" target="#tab_1">2</ref>) and our reruns on the test set (Table <ref type="table" coords="8,120.42,114.06,4.25,10.91" target="#tab_3">4</ref>) indicate that the chosen experimental setup might either not have been ideal for this task or the data sets were simply too small. While hyperparameter tuning is often useful, in this case we achieve better results without it. However, this could also be because of the validation/dev set we acquired. As seen in Table <ref type="table" coords="8,316.99,154.71,5.17,10.91" target="#tab_2">3</ref> all models perform worse here than on the actual test set. This indicates a bad seed for our validation set we optimized on. Also, a summarization ratio of 0.40 was picked quite arbitrarily which might or might not restrict the full potential of summarizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Future Work</head><p>In future it would certainly be interesting to explore more configurations and applications of automatic summarization. We believe summarization has the potential to enable better transferable knowledge. This could be useful for a variety of classification tasks as many models often only work in a certain domain. Therefore, it would be interesting to compare models trained on automatic summarizations and compare their performance in different domains working as a kind of "normalization" technique. We expect summarization of texts to limit overfitting in the future. With the results of Table <ref type="table" coords="8,311.87,326.38,5.04,10.91" target="#tab_3">4</ref> in mind, we hypothesize that there is a lot of room for improvement still available. We plan to apply our approaches on more datasets in the future and try to optimize the tuning further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions</head><p>We presented an approach for fake news detection that is based on the powerful paradigm of transformer-based embeddings and utilises text summarization as the main text transformation step before classifying a document. The results suggest that this is indeed a worthwhile direction of work and in future work we plan to explore this further. We note that using oversampling has a strong positive effect on system performance. What we did also observe was that the performance obtained on different datasets and based on different models of hypertuning varied substantially. One way forward is to apply our framework to larger datasets to see how robust extractive and abstractive summarisation might be for the task at hand.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.99,495.65,311.38,59.44"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="3,89.29,507.65,311.08,47.44"><row><cell>Dataset statistics</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Dataset False Partially False True Other</cell></row><row><cell cols="2">Training 486</cell><cell>235</cell><cell>153</cell><cell>76</cell></row><row><cell>Test</cell><cell>113</cell><cell>141</cell><cell>69</cell><cell>41</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,88.99,234.07,367.58,84.16"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="6,89.29,246.07,367.29,72.15"><row><cell>Official results</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="4">Accuracy Precision Recall F1-macro</cell></row><row><cell>BERT w/o oversampling</cell><cell>0.387</cell><cell>0.636</cell><cell>0.300</cell><cell>0.25570</cell></row><row><cell>BERT w/ original texts</cell><cell>0.432</cell><cell>0.409</cell><cell cols="2">0.402 0.40413</cell></row><row><cell cols="2">BERT w/ extractive summaries 6 0.370</cell><cell>0.549</cell><cell>0.362</cell><cell>0.32986</cell></row><row><cell cols="2">BERT w/ abstractive summaries 0.438</cell><cell>0.476</cell><cell>0.385</cell><cell>0.40415</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,88.99,402.14,367.58,98.72"><head>Table 3</head><label>3</label><figDesc>Performance on validation/dev set without hyperparameter tuning</figDesc><table coords="6,138.70,430.23,317.88,70.63"><row><cell>Model</cell><cell cols="4">Accuracy Precision Recall F1-macro</cell></row><row><cell>BERT w/o fine-tuning</cell><cell>0.421</cell><cell>0.379</cell><cell>0.370</cell><cell>0.356</cell></row><row><cell>BERT w/o oversampling</cell><cell>0.584</cell><cell>0.525</cell><cell>0.371</cell><cell>0.329</cell></row><row><cell>BERT w/ original texts</cell><cell>0.511</cell><cell>0.378</cell><cell>0.379</cell><cell>0.369</cell></row><row><cell>BERT w/ extractive summaries</cell><cell>0.568</cell><cell>0.498</cell><cell cols="2">0.463 0.459</cell></row><row><cell cols="2">BERT w/ abstractive summaries 0.542</cell><cell>0.362</cell><cell>0.397</cell><cell>0.376</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,88.99,90.49,367.58,98.72"><head>Table 4</head><label>4</label><figDesc>Performance on test set without hyperparameter tuning</figDesc><table coords="7,138.70,118.58,317.88,70.63"><row><cell>Model</cell><cell cols="4">Accuracy Precision Recall F1-macro</cell></row><row><cell>BERT w/o fine-tuning</cell><cell>0.251</cell><cell>0.328</cell><cell>0.315</cell><cell>0.251</cell></row><row><cell>BERT w/o oversampling</cell><cell>0.379</cell><cell>0.419</cell><cell>0.355</cell><cell>0.333</cell></row><row><cell>BERT w/ original texts</cell><cell>0.472</cell><cell>0.487</cell><cell>0.481</cell><cell>0.465</cell></row><row><cell>BERT w/ extractive summaries</cell><cell>0.531</cell><cell>0.525</cell><cell cols="2">0.523 0.508</cell></row><row><cell cols="2">BERT w/ abstractive summaries 0.489</cell><cell>0.509</cell><cell>0.450</cell><cell>0.459</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="3,108.93,671.04,192.37,8.97"><p>https://competitions.codalab.org/competitions/31238</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="4,108.93,671.04,176.49,8.97"><p>https://github.com/phHartl/CheckThatLab_2021</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="5,108.93,670.99,186.31,8.97"><p>https://huggingface.co/sshleifer/distilbart-cnn-12-6</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="6,108.93,649.12,313.31,8.97"><p>Because of the extremely close values in Table2we added additional fractional digits.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="6,108.93,660.08,397.06,8.97;6,89.29,671.04,163.49,8.97"><p>The value for extractive summarization has been calculated with the official evaluation script afterwards, as there was a problem when uploading the file</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported by the project <rs type="projectName">COURAGE: A Social Media Companion Safeguarding and Educating Students</rs> funded by the <rs type="funder">Volkswagen Foundation</rs>, grant number <rs type="grantNumber">95564</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_vYxvjzd">
					<idno type="grant-number">95564</idno>
					<orgName type="project" subtype="full">COURAGE: A Social Media Companion Safeguarding and Educating Students</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,112.66,637.75,366.34,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,187.55,637.75,93.54,10.91">History of Fake News</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Burkhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,290.47,637.75,124.89,10.91">Library Technology Reports</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="5" to="9" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,651.30,395.17,10.91;8,111.60,664.85,59.10,10.91" xml:id="b1">
	<monogr>
		<title level="m" coord="8,286.42,651.30,134.69,10.91">Modern Information Retrieval</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Ribeiro-Neto</surname></persName>
		</editor>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>2nd ed</note>
</biblStruct>

<biblStruct coords="9,112.66,86.97,393.33,10.91;9,112.66,100.52,179.52,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,232.29,86.97,218.03,10.91">Social media and fake news in the 2016 election</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Allcott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gentzkow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,460.42,86.97,45.57,10.91;9,112.66,100.52,100.66,10.91">Journal of economic perspectives</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="211" to="236" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,114.06,393.33,10.91;9,112.66,127.61,395.01,10.91;9,112.41,141.16,394.87,10.91;9,112.66,154.71,212.23,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,164.49,114.06,284.99,10.91">Fighting Information Pollution with Decision Support Systems</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Orman</surname></persName>
		</author>
		<idno type="DOI">10.1080/07421222.1984.11517704</idno>
		<ptr target="https://doi.org/10.1080/07421222.1984.11517704" />
	</analytic>
	<monogr>
		<title level="j" coord="9,460.39,114.06,45.60,10.91;9,112.66,127.61,153.07,10.91">Journal of Management Information Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="64" to="71" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,168.26,393.33,10.91;9,112.66,181.81,393.61,10.91;9,112.41,195.36,213.46,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,164.92,168.26,341.07,10.91;9,112.66,181.81,54.69,10.91">On deception and deception detection: Content analysis of computer-mediated stated beliefs</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">L</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,173.77,181.81,332.50,10.91">Proceedings of the American Society for Information Science and Technology</title>
		<meeting>the American Society for Information Science and Technology</meeting>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,208.91,394.53,10.91;9,112.66,222.46,393.33,10.91;9,112.66,236.01,393.33,10.91;9,112.66,249.56,394.53,10.91;9,112.66,263.11,393.33,10.91;9,112.66,276.66,394.53,10.91;9,112.39,290.20,394.89,10.91;9,112.31,303.75,374.62,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,488.80,222.46,17.19,10.91;9,112.66,236.01,393.33,10.91;9,112.66,249.56,99.49,10.91">The CLEF-2021 checkthat! lab on detecting check-worthy claims, previously fact-checked claims, and fake news</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">D S</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barr√≥n-Cede√±o</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>M√≠guez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Stru√ü</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mandl</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-72240-1_75</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-72240-1_75.doi:10.1007/978-3-030-72240-1\_75" />
	</analytic>
	<monogr>
		<title level="m" coord="9,202.85,263.11,303.14,10.91;9,112.66,276.66,105.71,10.91;9,414.33,276.66,89.84,10.91">Advances in Information Retrieval -43rd European Conference on IR Research, ECIR 2021</title>
		<title level="s" coord="9,185.14,291.22,144.92,9.72">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Moens</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Mothe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Perego</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Potthast</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</editor>
		<meeting><address><addrLine>Virtual Event</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021-04-01">March 28 -April 1, 2021. 2021</date>
			<biblScope unit="volume">12657</biblScope>
			<biblScope unit="page" from="639" to="649" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II</note>
</biblStruct>

<biblStruct coords="9,112.66,317.30,393.33,10.91;9,112.33,330.85,393.65,10.91;9,112.66,344.40,393.32,10.91;9,112.66,357.95,393.33,10.91;9,112.66,371.50,394.03,10.91;9,112.66,385.05,234.20,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,321.08,317.30,184.91,10.91;9,112.33,330.85,192.59,10.91">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://www.aclweb.org/anthology/N19-1423.doi:10.18653/v1/N19-1423" />
	</analytic>
	<monogr>
		<title level="m" coord="9,330.38,330.85,175.60,10.91;9,112.66,344.40,393.32,10.91;9,112.66,357.95,99.97,10.91">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="9,112.66,398.60,395.17,10.91;9,112.66,412.15,395.17,10.91;9,112.66,425.70,393.32,10.91;9,112.66,439.25,394.52,10.91;9,112.66,452.79,395.01,10.91;9,112.66,466.34,191.55,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,146.14,412.15,361.68,10.91;9,112.66,425.70,174.42,10.91">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.703.doi:10.18653/v1/2020.acl-main.703" />
	</analytic>
	<monogr>
		<title level="m" coord="9,311.16,425.70,194.82,10.91;9,112.66,439.25,390.36,10.91">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,479.89,395.17,10.91;9,112.66,493.44,395.17,10.91;9,112.66,506.99,395.17,10.91;9,112.66,520.54,395.01,10.91;9,112.66,534.09,209.35,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,193.35,479.89,314.48,10.91;9,112.66,493.44,29.71,10.91">Connecting the Dots Between Fact Verification and Fake News Detection</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.165</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.coling-main.165.doi:10.18653/v1/2020.coling-main.165" />
	</analytic>
	<monogr>
		<title level="m" coord="9,170.62,493.44,337.21,10.91;9,112.66,506.99,288.41,10.91">Proceedings of the 28th International Conference on Computational Linguistics, International Committee on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics, International Committee on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1820" to="1825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,547.64,393.33,10.91;9,112.66,561.19,393.33,10.91;9,112.66,574.74,394.04,10.91;9,112.41,588.29,394.77,10.91;9,112.66,601.84,48.38,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,336.53,547.64,169.46,10.91;9,112.66,561.19,393.33,10.91;9,112.66,574.74,69.52,10.91">FakeNewsNet: A Data Repository with News Content, Social Context, and Spatiotemporal Information for Studying Fake News on Social Media</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mahudeswaran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1089/big.2020.0062</idno>
		<ptr target="https://www.liebertpub.com/doi/abs/10.1089/big.2020.0062.doi:10.1089/big.2020.0062" />
	</analytic>
	<monogr>
		<title level="j" coord="9,191.34,574.74,39.38,10.91">Big Data</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="171" to="188" />
			<date type="published" when="2020">2020</date>
			<publisher>Mary Ann Liebert, Inc., publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,615.39,393.33,10.91;9,112.66,628.93,394.61,10.91;9,112.31,642.48,248.81,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.04088[cs</idno>
		<idno>arXiv: 2011.04088 version: 1</idno>
		<ptr target="http://arxiv.org/abs/2011.04088" />
		<title level="m" coord="9,249.99,615.39,255.99,10.91;9,112.66,628.93,210.74,10.91">MM-COVID: A Multilingual and Multidimensional Data Repository for CombatingCOVID-19 Fake News</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,656.03,395.17,10.91;9,112.41,669.58,393.57,10.91;10,112.66,86.97,393.73,10.91;10,112.34,100.52,367.30,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,293.97,656.03,213.86,10.91;9,112.41,669.58,127.24,10.91">ReCOVery: A Multimodal Repository for COVID-19 News Credibility Research</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mulay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Ferrara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zafarani</surname></persName>
		</author>
		<idno type="DOI">10.1145/3340531.3412880</idno>
		<ptr target="https://doi.org/10.1145/3340531.3412880" />
	</analytic>
	<monogr>
		<title level="m" coord="9,262.73,669.58,243.25,10.91;10,112.66,86.97,188.72,10.91">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3205" to="3212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,114.06,393.53,10.91;10,112.66,127.61,393.33,10.91;10,112.28,141.16,393.71,10.91;10,112.33,154.71,394.05,10.91;10,112.66,168.26,395.01,10.91;10,112.66,181.81,138.14,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,361.08,114.06,145.11,10.91;10,112.66,127.61,146.63,10.91">FEVER: a Large-scale Dataset for Fact Extraction and VERification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1074</idno>
		<ptr target="https://www.aclweb.org/anthology/N18-1074.doi:10.18653/v1/N18-1074" />
	</analytic>
	<monogr>
		<title level="m" coord="10,283.66,127.61,222.32,10.91;10,112.28,141.16,393.71,10.91;10,112.33,154.71,57.15,10.91">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="809" to="819" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="10,112.66,195.36,393.32,10.91;10,112.66,208.91,394.62,10.91;10,112.66,222.46,223.37,10.91" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gruppi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">D</forename><surname>Horne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Adalƒ±</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08444[cs</idno>
		<idno>arXiv: 2003.08444</idno>
		<ptr target="http://arxiv.org/abs/2003.08444" />
		<title level="m" coord="10,262.53,195.36,243.45,10.91;10,112.66,208.91,228.37,10.91">NELA-GT-2019: A Large Multi-Labelled News Dataset for The Study of Misinformation in News Articles</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,236.01,394.53,10.91;10,112.66,249.56,393.33,10.91;10,112.66,263.11,393.53,10.91;10,112.66,276.66,393.92,10.91;10,112.66,292.65,14.27,7.90" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,240.56,236.01,261.99,10.91">SAFE: Similarity-Aware Multi-modal Fake News Detection</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zafarani</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-47436-2_27</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,462.81,249.56,43.18,10.91;10,112.66,263.11,185.06,10.91">Advances in Knowledge Discovery and Data Mining</title>
		<title level="s" coord="10,305.41,263.11,155.29,10.91">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Lauw</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">C</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">-W</forename><surname>Wong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Ntoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E.-P</forename><surname>Lim</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S.-K</forename><surname>Ng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="354" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,303.75,393.32,10.91;10,112.41,317.30,76.13,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,178.53,303.75,199.81,10.91">Beurteilung der glaubhaftigkeit von aussagen</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Undeutsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,387.05,303.75,118.93,10.91">Handbuch der psychologie</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="26" to="181" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,330.85,395.17,10.91;10,112.66,344.40,395.17,10.91;10,112.66,357.95,395.17,10.91;10,112.66,371.50,395.17,10.91;10,112.66,385.05,395.01,10.91;10,112.66,398.60,209.35,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,317.70,330.85,190.13,10.91;10,112.66,344.40,395.17,10.91;10,112.66,357.95,12.79,10.91">Early Detection of Fake News by Utilizing the Credibility of News, Publishers, and Users based on Weakly Supervised Learning</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.475</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.coling-main.475.doi:10.18653/v1/2020.coling-main.475" />
	</analytic>
	<monogr>
		<title level="m" coord="10,160.88,357.95,346.95,10.91;10,112.66,371.50,288.41,10.91">Proceedings of the 28th International Conference on Computational Linguistics, International Committee on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics, International Committee on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5444" to="5454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,412.15,393.33,10.91;10,112.66,425.70,393.32,10.91;10,112.66,439.25,395.17,10.91;10,112.66,452.79,395.01,10.91;10,112.66,466.34,155.44,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,215.40,412.15,290.59,10.91;10,112.66,425.70,20.78,10.91">SAME: sentiment-aware multi-modal embedding for detecting fake news</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1145/3341161.3342894</idno>
		<ptr target="https://doi.org/10.1145/3341161.3342894.doi:10.1145/3341161.3342894" />
	</analytic>
	<monogr>
		<title level="m" coord="10,157.38,425.70,348.60,10.91;10,112.66,439.25,229.70,10.91">Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM &apos;19</title>
		<meeting>the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,479.89,393.33,10.91;10,112.66,493.44,393.33,10.91;10,112.66,506.99,395.17,10.91;10,112.66,520.54,395.01,10.91;10,112.66,534.09,155.44,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,329.45,479.89,176.53,10.91;10,112.66,493.44,38.53,10.91">The role of user profiles for fake news detection</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zafarani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3341161.3342927</idno>
		<ptr target="https://doi.org/10.1145/3341161.3342927.doi:10.1145/3341161.3342927" />
	</analytic>
	<monogr>
		<title level="m" coord="10,172.96,493.44,333.03,10.91;10,112.66,506.99,229.70,10.91">Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM &apos;19</title>
		<meeting>the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="436" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,547.64,393.33,10.91;10,112.66,561.19,393.32,10.91;10,112.66,574.74,395.01,10.91;10,112.66,588.29,138.28,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="10,308.68,547.64,197.30,10.91;10,112.66,561.19,211.32,10.91">Hierarchical Propagation Networks for Fake News Detection: Investigation and Exploitation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mahudeswaran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org/index.php/ICWSM/article/view/7329" />
	</analytic>
	<monogr>
		<title level="m" coord="10,332.34,561.19,173.64,10.91;10,112.66,574.74,167.97,10.91">Proceedings of the International AAAI Conference on Web and Social Media</title>
		<meeting>the International AAAI Conference on Web and Social Media</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="626" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,601.84,394.62,10.91;10,112.66,615.39,393.61,10.91;10,112.66,628.93,394.04,10.91;10,112.41,642.48,270.35,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="10,291.95,601.84,194.55,10.91">dEFEND: Explainable Fake News Detection</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3292500.3330935</idno>
		<ptr target="https://dl.acm.org/doi/10.1145/3292500.3330935.doi:10.1145/3292500.3330935" />
	</analytic>
	<monogr>
		<title level="m" coord="10,112.66,615.39,393.61,10.91;10,112.66,628.93,62.70,10.91">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>Anchorage AK USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="395" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,656.03,395.17,10.91;10,112.66,669.58,393.32,10.91;11,112.66,86.97,395.17,10.91;11,112.66,100.52,395.01,10.91;11,112.66,114.06,138.14,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="10,174.87,656.03,332.96,10.91;10,112.66,669.58,16.45,10.91">Liar, Liar Pants on Fire&quot;: A New Benchmark Dataset for Fake News Detection</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-2067</idno>
		<ptr target="https://www.aclweb.org/anthology/P17-2067.doi:10.18653/v1/P17-2067" />
	</analytic>
	<monogr>
		<title level="m" coord="10,152.01,669.58,353.97,10.91;11,112.66,86.97,49.38,10.91;11,286.65,86.97,191.87,10.91">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="422" to="426" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct coords="11,112.66,127.61,393.71,10.91;11,112.66,141.16,395.01,10.91;11,112.66,154.71,248.05,10.91" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ruchansky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.06437[cs,stat</idno>
		<idno>arXiv: 1901.06437</idno>
		<ptr target="http://arxiv.org/abs/1901.06437" />
		<title level="m" coord="11,391.23,127.61,115.14,10.91;11,112.66,141.16,233.10,10.91">Combating Fake News: A Survey on Identification and Mitigation Techniques</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,168.26,393.33,10.91;11,112.66,181.81,394.62,10.91;11,112.66,195.36,223.37,10.91" xml:id="b23">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07823[cs</idno>
		<idno>arXiv: 1703.07823</idno>
		<ptr target="http://arxiv.org/abs/1703.07823" />
		<title level="m" coord="11,484.83,168.26,21.15,10.91;11,112.66,181.81,236.39,10.91">Fake News Mitigation via Point Process Based Intervention</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,208.91,393.33,10.91;11,112.66,222.46,395.00,10.91;11,112.66,236.01,272.20,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="11,189.97,208.91,251.36,10.91">Fake News Propagation and Detection: A Sequential Model</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Papanastasiou</surname></persName>
		</author>
		<idno type="DOI">10.1287/mnsc.2019.3295</idno>
		<ptr target="https://pubsonline.informs.org/doi/10.1287/mnsc.2019.3295.doi:10.1287/mnsc.2019.3295" />
	</analytic>
	<monogr>
		<title level="j" coord="11,448.93,208.91,57.06,10.91;11,112.66,222.46,33.12,10.91">Management Science</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="1826" to="1846" />
			<date type="published" when="2020">2020</date>
			<publisher>INFORMS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,249.56,393.33,10.91;11,112.66,263.11,393.33,10.91;11,112.66,276.66,393.33,10.91;11,111.79,290.20,395.48,10.91;11,112.66,303.75,337.56,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="11,404.00,249.56,101.98,10.91;11,112.66,263.11,302.32,10.91">Leveraging the Crowd to Detect and Reduce the Spread of Fake News and Misinformation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Tabibian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sch√∂lkopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gomez-Rodriguez</surname></persName>
		</author>
		<idno type="DOI">10.1145/3159652.3159734</idno>
		<ptr target="https://doi.org/10.1145/3159652.3159734.doi:10.1145/3159652.3159734" />
	</analytic>
	<monogr>
		<title level="m" coord="11,438.96,263.11,67.03,10.91;11,112.66,276.66,393.33,10.91;11,111.79,290.20,11.69,10.91">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, WSDM &apos;18</title>
		<meeting>the Eleventh ACM International Conference on Web Search and Data Mining, WSDM &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="324" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,317.30,393.59,10.91;11,112.66,330.85,146.44,10.91" xml:id="b26">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00502</idno>
		<title level="m" coord="11,168.48,317.30,305.06,10.91">Amused: An annotation framework of multi-modal social media data</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,344.40,393.33,10.91;11,112.66,357.95,283.01,10.91" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="11,292.17,344.40,213.82,10.91;11,112.66,357.95,42.26,10.91">An exploratory study of covid-19 misinformation on twitter</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dirkson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Majchrzak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,162.86,357.95,154.86,10.91">Online Social Networks and Media</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">100104</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,371.50,393.33,10.91;11,112.66,385.05,393.33,10.91;11,112.66,398.60,221.66,10.91" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="11,295.74,371.50,210.25,10.91;11,112.66,385.05,235.91,10.91">Imbalanced-learn: A python toolbox to tackle the curse of imbalanced datasets in machine learning</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lema√Ætre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">K</forename><surname>Aridas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,356.90,385.05,149.09,10.91;11,112.66,398.60,39.91,10.91">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="559" to="563" />
			<date type="published" when="2017">2017</date>
			<publisher>Publisher</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,412.15,394.53,10.91;11,112.66,425.70,394.52,10.91;11,112.66,439.25,173.79,10.91" xml:id="b29">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<title level="m" coord="11,178.58,425.70,324.14,10.91">HuggingFace&apos;s Transformers: State-of-the-art natural language processing</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,452.79,393.32,10.91;11,112.66,466.34,393.33,10.91;11,112.66,479.89,299.20,10.91" xml:id="b30">
	<monogr>
		<title level="m" type="main" coord="11,462.95,452.79,43.03,10.91;11,112.66,466.34,311.14,10.91">Enriching BERT with Knowledge Graph Embeddings for Document Classification</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ostendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bourgonje</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Moreno-Schneider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Rehm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Gipp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08402[cs</idno>
		<idno>arXiv: 1909.08402</idno>
		<ptr target="http://arxiv.org/abs/1909.08402" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,493.44,394.53,10.91;11,112.66,506.99,393.33,10.91;11,112.66,520.54,395.01,10.91;11,112.66,534.09,204.07,10.91" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="11,239.55,493.44,262.99,10.91">BERT-Based Mental Model, a Better Fake News Detector</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3404555.3404607</idno>
		<ptr target="https://doi.org/10.1145/3404555.3404607" />
	</analytic>
	<monogr>
		<title level="m" coord="11,128.44,506.99,377.55,10.91;11,112.66,520.54,49.19,10.91">Proceedings of the 2020 6th International Conference on Computing and Artificial Intelligence</title>
		<meeting>the 2020 6th International Conference on Computing and Artificial Intelligence<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="396" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,547.64,394.53,10.91;11,112.30,561.19,393.68,10.91;11,112.66,574.74,107.17,10.91" xml:id="b32">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m" coord="11,173.53,561.19,256.77,10.91">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,588.29,393.33,10.91;11,112.66,601.84,382.45,10.91" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="11,268.49,588.29,237.50,10.91;11,112.66,601.84,35.49,10.91">Enhanced news sentiment analysis using deep learning methods</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Souma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Vodenska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Aoyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,157.15,601.84,178.77,10.91">Journal of Computational Social Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="33" to="46" />
			<date type="published" when="2019">2019</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,615.39,382.58,10.91" xml:id="b34">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<idno>arXiv preprint cs/0205028</idno>
		<title level="m" coord="11,189.88,615.39,150.96,10.91">Nltk: The natural language toolkit</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,628.93,393.53,10.91;11,112.66,642.48,394.03,10.91;11,112.66,656.03,152.16,10.91" xml:id="b35">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pappagari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>≈ªelasko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Carmiel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10781[cs,stat</idno>
		<idno>arXiv: 1910.10781</idno>
		<ptr target="http://arxiv.org/abs/1910.10781" />
		<title level="m" coord="11,372.34,628.93,133.85,10.91;11,112.66,642.48,135.22,10.91">Hierarchical Transformers for Long Document Classification</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,669.58,393.33,10.91;12,112.66,86.97,394.53,10.91;12,112.66,100.52,377.18,10.91" xml:id="b36">
	<monogr>
		<title level="m" type="main" coord="11,351.71,669.58,154.28,10.91;12,112.66,86.97,389.67,10.91">Phenotyping of Clinical Notes with Improved Document Classification Models Using Contextualized Neural Language Models</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mulyar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Schumacher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rouhizadeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dredze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13664[cs</idno>
		<idno>arXiv: 1910.13664</idno>
		<ptr target="http://arxiv.org/abs/1910.13664" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,114.06,393.33,10.91;12,112.66,127.61,393.57,10.91;12,112.33,141.16,29.19,10.91" xml:id="b37">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m" coord="12,419.52,114.06,86.47,10.91;12,112.66,127.61,242.29,10.91">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,154.71,393.53,10.91;12,112.66,168.26,393.33,10.91;12,112.33,181.81,29.19,10.91" xml:id="b38">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<title level="m" coord="12,408.01,154.71,98.18,10.91;12,112.66,168.26,237.41,10.91">Albert: A lite bert for self-supervised learning of language representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,195.36,393.33,10.91;12,112.66,208.91,280.07,10.91" xml:id="b39">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<title level="m" coord="12,324.88,195.36,181.11,10.91;12,112.66,208.91,98.46,10.91">Squad: 100,000+ questions for machine comprehension of text</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,222.46,393.33,10.91;12,112.66,236.01,226.59,10.91" xml:id="b40">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.09190</idno>
		<title level="m" coord="12,390.87,222.46,115.11,10.91;12,112.66,236.01,43.99,10.91">Eli5: Long form question answering</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,249.56,393.33,10.91;12,112.66,263.11,107.17,10.91" xml:id="b41">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04165</idno>
		<title level="m" coord="12,157.33,249.56,277.18,10.91">Leveraging BERT for extractive text summarization on lectures</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,276.66,394.52,10.91;12,112.66,290.20,395.01,10.91;12,112.66,303.75,111.33,10.91" xml:id="b42">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108[cs</idno>
		<idno>arXiv: 1910.01108</idno>
		<ptr target="http://arxiv.org/abs/1910.01108" />
		<title level="m" coord="12,295.22,276.66,211.96,10.91;12,112.66,290.20,116.00,10.91">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,317.30,394.53,10.91;12,112.66,330.85,393.59,10.91;12,112.66,344.40,146.44,10.91" xml:id="b43">
	<monogr>
		<title level="m" type="main" coord="12,112.66,330.85,358.51,10.91">Diverse beam search: Decoding diverse solutions from neural sequence models</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Vijayakumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02424</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,357.95,393.33,10.91;12,112.66,371.50,393.32,10.91;12,112.66,385.05,107.17,10.91" xml:id="b44">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Britz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kurzweil</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03185</idno>
		<title level="m" coord="12,381.58,357.95,124.41,10.91;12,112.66,371.50,319.48,10.91">Generating high-quality and informative conversation responses with sequence-to-sequence models</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,398.60,393.33,10.91;12,112.66,412.15,107.17,10.91" xml:id="b45">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04833</idno>
		<title level="m" coord="12,260.45,398.60,166.61,10.91">Hierarchical neural story generation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,425.70,394.53,10.91;12,112.66,439.25,173.79,10.91" xml:id="b46">
	<monogr>
		<title level="m" type="main" coord="12,315.26,425.70,187.34,10.91">The curious case of neural text degeneration</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09751</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,452.79,394.53,10.91;12,112.33,466.34,376.64,10.91" xml:id="b47">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Koƒçisk≈∑</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03340</idno>
		<title level="m" coord="12,112.33,466.34,193.65,10.91">Teaching machines to read and comprehend</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,479.89,393.33,10.91;12,112.66,493.44,107.17,10.91" xml:id="b48">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m" coord="12,238.15,479.89,182.94,10.91">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,506.99,395.17,10.91;12,112.66,520.54,395.17,10.91;12,112.33,534.09,393.65,10.91;12,112.66,547.64,395.17,10.91;12,112.66,561.19,395.17,10.91;12,112.41,574.74,394.86,10.91;12,112.31,588.29,230.16,10.91" xml:id="b49">
	<analytic>
		<title level="a" type="main" coord="12,251.05,506.99,256.78,10.91;12,112.66,520.54,98.63,10.91">What will it take to fix benchmarking in natural language understanding?</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2021.naacl-main.385/" />
	</analytic>
	<monogr>
		<title level="m" coord="12,438.45,534.09,67.53,10.91;12,112.66,547.64,395.17,10.91;12,112.66,561.19,317.96,10.91">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Rumshisky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Hakkani-T√ºr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bethard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Cotterell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Chakraborty</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</editor>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">June 6-11, 2021. 2021</date>
			<biblScope unit="page" from="4843" to="4855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,601.84,393.53,10.91;12,112.66,615.39,393.33,10.91;12,112.66,628.93,394.53,10.91;12,112.66,642.48,394.03,10.91;12,112.66,656.03,308.82,10.91" xml:id="b50">
	<analytic>
		<title level="a" type="main" coord="12,257.64,601.84,248.55,10.91;12,112.66,615.39,142.68,10.91">Evaluating Unsupervised Representation Learning for Detecting Stances of Fake News</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Guderlei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>A√üenmacher</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.558</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.coling-main.558.doi:10.18653/v1/2020.coling-main.558" />
	</analytic>
	<monogr>
		<title level="m" coord="12,281.18,615.39,224.80,10.91;12,112.66,628.93,390.21,10.91">Proceedings of the 28th International Conference on Computational Linguistics, International Committee on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics, International Committee on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6339" to="6349" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
