<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.02,75.44,451.09,17.04;1,72.02,96.20,203.22,17.04">SCUoL at CheckThat! 2021: An AraBERT Model for Check-Worthiness of Arabic Tweets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.02,129.10,69.67,10.80"><forename type="first">Saud</forename><surname>Althabiti</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Leeds</orgName>
								<address>
									<postCode>LS2 9JT</postCode>
									<settlement>Leeds</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">King Abdulaziz University</orgName>
								<address>
									<postCode>21589</postCode>
									<settlement>Jeddah</settlement>
									<country>Kingdom of Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,160.82,129.10,97.49,10.80"><forename type="first">Mohammad</forename><surname>Alsalka</surname></persName>
							<email>a.alsalka@leeds.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Leeds</orgName>
								<address>
									<postCode>LS2 9JT</postCode>
									<settlement>Leeds</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,288.89,129.10,55.65,10.80"><forename type="first">Eric</forename><surname>Atwell</surname></persName>
							<email>e.s.atwell@leeds.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Leeds</orgName>
								<address>
									<postCode>LS2 9JT</postCode>
									<settlement>Leeds</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.02,75.44,451.09,17.04;1,72.02,96.20,203.22,17.04">SCUoL at CheckThat! 2021: An AraBERT Model for Check-Worthiness of Arabic Tweets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">00C534F09CF7B9550D1EBB560DE07A39</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>AraBERTv2</term>
					<term>AraBERTv0.2</term>
					<term>Check-worthiness</term>
					<term>Fact-check</term>
					<term>CheckThat Lab</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many people nowadays tend to explore social media to obtain news and find information about various events and activities. However, an abundance of misleading and false information is spreading every day for many purposes, dramatically impacting societies. Therefore, it is vitally important to identify false information on social media to help individuals distinguish the truth and protect communities from the harmful effects of false information. For this reason, determining which information has the priority to be scrutinized is a significant prior step that several studies have considered. In this paper, we have addressed Subtask-1A(Arabic) of CLEF2021 CheckThat! Lab. We have done that in two steps. The first involved preprocessing the provided dataset with text segmentation and tokenization. In the second step, we implemented different models on the Arabic tweets in order to binary classify them according to whether a specific tweet is worth being considered for fact-checking or not. We mainly compared two versions of the pre-trained AraBERT model with some of the traditional word encoding methods, including the Linear SVC model with TF-IDF. The results indicate that the AraBERTv2 version outperforms the other models. Consequently, we used it for our final submission, and we were ranked third among eight other participating teams.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Social media has become a key tool for disseminating news as it allows news providers to distribute and propagate their messages to a broader range of users. The colossal number of people using social media every day has accelerated the rate at which fake news spreads <ref type="bibr" coords="1,389.71,515.13,11.70,9.94" target="#b0">[1]</ref>. This has been a common problem that affected many people, governments, and organizations. Consequently, researchers have been working on developing various methods for detecting misinformation in various languages. Because of the large amount of false information on social media, it is impossible for human moderators to examine every single piece of information that may involve false facts. Therefore, before factchecking by humans, there is a need to flag potential posts that may contain false information. Thus, organizations and researchers have started to develop systems that check for the most notable claims <ref type="bibr" coords="1,72.02,603.72,11.79,9.94" target="#b0">[1]</ref>. This paper concentrates on Subtask 1A in Arabic from CheckThat, a lab contest with different tasks for competitors <ref type="bibr" coords="1,141.14,616.32,11.70,9.94" target="#b1">[2]</ref>. This year, the lab provided the following three main tasks: Detecting Check-Worthy Claims (Task1), Previously Fact-Checked Claims (Task2), and Fake News Detection (Task3). Each of the main tasks has two subtasks. Subtask 1A was provided in five different languages (Arabic, Bulgarian, English, Spanish, and Turkish).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Task Definition:</head><p>Subtask 1A in the Arabic language aims at classifying real-world tweets into predefined categories. Each tweet has been labelled as to whether the tweet is worth checking or not. Hence, this is a supervised text classification problem that aims to classify upcoming tweets based on their content. We used a pretrained model that can make more accurate predictions. The main objective of this task is to develop a system that would automatically identify whether a tweet is worth checking or not.</p><p>This subtask comes with three CSV files containing collections of tweets and divided into the following three categories:</p><p>• A training dataset that includes 3439 labelled tweets.</p><p>• A development dataset with 661 labelled tweets.</p><p>• A testing dataset with 600 un-labelled tweets. The training and development datasets contain four features (topic_id, tweet_id, tweet_url, tweet_text) and two labelling features (claim, claim_worthiness) . The second labelling feature classifies the tweet's check-worthiness, which is defined as whether this tweet includes any concerning claims that may have a significantly detrimental impact.</p><p>The remaining sections of this paper are as follow. The next section gives a review of related work. Section 3 details the adopted approach in our experiment, including the pre-processing phase of the provided text and the fine-tuning stage of the used pre-trained model. Section 4 provides the results of the experiments, and Section 5 provides the conclusions. Finally, Section 6 contain some acknowledgements for this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This section briefly describes other work which we considered while developing our experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Check-Worthy Claims Detection using ClaimRank System.</head><p>ClaimRank is a multilingual system developed for the purpose of detecting claims that are worth checking <ref type="bibr" coords="2,117.50,412.65,11.70,9.94" target="#b2">[3]</ref>. The system supports two languages: English and Arabic. It is trained using real annotations from nine reliable organizations, such as CNN and PolitiFact. During the development of the ClaimRank system, the data underwent pre-processing, followed by the extraction of features. These features are then passed to the model. For Arabic claim detection, they used two datasets translated into English when training the model. By contrast, our experiment used a pre-trained model with a large Arabic dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Automatic Identification and Verification of Claims</head><p>Atanasova et al <ref type="bibr" coords="2,161.90,519.57,13.45,9.94" target="#b1">[2]</ref>did some work in CLEF'2019 CheckThat Lab. It was related to the same task (Task1-Check-worthy claims), which we present in our paper. They discussed the dataset, evaluation tools, and evaluated results for eleven teams participating in this task <ref type="bibr" coords="2,379.63,544.89,11.70,9.94" target="#b1">[2]</ref>. In terms of data, they used a previous version of the dataset called "CT-CWC-18" <ref type="bibr" coords="2,310.39,557.49,11.79,9.94" target="#b3">[4]</ref>. The training data in the 2019-version was a combination of both the training and testing English datasets from the previous year. In addition, they added labelled data from 16 different resources for testing purposes. In the evaluation phase, they used Mean Average Precision measurement to calculate the rank of sentences of each participant. Best results made by participating teams used supervised classification models. Linear machine learning models such as SVM, naive Bayes, regression trees and neural networks models such as LSTM were utilized <ref type="bibr" coords="2,72.02,633.48,11.79,9.94" target="#b1">[2]</ref>. Despite the relatively great results, none of them used pre-trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Accenture at CheckThat! 2020</head><p>The winner team from the previous year contest CheckThat! Lab proposed a system that introduces a claims fact-checking approach employing two models: BERT and RoBERTa <ref type="bibr" coords="2,427.78,689.88,11.70,9.94" target="#b4">[5]</ref>. This team scored 1st position in both the Arabic and the English track. The models were used to classify social media claims that require an expert fact-checker. They examined four models and used AraBERTv0.1 Upsampled model for the official submission, which acquired a P@30 of 0.7000 compared to AraBERTv1.0 Upsampled, AraBERTv0.1 Unmodified, and ArabicBERT-Base Upsampled that achieved a P@30 of 0.675, 0.669, and 0.664 respectively <ref type="bibr" coords="2,326.11,753.14,11.70,9.94" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">AraBERT Model Study</head><p>One of the best models that delivered state-of-the-art outcomes in various NLP tasks is the AraBERT model <ref type="bibr" coords="3,102.74,104.44,11.79,9.94" target="#b6">[7]</ref>. This study used the BERT model for the Arabic language in order to accomplish what the English BERT model did. They discussed performing several tasks using this model as well as the mBERT, which is a multilingual BERT. The results showed that AraBERTv0.1 outperformed mBERT in sentiment analysis, named entity recognition, and question answering tasks <ref type="bibr" coords="3,416.74,142.36,11.79,9.94" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Various models can be applied for solving a binary classification problem. For example, linear machine learning models such as SVM have been used by many researchers and showed excellent results. Additionally, deep learning is considered one of the most effective domains in tackling such problems. It has demonstrated satisfactory performance in solving diverse tasks in natural language processing (NLP), such as text classification. However, the scarcity of big labelled textual datasets may lower the accuracy of the results. A transformer model such as BERT was introduced in the NLP domain to support various state-of-the-art performance tasks. Since we are dealing with Arabic tweets, we pre-trained a BERT-based model designed explicitly for Arabic called AraBERT. The following subsections cover text pre-processing and explain the model used in this experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Text Pre-processing</head><p>In the first stage of this phase, all sentences were split using a pre-processing function called ArabertPreprocessor.preprocess. The main idea here is to divide words from punctuation symbols and numbers by a space, as in the following example:</p><formula xml:id="formula_0" coords="3,235.85,380.33,221.01,10.77"># ‫القرن‬ _ ‫صفقة‬  # ‫صفقة_القرن‬</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Segmentation and Tokenization</head><p>Because Arabic has a complicated lexical structure and same-meaning vocabularies may have various forms, we considered pre-processing steps such as segmentation. To explain this, take the Arabic word "AlElm -‫"العلم‬ which consists of the word ‫"علم"‬ (that means science) and the prefix " ‫"ال‬ (that means "the" in English). This word with and without the definite article can be duplicated and counted as two different words <ref type="bibr" coords="3,218.45,481.65,11.70,9.94" target="#b6">[7]</ref>. Therefore, all tweets, including the training, development, and testing datasets, were segmented before AraBERT tokenization.</p><p>There are a number of options for tokenization when working with a pre-trained BERT model. We used the base model 'aubmindlab/bert-base-arabertv2', which is more common in practice. Also, we utilized a helpful tool, called AutoTokenizer, which is essential for splitting inputs into tokens with each token encoded into a corresponding id. Then, we built a batch to feed the model using this tokenizer by setting the maximum length to 128 and the batch size to 32, padding inputs that include fewer tokens than the full length, and truncating longer inputs to the specified length. </p><formula xml:id="formula_1" coords="3,65.42,627.91,464.20,30.48">0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 attention_mask 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 decoded text [CLS] # ‫صفقة‬ _ ‫القرن‬ # ‫القدس‬ _ ‫عاصمة‬ _ ‫ن‬ ‫فلسطي‬ _ ‫األبدية‬ [SEP]</formula><p>The table above illustrates how the tokenizer function converts a given input to produce a dictionary. Due to the narrowed space of this table, we changed the max_length to 16 to exhibit this example. The keys of the returned dictionary are the strings: input_id, token_type_id, and attention_mask. Each token has a unique input id. For instance, the character hashtag (#) corresponds to the id 10. The attention_mask key determines what tokens should get noticed by the model. If the attention mask is zero, there is no input, and the padding argument fills the batch to complete the maximum length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Fine Tuning</head><p>We used a pre-trained model called AraBERT as reported by Antoun et al. <ref type="bibr" coords="4,413.50,91.72,11.70,9.94" target="#b6">[7]</ref>. AraBERT is a model derived from the BERT model, which stands for Bidirectional Encoder Representations from Transformers <ref type="bibr" coords="4,134.66,117.04,11.70,9.94" target="#b7">[8]</ref>. The BERT model has been used in various Natural Language processing tasks and showed significant results <ref type="bibr" coords="4,189.17,129.76,11.70,9.94" target="#b6">[7]</ref>.</p><p>In this experiment, we used BERT for sequence classification to fine-tune the model with the help of the transformer's library. The used classifier includes 12 BERT layers and fed into a hyperbolic tangent activation function to return a probability distribution across all the tokens. Training smaller datasets from scratch can cause overfitting. Therefore, it is good to use models previously trained on a vast dataset and fine-tune the model using a smaller dataset <ref type="bibr" coords="4,336.07,193.00,11.70,9.94" target="#b6">[7]</ref>.</p><p>We tried setting a high number of epochs during the fine-tuning step to look out for overfitting and take appropriate measures. Each time we trained the model using the training and validation datasets, there was overfitting in the fourth epochs. Since there is no accurate way to determine the number of epochs, we decided to set it to 3. The table below demonstrates the average training loss and the accuracy for the validation run. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and Discussion</head><p>Since AraBERT has many versions, we explored different practices to determine which version we should use for our submission. Although a study compared different AraBERT models and found that some larger models performed better than others in terms of accuracy <ref type="bibr" coords="4,394.75,392.95,11.72,9.94" target="#b8">[9]</ref>, we only dealt with base models due to the limitation of the used hardware. We first used the validation datasets to determine which model we should use for the final submission. Then, we used this dataset for development and compared the predicted results to the gold-labels dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Validation Datasets Results Analysis</head><p>We examined the results of both versions on the development set. The results show that using the AraBERTv2-base model may increase the accuracy and the weighted average of precision, recall, and f1-score. In another attempt, we executed a baseline model that can be used for classification, particularly the Linear Support Vector Classification model (SVC) with TF-IDF. The experiment of the validation datasets indicates that this traditional word encodings method yields a weighted average F1score of 0.67 compared to the AraBERTv0.2 and AraBERTv2 with 0.83 and 0.84, respectively. These results suggest that these methods might be insufficient to model the complicated Arabic tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Testing Datasets Results Analysis</head><p>After releasing the test gold labels dataset, we compared our predictions to the correct values. Table <ref type="table" coords="4,72.02,599.64,5.52,9.94" target="#tab_2">3</ref> shows a comparison between AraBERTv0.2-base and AraBERTv2-base for both labels using a classification report function. The main observation from Table <ref type="table" coords="4,350.47,612.24,5.52,9.94" target="#tab_2">3</ref> is that the two versions approximately performed the same. There is a slight improvement in general using AraBERTv2-base, which we decided to use for the official submission. Finally, the submitted predictions for Subtask1A on Check-worthiness of tweets in Arabic has achieved a 0.612 Mean-Average-Precision, and our team SCUoL ranked third among eight participating teams. Although the results were encouraging, the main concern is the insufficient improvement when examining the two versions of the AraBERT model. Another contributing factor is contrasting base models with larger-size models to provide more comprehensive selections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>It is essential to gain information from reliable sources. Therefore, a considerable number of models are used to solve this problem. Due to the massive number of tweets published online, moderators want to concentrate on tweets worth checking only. This experiment examined an Arabic dataset that is labelled as to whether a tweet is worth checking. We took two steps toward our goal, including preprocessing and fine-tuning the datasets. We first approach the task by pre-processing the dataset and then applied two versions of the transformer-based model AraBERT as well as the traditional machine learning method Linear SVC model with TF-IDF. The comparison indicated that AraBERT beats the regular ML model. However, there was not a significant improvement when comparing the base versions of this model.</p><p>In the future, investigating additional features should be considered. Some related highlights for this task can be tackled not only by transformer-based embeddings but also by word-vector based models. In addition, the larger versions of the AraBERT may give a better performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,65.18,589.30,464.44,46.71"><head>Table 1 :</head><label>1</label><figDesc>The output of the tokenization process with an example of a max_length of 16</figDesc><table coords="3,65.18,602.11,464.44,33.90"><row><cell cols="3"># ‫األبدية‬ _ ‫ن‬ ‫فلسطي‬ _ ‫عاصمة‬ _ ‫القدس‬ # ‫القرن‬ _ ‫صفقة‬</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>input_ids</cell><cell>2</cell><cell>10 5987 68 2890 10 3306 68 6552 68</cell><cell>2036</cell><cell>68 54632</cell><cell>3</cell><cell>0 0</cell></row><row><cell>token_type_ids</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,108.02,274.13,370.92,56.63"><head>Table 2 :</head><label>2</label><figDesc>The Training loss average and validation accuracy in the first three epochs.</figDesc><table coords="4,146.18,291.07,324.81,39.70"><row><cell></cell><cell>Epoch 1</cell><cell>Epoch 2</cell><cell>Epoch 3</cell></row><row><cell>Average training loss</cell><cell>0.32</cell><cell>0.19</cell><cell>0.12</cell></row><row><cell>Accuracy</cell><cell>0.81</cell><cell>0.83</cell><cell>0.84</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,72.02,656.62,424.97,106.58"><head>Table 3 :</head><label>3</label><figDesc>Comparison between AraBERTv0.2-base and AraBERTv2-base results</figDesc><table coords="4,93.62,673.56,403.37,89.64"><row><cell></cell><cell>AraBERTv0.2-base</cell><cell></cell><cell>AraBERTv2-base</cell><cell></cell></row><row><cell>Criterion</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>1</cell></row><row><cell>Precision</cell><cell>0.74</cell><cell>0.60</cell><cell>0.72</cell><cell>0.63</cell></row><row><cell>Recall</cell><cell>0.71</cell><cell>0.64</cell><cell>0.78</cell><cell>0.56</cell></row><row><cell>F1-score</cell><cell>0.72</cell><cell>0.62</cell><cell>0.75</cell><cell>0.59</cell></row><row><cell>Accuracy</cell><cell>0.68</cell><cell></cell><cell>0.69</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6.">Acknowledgements</head><p>The research behind this paper would not have been possible without the extraordinary assistance of the team and supervisors, <rs type="person">Mohammad Ammar Alsalka</rs> and <rs type="person">Eric Atwell</rs>. Their knowledge and consideration kept my work on track. Moreover, my colleague <rs type="person">Abdullah Alsaleh</rs> has helped me outline the work and added a significant enhancement. Similarly, we appreciate the insightful comments offered by the reviewers.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="5,76.16,458.97,4.14,9.94;5,108.02,458.97,415.02,9.94;5,108.02,471.57,414.88,9.94;5,108.02,484.17,124.71,9.94" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,253.25,458.97,269.79,9.94;5,108.02,471.57,38.40,9.94">Claim Check-Worthiness Detection as Positive Unlabelled Learning</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Augenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,165.50,471.57,357.40,9.94;5,108.02,484.17,92.69,9.94">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,76.16,496.89,4.14,9.94;5,108.02,496.89,415.15,9.94;5,108.02,509.49,388.52,9.94" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="5,202.97,496.89,320.19,9.94;5,108.02,509.49,114.97,9.94">Overview of the CLEF-2019 CheckThat! Lab: Automatic Identification and Verification of Claims</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Atanasova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Task 1: Check-Worthiness</note>
	<note>in CLEF. Working Notes</note>
</biblStruct>

<biblStruct coords="5,76.16,522.21,4.14,9.94;5,108.02,522.21,415.15,9.94;5,108.02,534.81,148.26,9.94" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="5,189.41,522.21,299.89,9.94">ClaimRank: Detecting check-worthy claims in Arabic and English</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Jaradat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07587</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,76.16,547.41,4.14,9.94;5,108.02,547.41,415.01,9.94;5,108.02,560.13,415.07,9.94;5,108.02,572.73,110.73,9.94" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="5,202.73,547.41,320.30,9.94;5,108.02,560.13,183.65,9.94">Overview of the CLEF-2018 CheckThat! lab on automatic identification and verification of political claims</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Atanasova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05542</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,76.16,585.48,4.14,9.94;5,108.02,585.48,415.24,9.94;5,108.02,598.08,415.09,9.94;5,108.02,610.80,24.84,9.94" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Novak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.02431</idno>
		<title level="m" coord="5,297.55,585.48,225.71,9.94;5,108.02,598.08,260.64,9.94">Accenture at CheckThat! 2020: If you say so: Posthoc fact-checking of claims using transformer-based models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,76.16,623.40,4.14,9.94;5,108.02,623.40,415.08,9.94;5,108.02,636.00,172.83,9.94" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Novak</surname></persName>
		</author>
		<title level="m" coord="5,161.78,623.40,361.32,9.94;5,108.02,636.00,140.38,9.94">Accenture at CheckThat! 2020: If you say so: Post-hoc fact-checking of claims using transformer-based models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,76.16,648.72,4.14,9.94;5,108.02,648.72,415.03,9.94;5,108.02,661.32,245.15,9.94" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="5,269.93,648.72,253.12,9.94;5,108.02,661.32,61.70,9.94">Arabert: Transformer-based model for arabic language understanding</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Antoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Baly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hajj</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00104</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,76.16,674.04,4.14,9.94;5,108.02,674.04,414.94,9.94;5,108.02,686.64,245.15,9.94" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="5,199.97,674.04,322.99,9.94;5,108.02,686.64,61.73,9.94">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,76.16,699.24,4.14,9.94;5,108.02,699.24,414.84,9.94;5,108.02,711.96,294.81,9.94" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Wadhawan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.01679</idno>
		<title level="m" coord="5,180.29,699.24,342.57,9.94;5,108.02,711.96,111.78,9.94">Arabert and farasa segmentation based approach for sarcasm and sentiment detection in arabic tweets</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
