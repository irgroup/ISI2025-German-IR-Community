<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,352.97,15.42;1,89.29,106.66,335.22,15.42;1,88.78,128.58,202.40,15.43">MUCIC at CheckThat! 2021: FaDo-Fake News Detection and Domain Identification using Transformers Ensembling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,156.89,136.80,11.96"><forename type="first">Fazlourrahman</forename><surname>Balouchzahi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Computing Research</orgName>
								<orgName type="institution">Instituto Politécnico Nacional</orgName>
								<address>
									<settlement>CDMX</settlement>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,238.74,156.89,108.35,11.96"><forename type="first">Hosahalli</forename><surname>Lakshmaiah</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Mangalore University</orgName>
								<address>
									<settlement>Mangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,170.84,75.90,11.96"><forename type="first">Grigori</forename><surname>Sidorov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Computing Research</orgName>
								<orgName type="institution">Instituto Politécnico Nacional</orgName>
								<address>
									<settlement>CDMX</settlement>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,352.97,15.42;1,89.29,106.66,335.22,15.42;1,88.78,128.58,202.40,15.43">MUCIC at CheckThat! 2021: FaDo-Fake News Detection and Domain Identification using Transformers Ensembling</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">99CC20C38862B39649CCA8541EB01BAF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fake News Detection</term>
					<term>Domain Identification</term>
					<term>Transformers</term>
					<term>BERT</term>
					<term>Roberta</term>
					<term>Distilbert</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Since the beginning of Covid-19 era in November 2019, the patient growth curve is closely accompanied by the growth of fake news. Therefore, developing tools and models for the detection of fake news from real ones in various domains have become more significant than the earlier days. To address the detection of fake news, in this paper, we, team MUCIC, describe the models submitted to 'Fake News Detection', a shared task organized by CLEF-2021-CheckThat! Lab. This shared task contains two subtasks namely; Fake News Detection of News Articles (Subtask 3A) and Topical Domain Classification of News Articles (Subtask 3B) and both are multi-class text classification tasks. The proposed models have been developed by fine-tuning the three transformer-based language models namely; Roberta, Distilbert, and BERT from HuggingFace using training data and then ensembling them as estimators with majority voting. The proposed models performances evaluated through the evaluation script provided by organizers obtained F1-scores of 0.5309 and 0.8550 for Subtask 3A and Subtask 3B respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Anonymity is a significant attribute of the cyber world <ref type="bibr" coords="1,333.36,460.36,12.76,10.91" target="#b0">[1]</ref> which also provides ample opportunities to mislead and manipulate peoples' thoughts and damnify social trust <ref type="bibr" coords="1,425.09,473.91,11.32,10.91" target="#b1">[2]</ref>. Ease of access, low cost, and swift broadcasting of the information in social media and network is exerting negative influence on sharing fake news in various domains <ref type="bibr" coords="1,349.70,501.00,11.28,10.91" target="#b2">[3]</ref>. Fake news detection in different domains has received much attention especially after the outbreak of Covid-19 and its effects on the entire world.</p><p>The nature of texts in social media is highly unstructured and noisy, and texts may belong to various domains as people comment or share messages about various topics. Therefore, identifying the domain of texts in which news are disseminating in social media is very important CLEF 2021 -Conference and Labs of the Evaluation Forum, September 21-24, 2021, Bucharest, Romania frs1_b@yahoo.com (F. Balouchzahi); hlsrekha@gmail.com (H. L. Shashirekha); sidorov@cic.ipn.mx (G. Sidorov) https://mangaloreuniversity.ac.in/dr-h-l-shashirekha (H. L. Shashirekha); http://www.cic.ipn.mx/~sidorov/ (G. Sidorov) 0000-0003-1937-3475 (F. <ref type="bibr" coords="1,190.31,649.06,47.50,8.97">Balouchzahi)</ref> and might help in designing the models/tools to detect fake news. Fake news detection and domain identification tasks could be modeled as binary Text Classification (TC) problems if there are only two classes, otherwise they will be modeled as multi-class TC problem. Hitherto many models based on Machine Learning (ML) and Deep Learning (DL) are experimented by researchers for TC in general. Of late Transfer Learning (TL) and transformers-based models are also getting attention due to their efficient performances in various Natural Language Processing (NLP) and TC tasks <ref type="bibr" coords="2,228.99,168.26,11.36,10.91" target="#b3">[4,</ref><ref type="bibr" coords="2,243.08,168.26,7.57,10.91" target="#b4">5]</ref>.</p><p>Many DL and Neural Network (NN) based models such as CNN, LSTM, BiLSTM, etc. are considered as best models for many NLP and TC tasks. But, the introduction of transformers have changed the game and since 2017 transformer <ref type="foot" coords="2,327.99,207.15,3.71,7.97" target="#foot_0">1</ref> -based models are beating DL models in many NLP related applications <ref type="foot" coords="2,240.97,220.70,3.71,7.97" target="#foot_1">2</ref> . Transformers are novel architectures with self-attention mechanism that are primarily used for NLP. Transformer-based models are able to handle long-range dependencies and usually are used for sequence-to-sequence NLP tasks <ref type="foot" coords="2,458.24,247.80,3.71,7.97" target="#foot_2">3</ref> .</p><p>NLP researchers are challenged by Conference and Labs of the Evaluation Forum (CLEF) 2021 <ref type="foot" coords="2,109.91,274.90,3.71,7.97" target="#foot_3">4</ref> CheckThat! Lab on Detecting Check-Worthy Claims, Previously Fact-Checked Claims, and Fake News<ref type="foot" coords="2,157.21,288.45,3.71,7.97" target="#foot_4">5</ref>  <ref type="bibr" coords="2,164.13,290.20,11.49,10.91" target="#b5">[6]</ref>. Fake News Detection (Task 3)<ref type="foot" coords="2,315.84,288.45,3.71,7.97" target="#foot_5">6</ref> is one of the three tasks evaluated by the CheckThat! Lab and this task has two subtasks. Figure <ref type="figure" coords="2,329.67,303.75,4.97,10.91" target="#fig_0">1</ref> illustrates the graphical representation of subtasks and details of the subtasks are as follows:</p><p>• Subtask 3A -Multi-Class Fake News Detection of News Articles (English): is a multi-class TC task that accepts a given English text and classifies it into one of four pre-defined categories namely, 'False', 'Partially False', 'True', and 'Other'. Table <ref type="table" coords="2,475.28,368.72,5.08,10.91" target="#tab_1">1</ref> gives the description of labels in Subtask 3A; Many tools and algorithms have been introduced by researchers for TC in general. However, an algorithm which performs well for a particular dataset may not give the same performance for other datasets <ref type="bibr" coords="2,171.56,516.21,11.58,10.91" target="#b2">[3]</ref>. Therefore, it is not logical to claim that an algorithm or a model leads to the same performance for all the datasets. As a result, inspired by H. L. Shashirekha et al. <ref type="bibr" coords="2,89.29,543.31,12.95,10.91" target="#b2">[3]</ref> and Gundapu S et al. <ref type="bibr" coords="2,201.70,543.31,11.53,10.91" target="#b6">[7]</ref>, we, team MUCIC, utilized available at HuggingFace<ref type="foot" coords="2,452.18,541.56,3.71,7.97" target="#foot_6">7</ref> to develop transformer-based models, namely, BERT, Distilbert, and Roberta, extended each of these models with a step of fine-tuning the respective Language Model (LM) and finally ensembled the extended models with majority voting for fake news detection and domain identification.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>False</head><p>The main content of given text is fake. Partially false Main claim in given text might be True but also contain false or misleading information, not surely true and not certainly false.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>True</head><p>The given text includes contents that are clearly apparent or capable of being logically proved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Other</head><p>Texts with no enough evidence to categorize as of one of earlier categories.</p><p>The rest of paper is organized as follows: Section 2 highlights the recent literature and related works followed by the proposed Ensemble of transformer-based model for fake news detection and domain identification in Section 3. While Section 4 describes the experiments and results, Section 5 concludes the paper with future scope.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Alongside the outbreak of Covid-19, fake news detection task has gained more and more attention due to critical situation the entire globe is facing. Many researchers have developed many models to combat fake news and avoid its spread thereby. Few of the latest and related works are briefly discussed in this section. Gundapu et al. <ref type="bibr" coords="3,354.21,586.57,12.99,10.91" target="#b6">[7]</ref> have proposed several models based on ML, DL and TL approaches for the task of fake news detection in English language in which a given news item is categorized as either 'fake' or 'real'. They conducted experiments on the dataset provided by ConstraintAI'21 <ref type="foot" coords="3,290.00,625.46,3.71,7.97" target="#foot_7">8</ref> shared task organizers and their ensemble of transformer-based models outperformed the rest of the models with an F1-score of 0.9855. The dataset as described in detail by Patwa et al. <ref type="bibr" coords="4,279.89,86.97,12.69,10.91" target="#b7">[8]</ref> consists of 10,700 texts from various social media platforms such as Instagram, Twitter, etc. Their proposed transformer-based ensemble model architecture consists of preprocessing and ensemble model construction. Data is preprocessed by converting emoji and hashtags to text, removing punctuation, digits, non-ASCII characters, stop words and stemming. Three transformer-based models namely, BERT, ALBERT, and XLNet are ensembled in such way that the average of all softmax values from estimators gives probability of the final predicted label.</p><p>A multilingual cross-domain dataset containing 5,182 fact-checked news articles related to Covid-19 is developed by Shahi et al. <ref type="bibr" coords="4,257.02,195.36,12.89,10.91" target="#b8">[9]</ref> by collecting articles from 92 fact checking websites shared during the first five months of 2020. The collected articles include texts in 40 languages, categorized into 11 classes and then filtered into two categories namely, 'False' and 'Other'. <ref type="bibr" coords="4,483.74,222.46,4.45,10.91" target="#b3">4</ref> Random Forest (RF), k-Nearest Neighbor (kNN), Logistic Regression (LR) and Support Vector Machine (SVM), with TF and TF-IDF as features and also DL models, namely, Convolutional Neural Network (CNN), Long Short Term Memory (LSTM), and Gated Recurrent Network (GRU), utilizing Glove word embedding have been surveyed by Jiang et al. <ref type="bibr" coords="4,383.07,466.34,17.75,10.91" target="#b10">[11]</ref> for fake news detection. Further, the authors proposed a stacking approach using the above mentioned classifiers and evaluated that model on two fake news datasets namely, ISOT 9 and KDnugget <ref type="bibr" coords="4,429.46,493.44,17.76,10.91" target="#b11">[12]</ref> and obtained accuracies of 99.94% and 96.05% respectively. However, in terms of individual performance of each estimator, LR classifier with an accuracy of 92.82% and RF with an accuracy of 99.87% both fed with TF-IDF as features outperformed other individual classifiers for both the datasets.</p><p>Despite the lack of availability of annotated dataset for fake news detection in low resource languages, Forum for Information Retrieval Evaluation (FIRE) 2020 10 called for UrduFake 11 , a shared task on fake news detection in Urdu language and provided a dataset consisting of 500 real and 400 fake news articles in Urdu <ref type="bibr" coords="4,266.63,588.29,16.34,10.91" target="#b12">[13]</ref>. Balouchzahi et al. <ref type="bibr" coords="4,372.85,588.29,18.00,10.91" target="#b13">[14]</ref> proposed various models based on ML, DL, TL, and hybrid approaches for UrduFake. ML model is a Voting Classifier (VC) with three estimators namely, Multinomial Naïve Bayes (MNB), Multilayer Perceptron (MLP), and LR, had been fed with set of char and word ngrams features. While DL model has been developed based on a multi-channel CNN and Skipgram word embedding, an implementation of Universal Language Model Fine-Tuning (ULMFiT) is based on TL approach. Further, all models are ensembled as an hybrid model based on majority voting. ML VC outperformed the rest of the models with an average F1-score of 0.7894.</p><p>Tools and modules for the analysis of fake news on social media are not only bounded to detect the category of a given text but also to identify the spreaders of such false information to find out the intention beyond sharing fake news. In this direction, PAN at Conference and Labs of the Evaluation Forum (CLEF) 2020 had called for shared task to identify the spreader of fake news in Spanish and English. Dataset provided by task organizers consists of 100 tweets per user (news spreaders) for 300 users. To tackle this task Shashirekha et al. <ref type="bibr" coords="5,418.78,222.46,11.41,10.91" target="#b1">[2,</ref><ref type="bibr" coords="5,432.92,222.46,8.99,10.91" target="#b2">3]</ref> proposed two classifiers namely, ULMFiT based on TL approach and ensemble of ML classifiers as a voting classifier. The ULMFiT model initially uses unlabelled data collection from Wikipedia to train the universal LMs for both English and Spanish languages. As texts from Wikipedia are from general domain, there is every chance of missing valuable words and features related to fake news. Hence, the LMs obtained by training on Wikipedia are fine-tuned using the training data provided by PAN to make LMs more specific for the task of fake news spreader detection. The Fast.ai 12 library is used to develop LMs and classifiers. The proposed ML VC is built with three estimators, namely, two linear SVM and LR classifiers. After preprocessing the texts by removing stopwords and punctuation, converting emoji to text and performing lemmatization, features are extracted. Unigram TFIDF, N-gram TF combined with Doc2vec features are scaled using MaxAbsScaler and used to train the VC. Chi-square test, Mutual Information, and F-test are used to select the important features. The final results reported by PAN illustrate that ULMFiT and ML VC obtained average accuracies of 0.63 and 0.70 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>The performance and effectiveness of ensembling models based on majority voting of ML classifiers have already been proved for many tasks. Inspired by Gundapu S et al. <ref type="bibr" coords="5,438.63,470.68,11.28,10.91" target="#b6">[7]</ref>, ensembling fine tuned transformer-based models are experimented in this work for fake news detection and domain identification. Transformer-based models generally include two steps of pre-training LM and then fine-tuning the model for the new task. Usually as pre-trained models are publicly available at HuggingFace it is only required to fine-tune LM with a labeled dataset for the target task. The structure of the proposed model consists of the following four steps:</p><p>1. Pre-training: pre-trained models available at HuggingFace are used; 2. Fine-tuning LMs with texts from training set to make LM more domain specific for the intended task; 3. Training the models obtained in step 2 for classification (each model separately); 4. Ensembling for prediction of labels based on majority voting.</p><p>As domain of texts used for pre-training a LM might be different from the training set, initially the individual LMs for transformer-based models namely, BERT <ref type="bibr" coords="5,392.45,649.63,16.41,10.91" target="#b14">[15]</ref>, Distilbert <ref type="bibr" coords="5,464.31,649.63,16.41,10.91" target="#b15">[16]</ref>, and Roberta <ref type="bibr" coords="6,126.81,204.07,17.93,10.91" target="#b16">[17]</ref> are fine-tuned using the training data and then these fine-tuned models are used as estimators in a VC. The main objectives of each estimator are given below:</p><p>• BERT is a pre-trained model that employs Masked Language Modeling (MLM) in a selfsupervised fashion. In other words, only raw texts are used to pre-train the model without manual annotation. MLM concept along with Next Sentence Prediction (NSP) enables the model to learn deep representation of a language to extract more efficient features from texts for downstream tasks. • Distilbert is a lightweight BERT model. It follows the objectives of BERT model with distillation loss and returns the same probabilities as BERT and it also utilizes Cosine embedding concept to generate hidden states as close as BERT model. • Roberta is an optimized BERT model re-trained with improved training methodology, more data and hardware resources <ref type="foot" coords="6,273.40,364.31,7.41,7.97" target="#foot_8">13</ref> . Roberta without NSP concept is similar to BERT and employs dynamic masking results in changing the masked token during the training epochs.</p><p>Fast-bert 14 library with configuration given in Table <ref type="table" coords="6,336.91,417.68,5.12,10.91" target="#tab_2">2</ref> is used to fine-tune the LMs in (step 2). Except for model and type, the same configuration is used for all the three LMs and finetuned only for 5 epochs due to resource (RAM and GPU) constraint. Fast-bert associated with HuggingFace enables a very simple manner to train and evaluate the transformer-based models. Each individual transformer-based model is trained for 20 epochs.</p><p>As per the general idea of the proposed model illustrated in Figure <ref type="figure" coords="6,394.50,485.43,3.67,10.91" target="#fig_2">2</ref>, preprocessing the texts has been skipped as the models performed better without preprocessing. The same architecture is followed to construct models for both the subtasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>For any supervised ML task such as TC, annotated data sets are essential to train the model and enable machine to quickly and clearly understand the input patterns <ref type="bibr" coords="6,397.52,584.55,16.28,10.91" target="#b17">[18]</ref>. Therefore, datasets provided by the CheckThat! Lab are used and the data collections steps are detailed by Shahi et al. <ref type="bibr" coords="6,102.74,611.65,16.25,10.91" target="#b18">[19]</ref>.</p><p>Due to mistakes while generating prediction files for the task, the results of the proposed models reported in the leaderboard are much less than the actual performances of the systems. Therefore, along with the results reported by task organizers, the non-official results, i.e., the actual performances of system evaluated through evaluation script <ref type="foot" coords="7,382.02,332.23,7.41,7.97" target="#foot_9">15</ref> provided by the organizer on re-generated prediction files are also included in this paper. The re-generated and re-evaluated submissions for both the subtasks can be found in our GitHub page <ref type="foot" coords="7,389.05,359.33,7.41,7.97" target="#foot_10">16</ref> .</p><p>The training set for Subtask 3A consists of 900 texts in four categories namely, 'False', 'Partially False', 'True', and 'Other'. Description of labels is given in Table <ref type="table" coords="7,376.37,388.18,5.08,10.91" target="#tab_1">1</ref> and label distributions over training sets are given in Figure <ref type="figure" coords="7,236.97,401.73,8.47,10.91" target="#fig_3">3a</ref>. It can be observed that the dataset is highly imbalanced and as expected this has affected the performance of the proposed model in a negative way. As per the results reported in leaderboard, for the test set consisting of 364 texts, the proposed model obtained an F1-score of 0.2334 which is far lower than the expectation due to earlier mentioned reason of mistakes in prediction files. However, non-officially the model obtained 0.5034 F1-score on re-generated prediction files.</p><p>A subset of fake news from the dataset of Subtask 3A is used for Subtask 3B. The dataset for Subtask 3B which includes 318 texts distributed into 6 categories is also imbalanced and the distribution of labels is shown in Figure <ref type="figure" coords="7,271.47,510.12,8.57,10.91" target="#fig_3">3b</ref>. Based on results reported in leaderboard, for the test set consisting of 137 texts, the proposed model obtained an F1-score of 0.1450 which is far less than the expectation but actual performance obtained non-officially is 0.8550 F1-score on re-generated prediction files.</p><p>Comparison of results both official (in leaderboard) and non-official, between the proposed models and top models in the shared task are given in Table <ref type="table" coords="7,351.87,577.87,3.66,10.91" target="#tab_3">3</ref>. It is very much clear that there is a huge gap between the results reported officially and the results obtained non-officially. Considering actual performances of the proposed model once again the effectiveness of ensembling classifiers to utilize the strength of single models has been proved.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this paper, we, team, MUCIC, have presented the description of the proposed Ensemble of transformer-based VC models for Fake News Detection, a shared task (Task 3) in CLEF-2021-CheckThat! Lab. Three transformer-based models namely, Roberta, Distilbert, and BERT are double fine-tuned (once on respective LM and then down streamed for respective TC task) and ensembled as VC that predicts the label of a given text by majority voting. The proposed models achieved low F1-scores of 0.233 and 0.145 for Subtask 3A: Multi-Class Fake News Detection of News Articles and Subtask 3B: Topical Domain Classification of News Articles respectively, against our expectations due to our mistakes in submission files. However, the actual performances of the systems show very competitive F1-scores of 0.5309 and 0.8550 for Subtask 3A and Subtask 3B respectively, on re-generated prediction files. Improving the performances of the proposed models by addressing the problems followed by exploring ML and DL approaches with various feature sets will be the future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,292.05,282.81,8.93;3,140.48,84.19,311.82,195.30"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Graphical representation of Fake News Detection subtasks</figDesc><graphic coords="3,140.48,84.19,311.82,195.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,488.19,222.46,17.79,10.91;4,89.29,236.01,416.69,10.91;4,88.42,249.56,417.57,10.91;4,89.29,263.11,259.56,10.91;4,100.20,276.66,405.78,10.91;4,89.29,290.20,416.70,10.91;4,89.29,303.75,416.69,10.91;4,88.89,317.30,417.10,10.91;4,89.29,330.85,416.69,10.91;4,89.29,344.40,416.70,10.91;4,89.29,357.95,416.97,10.91;4,89.29,371.50,416.69,10.91;4,89.29,385.05,416.70,10.91;4,89.29,398.60,416.69,10.91;4,89.29,412.15,202.16,10.91"><head></head><label></label><figDesc>,132 texts which belong to 'False' category contain false information and the remaining belongs to 'Other' category. Authors used Bert-based model as a baseline and obtained an average F1-score of 0.76 on identifying fake news for the developed dataset. Another work towards fake news detection in Covid-19 domain carried out by Paka et. al [10] includes developing a COVID-19 (Twitter) Fake news (CTF) dataset. CTF is a large-scale text dataset in Covid-19 domain collected from Twitter containing 21.85M unlabelled tweets along with 45.26K labeled tweets; out of which 18.55K are labeled as genuine and remaining 26.71K as fake. Data collection and annotation procedure has been done in four stages namely, Segregating COVID-19 related tweets, Collecting COVID-19 supporting statements, Filtering genuine and fake tweets, and Human annotation. The authenticity of the CTF dataset is guaranteed by fact checking websites such as PolitiFact, Snopes, TruthOrFiction, etc. and certain health organizations. The authors proposed a semi-supervised DL model based on Neural Attention model called Cross-SEAN which leverages huge unlabelled data to improve its performance and obtained F1-score of 0.95 on CTF dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,89.29,282.83,244.58,8.93;7,112.14,84.19,368.51,186.08"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Proposed Ensemble of Transformer-based models</figDesc><graphic coords="7,112.14,84.19,368.51,186.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,89.29,283.42,240.29,8.93;8,97.97,84.19,396.86,186.66"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Label distribution over training sets for subtasks</figDesc><graphic coords="8,97.97,84.19,396.86,186.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,107.28,396.88,398.99,78.66"><head>• Subtask 3B -Topical Domain Classification of News Articles: is</head><label></label><figDesc></figDesc><table coords="2,115.69,396.88,390.58,78.66"><row><cell>also a multi-class</cell></row><row><cell>TC task that further classifies a given fake news into one of six categories representing</cell></row><row><cell>six different domains namely, 'Health', 'Climate', 'Economy', 'Crime', 'Elections' and</cell></row><row><cell>'Education'. It is worth to note that classification results of Subtask 3A are incommunicable</cell></row><row><cell>to Subtask 3B and only known fake news provided by the organizers are used for Subtask</cell></row><row><cell>3B.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,88.99,325.20,109.10,36.73"><head>Table 1</head><label>1</label><figDesc>Classes in Subtask 3A</figDesc><table coords="3,94.64,353.99,103.45,7.94"><row><cell>Category</cell><cell>Description</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,88.99,90.49,410.47,87.15"><head>Table 2</head><label>2</label><figDesc>Configuration of transformer-based models</figDesc><table coords="6,95.84,122.65,403.62,54.99"><row><cell>Model</cell><cell>Type</cell><cell cols="4">Max Length Batch size Learning rate epochs</cell></row><row><cell>Roberta</cell><cell>roberta-base</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Distilbert distilbert-base-uncased</cell><cell>512</cell><cell>16</cell><cell>4e-5</cell><cell>5</cell></row><row><cell>BERT</cell><cell>bert-base-uncased</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,88.99,316.57,359.78,117.69"><head>Table 3</head><label>3</label><figDesc>Comparison of performances of the top models and proposed models in the shared task</figDesc><table coords="8,147.24,348.18,300.79,86.07"><row><cell>Team/ participant name</cell><cell>F1-score</cell><cell></cell></row><row><cell></cell><cell>Subtask 3A</cell><cell>Subtask 3B</cell></row><row><cell>sushmakumari</cell><cell>0.8376</cell><cell>0.8552</cell></row><row><cell>MUCIC(non-official)</cell><cell>0.5039</cell><cell>0.8550</cell></row><row><cell>kannanrrk</cell><cell>0.5034</cell><cell>0.8178</cell></row><row><cell>jmartinez595</cell><cell>0.4680</cell><cell>-</cell></row><row><cell>MUCIC(official in leaderboard)</cell><cell>0.2334</cell><cell>0.1450</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.93,605.29,138.64,8.97"><p>https://pypi.org/project/transformers/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,108.93,616.24,173.32,8.97"><p>https://yale-lily.github.io/public/matt_f2018.pdf</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,108.93,627.20,218.65,8.97"><p>https://towardsdatascience.com/transformers-89034557de14</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,108.93,638.16,117.00,8.97"><p>http://clef2021.clef-initiative.eu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="2,108.93,649.12,201.65,8.97"><p>https://sites.google.com/view/clef2021-checkthat/home</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="2,108.93,660.08,302.35,8.97"><p>https://sites.google.com/view/clef2021-checkthat/tasks/task-3-fake-news-detection</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="2,108.93,671.04,84.73,8.97"><p>https://huggingface.co/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="3,108.93,671.02,166.09,8.97"><p>https://constraint-shared-task-2021.github.io/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_8" coords="6,108.93,660.08,339.92,8.97"><p>https://towardsdatascience.com/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_9" coords="7,108.93,660.04,317.66,8.97"><p>https://gitlab.com/checkthat_lab/clef2021-checkthat-lab/-/tree/master/task3/evaluation</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_10" coords="7,108.93,671.00,214.02,8.97"><p>https://github.com/fazlfrs/CheckThat-_Task3_Submissions</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6.">Acknowledgment</head><p>Team MUCIC sincerely appreciates the efforts, guidance and support of the shared task organizers and reviewers for the valuable comments and suggestions.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,183.45,393.33,10.91;9,112.66,197.00,394.52,10.91;9,112.14,210.55,395.05,10.91;9,112.66,224.10,394.53,10.91;9,112.66,237.65,276.77,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,267.18,183.45,238.80,10.91;9,112.66,197.00,154.79,10.91">Las for HASOC -learning approaches for hate speech and offensive content identification</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Balouchzahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">L</forename><surname>Shashirekha</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2826/T2-6.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="9,112.14,210.55,335.16,10.91">Working Notes of FIRE 2020 -Forum for Information Retrieval Evaluation</title>
		<title level="s" coord="9,312.66,224.10,151.08,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Mehta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Mandl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Majumder</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Mitra</surname></persName>
		</editor>
		<meeting><address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">December 16-20, 2020. 2020</date>
			<biblScope unit="volume">2826</biblScope>
			<biblScope unit="page" from="145" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,251.20,394.62,10.91;9,112.66,264.75,395.17,10.91;9,112.66,278.30,394.53,10.91;9,112.39,291.85,394.31,10.91;9,112.30,305.40,108.83,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,274.07,251.20,211.55,10.91">Ulmfit for twitter fake news spreader profiling</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">L</forename><surname>Shashirekha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Balouchzahi</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_126.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="9,364.38,264.75,143.45,10.91;9,112.66,278.30,193.89,10.91">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="9,180.32,291.85,149.57,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25, 2020. 2696. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,318.95,393.33,10.91;9,112.66,332.50,393.33,10.91;9,112.66,346.05,394.52,10.91;9,112.66,359.59,395.00,10.91;9,112.66,373.14,218.47,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,328.21,318.95,177.77,10.91;9,112.66,332.50,89.23,10.91">Ensemble model for profiling fake news spreaders on twitter</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">L</forename><surname>Shashirekha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Anusha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">S</forename><surname>Prakash</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_136.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="9,466.22,332.50,39.77,10.91;9,112.66,346.05,293.34,10.91">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="9,287.49,359.59,151.17,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25, 2020. 2696. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,386.69,394.53,10.91;9,112.66,400.24,393.33,10.91;9,112.66,413.79,318.13,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,308.32,400.24,197.66,10.91;9,112.66,413.79,55.98,10.91">Limitations of transformers on clinical text classification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Alawad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gounley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Schaefferkoetter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X.-C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">B</forename><surname>Durbin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stroup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,176.56,413.79,222.31,10.91">IEEE journal of biomedical and health informatics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,427.34,393.32,10.91;9,112.66,440.89,217.16,10.91" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Misra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14289</idno>
		<title level="m" coord="9,269.96,427.34,236.02,10.91;9,112.66,440.89,35.05,10.91">Multi-class text classification using bert-based active learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,454.44,394.53,10.91;9,112.66,467.99,393.33,10.91;9,112.66,481.54,393.33,10.91;9,112.66,495.09,394.53,10.91;9,112.66,508.64,393.33,10.91;9,112.66,522.18,394.53,10.91;9,112.39,535.73,394.89,10.91;9,112.31,549.28,374.62,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,488.80,467.99,17.19,10.91;9,112.66,481.54,393.33,10.91;9,112.66,495.09,99.49,10.91">The CLEF-2021 checkthat! lab on detecting check-worthy claims, previously fact-checked claims, and fake news</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">D S</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Míguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Struß</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mandl</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-72240-1_75</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-72240-1_75.doi:10.1007/978-3-030-72240-1\_75" />
	</analytic>
	<monogr>
		<title level="m" coord="9,202.85,508.64,303.14,10.91;9,112.66,522.18,105.71,10.91;9,414.33,522.18,89.84,10.91">Advances in Information Retrieval -43rd European Conference on IR Research, ECIR 2021</title>
		<title level="s" coord="9,185.14,536.75,144.92,9.72">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Moens</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Mothe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Perego</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Potthast</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</editor>
		<meeting><address><addrLine>Virtual Event</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021-04-01">March 28 -April 1, 2021. 2021</date>
			<biblScope unit="volume">12657</biblScope>
			<biblScope unit="page" from="639" to="649" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II</note>
</biblStruct>

<biblStruct coords="9,112.66,562.83,394.53,10.91;9,112.66,576.38,173.79,10.91" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gundapu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mamid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00180</idno>
		<title level="m" coord="9,215.45,562.83,287.05,10.91">Transformer based automatic covid-19 fake news detection system</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,589.93,394.53,10.91;9,112.33,603.48,393.65,10.91;9,112.66,617.03,107.17,10.91" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Patwa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Pykl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Guptha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ekbal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chakraborty</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.03327</idno>
		<title level="m" coord="9,192.09,603.48,235.40,10.91">Fighting an infodemic: Covid-19 fake news dataset</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,630.58,393.33,10.91;9,112.66,644.13,393.33,10.91;9,112.66,657.68,387.87,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,220.22,630.58,285.77,10.91;9,112.66,644.13,49.79,10.91">FakeCovid -a multilingual cross-domain fact check news dataset for covid-19</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nandini</surname></persName>
		</author>
		<ptr target="http://workshop-proceedings.icwsm.org/pdf/2020_14.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="9,183.76,644.13,322.22,10.91;9,112.66,657.68,73.20,10.91">Workshop Proceedings of the 14th International AAAI Conference on Web and Social Media</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,86.97,393.33,10.91;10,112.66,100.52,393.33,10.91;10,112.66,114.06,133.42,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,391.52,86.97,114.46,10.91;10,112.66,100.52,328.38,10.91">Cross-sean: A cross-stitch semi-supervised neural attention model for covid-19 fake news detection</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">S</forename><surname>Paka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,449.42,100.52,56.57,10.91;10,112.66,114.06,50.39,10.91">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page">107393</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,127.61,393.33,10.91;10,112.66,141.16,260.14,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,325.48,127.61,180.51,10.91;10,112.66,141.16,97.58,10.91">A novel stacking approach for accurate detection of fake news</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">U</forename><surname>Haq</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Saboor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,219.27,141.16,54.37,10.91">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="22626" to="22639" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,154.71,394.53,10.91;10,112.66,168.26,234.00,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,250.24,154.71,252.24,10.91">Fake news detection in multiple platforms and languages</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">H A</forename><surname>Faustini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">F</forename><surname>Covões</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,112.66,168.26,150.97,10.91">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page">113503</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,181.81,393.33,10.91;10,112.66,195.36,393.33,10.91;10,112.66,208.91,216.59,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,258.99,181.81,246.99,10.91;10,112.66,195.36,163.56,10.91">Data augmentation using machine translation for fake news detection in the urdu language</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Amjad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sidorov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zhila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,300.50,195.36,205.49,10.91;10,112.66,208.91,118.30,10.91">Proceedings of The 12th Language Resources and Evaluation Conference</title>
		<meeting>The 12th Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2537" to="2542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,222.46,394.62,10.91;10,112.66,236.01,393.53,10.91;10,112.66,249.56,393.33,10.91;10,112.66,263.11,394.03,10.91;10,112.30,276.66,83.83,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,274.66,222.46,210.38,10.91">Learning models for urdu fake news detection</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Balouchzahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">L</forename><surname>Shashirekha</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2826/T3-7.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="10,329.86,236.01,176.34,10.91;10,112.66,249.56,141.17,10.91">Working Notes of FIRE 2020 -Forum for Information Retrieval Evaluation</title>
		<title level="s" coord="10,112.66,263.11,151.47,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Mehta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Mandl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Majumder</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Mitra</surname></persName>
		</editor>
		<meeting><address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">December 16-20, 2020. 2020</date>
			<biblScope unit="volume">2826</biblScope>
			<biblScope unit="page" from="474" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,290.20,393.33,10.91;10,112.66,303.75,363.59,10.91" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="10,353.43,290.20,152.55,10.91;10,112.66,303.75,181.08,10.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,317.30,394.53,10.91;10,112.66,330.85,295.45,10.91" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<title level="m" coord="10,303.00,317.30,204.19,10.91;10,112.66,330.85,113.82,10.91">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,344.40,394.53,10.91;10,112.30,357.95,393.68,10.91;10,112.66,371.50,107.17,10.91" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m" coord="10,173.53,357.95,256.77,10.91">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,385.05,393.59,10.91;10,112.66,398.60,146.44,10.91" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00502</idno>
		<title level="m" coord="10,168.48,385.05,305.06,10.91">Amused: An annotation framework of multi-modal social media data</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,412.15,393.33,10.91;10,112.66,425.70,393.33,10.91;10,112.66,439.25,293.98,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,274.26,412.15,231.73,10.91;10,112.66,425.70,103.38,10.91">Overview of the CLEF-2021 CheckThat! lab task 3 on fake news detection</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Struß</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mandl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,240.60,425.70,265.39,10.91;10,112.66,439.25,133.32,10.91">Working Notes of CLEF 2021-Conference and Labs of the Evaluation Forum, CLEF &apos;2021</title>
		<meeting><address><addrLine>Bucharest, Romania (online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
