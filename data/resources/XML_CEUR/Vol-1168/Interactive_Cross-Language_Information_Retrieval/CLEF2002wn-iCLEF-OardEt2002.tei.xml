<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,155.42,148.86,292.17,15.15">The CLEF 2002 Interactive Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,219.62,182.75,59.99,8.74;1,279.60,179.78,1.41,7.44"><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Departamento de Lenguajes y Sistemas Informáticos</orgName>
								<orgName type="department" key="dep2">E.T.S.I Industriales</orgName>
								<orgName type="institution">Universidad Nacional de Educación a Distancia</orgName>
								<address>
									<addrLine>Ciudad Universitaria s/n</addrLine>
									<postCode>28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">SPAIN</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,306.77,182.75,76.61,8.74;1,383.38,179.78,1.88,7.44"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
							<email>oard@glue.umd.edu</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">College of Information Studies</orgName>
								<orgName type="department" key="dep2">Institute for Advanced Computer Studies</orgName>
								<orgName type="laboratory">Human-Computer Interaction Laboratory</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<addrLine>College Park</addrLine>
									<postCode>20742</postCode>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,155.42,148.86,292.17,15.15">The CLEF 2002 Interactive Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BA7547C637AF5894024B9B0B57081969</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the CLEF 2002 Interactive Track, research groups interested in the design of systems to support interactive Cross-Language Retrieval used a shared experiment design to explore aspects of that question. Participating teams each compared two systems, both supporting a full retrieval task where users had to select relevant documents given a (native language) topic and a (foreign language) document collection. The two systems being compared at each site should differ in (at least) one of these aspects: a) support for document selection (how the system describes the content of a document written in a foreign language), b) support for query translation (how the system interacts with the user in order to obtain an optimal translation of the query), and c) support for query refinement (how the system helps the user refine their query based on previous search results). This paper describes the shared experiment design and summarizes preliminary results from the five teams that submitted runs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A Cross-Language Information Retrieval (CLIR) system, as that term is typically used, takes a query in some natural language and finds documents written in one or more other languages. From a user's perspective, that is only one component of a system to help a user search foreignlanguage collections and recognize relevant documents. We generally refer to this situated task as Multilingual Information Access. To emphasize the importance of interactive mechanisms in a situated, user-centered cross-language search, we might refer to systems that support that task as Cross-Language Search Assistants.</p><p>The Cross-Language Evaluation Forum's (CLEF) interactive track (iCLEF) aims to develop shared experiment designs that will allow research teams to compare their strategies for crosslanguage search assistance. In the first year of the track, iCLEF 2001 focused on comparing document selection strategies (i.e., approaches to facilitate fast and accurate relevance judgments for documents that the user could not read without assistance). The experiment guidelines combined CLEF resources (including the CLEF documents and topic descriptions, ranked lists from CLEF 2000, and relevance assessments made by native speakers in every document language) with the within-subject quantitative user study design that was used for many years in the TREC interactive track. The success of that evaluation <ref type="bibr" coords="1,304.16,598.84,10.52,8.74" target="#b2">[3]</ref> led to a decision to continue the track in the CLEF 2002 evaluation campaign. This paper describes the design of the iCLEF 2002 evaluation an presents some initial results.</p><p>The CLEF 2002 interactive track retained provisions for studying document selection, but added a new focus on comparing interactive query formulation and reformulation. Unlike document selection, which can reasonably be studied as an isolated process, query formulation and reformulation only make sense as part of a larger process; queries have no inherent value beyond their effect on search results. We therefore chose an experiment design that supported comparison of complete interactive CLIR systems, while retaining provisions for more focused experiments.</p><p>Each participating team compared two systems that differed in one or more aspects of the interaction with the user. Five groups submitted results: SICS (Sweden), Univ. of Sheffield (Great Britain), Univ. of Alicante (Spain), Univ. of Maryland (USA) and UNED (Spain).</p><p>In Section 2 we describe the experiment design in detail, and in Section 3 we enumerate the participants and the hypotheses that they sought to test. In Section 4, we summarize the initial results that we have been able to assess before the workshop, and discuss the suitability of our current design. Finally,in Section 5 we draw some conclusions and describe the prospects for future iCLEF evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experiment Design</head><p>The basic design for an iCLEF 2002 experiment consists of:</p><p>• Two systems to be compared, usually one of which is intended as a reference system;</p><p>• A set of searchers, in groups of 4;</p><p>• A set of topic descriptions, written in a language in which the searchers are fluent;</p><p>• A document collection in a different language (usually one in which the searchers lack language skills);</p><p>• A standardized procedure to be followed in every search session;</p><p>• A presentation order (i.e., a list of user/topic/system combinations which defines every search session and their relative order); and</p><p>• A set of evaluation measures for every search session and for the overall experiment, to permit comparison between systems.</p><p>In the remainder of this section, we describe these aspects in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Topics</head><p>Topics for iCLEF 2002 were selected from those used for evaluation of fully automated ranked retrieval systems in the CLEF 2001 evaluation campaign. The main reason that we selected a previous year's topics was that it allowed more time between topic release and the submission deadline, and important factor when performing user studies. The criteria for topic selection were:</p><p>• Select only broad (i.e., multi-faceted) topics. In iCLEF 2001, we observed that narrow (single-faceted) topics tended to have very few relevant documents, which made evaluation measures based on the fraction of relevant documents retrieved less insightful.</p><p>• Select topics that had at least 8 relevant documents in every document language, according to CLEF 2001 assessments.</p><p>• Select topics that could reasonably be expected to be found in collections from different years. This provided a degree of assurance that the new CLEF 2002 document collections could also be used by participating teams (the Finnish collection is mainly news from 1995, while the others are mainly 1994).</p><p>These restrictions were satisfied by eight topics, from which four were selected as iCLEF 2002 topics: &lt;num&gt; C053 &lt;/num&gt; &lt;EN-title&gt; Genes and Diseases &lt;/EN-title&gt; &lt;EN-desc&gt; What genes have been identified that are the source of or contribute to the cause of diseases or developmental disorders in human beings? &lt;/EN-desc&gt; &lt;EN-narr&gt; A document that identifies a gene or reports that a gene has been discovered that is the source of any type of disease, syndrome, behavioral or developmental disorder in humans is relevant. Any document that reports the discovery of a defective gene that causes problems in humans is relevant, but reports of diseases and disorders that are caused by the absence of a gene are not relevant. &lt;/EN-narr&gt; &lt;num&gt; C056 &lt;/num&gt; &lt;EN-title&gt; European Campaigns against Racism &lt;/EN-title&gt; &lt;EN-desc&gt; Find documents that talk about campaigns against racism in Europe. &lt;/EN-desc&gt; &lt;EN-narr&gt; Relevant documents describe informative or educational campaigns against racism (ethnic or religious, or against immigrants) in Europe. Documents should refer to organized campaigns rather than reporting mere opinions against racism. &lt;/EN-narr&gt; &lt;num&gt; C065 &lt;/num&gt; &lt;EN-title&gt; Treasure Hunting &lt;/EN-title&gt; &lt;EN-desc&gt; Find documents about treasure hunters and treasure hunting activities. &lt;/EN-desc&gt; &lt;EN-narr&gt; Identify types of current treasure hunting activities such as searching for gold, digging for buried relics, or searching underwater for sunken galleons. &lt;/EN-narr&gt; &lt;num&gt; C080 &lt;/num&gt; &lt;EN-title&gt; Hunger Strikes &lt;/EN-title&gt; &lt;EN-desc&gt; Documents will report any information relating to a hunger strike attempted in order to attract attention to a cause. &lt;/EN-desc&gt; &lt;EN-narr&gt; Identify instances where a hunger strike has been initiated, including the reason for the strike, and the outcome if known. &lt;/EN-narr&gt; and one was selected as a training topic: &lt;num&gt; C086 &lt;/num&gt; &lt;EN-title&gt; Renewable Power &lt;/EN-title&gt; &lt;EN-desc&gt; Find documents describing the use of or policies regarding "green" power, i.e., power generated from renewable energy sources. &lt;/EN-desc&gt; &lt;EN-narr&gt; Relevant documents discuss the use of renewable energy sources such as solar, wind, biomass, hydro, and geothermal sources. Low emission vehicles as for example electric or CNG cars are not relevant. Fuel cells are not relevant unless their fuel qualifies as renewable. &lt;/EN-narr&gt;</p><p>The number of relevant documents for these topics in the CLEF 2001 pools can be seen in Table <ref type="table" coords="3,117.40,681.88,3.88,8.74" target="#tab_0">1</ref>.</p><p>We did not impose any restriction on the topic language. Participants could pick any topic language provided by CLEF, or could prepare their own manual translations into any additional language that would be appropriate for their searcher population. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Document Collection</head><p>We allowed participants to search any CLEF document collections (Dutch, English, French, German, Italian, Spanish, Finnish and Swedish). To facilitate cross-site comparisons, we provided standard Machine Translations of the German collection (into English) and of the English collection (into Spanish) for use by teams that found those language pairs convenient, in each case using Systran Professional 3.0. A fraction of the German collection (the Frankfurter Rundschau set) was discarded for iCLEF experiments because both Systran and an alternative system that we tried produced no output for a significant fraction of those documents. The other collections were used in their entirety.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Search Procedure</head><p>For teams that chose the end-to-end experiment design, searchers were given a topic description written in a language that they could understand and asked to use one of the two systems to find as many relevant documents as possible in the foreign-language document collection. Searchers were instructed to favor precision rather than recall by asking them to envision a situation in which they might need to pay for a high-quality professional translation of the documents that they selected, but that they wished to avoid paying for translation of irrelevant documents. The searchers were asked to answer some questions at specific points during their session:</p><p>• Before the experiment, about computer/searching experience and attitudes, and their language skills.</p><p>• After completing the search for each topic (one per topic).</p><p>• After completing the use of each system (one per system).</p><p>• After the experiment, about system comparison and general feedback on the experiment design.</p><p>These questions were normally posed using questionnaires that closely followed the design of the questionnaires used in iCLEF 2001. This year, however, we did not require the use of standardized questionnaires; Participating teams could adapt the examples that we provided to their particular experiment conditions in whatever way they wished.</p><p>Every searcher performed four searches, first for two topics using one system and for the remaining two topics with the other system. Each search was limited to 20 minutes. The overall time required for one session was approximately three hours, including initial training with both systems, four 20-minute searches, all questionnaires, and two breaks (one following training, one between systems).</p><p>For teams that chose to focus solely on document selection, the experiment design was similar, but searchers were asked only to scan a frozen list of documents (returned by for some standard query by some automatic system) and select the ones that were relevant to the topic description from which the query had been generated. This is essentially the iCLEF 2001 task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Searcher/Topic/System Combinations</head><p>The presentation order for topics, searchers and systems was standardized to facilitate comparison between systems. We chose an order that was counterbalanced in a way that sought to minimize user/system and topic/system interactions when examining averages. We adopted a Latin square design similar to that used in the TREC interactive track. The presentation order for topics was varied systematically, with participants that saw the same topic-system combination seeing those topics in a different order. An eight-participant presentation order matrix is shown in Table <ref type="table" coords="5,500.78,190.18,3.88,8.74" target="#tab_1">2</ref>. <ref type="foot" coords="5,508.53,188.61,3.97,6.12" target="#foot_0">1</ref>The minimum number of participants was set at 4, in which case only the top half of the would be used. Additional participants could be added in groups of 8, with the same matrix being reused as needed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Searcher</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Evaluation</head><p>In this section we describe the common evaluation measure used by all teams, and the data that was available to individual teams to support additional evaluation activities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1">Data Collection</head><p>For every search (searcher/topic/system combination), two types of data are collected:</p><p>• The set of documents selected as relevant by the searcher. Optional attributes are the duration of the assessment process, the confidence in the assessment, and judgment values other than "relevant" (such as "somewhat relevant," "not relevant," or "viewed but not judged."</p><p>• The ranked lists of document identifiers created by the ranked retrieval system. One list was submitted by teams focusing on document selection; teams focused on query formulation and reformulation were asked to submit one ranked list for every query refinement iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2">Official Evaluation Measure</head><p>The set of documents selected as relevant was used to produce the official iCLEF measure, an unbalanced version of van Rijsbergen's F measure that we called F α :</p><formula xml:id="formula_0" coords="5,249.98,610.47,101.77,22.87">F α = 1 α/P + (1 -α)/R</formula><p>where P is precision and R is recall <ref type="bibr" coords="5,270.67,638.51,9.97,8.74" target="#b3">[4]</ref>. Values of α above 0.5 emphasize precision, values below 0.5 emphasize recall <ref type="bibr" coords="5,207.69,650.46,9.97,8.74" target="#b1">[2]</ref>. As in CLEF 2001, α = 0.8 was chosen, modeling the case in which missing some relevant documents would be less objectionable than finding too many documents that, after perhaps paying for professional translations, turn out not to be relevant.</p><p>The comparison of average F α=0.8 measures for both systems provides the official, first order differentiation of systems. All complementary material (ranked lists for each iteration, assessment duration, assessment confidence, questionnaire responses, observational notes, etc.) can be used by participating groups as a basis for further analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.3">Relevance assessments</head><p>We provided relevance assessments by native speakers of the document languages for at least:</p><p>• All documents manually selected by searchers (to compute F α=0.8 ).</p><p>• The first 20 documents in all iterative rankings produced along every search process.</p><p>For the CLEF 2001 document languages (English, German, Italian, Spanish, Dutch, and French) we already had some assessments available from the CLEF 2001 pools. In the case of Finnish and Swedish, all assessments had to be done from scratch. All iCLEF 2002 relevance judgments were done by CLEF assessors immediately after assessing the CLEF 2002 pools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Participants</head><p>Six teams expressed interest in participating, and five teams submitted experiment results: three that had participated in iCLEF 2001 (Univ. of Sheffield, Univ. of Maryland and UNED), and two new teams (SICS and Univ. of Alicante). Both newcomers focused on the document selection subtask:</p><p>• SICS (Sweden) focused on the document selection task. Their hypotheses was that assessing documents in one's native language would be less work than assessing documents in another language, even if that language is relatively well mastered. Therefore they used one searcher's language (Swedish) and two document languages: English and Swedish. Three groups of four Swedish users with high English skills participated in the experiment. The users were presented with prefabricated ranked lists of search results in an interface which allowed them to view each document and assess it for relevance. The ranked lists were either from the Swedish or the English CLEF collection, forming the two conditions being tested (native language versus foreign language assessments).</p><p>• University of Alicante (Spain) also focused on document selection, comparing full machine translations (as the reference condition) with topic-oriented summaries of the same translations, containing the title and the most relevant paragraph for the topic being searched (as the contrastive condition). They used Spanish as the searchers' language, and English as document language.</p><p>The other three groups focused on the query formulation and refinement aspects of interactive searches:</p><p>• University of Sheffield (Great Britain). The submission from the University of Sheffield used a prototype system being developed jointly between Sheffield University, the Swedish Institute of Computer Science (SICS), and the University of Tampere. The search engine was created by Tampere using a modified version of the Inquery search system. The interface was designed by SICS and Sheffield researchers basing their design on interviews and observations of users with CLIR needs. The system is a preliminary prototype; results from this year's iCLEF will inform changes in the next version of the system.</p><p>• University of Maryland (USA) focused on query formulation and reformulation. Four searchers were used in the official submission to compare user-assisted query translation with a fully automatic approach, and an additional eight searchers performed the same experiment with a smaller collection. The hypothesis being tested was that user-assisted query translation could improve search effectiveness. The document language was German, and the topic language was English. For the user-assisted query translation condition, searchers were provided two types of cues about the meaning of each translation: a list of other words sharing a common translation (potential synonyms) and a sentence in which the word was used in a translation-appropriate context selected from a word-aligned parallel corpus.</p><p>• UNED (Spain) also focused on query formulation and refinement. A set of 8 searchers were used to compare a) a reference system using words as units for query formulation, assisted translation and query refinement, and b) a contrastive system using phrases for query formulation and refinement. The hypothesis being tested was twofold: 1) that users would prefer not to examine candidate translations in a foreign language, and 2) that phrases as interactive query formulation units could provide enough context information for accurate automatic translation, as an alternative to word-by-word assisted translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>The official F α=0.8 measure for all systems is shown in Table <ref type="table" coords="7,361.44,238.52,3.88,8.74" target="#tab_2">3</ref>. A detailed discussion of each of the experiments can be found elsewhere in these workshop proceedings. Most experiments showed substantial differences between the systems being compared, suggesting that there is a good deal to be learned from the detailed analysis that each team will report on. A detailed analysis of the strengths and weaknesses of the shared experiment design cannot be done prior to the workshop discussions. There is, however, some data already available: the comparative assessment figures between CLEF 2001 and iCLEF 2002, for the same four topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group</head><p>German and English are the two languages for which a) there were available pools from CLEF 2001, and b) participants run end-to-end interactive cross-language sessions to contribute new documents to the assessment pools (either because they were selected by the searchers as relevant or because they appeared near the top of some ranked lists during the search processes).</p><p>As explained above, native speakers performed manual relevance assessments for:</p><p>• All documents manually selected by searchers (to compute F α=0.8 ).</p><p>• The first 20 documents in all every ranked list produced in and search process.</p><p>Voorhees has found that manual TREC runs (those which include any form of human intervention in the search process) often find document that are not present in assessment pools generated from the output of automatic systems <ref type="bibr" coords="7,260.15,663.67,9.97,8.74" target="#b4">[5]</ref>. The CLEF 2001 pools (produced from 198 submitted runs) were already large and stable <ref type="bibr" coords="7,247.14,675.62,9.97,8.74" target="#b0">[1]</ref>, so the iCLEF 2002 assessment pools provided us with an opportunity to explore this issue in a cross-language search context. We observed a similar effect. Table <ref type="table" coords="7,117.11,699.53,4.98,8.74" target="#tab_3">4</ref> summarizes the additional assessments and the additional relevant documents found with the new assessments.</p><p>In the case of the SDA and Der Spiegel subset of the German collection used in our evaluation, the large number of query reformulation iterations produced enormous pools, but only increased the set of known relevant documents by 10%. The newly judged pools were substantially smaller for English, but the set of known relevant documents still was increased by 12%.</p><p>From this, we can conclude that although human searchers do find relevant documents that automatic systems miss, the search strategies that they employed resulted in many more nonrelevant documents. This is a classic recall-precision tradeoff. It is important, of course, to caveat this observation by pointing out that we explored only a limited range of conditions (in particular, 20 minute searches for broad topics).</p><p>Another question that we might ask is whether number of documents added to the assessment pools is correlated with the number of relevant documents contained in those pools. As Table <ref type="table" coords="8,118.07,219.62,4.98,8.74" target="#tab_3">4</ref> indicates, there does seem to be a weak negative correlation; the topic with the fewest newly discovered relevant documents generated the largest assessment pools, for example. Our experiment design required relevance assessments for only 4 topics, so the assessment costs were not prohibitive in this case. But these observations may be helpful as we design future interactive CLIR studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic</head><p>German </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Together, the five teams that participated in iCLEF 2002 had 38 searchers perform 158 searches in four document languages to test a broad range of hypothesis related to the design of crosslanguage search assistance systems. To the best of our knowledge, this is the largest multilingual information access user study ever performed. We therefore believe that the results obtained by the participating teams will be a rich source of evidence from which we can learn more about the way cross-language information retrieval technology will ultimately be used. Perhaps even more importantly, we have enriched our understanding of the design of user studies for end-to-end cross-language search assistance systems, and have expanded the community of researchers that share an interest in this important question. We look forward to discussing this in Rome, and then to setting a direction for iCLEF 2003.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,124.57,110.82,353.87,91.17"><head>Table 1 :</head><label>1</label><figDesc>Number of relevant documents for iCLEF 2002 topics in previous pools.</figDesc><table coords="4,160.84,110.82,281.34,69.31"><row><cell cols="7">Topic Dutch English French German Italian Spanish</cell></row><row><cell>53</cell><cell>27</cell><cell>36</cell><cell>13</cell><cell>32</cell><cell>37</cell><cell>33</cell></row><row><cell>56</cell><cell>8</cell><cell>10</cell><cell>44</cell><cell>36</cell><cell>24</cell><cell>137</cell></row><row><cell>65</cell><cell>69</cell><cell>15</cell><cell>13</cell><cell>65</cell><cell>15</cell><cell>74</cell></row><row><cell>80</cell><cell>93</cell><cell>56</cell><cell>31</cell><cell>120</cell><cell>84</cell><cell>245</cell></row><row><cell>86</cell><cell>50</cell><cell>82</cell><cell>36</cell><cell>111</cell><cell>31</cell><cell>56</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,121.05,245.10,379.54,90.01"><head>Table 2 :</head><label>2</label><figDesc>Presentation order for topics and association of topics with systems.</figDesc><table coords="5,121.05,245.10,379.54,58.18"><row><cell></cell><cell>Block 1</cell><cell>Block 2</cell><cell>Searcher</cell><cell>Block 1</cell><cell>Block 2</cell></row><row><cell>1</cell><cell cols="2">System 1: 1-4 System 2: 3-2</cell><cell>5</cell><cell cols="2">System 1: 4-2 System 2: 1-3</cell></row><row><cell>2</cell><cell cols="2">System 2: 2-3 System 1: 4-1</cell><cell>6</cell><cell cols="2">System 2: 3-1 System 1: 2-4</cell></row><row><cell>3</cell><cell cols="2">System 2: 1-4 System 1: 3-2</cell><cell>7</cell><cell cols="2">System 2: 4-2 System 1: 1-3</cell></row><row><cell>4</cell><cell cols="2">System 1: 2-3 System 2: 4-1</cell><cell>8</cell><cell cols="2">System 1: 3-1 System 2: 2-4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,164.43,297.04,273.64,176.05"><head>Table 3 :</head><label>3</label><figDesc>Official iCLEF 2002 results.</figDesc><table coords="7,164.43,297.04,273.64,154.19"><row><cell></cell><cell>Experiment Condition</cell><cell>F α=0.8</cell></row><row><cell cols="3">Experiments in Query formulation and refinement</cell></row><row><cell cols="2">Maryland query formulation automatic translation</cell><cell>0.34</cell></row><row><cell cols="2">Maryland query formulation w. assisted translation</cell><cell>0.50</cell></row><row><cell>Sheffield</cell><cell>TRAN</cell><cell>0.06</cell></row><row><cell>Sheffield</cell><cell>NO TRAN</cell><cell>0.08</cell></row><row><cell>UNED</cell><cell>query formulation by words</cell><cell>0.23</cell></row><row><cell>UNED</cell><cell>query formulation by phrases</cell><cell>0.37</cell></row><row><cell></cell><cell>Experiments in Document selection</cell><cell></cell></row><row><cell>SICS</cell><cell>foreign language docs</cell><cell>0.36</cell></row><row><cell>SICS</cell><cell>native language docs</cell><cell>0.65</cell></row><row><cell>Alicante</cell><cell>Systran full translations</cell><cell>0.22</cell></row><row><cell>Alicante</cell><cell>Systran title + best passage</cell><cell>0.32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,165.18,287.76,272.65,187.01"><head>Table 4 :</head><label>4</label><figDesc>Contribution of interactive runs to CLEF 2001 pools.</figDesc><table coords="8,189.77,287.76,223.47,165.15"><row><cell></cell><cell></cell><cell>CLEF</cell><cell cols="2">iCLEF add-on</cell></row><row><cell></cell><cell cols="2">(SDA+Spiegel)</cell><cell cols="2">(SDA+Spiegel)</cell></row><row><cell></cell><cell cols="4">assessed relevant assessed relevant</cell></row><row><cell>53</cell><cell>220</cell><cell>17</cell><cell>225</cell><cell>5(+30%)</cell></row><row><cell>56</cell><cell>230</cell><cell>20</cell><cell>465</cell><cell>5(+25%)</cell></row><row><cell>65</cell><cell>249</cell><cell>47</cell><cell>835</cell><cell>0(=)</cell></row><row><cell>80</cell><cell>118</cell><cell>62</cell><cell>450</cell><cell>6(+10%)</cell></row><row><cell>Topic</cell><cell cols="2">English CLEF</cell><cell cols="2">iCLEF add-on</cell></row><row><cell></cell><cell cols="3">assessed relevant assessed</cell><cell>relevant</cell></row><row><cell>53</cell><cell>456</cell><cell>36</cell><cell>22</cell><cell>1 (+03%)</cell></row><row><cell>56</cell><cell>626</cell><cell>10</cell><cell>419</cell><cell>1 (+10%)</cell></row><row><cell>65</cell><cell>613</cell><cell>15</cell><cell>233</cell><cell>10 (+67%)</cell></row><row><cell>80</cell><cell>578</cell><cell>56</cell><cell>250</cell><cell>2 (+04%)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,105.24,737.54,407.74,6.99;5,90.00,747.00,166.25,6.99"><p>This table was prepared before the topics were chosen, and some participating teams refer to topics numbered 1-4 in their papers. The mapping for iCLEF</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2002" xml:id="foot_1" coords="5,278.83,747.00,144.93,6.99"><p>is 1=C053, 2=C065, 3=C056, 4=C080.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We are indebted to many people that helped along the organization of this iCLEF track: <rs type="person">Fernando López</rs> wrote the evaluation scripts and maintained the web site and distribution list; <rs type="person">Martin Braschler</rs> created the assessment pools; <rs type="person">Ellen Voorhees</rs>, <rs type="person">Michael Kluck</rs>, <rs type="person">Eija Airio</rs> and <rs type="person">Jorun Kugelberg</rs> provided native relevance assessments; and <rs type="person">Jianqiang Wang</rs> and <rs type="person">Dina Demner</rs> provided Systran translations for German and English collections. Finally, we also want to thank <rs type="person">Carol Peters</rs> for her permanent support and encouragement.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="9,105.49,178.74,407.51,8.74;9,105.50,190.70,232.37,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,170.53,178.74,132.71,8.74">Clef 2001 -overview of results</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,325.98,178.74,187.02,8.74;9,105.50,190.70,75.00,8.74">Evaluation of Cross-Language Information Retrieval Systems</title>
		<imprint>
			<publisher>Springer-Verlag LNCS</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2406</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.49,210.62,407.51,8.74;9,105.50,222.58,407.50,8.74;9,105.50,234.53,280.56,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,172.48,210.62,296.29,8.74">Evaluating cross-language information retrieval: Document selection</title>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,172.26,222.58,340.74,8.74;9,105.50,234.53,18.51,8.74">Cross-Language Information Retrieval and Evaluation: Proceedings of CLEF 2000</title>
		<title level="s" coord="9,202.88,234.53,152.87,8.74">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.49,254.46,407.50,8.74;9,105.50,266.41,310.22,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,281.13,254.46,148.14,8.74">The CLEF 2001 interactive track</title>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,137.07,266.41,113.87,8.74">Proceedings of CLEF 2001</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<meeting>CLEF 2001</meeting>
		<imprint>
			<publisher>Springer-Verlag LNCS Series</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.49,286.34,391.54,8.74" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="9,201.87,286.34,92.98,8.74">Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979">1979</date>
			<publisher>Butterworths</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct coords="9,105.49,306.26,407.50,8.74;9,105.50,318.22,407.50,8.74;9,105.50,330.17,194.57,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,163.42,306.26,345.59,8.74">Variations in relevance judgments and the measurement of retrieval effectiveness</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,118.93,318.22,394.07,8.74;9,105.50,330.17,164.41,8.74">Proceedings of the 21 st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 21 st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
