<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,165.51,87.68,264.26,12.63">PLIERS AND SNOWBALL AT CLEF 2002</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,276.86,114.11,55.20,8.96"><forename type="first">A</forename><surname>Macfarlane</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Science</orgName>
								<orgName type="laboratory">Centre for Interactive Systems Research</orgName>
								<orgName type="institution">City University</orgName>
								<address>
									<addrLine>Northampton Square</addrLine>
									<postCode>LONDON, EC1V 0HB</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,165.51,87.68,264.26,12.63">PLIERS AND SNOWBALL AT CLEF 2002</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C0E2C6E2B5606FC3A60CCAFEEA2A3E99</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We test the utility of European language stemmers created using the Snowball language [1]. This allows us to experiment with PLIERS in languages other than English. We also report on some BM25 tuning constant experiments conducted in order to find the best settings for our searches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this paper we briefly describe our experiments at CLEF 2002. We address a number of issues as follows. The snowball language was recently created by Porter <ref type="bibr" coords="1,287.93,254.38,11.71,8.96" target="#b1">[2]</ref> in order to provide generic mechanism for creating stemmers. The main purpose of the experiments was to investigate the utility of these stemmers and as to whether a reasonable level of retrieval effectiveness could be achieved by using Snowball in an information retrieval system. This allowed us to port the PLIERS system <ref type="bibr" coords="1,314.59,288.81,11.71,8.96" target="#b2">[3]</ref> for use on languages other than English. We also investigate the variation of tuning constants for the BM25 weighting function for the chosen European languages. The languages used for our experiments are as follows: German, French, Dutch, Italian, German and Finnish. All experiments were conducted on a Pentium 4 machine with 256 MB of memory and 240 GB of disk space. The operating system used was Red Hat Linux 7.2. All search runs were done using the Robertson/Sparck Jones Probabilistic model -the BM25 weighting model was used. All our runs are in the monolingual track. All queries derived from topics are automatic.</p><p>The paper is organised as follows. Section two describes our motivation for doing this research. In section three we describe our indexing methodology and results. In section 4 we describe some preparatory results using CLEF 2001 data. Section 5 describes our CLEF 2002 results, and a conclusion is given at the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Motivation for the Research</head><p>We have several different strands in our research agenda. The most significant of these is the issue of using Snowball stemmers in information retrieval, both in terms of retrieval effectiveness and retrieval efficiency. In terms of retrieval efficiency we want to quantify the cost of stemming both for indexing collections and servicing queries over them. How expensive is stemming in terms of time when processing words? Our hypothesis for search would be that stemming will increase inverted list size and that this would in turn lead to slower response times for queries. Stemming will slow down indexing, but by how much? Due to time constraints we restrict our discussion on search efficiency to CLEF 2002 runs. With respect to retrieval effectiveness we hope to demonstrate that using Snowball stemmers leads to an increase in retrieval effectiveness for all languages, and any deterioration in results should not be significantly worse. Our hypothesis is therefore: "stemming using Snowball will lead to an increase in retrieval effectiveness". A further issue that we wish to address is that of tuning constants for the BM25 weighting model. Our previous research on this issue has been done on collections of English <ref type="bibr" coords="1,167.45,567.07,10.69,8.96" target="#b2">[3]</ref>, but we want to investigate the issue on other types of European languages. A hypothesis is formulated in section 4 where the issue is discussed in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Indexing methodology and results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Indexing methodology</head><p>We used a simple and straightforward methodology for indexing: parsing, remove stop words, stemming in the given language. The PLIERS HTML/SGML parser needed to be altered to detect non-ascii characters such as those with umlauts, accents, circumflexes etc. The stemmers were easily incorporated into the PLIERS library. We used various stop word lists for the language, gathered from the internet. The official runs for Finnish did not use stemming, as no stemmer was available for that language while experiments were being conducted. The key results here are that stemmed builds take up slightly less space for most languages (it is a significant saving for French and Italian) and that builds with stemming take significantly longer than builds with no stemming. Builds with no stemming index text at a rate of 3.7 to 4.5 GB per hour compared with 0.48 to 0.89 GB per hour for stemmed builds. The results for stemmed builds are acceptable however.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Indexing results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Preparatory experiments: working with CLEF 2001 data</head><p>In order to find the best tuning constants for our CLEF 2002 runs we conducted tuning constant variation experiments on the CLEF 2001 data for the following languages: French, German, Dutch, Spanish and Italian. We were unable to conduct experiments with Finnish data as this track was not run in 2001: we arbitrarily chose K1=1.5 and B=0.8 for our Finnish runs.</p><p>We give a brief description of the BM25 tuning constants being discussed here <ref type="bibr" coords="2,399.15,588.91,10.69,8.96" target="#b3">[4]</ref>. The K1 constant alters the influence of term frequency in the BM25 function, while the B constant alters the influence of normalised average document length. Values of K1 can range from 0 to infinity, whereas the values of B are with the range 1 (document lengths used unaltered) to 0 (document length data not used at all).</p><p>We used the following strategy for tuning constant variation. For K1 we start with a value of 0.25 with increments of 0.25 to a maximum of 3.0, stopping when it was obvious that no further useful data could be gathered. For B we used a range of 0.1 to 0.9 with increments of 0.1. A maximum of 135 experiments were therefore conducted for each language. Due to lack of time, our aim in these experiments was to achieve a better than baseline retrieval effectiveness for our preparatory CLEF 2001 experiments. For the most part we succeeded in doing this for both types of build. However we can separate our results into three main groups:</p><p>• For Dutch and German we were able to better six systems with our runs.</p><p>• For French and Italian, we were unable to better more than one run, but our effectiveness is considerably better than the official baseline runs.</p><p>• For Spanish we were unable to show much of an improvement over the baseline run.</p><p>Having said that we have a long way to go before our runs achieve the levels of performance of groups such as the University of Neuchatel particularly for languages such as French and Spanish.</p><p>We were unable to investigate the reason for the levels of performance achieved because of time constraints, but it is believed that the automatic query generator used to select terms for queries is simplistic and needs to be replaced with a more sophisticated mechanism. Our reason for believing that this might be the problem is that our results for Title only queries on the Spanish run are superior to those queries that were derived from Title and Description: this is counter to what we would expect.</p><p>When comparing experiments on those builds which used stemming and those that did not, we can separate our runs into three main groups:</p><p>• Stemming is an advantage: We were able to demonstrate that stemming was a positive advantage for both Italian and Spanish runs. • Stemming makes no difference: Using stemming on French made very little difference either way.</p><p>• Stemming is a disadvantage: Stemming proved to be problematic for both Dutch and German.</p><p>We need to investigate the reason for these results. We are surprised that stemming is a disadvantage for any language Table <ref type="table" coords="4,96.51,564.07,4.98,8.96" target="#tab_7">6</ref> shows timings for each official run together with the average sizes of each query. All our runs have met the one to ten second response time criteria specified by Frakes <ref type="bibr" coords="4,336.37,575.59,10.64,8.96" target="#b4">[5]</ref>, and all bar one have sub second response times. Title description runs are significantly slower than title only. Table <ref type="table" coords="5,96.75,142.67,4.98,8.96" target="#tab_8">7</ref> show the alternative runs which means that searches were conducted on builds with no stemming, apart from Finnish where we did have a stemmer available. As with our official runs the response times are acceptable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CLEF 2002 results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Retrieval efficiency results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language Average elapsed</head><p>With respect to the difference in response time between the two types of build, we can separate the results into three main groups:</p><p>• Runs on Stemmed builds are faster: German (TD).</p><p>• Runs on No-Stemmed builds are faster: German (T), Dutch, Italian, and Spanish.</p><p>• No significant difference in response times: French, Finnish.</p><p>In general this is what we would expect as inverted lists on stemmed builds tend to be larger than those of builds with no stemming. It is interesting to examine the exceptions and outliers, however. The reason German (TD) runs are faster on stemmed builds, is that the average query size is slightly larger by about half a term (more inverted lists are being processed on average). The Dutch no stem run is significantly faster on average than those of stemmed runs, but this is largely due to query size: queries with no stems on the Dutch collection are have 1.7 less terms on average than those of stemmed queries (fewer inverted lists are being processed). An interesting result with title only Finnish runs is that queries with no stems are nearly 3.5 times smaller than stemmed queries, but runs times are virtually identical: execution speeds are so small here it is difficult to separate them. It should be noted when we compared the size of queries with stems to those without stems, there is no clear pattern. We would suggest therefore that a simple hypothesis which suggested that runs on builds without stemming is faster on average than runs on stemmed builds cannot be supported with the evidence given here. It is clear that the number of inverted lists processes is an important factor as well as the size of the inverted lists. Table <ref type="table" coords="6,96.86,341.13,4.98,8.96" target="#tab_9">9</ref> shows the results on comparative runs, all using builds without stemming apart from Finnish where we did have a stemmer available. When comparing runs on different types of build we can separate our results into two main groups:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language</head><p>• Stemming is an advantage: French and Italian.</p><p>• Stemming is a disadvantage: German, Dutch, Spanish and Finnish.</p><p>The status for German and Dutch is unchanged from our CLEF 2001 results (stemming runs produced worse results), and also for Italian where the stemmer runs produced better results. Our results for French have improved comparatively, but for Spanish the results have deteriorated. The runs on the Finnish collection are particularly disappointing: the results for average precision on builds with stemming being about 40% worse for title only queries and nearly 60% worse on title/description queries than experiments on builds without stemming. The reason for this loss in performance could be because of the morphological complexity of Finnish and merits significant further investigation. It is also interesting that the initial version of the Finnish stemmer did slightly better in terms of average precision than the final version: this is offset with a slight loss in precision at 5 and 10 documents retrieved. The reduced effectiveness found on title/description queries compared with title only queries in our Spanish CLEF 2001 experiments was not repeated in our CLEF 2002 runs. However there is a slight loss in performance on the second version of the Finnish stemmer when comparing title/description queries to title only queries: this loss is consistent across all shown precision measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>A number of research questions have been raised during this investigation of the effectiveness of stemming utilitising Snowball. They are as follows:</p><p>• Why are results on Dutch and German consistently worse using Snowball stemmers?</p><p>• Why are the results using the Finnish snowball stemmer significantly worse?</p><p>• Why are the results on the Spanish collection inconsistent, both with the Snowball stemmer and varying the query type (title only and title/description)? • Why are the runs on Italian collections consistent, and how do we use evidence from these runs to improve the results of other romance languages such as Spanish and French?</p><p>Our hypothesis that suggested that the use of Snowball stemmers is always beneficial has not been confirmed. It may be possible to investigate this hypothesis further when we have addressed the research questions given above. We also have some conclusions with regard to retrieval efficiency and stemming:</p><p>• Stemming using the Snowball stemmers is costly when indexing, but does not slow down the process of inverted file generation to an unacceptable level. • We have confirmed that inverted file size is not the only factor in search speed, query size that requires processing of more inverted lists play an important part as well.</p><p>We are unable to comment on our tuning constant experiments due to time constraints, and hence our hypothesis for this workshop version of the paper, but will have a poster which shows the results graphically and the information will be included in the final version of the paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,114.15,107.87,365.20,92.24"><head>Elapsed Time (mins) Dictionary file size MB Postings file size MB Map file size MB % of text</head><label></label><figDesc></figDesc><table coords="2,114.15,131.15,354.74,68.96"><row><cell>German</cell><cell>34.3</cell><cell>22.58</cell><cell>149.83</cell><cell>7.29</cell><cell>34%</cell></row><row><cell>French</cell><cell>23.6</cell><cell>4.36</cell><cell>48.16</cell><cell>3.13</cell><cell>23%</cell></row><row><cell>Dutch</cell><cell>36.1</cell><cell>12.49</cell><cell>138.98</cell><cell>6.00</cell><cell>30%</cell></row><row><cell>Italian</cell><cell>33.2</cell><cell>6.04</cell><cell>62.37</cell><cell>3.49</cell><cell>26%</cell></row><row><cell>Spanish</cell><cell>44.3</cell><cell>8.14</cell><cell>135.46</cell><cell>7.35</cell><cell>30%</cell></row><row><cell>Finnish</cell><cell>10.2</cell><cell>21.07</cell><cell>37.17</cell><cell>1.91</cell><cell>45%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,70.59,214.66,454.44,196.39"><head>Table 1 -</head><label>1</label><figDesc>Indexing results for builds using stemmersThe indexing results are given in tables 1 and 2. We report the following information: elapsed time in minutes, dictionary file size in MB, postings file size in MB, data file size in MB, and the size of the build files compared with the text file. The dictionary file contains keywords detected in the indexing process together with a link to the inverted lists contained in the postings file, while the data (or map) file contains information such as the CLEF identifier and location on disk.</figDesc><table coords="2,109.59,318.93,369.41,92.12"><row><cell cols="2">Language Elapsed Time</cell><cell>Dictionary file</cell><cell>Postings file</cell><cell>Map file</cell><cell>% of</cell></row><row><cell></cell><cell>(mins)</cell><cell>size MB</cell><cell>size MB</cell><cell>size MB</cell><cell>text</cell></row><row><cell>German</cell><cell>8.28</cell><cell>28.6</cell><cell>159.3</cell><cell>7.29</cell><cell>37%</cell></row><row><cell>French</cell><cell>3.28</cell><cell>5.93</cell><cell>67.3</cell><cell>3.13</cell><cell>32%</cell></row><row><cell>Dutch</cell><cell>7.72</cell><cell>15.6</cell><cell>145.1</cell><cell>6.00</cell><cell>32%</cell></row><row><cell>Italian</cell><cell>4.05</cell><cell>8.09</cell><cell>87.2</cell><cell>3.49</cell><cell>36%</cell></row><row><cell>Spanish</cell><cell>6.78</cell><cell>9.95</cell><cell>140.6</cell><cell>7.35</cell><cell>31%</cell></row><row><cell>Finnish</cell><cell>2.30</cell><cell>21.1</cell><cell>37.2</cell><cell>1.91</cell><cell>45%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="2,175.35,425.60,244.74,8.96"><head>Table 2 -</head><label>2</label><figDesc>Indexing results for builds without using stemmers</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="2,70.59,703.86,454.41,43.52"><head>Table 3</head><label>3</label><figDesc></figDesc><table coords="3,177.15,96.47,240.29,80.72"><row><cell>Language</cell><cell cols="3">K1 Constant B Constant Query Type</cell></row><row><cell>German</cell><cell>1.75</cell><cell>0.5</cell><cell>T and TD</cell></row><row><cell>French</cell><cell>2.0</cell><cell>0.5</cell><cell>T and TD</cell></row><row><cell>Dutch</cell><cell>2.75</cell><cell>0.8</cell><cell>T and TD</cell></row><row><cell>Italian</cell><cell>2.5</cell><cell>0.6</cell><cell>TD</cell></row><row><cell>Spanish</cell><cell>2.75</cell><cell>0.5</cell><cell>TD</cell></row><row><cell>Spanish</cell><cell>1.75</cell><cell>0.6</cell><cell>T</cell></row></table><note coords="2,106.70,703.86,418.20,8.96;2,70.59,715.38,454.40,8.96;2,70.59,726.90,454.41,8.96;2,70.59,738.41,23.04,8.96"><p>show the best tuning constants found for the CLEF 2002 together with the query type used for that tuning constants combination [Note: T = Title only queries, TD=Title and Description]. Note that these tuning constants were found by running experiments on the stemming builds, for application on runs of both type of build.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="3,70.59,191.74,454.34,230.34"><head>Table 3 -</head><label>3</label><figDesc>CLEF 2001 experiment results with chosen tuning constants (builds with stemming) Tables4 and 5show the best results using CLEF 2001 data: using the tuning constants declared in Table3. We formulate the following hypothesis for tuning constants on the CLEF collections: "the best tuning constants are independent of a given query set". In other words the best tuning constants found in our CLEF 2001 experiments should also be the best for our CLEF 2002 experiments.</figDesc><table coords="3,173.19,284.37,245.47,137.71"><row><cell cols="2">Language Average</cell><cell>Precision</cell><cell>Precision</cell><cell>Query</cell></row><row><cell></cell><cell>precision</cell><cell>@5</cell><cell>@10</cell><cell>Type</cell></row><row><cell>German</cell><cell>.213</cell><cell>.363</cell><cell>.326</cell><cell>T</cell></row><row><cell></cell><cell>.269</cell><cell>.392</cell><cell>.373</cell><cell>TD</cell></row><row><cell>French</cell><cell>.243</cell><cell>.253</cell><cell>.226</cell><cell>T</cell></row><row><cell></cell><cell>.254</cell><cell>.302</cell><cell>.265</cell><cell>TD</cell></row><row><cell>Dutch</cell><cell>.207</cell><cell>.268</cell><cell>.208</cell><cell>T</cell></row><row><cell></cell><cell>.242</cell><cell>.336</cell><cell>.238</cell><cell>TD</cell></row><row><cell>Italian</cell><cell>.242</cell><cell>.260</cell><cell>.260</cell><cell>T</cell></row><row><cell></cell><cell>.253</cell><cell>.306</cell><cell>.289</cell><cell>TD</cell></row><row><cell>Spanish</cell><cell>.210</cell><cell>.286</cell><cell>.273</cell><cell>T</cell></row><row><cell></cell><cell>.225</cell><cell>.375</cell><cell>.322</cell><cell>TD</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="3,159.51,436.64,276.41,172.87"><head>Table 4 -</head><label>4</label><figDesc>CLEF 2001 experiment results on builds without stemming</figDesc><table coords="3,173.19,471.80,245.47,137.71"><row><cell cols="2">Language Average</cell><cell>Precision</cell><cell>Precision</cell><cell>Query</cell></row><row><cell></cell><cell>precision</cell><cell>@5</cell><cell>@10</cell><cell>Type</cell></row><row><cell>German</cell><cell>.192</cell><cell>.335</cell><cell>.300</cell><cell>T</cell></row><row><cell></cell><cell>.207</cell><cell>.367</cell><cell>.342</cell><cell>TD</cell></row><row><cell>French</cell><cell>.238</cell><cell>.269</cell><cell>.247</cell><cell>T</cell></row><row><cell></cell><cell>.256</cell><cell>.286</cell><cell>.249</cell><cell>TD</cell></row><row><cell>Dutch</cell><cell>.193</cell><cell>.236</cell><cell>.208</cell><cell>T</cell></row><row><cell></cell><cell>.228</cell><cell>.360</cell><cell>.292</cell><cell>TD</cell></row><row><cell>Italian</cell><cell>.266</cell><cell>.315</cell><cell>.300</cell><cell>T</cell></row><row><cell></cell><cell>.305</cell><cell>.379</cell><cell>.336</cell><cell>TD</cell></row><row><cell>Spanish</cell><cell>.264</cell><cell>.343</cell><cell>.322</cell><cell>T</cell></row><row><cell></cell><cell>.255</cell><cell>.427</cell><cell>.365</cell><cell>TD</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="3,165.87,635.59,263.68,8.96"><head>Table 5 -</head><label>5</label><figDesc>CLEF 2001 experiment results on builds with stemming</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="4,128.79,376.77,337.70,161.83"><head>Table 6 -</head><label>6</label><figDesc>CLEF 2002 experiment results with chosen tuning constants (official runs)</figDesc><table coords="4,132.15,376.77,320.72,138.31"><row><cell></cell><cell>Average elapsed</cell><cell>Average</cell><cell>Query Type</cell><cell>Run</cell></row><row><cell></cell><cell>Time (secs)</cell><cell>Query Size</cell><cell></cell><cell>Identifier</cell></row><row><cell>German</cell><cell>0.97</cell><cell>16.1</cell><cell>TD</cell><cell>plge02td</cell></row><row><cell></cell><cell>0.22</cell><cell>3.84</cell><cell>T</cell><cell>plge02t</cell></row><row><cell>French</cell><cell>0.05</cell><cell>10.2</cell><cell>TD</cell><cell>plfr02td</cell></row><row><cell>Dutch</cell><cell>1.51</cell><cell>34.5</cell><cell>TD</cell><cell>ptdu02td</cell></row><row><cell></cell><cell>0.28</cell><cell>4.50</cell><cell>T</cell><cell>ptdu02t</cell></row><row><cell>Italian</cell><cell>0.33</cell><cell>12.0</cell><cell>TD</cell><cell>plit02td</cell></row><row><cell>Spanish</cell><cell>1.23</cell><cell>17.2</cell><cell>TD</cell><cell>plsp02td</cell></row><row><cell></cell><cell>0.52</cell><cell>5.06</cell><cell>T</cell><cell>plsp02t</cell></row><row><cell>Finnish</cell><cell>0.12</cell><cell>13.6</cell><cell>TD</cell><cell>plfn02td</cell></row><row><cell></cell><cell>0.01</cell><cell>12.2</cell><cell>T</cell><cell>plfn02t</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="4,168.15,633.78,254.13,114.68"><head>Table 7 -</head><label>7</label><figDesc>CLEF 2002 experiment results with chosen tuning constants (alternative runs)</figDesc><table coords="4,168.15,633.78,254.13,114.68"><row><cell></cell><cell></cell><cell>Average</cell><cell>Query Type</cell></row><row><cell></cell><cell>Time (secs)</cell><cell>Query Size</cell><cell></cell></row><row><cell>German</cell><cell>1.07</cell><cell>16.6</cell><cell>TD</cell></row><row><cell></cell><cell>0.19</cell><cell>3.82</cell><cell>T</cell></row><row><cell>French</cell><cell>0.05</cell><cell>10.4</cell><cell>TD</cell></row><row><cell>Dutch</cell><cell>1.47</cell><cell>34.9</cell><cell>TD</cell></row><row><cell></cell><cell>0.03</cell><cell>2.80</cell><cell>T</cell></row><row><cell>Italian</cell><cell>0.28</cell><cell>12.1</cell><cell>TD</cell></row><row><cell>Spanish</cell><cell>1.09</cell><cell>16.9</cell><cell>TD</cell></row><row><cell></cell><cell>0.47</cell><cell>4.90</cell><cell>T</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="5,150.15,410.00,294.21,161.71"><head>Table 9 -</head><label>9</label><figDesc>Table 8 shows our official results for CLEF 2002. In general the results are quite disappointing and much lower in terms of average precision than our CLEF 2001 runs. We believe that our results are on the low side compared with other participants. CLEF 2002 Comparative runs (* run on first stemmer attempt)</figDesc><table coords="5,150.15,410.00,294.21,161.71"><row><cell></cell><cell>Average</cell><cell>Precision</cell><cell>Precision</cell><cell>Query Type</cell></row><row><cell></cell><cell>precision</cell><cell>@5</cell><cell>@10</cell><cell></cell></row><row><cell>German</cell><cell>0.147</cell><cell>0.228</cell><cell>0.210</cell><cell>T</cell></row><row><cell></cell><cell>0.173</cell><cell>0.284</cell><cell>0.254</cell><cell>TD</cell></row><row><cell>French</cell><cell>0.248</cell><cell>0.340</cell><cell>0.298</cell><cell>TD</cell></row><row><cell>Dutch</cell><cell>0.193</cell><cell>0.308</cell><cell>0.282</cell><cell>T</cell></row><row><cell></cell><cell>0.259</cell><cell>0.396</cell><cell>0.346</cell><cell>TD</cell></row><row><cell>Italian</cell><cell>0.266</cell><cell>0.384</cell><cell>0.318</cell><cell>TD</cell></row><row><cell>Spanish</cell><cell>0.217</cell><cell>0.320</cell><cell>0.294</cell><cell>T</cell></row><row><cell></cell><cell>0.255</cell><cell>0.376</cell><cell>0.354</cell><cell>TD</cell></row><row><cell>Finnish</cell><cell>0.123</cell><cell>0.200</cell><cell>0.143</cell><cell>T</cell></row><row><cell></cell><cell>0.171</cell><cell>0.240</cell><cell>0.190</cell><cell>TD</cell></row><row><cell></cell><cell cols="3">Table 8 -CLEF 2002 official runs</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The author is grateful to <rs type="person">Martin Porter</rs> for his efforts to produce a snowball stemmer for Finnish.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="7,87.32,262.66,255.02,8.96;7,342.25,260.47,5.04,5.83;7,349.81,262.66,41.56,8.96" xml:id="b0">
	<monogr>
		<ptr target="http://snowball.sourceforge.net" />
		<title level="m" coord="7,87.32,262.66,72.82,8.96">Snowball web site</title>
		<imprint>
			<date type="published" when="2002-07-19">19 th July 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,84.87,285.69,236.93,8.96;7,70.59,297.21,273.11,8.96;7,343.69,295.03,5.04,5.83;7,351.25,297.21,41.56,8.96" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="7,130.95,285.69,186.78,8.96">Snowball: A language for stemming algorithms</title>
		<author>
			<persName coords=""><forename type="middle">M</forename><surname>Porter</surname></persName>
		</author>
		<ptr target="http://snowball.sourceforge.net/texts/introduction.html" />
		<imprint>
			<date type="published" when="2002-07-19">19 th July 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,86.37,320.25,438.48,8.96;7,70.59,331.65,454.32,8.96;7,70.59,343.17,123.15,8.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,293.85,320.25,82.24,8.96">PLIERS AT TREC</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Macfarlane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Mccann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,126.16,331.65,208.63,8.96">The Eighth Text Retrieval Conference (TREC-8)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting><address><addrLine>Gaithersburg</addrLine></address></meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="241" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,85.28,366.21,439.62,8.96;7,70.59,377.73,149.68,8.96;7,70.59,389.13,423.20,8.96;7,493.80,386.94,6.48,5.83;7,508.44,389.13,16.62,8.96;7,70.59,400.65,22.62,8.96" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="7,245.08,366.21,173.22,8.96">Simple, proven approaches to text retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sparck</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename></persName>
		</author>
		<ptr target="http://www.cl.cam.ac.uk/Research/Reports/TR356-ksj-approaches-to-text-retrieval.html" />
		<imprint>
			<date type="published" when="1997-05">May 1997. July 2002</date>
			<biblScope unit="volume">356</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Cambridge Technical report</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,85.76,423.68,439.17,8.96;7,70.59,435.20,360.82,8.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,145.30,423.68,229.91,8.96">Introduction to information storage and retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Frakes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,107.30,435.20,206.57,8.96">Information retrieval; data structures and algorithms</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Frakes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</editor>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
