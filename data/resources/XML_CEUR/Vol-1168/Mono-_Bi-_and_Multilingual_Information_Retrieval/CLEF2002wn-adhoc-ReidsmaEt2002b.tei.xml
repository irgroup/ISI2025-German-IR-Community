<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,210.02,97.16,175.50,12.63">Comparing search strategies</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,221.54,125.99,63.11,8.96"><forename type="first">Dennis</forename><surname>Reidsma</surname></persName>
							<email>d.reidsma@medialab.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">MediaLab BV</orgName>
								<address>
									<settlement>Schellinkhout</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,291.85,125.99,82.04,8.96"><forename type="first">Peter</forename><surname>Van Der Weerd</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MediaLab BV</orgName>
								<address>
									<settlement>Schellinkhout</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,210.02,97.16,175.50,12.63">Comparing search strategies</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C85BA80EF3C16C13E6939B4A3BAB8EB4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report describes the participation of MediaLab BV in the CLEF-2002 evaluations. This year we participated in the monolingual Dutch task for the second time. Our main objective last year was to get some experience with participating in these experiments and to get a first impression on how our search engine performed compared to other search engines. This year we wanted to experiment with different search strategies and parameterisations on our core search engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Approach</head><p>All experiments were done using the core search engine developed by MediaLab. We had two aims in mind: the first was to experiment with some completely different search strategies, the second was to run those strategies with variations on several parameters in order to investigate the effect on the quality of the results. The next section describes those strategies and parameters in detail. To compare the strategies we used the queries and assessments of last year, evaluating the results using the trec_eval program developed by NIST. Section 3 discusses those results, also comparing them to our performance last year. Section 4 is about our results with the topics for this year and section 5 presents our conclusions and some discussion on what we intend to do next year at CLEF-2003.   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Strategies</head><p>In the working notes of the CLEF-2001 workshop we stated that we would extend our approach with Natural Language Processing and query word weighting for our participation in 2002 <ref type="bibr" coords="1,396.67,464.00,10.68,8.96" target="#b1">[2]</ref>. However, when we started preparing our submission for this year we decided that we should start with deciding on a good base line algorithm that uses only the core functionality of our search engine. For this we investigated two basic strategies, which are discussed in the rest of this section. The first strategy only searches for query words in a collection of simple indexes over the document fields, the second strategy uses co-occurrence indexing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Common parts</head><p>The following is a list of parts that were common to both strategies.</p><p>• A stopword list was obtained by extending the CLEF stopword list for Dutch with our own default stopword collection. • Stemming was done using a default algorithm based on the Porter stemmer.</p><p>• Compound terms were generated from the words in the freetext index: if for instance both "klap", "roos" and "klaproos" are found in the index, "klaproos"is considered a compound term consisting of "roos" and "klap". So, searching for "roos" will also retrieve hits on "klaproos",</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Strategy 1</head><p>The first strategy was very straightforward, since our aim was not to find the best algorithm but to define a base line algorithm that performs well and is easy to understand and analyse. We defined a collection of indexes over the document fields 'TI', 'LE' and 'TE' and combined indexes over every combination of those fields. The next step was to define runs that differed on the following points:</p><p>• Which indexes were searched (at most 3 indexes per run)</p><p>• Which parts of the query were used in searching (title, description and/or narrative). The different indexes in one run could be searched using a different combination of query fields.</p><p>• How strong the influence of keyword stemming should be for the separate indexes in a run. Since we wanted to keep the number of runs small, we used only the settings no stemming, default stemming (relative weight of 0.6), full stemming (same weight as exact matches). Using these parameters, we defined about 500 runs on the topics of last year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Strategy 2</head><p>With the second strategy we tried to improve the results of a run by using co-occurrence information. For this we first defined a co-occurrence network on the document collection. This network then was used to modify the query results in the following way: For every word in the query it's co-occurrences were determined. Combining those co-occurrences for all words with an AND or an OR operation resulted in so called strong query expansions and weak query expansions. Those query expansions could then be used to find more query results or just rerank the results. The assumption was that this procedure might improve recall in the runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results on last year's collection</head><p>All variations on both strategies were tested on the topics of last year. The result files that were produced were analysed using the trec_eval program. This section describes the outcome of that analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Strategy 1</head><p>Table <ref type="table" coords="2,97.94,329.01,4.98,8.96">1</ref> shows a few of the best results we achieved using the first strategy. Given the fact that the average precision for the top 5 participants on that 2001 collection were 0.3917, 0.3844, 0.3775, 0.3497 and 0.2833 <ref type="bibr" coords="2,504.72,340.53,11.71,8.96" target="#b0">[1]</ref> it may be clear that the new strategy improved results drastically. We did not have time to do a proper statistical analysis of the results to determine which values for the different parameters resulted in the best performance. Still, the results seem to indicate a trend that the best results are achieved using the title and description field of the query, leaving the narrative out and giving stemmed variants of the query words a relative weight of 0.6 (so 2 stemmed keyword matches are slightly more important than one exact match). The results produced with the second searching strategy were not as much improved. On the contrary, every variation on this strategy performed worse with than without using the co-occurrence enrichment. With hindsight it is not hard to think of an explanation why this 'had to be unavoidable', although we would have to devise some other experiments to verify that the explanation is the right one. Currently we are inclined to think that since co-occurrence information describes which words occur often together in a document, the co-occurrences of the query words are already bound to be in the top part of the results. That would mean that using this information to expand the results would not result in much improvement in recall, whereas the noise introduced by the expansion would result in the retrieval of more irrelevant results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our results this year</head><p>To produce our results for this year we used several variations on the first strategy that performed not too badly on last year's topics. We decided not to use the second strategy because it produced such poor results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1: Retrieval results on 2001 collection</head><p>Though our best results on the topics of last year were pretty good, the comparisons to the median suggest that we did not perform as well on this year's topics. The difference is large enough to be more than somewhat surprising, but unfortunately we did not have time to find out what caused this behaviour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Our achievements in improving the results for the topics of last year with such a simple strategy gives us confidence that we are on the right track. For next year we might experiment with several extensions to this base line algorithm, such as for example:</p><p>• Blind (negative) relevance feedback on the lowest retrieved results, such as was used last year by McNamee and Mayfielsd among others <ref type="bibr" coords="3,266.87,191.02,10.64,8.96" target="#b2">[3]</ref>. • Weighting the different query words based on word category, corpus frequency and language frequency • More detailed experimenting with the optimal settings for stemming and compound word searching.</p></div>		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="3,85.23,268.78,410.62,8.96" xml:id="b0">
	<monogr>
		<title level="m" coord="3,85.23,268.78,178.67,8.96">Working Notes for the CLEF 2001 Workshop</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Darmstadt, Germany</addrLine></address></meeting>
		<imprint>
			<date>September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,85.23,290.25,408.32,8.96" xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">Wilfred</forename><surname>Clef Peter Van Der Weerd</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Blom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">V</forename><surname>Medialab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="3,411.52,290.25,53.46,8.96">Schellinkhout</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,85.71,306.81,417.15,8.96;3,85.11,318.21,182.32,8.96" xml:id="b2">
	<monogr>
		<title level="m" coord="3,85.71,306.81,417.15,8.96;3,85.11,318.21,34.53,8.96">APL Experiments at CLEF: Translation Resources and Score Normalization, Paul McNamee and James Mayfield</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint/>
		<respStmt>
			<orgName>Johns Hopkins University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
