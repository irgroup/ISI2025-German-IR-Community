<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,242.26,86.15,110.49,12.64">Océ at CLEF 2002</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,239.74,134.94,43.90,9.07"><forename type="first">Roel</forename><surname>Brand</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Océ Technologies</orgName>
								<address>
									<postBox>P.O. Box 101</postBox>
									<postCode>NL-5900</postCode>
									<settlement>MA, Venlo</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,290.66,134.94,64.75,9.07"><forename type="first">Marvin</forename><surname>Brünner</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Océ Technologies</orgName>
								<address>
									<postBox>P.O. Box 101</postBox>
									<postCode>NL-5900</postCode>
									<settlement>MA, Venlo</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,242.26,86.15,110.49,12.64">Océ at CLEF 2002</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">09632F8CD6FAA58236935D3A669C4923</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report describes the work done by the Information Retrieval Group at Océ Technologies B.V., for the 2002 edition of the Cross-Language Evaluation Forum (CLEF). We have participated in the mono, cross and multilingual tasks, using BM25 for ranking, Ergane, Logos and BabelFish for translations and the Knowledge Concepts semantic network for stemming and morphological expansion.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>To enlarge our knowledge and experience with information retrieval in multi-lingual document collections, we again participated in the Cross-Language Evaluation Forum (CLEF) this year. Last year we only participated in the Dutch monolingual task. Our goal for this year was to participate in all of the mono-lingual tasks, some of the cross lingual and in the multi-lingual task. Additionally we wanted to explore methods for combining results from different languages to obtain the best multi-lingual result. This report describes the details of the retrieval system we built to create our contribution and the results we obtained in the contest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Query construction</head><p>From the topics, queries were automatically constructed. Fields from the topics were split into terms on nonalphanumerical characters. Single character or stopword terms were removed. Each term was expanded with its root form by using morphological collapse (dictionary based stemming) from Knowledge Concepts' Content Enabler semantic network The root form was then expanded with the semantic network, with the morphological variants of the root form (such as plural form, etc.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Compound Splitting</head><p>For both Dutch and German, compound words were split using a simple recursive algorithm that takes a word, tries to split a front part that is a complete word, and recurses with the remaining tail of the word. If a word is successfully split into N words, these words are added to the query. For instance the word 'waterbeddenfabrikant' is expanded into 'water bed den fabrikant' and 'water bedden fabrikant'. Sometimes, for pronunciation, an additional 's' is inserted between two parts of the compound. A simple rule in the compound splitter handles these characters. Optionally, related terms or synonyms could be added, but tests show worse results than without.</p><p>For the German task, the semantic network was not working properly; hence a stemmer was used to generate a mapping from a term into its stemming variants <ref type="bibr" coords="1,264.22,683.04,10.66,9.07" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.3</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic translation</head><p>Because we participated in cross and multilingual retrieval, topics written in one language are used to query collections in another or multiple languages. In order to do these tasks, we translated the topics to the language(s) of the collection.</p><p>Experiments have been conducted using the semantic network for word-by-word translation. This however yields very bad results for translation, because a word can point to many concepts and we have no tools to select the proper concept. Therefore the semantic network based translation adds way too many terms, resulting in too broad queries. For instance, the English word 'baby' was translated into Dutch as three concepts with the following terms: 1. Liefje, lieveling, lieverd, schat, schatje, snoesje 2. Baby, dreumes, hummel, jong kind, kind, kleuter, pasgeborene, peuter, puk, uk, wurm, zuigeling 3. Bemoederen, in de watten leggen, koesteren, liefderijk verzorgen, moederen over, vertroetelen, verwekelijken, verwennen Clearly, only a very few terms are correct given the fact that a term in a topic is often used in a specific way. Therefore, this method of translation can not be used in a non-interactive way and hence was abandoned for the CLEF.</p><p>For bilingual tasks we did English to Dutch, Dutch to English, English to Spanish and English to Spanish. The International Training Center of Océ translated English to Spanish using Logos machine translation software. The Internet babelfish translation was used for Spanish to English. For the translation of English to Dutch and vice versa, we used the Ergane dictionaries to translate on a word-by-word base. This translation did not have the problem of query broadening, probably because of its limited amount of words. After translation, the resulting topics were processed similar as the non-translated topics.</p><p>For the multilingual task we used English as the source language and translated it to German, Spanish, Italian and French, with Babelfish and Logos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Indexing</head><p>In the indexes we built for each of the languages, documents were split on non-alphanumerical characters and single character words were ignored. We performed no stemming. For stop word elimination we used the stop list from Knowledge Concepts' Content Enabler semantic network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Ranking</head><p>Instead of using our own model like last year, we based our system on BM25 <ref type="bibr" coords="2,382.81,514.20,11.82,9.07" target="#b1">[2]</ref> this year. Experiments with last year's topics showed that BM25 outperformed our own model. To work with expanded query terms in our implementation of BM25, we summed term frequencies over the expansion of each term and we defined document frequency as the number of documents in which a term or any of its expansions occurs (Equation <ref type="formula" coords="2,503.59,548.58,3.66,9.07">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Result merging</head><p>Due to lack of time we were not able to invest much effort in merging retrieval results from multiple languages into a single multilingual result. We therefore choose two basic approaches: -round robin over all languages in the order es, fr, it, de, en ; -merge sort based on document scores, normalised by dividing them by the score of the highest ranked document of the same language. </p><formula xml:id="formula_0" coords="3,71.26,98.99,236.39,228.87">d ndl(d) d q tf d ndl b b k k d q tf q df N Q d q q df d q tf d q tf q q d q tf q q q q q q Q q Q q i i i j j i i j j i i i j i j i i i i n i i i i i ¦ ¦ ∈ + ⋅ + - ⋅ + ⋅ ⋅ - = = = = 3 Activities</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">BM25 Parameters</head><p>The performance of the BM25 ranking algorithm depends greatly on the choice for the values of the parameters k1 and b. Using the CLEF 2001 relevance assessments, we have executed a brute-force search over the parameter space for Dutch, English and Spanish. In Appendix A, plots show the average precision measure as a function of k1 and b. Based on this, we chose to use the parameter values as shown in Table <ref type="table" coords="3,451.37,411.42,5.04,9.07" target="#tab_1">1</ref> for our runs. Of course we are not sure that the parameter values we found are indeed language dependent. They might be strongly dependent on the document collections used in CLEF, on the way in which we split terms or on something else. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Different parts of the topic</head><p>We also experimented with using different parts of the topics generate query terms from. Listing 1 shows that each topic consists of three parts: a title, a description and a narrative. We experimented with building queries from the title only, title+description and title+description+narrative. We found that we got the best results using title+description. Probably because using title only yields too few query terms and using all parts yields too many irrelevant terms. We did not test the influence of choosing different settings for the BM25 parameters with using different parts of the topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Synonym expansion</head><p>As we described in section 2, we used the Knowledge Concepts semantic network for term expansion during query construction. We found that adding synonyms of query terms to their expansions only resulted in poorer average precision. Looking at the kinds of synonyms that are added we suspect that the problem lies in failing to select the right meaning for a term before adding synonyms and thereby adding many irrelevant terms. For instance, when adding synonyms for the term baby, we could choose to add the nouns child and infant or the verbs to nurse and to care.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Logos vs. Babelfish</head><p>In the cross lingual task, we experimented with Logos and Babelfish for translating English topics into Spanish and Spanish topics into English. Of these, Logos yielded the better translations and also the better average query results. For translating topics between English and Dutch, we used Ergane, which translates via Esperanto. With this, translations were not very good and neither were query results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Official runs</head><p>This year we submitted the following official runs: -Mono lingual Dutch, German, Spanish, Italian and French, queries built from title only and title+description; -Cross lingual Spanish to English, English to Spanish, topic translation with Logos and Babelfish; -Cross lingual Dutch to English and English to Dutch, topic translation with Ergane; -Multi lingual from English topics, result merging using round robin and merge sort, topic translation with Logos and Babelfish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results for the Clef 2002</head><p>This section presents the improvement made, compared to the run we submitted in 2001. It also presents the results obtained on the monolingual tasks of 2001, compared with the groups that participated that year. The last part of this section presents the comparison of our runs to the median of all runs submitted this year. Only after the conference, a better comparison with the other participants will be possible.   <ref type="figure" coords="5,99.54,72.84,5.04,9.07" target="#fig_0">1</ref> presents the improvement made on the average precision per topic for the Dutch monolingual task. This considerable improvement has been reached primarily by abandoning the model described in <ref type="bibr" coords="5,460.22,84.36,11.83,9.07" target="#b0">[1]</ref> and using a classical BM25 algorithm instead. Additionally, using morphological tools from Knowledge Concepts and our own compound splitter yielded quite a lot too. The five tables above give an indication where we would have ended in the rank for performing the monolingual tasks last year, with our current algorithms. These figures will be available for 2002 after the conference in September 2002.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dutch Monolingual</head><note type="other">Participant</note><p>Statistics we can produce now are the comparison with the median of all submitted runs. It is important to note that a bilingual run, for instance English to Spanish will be compared to all bilingual runs with Spanish as the document collection language. These are presented in Figure <ref type="figure" coords="5,316.58,725.70,3.78,9.07">2</ref>.  Based on the comparison with the median, there is not really much to estimate on how well we currently do, compared to others. It is also true that we did not participate with a revolutionary new information retrieval model, but with a proven approach. A common technique we did not use in our system is Blind Relevance Feedback. This method extracts from the top-n documents retrieved from the initial query terms, and adds them to the original query. This query is then executed, and these retrieved documents are the final ranking. Literature shows that it helps to improve the mean average precision, but we did not manage to implement it.</p><p>It is clear that we have participated in many more runs than last year, that we have improved our retrieval algorithms, made a more serious implementation of the indexer and ranker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Our goals for taking part in the CLEF this year were to enter official runs for all tasks and to explore techniques for merging results from different languages. Due to time constraints, we succeeded in doing the former but not the latter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Parameter optimisation</head><p>Dutch: maximum at k1=0.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,70.90,729.66,373.43,9.07"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Dutch monolingual task 2001, improvement by using 2002 retrieval algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,70.90,72.84,453.38,9.07;5,70.90,84.36,453.38,9.07;5,70.90,95.88,453.46,9.07;5,70.90,107.22,184.51,9.07"><head>Figure</head><label></label><figDesc>Figure1presents the improvement made on the average precision per topic for the Dutch monolingual task. This considerable improvement has been reached primarily by abandoning the model described in<ref type="bibr" coords="5,460.22,84.36,11.83,9.07" target="#b0">[1]</ref> and using a classical BM25 algorithm instead. Additionally, using morphological tools from Knowledge Concepts and our own compound splitter yielded quite a lot too.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,70.90,73.20,435.96,220.73"><head>Equation 1: calculating tf and df for expanded terms, and the total score for a document given a query</head><label></label><figDesc></figDesc><table coords="3,73.42,98.99,333.68,194.94"><row><cell>Let</cell><cell></cell><cell></cell><cell cols="3">be</cell><cell>a</cell><cell cols="8">query term</cell><cell cols="2">in</cell><cell>query</cell></row><row><cell>Let</cell><cell></cell><cell>,</cell><cell>0</cell><cell>,</cell><cell></cell><cell>,</cell><cell>1</cell><cell cols="2">,..,</cell><cell>,</cell><cell></cell><cell cols="2">be</cell><cell cols="3">the</cell><cell>expansion</cell><cell>of</cell><cell>in which</cell><cell>,</cell><cell>0</cell></row><row><cell>Let</cell><cell></cell><cell cols="2">(</cell><cell cols="2">,</cell><cell cols="2">,</cell><cell>)</cell><cell cols="2">be</cell><cell cols="2">the</cell><cell cols="5">term</cell><cell>frequency</cell><cell>of</cell><cell>expansion</cell><cell>term</cell><cell>,</cell></row><row><cell>We</cell><cell cols="4">now</cell><cell cols="7">calculate</cell><cell cols="2">the</cell><cell cols="4">document</cell><cell>and</cell><cell>term</cell><cell>frequency</cell><cell>of</cell><cell>as</cell><cell>follows</cell><cell>:</cell></row><row><cell>(</cell><cell>,</cell><cell></cell><cell></cell><cell>)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(</cell><cell>,</cell><cell>,</cell><cell></cell><cell>)</cell><cell></cell></row><row><cell>(</cell><cell></cell><cell cols="2">)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">set</cell><cell cols="2">of</cell><cell cols="6">documents</cell><cell>in which</cell><cell>occurs</cell></row><row><cell cols="2">Then</cell><cell cols="3">for</cell><cell></cell><cell>a</cell><cell cols="7">Document</cell><cell cols="2">d,</cell><cell cols="2">and</cell><cell>Query</cell><cell>Q,</cell><cell>the</cell><cell>score</cell><cell>is</cell><cell>calculated</cell><cell>as</cell><cell>:</cell></row><row><cell>Rel</cell><cell>(</cell><cell></cell><cell>,</cell><cell></cell><cell></cell><cell>)</cell><cell></cell><cell></cell><cell></cell><cell cols="3">1 log(</cell><cell></cell><cell cols="3">1 (( )</cell><cell>) log(</cell><cell>(</cell><cell>(</cell><cell>))</cell><cell>(</cell><cell>))) (</cell><cell>,</cell><cell>)</cell><cell>(</cell><cell>(</cell><cell>) , , ) 1 1</cell></row><row><cell cols="5">In which</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">is</cell><cell cols="2">the</cell><cell cols="4">document</cell><cell>length</cell><cell>of</cell><cell>,</cell><cell>divided</cell><cell>by the</cell><cell>average</cell><cell>document</cell><cell>length</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,204.10,468.84,131.72,96.55"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="3,211.30,468.84,124.52,96.55"><row><cell cols="3">: BM25 parameter values</cell></row><row><cell></cell><cell>k1</cell><cell>b</cell></row><row><cell>Dutch</cell><cell>1.2</cell><cell>0.6</cell></row><row><cell>German</cell><cell>1.2</cell><cell>0.6</cell></row><row><cell>English</cell><cell>2.0</cell><cell>0.75</cell></row><row><cell>Spanish</cell><cell>1.4</cell><cell>0.5</cell></row><row><cell>Italian</cell><cell>1.4</cell><cell>0.5</cell></row><row><cell>French</cell><cell>1.4</cell><cell>0.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,70.90,264.76,450.69,403.95"><head>Percentage of topics on or above the median, for each submitted run</head><label></label><figDesc></figDesc><table coords="6,70.90,264.76,450.69,403.95"><row><cell>oce02monITto</cell><cell>oce02monIT</cell><cell>oce02monFRto</cell><cell>oce02monFR</cell><cell>oce02monESto</cell><cell>oce02monES</cell><cell>oce02monDEto</cell><cell>oce02monDE</cell><cell>oce02nl2enER</cell><cell>oce02en2nlER</cell><cell>oce02es2enBF</cell><cell>oce02es2enLO</cell><cell>oce02en2esBF</cell><cell>oce02en2esLO</cell><cell>oce02mulRRloTO</cell><cell>oce02mulRRbf</cell><cell>oce02mulRRlo</cell><cell>oce02mulMSbf</cell><cell>oce02mulMSlo</cell></row><row><cell>Figure 2: Run Name</cell><cell cols="3">Description</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>oce02monNLto</cell><cell cols="8">Monolingual Dutch Topic Title Only</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>oce02monNL</cell><cell cols="9">Monolingual Dutch Topic Title + Description</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>oce02monITto</cell><cell cols="8">Monolingual Italian Topic Title Only</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>oce02monIT</cell><cell cols="9">Monolingual Italian Topic Title + Description</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>oce02monFRto</cell><cell cols="8">Monolingual French Topic Title Only</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>oce02monFR</cell><cell cols="10">Monolingual French Topic Title + Description</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>oce02monESto</cell><cell cols="8">Monolingual Spanish Topic Title Only</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>oce02monES</cell><cell cols="10">Monolingual Spanish Topic Title + Description</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>oce02monDEto</cell><cell cols="8">Monolingual German Topic Title Only</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>oce02monDE</cell><cell cols="10">Monolingual German Topic Title + Description</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>oce02nl2enER</cell><cell cols="15">Bilingual Dutch to English Ergane Dictionary Topic Title + Description</cell><cell></cell><cell></cell><cell></cell></row><row><cell>oce02en2nlER</cell><cell cols="15">Bilingual English to Dutch Ergane Dictionary Topic Title + Description</cell><cell></cell><cell></cell><cell></cell></row><row><cell>oce02es2enBF</cell><cell cols="16">Bilingual Spanish to English Babelfish Translation Topic Title + Description</cell><cell></cell><cell></cell></row><row><cell>oce02es2enLO</cell><cell cols="15">Bilingual Spanish to English Logos Translation Topic Title + Description</cell><cell></cell><cell></cell><cell></cell></row><row><cell>oce02en2esBF</cell><cell cols="16">Bilingual English to Spanish Babelfish Translation Topic Title + Description</cell><cell></cell><cell></cell></row><row><cell>oce02en2esLO</cell><cell cols="15">Bilingual English to Spanish Logos Translation Topic Title + Description</cell><cell></cell><cell></cell><cell></cell></row><row><cell>oce02mulRRloTO</cell><cell cols="13">Multilingual English Round Robin Logos Translation Topic Title</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>oce02mulRRbf</cell><cell cols="17">Multilingual English Round Robin Babelfish Translation Topic Title + Description</cell><cell></cell></row><row><cell>oce02mulRRlo</cell><cell cols="16">Multilingual English Round Robin Logos Translation Topic Title + Description</cell><cell></cell><cell></cell></row><row><cell>oce02mulMSbf</cell><cell cols="16">Multilingual English Merge Sort Babelfish Translation Topic Title + Description</cell><cell></cell><cell></cell></row><row><cell>oce02mulMSlo</cell><cell cols="16">Multilingual English Merge Sort Logos Translation Topic Title + Description</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,106.36,201.90,417.88,9.07;7,106.36,213.42,221.76,9.07" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,302.38,201.90,221.86,9.07;7,106.36,213.42,24.75,9.07">Océ retrieval engine at the Cross-Language Retrieval Forum</title>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><surname>Klok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marvin</forename><surname>Brünner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samuel</forename><surname>Driessen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="7,138.58,213.42,143.36,9.07">Lecture Notes on Computer Science</title>
		<imprint/>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="7,106.36,224.94,417.93,9.07;7,106.36,236.46,159.32,9.07" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="7,219.76,224.94,172.33,9.07">Simple proven approaches to text retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Jones</surname></persName>
		</author>
		<idno>TR356</idno>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
		<respStmt>
			<orgName>Cambridge University Computer Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct coords="7,106.36,247.80,340.31,9.07" xml:id="b2">
	<monogr>
		<ptr target="http://snowball.sourceforge.net/german/stemmer.html" />
		<title level="m" coord="7,106.36,247.80,117.98,9.07">German Stemming Algorithm</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
