<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,218.58,123.13,64.56,12.19;1,303.14,123.13,73.51,12.19;1,164.82,139.21,265.56,12.19">UTACLIR</title>
				<funder ref="#_53mxvdc">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,181.26,176.97,38.35,8.74"><forename type="first">Eija</forename><surname>Airio</surname></persName>
							<email>eija.airio@uta.fi</email>
						</author>
						<author>
							<persName coords="1,226.03,176.97,72.41,8.74"><forename type="first">Heikki</forename><surname>Keskustalo</surname></persName>
							<email>heikki.keskustalo@uta.fi</email>
						</author>
						<author>
							<persName coords="1,305.19,176.97,57.05,8.74"><forename type="first">Turid</forename><surname>Hedlund</surname></persName>
							<email>turid.hedlund@shh.fi</email>
						</author>
						<author>
							<persName coords="1,369.33,176.97,44.76,8.74"><forename type="first">Ari</forename><surname>Pirkola</surname></persName>
							<email>pirkola@tukki.jyu.fi</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Tampere</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Studies</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,218.58,123.13,64.56,12.19;1,303.14,123.13,73.51,12.19;1,164.82,139.21,265.56,12.19">UTACLIR</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5A1CE102F6D802F3B9449E38D15729B5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The UTACLIR query translation system was originally designed for the CLEF 2000 and 2001 campaigns. In the two first years the query translation application consisted of separate programs based on common translation principles for the language pairs Finnish -English, German -English and Swedish -English. The idea of UTACLIR is based on recognizing distinct source key types and processing them accordingly. The linguistic resources utilized by the framework include morphological analysis or stemming in indexing, stop word removal, normalization of topic words, splitting of compounds written together, handling of non-translated words, phrase composition of compounds in the target language, bilingual dictionaries and structured queries. This year we participated in CLEF with the new UTACLIR system, which is a single program unified for all the languages. The user gives the system the codes of source and target language as well as the query to be translated. The UTACLIR system chooses the language resources upon the codes the user has given. A morphological analyser is used to process the source language words in order to match the words in the translation dictionary. We have utilized an 18-language dictionary (with all possible language pairs) as the main translation resource of the UTACLIR. It is also possible to implement other parallel dictionaries. For the target language it is possible to use either a morphological analyser or a stemmer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>University of Tampere has participated in the bilingual tasks of CLEF years 2000 and 2001 utilizing the UTACLIR process. UTACLIR has consisted of separate, but similar kind of programs for the language pairs Finnish -English, German -English and Swedish -English. The idea of UTACLIR is based on translating topic words one by one, and then combining the translations into the query.</p><p>The source word processing can be described in general level as follows. First the topic words are normalized with a morphological analyser, if possible, and after that source stop words are removed. Then translation is attempted. Translated words are normalized, because it is possible that a dictionary returns words in inflected form (e.g. "United States"). Finally the target stop word removal is done. Normalized translation variants are enveloped with a synonym operator and added to the query. The untranslatable words are mostly proper names and technical terms. Typically words like these are spelling variants of each other in different languages, which allows the use of approximate string matching techniques. These techniques are language-independent. <ref type="bibr" coords="1,492.26,618.57,32.23,8.74;1,70.92,630.03,49.06,8.74">(Pirkola &amp; al. 2002.)</ref> The best matching strings are searched from the target index. These are enveloped with a synonym operator and added to the query. UTACLIR has a special procedure for untranslatable compounds written together. They are first splitted into their constituents and then translated separately. Translated parts are enveloped with a proximity operator. <ref type="bibr" coords="1,70.92,687.51,88.69,8.74" target="#b1">(Hedlund &amp; al. 2001.)</ref> Structuring of queries using the synonym operator, which means grouping of the target words derived from the same source word into the same facet, is applied in the UTACLIR system. This has proved to be an effective strategy in CLIR by earlier studies <ref type="bibr" coords="1,211.57,733.53,89.89,8.74">(Pirkola 1998, 60 -61)</ref>.</p><p>This year we participated in the Finnish monolingual task, the English -Finnish, English -French and English -Dutch bilingual tasks, and the multilingual task. The monolingual task is a traditional retrieval task, the only novelty being the language, which is not the traditionally used language, English. Finnish is introduced as a target language in CLEF 2002. The bilingual task adds the topic translation to the previous one, as well as some extra problems, for example the problem of non-translatable proper names. The multilingual task involves the result merging phase in addition to the previous one, if the most usual approach, building the separate indexes for all the languages, is followed. There are at least three possible ways to merge the results. The simplest of them is the Round Robin approach, which means that a line of every result set is taken, one by one from each, until there are as many lines as needed. This is based on the fact that the distribution of relevant documents in the lists is not known, because the scores are not comparable, and there is no way to compare them. The second approach is the raw score approach, which assumes that document scores are comparable across separate collections. The third is the rank based approach. It bases on the fact that the relationship between probability of relevance and the log of the rank of a document can be approximated by a linear function. Merging can subsequently be based on the estimated probability of relevance. Actual score of a document is then applied only to rank documents, but the merging is based on the rank, not on the score. <ref type="bibr" coords="2,214.90,257.49,110.73,8.74">(Hiemstra &amp; al. 2001, 108.)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The new UTACLIR process</head><p>This year we have a new unified version of UTACLIR in use. The basic process is the same for all the source and target languages. As an input for UTACLIR system the user gives the codes expressing the source and target language, and the source language query. Depending on the codes the system uses external linguistic resources: bilingual dictionaries, morphological analysers, source and target stop lists, and stemmers. The new UTACLIR system has the same basic elements as the old one (see Figure <ref type="figure" coords="2,437.17,374.79,3.62,8.74">1</ref>). The source word processing has not changed, but there are new features in the target word processing. If translation variants are found, either a morphological analyser or a stemmer is utilized, depending on the index type of the target language. The stemmer produces ready components for the target query, in which case stop word removal is not done. However in case a morphological analyser is used to process the target words, stop word removal is done. Stop words are in a morphologically analysed form, and cannot be utilized in the stop word removal of stemmed target words.</p><p>The compound splitting procedure was not yet implemented in UTACLIR during CLEF 2002 runs.</p><p>It is possible to use input codes for denoting parallel resources in the new UTACLIR system. In that case the input codes denote not only the source and target language, but also the resource used. If we have for example three different English -Finnish bilingual dictionaries in use, we can easily test their performance with UTACLIR. The source words must be processed by a morphological analyser, not by a stemmer. There is no sense to stem source words, because we do not have dictionaries for stemmed source words at the moment.</p><p>The UTACLIR system constructs a three level tree data structure from the source query: 1) Original source keys given by the user; 2) Processed source language strings, for example processed by morphological analysers; 3) Post-processed word-by-word translations. The tree can be traversed and interpreted in different ways, and the final translated query can be constructed by interpreting the tree. <ref type="bibr" coords="2,331.80,593.25,123.09,8.74">(Hedlund &amp; al 2002a, 16 -17.)</ref> Figure <ref type="figure" coords="3,101.76,424.59,3.77,8.74">1</ref>. An overview of processing a word in the new UTACLIR process.</p><p>(*) Depending on the target language, either morphological analysis or stemming was performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Runs and results</head><p>In this chapter, we first describe the language resources used, then the collections, and the indexing strategy adapted. Finally, we report results of the monolingual, bilingual and multilingual runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language resources</head><p>• Motcom GlobalDix multilingual translation dictionary (18 languages, total number of words 665 000) by Kielikone plc. Finland • Motcom English -Finnish bilingual translation dictionary (110 000 entries) by Kielikone plc. Finland • Morphological analysers FINTWOL, GERTWOL and ENGTWOL by Lingsoft plc. Finland • Stemmers for Spanish and French, by ZPrise • A stemmer for Italian, by the Univeristy of Neuchatel • English stop word list, created on the basis of InQuery's default stop list for English • Finnish stop word list, created on the basis of the English stop list • German stop word list, created on the basis of the English stop list • French stop word list, granted by Université de Provence • Italian stop word list, granted by University of Alberta • Spanish stop word list, InQuery's default stop list for Spanish</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test collections</head><p>The following test collections were used for the tests: English LA Times, Finnish Aamulehti, French Le Monde, French SDA, German Der Spiegel, German SDA, Italian La Stampa, Italian SDA and Spanish EFE. We had to exclude German Frankfurter Rundschau because of indexing problems. Next, the indexing of the databases is described.</p><p>Lingsoft's morphological analyser FINTWOL was utilized in indexing the Finnish dataset, and GERTWOL in indexing the German datasets. As we did not have morphological analysers for Spanish, Italian and French, we decided to index those databases by utilizing stemmers. We used Zprise's Spanish stemmer, Zprise's French stemmer and the Italian stemmer granted by the Univeristy of Neuchatel.</p><p>We built separate index for every dataset instead of indexing by language, for example separate indexes for Le Monde and French SDA. Thus, we had eight separate indexes instead of five. This choice has an impact on merging phase, and also affects n-gramming. We will discuss these aspects later in this paper.</p><p>The InQuery system, provided by the Center for Intelligent Information Retrieval at the University of Massachusetts, was utilized in indexing the databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monolingual runs</head><p>We made two monolingual runs, both in Finnish. The approach of these runs was similar to our bilingual runs, only excluding translation (see Figure <ref type="figure" coords="4,236.52,337.95,3.61,8.74" target="#fig_0">2</ref>). In the first run topic words are normalized by using Lingsoft's morphological analyser FINTWOL. Compounds written together are splitted into their constituents. If a word is recognized by FINTWOL, it is checked against the stop word list, and the result (the normalized word, or nothing in the case of stop word) is processed further. If the word is not recognized, it is n-grammed. The n-gram function compares the word with the database index contents. It returns the best match form among morphologically recognized index words and the best match form among non-recognized index words, and combines them with InQuery's synonym operator (#syn operator, see <ref type="bibr" coords="4,350.84,406.95,114.00,8.74" target="#b5">Kekäläinen &amp; Järvelin 1998)</ref>.</p><p>The second monolingual Finnish run is similar to the first one, but no n-gramming is done. Unrecognised words are added to the query as such. There was no big difference in performance between the results of our two Finnish monolingual runs. Finnish is a language rich in compounds written together. Parts of a compound are often content bearing words. <ref type="bibr" coords="5,70.92,96.51,94.24,8.74">(Hedlund &amp; al. 2002b.)</ref> In a monolingual run it is reasonable to split a compound into its components, normalize the components separately, and envelope the normalized components with an appropriate operator. In the original run, we used the synonym operator in the monolingual runs for this purpose instead of the proximity operator, which turned out to be not a good approach. For example topic 140 contains the word matkapuhelin (mobile phone). The query constructed for this topic contains a synonym clause #syn(matka puhelin), which means, that occurrences of the word matka (travel) or puhelin (phone) are allowed, instead of a phrase "matka puhelin".</p><p>We made an additional run in order to get a more precise view of the effect of the synonym operator in the compounds compared with the proximity operator. There, we replaced the synonym operator with the InQuery's #uw3 operator (proximity with the window size 3) in the cases of compounds. We compared these new results to the corresponding results of our CLEF runs (see table <ref type="table" coords="5,295.41,222.99,3.61,8.74" target="#tab_0">1</ref>). Average precision of this additional run was 30.4 % better in the run using n-grams, and 33.3 % better in the run with no n-grams. We can conclude, that demanding of all the parts of the compound to occur in the document is essential to get better results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Common features of bilingual and multilingual runs</head><p>Handling of source words by ENGTWOL and the processing of source language stop words were similar in all the bilingual and multilingual runs we made, because we used only English as a source language in all these. GlobalDix dictionary by Kielikone was utilized in all the translations.</p><p>We had a beta-version of UTACLIR in use during the CLEF-runs. There were some deficiencies compared to the old version, because all the features of UTACLIR were not yet implemented in the new one. Splitting of compounds was not yet implemented, and non-translated words were handled (using the n-gram method) only in German as a target language. We did not utilize target stop word removal in the case of stemming. Our stop word lists consist of morphologically normalized words at the moment, thus they cannot be used as such to remove the stemmed forms.</p><p>The n-gramming functions must be applied separately for each target index. Because we have two distinct indexes in German, French and Italian, we should make eight n-gramming functions. Due to time limitations we made the function only for the German SDA index, and utilized the same with the Der Spiegel index. We excluded n-gramming in other cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bilingual runs</head><p>We made this year three bilingual runs: English -Finnish, English -Dutch and English -French. The English -Dutch run is not reported because of a severe failure in the indexing of the Dutch database. The result of English -French run was utilized also in the multilingual run.</p><p>In the English -Finnish run, FINTWOL was used for normalizing the target words. Target language stop word removal was done after the translation and normalization processes. In the English -French run the stemming approach was used for normalizing the target words in these runs. The French databases were indexed using the stemmer, correspondingly.</p><p>The result of the English -Finnish run is in the table 2. The obvious reasons for the quite poor performance of the run would be the defective testing of UTACLIR, and absence of gramming and compound handling. The translations given by the GlobalDix, which were sometimes curious, were doubted to have an impact on the result. We made additional English -Finnish runs to clarify the effect of the dictionary on the result. First we made a run where the untranslatable words were added to the query in two forms: as such and preceded by the character "@" (unrecognised words are preceded by "@" in the index). The average precision was 24.6 %, 21.8 % better than the CLEF run (Table <ref type="table" coords="6,227.78,199.95,3.61,8.74" target="#tab_1">2</ref>). The second comparable run was done utilizing another translation dictionary, MOT with 110 000 Finnish -English entries (compared to 26 000 entries of GlobalDix). The result was 61.4 % better than the original CLEF result. The both dictionaries are from the same producer, Kielikone plc. As we did not have an alternative English -French -dictionary to translate from English to French, we could not compare the effect of the dictionary on the results. However, some considerations can be done examining the topic translations. The GlobalDix dictionary seems to return some odd translations. As an example, topic 101 deals with Cyprus. The proper name "Cyprus" is translated to the French word "cyprè", which means a cypress in English. The right translation would be "Chypre". Also untranslatable proper names cause problems in retrieval. For example, topic number 94 achieved a poor result, because it includes the proper name "Solzenitsyn", which does not exist as such in the French dataset: the French layout is "Soljenitsyne". Better results will presumably be achieved with the French n-gramming function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multilingual runs</head><p>University of Tampere participated for the first time in the multilingual task this year. The main goal was to gain experience for developing a general query translation framework.</p><p>The topics were in English, so the beginning of the process was similar in every language: topic words were normalized using ENGTWOL and after that the source stop words were removed. TheGlobalDix dictionary was used to translate normalized source words to the target languages. As we have a morphological analyser for German, GERTWOL by Lingsoft, it was used for normalizing the target words. For Spanish, French and Italian we had no morphological analysers, thus we chose to utilize stemmers instead. We used ZPrise's Spanish and French stemmers, and the Italian stemmer of the Univeristy of Neuchatel. Target stop word removal was done only for morphologically analysed target queries (so it was done only in the German run).</p><p>There are several different strategies to merge the results obtained from distinct databases. In the first run we applied merging method described by Voorhees and others: treating the similarity values across the collections as they were comparable, and selecting 1000 greatest similarities across all collections <ref type="bibr" coords="6,419.10,685.41,97.41,8.74">(Voorhees &amp; al 1995, 96</ref>). It's obvious that the similarity values are not comparable in all the cases, but we chose this approach because of its simplicity.</p><p>Our second multilingual run was similar to the first one, except that a different merging strategy was applied. This was the Round Robin approach: from every result set one line was taken by turn, beginning from the top.</p><p>As described in the chapter dealing with databases earlier, we made distinct indexes for all the data sets. So we have eight indexes: one English, one Spanish, two French, two Italian and two German, which means, that we have eight result sets to merge, too. When we have distinct result sets for every data set, we in a way favour the languages which have more than one dataset: French, Italian and German. Whether this is good or not depends on the topic.</p><p>We calculated the average precision for the bilingual subtasks present in the multilingual task. The average precision for the English run was 47.6 %, English -French 23.9 %, English -German 13.5 %, English -Italian 20.1 %, and English -Spanish 21.8 %. The absence of one German dataset affects the poor result of the English -German run. Implementing the Italian and Spanish dictionaries was not ready when making the runs. We can expect better result with those languages after some development of UTACLIR.</p><p>The average precision of our multilingual run with raw score merging method was 16.4 %, and with the Round Robin method 11.7 %. We have not tested any other merging methods, but probably it would be possible to achieve better results with a more developed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion and conclusion</head><p>Cross-lingual information retrieval has become a significant part of information retrieval research last years, driven mostly by the growth of Internet documents and users. The ultimate goal of cross-lingual information retrieval research is to achieve a situation, where the user can retrieve documents in any language typing a single search topic in one language.</p><p>Internet indexes are enormous fusions of documents around the world, written in multiple languages. Internet is too large and too unstable to be used as a test environment. The CLEF test data offer suitable possibilities for interpreting bilingual and multilingual retrieval in an environment simulating real retrieval. The bilingual CLEF task is simple: translating the topics to the target language, or translating the documents to the topic language, and performing the retrieval. The multilingual task includes an extra problem compared to the bilingual task: what to do with the distinct datasets? Most of CLEF participants build distinct indexes for the different languages and then merge the results. Actually this approach differs from that of Internet. If we want to simulate Internet, merging the indexes would be reasonable, not merging the results. The idea of merging the indexes was introduced by Chen in CLEF 2001, as well as an idea of translating the documents and building a monolingual index <ref type="bibr" coords="7,96.25,455.25,49.07,8.74" target="#b0">(Chen 2001)</ref>. In addition that result merging differs from the Internet approach it is an obvious source of errors <ref type="bibr" coords="7,96.77,466.77,57.14,8.74">(Nie 2002, 11)</ref>.</p><p>It is possible to merge the indexes of different languages, and preserve the language information as well. It can be done for example so that English index words get language code "_e": "chair_e". <ref type="bibr" coords="7,410.76,501.27,57.11,8.74">(Nie 2002, 12)</ref>. This method helps in recognizing the languages, but still differs from the real situation in Internet.</p><p>We are participating the multilingual task first time this year, and our approach is the most usual: merging the results, not indexes. Our main goal in CLEF is to test the new unified UTACLIR system this year. The questions of index building alternatives was not current for us, but in future we may address this topic.</p><p>We learnt many important points in the CLEF process this year. Our Finnish monolingual runs repeated the fact, that using a proximity operator instead of the synonym operator with phrases improves the result remarkably.</p><p>The English -Finnish runs with different translation dictionaries revealed the significance of the dictionary for the result. In general, our multilingual runs prove that a unified process for different languages is possible. The CLEF runs raised many interesting questions concerning the development of UTACLIR. Should we develop a dictionary for stemmed words? If so, we could utilize UTACLIR process with stemmed source languages, without demanding the morphological analyser. Would it be reasonable to construct stemmed stop list? Then we could have the target stop word removal with stemmed target languages as well. A further issue is, what is the implication of result merging on the multilingual run result? Would it be possible to do without merging?</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,70.92,729.15,362.94,8.74;4,151.86,484.86,291.73,218.70"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. An overview of processing a word in the monolingual run utilizing n-gramming.</figDesc><graphic coords="4,151.86,484.86,291.73,218.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="3,70.92,70.92,452.82,339.60"><head></head><label></label><figDesc></figDesc><graphic coords="3,70.92,70.92,452.82,339.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,70.92,280.59,421.52,126.16"><head>Table 1 . Average precision for Finnish monolingual runs using synonym and uw3 operator</head><label>1</label><figDesc></figDesc><table coords="5,70.92,303.99,412.41,102.76"><row><cell></cell><cell>Average precision %</cell><cell>Difference % units</cell><cell>Difference %</cell></row><row><cell>Gramming and</cell><cell>27.0</cell><cell></cell><cell></cell></row><row><cell>Synonym operator</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Gramming and</cell><cell>35.2</cell><cell>+8.2</cell><cell>+30.4</cell></row><row><cell>uw3 operator</cell><cell></cell><cell></cell><cell></cell></row><row><cell>No gramming,</cell><cell>24.0</cell><cell></cell><cell></cell></row><row><cell>Synonym operator</cell><cell></cell><cell></cell><cell></cell></row><row><cell>No gramming,</cell><cell>32.0</cell><cell>+8.0</cell><cell>+33.3</cell></row><row><cell>uw3 operator</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,70.92,269.07,418.75,102.70"><head>Table 2 . Average precision for English -Finnish bilingual runs using alternative resources</head><label>2</label><figDesc></figDesc><table coords="6,70.92,292.53,412.41,79.24"><row><cell></cell><cell>Average precision %</cell><cell>Difference % units</cell><cell>Difference %</cell></row><row><cell>GlobalDix + no mark-up</cell><cell>20.2</cell><cell></cell><cell></cell></row><row><cell>of unrecognised words</cell><cell></cell><cell></cell><cell></cell></row><row><cell>GlobalDix + mark-up of</cell><cell>24.6</cell><cell>+4.4</cell><cell>+21.8</cell></row><row><cell>unrecognised words</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MOT + mark-up of</cell><cell>32.6</cell><cell>+12.4</cell><cell>+61.4</cell></row><row><cell>unrecognised words</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The InQuery search engine was provided by the <rs type="institution">Center for Intelligent Information Retrieval at the University of Massachusetts</rs>.</p></div>
<div><head>ENGTWOL (Morphological</head><p><rs type="projectName">Transducer Lexicon Description</rs> of English): Copyright (c) 1989-1992 <rs type="person">Atro Voutilainen</rs> and <rs type="person">Juha Heikkilä</rs>. FINTWOL (Morphological Description of Finnish): Copyright (c) <rs type="person">Kimmo Koskenniemi</rs> and Lingsoft plc. 1983-1993. GERTWOL (Morphological Transducer Lexicon Description of German): Copyright (c) 1997 Kimmo Koskenniemi and Lingsoft plc. TWOL-R (Run-time Two-Level Program): Copyright (c) <rs type="person">Kimmo Koskenniemi</rs> and Lingsoft plc. 1983-1992. GlobalDix Dictionary Software was used for automatic word-by-word translations. Copyright (c) 1998 Kielikone plc, Finland. MOT Dictionary Software was used for automatic word-by-word translations. Copyright (c) 1998 Kielikone plc, Finland.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_53mxvdc">
					<orgName type="project" subtype="full">Transducer Lexicon Description</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,70.92,319.59,432.16,8.74;8,70.92,331.05,371.41,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,134.27,319.59,274.35,8.74">Multilingual information retrieval using English and Chinese queries</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="http://www.ercim.org/publication/ws-proceedings/CLEF2/a-chen.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="8,414.98,319.59,88.10,8.74;8,70.92,331.05,86.74,8.74">Working notes for the CLEF 2001 workshop</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,70.92,354.09,442.25,8.74;8,70.92,365.55,439.84,8.74;8,70.92,377.07,282.57,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,350.37,354.09,162.80,8.74;8,70.92,365.55,250.76,8.74">UTACLIR @ CLEF 2001: New features for handling compound words and untranslatable proper names</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hedlund</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Keskustalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pirkola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Airio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</author>
		<ptr target="http://www.ercim.org/publication/ws-proceedings/CLEF2/hedlund.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="8,328.84,365.55,177.26,8.74">Working notes for the CLEF 2001 workshop</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,70.92,400.05,446.53,8.74;8,70.92,411.57,409.44,8.74;8,70.92,423.03,357.41,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,303.44,400.05,214.01,8.74;8,70.92,411.57,226.71,8.74">UTACLIR -An extendable query translation system. Towards a unified approach to CLIR and multilingual IR</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hedlund</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Keskustalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Airio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pirkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,315.06,411.57,165.30,8.74;8,70.92,423.03,149.75,8.74">SIGIR 2002 Workshop I, Cross-language information retrieval: a research map</title>
		<meeting><address><addrLine>Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="15" to="18" />
		</imprint>
		<respStmt>
			<orgName>University of Tampere</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="8,70.92,446.07,440.80,8.74;8,70.92,457.53,408.56,8.74" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="8,304.03,446.07,207.69,8.74;8,70.92,457.53,56.92,8.74">Cross-language information retrieval using multiple language pairs</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hedlund</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pirkola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Keskustalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Airio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002-10">2002. October 2002</date>
			<pubPlace>Pretoria</pubPlace>
		</imprint>
	</monogr>
	<note>Accepted for presentation at the ProLISSA conference 24 -25</note>
</biblStruct>

<biblStruct coords="8,70.92,480.57,431.03,8.74;8,70.92,492.03,436.84,8.74;8,70.92,503.55,416.89,8.74;8,70.92,515.07,185.57,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,318.18,480.57,183.77,8.74;8,70.92,492.03,238.03,8.74">Translation resources, merging strategies, and relevance feedback for cross-language information retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pohlmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Westerveld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,394.18,492.03,113.58,8.74;8,70.92,503.55,391.92,8.74">Cross-language information retrieval and evaluation: Proceedings of the CLEF 2000 Workshop, Lectures in computer science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001">2001. 2069. 2001</date>
			<biblScope unit="page" from="102" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,70.92,538.05,452.11,8.74;8,70.92,549.57,72.84,8.74;8,143.76,547.39,4.32,5.65;8,150.54,549.57,160.04,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,204.33,538.05,303.43,8.74">The impact of query structure and query expansion on retrieval performance</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kekäläinen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,70.92,549.57,72.84,8.74;8,143.76,547.39,4.32,5.65;8,150.54,549.57,95.59,8.74">Proceedings of 21 st ACM/SIGIR Conference</title>
		<meeting>21 st ACM/SIGIR Conference</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="130" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,70.92,572.55,425.48,8.74;8,70.92,584.07,393.74,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,124.23,572.55,226.71,8.74">Towards a unified approach to CLIR and multilingual IR</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,368.40,572.55,128.00,8.74;8,70.92,584.07,189.48,8.74">SIGIR 2002 Workshop I, Crosslanguage information retrieval: a research map</title>
		<meeting><address><addrLine>Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="8" to="14" />
		</imprint>
		<respStmt>
			<orgName>University of Tampere</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="8,70.92,607.05,418.90,8.74;8,70.92,618.57,186.41,8.74;8,257.34,616.39,4.32,5.65;8,264.18,618.57,143.32,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,142.07,607.05,347.75,8.74;8,70.92,618.57,82.00,8.74">The effects of query structure and dictionary setups in dictionary-based cross-language information retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pirkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,169.80,618.57,87.53,8.74;8,257.34,616.39,4.32,5.65;8,264.18,618.57,95.52,8.74">Proceedings of the 21 st ACM/SIGIR Conference</title>
		<meeting>the 21 st ACM/SIGIR Conference</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="55" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,70.92,641.55,434.42,8.74;8,70.92,653.01,452.08,8.74;8,70.92,664.53,185.56,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,390.16,641.55,115.19,8.74;8,70.92,653.01,323.10,8.74">Targeted s-gram matching: a novel n-gram matching technique for cross-and monolingual word form variants</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pirkola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Keskustalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Leppänen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>Känsälä</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</author>
		<ptr target="http://InformationR.net/ir/7-2/paper126.html" />
	</analytic>
	<monogr>
		<title level="j" coord="8,411.18,653.01,85.56,8.74">Information Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,70.92,687.51,418.59,8.74;8,70.92,699.03,281.90,8.74;8,70.92,710.55,205.69,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,292.65,687.51,118.77,8.74">The collection fusion problem</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Johnson-Laird</surname></persName>
		</author>
		<idno>#500-225</idno>
		<ptr target="http://trec.nist.gov/pubs/trec3/t3_proceedings.html" />
	</analytic>
	<monogr>
		<title level="m" coord="8,429.24,687.51,60.27,8.74;8,70.92,699.03,30.28,8.74">Proceedings of TREC&apos;3</title>
		<meeting>TREC&apos;3<address><addrLine>Gaithersburg</addrLine></address></meeting>
		<imprint>
			<publisher>NIST Publication</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
