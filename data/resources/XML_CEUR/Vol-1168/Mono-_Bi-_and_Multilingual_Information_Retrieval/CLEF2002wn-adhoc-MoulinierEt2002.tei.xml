<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,113.51,123.16,368.25,12.91">Thomson Legal and Regulatory experiments for CLEF 2002</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,188.76,162.65,87.99,10.76"><forename type="first">Isabelle</forename><surname>Moulinier</surname></persName>
							<email>isabelle.moulinier@westgroup.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Thomson Legal and Regulatory Research and Development Group</orgName>
								<address>
									<addrLine>610 Opperman Drive</addrLine>
									<postCode>55123</postCode>
									<settlement>Eagan</settlement>
									<region>MN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,299.99,162.65,106.53,10.76"><forename type="first">Hugo</forename><surname>Molina-Salgado</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Thomson Legal and Regulatory Research and Development Group</orgName>
								<address>
									<addrLine>610 Opperman Drive</addrLine>
									<postCode>55123</postCode>
									<settlement>Eagan</settlement>
									<region>MN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,113.51,123.16,368.25,12.91">Thomson Legal and Regulatory experiments for CLEF 2002</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">519C691AB87CEDBBC9A19267EA72285B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Thomson Legal and Regulatory participated in the monolingual, the bilingual and the multilingual tracks. Our monolingual runs added Swedish to the languages we had submitted in previous participations. Our bilingual and multilingual efforts used English as the query language. We experimented with dictionaries and similarity thesauri for the bilingual task, while we used machine translations in our multi-lingual runs. Our various merging strategies had limited success compared to a simple round robin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For CLEF-2002, Thomson Legal and Regulatory (TLR) participated in monolingual, bilingual, and multilingual retrieval. Our monolingual experiments benefited from previous efforts. We added Swedish to the languages we submitted last year (Dutch, French, German, Italian and Spanish). In addition, we tried to improve our Italian runs by refining language resources. Our bilingual runs were from English to either French or Spanish. We translated query concepts using a combination of similarity thesauri and machine-readable dictionaries. Translated queries were structured to take into account multiple translations as well as translations of word pairs rather than words. In our multilingual experiments, we used a machine translation system rather than our bilingual approach. We mostly focused on merging strategies, using CORI, normalization or round-robin.</p><p>We give some background to our experiments in Section 2. Sections 3, 4 and 5 respectively present our monolingual, bilingual, and multilingual experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Previous research</head><p>Our participation at CLEF-2002 benefits from our earlier work, as well as from the work of others. Our bilingual effort relies on similarity thesauri for translating query terms from English to French or Spanish. In addition to translating words <ref type="bibr" coords="1,142.10,607.26,10.58,8.97" target="#b5">[6]</ref>, we also translate word pairs which loosely capture noun and verb phrases. This differs from our approach last year when we generated word bigrams rather than pairs <ref type="bibr" coords="1,375.30,619.22,10.58,8.97" target="#b2">[3]</ref>. In addition, we follow Pirkola's approach for handling multiple translations. By taking advantage of query structures available in INQUERY, Pirkola <ref type="bibr" coords="1,101.83,643.13,11.62,8.97" target="#b3">[4]</ref> has shown that grouping translations for a given term is a better technique than allowing all translations to contribute equally. This has been developed further by Sperer and Oard <ref type="bibr" coords="1,368.20,655.08,10.58,8.97" target="#b6">[7]</ref>.</p><p>One of the main issues in multilingual retrieval remains collection merging. In our experiments, we use simple merging techniques like round robin, normalized scores, as well as a variant of the CORI algorithm <ref type="bibr" coords="1,478.10,678.99,10.58,8.97" target="#b0">[1]</ref>. This is similar to Savoy's work at CLEF-2001 <ref type="bibr" coords="1,227.45,690.95,11.62,8.97" target="#b4">[5]</ref> and others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The WIN system</head><p>The WIN system is a full-text natural language search engine, and corresponds to TLR/West Group's implementation of the inference network retrieval model. While based on the same retrieval model as the INQUERY system <ref type="bibr" coords="2,70.87,81.22,10.58,8.97" target="#b1">[2]</ref>, WIN has evolved separately and focused on the retrieval of legal material in large collections in a commercial environment that supports both Boolean and natural language searches <ref type="bibr" coords="2,354.63,93.18,10.58,8.97" target="#b7">[8]</ref>.</p><p>In addition, WIN has shifted from supporting mostly English content to supporting a large number of Western-European languages as well. This was performed by localizing tokenization rules (mostly for French and Italian) and adopting morphological stemming. Stemming of non-English terms is performed using a third-party toolkit, the LinguistX platform commercialized by Inxight. A variant of the Porter stemmer is used for English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Document Scoring</head><p>WIN supports various strategies for computing term beliefs and scoring documents. We used a standard tf-idf for computing term beliefs in all our runs. The document is scored by combining term beliefs using a different rule for each query operator <ref type="bibr" coords="2,152.24,210.12,10.58,8.97" target="#b1">[2]</ref>. The final document score is an average of the document score as a whole and the score of the best portion. The best portion is dynamically computed based on query term occurrences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Query formulation</head><p>Query formulation identifies concepts in natural language text, and imposes a structure on these queries. In many cases, each term represents a concept, and a flat structure gives the same weight to all concepts. The processing of English queries eliminates stopwords and other noise phrases (such as "Find cases about", or "Relevant documents will include"), identifies (legal) phrases based on a phrase dictionary and detects common misspellings.</p><p>In the experiments reported below, we use our standard English stopword and noise phrase lists, but do not identify phrases or misspellings. We have expanded the English noise phrase list with noise phrases extracted from queries used in previous years. Our German, French, Spanish, and Dutch runs use the same stopword lists as last year, but noise phrase patterns have been updated to cover query sets from CLEF-2001. Our Italian stopword and noise phrase list was validated by a native speaker, while our Swedish resources were extracted from the web and from available query sets.</p><p>Concept identification depends on text segmentation. In our experiments, we follow two main definitions for a concept: a concept is an indexing unit (typically a word) or a concept is a construct of indexing units. Constructs are expressed in terms of operators (average, proximity, synonym, etc.) and indexing units. For instance, we use a construct when a term has multiple translations, or when we identify word pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Monolingual experiments</head><p>Our approach for monolingual runs is similar to last year's. We have revised the Italian stopword and noise phrase lists with the help of a native speaker. Our stemming procedure, although still based on the LinguistX toolkit, has been altered slightly to limit the occurrence of multiple stems.</p><p>German, Dutch, and Swedish are all compounding languages. However, the LinguistX platform does not support compound breaking for Swedish. We thus index and search using compound parts only German and Dutch content. Swedish is treated as a non-compounding language.</p><p>For all languages, we allow the stemmer to generate several stems for each term, as we do not rely on part-ofspeech tagging for disambiguation. Multiple stems were grouped under a single concept in the structure query.</p><p>Results from our official runs are reported in Table <ref type="table" coords="2,294.06,572.13,3.74,8.97" target="#tab_0">1</ref>. All runs used the title and description fields from the topics. Our results are comparable to those of previous years. Introducing revised stopword and noise phrase lists for Italian allows us to achieve good performance.</p><p>While most languages achieve an average precision in the same range (between 0.4 and 0.5), the figures for Swedish are much lower. We suspect that not breaking compounds may be the main cause, since previous work with German and Dutch has shown that retrieval performance was enhanced by compound breaking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Bilingual experiments</head><p>Our bilingual runs were from English queries to Spanish and French collections. As in our previous work <ref type="bibr" coords="2,496.60,685.71,10.58,8.97" target="#b2">[3]</ref>, we used a combination of similarity thesauri and machine-readable dictionaries. The machine-readable dictionaries were downloaded from the Internet (freedict).</p><p>We implemented a variant of the similarity thesaurus approach described in <ref type="bibr" coords="2,390.70,721.57,11.62,8.97" target="#b5">[6]</ref> for multilingual retrieval. We constructed two similarity thesauri: a word thesaurus and a word pair thesaurus. trained on a collection merging the UN parallel text corpus produced by the Linguistic Data Consortium, and an European Union (E.U.) parallel corpus that we have at TLR. Using a part-of-speech tagger, we restricted the set of words to nouns, verbs, adjectives and adverbs. Word pairs were generated using sliding windows centered only on nouns, and components in pairs were ordered alphabetically. Terms, words, or pairs, were considered as translations when their similarity was above a predefined threshold. This threshold was chosen as the best configuration on CLEF-2001 data.</p><p>While we identified noise phrase patterns in our official runs, stopwords were expected to have a different part-of-speech (like auxiliary, prepositions, etc). We later added a stopword list in conjunction with noise phrase patterns.</p><p>Table <ref type="table" coords="3,106.66,334.41,4.98,8.97" target="#tab_1">2</ref> reports our official runs. The translation resources for our official runs were a combination of the word similarity thesaurus and the dictionary.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stopwords is the same as the official runs but we use an explicit stopword list. Pairs correspond to the combination of the translation from all three sources. The Machine Translation runs use Babelfish.</head><p>A comparison of Tables <ref type="table" coords="3,180.21,667.89,4.98,8.97" target="#tab_1">2</ref> and<ref type="table" coords="3,204.71,667.89,4.98,8.97" target="#tab_2">3</ref> shows that using an explicit list of stopwords helps enhance the average precision. We have identified inaccuracies in part-of-speech tagging as one of the main reasons. Inaccuracies are often caused by inadequate context, or by the lack of a specific tag in one of the languages, e.g. auxiliary versus verb.</p><p>Our approach using similarity thesauri has some shortcomings in comparison with the machine translation approach. In particular, it is very dependent on the parallel corpus used for training. In our experiments, using E.U. material lead to some E.U.-oriented translations. For instance, European is translated into the French terms européen and communauté, and the Spanish terms europeo, comunidad and constitutivo. One way of addressing that issue may be to filter out corpus-specific terminology.</p><p>Unlike our results with bigrams at CLEF-2001 <ref type="bibr" coords="4,268.38,81.22,10.57,8.97" target="#b2">[3]</ref>, translating word pairs provides little advantage over translating individual words. One plausible hypothesis is that the window used to generate word pairs (we used a window of 9 centered on a noun) and the query structure are not compatible (we used a phrase node, i.e. a proximity of 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Multilingual experiments</head><p>During our multilingual experiments, we translated queries only. We used the indices generated for the monolingual runs for German, French, English, Italian and Spanish. Queries were translated from English to the other languages using Babelfish.</p><p>Our main focus was merging, although we have not been very successful so far. We tried a variety of merging approaches:</p><p>• round robin, i.e. a rank-based approach that alternates documents from each collection. In our setting, documents with identical score were given the same rank.</p><p>• raw score, which may or may not be comparable across collections</p><p>• CORI, where the collection score is estimated by the maximum score a (translated) query can achieve on that collection, not the original collection score in Callan et al <ref type="bibr" coords="4,344.26,289.32,10.58,8.97" target="#b0">[1]</ref>.</p><p>cori score = score within collection i * (1 + nb lang * collection score i -avg collection score avg collection score )</p><p>• normalized score, where the maximum score within collection i is the score of the document at rank 1.</p><p>norm score = score within collection i -0.4 maximum score within collection i -0.4 0.4 represents the minimum score any document can achieve in the belief network retrieval model.</p><p>• collection-weighted normalized score, where collection score i is the same as in the CORI approach above, and maximum collection score is the maximum of these scores.</p><p>weighted norm score = norm score * collection score i maximum collection score Our official run tlren2multi used round robin. Table <ref type="table" coords="4,289.46,480.92,4.98,8.97" target="#tab_4">4</ref> reports results from the different merging approaches. As reported too often, we found it hard to outperform the round robin approach. Our collection-weighted normalized score is the only merging approach to perform better but the difference is not significant. Our results with the CORI merging strategy are comparable to those obtained by Savoy <ref type="bibr" coords="4,311.34,516.78,10.58,8.97" target="#b4">[5]</ref>. It is possible that the CORI algorithm is impacted by our choice of collection score i . More analysis is required to assess the difference between the original CORI and our version. English was the query language. The median was computed from all submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>There are two issues with multilingual retrieval, the quality of the individual runs and the effectiveness of the merging strategy. The quality of the individual runs can easily be assessed by comparing their performance to the performance of monolingual runs. As can be seen in Table <ref type="table" coords="4,334.71,715.71,3.73,8.97">5</ref>, using translated queries leads to an average degradation of 25% in performance (performance is measured in terms of average precision).</p><p>How to quantify the effectiveness of merging strategies remains an open issue. We can observe the following properties in an attempt to measure the effectiveness of merging. In Table <ref type="table" coords="5,111.56,162.07,4.15,9.22">5</ref>. The impact of translation in multilingual retrieval. The percentages reflect differences in average precision when we compare retrieval using an English query with retrieval using a query in the collection language.</p><p>individual runs (the monolingual column vs. the translated column) leads to better performance. We can also compare the average of the individual run performances with the performance of the multilingual runs, and find that the average of individual runs is higher that any multilingual run. These observations tend to indicate that merging also deteriorates the effectiveness of multilingual runs, but do not tell us how much so. The poor performance of our English monolingual run (around 25% average precision)<ref type="foot" coords="5,448.87,449.96,3.49,6.28" target="#foot_0">1</ref> had a noticeable impact on multilingual runs. We found that round robin, cori score and weighted norm score were not affected as much as raw score and norm score by the English run. We expected round robin to be more sensitive to English documents, since one fifth of the documents are English. In effect, our modified version of round robin limited that effect for 40 queries, and aggravated it for 10 others. As could be expected, raw score was mislead by the higher score of English documents for a large number of queries. norm score suffers a similar problem: it is mislead when document scores are close to the highest document score in the retrieved list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Our participation at CLEF-2002 has mixed results. On the one hand, we consider that our monolingual runs successful, even though we intend to evaluate how much improvement can be achieved by relevance feedback. On the other hand, our bilingual and multilingual runs did not lead to the expected results. For instance, we did not find any evidence that translating word pairs was helpful in our bilingual runs. We also encountered an over-fitting problem when training similarity thesauri on the E.U. corpus. Finally, we are still in the process of investigating alternative merging algorithms, since our current approach has shown limited success.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,405.79,733.53,118.62,8.97"><head>Table 1 . Summary of all monolingual experiments using the title and description fields. Comparison to the median is expressed in the number of queries above, equal, and below.</head><label>1</label><figDesc>Both similarity thesauri were</figDesc><table coords="3,119.76,80.03,355.76,81.10"><row><cell>Run ID</cell><cell>Lang.</cell><cell cols="5">Avg. Prec. R-Prec. Above Median Median Below Median</cell></row><row><cell>tlrde</cell><cell>German</cell><cell>0.4221</cell><cell>0.4294</cell><cell>21</cell><cell>6</cell><cell>23</cell></row><row><cell>tlres</cell><cell>Spanish</cell><cell>0.4993</cell><cell>0.4816</cell><cell>31</cell><cell>3</cell><cell>16</cell></row><row><cell>tlrfr</cell><cell>French</cell><cell>0.4232</cell><cell>0.4134</cell><cell>17</cell><cell>8</cell><cell>25</cell></row><row><cell>tlrit</cell><cell>Italian</cell><cell>0.4159</cell><cell>0.4072</cell><cell>24</cell><cell>7</cell><cell>18</cell></row><row><cell>tlrnl</cell><cell>Dutch</cell><cell>0.4141</cell><cell>0.4211</cell><cell>27</cell><cell>3</cell><cell>20</cell></row><row><cell>tlrsv</cell><cell>Swedish</cell><cell>0.2439</cell><cell>0.2700</cell><cell>17</cell><cell>11</cell><cell>21</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,82.82,368.52,429.63,67.34"><head>Table 2 . Summary of our official bilingual experiments using the title and description fields. The median was computed from all submitted runs.</head><label>2</label><figDesc></figDesc><table coords="3,103.29,368.52,388.70,33.28"><row><cell>ID</cell><cell>Lang.</cell><cell cols="5">Avg. Prec. R-Prec. Above Median Median Below Median</cell></row><row><cell cols="2">tlren2es English/Spanish</cell><cell>0.2873</cell><cell>0.2857</cell><cell>12</cell><cell>4</cell><cell>34</cell></row><row><cell cols="2">tlren2fr English/French</cell><cell>0.3198</cell><cell>0.3440</cell><cell>13</cell><cell>3</cell><cell>34</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,70.87,460.07,453.55,127.56"><head>Table 3</head><label>3</label><figDesc>summarizes our unofficial runs. These runs used an explicit stopword list, instead of relying on part-ofspeech tags. We also translated word pairs after we completed training the word pairs similarity thesauri. The last runs use automatic translation and are part of our multilingual run.</figDesc><table coords="3,169.96,506.14,255.36,81.49"><row><cell>Run Description</cell><cell>Lang.</cell><cell cols="2">Avg. Prec. R-Prec.</cell></row><row><cell>Stopwords</cell><cell>English/Spanish</cell><cell>0.3123</cell><cell>0.3047</cell></row><row><cell>Stopwords + Pairs</cell><cell>English/Spanish</cell><cell>0.3118</cell><cell>0.3038</cell></row><row><cell cols="2">Machine Translation English/Spanish</cell><cell>0.3391</cell><cell>0.3414</cell></row><row><cell>Stopwords</cell><cell>English/French</cell><cell>0.3263</cell><cell>0.3474</cell></row><row><cell>Stopwords + Pairs</cell><cell>English/French</cell><cell>0.3257</cell><cell>0.3605</cell></row><row><cell cols="2">Machine Translation English/French</cell><cell>0.3513</cell><cell>0.3543</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="3,82.82,600.53,429.63,9.22"><head>Table 3 . Summary of our unofficial bilingual runs using the title and description fields.</head><label>3</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,82.82,561.37,429.63,91.65"><head>Table 4 . Summary of our multilingual experiments using the title and description fields.</head><label>4</label><figDesc></figDesc><table coords="4,107.33,561.37,380.61,69.54"><row><cell>ID</cell><cell cols="5">Avg. Prec. R-Prec. Above Median Median Below Median</cell></row><row><cell>tlren2multi (round robin)</cell><cell>0.2049</cell><cell>0.2803</cell><cell>17</cell><cell>4</cell><cell>29</cell></row><row><cell>raw score</cell><cell>0.1883</cell><cell>0.2521</cell><cell></cell><cell></cell><cell></cell></row><row><cell>cori score</cell><cell>0.1023</cell><cell>0.1489</cell><cell></cell><cell></cell><cell></cell></row><row><cell>norm score</cell><cell>0.1827</cell><cell>0.2496</cell><cell></cell><cell></cell><cell></cell></row><row><cell>weighted norm score</cell><cell>0.2160</cell><cell>0.2794</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="4,359.00,751.58,165.41,8.97"><head>Table 6 ,</head><label>6</label><figDesc>we observe that merging better</figDesc><table coords="5,187.17,80.03,220.93,69.14"><row><cell cols="2">Collection language Monolingual</cell><cell>Translated</cell></row><row><cell></cell><cell></cell><cell>from English</cell></row><row><cell>German</cell><cell>0.4221</cell><cell>0.2849 (-32.5%)</cell></row><row><cell>French</cell><cell>0.4232</cell><cell>0.3513 (-17.0%)</cell></row><row><cell>Spanish</cell><cell>0.4993</cell><cell>0.3391 (-32.1%)</cell></row><row><cell>Italian</cell><cell>0.4159</cell><cell>0.3212 (-22.8%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="5,82.82,287.07,429.63,139.47"><head>Table 6 . Average precision of merging strategies. The monolingual column uses results from our monolingual runs (English, German, French, Spanish and Italian). The translated column refers to English queries translated to the collection language. The row Average of individual runs does not rely on merging.</head><label>6</label><figDesc></figDesc><table coords="5,174.19,287.07,246.90,81.49"><row><cell>Merging strategy</cell><cell>Monolingual</cell><cell>Translated</cell></row><row><cell>round robin</cell><cell>0.2948</cell><cell>0.2049 (-30.5%)</cell></row><row><cell>raw score</cell><cell>0.3230</cell><cell>0.1883 (-41.7%)</cell></row><row><cell>cori score</cell><cell>0.1354</cell><cell>0.1023 (-24.5%)</cell></row><row><cell>norm score</cell><cell>0.2663</cell><cell>0.1827 (-31.4%)</cell></row><row><cell>weighted norm score</cell><cell>0.3042</cell><cell>0.2160 (-29.0%)</cell></row><row><cell>Average of individual runs</cell><cell>0.4007</cell><cell>0.3077 (-23.2%)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,85.21,750.20,300.18,7.17"><p>We suspect that this run encountered problems. We have not yet identified what the issues are.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,87.47,690.32,436.94,8.07;5,87.46,701.28,436.95,8.07;5,87.46,712.24,67.41,8.07" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,227.50,690.32,208.41,8.07">Searching distributed collections with inference networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,456.02,690.32,68.39,8.07;5,87.46,701.28,383.58,8.07">Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="21" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,87.47,722.70,436.94,8.07;5,87.46,733.66,215.18,8.07" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,228.84,722.70,101.74,8.07">The inquery retrieval system</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Broglio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,348.77,722.70,175.63,8.07;5,87.46,733.66,164.20,8.07">Proceedings of the 3rd International Conference on Database and Expert Systems Applications</title>
		<meeting>the 3rd International Conference on Database and Expert Systems Applications<address><addrLine>Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,87.47,81.91,436.94,8.07;6,87.46,92.86,425.60,8.07" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,363.14,81.91,161.27,8.07;6,87.46,92.86,138.99,8.07">Thomson legal and regulatory at clef 2001: monolingual and bilingual experiments</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Molina-Salgado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Moulinier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Knutson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sekhon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,243.23,92.86,164.26,8.07">Workshop Notes for the CLEF 2001 Workshop</title>
		<meeting><address><addrLine>Darmstadt, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,87.47,103.33,436.94,8.07;6,87.46,114.28,436.94,8.07;6,87.46,125.24,188.13,8.07" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,130.92,103.33,390.32,8.07">The effects of query structure and dictionary setups in dictionary-based cross-language information retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pirkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,97.84,114.28,426.57,8.07;6,87.46,125.24,31.48,8.07">Proceedings of the 21th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 21th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="55" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,87.47,135.70,436.95,8.07" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,122.10,135.70,116.38,8.07">Report on clef-2001 experiments</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,255.07,135.70,163.91,8.07">Workshop Notes for the CLEF 2001 Workshop</title>
		<meeting><address><addrLine>Darmstadt, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,87.47,146.16,436.94,8.07;6,87.46,157.12,436.95,8.07;6,87.46,168.08,40.51,8.07" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,241.62,146.16,233.07,8.07">Cross-lingual information retrieval in a multilingual legal domain</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sheridan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schuble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,491.95,146.16,32.46,8.07;6,87.46,157.12,355.18,8.07">Proceedings of the First European Conference on Research and Advanced Technology for Digital Libraries</title>
		<meeting>the First European Conference on Research and Advanced Technology for Digital Libraries<address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="253" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,87.47,178.54,436.94,8.07;6,87.46,189.50,436.94,8.07;6,87.46,200.46,79.92,8.07" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,191.89,178.54,225.56,8.07">Structured translation for cross-language information retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sperer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,436.96,178.54,87.45,8.07;6,87.46,189.50,373.02,8.07">Proceedings of the 23th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 23th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,87.47,210.92,436.94,8.07;6,87.46,221.88,436.94,8.07;6,87.46,232.84,115.30,8.07" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,129.84,210.92,318.54,8.07">Natural language vs. boolean query evaluation: a comparison of retrieval performance</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,469.72,210.92,54.69,8.07;6,87.46,221.88,409.41,8.07">Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="212" to="220" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
