<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,165.80,85.98,247.97,12.64">Scalable Multilingual Information Access</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,217.28,114.92,61.92,8.96"><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
							<email>mcnamee@jhuapl.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University Applied Physics Laboratory</orgName>
								<address>
									<addrLine>11100 Johns Hopkins Road</addrLine>
									<postCode>20723-6099</postCode>
									<settlement>Laurel</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,298.77,114.92,63.55,8.96"><forename type="first">James</forename><surname>Mayfield</surname></persName>
							<email>mayfield@jhuapl.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University Applied Physics Laboratory</orgName>
								<address>
									<addrLine>11100 Johns Hopkins Road</addrLine>
									<postCode>20723-6099</postCode>
									<settlement>Laurel</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,165.80,85.98,247.97,12.64">Scalable Multilingual Information Access</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3FC07BE812122673BC5414BB3A2721C8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The third Cross-Language Evaluation Forum workshop (CLEF-2002)  provides the unprecedented opportunity to evaluate retrieval in eight different languages using a uniform set of topics and assessment methodology. This year the Johns Hopkins University Applied Physics Laboratory participated in the monolingual, bilingual, and multilingual retrieval tasks. We contend that information access in a plethora of languages requires approaches that are inexpensive in developer and run-time costs. In this paper we describe a simplified approach that seems suitable for retrieval in many languages; we also show how good retrieval is possible over many languages, even when translation resources are scarce, or when query-time translation is infeasible. In particular, we investigate the use of character n-grams for monolingual retrieval, pre-translation expansion as a technique to mitigate errors due to limited translation resources, and translation of document representations to an interlingua for computationally efficient retrieval against multiple languages.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The number of languages in the CLEF document collection has grown to eight in 2002: Dutch, English, Finnish, French, German, Italian, Spanish, and Swedish. While the Romance languages have a great deal in common with one another, the Teutonic languages and Finnish have different origins; this set of modern languages provide challenges in word decompounding, complex morphology, and handling diacritical marks. For many years research in information retrieval was focused on the English language where these problems are less significant. As a result simple rules for stemming words and case-folding are really the only common improvements to exact string matching used by retrieval systems. The use of stopword lists is also routine, but seems to have little effect except to reduce the size of inverted files and to improve runtime efficiency.</p><p>We have been interested in discovering how simple methods can be applied to combat the aforementioned problems. Though their use has not found favor in English, we have demonstrated that overlapping character n-grams are remarkably effective for retrieval in many languages, including those most widely used in Europe. This simple technique appears to provide a surrogate means to normalize word forms, an efficient approximation to word bigrams (when n-grams with interior spaces are formed), and a solution to the problem of decompounding agglutinative languages. For the CLEF-2002 evaluation we continued to use the Hopkins Automated Information Retriever for Combing Unstructured Text (HAIRCUT) system which supports n-gram processing.</p><p>We participated in three tasks at this year's workshop, monolingual, cross-language, and multilingual retrieval. All of our official submissions were automated runs. This year we relied on an aligned parallel corpus as our sole translation resource -this resource was automatically mined from the Web and can be used to support retrieval between any pair of E.U. languages, except Greek. In the sections that follow, we first describe the standard methodology used for each language's sub-collection and we then present initial results in monolingual, bilingual, and multilingual retrieval. Highlights include an investigation into the use of pre-translation expansion from a comparable collection to improve retrieval performance, a discovery that character n-grams provide a means for effective bilingual retrieval for a close language pair, without translation, and an efficient method for multilingual retrieval that involves no query-time translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>For the monolingual tasks we created sixteen indexes, a word and an n-gram (n=6) index for each of the eight languages. For the bilingual and multilingual tasks we used the same indexes but translated topic statements to produce our official runs; however, we also report on another approach for multilingual retrieval that required a separate index. Information about each index is provided in   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index Construction</head><p>Our methods for scanning documents, creating an index, and processing queries are essentially unchanged from last year. We include below a description from our CLEF-2001 paper <ref type="bibr" coords="2,381.89,339.56,10.86,8.96" target="#b2">[3]</ref>; those already familiar with our our previous work using HAIRCUT should skip ahead to a description of this year's experiments. Documents were processed using only the permitted tags specified in the workshop guidelines. First SGML macros were expanded to their appropriate Unicode character. Then punctuation was eliminated, letters were downcased, and only the first four of a sequence of digits were preserved (e.g., 010394 became 0103##). Diacritical marks were preserved. The result is a stream of words separated by spaces. Exceedingly long words were truncated; the limit was 35 characters in the Dutch and German languages and 20 otherwise. When using n-grams we extract indexing terms from the same stream of words; thus, the n-grams may span word boundaries, but sentence boundaries are noted so that n-grams spanning sentence boundaries are not recorded. N-grams with leading, central, or trailing spaces are formed at word boundaries. For example, given the phrase, "the prime minister," the following 6-grams are produced. The use of overlapping character n-grams provides a surrogate form of morphological normalization. For example, in Table <ref type="table" coords="2,146.42,698.00,4.98,8.96">2</ref> above, the n-gram "minist" could have been generated from several different forms like administer, administrative, minister, ministers, ministerial, or ministry. It could also come from an unrelated word like feminist. Another advantage of n-gram indexing comes from the fact that n-grams containing spaces can convey phrasal information. In the table above, 6-grams such as "rime-m", "ime-mi", and "memin" may act much like the phrase "prime minister" in a word-based index using multiple word phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Term</head><p>At last year's workshop we examined different types of translation resources for bilingual retrieval and espoused a language-neutral approach to retrieval. We continued in this vein and did not utilize stopword lists or morphological analyzers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Processing</head><p>HAIRCUT performs rudimentary preprocessing on topic statements to remove stop structure, e.g., phrases such as "… would be relevant" or "relevant documents should…." . We have constructed a list of about 1000 such English phrases from previous topic sets (mainly TREC topics) and these have been translated into other languages using commercial machine translation. Other than this preprocessing, queries are parsed in the same fashion as documents in the collection.</p><p>In all of our experiments we used a linguistically motivated probabilistic model for retrieval. Our official runs all used blind relevance feedback, though it did not improve retrieval performance in every instance. To perform relevance feedback we first retrieved the top 1000 documents. We then used the top 20 documents for positive feedback and the bottom 75 documents for negative feedback; however, we removed any duplicate or near duplicate documents from these sets. We then select terms for the expanded query based on three factors, a term's initial query term frequency (if any); the cube root of the (α=3, β=2, γ=2) Rocchio score; and a term similarity metric that incorporates IDF weighting. The 60 top ranked terms are then used as the revised query with words as indexing terms; 400 terms are used with 6-grams. In previous work we penalized documents containing only a fraction of the query terms; we are no longer convinced that this technique adds much benefit and have discontinued its use. As a general trend we observe a decrease in precision at very low recall levels when blind relevance feedback is used, but both overall recall and mean average precision are improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monolingual Experiments</head><p>We submitted an official run for each target language only using the &lt;title&gt; and &lt;desc&gt; fields and only automatic processing. These official runs were actually the combination of two base-runs, one using words and one using 6-grams; both base-runs also make use of blind relevance feedback. We again relied on a statistical language model of retrieval and used the same parameters as last year. With words as indexing terms we used queries expanded to include 60 terms and a smoothing parameter, alpha, of 0.30. When 6grams were used instead, queries were expanded to 400 terms and alpha was set to 0.15. In both cases the top 20 documents were used as positive examples and the bottom 75 of 1000 were presumed irrelevant for the purposes of query expansion. Our official results are shown below in  <ref type="table" coords="3,96.71,586.52,3.75,8.96" target="#tab_4">3</ref>. Official results for monolingual task. The shaded row contains results for a comparable, unofficial English run.</p><p>The recall at 1000 documents is very high relative to the number of relevant documents in each of the subcollections. Since we created runs by combining distinct runs (one using words, one using 6-grams) we should examine the individual performance using each method. Figure <ref type="figure" coords="3,358.12,642.32,4.98,8.96" target="#fig_0">1</ref> contains a plot that shows the mean average precision obtained for each sub-collection using both approaches. We note that in English and the Romance languages, the use of words yields slightly better performance, an improvement of 0.010 to 0.025 in absolute terms. We reported observing the same trend for French and Italian during last year's evaluation <ref type="bibr" coords="3,70.64,686.96,10.69,8.96" target="#b2">[3]</ref>. In the Dutch sub-collection, little difference is seen, but 6-grams are clearly advantageous in the remaining languages. A sizeable difference is seen in German (0.035) and Swedish (0.023), and far more significantly,, in Finnish (0.13)</p><p>In Figure <ref type="figure" coords="3,111.88,731.60,4.98,8.96" target="#fig_0">1</ref> we also plot the performance of the combined runs. Combination was generally beneficial, but due to the large disparity between n-grams and words for Finnish, the technique depressed performance in that language compared to that which would have observed using n-grams alone. No difference due to combination was seen for German, but an improvement of between 0.016 and 0.023 was found in the remaining collections, an improvement of 3-5% in relative terms. We performed the same analysis when blind relevance feedback was not performed and found similar results. There the performance was generally less than when automated feedback was performed. Also, within each language, differences between techniques were larger without feedback. By averaging across all languages, we saw that feedback improved the microaveraged mean average precision from 0.3479 to 0.4141 when words were used, and from 0.3729 to 0.4295 when 6-grams were used. If, as it seems, n-grams are more effective for retrieval in languages with complex morphology, then the fact that the two approaches achieved more similar performance when feedback was employed would support the notion that automatic relevance feedback improves performance by redressing the effect of inflectional variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bilingual Experiments</head><p>Our official bilingual submissions were based on query translation when some attempt at translation was made; we submitted one run for each document collection. For each collection, save English, we created one run using the English query statements and the title and description fields. The runs are named using the template aplbienxx. For these 7 runs, we used pre-translation expansion using the L.A. Times collection; queries were expanded to 60 terms and we used statistical word-by-word translations mined from an aligned parallel collection. This collection is an expanded version of the corpus we obtained from the Europa web site (details follow). We used unnormalized words for these bilingual experiments because we have not yet used our parallel collection to generate statistical translations that are character n-grams -we want to investigate this, but for the evaluation, we simply used words. The final two runs, aplbiptesa and aplbiptesb, made no use of translation whatsoever. Since 10 runs were allowed to be submitted according to the track guidelines, we did submit three other runs. The first, aplbipten, used the Portuguese topic statements to search the English sub-collection; our motivation here was only to submit a run using these statements.</p><p>The seven runs produced using English queries first performed pre-translation expansion using the L.A. Times sub-collection. The query was expanded to include 60 words, and then each term was translated, if possible, using the Europa corpus for translation. Then two runs were made, one using pre-translation expansion alone and one using both pre-and post-translation query expansion. Scores for these two runs were normalized and merged together to form our official submission. The eighth run, aplbipten, used the Portuguese topic statements and no pre-translation expansion was attempted. However, two runs were still combined, one with no expansion and one that made use of blind relevance feedback. Results for these runs are shown below in  <ref type="table" coords="5,97.19,189.32,3.75,8.96" target="#tab_5">4</ref>. Official results for the bilingual task. Except for the shaded run, English was used as the source language.</p><p>The results for each run in Table <ref type="table" coords="5,211.65,222.80,4.98,8.96" target="#tab_5">4</ref> are not comparable to one another because a different target language collection was involved. Furthermore, the last column, which reports the comparison to a target language monolingual baseline using mean average precision, is not especially meaningful. It is unfair to compare against our monolingual baseline for two reasons. First, Voorhees has pointed out that a comparison between test-sets using different topic statements (as is the case here) is not justified even though the document collections are the same <ref type="bibr" coords="5,165.01,278.60,13.11,8.96" target="#b4">[5]</ref>; the various translations of each topic may result in queries that are significantly easier in one language than another. Second, slightly different algorithms were used in our monolingual and bilingual results. Our monolingual runs were formed through merging n-gram and word-based runs while our bilingual results only used words. Also, the bilingual runs all used pre-translation over the English collection, which itself only contained relevant documents for 42 of the 50 topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Improved Translation Resource</head><p>The quality of translation resources is a critical driver for CLIR performance. Therefore, it is important to select a translation approach that ensures translation of important query terms. At our disposal we had translation software (Systran, L&amp;H Power Translator, and various on-line services), bilingual dictionaries automatically extracted from lists on the Web, and a large parallel corpus. We investigated each of these methods in our 2001 paper and found that when only a single source was used, best performance was obtained by using the parallel collection for translation. We decided to expand the parallel collection and use it for our official runs.</p><p>The collection was obtained through a nightly crawl of the Europa web site where we targeted the Official Journal of the European Union [6]. The Journal is available in each of the E.U. languages and consists mainly of governmental topics, for example, trade and foreign relations. We had data available from December 2000 through May 2002. Though focused on European topics, the time span is 5-7 years after the CLEF-2002 document collection. So, it is possible that many proper names in 1994 and 1995 will be rarely mentioned, if at all. The Journal is published electronically in PDF format and we wanted to create an aligned collection. Rather than attempt an 11-language, multiple aligned collection, we simply wasted disk space and preformed redundant alignments. At the present we have not aligned all O(n2) pairs, but instead created n alignments between English and the other languages. We used the publicly available pdftotext package to extract text from the PDF, but Greek text is not supported by the software so we neglected this language <ref type="foot" coords="5,106.88,571.39,2.52,4.54" target="#foot_0">1</ref> . Once converted to text, documents were split into pieces using conservative rules for page-breaks and paragraph breaks. Many of the documents are written in outline form, or contain large tables, so this task is not trivial. Approximately 20GB of PDF documents are involved; we find that the PDF files are approximately ten times larger than the plain text versions. Thus we have about 200 MB of text in each language that may be aligned.</p><p>Once aligned, we indexed each sub-collection using the same technique described for the CLEF-2002 document collections; in particular, unnormalized words were used as indexing terms. We relied on query term translation and extracted candidate translations as follows. First, we would take a candidate term as input and identify documents containing this word in the English subset of the aligned. Up to 5000 documents were considered; we bounded the number for reasons of efficiency and because we felt that performance was not enhanced appreciably when a greater number of documents was used. If no document contained this term, then the word itself was left untranslated. Second, we would identify the corresponding documents in the target language. Third, using a similarity metric that is similar to mutual information, we would extract a single potential translation using the frequency of occurrence in the whole collection and the frequency in the subset of aligned documents found that are believed to contain a mapping for the original source term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-translation Expansion</head><p>We are still analyzing the effect of query expansion on retrieval performance and will report on it in the final version of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No translation</head><p>In previous work we have shown that reasonably good retrieval between two related languages is possible, without any translation at all. Though the use of cognate matches as been known for some time (e.g., <ref type="bibr" coords="6,477.44,215.36,10.54,8.96" target="#b0">[1]</ref>), we found that pre-translation expansion using a comparable expansion corpus enhances performance -in some cases, by 200-300% <ref type="bibr" coords="6,158.32,237.68,10.68,8.96" target="#b3">[4]</ref>. During last year's campaign we also noted that n-grams were almost twice as effective as words in this scenario <ref type="bibr" coords="6,211.13,248.84,10.66,8.96" target="#b2">[3]</ref>. This year, we wanted to conduct similar work that looked at a variety of language pair in comparison to our pervious work which only used English as the target language. We looked at several language pairs and hoped to see a difference in performance when this method was used between close languages. Our hypothesis is that translation-less retrieval between related languages (say the Romance group) would be more effective than when this approach was used between, say, German and Spanish.</p><p>For these runs, we did not use pre-translation expansion (though we hope to examine this in the future). We did compare performance using words and n-grams. Our two official runs for this experiment we aplbiptesa and aplbiptesb. The first used 6-grams as indexing terms while the later used words. Both urns used blind relevance feedback. Results for these two runs are shown below in Table <ref type="table" coords="6,365.32,360.44,3.75,8.96">5</ref>  <ref type="table" coords="6,96.71,429.32,3.75,8.96">5</ref>. Official results for the bilingual task using no translation, the Portuguese topic statements, and the Spanish news collection.</p><p>It is interesting to note that with no translation whatsoever and the use of 6-grams as indexing terms, performance was 92% of that when English topics where translated to Spanish. This is still not a fair comparision (the English topics might be particularly hard, for example), but, it is surprisingly good. The mean precision at 5 docs for aplbiptesa was 0.3920; on average, two out of the five top documents were relevant, despite not translating the queries. We examined several other language pairs as well, but have not looked at all n(n-1) cases. These other results were not official runs.  <ref type="table" coords="6,96.00,680.84,3.76,8.96">6</ref>. Results using no translation between other language pairs (languages are encoded in the run ids).</p><p>As would be expected, retrieval without translation is more effective in closely related language pairs. In the table above, we see that German retrieval against Dutch is almost 50% as effective as monolingual Dutch retrieval when using 6-grams; similarly, Dutch retrieval against German is about 60% as effective as monolingual German retrieval. This strongly suggests that for language pairs with few direct translation resources, translation to a closely related language for which translation is feasible from the source language, can result in good cross-language retrieval performance. It remains to examine whether small length n-grams result in even greater performance, and whether pre-translation expansion improves this approach. Our previous experiments on the CLEF-2001 collection would suggest the later, but in those we only examined language pairs where English was the target language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multilingual Experiments</head><p>To date, our experiments in multilingual merging have not found a technique that results in producing a high quality, single ranked list from documents in many languages. Last year we experimented with methods that tried to normalize document similarity scores and to produce a single list. This year we submitted two official runs that used either merge-by-score (aplmuena) or merge-by-rank (aplmuenb). As in the past, we found these two methods comparable, but not tremendously effective. However, no more suitable method has been proposed.</p><p>We have been intrigued by work by researchers at the University of California at Berkeley that address this problem in a way that does not require score normalization. Gey et al., create a single inverted file from documents in many languages and then, to score documents, they create a composite query composed of a query statement in a single language concatenated with translations of that query in the other collection languages <ref type="bibr" coords="7,114.76,289.04,10.67,8.96" target="#b1">[2]</ref>. This approach results in a single ranked list, and it appears to work well with Berkeley's logisitic regression approach to retrieval. In the CLEF-2001 campaign we examined this method using both unnormalized words and character 5-grams. Our results with simple words were disappointing and the 5grams, though significantly better, did not perform as well as simple merging approaches. We do not yet understand why our results are different than those reported by Berkeley, but the fact that we use a different model of retrieval may be responsible.</p><p>This year, we also attempted a dual solution to the approach described above. Rather than translate queries into every language, we created an index that contained a document that was transformed into a single language. We picked English as our interlingua and mapped each document into English using a bag-ofwords approach to translation. Strictly speaking, we did not perform translation of the documents. Rather, we took the indexed document representations from our monolingual indexes, loaded a hash-table into memory that contained a bilingual wordlist, and created a new inverted-file where the posting lists were English words (or untranslatable foreign terms) that refered to documents from the different languages. We also included the native English documents. Because we felt lexical coverage was most important, we translated the documentation representations by mapping each source word into all of its candidate (English) translations. We probably should have removed stopwords, but did not do so. This process is linear in the size of the collection since the hash-table lookups are O(1) per word occurrence. This approach creates an index with several peculiar characteristics. First, it makes the foreign language document representations a bit larger, since on average, a term may have 2 or 3 potential translations. Also, the original English documents are somewhat more focused since they don't have erroneous translations in their representations. Still, we are left with an approach where we can take a query in our preferred language (preferred here because we have good resources for it) and simply run it against our transformed document collection. This approach (aplmuend) appears to be 18% more effective than our officially submitted runs using normalization and merging. Interestingly, precision at a small number of documents was greatly enhanced, and recall at 1000 docs suffered; however, a subsequent combination with run aplmuena restored the overall recall (aplmuenq). Furthermore, this method creates a composite 'English' index in time linear with the collection size and requires no query-time translation or post-retrieval processing (e.g., merging). See Table <ref type="table" coords="7,112.96,612.69,4.98,8.96" target="#tab_8">7</ref>  One final thing we did for this year's mutlingual task was to try and isolate the effect of losses due to query translation and multi-collection merging. What we did was to take monolingual runs for each of the collections and attempt to merge them (aplmuenz). We found slightly better average precision when doing this, as might be expected. We think this is an interesting way to investigate the multilingual problem; it reduced the problem to that finding a good merging strategy, which still seems like one of the most viable approaches to MLIR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>We set out to investigate how well a simplified approach to CLIR would work. By applying our languageneutral philosophy, we were able to submit monolingual and bilingual runs for each of the document collections. We repeated previous experiments and confirmed that character n-grams work well in many languages, including Finnish and Swedish which we had not previously studied. N-grams appear to have a decided advantage over words in Finnish retrieval. We also examined retrieval using cognate matches between close, and less close language pairs; as expected, performance is higher (relative to a monolingual baseline) with related pairs. Finally, we implemented a novel approach to multilingual retrieval that is similar to document translation -we transformed a bag-of-words representation of documents in many languages into a corresponding set of English terms using a bilingual dictionary. This processing is efficient and can be done at indexing time. As a result, multilingual queries from a single interlingua can be processed with no additional query-time processing beyond that normal for monolingual retrieval. Our preliminary results indicate that this approach is also 18% more effective than a baseline using score normalization and merging.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,70.64,329.24,282.43,9.09"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Comparing words and character n-grams (n=6) by language.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,408.45,725.24,22.81,8.96"><head>Table</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="1,437.56,725.24,3.75,8.96"><head>Table 1 .</head><label>1</label><figDesc>. Information about indexes for the CLEF-2002 test collection</figDesc><table coords="2,144.44,73.16,290.95,206.72"><row><cell></cell><cell cols="2"># docs collection size</cell><cell>type</cell><cell># terms</cell><cell>index size</cell></row><row><cell></cell><cell></cell><cell>(MB zipped)</cell><cell></cell><cell></cell><cell>(MB)</cell></row><row><cell>Dutch</cell><cell>190,604</cell><cell>203</cell><cell>words</cell><cell>692,754</cell><cell>160</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">6-grams 3,816,580</cell><cell>1133</cell></row><row><cell cols="2">English 110,282</cell><cell>166</cell><cell>words</cell><cell>235,713</cell><cell>98</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">6-grams 2,944,813</cell><cell>889</cell></row><row><cell>Finnish</cell><cell>55,344</cell><cell>51</cell><cell>words</cell><cell>981,174</cell><cell>87</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">6-grams 2,524,529</cell><cell>383</cell></row><row><cell>French</cell><cell>87,191</cell><cell>92</cell><cell>words</cell><cell>248,225</cell><cell>68</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">6-grams 2,343,009</cell><cell>511</cell></row><row><cell cols="2">German 225,371</cell><cell>207</cell><cell>words</cell><cell>1,079,453</cell><cell>221</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">6-grams 4,203,047</cell><cell>1,325</cell></row><row><cell>Italian</cell><cell>108,578</cell><cell>107</cell><cell>words</cell><cell>338,634</cell><cell>89</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">6-grams 2,162,249</cell><cell>607</cell></row><row><cell cols="2">Spanish 215,737</cell><cell>186</cell><cell>words</cell><cell>382,666</cell><cell>150</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">6-grams 3,193,404</cell><cell>1098</cell></row><row><cell cols="2">Swedish 142,819</cell><cell>94</cell><cell>words</cell><cell>510,245</cell><cell>95</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">6-grams 3,254,595</cell><cell>628</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="2,70.64,474.19,438.49,199.29"><head>Document Frequency Collection Frequency IDF RIDF</head><label></label><figDesc></figDesc><table coords="2,70.64,494.66,438.49,178.83"><row><cell>-the-p</cell><cell>72,489</cell><cell>241,648 0.605 0.434</cell></row><row><cell>the-pr</cell><cell>41,729</cell><cell>86,923 1.402 0.527</cell></row><row><cell>he-pri</cell><cell>8,701</cell><cell>11,812 3.663 0.364</cell></row><row><cell>e-prim</cell><cell>2,827</cell><cell>3,441 5.286 0.261</cell></row><row><cell>-prime</cell><cell>3,685</cell><cell>5,635 4.903 0.576</cell></row><row><cell>prime-</cell><cell>3,515</cell><cell>5,452 4.971 0.597</cell></row><row><cell>rime-m</cell><cell>1,835</cell><cell>2,992 5.910 0.689</cell></row><row><cell>ime-mi</cell><cell>1,731</cell><cell>2,871 5.993 0.711</cell></row><row><cell>me-min</cell><cell>1,764</cell><cell>2,919 5.966 0.707</cell></row><row><cell>e-mini</cell><cell>3,797</cell><cell>5,975 4.860 0.615</cell></row><row><cell>-minis</cell><cell>4,243</cell><cell>8,863 4.699 1.005</cell></row><row><cell>minist</cell><cell>15,428</cell><cell>33,731 2.838 0.914</cell></row><row><cell>iniste</cell><cell>4,525</cell><cell>8,299 4.607 0.821</cell></row><row><cell>nister</cell><cell>4,686</cell><cell>8,577 4,557 0.816</cell></row><row><cell>ister-</cell><cell>7,727</cell><cell>12,860 3.835 0.651</cell></row><row><cell cols="3">Table 2. Example 6-grams produced for the input "the prime minister." Term statistics are based on the LA</cell></row><row><cell cols="3">Times subset of the English collection. Dashes indicate whitespace characters.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="3,70.64,447.56,338.43,147.92"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table coords="3,70.64,470.36,338.43,125.12"><row><cell>run id</cell><cell cols="2">topic fields average</cell><cell>recall</cell><cell># topics</cell></row><row><cell></cell><cell></cell><cell>precision</cell><cell>(at 1000)</cell><cell></cell></row><row><cell>aplmode</cell><cell>TD</cell><cell>0.4663</cell><cell>1792 / 1938</cell><cell>50</cell></row><row><cell>aplmoen</cell><cell>TD</cell><cell>0.3957</cell><cell>800 / 821</cell><cell>50</cell></row><row><cell>aplmoes</cell><cell>TD</cell><cell>0.5192</cell><cell>2659 / 2854</cell><cell>50</cell></row><row><cell>aplmofi</cell><cell>TD</cell><cell>0.3280</cell><cell>483 / 502</cell><cell>30</cell></row><row><cell>aplmofr</cell><cell>TD</cell><cell>0.4509</cell><cell>1364 / 1383</cell><cell>50</cell></row><row><cell>aplmoit</cell><cell>TD</cell><cell>0.4599</cell><cell>1039 / 1072</cell><cell>49</cell></row><row><cell>aplmonl</cell><cell>TD</cell><cell>0.5028</cell><cell>1773 / 1862</cell><cell>50</cell></row><row><cell>aplmosv</cell><cell>TD</cell><cell>0.4317</cell><cell>1155 / 1196</cell><cell>49</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="4,143.46,115.94,297.99,594.86"><head>Table 4 . Monolingual Performance: Words vs. Six-grams</head><label>4</label><figDesc></figDesc><table coords="4,143.46,142.11,297.99,158.63"><row><cell></cell><cell>0.55</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean Average Precision</cell><cell>0.25 0.30 0.35 0.40 0.45 0.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Words Six Combined</cell></row><row><cell></cell><cell>0.20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>DE</cell><cell>EN</cell><cell>ES</cell><cell>FI</cell><cell>FR</cell><cell>IT</cell><cell>NL</cell><cell>SV</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Language</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="6,70.64,360.44,399.01,77.84"><head></head><label></label><figDesc>.</figDesc><table coords="6,70.64,383.24,399.01,55.04"><row><cell>run id</cell><cell>topic</cell><cell>type</cell><cell>average</cell><cell>recall</cell><cell cols="2"># topics % mono</cell><cell>%Eng</cell></row><row><cell></cell><cell>fields</cell><cell></cell><cell>precision</cell><cell>(at 1000)</cell><cell></cell><cell></cell><cell>bilingual</cell></row><row><cell>aplbiptesa</cell><cell>TD</cell><cell>6-grams</cell><cell>0.3325</cell><cell>2071 / 2854</cell><cell>50</cell><cell cols="2">64.04% 92.31%</cell></row><row><cell>aplbiptesb</cell><cell>TD</cell><cell>words</cell><cell>0.2000</cell><cell>1589 / 2854</cell><cell>50</cell><cell cols="2">38.52% 55.52%</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="7,70.64,612.69,426.05,112.88"><head>Table 7 .</head><label>7</label><figDesc>for a comparison of this and our two official runs. Multilingual results.</figDesc><table coords="7,83.12,635.49,413.58,78.44"><row><cell>run id</cell><cell>topic</cell><cell>average</cell><cell>recall</cell><cell>precision</cell><cell>remarks</cell></row><row><cell></cell><cell>fields</cell><cell>precision</cell><cell>(at 1000)</cell><cell>at 5 docs</cell><cell></cell></row><row><cell>aplmuena</cell><cell>TD</cell><cell>0.2070</cell><cell>4729 / 8068</cell><cell>0.4680</cell><cell>official; score-based merge</cell></row><row><cell>aplmuenb</cell><cell>TD</cell><cell>0.2082</cell><cell>4660 / 8068</cell><cell>0.4480</cell><cell>official; rank-based merge</cell></row><row><cell>aplmuend</cell><cell>TD</cell><cell>0.2447</cell><cell>3394 / 8068</cell><cell>0.5760</cell><cell>translation of document representations</cell></row><row><cell>aplmuenq</cell><cell>TD</cell><cell>0.2456</cell><cell>4766 / 8068</cell><cell>0.5600</cell><cell>combination of aplmuena and aplmuend</cell></row><row><cell>aplmuenz</cell><cell>TD</cell><cell>0.2265</cell><cell>4772 / 8068</cell><cell>0.4840</cell><cell>score-based merge using monolingual runs</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,75.68,747.68,426.72,8.96"><p>We are very interested in having Danish and Portuguese document collections added to the CLEF test set.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,88.63,360.23,420.46,8.10;8,70.64,370.19,438.45,8.11;8,70.64,380.27,98.96,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,263.37,360.23,226.79,8.10">Using Clustering and Super Concepts within SMART: TREC-6</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Walz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,212.84,370.20,236.88,8.10">Proceedings of the Sixth Text REtrieval Conference (TREC-6)</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Sixth Text REtrieval Conference (TREC-6)</meeting>
		<imprint>
			<publisher>NIST Special Publication</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="500" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,88.64,390.35,420.41,8.10;8,70.64,400.31,438.52,8.11;8,70.64,410.39,240.25,8.11" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,252.72,390.35,256.33,8.10;8,70.64,400.31,193.07,8.10">Manual Queries and Machine Translation in Cross-language Retrieval and Interactive Retrieval with Cheshire II at TREC-7</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,440.12,400.32,69.04,8.10;8,70.64,410.40,162.45,8.10">Proceedings of the Seventh Text REtrieval Conference (TREC-7)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Seventh Text REtrieval Conference (TREC-7)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="527" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,88.64,421.43,420.54,8.10;8,70.64,431.39,438.47,8.11;8,70.64,441.36,438.47,8.10;8,70.64,451.31,127.60,8.11" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,231.99,421.43,277.19,8.10;8,70.64,431.39,48.50,8.10">JHU/APL Experiments at CLEF: Translation Resources and Score Normalization</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,429.56,431.40,79.55,8.10;8,70.64,441.36,303.72,8.10">Evaluation of Cross-Language Information Retrieval Systems: Proceedings of the CLEF 2001 Workshop</title>
		<title level="s" coord="8,381.12,441.36,127.99,8.10">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Martin</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Kluck</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2406</biblScope>
			<biblScope unit="page" from="193" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,88.64,462.35,420.49,8.10;8,70.64,472.31,438.43,8.10;8,70.64,482.39,253.69,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,237.12,462.35,272.00,8.10;8,70.64,472.31,77.86,8.10">Comparing Cross-Language Query Expansion Techniques by Degrading Translation Resources</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Mayfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,168.05,472.31,341.02,8.10;8,70.64,482.39,129.68,8.10">the Proceedings of the 25th Annual International Conference on Research and Development in Information Retrieval (SIGIR-2002)</title>
		<meeting><address><addrLine>Tampere, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-08">August 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,88.64,493.31,420.50,8.10;8,70.64,503.39,438.52,8.11;8,70.64,513.35,343.72,8.11" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,157.47,493.31,190.15,8.10">The Philosophy of Information Retrieval Evaluation</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,206.36,503.40,302.80,8.10;8,70.64,513.36,79.36,8.10">Evaluation of Cross-Language Information Retrieval Systems: Proceedings of the CLEF 2001 Workshop</title>
		<title level="s" coord="8,156.64,513.36,127.86,8.10">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Martin</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Kluck</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2406</biblScope>
			<biblScope unit="page" from="355" to="370" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
