<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
				<funder ref="#_kVax6YR #_sA44T46 #_QaANksU">
					<orgName type="full">Netherlands Organization for Scientific Research (NWO)</orgName>
				</funder>
				<funder ref="#_YW2NdWa">
					<orgName type="full">Netherlands Organization for Scientific Research (NWO</orgName>
				</funder>
				<funder>
					<orgName type="full">Physical Sciences Council</orgName>
				</funder>
				<funder ref="#_h7XjFgr #_eZewRA8 #_JBG5JYP #_vFx8rXY">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,146.66,175.42,70.07,10.76"><forename type="first">Christof</forename><surname>Monz</surname></persName>
							<email>christof@science.uva.nl</email>
						</author>
						<author>
							<persName coords="1,264.55,175.42,58.10,10.76"><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
							<email>kamps@science.uva.nl</email>
						</author>
						<author>
							<persName coords="1,373.46,175.42,82.88,10.76"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The University of Amsterdam at CLEF</orgName>
								<address>
									<postCode>2002</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Language &amp; Inference Technology Group</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<addrLine>Nieuwe ; Achtergracht 166</addrLine>
									<postCode>1018 WV</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D22C62655CF048B5EB81517E4519958C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the official runs of our team for CLEF 2002. We took part in the monolingual tasks for each of the seven non-English languages for which CLEF provides document collections (Dutch, Finnish, French, German, Italian, Spanish, and Swedish). We also conducted our first experiments for the bilingual task (English to Dutch, and English to German), and took part in the GIRT and Amaryllis tasks. Finally, we experimented with the combination of runs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this year's CLEF evaluation exercise we participated in four tasks. We took part in the monolingual tasks for each of the seven non-English languages for which CLEF provides document collections (Dutch, Finnish, French, German, Italian, Spanish, and Swedish). We also conducted our first experiments for the bilingual task (English to Dutch, and English to German), and took part in the GIRT and Amaryllis tasks.</p><p>Our participation in the monolingual task was motivated by a number of aims. First, we wanted to refine and improve our morphological normalization tools for the languages for which we took part in CLEF 2001: Dutch, German, and Italian. Furthermore, during the 2001 evaluation exercise we found that compound splitting significantly improved retrieval effectiveness for Dutch and German <ref type="bibr" coords="1,450.65,475.76,15.27,8.97" target="#b13">[14]</ref>. However, building tools such as compound splitters is not only highly language dependent but also resource intensive. And for some languages lexical resources are hard to obtain. For this reason we also wanted to develop, and experiment with, 'zero knowledge' language independent morphological normalization tools. As an aside, the availability of different kinds of runs (such as linguistically motivated vs. zero-knowledge runs) made it possible to experiment with combinations of runs, a method which has been shown to lead to improvements in retrieval effectiveness over the underlying base runs; see e.g., <ref type="bibr" coords="1,405.88,547.49,10.79,8.97" target="#b7">[8,</ref><ref type="bibr" coords="1,419.16,547.49,7.47,8.97" target="#b0">1,</ref><ref type="bibr" coords="1,429.12,547.49,12.45,8.97" target="#b11">12,</ref><ref type="bibr" coords="1,444.07,547.49,11.83,8.97" target="#b12">13]</ref>.</p><p>This year was the first time we participated in the bilingual task. Therefore our experiments were rather modest, the main purpose being to establish a reasonable base line for English-to-Dutch and English-to-German retrieval. We used a simple dictionary-based approach to query translation, where all possible translations of a phrase or word are considered and no attempts to disambiguate the query were made.</p><p>One of the main goals of our participation in the GIRT and Amaryllis tasks was to experiment with the keywords used in the collections. Many domain-specific collections, such as the scientific collections of GIRT and Amaryllis, contain keywords. Our strategy for CLEF 2002 was to compute the similarity of keywords based on their occurrence in the collection, and explore whether the resulting keyword space can be used to improve retrieval effectiveness.</p><p>The paper is organized as follows. In Section 2 we describe the FlexIR system as well as the approaches used for each of the tasks in which participated. Section 3 describes our official runs for CLEF 2002, and in Section 4 we discuss the results we have obtained. Finally, in Section 5 we offer some conclusions regarding our document retrieval efforts.</p><p>All submitted runs used FlexIR, an information retrieval system developed by the first author. The main goal underlying FlexIR's design is to facilitate flexible experimentation with a wide variety of retrieval components and techniques. FlexIR is implemented in Perl; as it is built around the standard UNIX pipeline architecture, and supports many types of preprocessing, scoring, indexing, and retrieval tools, which proved to be a major asset for the wide variety of tasks in which we took part this year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Approach</head><p>The retrieval model underlying FlexIR is the standard vector space model. All our official mono-and bilingual runs for CLEF 2002 used the Lnu.ltc weighting scheme <ref type="bibr" coords="2,354.77,241.26,11.62,8.97" target="#b1">[2]</ref> to compute the similarity between a query and a document. For the experiments on which we report in this note, we fixed slope at either 0.1 or 0.2; the pivot was set to the average number of unique words per document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Morphological Normalization</head><p>Previous retrieval experiments <ref type="bibr" coords="2,212.32,312.37,11.62,8.97" target="#b8">[9]</ref> in English have not demonstrated that morphological normalization such as rule-based stemming <ref type="bibr" coords="2,189.36,324.32,16.60,8.97" target="#b16">[17]</ref> or lexical stemming <ref type="bibr" coords="2,293.65,324.32,16.60,8.97" target="#b10">[11]</ref> consistently yields significant improvements. As to the effect of stemming on retrieval performance for languages that are morphologically richer than English, such as Dutch, German, or Italian, in our experiments for CLEF 2001 we consistently found that morphological normalization does improve retrieval effectiveness <ref type="bibr" coords="2,353.62,360.19,15.27,8.97" target="#b13">[14]</ref>.</p><p>Stemming/Lemmatizing. For this year's monolingual experiments the aim was to improve our existing morphological analysis for languages that we had dealt with before (i.e, Dutch, German, and Italian), and to extend it to languages that we had not dealt with before (i.e., Finnish, French, Spanish, and Swedish). Where available we tried to use a lexical-based stemmer, or lemmatizer: for French, German, and Italian we used lemmatizers that are part of TreeTagger <ref type="bibr" coords="2,292.33,434.53,15.27,8.97" target="#b18">[19]</ref>. For Dutch we used a Porter stemmer developed within the Uplift project <ref type="bibr" coords="2,188.90,446.49,15.49,8.97" target="#b20">[21]</ref>; for Spanish we also used a version of Porter's stemmer <ref type="bibr" coords="2,431.51,446.49,10.58,8.97" target="#b2">[3]</ref>. We did not have access to (linguistically informed) morphological normalization tools for Finnish or Swedish.</p><p>For the GIRT and Amaryllis task, we used TreeTagger for processing the main text. The keywords, i.e., GIRT's controlled-terms and Amaryllis' controlled vocabulary, were indexed as given, indexing the keywords or keyword-phrases as a single token.</p><p>Compound splitting. For Dutch and German, we applied a compound splitter to analyze complex words, such as, Autobahnraststätte (English: highway restaurant), Menschenrechte (English: human rights), Friedensvertrag (English: peace agreement), etc. In addition to these noun-noun compounds, there are several other forms of compounding, including verb-noun (e.g., German: Tankstelle, English: gas station), verbverb (e.g., German: spazierengehen, English: taking a walk), noun-adjective (e.g., German: arbeitslos, English: unemployed), adjective-verb (e.g., German: sicherstellen, English: to secure); etc., see <ref type="bibr" coords="2,486.43,580.61,11.62,8.97" target="#b5">[6]</ref> for a more detailed overview. In last year's participation we focused on noun-noun compound splitting, but this year we tried to cover the other forms for German as well. This resulted in a much larger compound dictionary for German. Whereas last year's dictionary contained 108,489 entries, it grew up to 772,667 for this year's participation. An entry in the compound dictionary consists of a complex word and its parts, where each part is lemmatized. See <ref type="bibr" coords="2,233.63,640.39,16.60,8.97" target="#b13">[14]</ref> for further details on the actual process of compound splitting.</p><p>For retrieval purposes, each document in the collection is analyzed and if a compound is identified, both the compound and all of its parts all of its parts are added to the document. Compounds occurring in a query are analyzed in a similar way: the parts are simply added to the query. Since we expand both the documents and the queries with compound parts, there is no need for compound formation <ref type="bibr" coords="2,453.30,688.21,15.27,8.97" target="#b15">[16]</ref>.</p><p>Ngrams. To obtain a zero-knowledge language independent approach to morphological normalization, we implemented an ngram-based method in addition to a linguistically informed methods. For each of the seven non-English languages in the monolingual task we determined the average word length, and set the ngram-length to be 1 less than the average word length, except for Finnish, where we set the ngram-length to be 2 less than the average word length; see Table <ref type="table" coords="3,385.60,206.60,4.98,8.97" target="#tab_0">1</ref> for the details. For each word we stored both the word itself and all possible ngrams that can be obtained from it without crossing word boundaries. For instance, the Dutch version of Topic 108 contains the phrase maatschappelijke gevolgen (English: societal consequences); using ngrams of length 5, this becomes: maatschappelijke maats aatsc atsch tscha schap chapp happe appel ppeli pelij elijk lijke gevolgen gevol evolg volge olgen</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dutch Finnish French German Italian</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Blind Feedback</head><p>Blind feedback was applied to expand the original query with related terms. Term weights were recomputed by using the standard Rocchio method <ref type="bibr" coords="3,245.76,329.28,15.27,8.97" target="#b17">[18]</ref>, where we considered the top 10 documents to be relevant and the bottom 500 documents to be non-relevant. We allowed at most 20 terms to be added to the original query. For Dutch and German, the added words are also decompounded, and the complex words and their parts are added to the query.</p><p>The text runs for the GIRT and Amaryllis tasks used blind feedback, while it was switched off for the keyword runs. To aid comparison with the monolingual runs, the same feedback settings were used. There is a remarkable difference in the effect of feedback: virtually no words are added for the GIRT and Amaryllis tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Combined Runs</head><p>In addition to our morphological interests we also wanted to experiment with combinations of (what we believed to be) different kinds runs of runs in an attempt to determine their impact on retrieval effectiveness. More specifically, for each of the languages for which we had access to language specific morphological normalization tools (i.e., stemmers or lemmatizers), we created a base run using those tools. In addition, we used ngrams in the manner described above to create a second base run. We then combined these two base runs in the following manner. First, we normalized the retrieval status values (RSVs), since different runs may have radically different RSVs. For each run we re-ranked these values in [0.5, 1.0] using:</p><formula xml:id="formula_0" coords="3,239.64,546.97,122.02,23.48">RSV i = 0.5 + 0.5 • RSV i -min i max i -min i</formula><p>and assigned all documents not occurring in the top 1000, the value 0.5; this is a variation of the Min Max -Norm considered in <ref type="bibr" coords="3,174.08,587.54,15.27,8.97" target="#b12">[13]</ref>. Next, we assigned new weights to the documents using a linear interpolation factor λ representing the relative weight of a run:</p><formula xml:id="formula_1" coords="3,231.50,613.86,140.01,13.16">RSV new = λ • RSV 1 + (1 -λ) • RSV 2 .</formula><p>For λ = 0.5 this is similar to the simple (but effective) summation function used by Fox and Shaw <ref type="bibr" coords="3,482.03,634.73,10.58,8.97" target="#b7">[8]</ref>, and later by Belkin et al. <ref type="bibr" coords="3,174.46,646.69,11.62,8.97" target="#b0">[1]</ref> and Lee <ref type="bibr" coords="3,223.39,646.69,15.77,8.97" target="#b11">[12,</ref><ref type="bibr" coords="3,241.83,646.69,11.83,8.97" target="#b12">13]</ref>. The interpolation factors λ were obtained from experiments on the CLEF 2000 and 2001 data sets (whenever available).</p><p>For the GIRT and Amaryllis task, we created alternative base runs based on the usage of the keywords in the collection, and combined these with the text-based runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Runs</head><p>We submitted a total of 27 runs: 10 for the monolingual task, 7 for the bilingual task, and 5 each for the GIRT and Amaryllis tasks. Below we discuss our runs in some detail.  <ref type="table" coords="4,114.96,249.72,3.88,8.97">2</ref>: Overview of the monolingual runs submitted. For combined runs column 3 gives the base runs that were combined, and column 4 gives the interpolation factor λ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Monolingual Runs</head><p>All our monolingual runs used the title and description fields of the topics. Table <ref type="table" coords="4,419.41,312.40,4.98,8.97">2</ref> provides an overview of the runs that we submitted for the monolingual task. The third column in Table <ref type="table" coords="4,423.67,324.36,4.98,8.97">2</ref> indicates the type of run:</p><p>• Morphological -topic and document words are lemmatized and compounds are split (Dutch, German), using the morphological tools described in Section 2.</p><p>• Ngram -both topic and document words are ngram-ed, using the settings discussed in Section 2.</p><p>• Combined -two base runs are combined, an ngram run and a morphological run, using the interpolation factor λ given in the fourth column.</p><p>Both topics and documents were stopped. First of all, for each language we used a stop phrase list containing phrases such as 'Find documents that discuss . . . '; stop phrases were automatically removed from the topics. We then stopped both topics and documents using the same stop word list. We determined the 400 most frequent words, then removed from this list content words that we felt might be important despite their high frequency. For instance, in most of the document collections terms such as 'Europe' and 'dollar' occur with high frequency. We did not use a stop ngram list, but in our ngram runs we first used a stop word list, and then ngram-ed the topics and documents. For the ngram runs we did not replace diacritic letters by their non-diacritic counterparts, for the morphological runs we did.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Bilingual Task</head><p>We submitted a total of 7 bilingual runs, using English as the topic language, and Dutch and German as document languages. For the bilingual runs, we followed a dictionary-based approach. The translations of the words and phrases of the topic are simply added to the query in an unstructured way; see <ref type="bibr" coords="4,376.10,744.54,16.60,8.97" target="#b14">[15]</ref> for a more elaborated way of query formulation. The original queries are translated to Dutch using the Ergane dictionary <ref type="bibr" coords="5,469.66,112.02,10.58,8.97" target="#b6">[7]</ref>, and to German using the Ding dictionary <ref type="bibr" coords="5,231.51,123.97,10.58,8.97" target="#b4">[5]</ref> Since the Ergane dictionary is rather small, we used a pattern-based approach to extend the translation dictionary with additional translation pairs. Table <ref type="table" coords="5,300.27,405.72,4.98,8.97" target="#tab_3">4</ref> shows some of the patterns. Notice that the vast majority of the words that match one or more of these patterns are words that are derived from Latin. If an English word was not in the Ergane dictionary each matching pattern was applied and all translations were added to the query. Of course, this rather ad-hoc approach to translation is far from perfect. For instance, privatization will be translated as privatisering (correct), by applying pattern <ref type="bibr" coords="5,434.43,453.54,10.58,8.97" target="#b4">(5)</ref>, and privatisatie (incorrect), by applying pattern <ref type="bibr" coords="5,220.75,465.49,10.58,8.97" target="#b5">(6)</ref>. Although this is unacceptable for machine translation applications, those erroneous translations have virtually no impact on retrieval effectiveness, because almost all of them are non-existing words that do not occur in the inverted index anyway. Just like our Dutch and German monolingual runs, we prepared morphological and ngram-based runs, and combined these in order to improve effectiveness; see Table <ref type="table" coords="5,347.22,513.31,4.98,8.97" target="#tab_2">3</ref> for the details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The GIRT and Amaryllis Tasks</head><p>As pointed out in Section 1, our strategy for the GIRT and Amaryllis tasks in CLEF 2002 was to compute the similarity of keywords based on their occurrence in the collection, and investigate whether the resulting keyword space can be used to improve retrieval effectiveness. We assumed that keywords that are frequently assigned to the same documents, will have similar meaning. We determined the number of occurrence of keywords and of co-occurrences of pairs of keywords used in the collection, and used these to define a distance metric. Specifically, we used the Jaccard similarity coefficient on the log of (co)occurrences, and used 1 minus the Jaccard score as a distance metric <ref type="bibr" coords="5,389.41,632.24,15.27,8.97" target="#b9">[10]</ref>. For creating manageable size vectors for each of the keywords, we reduced the matrix using metric multi-dimensional scaling techniques <ref type="bibr" coords="5,118.95,656.15,10.58,8.97" target="#b3">[4]</ref>. For all calculations we used the best approximation of the distance matrix on 10 dimensions. This resulted in a 10-dimensional vector for each of the 6745 keywords occurring in the GIRT collection. The Amaryllis collection uses a much richer set of 125360 keywords, which we reduced by selecting the most frequent ones; this resulted in vectors for 10274 keywords occurring ≥ 25 times in the collection. For our official CLEF runs we experimented with these keywords spaces for two specific purposes: keyword recovery and for document re-ranking.</p><p>We used the following strategy for determining vectors for the documents and for the topics: we took the top 10 documents from a base run (not using the keywords). For each of these documents we collected the keywords, and determined a document vector by taking the mean of the keyword vectors. Next, we determined a vector for the topic by taking the weighted mean of the vectors for the top 10 documents. For document re-ranking, we simply re-ranked the documents retrieved in the base run by the distance between the document and topic vectors. For keyword recovery, we considered the keywords used in the top 10 document, and selected the ten keywords that are closest to the topics vector. Table <ref type="table" coords="6,219.03,346.13,3.88,8.97" target="#tab_4">5</ref>: Keywords for the GIRT and Amaryllis tasks.</p><p>For the Amaryllis task, we can compare the provided topic-keywords in the narrative-field (shown in Table 5(b)), with the topic-keywords resulting from our automatic keyword recovery (shown in Table <ref type="table" coords="6,491.15,381.99,3.64,8.97" target="#tab_4">5</ref>(c)).</p><p>The recovered keywords are subsequently used in a keyword-only run.</p><p>For the GIRT task, we submitted three monolingual runs and two bilingual (English to German) runs. All our GIRT runs use the title and description fields of the topics. The morphological base run mimics the settings of our monolingual morphological base run for German. Based on the top 10 documents from the base run, we use the keyword space for recovering keywords for the topics as discussed above. The topic vector based on the top 10 documents of the base run is also used for re-ranking the documents retrieved in our base run. Experimentation on topics of CLEF 2000 and CLEF 2001 revealed that the keyword and re-rank runs perform worse than the base text run, yet a combination of the base run and either a keyword or a re-rank run helps to improve the performance.</p><p>Our runs for the bilingual GIRT task (English topics) used the translation method of the German bilingual task (using the ding dictionary) for translation of the title and description fields. For the rest, the bilingual tasks mimics the monolingual runs. We made a base morphological run, and recovered keywords for a keyword-only run, and a document re-ranking.</p><p>For the monolingual Amaryllis task, we submitted three monolingual runs and two bilingual (English to French) runs. Our morphological base run uses the same settings as the monolingual French run. For the keyword-only run, keywords were taken from the narrative fields of the topics. For the bilingual Amaryllis task, we used Systran <ref type="bibr" coords="7,178.62,112.02,16.60,8.97" target="#b19">[20]</ref> to translate the title and description fields of the English topics. We did not use the provided English keywords, nor the special dictionary provided. We made a morphological base run (similar to the monolingual task), and collected the keywords from the top 10 documents, which were then used for determining a document re-ranking and for keyword recovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>This section summarizes the results of our CLEF 2002 submissions. We were somewhat surprised by the low scores of our morphological run for Dutch (0.3673) and of the ngram run for Italian (0.3672). The former is probably due to the fact that we used a reasonably crude stemmer, instead of a proper lemmatizer; the latter may be due to the fact that we did not replace diacritic characters by the corresponding non-diacritic letters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Monolingual Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recall</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Precision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Morphological Ngram Combined</head><p>Italian Spanish Figure <ref type="figure" coords="7,119.45,724.97,3.88,8.97">1</ref>: 11pt interpolated avg. precision for all combined monolingual runs, and the underlying base runs.</p><p>Observe that for all languages for which we submitted combined runs, the combined run outperforms the underlying base runs; in some cases the differences don't seem to be significant, but in others they do. Figure <ref type="figure" coords="8,137.93,135.93,4.98,8.97">1</ref> displays the interpolated precision-recall curves for all languages for which we submitted combined runs. The superior performance of the combined runs can again be observed here. Several authors have proposed the following rationale for combining (high quality) runs: one should maximize the overlap of relevant documents between base runs, while minimizing the overlap of non-relevant documents. Lee <ref type="bibr" coords="8,107.65,183.75,16.60,8.97" target="#b11">[12]</ref> proposed the following coefficients R overlap and N overlap for determining the overlap between two runs run 1 and run 2 :</p><formula xml:id="formula_2" coords="8,172.58,212.99,257.85,23.48">R overlap = R common × 2 R 1 + R 2 N overlap = N common × 2 N 1 + N 2 ,</formula><p>where R common (N common ) is the number of common relevant (non-relevant) documents, and R i (N i ) is the number of relevant (non-relevant) documents in run i . (A document is relevant if, and only if, it receives relevance score equal to 1 in the qrels provided by CLEF.) Table <ref type="table" coords="8,354.35,266.68,4.98,8.97">8</ref> shows the overlap coefficients for the base runs used to produce combined runs.</p><p>Dutch French German Italian Spanish R overlap 0.9359 0.9606 0.9207 0.9021 0.9172 N overlap 0.4112 0.5187 0.4180 0.4510 0.5264 Table <ref type="table" coords="8,115.34,347.15,3.88,8.97">8</ref>: Degree of overlap among relevant and non-relevant documents for the base runs used to form the combined ngram/morphological runs for the monolingual task. The coefficients are computed over all topics.</p><p>A few comments are in order. First, for French and Spanish the base runs are of similar (high) quality, but because the N overlap coefficient is high, the combinations do not improve all that much. Furthermore, we conjecture that the reason for the limited gains of the combined runs over the best base runs for Dutch and Italian is due to the somewhat low quality of one of the base runs for these languages. Finally, the significant improvement obtained by combining the two German base runs may be explained as follows: both base runs are high quality runs, their R overlap coefficient is high, and their N overlap is fairly lowunder these circumstances, Lee's rationale predicts that the combined run is of high quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Bilingual Results</head><p>After we had received our results from CLEF, it emerged that one of the base runs submitted the for English to German task (UAmsC02EnGeLC2F) was not the correct one. As a consequence, the combinations in which this base run was used were also incorrect (UAmsC02EnGeNGiMO and UAmsC02EnGeMOiMO). The results and figures below have been obtained with the correct version of UAmsC02EnGeLC2F, using the qrels provided by CLEF.</p><p>To begin with, Table <ref type="table" coords="8,188.77,574.35,4.98,8.97" target="#tab_9">9</ref> shows our non-interpolated average precision scores for both bilingual sub task: English to Dutch and English to German. If we consider the decrease in retrieval effectiveness between monolingual and bilingual, we can observe a significant difference between Dutch and German. It is very likely that this is due to the difference in size between the translation dictionaries that were used to formulate the target queries: the Dutch translation dictionary contained 15,103 head words plus translation rules, whereas the German dictionary contained 103,041 head words; see Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English to Dutch English to</head><p>As with the monolingual runs, we also analyzed the overlap coefficients for base runs that were combined; see Table <ref type="table" coords="9,156.21,552.69,8.30,8.97" target="#tab_11">11</ref>. The gains in effectiveness of the combination over the best base runs is consistent with the coefficients, with comparable gains for the ngram/morphological combinations for Dutch and German; note that both have a fairly low N overlap coefficient. The two (German) morphological runs share many non-relevant documents, and as consequence the combination of these two runs is less effective that the combination of the ngram run with the morphological 1 run.  The results for the GIRT tasks are outright disappointing. Our morphological base run fails to life up to the performance of the corresponding monolingual German runs (average precision 0.1639 for GIRT versus 0.4476 for German). On our pre-submission experiments on the GIRT topics of CLEF 2000 and CLEF 2001, we also noticed a drop in performance, but far less dramatic as for the CLEF 2002 run (average precision around 0.31 for both runs versus 0.1639 this year). Still, the combination of the morphological run with either the keyword run or re-rank run improves retrieval effectiveness. For the English to German GIRT task, only the combination of the morphological and re-rank base runs improves compared to the base runs; this may be due to the extremely low precision at 10 of the bilingual base run (0.1417).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English to</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results for the GIRT and Amaryllis Tasks</head><p>Our runs for Amaryllis are more in line with the results for the monolingual French task (average precision 0.2681 for the base run versus 0.4063 for French). The keyword-only run using the provided keywords even out-performs the morphological base run. The combination of the two runs leads to an impressive improvement of retrieval effectiveness (+26.7%). The English to French Amaryllis task performs fairly well compared to the monolingual Amaryllis task. The combination runs of the morphological base run with the recovered keywords, and of the morphological base run with the re-ranking show significant improvement.</p><p>Figure <ref type="figure" coords="10,134.36,480.09,4.98,8.97">3</ref> contains precision-recall plots for the GIRT and Amaryllis tasks. In addition to the scores for our submitted runs, the figure also plots the scores for the base runs that were used to generate the combined runs.</p><p>GIRT (mono) GIRT (bi) Amaryllis (mono) Amaryllis (bi) R overlap 0.4493 0.2984 0.6586 0.6506 N overlap 0.1031 0.0756 0.1236 0.1301 Table <ref type="table" coords="10,114.17,570.88,8.49,8.97" target="#tab_2">13</ref>: Degree of overlap among relevant and non-relevant documents for the base runs used to form the combined morphological/keyword runs for the GIRT and Amaryllis tasks. The coefficients are computed over all topics.</p><p>As above, we analyzed the overlap coefficients for base runs that were combined; see Table <ref type="table" coords="10,483.97,619.93,8.30,8.97" target="#tab_2">13</ref>. As expected, gains in effectiveness are due to a high R overlap coefficient combined with a relatively low N overlap coefficient. This gives some rationale for why the combination of a baserun with a much lower quality run can improve retrieval effectiveness. It is interesting to note that the coefficients for the combined monolingual Amaryllis (using the provided keywords) are similar to those of the bilingual (using the recovered keywords).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>The experiments on which we report in this note indicate a number of things. First, morphological normalization does improve retrieval effectiveness significantly, especially for languages such as Dutch and Amaryllis monolingual Amaryllis bilingual Figure <ref type="figure" coords="11,118.85,456.92,3.88,8.97">3</ref>: 11pt interpolated avg. precision for all submitted GIRT and Amaryllis runs, and the underlying base runs.</p><p>German, that have a more complex morphology than English. We also showed that ngram-based retrieval can be a viable option in the absence of linguistic resources to support deep morphological normalization. Furthermore, combining runs provides a method that can consistently improve base runs, even high quality base runs; moreover, the interpolation factors required for the best gain in performance seem to be fairly robust across topics. Finally, our results for the bilingual task indicate that simple word/phrase translation, where all possible translations are used to formulate the target query in an unstructured way, leads to a significant decrease in effectiveness, when compared to the respective monolingual runs. Therefore, we are planning to investigate more restrictive ways of formulating target queries.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,123.35,110.43,356.30,52.62"><head>Table 1 :</head><label>1</label><figDesc>Average word length and ngram length used for the ngram base runs.</figDesc><table coords="3,403.48,110.43,76.17,8.97"><row><cell>Spanish Swedish</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,112.43,605.24,378.14,112.40"><head>Table 3 :</head><label>3</label><figDesc>Overview of the bilingual runs submitted.</figDesc><table coords="4,112.43,605.24,378.14,93.05"><row><cell></cell><cell>Topics Documents</cell><cell>Type</cell><cell>Factor</cell></row><row><cell>UAmsC02EnDuMorph</cell><cell>English Dutch</cell><cell>Morphological</cell><cell>-</cell></row><row><cell cols="2">UAmsC02EnDuNGiMO English Dutch</cell><cell>Ngram/Morphological</cell><cell>0.71</cell></row><row><cell>UAmsC02EnDuNGram</cell><cell>English Dutch</cell><cell>Ngram</cell><cell>-</cell></row><row><cell>UAmsC02EnGeLC2F</cell><cell>English German</cell><cell>Morphological 1</cell><cell>-</cell></row><row><cell cols="2">UAmsC02EnGeMOiMO English German</cell><cell>Morphological/Morphological 2</cell><cell>0.50</cell></row><row><cell cols="2">UAmsC02EnGeNGiMO English German</cell><cell>Ngram/Morphological 1</cell><cell>0.285</cell></row><row><cell>UAmsC02EnGeNGram</cell><cell>English German</cell><cell>Ngram</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,90.00,123.97,423.00,254.84"><head>Table 4 :</head><label>4</label><figDesc>, version 1.1. The Ergane dictionary contains 15,103 English head words and 45,068 translation pairs in total. The Ding dictionary contains 103,041 English head words and 145,255 translation pairs in total. Patterns to extend the English-Dutch dictionary.</figDesc><table coords="5,161.89,170.37,279.21,189.11"><row><cell></cell><cell cols="2">Example Translation Pairs</cell></row><row><cell>Patterns</cell><cell>English</cell><cell>Dutch</cell></row><row><cell>(1) s/acy$/atie/</cell><cell>democracy</cell><cell>democratie</cell></row><row><cell>(2) s/ency$/entie/</cell><cell>urgency</cell><cell>urgentie</cell></row><row><cell>(3) s/ency$/ens/</cell><cell>tendency</cell><cell>tendens</cell></row><row><cell cols="3">(4) s/([aeiou])ssion$/$1ssie/ commission commissie</cell></row><row><cell>(5) s/zation$/sering/</cell><cell cols="2">privatization privatisering</cell></row><row><cell>(6) s/zation$/satie/</cell><cell>realization</cell><cell>realisatie</cell></row><row><cell>(7) s/ation$/atie/</cell><cell>relation</cell><cell>relatie</cell></row><row><cell>(8) s/ical$/isch/</cell><cell>medical</cell><cell>medisch</cell></row><row><cell>(9) s/ical$/iek/</cell><cell>identical</cell><cell>identiek</cell></row><row><cell>(10) s/idal$/idaal/</cell><cell>suicidal</cell><cell>suicidaal</cell></row><row><cell>(11) s/ic$/iek/</cell><cell>specific</cell><cell>specifiek</cell></row><row><cell>(12) s/([gmr])y$/$1ie/</cell><cell>industry</cell><cell>industrie</cell></row><row><cell>(13) s/ˆty$/teit/</cell><cell>university</cell><cell>universiteit</cell></row><row><cell>(14) s/ism$/isme/</cell><cell>realism</cell><cell>realisme</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,90.00,159.84,423.00,175.90"><head>Table 5</head><label>5</label><figDesc>(a)  shows the keywords recovered for GIRT topic 51.</figDesc><table coords="6,101.96,195.27,385.05,140.47"><row><cell>Selbstbewußtsein</cell><cell>Concentration et toxicité des polluants</cell><cell>Qualité air</cell></row><row><cell>familiale Sozialisation</cell><cell>Mécanisme de formation des polluants</cell><cell>Moteur diesel</cell></row><row><cell>Junge</cell><cell>Réduction de la pollution</cell><cell>Trafic routier urbain</cell></row><row><cell>Adoleszenz</cell><cell>Choix du carburant</cell><cell>Autobus</cell></row><row><cell>Subkultur</cell><cell>Réglage de la combustion</cell><cell>Azote oxyde</cell></row><row><cell>Erziehungsstil</cell><cell>Traitement des gaz d'échappement</cell><cell>Exposition professionnelle</cell></row><row><cell>soziale Isolation</cell><cell>Législation et réglementation</cell><cell>Véhicule à moteur</cell></row><row><cell>Marginalität</cell><cell></cell><cell>Carburant diesel</cell></row><row><cell>Bewußtseinsbildung</cell><cell></cell><cell>Inventaire source pollution</cell></row><row><cell>Pubertät</cell><cell></cell><cell>Carburant remplacement</cell></row><row><cell>(a) GIRT topic 51</cell><cell>(b) Amaryllis topic 1</cell><cell>(c) Amaryllis topic 1</cell></row><row><cell>(recovered)</cell><cell>(monolingual, given)</cell><cell>(bilingual, recovered)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,100.94,603.72,401.13,148.26"><head>Table 6 :</head><label>6</label><figDesc>Overview of the runs submitted for the GIRT and Amaryllis tasks.</figDesc><table coords="6,100.94,603.72,401.13,128.92"><row><cell></cell><cell>Task</cell><cell>Topics</cell><cell>Documents</cell><cell>Type</cell><cell>Factor</cell></row><row><cell>UAmsC02GeGiTT</cell><cell>GIRT</cell><cell cols="2">German German</cell><cell>Morphological</cell><cell>-</cell></row><row><cell>UAmsC02GeGiTTiKW</cell><cell>GIRT</cell><cell cols="2">German German</cell><cell>Morphological/Keyword</cell><cell>0.70</cell></row><row><cell>UAmsC02GeGiTTiRR</cell><cell>GIRT</cell><cell cols="2">German German</cell><cell>Morphological/Re-rank</cell><cell>0.60</cell></row><row><cell>UAmsC02EnGiTTiKW</cell><cell>GIRT</cell><cell cols="2">English German</cell><cell>Morphological/Keyword</cell><cell>0.70</cell></row><row><cell>UAmsC02EnGiTTiRR</cell><cell>GIRT</cell><cell cols="2">English German</cell><cell>Morphological/Re-rank</cell><cell>0.60</cell></row><row><cell>UAmsC02FrAmTT</cell><cell cols="2">Amaryllis French</cell><cell>French</cell><cell>Morphological</cell><cell>-</cell></row><row><cell>UAmsC02FrAmKW</cell><cell cols="2">Amaryllis French</cell><cell>French</cell><cell>Keyword</cell><cell>-</cell></row><row><cell cols="3">UAmsC02FrAmTTiKW Amaryllis French</cell><cell>French</cell><cell>Morphological/Keyword</cell><cell>0.70</cell></row><row><cell cols="4">UAmsC02EnAmTTiKW Amaryllis English French</cell><cell>Morphological/Keyword</cell><cell>0.70</cell></row><row><cell>UAmsC02EnAmTTiRR</cell><cell cols="3">Amaryllis English French</cell><cell>Morphological/Re-rank</cell><cell>0.60</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="7,90.00,246.99,423.00,104.57"><head>Table 7</head><label>7</label><figDesc>contains our non-interpolated average precision scores for all languages. In addition to the scores for our submitted runs, the table also lists the scores for the base runs that were used to generate the interpolated runs.</figDesc><table coords="7,95.98,294.37,412.47,57.19"><row><cell></cell><cell>Dutch</cell><cell>Finnish</cell><cell>French</cell><cell>German</cell><cell>Italian</cell><cell cols="2">Spanish Swedish</cell></row><row><cell>Morphological</cell><cell>0.3673</cell><cell>-</cell><cell>0.4063</cell><cell>0.4476</cell><cell>0.4285</cell><cell>0.4370</cell><cell>-</cell></row><row><cell>Ngram</cell><cell>0.4542</cell><cell cols="2">0.3034 0.4481</cell><cell>0.4177</cell><cell>0.3672</cell><cell>0.4512</cell><cell>0.4187</cell></row><row><cell cols="2">Combined Ngrm./Mrph. 0.4598</cell><cell>-</cell><cell>0.4535</cell><cell>0.4802</cell><cell>0.4407</cell><cell>0.4734</cell><cell>-</cell></row><row><cell></cell><cell>(+1.2%)</cell><cell></cell><cell cols="4">(+1.2%) (+7.3%) (+2.8%) (+4.9%)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="7,90.00,361.94,423.00,32.88"><head>Table 7 :</head><label>7</label><figDesc>Overview of non-interpolated average precision scores for all submitted monolingual runs and for the underlying base runs. Best scores are in boldface; base runs that were not submitted are in italics. The figures in brackets indicate the improvement of the combined run over the best underlying base run.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="8,90.00,607.56,423.00,147.04"><head>Table 9 :</head><label>9</label><figDesc>Overview of non-interpolated average precision scores for all correct bilingual runs. Best scores are in boldface. The figures in brackets indicate the improvement of the combined run over the best underlying base run.For English to Dutch, we submitted one morphological run, where both stemming and compound splitting was applied. For English to German, we created two morphological runs, one with a large decompounding For both target languages we also submitted one n-gram run. In addition, we combined the n-gram run with the morphological run for both languages, and for German we also combined both morphological runs.Table10shows the decrease in effectiveness compared to the best monolingual run for the respective target language.</figDesc><table coords="8,413.19,607.56,32.65,8.97"><row><cell>German</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="9,194.02,457.04,214.95,8.97"><head>Table 10 :</head><label>10</label><figDesc>Decrease in effectiveness for bilingual runs.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="9,90.00,624.21,423.00,80.13"><head>Table 11 :</head><label>11</label><figDesc>Degree of overlap among relevant and non-relevant documents for the base runs used to form the combined bilingual runs. The coefficients are computed over all topics.</figDesc><table coords="9,102.71,624.21,397.58,50.36"><row><cell>Dutch</cell><cell>English to German</cell><cell>English to German</cell></row><row><cell cols="3">Ngram/Morphological Ngram/Morphological 1 Morphological 1/Morphological 2</cell></row><row><cell>R overlap 0.7737</cell><cell>0.7898</cell><cell>0.9338</cell></row><row><cell>N overlap 0.2516</cell><cell>0.3588</cell><cell>0.5853</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="10,90.00,130.70,423.00,114.72"><head>Table 12</head><label>12</label><figDesc>contains our non-interpolated average precision scores for the GIRT and Amaryllis tasks. In addition to the scores for our submitted runs, the table also lists the scores for the base runs that were used to generate the combined runs.</figDesc><table coords="10,96.82,176.28,409.35,69.14"><row><cell></cell><cell>GIRT (mono)</cell><cell>GIRT (bi)</cell><cell>Amaryllis (mono)</cell><cell>Amaryllis (bi)</cell></row><row><cell>Morphological</cell><cell>0.1639</cell><cell>0.0666</cell><cell>0.2681</cell><cell>0.2325</cell></row><row><cell>Keyword</cell><cell>0.0349</cell><cell>0.0210</cell><cell>0.2684</cell><cell>0.0890</cell></row><row><cell>Re-rank</cell><cell>0.1015</cell><cell>0.0405</cell><cell>-</cell><cell>0.1029</cell></row><row><cell cols="2">Combined Mrph./KW 0.1687 (+2.9%)</cell><cell cols="3">0.0620 (-6.9%) 0.3401 (+26.7%) 0.2660 (+14.4%)</cell></row><row><cell>Combined Mrph./RR</cell><cell cols="3">0.1906 (+16.3%) 0.0704 (+5.7%) -</cell><cell>0.2537 (+9.1%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="10,90.00,255.80,423.00,44.83"><head>Table 12 :</head><label>12</label><figDesc>Overview of non-interpolated average precision scores for all submitted GIRT and Amaryllis runs, and for the underlying base suns. Best scores are in boldface; base runs that were not submitted are in italics. The figures in brackets indicate the improvement of the combined run over the best underlying base run.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We want to thank <rs type="person">Willem van Hage</rs> and <rs type="person">Vera Hollink</rs> for their technical support, and <rs type="person">Maarten Marx</rs> for useful discussions.</p><p><rs type="person">Jaap Kamps</rs> was supported by the <rs type="funder">Netherlands Organization for Scientific Research (NWO</rs>, grant # <rs type="grantNumber">400-20-036</rs>). <rs type="person">Christof Monz</rs> was supported by the <rs type="funder">Physical Sciences Council</rs> with financial support from the <rs type="funder">Netherlands Organization for Scientific Research (NWO)</rs>, project <rs type="grantNumber">612-13-001</rs>. Maarten de Rijke was supported by grants from the <rs type="funder">Netherlands Organization for Scientific Research (NWO)</rs>, under project numbers <rs type="grantNumber">612-13-001</rs>, <rs type="grantNumber">365-20-005</rs>, <rs type="grantNumber">612.069.006</rs>, <rs type="grantNumber">612.000.106</rs>, <rs type="grantNumber">220-80-001</rs>, and <rs type="grantNumber">612.000.207</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_YW2NdWa">
					<idno type="grant-number">400-20-036</idno>
				</org>
				<org type="funding" xml:id="_kVax6YR">
					<idno type="grant-number">612-13-001</idno>
				</org>
				<org type="funding" xml:id="_sA44T46">
					<idno type="grant-number">612-13-001</idno>
				</org>
				<org type="funding" xml:id="_QaANksU">
					<idno type="grant-number">365-20-005</idno>
				</org>
				<org type="funding" xml:id="_h7XjFgr">
					<idno type="grant-number">612.069.006</idno>
				</org>
				<org type="funding" xml:id="_eZewRA8">
					<idno type="grant-number">612.000.106</idno>
				</org>
				<org type="funding" xml:id="_JBG5JYP">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_vFx8rXY">
					<idno type="grant-number">612.000.207</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,111.58,134.29,401.42,8.97;12,111.58,146.24,374.34,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,310.67,134.29,202.33,8.97;12,111.58,146.24,117.17,8.97">Combining evidence of multiple query representations for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kantor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,235.86,146.24,156.02,8.97">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="431" to="448" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.58,165.02,401.42,8.97;12,111.58,176.98,401.42,8.97;12,111.58,188.93,140.30,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,266.79,165.02,198.05,8.97">New retrieval approaches using SMART: TREC 4</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,164.70,176.98,259.87,8.97">Proceedings of the Fourth Text REtrieval Conference (TREC-4)</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Fourth Text REtrieval Conference (TREC-4)</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="500" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.58,207.71,342.55,8.97" xml:id="b2">
	<monogr>
		<ptr target="http://www.unine.ch/info/clef" />
		<title level="m" coord="12,111.58,207.71,64.47,8.97">CLEF resources</title>
		<imprint>
			<publisher>University of Neuchâtel</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.58,226.49,366.50,8.97" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="12,221.01,226.49,101.12,8.97">Multidimensional Scaling</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">F</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A A</forename><surname>Cox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Chapman &amp; Hall</publisher>
			<pubPlace>London UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.58,245.27,360.15,8.97" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><surname>Ding</surname></persName>
		</author>
		<ptr target="http://www-user.tu-chemnitz.de/˜fri/ding/" />
		<title level="m" coord="12,137.36,245.27,114.45,8.97">A dictionary lookup program</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.58,264.05,401.42,8.97;12,111.58,276.00,55.07,8.97" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="12,206.78,264.05,216.76,8.97">Duden: Grammatik der deutschen Gegenwartssprache</title>
		<editor>G. Drosdowski</editor>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Dudenverlag</publisher>
		</imprint>
	</monogr>
	<note>fourth edition</note>
</biblStruct>

<biblStruct coords="12,111.58,294.78,401.43,8.97;12,111.58,307.99,74.12,7.04" xml:id="b6">
	<monogr>
		<ptr target="http://download.travlang.com/Ergane/frames-en.html" />
		<title level="m" coord="12,111.58,294.78,206.47,8.97">Ergane: a free multi-lingual dictionary programme</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.58,325.51,401.42,8.97;12,111.58,337.47,22.42,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,211.52,325.51,132.51,8.97">Combination of multiple searches</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,361.66,325.51,81.73,8.97">Proceedings TREC-2</title>
		<meeting>TREC-2</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="243" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.58,356.25,401.42,8.97;12,111.58,368.20,269.34,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,158.91,356.25,84.78,8.97">Stemming algorithms</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Frakes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,422.90,356.25,90.09,8.97;12,111.58,368.20,117.89,8.97">Information Retrieval: Data Strcutures &amp; Algorithms</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Frakes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</editor>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="131" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.58,386.98,401.42,8.97;12,111.58,398.94,125.10,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,234.65,386.98,238.98,8.97">Metric and euclidean properties of dissimilarity coefficients</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Gower</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Legendre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,482.25,386.98,30.75,8.97;12,111.58,398.94,63.72,8.97">Journal of Classification</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="5" to="48" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.58,417.72,401.42,8.97;12,111.58,429.67,60.06,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,164.54,417.72,108.90,8.97">How effective is suffixing?</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,280.88,417.72,228.01,8.97">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="7" to="15" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.58,448.45,401.42,8.97;12,111.58,460.40,22.42,8.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,155.02,448.45,174.81,8.97">Analyses of multiple evidence combination</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,352.45,448.45,88.92,8.97">Proceedings SIGIR&apos;97</title>
		<meeting>SIGIR&apos;97</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="267" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.58,479.18,401.42,8.97;12,111.58,491.14,232.72,8.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,156.25,479.18,294.73,8.97">Combining multiple evidence from different relevant feedback networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,474.81,479.18,38.19,8.97;12,111.58,491.14,138.72,8.97">Database Systems for Advanced Applications</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="421" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.58,509.92,401.42,8.97;12,111.58,521.87,401.42,8.97;12,111.58,533.83,259.84,8.97" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,221.46,509.92,291.54,8.97;12,111.58,521.87,103.68,8.97">Shallow morphological analysis in monolingual information retrieval for Dutch, German and Italian</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,463.64,521.87,49.36,8.97;12,111.58,533.83,24.36,8.97">Proceedings CLEF</title>
		<title level="s" coord="12,163.33,533.83,25.47,8.97">LNCS</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<meeting>CLEF</meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2001">2001. 2002</date>
			<biblScope unit="volume">2406</biblScope>
			<biblScope unit="page" from="262" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.58,552.61,401.42,8.97;12,111.58,564.56,393.70,8.97" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,332.98,552.61,180.02,8.97;12,111.58,564.56,203.23,8.97">Dictionary-based cross-language information retrieval: Problems, methods, and research findings</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pirkola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hedlund</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Keskustalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,322.24,564.56,85.07,8.97">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="209" to="230" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.58,583.34,401.42,8.97;12,111.58,595.30,401.42,8.97;12,111.58,607.25,401.42,8.97" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,226.47,583.34,286.53,8.97;12,111.58,595.30,9.76,8.97">Improving the precision of a text retrieval system with compound analysis</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pohlmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,463.64,595.30,49.36,8.97;12,111.58,607.25,307.62,8.97">Proceedings of the 7th Computational Linguistics in the Netherlands Meeting (CLIN 1996)</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Landsbergen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Odijk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Van Deemter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Veldhuijzen Van Zanten</surname></persName>
		</editor>
		<meeting>the 7th Computational Linguistics in the Netherlands Meeting (CLIN 1996)</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="115" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.58,626.03,307.04,8.97" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,155.30,626.03,129.06,8.97">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,291.74,626.03,33.05,8.97">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.58,644.81,401.42,8.97;12,111.58,656.76,319.22,8.97" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,160.72,644.81,174.33,8.97">Relevance feedback in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rocchio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,425.23,644.81,87.77,8.97;12,111.58,656.76,232.83,8.97">The SMART Retrieval System -Experiments in Automatic Document Processing</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</editor>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.58,675.54,401.42,8.97;12,111.58,687.50,240.96,8.97" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,159.64,675.54,221.79,8.97">Probabilistic part-of-speech tagging using decision trees</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,398.73,675.54,114.27,8.97;12,111.58,687.50,211.85,8.97">Proceedings of International Conference on New Methods in Language Processing</title>
		<meeting>International Conference on New Methods in Language Processing</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.58,706.28,248.14,8.97" xml:id="b19">
	<monogr>
		<ptr target="http://www.systransoft.com/" />
		<title level="m" coord="12,111.58,706.28,100.97,8.97">Systran Online Translator</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,111.58,725.06,401.42,8.97;12,111.58,738.27,73.62,7.04" xml:id="b20">
	<monogr>
		<ptr target="http://www-uilots.let.uu.nl/˜uplift/" />
		<title level="m" coord="12,111.58,725.06,281.07,8.97">UPLIFT: Utrecht project: Linguistic information for free text retrieval</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
