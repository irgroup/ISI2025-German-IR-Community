<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,137.71,146.03,327.56,18.08;1,282.67,167.95,37.66,18.08;1,109.94,189.87,234.96,18.08;1,344.90,188.10,22.11,12.55;1,373.96,189.87,119.08,18.08">Experiments in 8 European Languages with Hummingbird SearchServer TM at CLEF 2002</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2002-08-30">August 30, 2002</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,259.82,224.93,83.37,10.46;1,271.19,238.87,60.64,10.46"><forename type="first">Stephen</forename><surname>Tomlinson Hummingbird</surname></persName>
							<email>stephen.tomlinson@hummingbird.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Ottawa</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,137.71,146.03,327.56,18.08;1,282.67,167.95,37.66,18.08;1,109.94,189.87,234.96,18.08;1,344.90,188.10,22.11,12.55;1,373.96,189.87,119.08,18.08">Experiments in 8 European Languages with Hummingbird SearchServer TM at CLEF 2002</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2002-08-30">August 30, 2002</date>
						</imprint>
					</monogr>
					<idno type="MD5">949F8AB2F51B971F4680CA0776A76E20</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hummingbird submitted ranked result sets for all Monolingual Information Retrieval tasks of the Cross-Language Evaluation Forum (CLEF) 2002. Enabling stemming in SearchServer increased average precision by 16 points in Finnish, 9 points in German, 4 points in Spanish, 3 points in Dutch, 2 points in French and Italian, and 1 point in Swedish and English. Accent-indexing increased average precision by 3 points in Finnish and 2 points in German, but decreased it by 2 points in French and 1 point in Italian and Swedish. Treating apostrophes as word separators increased average precision by 3 points in French and 1 point in Italian. Confidence intervals produced using the bootstrap percentile method were found to be very similar to those produced using the standard method; both were of similar width to rank-based intervals for differences in average precision, but substantially narrower for differences in Precision@10.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Hummingbird SearchServer<ref type="foot" coords="1,208.98,517.14,3.97,7.32" target="#foot_0">1</ref> is an indexing, search and retrieval engine for embedding in Windows and UNIX information applications. SearchServer, originally a product of Fulcrum Technologies, was acquired by Hummingbird in 1999. Founded in 1983 in Ottawa, Canada, Fulcrum produced the first commercial application program interface (API) for writing information retrieval applications, Fulcrum r Ful/Text TM . The SearchServer kernel is embedded in many Hummingbird products, including SearchServer, an application toolkit used for knowledge-intensive applications that require fast access to unstructured information.</p><p>SearchServer supports a variation of the Structured Query Language (SQL), SearchSQL TM , which has extensions for text retrieval. SearchServer conforms to subsets of the Open Database Connectivity (ODBC) interface for C programming language applications and the Java Database Connectivity (JDBC) interface for Java applications. Almost 200 document formats are supported, such as Word, WordPerfect, Excel, PowerPoint, PDF and HTML.</p><p>SearchServer works in Unicode internally <ref type="bibr" coords="1,285.64,661.67,10.52,10.46" target="#b2">[3]</ref> and supports most of the world's major character sets and languages. The major conferences in text retrieval evaluation (CLEF <ref type="bibr" coords="1,431.03,673.63,9.96,10.46" target="#b1">[2]</ref>, NTCIR <ref type="bibr" coords="1,483.33,673.63,10.52,10.46" target="#b4">[5]</ref> and TREC <ref type="bibr" coords="1,121.04,685.59,15.50,10.46" target="#b9">[10]</ref>) have provided opportunities to objectively evaluate SearchServer's support for a dozen languages. This paper focuses on evaluating SearchServer's options for 8 European languages using the CLEF 2002 test collections. 2 Setup</p><p>For the experiments described in this paper, an internal development build of SearchServer 5.3 was used (5.3.500.279).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data</head><p>The CLEF 2002 document sets consisted of tagged (SGML-formatted) news articles (mostly from 1994) in 8 different languages: German, French, Italian, Spanish, Dutch, Swedish, Finnish and English. Table <ref type="table" coords="2,157.74,379.84,4.98,10.46" target="#tab_0">1</ref> gives their sizes. For more information on the CLEF collections, see the CLEF web site <ref type="bibr" coords="2,128.52,391.80,9.96,10.46" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Text Reader</head><p>The custom text reader called cTREC, originally written for handling TREC collections <ref type="bibr" coords="2,494.74,438.08,14.61,10.46" target="#b12">[12]</ref>, handled expansion of the library files of the CLEF collections and was extended to support the CLEF guidelines of only indexing specific fields of specific documents. The entities described in the DTD files were also converted, e.g. "&amp;equals;" was converted to the equal sign "=". The documents were assumed to be in the Latin-1 character set, the code page which, for example, assigns e-acute (é) hexadecimal 0xe9 or decimal 233. cTREC passes through the Latin-1 characters, i.e. does not convert them to Unicode. SearchServer's Translation Text Reader (nti), was chained on top of cTREC and the Win 1252 UCS2 translation was specified via its /t option to translate from Latin-1 to the Unicode character set desired by SearchServer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Indexing</head><p>A separate SearchServer table was created for each language, created with a SearchSQL statement such as the following: The TABLE LANGUAGE parameter specifies which language to use when performing stemming operations at index time. The STOPFILE parameter specifies a stop file containing typically a couple hundred stop words to not index; the stop file also contains instructions on changes to the default indexing rules, for example, to enable accent-indexing, or to change the apostrophe to a word separator. Here are the first few lines of the stop file used for the French task: IAC = "\u0300-\u0345" PST = "''" STOPLIST = a à afin</p><formula xml:id="formula_0" coords="2,100.46,611.88,151.68,10.46">CREATE SCHEMA CLEF02DE CREATE</formula><p>The IAC line enables indexing of the specified accents (Unicode combining diacritical marks 0x0300-0x0345). Accent-indexing was enabled for all runs except the Italian and English runs. Accents were known to be specified in the Italian queries but were not consistently used in the Italian documents. The PST line adds the specified characters (apostrophes in this case) to the list of word separators. The apostrophes were changed to word separators except for English runs.</p><p>Into each table, we just needed to insert one row, specifying the top directory of the library files for the language, using an Insert statement such as the following:</p><p>INSERT INTO CLEF02DE ( FT_SFNAME, FT_FLIST ) VALUES ('German','cTREC/E/d=128:s!nti/t=Win_1252_UCS2:cTREC/C/@:s');</p><p>To index each table, we just executed a Validate Index statement such as the following:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VALIDATE INDEX CLEF02DE VALIDATE TABLE;</head><p>By default, the index supports both exact matching (after some Unicode-based normalizations, such as converting to upper-case and decomposed form) and matching on stems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Search Techniques</head><p>The CLEF organizers created 50 "topics" (numbered 91-140) and translated them into many languages. Each topic contained a "Title" (subject of the topic), "Description" (a one-sentence specification of the information need) and "Narrative" (more detailed guidelines for what a relevant document should or should not contain). The participants were asked to use the Title and Description fields for at least one automatic submission per task this year to facilitate comparison of results.</p><p>We created an ODBC application, called QueryToRankings.c, based on the example stsample.c program included with SearchServer, to parse the CLEF topics files, construct and execute corresponding SearchSQL queries, fetch the top 1000 rows, and write out the rows in the results format requested by CLEF. SELECT statements were issued with the SQLExecDirect api call. Fetches were done with SQLFetch (typically 1000 SQLFetch calls per query).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Intuitive Searching</head><p>For all runs, we used SearchServer's Intuitive Searching, i.e. the IS ABOUT predicate of Search-SQL, which accepts unstructured text. For example, for the German version of topic 41 (from last year), the Title was "Pestizide in Babykost" (Pesticides in Baby Food), and the Description was "Berichte über Pestizide in Babynahrung sind gesucht" (Find reports on pesticides in baby food). A corresponding SearchSQL query would be:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SELECT RELEVANCE('V2:3') AS REL, DOCNO FROM CLEF02DE WHERE FT TEXT IS ABOUT 'Pestizide in Babykost Berichte über Pestizide in Babynahrung sind gesucht' ORDER BY REL DESC;</head><p>This query would create a working table with the 2 columns named in the SELECT clause, a REL column containing the relevance value of the row for the query, and a DOCNO column containing the document's identifier. The ORDER BY clause specifies that the most relevant rows should be listed first. The statement "SET MAX SEARCH ROWS 1000" was previously executed so that the working table would contain at most 1000 rows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Stemming</head><p>SearchServer "stems" each distinct word to one or more base forms, called stems. For example, in English, "baby", "babied", "babies", "baby's" and "babying" all have "baby" as a stem. Compound words in German, Dutch and Finnish produce multiple stems; e.g., in German, "babykost" has "baby" and "kost" as stems. SearchServer 5.3 uses the lexicon-based Inxight LinguistX Platform 3.3.1 for stemming operations.</p><p>By default, Intuitive Searching stems each word in the query, counts the number of occurrences of each stem, and creates a vector. Optionally some stems are discarded (secondary term selection) if they have a high document frequency or to enforce a maximum number of stems, but we didn't discard any for our CLEF runs. The index is searched for documents containing terms which stem to any of the stems of the vector.</p><p>The VECTOR GENERATOR set option controls which stemming operations are performed by Intuitive Searching. To enable stemming, we used the same setting for each language except for the /lang parameter. For example, for German, the setting was 'word!ftelp/lang=german/base/noalt | * | word!ftelp/lang=german/inflect'. To disable stemming, the setting was just ''.</p><p>Besides linguistic expansion from stemming, we did not do any other kinds of query expansion. For example, we did not use approximate text searching for spell-correction because the queries were believed to be spelled correctly. We did not use row expansion or any other kind of blind feedback technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Statistical Relevance Ranking</head><p>SearchServer calculates a relevance value for a row of a table with respect to a vector of stems based on several statistics. The inverse document frequency of the stem is estimated from information in the dictionary. The term frequency (number of occurrences of the stem in the row (including any term that stems to it)) is determined from the reference file. The length of the row (based on the number of indexed characters in all columns of the row, which is typically dominated by the external document), is optionally incorporated. The already-mentioned count of the stem in the vector is also used. To synthesize this information into a relevance value, SearchServer dampens the term frequency and adjusts for document length in a manner similar to Okapi <ref type="bibr" coords="4,442.51,525.70,10.52,10.46" target="#b5">[6]</ref> and dampens the inverse document frequency in a manner similar to <ref type="bibr" coords="4,339.03,537.65,9.96,10.46" target="#b7">[8]</ref>. SearchServer's relevance values are always an integer in the range 0 to 1000.</p><p>SearchServer's RELEVANCE METHOD setting can be used to optionally square the importance of the inverse document frequency (by choosing a RELEVANCE METHOD of 'V2:4' instead of 'V2:3'). The importance of document length to the ranking is controlled by Search-Server's RELEVANCE DLEN IMP setting (scale of 0 to 1000). For all runs in this paper, REL-EVANCE METHOD was set to 'V2:3' and RELEVANCE DLEN IMP was set to 750.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Query Stop Words</head><p>Our QueryToRankings program removed words such as "find", "relevant" and "document" from the topics before presenting them to SearchServer, i.e. words which are not stop words in general but were commonly used in the CLEF topics as general instructions. For the submitted runs, the lists were developed by examining the CLEF 2000 and 2001 topics (not this year's topics). For the diagnostic runs in this paper, "finde" was added as a query stop word because it was noticed to be common in the German topics this year. An evaluation of the impact of query stop words is provided below. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>The evaluation measures are likely explained in an appendix of this volume. Briefly: "Precision" is the percentage of retrieved documents which are relevant. "Precision@n" is the precision after n documents have been retrieved. "Average precision" for a topic is the average of the precision after each relevant document is retrieved (using zero as the precision for relevant documents which are not retrieved). "Recall" is the percentage of relevant documents which have been retrieved. "Interpolated precision" at a particular recall level for a topic is the maximum precision achieved for the topic at that or any higher recall level. For a set of topics, the measure is the average of the measure for each topic (i.e. all topics are weighted equally). The Monolingual Information Retrieval tasks were to run 50 queries against document collections in the same language and submit a list of the top-1000 ranked documents to CLEF for judging (in June 2002). CLEF produced a "qrels" file for each of the 8 tasks: a list of documents judged to be relevant or not relevant for each topic. From these, the evaluation measures were calculated with Chris Buckley's trec eval program.</p><p>For some topics and languages, no documents were judged relevant. The precision scores are just averaged over the number of topics for which at least one document was judged relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Impact of Stemming</head><p>Table <ref type="table" coords="5,116.76,629.85,4.98,10.46" target="#tab_2">2</ref> shows two runs for each language. The first run uses the same settings as were used for the submitted runs which used the Title and Description fields; in particular, stemming was enabled. The second run uses the same settings except that VECTOR GENERATOR was set to the empty string, which disables stemming. Listed for each run are its average precision (AvgP), the precision after 5, 10 and 20 documents retrieved (P@5, P@10 and P@20 respectively), and the interpolated precision at 0% and 30% recall (Rec0 and Rec30 respectively). Additionally listed for the runs with stemming enabled is the number of topics which contained at least one relevant document for that language. The languages are ordered by descending difference in average precision. Stemming increased average precision in Finnish by 69%, German 27%, Spanish 8%, Dutch 8%, French 6%, Italian 4%, Swedish 4% and English 2%. Most of the remaining tables will focus on one particular precision measure (usually average precision), comparing the scores when a particular feature (such as stemming) is enabled to when it is disabled. The columns of these tables are as follows:</p><p>• "Experiment" is the language and topic fields used (for example, "-td" indicates the Title and Description fields were used).</p><p>• "AvgDiff" is the average difference in the precision score. In <ref type="bibr" coords="6,388.39,510.76,9.96,10.46" target="#b8">[9]</ref>, a difference of at least 2 full points (i.e. &gt;=0.020) is considered "noticeable", 4 points "material", 6 points "striking" and 8 points "dramatic".</p><p>• "95% Confidence" is an approximate 95% confidence interval for the average difference calculated using the bootstrap percentile method (described in the last section). If zero is not in the interval, the result is "statistically significant" (at the 5% level), i.e. the feature is unlikely to be of neutral impact, though if the average difference is small (e.g. &lt;0.020) it may still be too minor to be considered "significant" in the magnitude sense.</p><p>• "vs." is the number of topics on which the precision was higher, lower and tied (respectively) with the feature enabled. These numbers should always add to the number of topics for the language (as per Table <ref type="table" coords="6,217.38,645.49,3.87,10.46" target="#tab_2">2</ref>).</p><p>• "2 Largest Diffs (Topic)" lists the two largest differences in the precision score (based on the absolute value), with each followed by the corresponding topic number in brackets (the topic numbers range from 91 to 140).</p><p>Table <ref type="table" coords="6,131.24,708.11,4.98,10.46" target="#tab_3">3</ref> shows the impact of stemming on the average precision measure. The benefit for Finnish and German, for which stemming includes compound-breaking, is dramatic. For example, Finnish topic 115, regarding "avioerotilastoja" (divorce statistics), apparently benefits from compoundbreaking. Surprisingly, the other investigated language for which compounds are broken, Dutch, does not similarly stand out, unlike last year <ref type="bibr" coords="7,288.09,277.82,14.61,10.46" target="#b10">[11]</ref>, though its confidence interval still overlaps the one for German. Table <ref type="table" coords="7,133.51,301.73,4.98,10.46" target="#tab_4">4</ref> shows the impact of stemming on the shorter (Title-only) queries. It appears the benefits are a little bigger for the shorter queries in most languages, with English the only language without a noticeable benefit on average. Of course, stemming can hurt precision for some queries, as in English topic 139 (EU fishing quotas), so an application probably should make stemming a user-controllable option.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Impact of Query Stop Words</head><p>Table <ref type="table" coords="7,118.08,395.82,4.98,10.46" target="#tab_5">5</ref> shows the impact of discarding query stop words, such as "find", "relevant" and "documents". Query stop words differ from general stop words (such as "the", "of", "by") in that they do not seem to be noise words in general, but their common use in past CLEF topic sets (particularly the Description and Narrative fields) suggests they are likely not useful terms when encountered in CLEF queries. In the table, a positive difference indicates a benefit from removing query stop words from the topics.</p><p>Table <ref type="table" coords="7,132.57,467.55,4.98,10.46" target="#tab_5">5</ref> shows that the impact of discarding query stop words was always minor (the biggest average benefit was just 1.6 points), though some of the differences are "statistically significant" because of the consistency of the minor benefits. This is a case where a "statistically significant" benefit is still not a "significant" benefit.</p><p>Sometimes noise words may occur in relevant documents by chance and scores may fall if the noise words are discarded. Apparently that happened in French topics 123 and 132 (regarding "mariage" and "Kaliningrad" respectively) in which excluding "trouver" and "documents" decreased the scores, even though they don't seem to be meaningful terms for their queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Impact of Stop Words</head><p>Tables <ref type="table" coords="7,122.21,597.52,4.98,10.46" target="#tab_6">6</ref> and<ref type="table" coords="7,151.65,597.52,4.98,10.46" target="#tab_7">7</ref> show the impact of using stop words on the average precision measure. To do this experiment, two tables were created for each language, one indexed with a stopfile containing typically a couple hundred stop words, the other with no stop words (though other SearchServer stopfile instructions, such as accent-indexing and apostrophes as word separators, were kept the same as used for the submitted runs). For this experiment, query stop words were not discarded for either run, to isolate the impact of the general stop words on precision. In the tables, a positive difference indicates a benefit to specifying stop words.</p><p>Table <ref type="table" coords="7,131.95,681.20,4.98,10.46" target="#tab_6">6</ref> shows the impact of using stop words for Title plus Description queries was very slight on average, and none of the differences were statistically significant. Table <ref type="table" coords="7,424.43,693.17,4.98,10.46" target="#tab_7">7</ref> shows there was a noticeable benefit for full topic queries (i.e. when additionally including the Narrative) for some languages, and a statistically significant benefit for most of them. Other benefits of specifying stop words are to reduce search time, indexing time and index size. However, there may be cases when what is usually a stop word is meaningful to a query (e.g. find documents containing "to be or not to be"), so it may be better to make stop word elimination an option at search time rather than at index time, depending on the goals of the application. Stop word lists for many languages are on the Neuchâtel resource page <ref type="bibr" coords="8,412.58,461.22,9.96,10.46" target="#b6">[7]</ref>. Our stop word lists may contain differences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,155.18,117.28,292.64,125.23"><head>Table 1 :</head><label>1</label><figDesc>Sizes of CLEF 2002 Document Sets</figDesc><table coords="2,155.18,130.03,292.64,112.48"><row><cell>Language</cell><cell>Text Size (uncompressed)</cell><cell>Number of Documents</cell></row><row><cell>German</cell><cell>555,285,140 bytes (530 MB)</cell><cell>225,371</cell></row><row><cell>Spanish</cell><cell>544,347,121 bytes (519 MB)</cell><cell>215,738</cell></row><row><cell>Dutch</cell><cell>558,560,087 bytes (533 MB)</cell><cell>190,604</cell></row><row><cell>Swedish</cell><cell>374,371,465 bytes (357 MB)</cell><cell>142,819</cell></row><row><cell>English</cell><cell>441,048,231 bytes (421 MB)</cell><cell>113,005</cell></row><row><cell>Italian</cell><cell>290,771,116 bytes (277 MB)</cell><cell>108,578</cell></row><row><cell>French</cell><cell>253,528,734 bytes (242 MB)</cell><cell>87,191</cell></row><row><cell>Finnish</cell><cell>143,902,109 bytes (137 MB)</cell><cell>55,344</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,100.46,611.88,230.14,70.24"><head>TABLE CLEF02DE</head><label>CLEF02DE</label><figDesc></figDesc><table coords="2,100.46,623.83,125.53,58.28"><row><cell>(DOCNO VARCHAR(256) 128)</cell></row><row><cell>TABLE_LANGUAGE 'GERMAN'</cell></row><row><cell>STOPFILE 'LANGDE.STP'</cell></row><row><cell>PERIODIC</cell></row><row><cell>BASEPATH 'e:\data\clef';</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,129.38,117.28,344.26,243.73"><head>Table 2 :</head><label>2</label><figDesc>Precision with Stemming Enabled and Disabled</figDesc><table coords="5,129.38,131.96,344.26,229.04"><row><cell>Run</cell><cell>AvgP</cell><cell>P@5</cell><cell>P@10</cell><cell>P@20</cell><cell>Rec0</cell><cell cols="2">Rec30 #Topics</cell></row><row><cell>Finnish</cell><cell>0.393</cell><cell>44.0%</cell><cell>36.0%</cell><cell>28.2%</cell><cell>0.707</cell><cell>0.502</cell><cell>30</cell></row><row><cell></cell><cell>0.232</cell><cell>29.3%</cell><cell>23.3%</cell><cell>17.7%</cell><cell>0.540</cell><cell>0.299</cell><cell></cell></row><row><cell>German</cell><cell>0.442</cell><cell>64.4%</cell><cell>55.4%</cell><cell>46.7%</cell><cell>0.819</cell><cell>0.538</cell><cell>50</cell></row><row><cell></cell><cell>0.348</cell><cell>55.2%</cell><cell>48.2%</cell><cell>38.7%</cell><cell>0.726</cell><cell>0.426</cell><cell></cell></row><row><cell>Spanish</cell><cell>0.491</cell><cell>68.0%</cell><cell>58.8%</cell><cell>51.2%</cell><cell>0.871</cell><cell>0.617</cell><cell>50</cell></row><row><cell></cell><cell>0.454</cell><cell>64.8%</cell><cell>57.2%</cell><cell>50.9%</cell><cell>0.833</cell><cell>0.586</cell><cell></cell></row><row><cell>Dutch</cell><cell>0.442</cell><cell>58.4%</cell><cell>50.6%</cell><cell>41.3%</cell><cell>0.822</cell><cell>0.529</cell><cell>50</cell></row><row><cell></cell><cell>0.410</cell><cell>54.8%</cell><cell>48.4%</cell><cell>40.6%</cell><cell>0.779</cell><cell>0.516</cell><cell></cell></row><row><cell>French</cell><cell>0.428</cell><cell>52.8%</cell><cell>44.6%</cell><cell>35.6%</cell><cell>0.774</cell><cell>0.554</cell><cell>50</cell></row><row><cell></cell><cell>0.404</cell><cell>49.6%</cell><cell>39.8%</cell><cell>32.6%</cell><cell>0.824</cell><cell>0.488</cell><cell></cell></row><row><cell>Italian</cell><cell>0.409</cell><cell>50.2%</cell><cell>45.3%</cell><cell>36.0%</cell><cell>0.740</cell><cell>0.537</cell><cell>49</cell></row><row><cell></cell><cell>0.395</cell><cell>51.4%</cell><cell>43.3%</cell><cell>35.4%</cell><cell>0.760</cell><cell>0.491</cell><cell></cell></row><row><cell>Swedish</cell><cell>0.348</cell><cell>44.1%</cell><cell>37.6%</cell><cell>29.4%</cell><cell>0.754</cell><cell>0.439</cell><cell>49</cell></row><row><cell></cell><cell>0.334</cell><cell>44.5%</cell><cell>37.1%</cell><cell>28.8%</cell><cell>0.705</cell><cell>0.436</cell><cell></cell></row><row><cell>English</cell><cell>0.508</cell><cell>57.6%</cell><cell>45.2%</cell><cell>34.3%</cell><cell>0.909</cell><cell>0.682</cell><cell>42</cell></row><row><cell></cell><cell>0.500</cell><cell>55.2%</cell><cell>42.9%</cell><cell>33.2%</cell><cell>0.894</cell><cell>0.644</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,120.83,117.28,361.33,127.16"><head>Table 3 :</head><label>3</label><figDesc>Impact of Stemming on Average Precision</figDesc><table coords="6,120.83,131.96,361.33,112.48"><row><cell cols="2">Experiment AvgDiff</cell><cell>95% Confidence</cell><cell>vs.</cell><cell>2 Largest Diffs (Topic)</cell></row><row><cell>Finnish-td</cell><cell>0.161</cell><cell>( 0.097, 0.232)</cell><cell>24-5-1</cell><cell>0.753 (115), 0.453 (122)</cell></row><row><cell>German-td</cell><cell>0.093</cell><cell>( 0.052, 0.136)</cell><cell>35-15-0</cell><cell>0.553 (105), 0.410 (140)</cell></row><row><cell>Spanish-td</cell><cell>0.038</cell><cell>( 0.012, 0.064)</cell><cell>32-18-0</cell><cell>0.337 (119), 0.248 (137)</cell></row><row><cell>Dutch-td</cell><cell>0.032</cell><cell>( 0.001, 0.065)</cell><cell>31-17-2</cell><cell>0.437 (116), 0.350 (109)</cell></row><row><cell>French-td</cell><cell>0.024</cell><cell>(-0.007, 0.059)</cell><cell>23-24-3</cell><cell>0.406 (115), 0.322 (140)</cell></row><row><cell>Italian-td</cell><cell>0.015</cell><cell>(-0.006, 0.037)</cell><cell>27-22-0</cell><cell>0.185 (140), 0.178 (137)</cell></row><row><cell>Swedish-td</cell><cell>0.014</cell><cell>(-0.004, 0.032)</cell><cell>27-18-4</cell><cell>-0.217 (93), 0.204 (129)</cell></row><row><cell>English-td</cell><cell>0.008</cell><cell>(-0.020, 0.035)</cell><cell>22-16-4</cell><cell>-0.283 (139), 0.213 (129)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,116.86,276.39,369.28,127.16"><head>Table 4 :</head><label>4</label><figDesc>Impact of Stemming on Average Precision, Title-only queries</figDesc><table coords="6,116.86,291.08,369.28,112.48"><row><cell cols="2">Experiment AvgDiff</cell><cell>95% Confidence</cell><cell>vs.</cell><cell>2 Largest Diffs (Topic)</cell></row><row><cell>Finnish-t</cell><cell>0.175</cell><cell>( 0.103, 0.259)</cell><cell>26-4-0</cell><cell>0.799 (115), 0.797 (98)</cell></row><row><cell>German-t</cell><cell>0.108</cell><cell>( 0.057, 0.167)</cell><cell>36-13-1</cell><cell>1.000 (137), 0.521 (140)</cell></row><row><cell>Spanish-t</cell><cell>0.036</cell><cell>( 0.016, 0.061)</cell><cell>30-8-12</cell><cell>0.485 (119), 0.201 (139)</cell></row><row><cell>Italian-t</cell><cell>0.035</cell><cell>( 0.012, 0.062)</cell><cell>20-17-12</cell><cell>0.377 (115), 0.263 (137)</cell></row><row><cell>Swedish-t</cell><cell>0.033</cell><cell>( 0.013, 0.055)</cell><cell>25-7-17</cell><cell>0.323 (129), 0.250 (134)</cell></row><row><cell>Dutch-t</cell><cell>0.030</cell><cell>( 0.000, 0.065)</cell><cell>26-19-5</cell><cell>0.567 (116), -0.249 (137)</cell></row><row><cell>French-t</cell><cell>0.030</cell><cell>( 0.005, 0.060)</cell><cell>24-19-7</cell><cell>0.506 (115), 0.261 (103)</cell></row><row><cell>English-t</cell><cell>-0.002</cell><cell>(-0.028, 0.025)</cell><cell>20-14-8</cell><cell>0.312 (103), -0.276 (139)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,112.88,117.28,377.24,127.16"><head>Table 5 :</head><label>5</label><figDesc>Impact of Discarding Query Stop Words on Average Precision</figDesc><table coords="7,112.88,131.96,377.24,112.48"><row><cell cols="2">Experiment AvgDiff</cell><cell>95% Confidence</cell><cell>vs.</cell><cell>2 Largest Diffs (Topic)</cell></row><row><cell>Spanish-td</cell><cell>0.016</cell><cell>( 0.007, 0.027)</cell><cell>24-4-22</cell><cell>0.151 (138), 0.137 (100)</cell></row><row><cell>English-td</cell><cell>0.014</cell><cell>(-0.001, 0.032)</cell><cell>17-8-17</cell><cell>0.250 (137), 0.149 (106)</cell></row><row><cell>Finnish-td</cell><cell>0.009</cell><cell>( 0.002, 0.020)</cell><cell>11-1-18</cell><cell>0.132 (122), 0.050 (114)</cell></row><row><cell>Italian-td</cell><cell>0.007</cell><cell>(-0.002, 0.016)</cell><cell>20-8-21</cell><cell>0.113 (93), -0.107 (91)</cell></row><row><cell>German-td</cell><cell>0.006</cell><cell>( 0.000, 0.014)</cell><cell>19-15-16</cell><cell>0.099 (138), 0.091 (99)</cell></row><row><cell>Swedish-td</cell><cell>0.006</cell><cell>(-0.001, 0.015)</cell><cell>17-10-22</cell><cell>0.106 (111), 0.100 (132)</cell></row><row><cell>Dutch-td</cell><cell>0.001</cell><cell>(-0.003, 0.006)</cell><cell>15-12-23</cell><cell>0.062 (138), -0.049 (123)</cell></row><row><cell>French-td</cell><cell>-0.000</cell><cell>(-0.013, 0.012)</cell><cell>18-10-22</cell><cell>-0.167 (123), -0.150 (132)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,115.37,117.28,372.26,127.16"><head>Table 6 :</head><label>6</label><figDesc>Impact of Stop Words on Average Precision</figDesc><table coords="8,115.37,131.96,372.26,112.48"><row><cell cols="2">Experiment AvgDiff</cell><cell>95% Confidence</cell><cell>vs.</cell><cell>2 Largest Diffs (Topic)</cell></row><row><cell>Spanish-td</cell><cell>0.006</cell><cell>(-0.004, 0.018)</cell><cell>25-24-1</cell><cell>0.186 (99), 0.125 (98)</cell></row><row><cell>French-td</cell><cell>0.005</cell><cell>(-0.002, 0.015)</cell><cell>23-22-5</cell><cell>0.167 (123), 0.099 (105)</cell></row><row><cell>German-td</cell><cell>0.004</cell><cell>(-0.001, 0.011)</cell><cell>26-21-3</cell><cell>0.064 (106), 0.050 (99)</cell></row><row><cell>Finnish-td</cell><cell>0.004</cell><cell>(-0.007, 0.015)</cell><cell>17-9-4</cell><cell>-0.088 (139), 0.083 (132)</cell></row><row><cell>Swedish-td</cell><cell>0.002</cell><cell>(-0.003, 0.009)</cell><cell>20-23-6</cell><cell>0.071 (99), -0.054 (130)</cell></row><row><cell>Dutch-td</cell><cell>-0.000</cell><cell>(-0.010, 0.009)</cell><cell>28-19-3</cell><cell>-0.117 (138), -0.114 (104)</cell></row><row><cell>Italian-td</cell><cell>-0.000</cell><cell>(-0.011, 0.010)</cell><cell>27-22-0</cell><cell>-0.164 (96), -0.083 (139)</cell></row><row><cell>English-td</cell><cell>-0.001</cell><cell>(-0.011, 0.011)</cell><cell>14-22-6</cell><cell>0.167 (126), -0.114 (133)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="8,118.07,276.76,366.86,127.17"><head>Table 7 :</head><label>7</label><figDesc>Impact of Stop Words on Average Precision, Full Topic Queries</figDesc><table coords="8,118.07,291.46,366.86,112.48"><row><cell>Experiment</cell><cell>AvgDiff</cell><cell>95% Confidence</cell><cell>vs.</cell><cell>2 Largest Diffs (Topic)</cell></row><row><cell>Spanish-tdn</cell><cell>0.023</cell><cell>( 0.012, 0.034)</cell><cell>36-13-1</cell><cell>0.156 (99), 0.126 (98)</cell></row><row><cell>Italian-tdn</cell><cell>0.020</cell><cell>( 0.007, 0.034)</cell><cell>33-16-0</cell><cell>0.172 (132), -0.127 (96)</cell></row><row><cell>Swedish-tdn</cell><cell>0.017</cell><cell>( 0.008, 0.028)</cell><cell>33-14-2</cell><cell>0.163 (102), 0.096 (96)</cell></row><row><cell>French-tdn</cell><cell>0.016</cell><cell>( 0.004, 0.033)</cell><cell>34-12-4</cell><cell>0.303 (109), 0.094 (128)</cell></row><row><cell>German-tdn</cell><cell>0.015</cell><cell>( 0.003, 0.029)</cell><cell>33-14-3</cell><cell>0.262 (137), 0.080 (102)</cell></row><row><cell>Finnish-tdn</cell><cell>0.012</cell><cell>( 0.004, 0.020)</cell><cell>20-7-3</cell><cell>0.083 (132), 0.054 (116)</cell></row><row><cell>English-tdn</cell><cell>0.007</cell><cell>(-0.001, 0.018)</cell><cell>20-15-7</cell><cell>0.164 (126), 0.101 (99)</cell></row><row><cell>Dutch-tdn</cell><cell>0.005</cell><cell>(-0.013, 0.023)</cell><cell>29-17-4</cell><cell>0.213 (116), -0.177 (109)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,105.24,725.62,407.77,9.50;1,90.00,735.09,423.10,9.49;1,90.00,745.67,114.25,8.37"><p>Fulcrum r is a registered trademark, and SearchServer TM , SearchSQL TM , Intuitive Searching TM and Ful/Text TM are trademarks of Hummingbird Ltd. All other copyrights, trademarks and tradenames are the property of their respective owners.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Impact of Indexing Accents</head><p>Tables <ref type="table" coords="8,121.48,519.45,4.98,10.46">8</ref> and<ref type="table" coords="8,149.47,519.45,4.98,10.46">9</ref> show the impact of accent-indexing on the average precision measure. To do this experiment, two tables were created for each language, one preserving accents (e.g. "bébé" and "bebe" would be distinct words) and one which dropped the accents (e.g. "bébé" would be the same as "bebe"). Of course, at search-time SearchServer uses the same rules as at index-time. For this experiment, no stop words were used and no query stop words were discarded. Otherwise, the   settings were the same as for the submitted runs; in particular, apostrophes were used as word separators except in English. Tables <ref type="table" coords="9,135.83,461.22,4.98,10.46">8</ref> and<ref type="table" coords="9,162.62,461.22,4.98,10.46">9</ref> show that topic 98, regarding the Kaurismäki brothers, was strongly affected in many languages by whether or not accents were preserved. Spanish, French and Italian topics 98 included the accent in Kaurismäki, but the documents more often did not include the accent, so accent-indexing hurt precision in those cases. But accent-indexing was helpful for Finnish for this topic, apparently because in Finnish there were variants which required stemming to match (e.g. Kaurismäkien and Kaurismäen), and the stemmer was more effective when given the words with the accents preserved. It appears it would help if the stemmer was modified to be more tolerant of missing accents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Impact of Apostrophes as Word Separators</head><p>Table <ref type="table" coords="9,118.07,591.18,9.96,10.46">10</ref> shows the impact of treating apostrophes as word separators on the average precision measure. To do this experiment, two tables were created for each language, one treating apostrophes as word separators, the other not. No stop words were used and no query stop words were discarded. Otherwise, the settings were the same as for the submitted runs; in particular, accent-indexing was enabled except in Italian and English.</p><p>Table <ref type="table" coords="9,133.88,650.96,9.96,10.46">10</ref> shows that treating apostrophes as word separators had a noticeable benefit for French. For example, French topic 121 may be benefiting from breaking "d'Ayrton" at its apostrophe. The benefit for Italian may have been less because stemming appears to be handling apostrophes. For example, in Italian, if apostrophes are not word separators, "l'ombrello" still matches "ombrello" when stemming is enabled, whereas in French, "l'école" still does not match "école" (again, this difference is moot when apostrophes are treated as word separators). The impact for other languages is slight, including for English. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Submitted runs</head><p>We submitted 10 monolingual runs (the maximum allowed) in June 2002. Runs humDE02, humFR02, humIT02, humES02, humNL02, humSV02 and humFI02 provided a run for each language using the Title and Description fields as requested by the organizers (note that English monolingual runs were not accepted). For the remaining 3 runs, we submitted an extra run for Finnish, Swedish and Dutch including the Narrative field (runs humFI02n, humSV02n, humNL02n); these languages were expected to have the fewest participants, so additional submissions seemed more likely to be helpful for the judging pools. The precision scores of the submitted runs are expected to be included in an appendix of this volume. Table <ref type="table" coords="10,317.76,403.81,9.96,10.46">11</ref> shows a comparison of the submitted runs with the median scores of submitted monolingual runs for each language. In all but one case, SearchServer scored higher than the median on more topics than it scored lower. Note that the relative performance on different languages may not be meaningful for several reasons, including that the medians are from a mix of runs where some may have used the Narrative field, multiple runs may be submitted by the same group, and the mixture can vary across languages. The submitted runs of June used an older, experimental build than was used for the diagnostic runs in August, and there may be minor differences in the scores even when the settings are the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Confidence Intervals for Precision Differences</head><p>The 95% confidence intervals presented in this paper have been produced using Efron's Bootstrap (percentile method). If there are 50 topics (i.e. 50 precision differences), then precision differences are chosen randomly (with replacement) 50 times, producing a "bootstrap sample", and a mean (average) is computed from this sample. This step is repeated B times (e.g. B=100,000). The B sample means are sorted, the bottom and top 2.5% are discarded, and the endpoints of the remaining range of sample means are an approximate 95% confidence interval for the average difference in precision (we always rounded so that the listed endpoints are not actually in the produced interval). The bootstrap percentile method is considered to work well in more cases than the standard method of using the mean plus/minus 1.96 times the standard error, though there are more complicated bootstrap methods which are considered even more general <ref type="bibr" coords="10,473.95,661.81,9.96,10.46" target="#b0">[1]</ref>.</p><p>Table <ref type="table" coords="10,133.23,673.76,9.96,10.46">12</ref> shows the bootstrap confidence intervals produced for the impact of stemming on average precision with different numbers of iterations. Even at just 1000 iterations the values are fairly close to the values at 1 million iterations. When comparing 1,000,000 iterations to 100,000, very few of the endpoints changed, and they only changed by 0.001. For the confidence intervals in this paper, we used B=100,000.</p><p>Tables <ref type="table" coords="10,135.67,733.54,9.96,10.46">13</ref> and<ref type="table" coords="10,167.12,733.54,9.96,10.46">14</ref> contain side-by-side comparisons of the approximate 95% confidence intervals produced by the bootstrap percentile method and the standard method. It turns they are very similar. There is a disagreement on statistical significance (i.e. when zero is not in the interval) in the case of Dutch in Table <ref type="table" coords="12,220.26,134.27,8.49,10.46">13</ref>, but it is a borderline case. Tables <ref type="table" coords="12,135.85,146.22,9.96,10.46">13</ref> and<ref type="table" coords="12,167.68,146.22,9.96,10.46">14</ref> also include an estimator and 95% confidence interval based on the Wilcoxon signed rank test (the 2 rightmost columns). (We implemented an exact computation, including for the case of ties in the absolute values of the differences <ref type="bibr" coords="12,335.12,170.13,10.30,10.46" target="#b3">[4]</ref>). For differences in average precision, the widths of the intervals are very similar (the bootstrap intervals are a little smaller than the Wilcoxon intervals for the Finnish and German results, and the Wilcoxon intervals are a little smaller for the others); the methods agree on which differences are statistically significant. However, for differences in Precision@10, the bootstrap intervals are a lot smaller than the Wilcoxon intervals (because the Wilcoxon is based on ranks, it cannot distinguish between a shift of 0.01 and 0.09 (they have the same effect on the ranks because every difference is a multiple of 0.10)); the methods still agree on statistically significant results (for the 8 cases listed).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="12,105.49,308.23,405.28,10.46" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="12,200.86,308.23,188.68,10.46">Bootstrap Methods: A Practitioner&apos;s Guide</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Chernick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.49,327.34,332.21,10.46" xml:id="b1">
	<monogr>
		<ptr target="http://www.clef-campaign.org/" />
		<title level="m" coord="12,105.49,327.34,188.59,10.46">Cross-Language Evaluation Forum web site</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.49,346.44,407.52,10.46;12,100.52,358.39,310.45,10.46" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,188.11,346.44,223.00,10.46">Converting the Fulcrum Search Engine to Unicode</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Hodgson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,433.11,346.44,79.91,10.46;12,100.52,358.39,113.03,10.46">Sixteenth International Unicode Conference</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-03">March 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.49,377.50,407.52,10.46;12,100.52,389.45,113.10,10.46" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="12,283.68,377.50,151.55,10.46">Nonparametric Statistical Methods</title>
		<author>
			<persName coords=""><forename type="first">Myles</forename><surname>Hollander</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">A</forename><surname>Wolfe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
	<note>Second Edition</note>
</biblStruct>

<biblStruct coords="12,105.49,408.56,32.79,10.46;12,156.14,408.56,58.68,10.46;12,232.66,408.56,18.60,10.46;12,269.13,408.56,43.73,10.46;12,330.71,408.56,11.93,10.46;12,360.51,408.56,10.93,10.46;12,389.29,408.56,39.13,10.46;12,446.29,408.56,25.18,10.46;12,489.34,408.56,23.67,10.46;12,100.52,420.52,213.76,10.46" xml:id="b4">
	<monogr>
		<ptr target="http://research.nii.ac.jp/∼ntcadm/index-en.html" />
		<title level="m" coord="12,105.49,408.56,32.79,10.46;12,156.14,408.56,58.68,10.46;12,232.66,408.56,18.60,10.46;12,269.13,408.56,43.73,10.46;12,330.71,408.56,11.93,10.46;12,360.51,408.56,10.93,10.46;12,389.29,408.56,39.13,10.46;12,446.29,408.56,25.18,10.46;12,489.34,408.56,18.94,10.46">NTCIR (NII-NACSIS Test Collection for IR Systems) Home Page</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.49,439.63,407.51,10.46;12,100.52,451.58,412.49,10.46;12,100.52,463.53,412.48,10.46;12,100.52,475.49,227.72,10.46" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,192.16,451.58,87.01,10.46">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec3/t3proceedings.html" />
	</analytic>
	<monogr>
		<title level="m" coord="12,431.93,451.58,81.07,10.46;12,100.52,463.53,221.57,10.46">Overview of the Third Text REtrieval Conference (TREC-3)</title>
		<title level="s" coord="12,338.65,463.53,65.81,10.46">NIST Special</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="500" to="226" />
		</imprint>
		<respStmt>
			<orgName>City University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.49,494.59,407.51,10.46;12,100.52,506.55,194.36,10.46" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Jacques</forename><surname>Savoy</surname></persName>
		</author>
		<ptr target="http://www.unine.ch/info/clef/" />
		<title level="m" coord="12,296.39,494.59,216.61,10.46;12,100.52,506.55,48.68,10.46">CLEF and Multilingual information retrieval resource page</title>
		<imprint/>
		<respStmt>
			<orgName>Université de Neuchâtel</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.49,525.65,407.53,10.46;12,100.52,537.60,412.48,10.46;12,100.52,549.57,412.48,10.46;12,100.52,561.52,227.72,10.46" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,100.52,537.60,88.39,10.46">AT&amp;T at TREC-7</title>
		<author>
			<persName coords=""><forename type="first">Amit</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Hindle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec7/t7proceedings.html" />
	</analytic>
	<monogr>
		<title level="m" coord="12,444.58,537.60,68.42,10.46;12,100.52,549.57,236.80,10.46">Proceedings of the Seventh Text REtrieval Conference (TREC-7)</title>
		<title level="s" coord="12,350.06,549.57,119.47,10.46">NIST Special Publication</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Seventh Text REtrieval Conference (TREC-7)</meeting>
		<imprint>
			<biblScope unit="page" from="500" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.49,580.63,407.52,10.46;12,100.52,592.58,303.97,10.46" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="12,405.88,580.63,107.13,10.46;12,100.52,592.58,198.92,10.46">A probabilistic model of information retrieval: development and status</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Sparck</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998-08">August 1998</date>
			<biblScope unit="page">15</biblScope>
		</imprint>
		<respStmt>
			<orgName>City University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.47,611.69,303.88,10.46" xml:id="b9">
	<monogr>
		<ptr target="http://trec.nist.gov/" />
		<title level="m" coord="12,110.47,611.69,206.35,10.46">Text REtrieval Conference (TREC) Home Page</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.47,630.79,389.19,10.46;12,499.65,629.71,12.85,7.32;12,100.52,642.74,64.82,10.46" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="12,200.68,630.79,298.98,10.46;12,499.65,629.71,12.85,7.32;12,100.52,642.74,38.84,10.46">Stemming Evaluated in 6 Languages by Hummingbird SearchServer TM at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,168.63,642.74,344.38,10.46;12,100.52,654.70,412.49,10.46;12,100.52,666.65,412.48,10.46;12,100.52,678.61,304.26,10.46" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,424.24,642.74,88.77,10.46;12,100.52,654.70,412.49,10.46;12,100.52,666.65,25.97,10.46">Evaluation of Cross-Language Information Retrieval Systems: Second Workshop of the Cross-Language Evaluation Forum</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kluck</surname></persName>
		</author>
		<idno>2406</idno>
		<ptr target="http://link.springer.de/link/service/series/0558/tocs/t2406.htm" />
	</analytic>
	<monogr>
		<title level="m" coord="12,134.19,666.65,47.18,10.46">CLEF 2001</title>
		<meeting><address><addrLine>Darmstadt, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer LNCS</publisher>
			<date type="published" when="2001">September 3-4, 2001</date>
		</imprint>
	</monogr>
	<note>Revised Papers</note>
</biblStruct>

<biblStruct coords="12,110.47,697.71,402.54,10.46;12,100.52,709.67,412.49,10.46;12,100.52,721.63,412.48,10.46;12,100.52,733.58,227.72,10.46" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,329.89,697.71,183.13,10.46;12,100.52,709.67,52.49,10.46">Hummingbird&apos;s Fulcrum SearchServer at TREC-9</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tom</forename><surname>Blackwell</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec9/t9proceedings.html" />
	</analytic>
	<monogr>
		<title level="m" coord="12,419.72,709.67,93.29,10.46;12,100.52,721.63,221.56,10.46">Proceedings of the Ninth Text REtrieval Conference (TREC-9)</title>
		<title level="s" coord="12,338.64,721.63,127.08,10.46">NIST Special Publication</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Ninth Text REtrieval Conference (TREC-9)</meeting>
		<imprint>
			<biblScope unit="page" from="500" to="249" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
