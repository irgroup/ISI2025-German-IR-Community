<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,117.75,102.56,359.82,12.63">Merging Mechanisms in Multilingual Information Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,226.34,149.75,64.16,8.96"><forename type="first">Wen-Cheng</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Taiwan University Taipei</orgName>
								<address>
									<country>TAIWAN R.O.C</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,309.88,149.75,59.19,8.96"><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
							<email>hh_chen@csie.ntu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Taiwan University Taipei</orgName>
								<address>
									<country>TAIWAN R.O.C</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,117.75,102.56,359.82,12.63">Merging Mechanisms in Multilingual Information Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EA4785DC30B453253C70B7AEBFFF187E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>National Taiwan University (NTU) Natural Language Processing Laboratory (NLPL) participated in MLIR task in CLEF 2002. We submitted five official multilingual runs. In this paper, we try to resolve the collection fusion problem. We experimented with several merging strategies that merge the results of several intermediate runs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multilingual Information Retrieval <ref type="bibr" coords="1,213.14,349.77,11.69,8.96" target="#b3">[4]</ref> uses a query in one language to retrieve documents in different languages. A multilingual data collection is a set of documents that are written in different languages. There are two types of multilingual data collection. The first one contains several monolingual document collections. The second one consists of multilingual-documents. A multilingual-document is written in more than two languages. Some multilingual-documents have a major language, i.e. most part of the document is written in the same language. For example, a document is written in Chinese, but the abstract is in English. Therefore this document is a multilingual-document and Chinese is its major language. The significances of different languages in a multilingual-document may be different. For example, the English translation of a Chinese proper noun is a useful clue when using English queries to retrieve Chinese documents. Therefore, the English translation should have higher weight. Figure <ref type="figure" coords="1,182.94,453.20,4.98,8.96">1</ref> shows these two types of multilingual data collections.</p><p>In Multilingual Information Retrieval, queries and documents are in different languages. We can translate queries, or translate documents, or translate both queries and documents into an intermediate language to unify the languages of queries and documents. Figure <ref type="figure" coords="1,275.10,487.76,4.98,8.96">2</ref> shows some MLIR architectures when query translation is adopted. The front-end controller processes queries, translates queries, submits translated queries to the monolingual IR systems, collects the relevant document lists reported by IR systems and merges them. Figure <ref type="figure" coords="1,519.39,510.67,4.98,8.96">3</ref> shows document translation architectures. In addition to language barrier issue, how to conduct a ranked list that contains documents in different languages from several text collections is also critical. There are two architectures in MLIR: centralized and distributed. The first two architectures in Figure <ref type="figure" coords="3,269.59,95.99,4.98,8.96">2</ref> and first architecture in Figure <ref type="figure" coords="3,402.64,95.99,4.98,8.96">3</ref> are distributed architectures; the remaining architectures in Figures <ref type="figure" coords="3,226.43,107.51,4.98,8.96">2</ref> and<ref type="figure" coords="3,251.63,107.51,4.98,8.96">3</ref> are centralized architectures. In a centralized architecture, a huge collection that contains documents in different languages is used. In a distributed architecture, documents in different languages are indexed and retrieved separately. The results of each run are merged into a multilingual ranked list. Several merging strategies have been proposed. Raw-score merging selects documents based on their original similarity scores. Normalized-score merging normalizes the similarity score of each document and sorts all documents by the normalized score. For each topic, the similarity score of each document is divided by the maximum score in this topic. Round-robin merging interleaves the results in the intermediate runs. In this experiment, we adopted distributed architecture and proposed a merging strategy to merge the result lists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig 1. Multilingual Data Collections</head><formula xml:id="formula_0" coords="1,149.67,555.01,290.92,161.81">L1-1 L1-m L2-1 Lt-1 L2-n Lt-k … … … … (a) A set of monolingual document collections L1 L2 . . . L2 L1 L2 L1 L2 … L1 L2 L1 L2 L3 L4 L5 L1 L2 L3 L4 L5 L1 L3 . . . (b) Some types of multilingual-documents</formula><p>The rest of this paper is organized as follows. Section 2 describes the indexing method. Section 3 shows the query translation process. Section 4 describes our merging strategies. Section 5 shows the experiment results. Section 6 concludes the remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Indexing</head><p>The document set used in CLEF2002 MLIR task consists of English, French, German, Spanish and Italian. The numbers of documents in English, French, German, Spanish and Italian document sets are 113,005, 87,191, 225,371, 215,738 and 108,578 respectively.</p><p>The IR model we used is the basic vector space model. Documents and queries are represented as term vectors, and cosine vector similarity formula is used to measure the similarity of a query and a document. The term weighting function is tf*idf. Appropriate terms are extracted from each document in indexing stage. In the experiment, the &lt;HEADLINE&gt; and &lt;TEXT&gt; sections in English documents were used for indexing. For Spanish documents, the &lt;TITLE&gt; and &lt;TEXT&gt; sections were used. When indexing French, German and Italian documents, the &lt;TITLE&gt;, &lt;TEXT&gt;, &lt;TI&gt;, &lt;LD&gt; and &lt;TX&gt; sections were used. The words in these sections were stemmed, and stopwords were removed. The stopword lists and stemmers were developed by University of Neuchatel (The stopword lists and stemmers are available at http://www.unine.ch/info/clef/) <ref type="bibr" coords="3,439.61,388.04,11.71,8.96" target="#b4">[5]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Query translation</head><p>In the experiment, the English queries were used as source queries and translated into target languages, i.e. French, German, Spanish and Italian. In our previous experiments <ref type="bibr" coords="3,340.56,452.48,10.84,8.96" target="#b0">[1,</ref><ref type="bibr" coords="3,353.95,452.48,7.23,8.96" target="#b2">3]</ref>, we used CO model to translate queries. CO model uses word co-occurrence information trained from a target language text collection to disambiguate the translations of query terms. In this experiment, we didn't have enough time to train the word co-occurrence information for the languages used in CLEF 2002 MLIR task. Thus, we used a simple method to translate the queries. We adopted a dictionary-based approach to translate the queries. For each English query term, we found its translation equivalents by looking up a dictionary and selected the first two translation equivalents to be the target language query terms. The dictionaries we used are Ergane English-French, English-German, English-Spanish and English-Italian dictionaries (The Ergane dictionaries are available at http://www.travlang.com/Ergane). There are 8,839, 9,046, 16,936 and 4,595 terms in Ergane English-French, English-German, English-Spanish and English-Italian dictionaries, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Merging strategies</head><p>There are two architectures in MLIR, i.e., centralized and distributed. In a centralized architecture, document collections in different languages are viewed as a single document collection and are indexed in one huge index file. The advantage of a centralized architecture is that it avoids the merging problem. It needs only one retrieving phase to produce a result list that contains documents in different languages. One of problems of centralized architecture is that the weights of index terms are over weighting. The total number of documents increases but the number of occurrences of a term does not. Thus, the idf of a term is increased and the weight is over-weighting. This phenomenon is acuter in small text collection. For example, the N in idf formula is 87,191 when French document is used. However, this value is increased to 749,883, i.e. about 8.6 times larger, if the three document collections are merged together. Comparatively, the weights of German index terms are increased 3.33 times due to the size of N. The increments of weights are unbalance for document collections in different size. Thus, it makes retrieval result preferring documents in small document collection.</p><p>The second architecture is a distributed MLIR. Documents in different languages are indexed and retrieved separately. The ranked lists of all monolingual and cross-lingual runs are merged into one multilingual ranked list. How to merge result lists is a problem. Recent works have proposed various approaches to deal with merging problem. A simple merging method is raw-score merging that sorts all results by their original similarity scores and then selects the top ranked documents. Raw-score merging is based on the assumption that the similarity scores across collections are comparable. However, the collection-dependent statistics in document or query weights invalidates this assumption <ref type="bibr" coords="4,314.27,119.03,10.88,8.96" target="#b1">[2,</ref><ref type="bibr" coords="4,330.60,119.03,7.25,8.96" target="#b5">6]</ref>. Another approach, round-robin merging, interleaves the results based on the rank. This approach assumes that each collection has approximately the same number of relevant documents and the distribution of relevant documents is similar across the result lists. Actually, different collections do not contain equal numbers of relevant documents. Thus the performance of round-robin merging may be poor. The third approach is normalized-score merging. For each topic, the similarity score of each document is divided by the maximum score in this topic. After adjusting scores, all results are put into a pool and sorted by the normalized score. This approach maps the similarity scores of different result lists into the same range, from 0 to 1, and makes the scores more comparable. But it has a problem. If the maximum score is much higher than the second one in a result list, the normalized-score of document at rank 2 would be low even if its original score is high. Thus, the final rank of this document would be lower than that of the top ranked documents with very low but similar original scores in another result list.</p><p>Similarity score reflects the degree of similarity between a document and a query. A document with higher similarity score seems more relevant to the desired query. But, if the query is not formulated well, e.g., unappropriate translation of a query, a document with high score still does not meet the user's information need. When merging results, such documents that have high scores should not be included in the final result list. Thus, we have to consider the effectiveness of each individual run in the merging stage. The basic idea of our merging strategy is that adjusting the similarity scores of documents in each result list to make them more comparable and to reflect their confidence. The similarity scores are adjusted by the following formula.</p><formula xml:id="formula_1" coords="4,81.75,326.85,73.90,27.15">i k ij ij W S S S × × = 1 ˆ</formula><p>where S ij is the original similarity score of the document at rank j in the ranked list of topic i, is the adjusted similarity score of the document at rank j in the ranked list of topic i, is the average similarity score of top k documents, and W i is the weight of query i in a cross lingual run. We divide the weight adjusting process into two steps. First, we use a modified score normalization method to normalize the similarity scores. The original score of each document is divided by the average score of top k documents instead of the maximum score. We call this normalized-by-top-k. Second, the normalized score multiplies a weight that reflects the retrieval effectiveness of the desired topic in each text collection. However, due to not knowing the retrieval performance in advance, we have to guess the performance of each run. For each language pair, the queries are translated into target language and then the target language documents are retrieved. A good translation should have better performance. We can predict the retrieval performance based on the translation performance. There are two factors affecting the translation performance, i.e., the degree of translation ambiguity and the number of unknown words. For each query, we compute the average number of translation equivalents of query terms and the number of unknown words in each language pair, and use them to compute the weights of each cross lingual run. The weight can be determined by the following formula:</p><formula xml:id="formula_2" coords="4,80.55,528.35,159.38,35.73">              - × +                 × + = i i i i n U c T c c W 1 1 3 2 1</formula><p>where W i is the weight of query i in a cross lingual run, T i is the average number of translation equivalents of query terms in query i, U i is the number of unknown words in query i, n i is the number of query terms in query i, and c 1 , c 2 and c 3 are tunable parameters, and c 1 +c 2 +c 3 =1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>We submitted five multilingual runs. All runs use title and description fields. The five multilingual runs use English topics as source queries. The English topics were translated into French, German, Spanish and Italian. The source English topics and translated French, German, Spanish and Italian topics were used to retrieve the corresponding document collections. Then, we merged these five result lists. We used different merging strategies for the five multilingual runs:</p><formula xml:id="formula_3" coords="4,70.95,542.35,445.36,189.67">1. NTUmulti01<label>(2)</label></formula><p>(1</p><formula xml:id="formula_4" coords="4,99.87,334.29,416.32,45.69">) ij S ˆk</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S</head><p>The result lists were merged by normalized-score merging strategy. The maximum similarity score was used for normalization. After normalization, all results were put in a pool and were sorted by the adjusted score. The top1000 documents were selected as the final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">NTUmulti02</head><p>In this run, we used the modified normalized-score merging method. The average similarity score of top 100 documents were used for normalization. We did not consider the performance drop down caused by query translation. That is, the weight W i in formula (1) was 1 for every sub run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">NTUmulti03</head><p>First, the similarity scores of each document were normalized. The maximum similarity score was used for normalization. Then we assigned a weight W i to each intermediate run. The weight was determined by formula <ref type="bibr" coords="5,122.79,188.02,10.64,8.96" target="#b1">(2)</ref>. The values of c 1 , c 2 and c 3 were 0, 0.4 and 0.6, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">NTUmulti04</head><p>We used formula <ref type="bibr" coords="5,161.53,211.06,11.66,8.96" target="#b0">(1)</ref> to adjust the similarity score of each document. We used the average similarity score of top 100 documents for normalization. The weight W i was determined by formula <ref type="bibr" coords="5,435.40,222.58,10.64,8.96" target="#b1">(2)</ref>. The values of c 1 , c 2 and c 3 were 0, 0.4 and 0.6, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">NTUmulti05</head><p>In this run, the merging strategy is similar to run NTUmulti04. The difference was that each intermediate run was assigned a constant weight. The weights assigned to English-English, English-French, English-German, English-Italian and English-Spanish intermediate runs were 1, 0.7, 0.4, 0.6 and 0.6 respectively. The results of our official runs are shown in Table <ref type="table" coords="5,291.64,291.57,3.75,8.96" target="#tab_0">1</ref>. The performance of normalized-score merging is bad. The average precision of run NTUmulti01 is 0.0173. When using our modified normalized-score merging strategy, the performance is better. The average precision is increased to 0.0266. Run NTUmulti03 and NTUmulti04 have considered the performance drop down caused by query translation. Table <ref type="table" coords="5,473.56,326.01,4.98,8.96">2</ref> shows the unofficial evaluation of intermediate monolingual and cross-lingual runs. The performance of English monolingual run is much better than cross-lingual runs. Therefore, the cross-lingual runs should have lower weights when merging results. The results show that the performances are improved by decreasing the importance of un-effective cross-lingual runs. The average precisions of runs NTUmulti03 and NTUmulti04 are 0.0336 and 0.0373, which are better than run NTUmulti01 and NTUmulti02. Run NTUmulti05 assigned constant weights to each intermediate runs. Its performance is slightly worse than run NTUmulti04. All our official runs didn't perform well. This is because that the performances of cross-lingual runs are very bad. In the experiments, we did not disambiguate the senses of query terms when translating queries and the numbers of words contained in the bilingual dictionaries we used are too few, the queries were not translated well, thus the performances of cross-lingual runs are very poor. If we use larger dictionaries and disambiguate the word senses, the performance should be better.</p><p>In order to compare the effectiveness of different merging strategies, we also conducted several unofficial runs: 1. ntu-multi-raw-score</p><p>We used raw-score merging to merge result lists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">ntu-multi-round-robin</head><p>We used round-robin merging to merge result lists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ntu-multi-centralized</head><p>This run adopted centralized architecture. All document collections were indexed in one index file. The topics contained source English query terms, and other translated query terms. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,168.27,429.68,270.10,8.96"><head>Fig 2 .Fig 3 .</head><label>23</label><figDesc>Fig 2. MLIR Architectures when Query Translation is Adopted</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,149.67,596.24,279.74,123.54"><head>Table 1 . The Results of Official Runs</head><label>1</label><figDesc></figDesc><table coords="5,149.67,618.12,279.74,101.66"><row><cell>Run</cell><cell>Average Precision</cell><cell>Recall</cell></row><row><cell>NTUmulti01</cell><cell>0.0173</cell><cell>1083 / 8068</cell></row><row><cell>NTUmulti02</cell><cell>0.0266</cell><cell>1135 / 8068</cell></row><row><cell>NTUmulti03</cell><cell>0.0336</cell><cell>1145 / 8068</cell></row><row><cell>NTUmulti04</cell><cell>0.0373</cell><cell>1195 / 8068</cell></row><row><cell>NTUmulti05</cell><cell>0.0361</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The results of unofficial runs are shown in Table <ref type="table" coords="6,279.30,309.45,3.75,8.96">3</ref>. The performance of raw-score merging is good. This is probably because we use the same IR model and term weighting scheme for all text collections. When using round-robin merging strategy, the performance is bad. The best run is ntu-multi-centralized which indexes all documents in different languages together. In this run, most top ranked documents are in English, French and Italian in most topics. The performance of English monolingual retrieval is much better than other runs. The average precisions of English-German and English-Spanish cross-lingual runs are quite low. Therefore, the result list should not contain too many German and Spanish documents. The over-weighting phenomenon in centralized architecture makes the scores of French, Italian and English documents increase. Thus, more French, Italian and English documents are included in the result list of run ntu-multi-centralized. This makes the performance better. If we use the German or Spanish queries as source queries, the performance of centralized architecture may be not so good.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Concluding Remarks</head><p>In the experiment, we proposed some merging strategies to integrate the result lists of collections in different languages. The results showed that the performance of our merging strategies was similar to that of raw-score merging and was better than normalized-score and round-robin merging. The performance of run NTUmulti05 was good. The weights of English-German and English-Spanish runs were not low enough. If we decrease the weights of these two runs, the performance should be better. The results showed that we could gain better performance by adjusting the similarity score appropriately. The similar results also appeared in NTCIR multilingual IR task. How to determine appropriate weights is an important issue. Considering the degree of ambiguity, i.e. lowering the weights of more ambiguous query terms, improves some performance. The centralized approach performed well. We will do more experiments to find out if the centralized approach still works when using German or Spanish queries as source queries.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="6,88.95,621.96,435.51,8.10;6,88.95,632.28,171.52,8.10;6,260.42,630.37,4.68,5.40;6,269.78,632.28,254.71,8.10;6,88.95,642.72,273.86,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,265.03,621.96,259.44,8.10;6,88.95,632.28,76.05,8.10">Resolving translation ambiguity and target polysemy in cross-language information retrieval</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,189.86,632.28,70.61,8.10;6,260.42,630.37,4.68,5.40;6,269.78,632.28,251.22,8.10">Proceedings of 37 th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s" coord="6,172.28,642.72,152.19,8.10">Association for Computational Linguistics</title>
		<meeting>37 th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-06">1999. June 1999</date>
			<biblScope unit="page" from="215" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,88.95,653.04,435.59,8.10;6,88.95,663.36,271.15,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,166.30,653.04,123.47,8.10">LSI meets TREC: A Status Report</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,308.16,653.04,216.38,8.10;6,88.95,663.36,6.54,8.10">Proceedings of the First Text REtrieval Conference (TREC-1)</title>
		<meeting>the First Text REtrieval Conference (TREC-1)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>NIST Publication</publisher>
			<date type="published" when="1992-11">1992. November, 1992</date>
			<biblScope unit="page" from="137" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,88.95,673.68,407.78,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,213.29,673.68,103.46,8.10">NTU at NTCIR3 MLIR Task</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,335.17,673.68,134.64,8.10">Working Notes for NTCIR3 workshop</title>
		<imprint>
			<date type="published" when="2000">2002. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,89.00,684.12,435.47,8.10;6,88.95,694.44,239.70,8.10" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,221.73,684.12,149.64,8.10">A Survey of Multilingual Text Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">J</forename><surname>Dorr</surname></persName>
		</author>
		<idno>UMIACS-TR-96-19</idno>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>University of Maryland, Institute for Advanced Computer Studies</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="6,88.95,704.76,435.70,8.10;6,88.95,715.08,437.71,8.10;6,88.95,725.52,127.09,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,152.54,704.76,313.43,8.10">Report on CLEF-2001 Experiments: Effective Combined Query-Translation Approach</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,485.55,704.76,39.10,8.10;6,88.95,715.08,179.80,8.10">Evaluation of Cross-Language Information Retrieval Systems</title>
		<title level="s" coord="6,275.21,715.08,127.15,8.10">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Darmstadt, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001-09">2001. September, 2001</date>
			<biblScope unit="volume">2406</biblScope>
			<biblScope unit="page" from="27" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,88.95,735.83,435.61,8.10;6,88.95,746.15,415.39,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,318.77,735.83,114.92,8.10">The Collection Fusion Problem</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johnson-Laird</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,455.76,735.83,68.80,8.10;6,88.95,746.15,154.41,8.10">proceedings of the Third Text REtrieval Conference (TREC-3)</title>
		<meeting>the Third Text REtrieval Conference (TREC-3)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>NIST Publication</publisher>
			<date type="published" when="1994">1995. November, 1994</date>
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
