<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,102.63,74.52,390.14,12.64">Portuguese-English Experiments using Latent Semantic Indexing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,248.25,118.99,66.56,9.07"><forename type="first">Viviane</forename><surname>Moreira</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science Middlesex</orgName>
								<orgName type="institution">University The Burroughs</orgName>
								<address>
									<postCode>NW4 4BT [</postCode>
									<settlement>Londonv.orengo</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,317.35,118.99,29.90,9.07;1,264.81,131.95,65.88,9.07"><forename type="first">Orengo</forename><forename type="middle">Christian</forename><surname>Huyck</surname></persName>
							<email>c.huyck]@mdx.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science Middlesex</orgName>
								<orgName type="institution">University The Burroughs</orgName>
								<address>
									<postCode>NW4 4BT [</postCode>
									<settlement>Londonv.orengo</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,102.63,74.52,390.14,12.64">Portuguese-English Experiments using Latent Semantic Indexing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2FFA96FDF1831E984AB5B1C7605D2E13</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper reports the work of Middlesex University for the CLEF bilingual task. We have carried out experiments using Portuguese queries to retrieve documents in English. The approach used was Latent Semantic Indexing, which is an automatic method not requiring dictionaries or thesauri. We describe the methods used along with an analysis of the results obtained.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Middlesex University is participating in CLEF for the first time. We have submitted runs for the bilingual task, using Portuguese queries to retrieve English documents. The approach adopted was Latent Semantic Indexing (LSI), which has achieved some promising results in previous experiments using other languages <ref type="bibr" coords="1,466.54,394.75,12.32,9.07" target="#b2">[5,</ref><ref type="bibr" coords="1,478.86,394.75,12.32,9.07" target="#b5">10]</ref> and has the great advantage of not requiring expensive resources such as thesauri. This paper is organised as follows: Section 2 presents some reasons for choosing Portuguese; Section 3 describes LSI and how it can be applied to CLIR; Section 4 reports our experiments and Section 5 analyses our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Why Portuguese?</head><p>Portuguese is the fifth biggest language in number of native speakers (see Figure <ref type="figure" coords="1,397.57,496.45,3.66,9.07" target="#fig_0">1</ref>). It is spoken on 4 continents: Europe (Portugal, Madeira, Azores), South America (Brazil), Africa (Angola, Mozambique, Cape Verde Islands, Guinea-Bissau) and Asia (Goa, Macau, East Timor). There are over 176 million native speakers and another 15 million people use it as a second language. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In Millions</head><p>On the other hand, according to the Internet Software Consortium [4], less than 1% of all web hosts are in Portuguese. In addition, only a small percentage of this population is competent in English, the vast majority (including university students) are not able to present good queries (in English) to a search engine like Google, Altavista, Lycos, etc. As a consequence, sources of information for the 8 million Portuguese speakers accessing the Internet are extremely limited compared to the immense amount of information available to the English speaking population. Moreover, no CLIR research has been done using Portuguese. For all those reasons we decided to put Portuguese in the picture, by using it in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CLIR using Latent Semantic Indexing</head><p>Latent Semantic Indexing <ref type="bibr" coords="2,179.32,209.53,11.82,9.07" target="#b0">[1]</ref> is a technique developed in 1990 by Dumais, Derweester, Landauer, Furnas and Harshman. The main goal is to retrieve on the basis of conceptual content rather than the actual terms used in the query. There are several ways of expressing a concept and the terms used in the query may not match the terms used in the documents.</p><p>LSI seeks to tackle synonymy and polysemy as they are the main problems with keyword matching. Synonymy is the fact that there are many ways to refer to the same object. And polysemy refers to the fact that many words have more than one distinct meaning. Attempts to solve the synonymy problem have been addressed by query expansion, which works by looking up a thesaurus and augmenting the query with related terms. The polysemy problem is considerably more difficult. Attempts to solve it include research done on word sense disambiguation. However, we are not aware of any adequate automatic method for dealing with it.</p><p>The main goal of using LSI for CLIR is to provide a method for matching text segments in one language with text segments of similar meaning in another language without needing to translate either, by creating a languageindependent representation of the words. This means that words are given an abstract description that does not depend on the original language. LSI is initially applied to a matrix of terms by documents (see Figure <ref type="figure" coords="2,352.45,545.41,3.66,9.07" target="#fig_1">2</ref>). Therefore, the first step is to build such a matrix based on a set of dual-language documents 1 . The matrix contains the number of occurrences (or weights) of each term in each document. In a ideal situation the pattern of occurrence of a term in language A should be identical to the pattern of occurrence of its match in language B. The resulting matrix tends to be very sparse, since most terms do not occur in every document. This matrix is then factorised by singular value decomposition 2 (SVD). SVD reduces the number of dimensions, throwing away the small sources of variability in term usage. The k most important dimensions are kept. Roughly speaking, these dimensions (or factors) may be thought as artificial concepts; they represent extracted common meaning components of many different words and documents. Each term or document is then characterised by a vector of weights indicating its strength of association with each of these underlying concepts. Since the number of factors or dimensions is much smaller than the number of unique terms, words will not be independent. For example, if two terms are used in similar documents, they will have similar vectors in the reduced-dimension representation.</p><formula xml:id="formula_0" coords="2,147.99,408.44,326.25,112.68">D 1 D 2 D 3 D 4 D 5 D 6 D 7 … D n Á g u a 3 0 4 0 0 1 1 … 0 A C a s a 0 0 2 1 1 1 0 … 0 P a p e l 0 0 0 0 0 0 3 … 1 P o r t a 0 0 1 1 0 1 1 … 0 … D o o r 0 0 1 1 0 1 1 … 0 B H o u s e 0 0 2 1 1 1 0 … 0 P a p e r 0 0 0 0 0 0 3 … 1 W a t e r 3 0 4 0 0 1 1 … 0 …</formula><p>It is possible to reconstruct the original term by document matrix from its factor weights with reasonable accuracy. However, it t not advisable to reconstruct it with perfect accuracy, as the original matrix contains noise, which can be eliminated through dimension reduction. LSI implements the vector-space model, in which terms, documents and queries are represented as vectors in a k-dimensional semantic space. The SVD of a sparse matrix A is given by: Where U and V are orthogonal matrices and Σ is the diagonal matrix of singular values. The columns of U and V contain the left and right singular vectors, respectively. The m × n matrix A k , which is constructed from the klargest single triplets of A, is the closest rank-k matrix to A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3 -Singular value decomposition</head><p>After deriving the semantic space with an initial sample of documents, new documents can be added (folded in), those are placed at the average of its corresponding terms. Queries are treated as pseudo-documents and placed at the weighted sum of its component term vectors. The similarity between query and documents is measured using the cosine between their vectors. SVD causes synonyms to be represented by similar vectors (since they would have many co-occurrences), which allows relevant documents to be retrieved even if they do not share any terms with the query. This is what makes LSI suitable for CLIR, given that a term in one language will be treated as a synonym to its match in the other language. The main advantages of using LSI for CLIR are:</p><p>• There is no translation. All terms and documents are transformed to a language-independent representation.</p><p>• New languages can be added easily.</p><p>• There is no need for expensive resources such as dictionaries, thesauri or machine translation systems.</p><p>As with any method, LSI also has its drawbacks:</p><p>• SVD is computationally expensive and it may take several hours to be computed, for a reasonably sized collection.</p><p>• The only currently known way to determine the number of dimensions is through trial and error as there is no automatic way of establishing the optimal number. LSI research suggests a number which is large enough to incorporate important concepts and small enough to not include noise. In previous experiments, this number was typically between 100 and 300.</p><p>• Parallel corpora are not always available In order to use LSI for a bilingual experiment we needed an initial sample of parallel documents, i.e. the original documents and their translations, to derive the concept space. However, the collection we used, the Los Angeles Times, was in English only. Therefore we used Systran [8] to translate 20% (approximately 22000 documents) of the collection into Portuguese. Figure <ref type="figure" coords="4,235.65,133.57,5.04,9.07">4</ref> shows a sample dual-language document used in the experiment. The translation is far from perfect, and many times the incorrect sense of a word was used, e.g. "branch" was translated to "filial" (shop branch), when the correct sense was "ramo" (tree branch). When the system did not have a translation for a term, it remained in the original language. Nevertheless, we did not perform any corrections or modifications on the resulting translations. </p><formula xml:id="formula_1" coords="3,148.35,195.52,289.03,242.05">A U V T = Σ A k U Σ V T = k k k k m × n m × r r × r r × n A k = Best</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4 -Sample dual-language document</head><p>We used the Porter stemmer <ref type="bibr" coords="4,186.65,300.97,11.82,9.07" target="#b4">[7]</ref> to stem the English documents, and our own stemmer <ref type="bibr" coords="4,418.70,300.97,12.00,9.07" target="#b3">[6]</ref> to stem the Portuguese translations. Stop words were also removed. Next step was to run the SVD on the 22,000 dual-language documents. We used a binary version of LSI provided by Telcordia Technologies <ref type="bibr" coords="4,401.45,326.89,10.80,9.07">[9]</ref>. An important aspect is the choice of the number of dimensions that will compose the concept space. We chose 700 dimensions since this is the number which gave best performance, within reasonable indexing time, when using last year's query topics.</p><p>It was also the highest number that our system could support.</p><p>The entries in the term by document matrix were the local weight (frequency of a term in a document) multiplied by the global weight of the term (number of occurrences of a term across the entire collection). The weighting scheme used was "log-entropy" which is given by the formula below. A term whose appearance tends to be equally likely among the documents is given a low weight and a term whose appearance is concentrated in a few documents is given a higher weight. The elements of our matrix will be of the form: tf ij = frequency of term i in document j gf i = total number of times term i occurs in the entire collection N = number of documents in the collection The next step was to "fold in" the remaining 91,000 English-only documents into that semantic space, which means that vector representations were calculated for those remaining documents. The resulting index had 70,000 unique terms, covering both languages. We did not index terms which were less than 3 characters long. We did not use phrases or multiword recognition, syntactic or semantic parsing, word sense disambiguation, heuristic association, spelling checking or correction, proper noun identification, a controlled vocabulary, a thesaurus, or any manual indexing.</p><formula xml:id="formula_2" coords="4,106.95,436.86,365.16,65.28">L( i , j ) * G( i ) Local Weighting: L( i , j ) = ) 1 log( + ij tf Global Weighting: G( i ) = ∑ = - N j ij ij N p p</formula><p>All terms and documents from both languages are represented in the same concept space. Therefore a query in one language may retrieve documents in any languages. This situation benefits from cross-linguistic homonyms, i.e. words that have the same spelling and meaning in both languages; e.g. "singular" is represented by one vector only, accounting for both languages. On the other hand, it suffers with "false friends", i.e. words that have the same spelling across languages but different meanings; e.g. "data" in Portuguese means "date" instead of "information". The problem in this case is that false friends are wrongly represented by only one point in space, placed at the average of all meanings. The ideal scenario would be taking advantage from cross-linguistic homonyms and at the same time avoiding false friends. We are still looking for a way to do that automatically.</p><p>In and ideal situation the similarity between a term and its translation should be very high (close to 100%). In order to evaluate how accurately LSI represents the correspondence between terms, we calculated the similarities between some English words and their counterparts in Portuguese. Table <ref type="table" coords="5,366.83,120.61,5.04,9.07" target="#tab_1">1</ref> shows the results for this experiment. The scores are generally quite high. However, when cross-linguistic polysemy is present, the similarity decreases significantly (see second column). This happens because term co-occurrences decrease in the presence of polysemy and results in a term and its translation being placed further apart in the concept space. We submitted three official runs:</p><p>• MDXman -keywords manually chosen from topic, description and narrative. The average number of words per query was 3.82 • MDXtpc -automatically chosen all terms from topic • MDXtd -automatically chosen all terms from topic and description Our results are summarised at Table <ref type="table" coords="5,223.02,376.57,3.79,9.07" target="#tab_2">2</ref>. Our best score was MDXman, closely followed by MDXtd. The worst result was MDXtpc. We attribute this difference to the fact that shorter queries provide less (or no) context information to the retrieval system. Our performance is consistent with the scores obtained using LSI for a monolingual task <ref type="bibr" coords="5,142.58,415.45,10.80,9.07" target="#b3">[6]</ref>. We are less than happy with this results, but we think they are quite reasonable considering that we used machine translation to create a sample of parallel documents. A better quality translation would almost certainly improve the scores. Analysing the sets of documents retrieved for each query, we observed that most highly ranked but irrelevant documents were generally on the query topic, but did not specifically meet the requirements. As an example, for query 121 -"Successes of Ayrton Senna", the documents retrieved were about him, but sometimes did not mention his sporting achievements. We also did not score well in queries 97, 98 and 131 for which there was only one relevant document. We believe that happened because we did not index words that occurred in one document only, and "Kaurismäkis", for example, was one of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run Average Precision R-Precision</head><p>In comparison to the other groups, we have achieved the best score in four topics: 126,130, 137 (MDXman) and 106 (MDXtd). Also in four topics we achieved the worst result: 91, 98, 134 (MDXtpc) and 108 (MDXtd). For MDXman, 22 topics were at or above the median and 20 topics were below the median. LSI outperforms keyword based searching in the cases where the words used in the query do not match the terms used in the relevant documents. This was the case of topic 137 -"international beauty contests", both relevant documents did not contain the keywords present in the topic. It had, however, related terms such as "miss world" and "miss universe". Our superior performance in those cases confirms that LSI can efficiently tackle the synonymy problem, modelling relationships between related terms and placing them close together in the vector-space.</p><p>Future work will include further analysis of those results in order to establish methods for improvement. We are also interested in finding automatic ways to minimise the effects of polysemy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,196.05,733.16,203.29,8.10"><head>Figure 1 -</head><label>1</label><figDesc>Figure 1-languages in number of native speakers [3]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,226.29,527.78,142.94,8.10"><head>Figure 2 -</head><label>2</label><figDesc>Figure 2 -Term by document matrix</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,185.43,427.53,99.15,8.33;3,148.35,439.77,149.04,8.33;3,148.35,449.17,75.06,12.05;3,148.35,464.07,171.74,8.33;3,329.61,426.63,78.29,8.33;3,329.61,437.79,95.21,8.33;3,329.61,448.95,122.41,8.33;3,329.61,459.93,113.83,8.33"><head></head><label></label><figDesc>rank-k approximation to A U = Left singular vectors (term vectors) Σ = Singular values V = Right singular vectors (document vectors) m = number of terms n = number of documents k = rank (number of dimensions) r = number of dimensions of A</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,150.33,179.12,285.46,122.58"><head>Table 1 -Term Similarity</head><label>1</label><figDesc></figDesc><table coords="5,150.33,179.12,285.46,108.18"><row><cell>baby</cell><cell>bebê</cell><cell>99.67%</cell><cell>Train</cell><cell>trem</cell><cell>12.30%</cell></row><row><cell>England</cell><cell cols="2">Inglaterra 99.24%</cell><cell>Train</cell><cell>treinar</cell><cell>84.97%</cell></row><row><cell>eat</cell><cell>comer</cell><cell>94.19%</cell><cell>Shade</cell><cell>sombra</cell><cell>31.88%</cell></row><row><cell>paper</cell><cell>papel</cell><cell>84.38%</cell><cell>Shadow</cell><cell>sombra</cell><cell>31.88%</cell></row><row><cell>book</cell><cell>livro</cell><cell>77.11%</cell><cell>Bank</cell><cell>banco</cell><cell>97.33%</cell></row><row><cell>Cyprus</cell><cell>Chipre</cell><cell>97.22%</cell><cell>Bench</cell><cell>banco</cell><cell>32.94%</cell></row><row><cell>car</cell><cell>carro</cell><cell>91.28%</cell><cell>Match</cell><cell>fósforo</cell><cell>96.72%</cell></row><row><cell>run</cell><cell>correr</cell><cell>51.03%</cell><cell>Match</cell><cell>jogo</cell><cell>36.28%</cell></row><row><cell>find</cell><cell>encontrar</cell><cell>80.06%</cell><cell>Game</cell><cell>jogo</cell><cell>91.30%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,191.01,471.62,209.28,45.90"><head>Table 2 -Summary of results</head><label>2</label><figDesc></figDesc><table coords="5,191.01,471.62,209.28,31.14"><row><cell>MDXman</cell><cell>20.85%</cell><cell>23.10%</cell></row><row><cell>MDXtpc</cell><cell>15.51%</cell><cell>16.76%</cell></row><row><cell>MDXtd</cell><cell>20.88%</cell><cell>21.68%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,133.77,705.43,390.79,9.07;2,70.95,720.36,138.08,9.07"><p>Dual-language documents are composed by the document in the original language together with its translation in another language.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,133.41,741.43,169.20,9.07"><p>Mathematics are presented in detail in<ref type="bibr" coords="2,288.21,741.43,10.80,9.07" target="#b0">[1]</ref>.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,92.19,94.52,432.43,8.10;6,92.19,106.58,321.56,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,418.70,94.52,105.92,8.10;6,92.19,106.58,29.84,8.10">Indexing by Latent Semantic Analysis</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,128.19,106.58,216.03,8.10">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.19,121.52,230.02,8.10;6,70.95,135.91,5.04,9.07;6,92.19,136.64,164.30,8.10" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<ptr target="http://www.ethnologue.com" />
		<title level="m" coord="6,141.89,121.52,180.32,8.10;6,70.95,135.91,5.04,9.07;6,92.19,136.64,60.98,8.10">Latent Semantic Indexing (LSI) : TREC-3 Report 3 ETHNOLOGUE</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.19,166.52,432.15,8.10;6,92.19,178.40,432.27,8.10;6,92.19,190.64,307.02,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,275.65,166.52,248.69,8.10;6,92.19,178.40,67.77,8.10">Fully Automatic Cross-Language Document Retrieval Using Latent Semantic Indexing</title>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Litman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Michael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,182.01,178.40,342.45,8.10;6,92.19,190.64,144.43,8.10">Proceedings of the Sixth Annual Conference of the UW Centre for the New Oxford English Dictionary and Text Research</title>
		<meeting>the Sixth Annual Conference of the UW Centre for the New Oxford English Dictionary and Text Research<address><addrLine>Waterloo, Ontario</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990-10">Oct. 1990</date>
			<biblScope unit="page" from="31" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.19,205.40,382.98,8.10;6,92.19,217.46,395.54,8.10;6,92.19,229.52,60.37,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,213.30,205.40,186.76,8.10">A Stemming algorithm for the Portuguese Language</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">M</forename><surname>Orengo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Huyck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,418.89,205.40,56.28,8.10;6,92.19,217.46,278.03,8.10">Proceedings of SPIRE&apos;2001 Symposium on String Processing and Information Retrieval</title>
		<meeting>SPIRE&apos;2001 Symposium on String Processing and Information Retrieval<address><addrLine>Laguna de San Raphael, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-11">November 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.19,244.64,321.93,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,151.04,244.64,122.34,8.10">An Algorithm for Suffix Stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,279.39,244.64,32.05,8.10">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980-07">July 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.19,289.46,372.60,8.10;6,464.79,287.59,5.28,5.35;6,473.61,289.46,51.09,8.10;6,96.51,301.52,336.18,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,175.00,289.46,256.20,8.10">Translingual Information Retrieval: Learning from Bilingual Corpora</title>
		<author>
			<persName coords=""><surname>Yang Yiming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,455.79,289.46,9.00,8.10;6,464.79,287.59,5.28,5.35;6,473.61,289.46,51.09,8.10;6,96.51,301.52,197.94,8.10">15 th International Joint Conference on Artificial Intelligence IJCAI&apos;97</title>
		<meeting><address><addrLine>Nagoya, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">August 23-29, 1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
