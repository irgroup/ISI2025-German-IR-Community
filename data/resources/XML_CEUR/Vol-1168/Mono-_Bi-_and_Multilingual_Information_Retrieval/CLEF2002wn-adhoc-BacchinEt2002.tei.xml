<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,386.26,137.19,36.78,12.62;1,95.10,155.12,405.72,12.62">2002: Experiments to evaluate a statistical stemming algorithm</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,175.50,188.67,71.54,8.74"><forename type="first">Michela</forename><surname>Bacchin</surname></persName>
							<email>michela.bacchin@unipd.it</email>
						</author>
						<author>
							<persName coords="1,267.85,188.67,53.60,8.74"><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
							<email>nicola.ferro@unipd.it</email>
						</author>
						<author>
							<persName coords="1,342.27,188.67,74.83,8.74"><forename type="first">Massimo</forename><surname>Melucci</surname></persName>
							<email>massimo.melucci@unipd.it</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua at CLEF</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">University of Padua</orgName>
								<address>
									<addrLine>Via Gradenigo 6</addrLine>
									<postCode>a -35131</postCode>
									<settlement>Padova</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,386.26,137.19,36.78,12.62;1,95.10,155.12,405.72,12.62">2002: Experiments to evaluate a statistical stemming algorithm</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">648A5C3D97195DA0DE9DA9B193AFA91A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In Information Retrieval (IR), stemming is used to reduce variant word forms to common root. The assumption is that if two words have the same root, then they represent the same concept. Hence stemming permits a IR system to match query and document terms which are related to a same meaning but which can appear in different morphological variants. In this paper we will report our participation in CLEF 2002 Italian monolingual task, whose aim was to evaluate a statistical stemming algorithm based on link analysis. Considering that a word is formed by a prefix (stem) and a suffix, the key idea is that the interlinked prefixes and suffixes form a community of substrings. Hence discovering these communities means searching for the best word splits which give the best word stems. The results show that stemming improves the IR effectiveness. They also show that effectiveness level of our algorithm, which does not incorporate any heuristics nor linguistic knowledge, is comparable to that of an algorithm based on a-priori linguistic knowledge. This is an encouraging result, particularly in a multi-lingual context.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.913" lry="842.74"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.913" lry="842.74"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.913" lry="842.74"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.913" lry="842.74"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.913" lry="842.74"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.913" lry="842.74"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.913" lry="842.74"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.913" lry="842.74"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The main objective of our current research in multi-lingual Information Retrieval is to design, implement and evaluate a language-independent stemming algorithm based on statistical methods. We participated in CLEF 2002 Italian monolingual task with the aim of evaluating a statistical algorithm based on linkanalysis methods. To accomplish this objectives we designed our experiments to investigate if stemming does not deteriorate or even enhances the effectiveness of retrieval of documents written in Italian by making use of both a linguistic stemmer and a statistical stemmer. Then we investigated if a statistical and language-independent stemming algorithm can perform as effectively as an algorithm developed on the basis of a-priori linguistic knowledge. This paper reports our participation in CLEF 2002 and it is organized as follows: Section 2 introduces the stemming process, section 3 reports the methodological approach we followed to build a new statistical and language-independent stemming algorithm, section 4 describes our official runs and the results we obtained, and section 5 reports some conclusions and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Stemming</head><p>Stemming is used to reduce variant word forms to common root. The assumption is that if two words have the same root, then they represent the same concept. Hence stemming permits a IR system to match query and document terms which are related to a same meaning but which can appear in different morphological variants.</p><p>The effectiveness of stemming is a debated issue, and there are different results and conclusions. If effectiveness is measured by the traditional precision and recall measures, it seems that for a language with a relatively simple morphology, like English, stemming influences the overall performance little <ref type="bibr" coords="2,496.15,126.28,9.97,8.74" target="#b5">[6]</ref>. In contrast, stemming can significantly increase the retrieval effectiveness <ref type="bibr" coords="2,376.10,138.24,15.50,8.74" target="#b12">[13]</ref> and can also increase precision for short queries, <ref type="bibr" coords="2,146.71,150.19,10.52,8.74" target="#b7">[8]</ref> for languages with a more complex morphology, like the romance languages. Finally, as the system performance must reflect user's expectations it has to be considered that the use of a stemmer is intuitive to many users <ref type="bibr" coords="2,229.08,174.10,9.97,8.74" target="#b5">[6]</ref>, who can express the query to the system using a specific word without keeping in mind that only a variant of this word can appear in a relevant document. Hence, stemming can be viewed also as a sort of feature related to the user-interaction interface of an IR service.</p><p>To design a stemming algorithm, it is possible to follow a linguistic approach, using prior knowledge of the morphology of the specific language, or a statistical approach using some methods based on statistical principles to infer from the corpus of documents the word formation rules in the language studied. The former implies manual labor which has to be done by experts in linguistics -as matter of the fact, it is necessary to formalize the word formation rules, the latter being hard work, especially for those languages whose morphology is complex. Stemming algorithms based on statistical methods ensure no costs for inserting new languages on the system, and this is an advantage that becomes crucial especially for multilingual IR systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodological Approach</head><p>We will consider a special case of stemming, which belongs to the category known as affix removal stemming <ref type="bibr" coords="2,116.57,356.39,9.96,8.74" target="#b2">[3]</ref>. In particular our approach stays on a suffix stripping paradigm which is adopted by most stemmers currently in use by IR, like those reported in <ref type="bibr" coords="2,307.24,368.35,10.52,8.74" target="#b8">[9,</ref><ref type="bibr" coords="2,320.42,368.35,12.73,8.74" target="#b11">12,</ref><ref type="bibr" coords="2,335.82,368.35,11.62,8.74" target="#b14">15]</ref>. This stemming process splits each word into two parts, prefix and suffix, and considers the stem as the substring corresponding to the obtained prefix. Let us consider a finite collection of unique words W = {w 1 , ..., w N } and a word w ∈ W of length |w|, then w can be written as w = xy where x is a prefix and y is a suffix. If we split each word w into all the |w| -1 possible pairs of substrings, we build a collection of substrings, and each substring may be either a prefix, a suffix or both of at least an element w ∈ W . Let X be the set of the prefixes of the collection and S ⊆ X be the set of the stems. We are interested in detecting the prefix x that is the most probable stem for the observed word w. Hence, we have to determine the prefix x * such as:</p><formula xml:id="formula_0" coords="2,195.67,471.88,329.38,16.21">x * = arg max x P r(x ∈ S | w ∈ W )<label>(1)</label></formula><p>= arg max</p><formula xml:id="formula_1" coords="2,256.29,492.38,268.76,22.31">x P r(w ∈ W | x ∈ S)P r(x ∈ S) P r(w ∈ W )<label>(2)</label></formula><p>where ( <ref type="formula" coords="2,104.11,526.88,4.24,8.74" target="#formula_1">2</ref>) is obtained applying the Bayes' theorem which lets us swap the order of dependence between events. We can ignore the denominator, which is the same for all splits of w. P r(w</p><formula xml:id="formula_2" coords="2,459.18,538.83,65.87,8.74">∈ W | x ∈ S)</formula><p>is the probability of observing w given that the stem x has been observed. A reasonable estimation of that probability would be the reciprocal of the number of words beginning by that stem if the stems were known. However note that the stems are unknown -indeed stem detection is the target of this method -and the number of words beginning by a stem cannot be computed. Therefore we estimated that probability by the reciprocal of the number of words beginning by that prefix. As regards P r(x ∈ S) we estimated this probability using an algorithm that discloses the mutual relationship between stems and derivations in forming the words of the collection. The rationale of using mutual reinforcement is based on the idea that stems extracted from W are those substrings that:</p><p>-are very frequent, and -form words together with very frequent suffixes. This means that very frequent prefixes are candidate to be stems, but they are discarded if they are not followed by very frequent suffixes; for example, all initials are very frequent prefixes but they are unlikely stems because the corresponding suffixes are rather rare, if not unique -the same holds for suffixes corresponding to ending vowels or consonants. Thus, there are prefixes being less frequent than initials, but followed by frequent suffixes, yet less frequent than ending characters: these suffixes and prefixes correspond to candidate correct word splits and we label them as "good". The key idea is that interlinked good prefixes and suffixes form a community of substrings whose links correspond to words, i.e. to splits. Discovering these communities is like searching for the best splits.</p><p>To compute the best split, we used the quite well-known algorithm called HITS reported in <ref type="bibr" coords="3,494.75,174.10,10.51,8.74" target="#b6">[7]</ref> and often discussed in many research papers as a paradigmatic algorithm for Web page retrieval. It considers a mutually reinforcing relationship among good authorities and good hubs, where an authority is a web page pointed to by many hubs and a hub is a web page which points to many authorities. The parallel with our context will be clear when we associate the concept of a hub to a prefix and that of authority to a suffix. The method belongs to the larger class of approaches based on frequencies of substrings to decide the goodness of prefixes and suffixes, often used in statistical morphological analysis <ref type="bibr" coords="3,466.01,245.83,15.50,8.74" target="#b10">[11,</ref><ref type="bibr" coords="3,484.37,245.83,7.01,8.74" target="#b3">4]</ref>, and in the pioneer work <ref type="bibr" coords="3,146.74,257.79,9.96,8.74" target="#b4">[5]</ref>. The contribution of this paper is the use of mutual reinforcement notion applied to prefix frequencies and suffix frequencies, to compute the best word splits which give the best word stems as explained in the following.</p><p>Using a graphical notation, the set of prefixes and suffixes can be written as a graph g such that nodes are substrings and an edge occurs between nodes x, y if w = xy is a word in W . By definition of g, no vertex is isolated. As an example, let us consider the following toy set of words: W ={aba, abb, baa}; splitting these into all the possible prefixes and suffixes produces a graph, reported in Figure <ref type="figure" coords="3,479.25,329.52,8.49,8.74" target="#fig_2">3a</ref>. If a directed edge exists between x and y, the mutual reinforcement notion can be stated as follows:</p><p>good prefixes point to good suffixes, and good suffixes are pointed to by good prefixes.</p><p>Let us define P (y) = {x : ∃w, w = xy} and S(x) = {y : ∃w, w = xy} that are, respectively, the set of all prefixes of a given suffix y and the set of all suffixes of a given prefix x. If p x and s x indicate, respectively, the prefix score and the suffix score, the criteria can be expressed as:</p><formula xml:id="formula_3" coords="3,222.47,623.65,302.58,20.53">p x = y∈S(x) s y s y = x∈P (y) p x<label>(3)</label></formula><p>under the assumption that scores are expressed as sums of scores and splits are equally weighed.</p><p>The method of mutual reinforcement has been formalized through the HITS iterative algorithm. Here we map HITS in our study context, as follows:</p><p>Compute suffix scores and prefix scores from W V : the set of substrings extracted from all the words in W P (y): the set of all prefixes of a given suffix y S(x): the set of all suffixes of a given prefix x N : the number of all substrings in V n: the number of iterations 1: the vector (1, ..., 1) ∈ R |V | 0: the vector (0, ..., 0) ∈ R |V | s (k) : suffix score vector at step k p (k) : prefix score vector at step k</p><formula xml:id="formula_4" coords="4,70.87,198.82,262.37,140.46">s (0) = 1 p (0) = 1 for each iteration k = 1, ..., n s (k) = 0 p (k) = 0 for each y ∈ V s (k) y = x∈P (y) p (k-1) x ; for each x ∈ V p (k) x = y∈S(x) s (k) y ; normalize p (k) and s (k) so that 1 = x p (k) x = y s (k) y end.</formula><p>Using the matrix notation, the graph g can be described with a |V | × |V | matrix M such that m ij = 1 if prefix i and suffix j form a word 0 otherwise</p><p>As explained in <ref type="bibr" coords="4,140.05,419.66,9.96,8.74" target="#b6">[7]</ref>, the algorithm computes two matrices: A = M T M and B = MM T , where the generic element a ij of A is the number of vertices that are pointed by both i and j, whereas the generic element b ij of B is the number of vertices that point to both i and j. The n-step iteration of the algorithm corresponds to computing A n and B n . In the same paper, it has been argued that s = [s y ] and p = [p x ] converge to the eigenvectors of A and B, respectively. The scores computed for the toy set of words are reported in Table <ref type="table" coords="4,150.07,479.44,8.85,8.74" target="#tab_2">3b</ref>.</p><p>As explained previously, we argue that the probability that x is a stem, can be estimated with the prefix score p x just calculated. The underlying assumption is that the scores can be seen as probabilities, and, in effect, it has been proved in a recent work that HITS scores can be considered as a stationary distribution of a random walk <ref type="bibr" coords="4,208.65,527.26,9.96,8.74" target="#b0">[1]</ref>. In particular, the authors proved the existence of a Markov chain, which has the stationary distribution equal to the hub vector after the n th iteration of the Kleinberg's algorithm, which is, in our context, the prefix score vector p = [p x ]. The generic element q (n) ij of the transition matrix referred to the chain is the probability that, starting from i, one reaches j after n "bouncing" to one of the suffixes which begins to be associated with i and j. To interpret the result in a linguistic framework, p i can be seen as the probability that i is judged as a stem by the same community of substrings (suffixes) being resulted by the process of splitting words of a language. In Table <ref type="table" coords="4,486.31,600.37,3.88,8.74" target="#tab_0">1</ref>, all the possible splits for all the words are reported and measured using the estimated probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>The aim of CLEF 2002 experiments is to compare the retrieval effectiveness of the link analysis-based algorithm illustrated in the previous Section with that of an algorithm based on a-priori linguistic knowledge, because the hypothesis is that a language-independent algorithm, such as the one we propose, might effectively replace one developed on the basis of manually coded derivational rules. Before comparing the algorithms, we assessed the impact of both stemming algorithms by comparing their effectiveness with that reached without any stemmer. In fact, we did want to test if the system performance is not significantly hurt by the application of stemming, as hypothesized in <ref type="bibr" coords="5,369.45,250.02,9.97,8.74" target="#b5">[6]</ref>. If stemming did on the contrary improve effectiveness, and the effectiveness of the tested algorithms were comparable, the link-based algorithm would ensure low costs for extending it also to other languages, which is crucial in multi-lingual settings. To evaluate stemming, we decided to compare the performance of an IR system changing only the stemming algorithms for different runs, all other things being equal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Prototype System</head><p>For indexing and retrieval, we used an experimental IR system, called IRON, which has been realized by our research group with the aim of having a robust tool for carrying out IR experiments. IRON is built on top of the Lucene 1.2 RC4 library, which is an open-source library for IR written in Java and publicly available in <ref type="bibr" coords="5,124.69,379.98,14.62,8.74" target="#b9">[10]</ref>. The system implements the vector space model <ref type="bibr" coords="5,363.34,379.98,14.61,8.74" target="#b15">[16]</ref>, and a (tf • idf)-based weighting scheme <ref type="bibr" coords="5,104.20,391.94,14.61,8.74" target="#b16">[17]</ref>. The stop-list which was used consists of 409 Italian frequent words and it is publicly available in <ref type="bibr" coords="5,82.49,403.89,14.61,8.74" target="#b17">[18]</ref>.</p><p>As regards the realization of the statistical stemming algorithm, we built a suite of tools, called Stemming Program for Language Independent Tasks (SPLIT), which implements the link-based algorithm and chooses the best stem, according to the probabilistic criterion described in Section 3. From the vocabulary of the Italian CLEF sub-collection, SPLIT spawns a 2,277,297-node and 1,215,326-edge graph, which is processed to compute prefix and suffix scores -SPLIT took 2.5 hours for 100 iterations on a personal computer equipped with Linux, an 800 MHz Intel CPU and 256MB RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Runs</head><p>We tested four different stemming algorithms:</p><p>1. NoStem: No stemming algorithm was applied.</p><p>2. Porter-like: We used the stemming algorithm for the Italian language, which is freely available in the Snowball Web Site <ref type="bibr" coords="5,212.86,573.71,15.50,8.74" target="#b13">[14]</ref> edited by M. Porter. Besides being publicly available for research purposes, we have chosen this algorithm because it uses a kind of a-priori knowledge of the Italian language, so comparing our SPLIT algorithm with this particular "linguistic" algorithm could give some information about the possibility of estimating linguistic knowledge with statistically inferred knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SPLIT:</head><p>We implemented our first version of the stemming algorithm based on a link-analysis with 100 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SPLIT-L3:</head><p>We included in our stemming algorithm a little ignition of linguistic knowledge, inserting a heuristic rule which forces the length of the stem to be at least 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">A Global Evaluation</head><p>We carried out a macro evaluation by averaging the results over all the queries of the test collection. Note that both for 2001 and 2002 topics, all the considered stemming algorithms improve recall, since the number of retrieved relevant documents is larger than the number of retrieved relevant documents observed in the case of retrieval without any stemmer; the increase has been observed for all the stemming algorithms. As regards the precision, while for 2002 topics, stemming does not hurt the overall performances of the system, for 2001 data, stemming even increases the precision, and then the overall performance is higher thanks to the application of stemming.</p><p>Figure <ref type="figure" coords="6,116.74,455.19,4.98,8.74" target="#fig_1">2</ref> shows the Averaged Recall-Precision curve at different levels of recall and Figure <ref type="figure" coords="6,474.51,455.19,4.98,8.74" target="#fig_2">3</ref>   the use of link-based stemming algorithms, it is worth noting that SPLIT can attain levels of effectiveness being comparable to one based on linguistic knowledge. This is surprising if you know that SPLIT was  built without any sophisticated extension to HITS and that neither heuristics nor linguistic knowledge was used to improve effectiveness. It should also be considered as a good result, if you consider that it has also been obtained for the Italian language, which is morphologically more complex than English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>The objective of this research was to investigate a stemming algorithm based on link analysis procedures. The idea has been that prefixes and suffixes, that are stems and derivations, form communities once extracted from words. We tested this hypothesis by comparing the retrieval effectiveness of SPLIT, a link analysis based algorithm derived from HITS, with a linguistic knowledge based algorithm, on a quite morphologically complex language as it is the Italian language.</p><p>The results are encouraging because effectiveness level of SPLIT is comparable to that developed by Porter. The results should be considered even better since SPLIT does not incorporate any heuristics nor linguistic knowledge. Moreover, stemming, and then SPLIT, showed to improve effectiveness with respects to not using any stemmer.</p><p>We are carrying out further analysis at a micro level to understand the conditions under which SPLIT performs better or worse compared to other algorithms. Further work is in progress to improve the probabilistic decision criterion and to insert linguistic knowledge directly in the link-based model by thus weighting links among prefixes and suffixes with a probabilistic function which could capture available information on the language, such as, for example, the minimum length of a stem. Finally, further experimental work is in progress with other languages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,115.49,515.12,363.55,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) The graph obtained from W . (b) The prefix and suffix scores from W</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,153.71,687.30,288.51,8.74"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Average Precision Curves for four stemming algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,167.63,299.29,260.66,8.74"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: R-Precision Curves for four stemming algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,126.47,101.18,342.96,125.48"><head>Table 1 :</head><label>1</label><figDesc>The candidate splits from W ={aba, baa, abb}.</figDesc><table coords="5,126.47,101.18,342.96,93.10"><row><cell cols="7">word prefix suffix words beginning words ending probability choice</cell></row><row><cell></cell><cell></cell><cell></cell><cell>by prefix</cell><cell>by suffix</cell><cell></cell><cell></cell></row><row><cell>baa</cell><cell>b</cell><cell>aa</cell><cell>1</cell><cell>1</cell><cell>0.1250</cell><cell></cell></row><row><cell>baa</cell><cell>ba</cell><cell>a</cell><cell>1</cell><cell>2</cell><cell>0.2500</cell><cell>*</cell></row><row><cell>aba</cell><cell>a</cell><cell>ba</cell><cell>2</cell><cell>1</cell><cell>0.1250</cell><cell></cell></row><row><cell>aba</cell><cell>ab</cell><cell>a</cell><cell>2</cell><cell>2</cell><cell>0.1875</cell><cell>*</cell></row><row><cell>abb</cell><cell>a</cell><cell>bb</cell><cell>2</cell><cell>1</cell><cell>0.1250</cell><cell></cell></row><row><cell>abb</cell><cell>ab</cell><cell>b</cell><cell>2</cell><cell>1</cell><cell>0.1875</cell><cell>*</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,70.87,132.72,454.18,204.65"><head>Table 2 :</head><label>2</label><figDesc>Table 2 shows a summary of the figures related to the macro analysis of the stemming algorithm for 2002 topics, while table 3 reports 2001 data. Macro comparison among runs for 2002 topics.</figDesc><table coords="6,121.38,166.71,353.15,170.65"><row><cell>Run ID</cell><cell cols="2">Algorithm</cell><cell cols="3">N. Relevant Retrieved Av. Precision R-Precision</cell></row><row><cell>PDDN</cell><cell>NoStem</cell><cell></cell><cell>887</cell><cell>0.3193</cell><cell>0.3367</cell></row><row><cell>PDDP</cell><cell cols="2">Porter-like</cell><cell>914</cell><cell>0.3419</cell><cell>0.3579</cell></row><row><cell>PDDS2PL</cell><cell>SPLIT</cell><cell></cell><cell>913</cell><cell>0.3173</cell><cell>0.3310</cell></row><row><cell cols="2">PDDS2PL3 SPLIT-L3</cell><cell></cell><cell>911</cell><cell>0.3200</cell><cell>0.3254</cell></row><row><cell cols="2">Algorithm</cell><cell cols="4">N. Relevant Retrieved Av. Precision R-Precision</cell></row><row><cell cols="2">NoStem</cell><cell></cell><cell>1093</cell><cell>0.3387</cell><cell>0.3437</cell></row><row><cell cols="2">Porter-like</cell><cell></cell><cell>1169</cell><cell>0.3753</cell><cell>0.3619</cell></row><row><cell>SPLIT</cell><cell></cell><cell></cell><cell>1143</cell><cell>0.3519</cell><cell>0.3594</cell></row><row><cell cols="2">SPLIT-L3</cell><cell></cell><cell>1149</cell><cell>0.3589</cell><cell>0.3668</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,175.91,360.17,244.08,8.74"><head>Table 3 :</head><label>3</label><figDesc>Macro comparison among runs for 2001 topics.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,70.87,455.19,454.17,215.61"><head></head><label></label><figDesc>illustrates the Recall-Precision curve at given document cutoff values, both for 2002 and 2001 topic sets. As regards</figDesc><table coords="6,80.79,489.49,434.03,181.31"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Topic 2002 -Interpolated recall vs average precision</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Topic 2001 -Interpolated recall vs average precision</cell><cell></cell><cell></cell></row><row><cell></cell><cell>80%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NoStem</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NoStem</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Porter-like</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Porter-like</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SPLIT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SPLIT</cell></row><row><cell></cell><cell>70%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SPLIT-L3</cell><cell></cell><cell>70%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SPLIT-L3</cell></row><row><cell></cell><cell>60%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>60%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Average Precision</cell><cell>40% 50%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Average Precision</cell><cell>40% 50%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>30%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0% 0%</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>50%</cell><cell>60%</cell><cell>70%</cell><cell>80%</cell><cell>90%</cell><cell>100%</cell><cell>0% 0%</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>50%</cell><cell>60%</cell><cell>70%</cell><cell>80%</cell><cell>90%</cell><cell>100%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Interpolated Recall</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Interpolated Recall</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(a) 2002 Topics</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(b) 2001 Topics</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,91.35,627.95,433.70,8.74;7,91.35,639.90,433.70,8.74;7,91.35,651.86,175.18,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,370.57,627.95,154.48,8.74;7,91.35,639.90,170.97,8.74">Finding authorities and hubs from link structures on the World Wide Web</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Borodin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">O</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Tsaparas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,285.29,639.90,207.79,8.74">Proceedings of the World Wide Web Conference</title>
		<meeting>the World Wide Web Conference<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="415" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,91.35,671.79,433.70,8.74;7,91.35,683.74,356.30,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,156.62,671.79,208.77,8.74">The Cranfield Tests on Index Language Devices</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cleverdon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,125.29,683.74,147.10,8.74">Readings in Information Retrieval</title>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Sparck</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Willett</surname></persName>
		</editor>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="47" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,91.35,703.67,433.69,8.74;7,91.35,715.62,46.78,8.74" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="7,244.21,703.67,235.14,8.74">Information Retrieval: data structures and algorithms</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Frakes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.35,102.37,433.71,8.74;8,91.35,114.33,142.18,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,161.33,102.37,286.30,8.74">Unsupervised learning of the morphology of a natural language</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Goldsmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,461.14,102.37,63.91,8.74;8,91.35,114.33,45.61,8.74">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="198" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.35,134.25,433.70,8.74;8,91.35,146.21,121.18,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,199.57,134.25,208.04,8.74">Word segmentation by letter successor varieties</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hafer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,417.25,134.25,107.80,8.74;8,91.35,146.21,37.33,8.74">Information Storage and Retrieval</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="371" to="385" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.35,166.13,433.70,8.74;8,91.35,178.09,74.16,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,149.34,166.13,113.66,8.74">How effective is suffixing?</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,270.97,166.13,249.73,8.74">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="15" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.35,198.01,433.70,8.74;8,91.35,209.97,93.01,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,152.27,198.01,223.25,8.74">Authoritative sources in a hyperlinked environment</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,385.01,198.01,84.73,8.74">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="604" to="632" />
			<date type="published" when="1999-09">September 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.35,229.89,433.70,8.74;8,91.35,241.85,361.21,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,145.31,229.89,190.90,8.74">Viewing Morphology as an Inference Process</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Krovetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,360.86,229.89,164.19,8.74;8,91.35,241.85,330.30,8.74">Proceedings of the ACM International Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<meeting>the ACM International Conference on Research and Development in Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.35,261.77,433.70,8.74;8,91.35,273.73,104.59,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,138.84,261.77,166.07,8.74">Development of a stemming algorithm</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lovins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,314.33,261.77,210.72,8.74;8,91.35,273.73,30.98,8.74">Mechanical Translation and Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="22" to="31" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.35,293.65,413.12,9.02" xml:id="b9">
	<monogr>
		<ptr target="http://jakarta.apache.org/lucene/docs/index.html" />
		<title level="m" coord="8,91.35,293.65,86.76,8.74">The Jakarta Project</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.35,313.58,433.69,8.74;8,91.35,325.53,51.76,8.74" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="8,237.35,313.58,234.72,8.74">Foundations of statistical natural language processing</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.35,345.46,314.36,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,145.30,345.46,75.40,8.74">Another Stemmer</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Paice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,242.76,345.46,84.68,8.74">ACM SIGIR Forum</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="56" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.35,365.38,433.70,8.74;8,91.35,377.34,403.96,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,222.14,365.38,302.91,8.74;8,91.35,377.34,51.75,8.74">The effectiveness of stemming for natural-language access to sloven textual data</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Popovic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Willett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,151.94,377.34,246.58,8.74">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="383" to="390" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.35,397.26,433.70,9.02;8,91.35,409.22,22.70,8.74" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="8,143.30,397.26,206.52,8.74">Snowball: A language for stemming algorithms</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Porter</surname></persName>
		</author>
		<ptr target="http://snowball.sourceforge.net" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.35,429.15,340.36,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,150.61,429.15,140.77,8.74">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,299.96,429.15,34.40,8.74">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.34,449.07,433.71,8.74;8,91.35,461.03,43.73,8.74" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="8,208.82,449.07,198.44,8.74">Introduction to modern Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mcgill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>McGraw-Hill</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.34,480.95,433.71,8.74;8,91.35,492.91,210.57,8.74" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="8,217.78,480.95,243.47,8.74">Term weighting approaches in automatic text retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,472.96,480.95,52.09,8.74;8,91.35,492.91,112.84,8.74">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="513" to="523" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.34,512.83,433.71,8.74;8,91.35,524.79,236.05,9.02" xml:id="b17">
	<monogr>
		<ptr target="http://www.unine.ch/info/clef/" />
		<title level="m" coord="8,91.34,512.83,368.46,8.74">Institut interfacultaire d&apos;informatique. CLEF and Multilingual information retrieval</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>University of Neuchatel</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.34,544.71,312.72,9.02" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<ptr target="ftp://ftp.cs.cornell.edu/pub/smart/" />
		<title level="m" coord="8,145.44,544.71,42.35,8.74">Trec eval</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
