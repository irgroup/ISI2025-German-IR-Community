<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,113.52,110.94,368.27,15.49">Cross-language Retrieval Experiments at CLEF-2002</title>
				<funder ref="#_M622eU5">
					<orgName type="full">DARPA</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,270.24,143.39,54.82,10.76"><forename type="first">Aitao</forename><surname>Chen</surname></persName>
							<email>aitao@sims.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Management and Systems</orgName>
								<orgName type="institution">University of California at Berkeley</orgName>
								<address>
									<postCode>94720</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,113.52,110.94,368.27,15.49">Cross-language Retrieval Experiments at CLEF-2002</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">784C00310EE35AA77A94070EC639DD71</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes monolingual, cross-language, and multilingual retrieval experiments using CLEF-2002 test collection. The paper presents a technique for incorporating blind relevance feedback into a document ranking formula based on logistic regression analysis, and a procedure for decomposing German or Dutch compounds into their component words.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multilingual text retrieval is the task of searching for relevant documents in a collection of documents in more than one language in response to a query, and presenting a unified ranked list of documents regardless of language. Multilingual retrieval is an extension of bilingual retrieval where the collection consists of documents in a single language that is different from the query language. Recent developments on multilingual retrieval were reported in CLEF-2000 <ref type="bibr" coords="1,130.98,375.88,15.27,8.97" target="#b11">[12]</ref>, and CLEF-2001 <ref type="bibr" coords="1,219.18,375.88,15.27,8.97" target="#b12">[13]</ref>. Most of the multilingual retrieval methods fall into one of three groups. The first approach translates the source topics separately into all the document languages in the document collection. Then monolingual retrieval is carried out separately for each document language, resulting in one ranked list of documents for each document language. Finally the intermediate ranked lists of retrieved documents, one for each language, are merged to yield a combined ranked list of documents regardless of language. The second approach translates a multilingual document collection into the topic language. Then the topics are used to search against the translated document collection. The third one also translates topics to all document languages as in the first approach. The source topics and the translated topics are concatenated to form a set of multilingual topics. The multilingual topics are then searched directly against the multilingual document collection, which directly produces a ranked list of documents in all languages. The latter two approaches do not involve merging two or more ranked lists of documents, one for each document language, to form a combined ranked list of documents in all document languages. The merging task is hard and challenging. To the best of our knowledge, no effective technique has been developed yet. It appears most participating groups of the multilingual retrieval tasks in the TREC or CLEF evaluation conferences applied the first approach. Translating large collections of documents in multiple languages into topic languages requires the availability of machine translation systems that support the necessary language pairs, which is sometime problematic. For example, if the document collection consists of documents in English, French, German, Italian, and Spanish, and the topics are in English. To perform the multilingual retrieval task using English topics, one would have to translate the French, German, Italian, and Spanish documents into English. In this case, there exist translators, such as Babelfish, that can do the job. However, if the topics are in Chinese or Japanese, it may be more difficult or even not possible to find the translators to do the work. The availability of the translation resources and the need for extensive computation are factors that limit the applicability of the second approach. The third approach is appealing in that it does not require to translate the documents, and circumvents the difficult merging problem. However, there is some empirical evidence showing that the third approach is less effective than the first one <ref type="bibr" coords="1,295.74,650.86,10.58,8.97" target="#b2">[3]</ref>.</p><p>We believe that three of the core components of the first approach are monolingual retrieval, topic translation, and merging. Performing multilingual retrieval requires many language resources such as stopwords, stemmers, bilingual dictionaries, machine translation systems, parallel or comparable corpora. At the same time, we see more and better language resources publicly available on the Internet. The end performance of multilingual retrieval can be affected by many factors such as monolingual retrieval performance of the document ranking algorithm, the quality and coverage of the translation resources, the availability of language-dependent stemmers and stopwords, and the effectiveness of merging algorithm. Since merging of ranked lists of documents is a challenging task, we seek to improve multilingual retrieval performance by improving monolingual retrieval performance and exploiting translation resources publicly available on the Internet.</p><p>At CLEF 2002, we participated in the monolingual, crosss-language, and multilingual retrieval tasks. For monolingual task, we submitted retrieval runs for Dutch, French, German, Italian, and Spanish. For cross-language task, we submitted cross-language retrieval runs from English topics to document languages Dutch, French, German, Italian, and Spanish, one French-to-German run, and one German-to-French run. And for multilingual task, we submitted two runs using English topics. All of our runs used only the title and desc fields in the topics. The document collection for multilingual task consists of documents in English, French, German, Italian and Spanish. More details on document collections are presented below in section 5. Realizing the difficulty of merging multiple disjoint ranked lists of retrieved documents in multilingual retrieval, we have put little effort on the merging problem. We mainly worked on improving the performances of monolingual retrieval and cross-language retrieval since we believe improved performances in monolingual and cross-language retrieval should ultimately lead to better performance in multilingual retrieval. For all of our runs in cross-language and multilingual tasks, the topics was translated into document languages. The main translation resources we used are the SYSTRAN-based online machine translation system Babelfish translation and L&amp;H Power Translator Pro Version 7.0. We also used parallel English/French texts in one of the English-to-French retrieval runs. The Babylon English-Dutch dictionary was used in cross-language retrieval from English to Dutch.</p><p>The same document ranking formula developed at Berkeley <ref type="bibr" coords="2,330.36,277.24,11.62,8.97" target="#b3">[4]</ref> back in 1993 was used for all retrieval runs reported in this paper. It was also used in our participation in the previous CLEF workshops. It has been shown that query expansion via blind relevance feedback can be effective in monolingual and cross-language retrieval. The Berkeley formula based on logistic regression has been used for years without blind relevance feedback. We developed a blind relevance feedback procedure for the Berkeley document ranking formula. All of our official runs were produced with blind relevance feedback. We will present a brief overview of the Berkeley document ranking formula in section 2. We will describe the blind relevance feedback procedure in section 3.</p><p>At CLEF 2001, we presented a German decompounding procedure that was hastily developed. The decompounding procedure uses a German base dictionary consisting of words that should not be further decomposed into smaller components. When a compound can be split into component words found in the base dictionary in more than one way, we choose to split up the compound so that the number of component words is the smallest. However if there two or more decompositions with the smallest number of component words, we choose the decomposition that is most likely. The probability for a decomposition of a compound is computed based on the relative frequencies of the component words in a German collection. We reported a slight decrease in German monolingual performance with German decompounding <ref type="bibr" coords="2,303.30,444.64,11.62,8.97" target="#b2">[3]</ref> at CLEF 2001. The slight decline in performance may be attributed to the fact that we kept both the original compounds and the component words resulted from decompounding in topic index. When we re-ran the same German monolingual retrieval with only the component words of compounds in the topics were retained, the average precision was improved by 8.88% with decompounding over without it <ref type="bibr" coords="2,147.96,492.46,10.58,8.97" target="#b2">[3]</ref>. Further improvements in performance brought by German decompounding were reported in <ref type="bibr" coords="2,81.12,504.40,11.62,8.97" target="#b2">[3]</ref> when a different method was used to compute the relative frequencies of component words.</p><p>At CLEF 2002, we used the improved version of the German decompounding procedure first described in <ref type="bibr" coords="2,500.15,516.40,10.58,8.97" target="#b2">[3]</ref>. A slightly different presentation of the same decompounding procedure is given in section 4. Two small changes were made in performing German retrieval with decompounding. Firstly, in both topic and document indexes, only the component words resulted from decompounding were kept. When a compound was split into component words, the compound itself was not indexed. Secondly, additional non-German words in the German base dictionary were removed. Our current base dictionary still has 762,342 words, some being non-German words and some being German compounds that should be excluded. It would take a major effort to clean up the base dictionary so that it contains only the German words that should not be further decomposed. The decompounding procedure initially developed for splitting up German compounds was also used to decompose Dutch compounds with a Dutch base dictionary.</p><p>For the submitted two official multilingual runs, one used unnormalized raw score to re-rank the documents from intermediate runs to produce the unified ranked list of documents. The other run used normalized score in the same way to produce the final list. To measure the effectiveness of different mergers, we developed an algorithm for computing the best performance that could possibly be achieved by merging multiple ranked lists of documents under the conditions that the relevances of the documents are known, and that the relative ranking of the documents in individual ranked lists is preserved in the unified ranked list. That is, if document is ranked higher than document ¡ in some ranked list, then in the unified ranked list, document should also be ranked higher that document ¡ . The simple mergers based on unnormalized raw score, normalized raw score, or rank all preserve the relative ranking order. This procedure cannot be used to predict merging, however it should be useful for measuring the performance of merging algorithms. The procedure for producing optimal performance given document relevances is presented in section 6.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Document Ranking</head><p>All of our retrieval runs used the same document ranking formula developed at Berkeley <ref type="bibr" coords="3,426.24,129.94,11.66,8.97" target="#b3">[4]</ref> to rank documents in response to a query. The log odds of relevance of document ¢ with respect to query £ , denoted by ¤ ¦¥ ¨ § © ¢ £ ! , is given by</p><formula xml:id="formula_0" coords="3,88.60,170.20,132.20,26.90">¤ "¥ # § © ¢ £ ! %$ '&amp; )( 10 2 ¢ 3£ 4 2 ¢ 3£ 4</formula><p>$ 65 87 @9 BA @C ED F7 HG I9 P RQ TS VU WD YX `9 a7 ¨7 #X bQ TS dc e5 fX `9 "C hg ¨7 HG 8Q TS pi ED qX `9 aX ¨g ¨r ¨g bQ sS ut where 2 v ¢ 3£ 4 is the probability that document ¢ is relevant to query £ , 2 ¢ 3£ 4 the probability that document ¢ is irrelevant to query £ , which is 1.0 -2 v ¢ 3£ 4 . The four composite variables S VU h wS dc x wS pi , and S ut are defined as follows:</p><formula xml:id="formula_1" coords="3,163.20,232.20,317.50,18.90">S VU $ U y H U % U w i , S dc $ U y H U % U ¤ ¦¥ ¨ § d e , S pi $ U y H U s U ¤ "¥ # § gf f</formula><p>, S ut b$ ih , where h is the number of matching terms between a document and a query, j lk m is the within-query frequency of the n th matching term, o ¨k m is the within-document frequency of the n th matching term, p k m is the occurrence frequency in a collection of the n th matching term, j #&amp; is query length (i.e., number of terms in a query), o @&amp; is document length (i.e., number of terms in a document), and p q&amp; is collection length (i.e., number of terms in a test collection). If stopwords are removed from indexing, then j #&amp; , o I&amp; , and p r&amp; are the query length, document length, and collection length, respectively, after removing stopwords. If the query terms are re-weighted, then j lk m is no longer the original term frequency, but the new weight, and j #&amp; is the sum of the new weight values for the query terms. In the original training matrix, j lk m is the within-query term frequency, and j #&amp; is the query length. Note that, unlike S dc and S pi , the variable S VU sums the "optimized" relative frequency without first taking the log over the matching terms. The relevance probability of document ¢ with respect to query £ can be written as follows, given the log odds of relevance. The documents are ranked in decreasing order by their relevance probability 2 ¢ 3£ 4 with respect to a query. The coefficients were determined by fitting the logistic regression model specified in ¤ ¦¥ ¨ § E© ¢ s £ ! to training data using a statistical software package. We refer readers to reference <ref type="bibr" coords="3,353.82,442.30,11.66,8.97" target="#b3">[4]</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Relevance Feedback</head><p>It is well known that blind (also called pseudo) relevance feedback can substantially improve retrieval effectiveness. It is commonly implemented in research text retrieval systems. For example, see the papers of the groups who participated in the Ad Hoc tasks in TREC-7 <ref type="bibr" coords="3,243.48,522.16,16.60,8.97" target="#b14">[15]</ref> and TREC-8 <ref type="bibr" coords="3,313.98,522.16,15.30,8.97" target="#b15">[16]</ref>. Blind relevance feedback is typically performed in two stages. First, an initial search using the original queries is performed, after which a number of terms are selected from the top-ranked documents that are presumed relevant. The selected terms are merged with the original query to formulate a new query. Finally the new query is searched against the document collection to produce a final ranked list of documents. The techniques for deciding the number of terms to be selected, the number of top-ranked documents from which to extract terms, and ranking the terms varies.</p><p>The Berkeley document ranking formula has been in use for many years without blind relevance feedback. In this paper we present a technique for incorporating blind relevance feedback into the logistic regression-based document ranking framework. Some of the issues involved in implementing blind relevance feedback include determining the number of top ranked documents that will be presumed relevant and from which new terms will be extracted, ranking the selected terms and determining the number of terms that should be selected, and assigning weight to the selected terms. We refer readers to <ref type="bibr" coords="3,265.98,653.62,11.62,8.97" target="#b8">[9]</ref> for a survey of relevance feedback techniques.</p><p>Two factors are import in relevance feedback. The first one is how to select the terms from top-ranked documents after the initial search, the second is how to assign weight to the selected terms with respect to the terms in the initial query. For term selection, we assume the top-ranked documents in the initial search are relevant, and the rest of the documents in the collection are irrelevant. For the terms in the documents that are presumed relevant, we compute the odds ratio of seeing a term in the set of relevant documents and in the set of irrelevant documents. This is the term relevance weighting formula proposed by Robertson and Sparck Jones in <ref type="bibr" coords="3,435.42,725.38,15.27,8.97" target="#b13">[14]</ref>. , because documents out of the relevant documents contain the term k . Likewise, the probability of not finding the term k in a relevant document is u 8</p><p>. The odds of finding a term k in a relevant document is 8 u . Likewise, the odds of finding a term k in an irrelevant document is For every term k , except for stopwords, found in the top-ranked documents, we compute its weight according to the above formula. Then all the terms are ranked in decreasing order by their weight . The top-ranked terms, including the ones that are in the initial query, are added to the initial query to create a new query. For the selected top-ranked terms that are not in the initial query, the weight is set to 0.5. For those top-ranked terms that are in the initial query, the weight is set to 0.5*k , where k is the occurrence frequency of term k in the initial query. The weights are unchanged for the initial query terms that are not in the set of selected terms. The selected terms are merged with the initial query to formulate an expanded query. When a selected term is one of the query terms in the initial query, its weight in the expanded query is the sum of its weight in the initial query and its weight assigned in the term selection process. For a selected term that is not in the initial query, its weight in the final query is the same as the weight assigned in the term selection process, which is 0.5. The weights for the initial query terms that are not in the list of selected terms remain unchanged. Table <ref type="table" coords="4,386.82,517.24,4.98,8.97" target="#tab_2">2</ref> presents an example to illustrate how the expanded query is created from the initial query and the selected terms. The numbers in parentheses are term weights. For example, the weight for term k i in the expanded query is 3.0, since it is in the initial query with a weight value of 2.0 and it is one of the selected terms assigned the weight of 2*0.5.</p><p>Three minor changes are made to the blind relevance feedback procedure described above. First, a constant of 0.5 was added to every item in the formula used to compute the weight. Second, the selected terms must occur in at least 3 of the top-ranked documents that are presumed relevant. Third, the top-ranked two documents in the initial search remained as the top-ranked two documents in the final search. That is, the final search does not affect the ranking of the first two documents in the initial search. The rationale for not changing the top-ranked two documents is that when a query has only one or two relevant documents in the entire collection and if they are not ranked in the top in the initial search, it is unlikely these few relevant documents would be risen to the top in the second search. On the other hand, if these few relevant documents are ranked in the top in the initial search, after expansion, they are likely to be ranked lower in the final search. We believe a good strategy is to not change the ranking of the top two documents.</p><p>Note that in computing the relevance probability of a document with respect to a query in the initial search, the j #&amp; is the number of terms in the initial query, and j lk m is the number of times that term k occurs in the initial query. After query expansion, j lk m is no longer the raw term frequency in the initial query, instead it is now the weight of term k in the expanded query, and j #&amp; is the sum of the weight values of all the terms in the expanded query. For the example presented in table 2, j lk m is 1.5, and j #&amp; is 6.0 (i.e., 1.0 + 3.0 + 1.5 + 0.5). The relevance clues related to documents and the collection are the same in computing relevance probability using the expanded query as in computing relevance probability using the initial query. For all the experiments reported below, we selected the top 10 terms ranked by from 10 top-ranked documents in the initial search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Decompounding</head><p>It appears most German compounds are formed by directly joining two or more words. Such examples are Computerviren (computer viruses), which is the concatenation of Computer and Viren, and Sonnenenergie (solar energy), which is formed by joining sonnen and Energie together. Sometimes a linking element such as s or e is inserted between two words. For example, the compound Schönheitskönigin (beauty queen) is derived from Schönheit and königin with s inserted between them. There are also cases where compounds are formed with the final letter e of the first word elided. For example, the compound Erdbeben (earthquake) is derived from Erde (earth) and Beben (trembling). When the word Erde is combined with the word Atmoshpäre to create a compound, the compound is not Erdeatmoshpäre, but Erdatmoshpäre. The final letter e of the word Erde is elided from the compound. We refer readers to, for example, <ref type="bibr" coords="5,180.66,237.46,11.62,8.97" target="#b5">[6]</ref> for discussions of German compounds formations. The example earthquake shows compounds are also used in English, just not nearly as commonly used as in German.</p><p>We present a German decompounding procedure in this section which will only address the cases where the compounds are directly formed by joining words and the cases where the linking element s is inserted. The procedure is described as follows:</p><p>1. Create a German base dictionary consisting of German words in various forms, but not compounds.</p><p>2. Decompose a German compound with respect to the base dictionary. That is, find all possible ways to break up a compound with respect to the base dictionary.</p><p>3. Choose the decomposition of the minimum number of component words.</p><p>4. If there are more than one decompositions that have the smallest number of component words, choose the one with the highest probability of decomposition. The probability of a decomposition is estimated by product of the relative frequency of the component words. More details are presented below.</p><p>For example, when the German base dictionary contains ball, europa, fuss, fussball, meisterschaft and others, the German compound fussballeuropameisterschaft can be decomposed into component words with respect to the base dictionary in two different ways as shown in compound wintersports has three decompositions with respect to the base dictionary. Because two decompositions have the smallest number of component words, the rule of selecting the decomposition with the smallest number of component words cannot be applied here. We have to compute the probability of the decomposition for the decompositions with the smallest number of component words. The last column in Table <ref type="table" coords="5,434.58,720.46,4.98,8.97" target="#tab_5">4</ref> shows the log of the decomposition probability for all three decompositions that were computed using relative frequencies of the components words in the German test collection. According to the rule of selecting the decomposition of the highest probability, the second decomposition should be chosen as the decomposition of the compound wintersports. That is, the compound wintersports should be split into winter and sports. Consider the decomposition of compound p into h component words, p 8$ U c W9 q9 q9</p><p>. The probability of a decomposition is computed as follows:</p><formula xml:id="formula_2" coords="6,208.30,130.10,139.40,15.80">p 1 W$ U 3 c h p9 q9 q9 s$ U</formula><p>where the probability of component word is computed as follows:</p><formula xml:id="formula_3" coords="6,258.40,181.20,59.00,22.50">%$ k m pp x U k m up #</formula><p>where k m up x is the number of occurrences of word in a collection, is the number of unique words, including compounds, in the collection. The occurrence frequency of a word is the number of times the word occurs alone in the collection. The frequency count of a word does not include the cases where the word is a component word in a larger compound. Also, the base dictionary does not contain any words that are three-letter long or shorter except for the letter s. We created a German base dictionary of about 762,000 words by combining a lexicon extracted from Morphy, a German morphological analyzer <ref type="bibr" coords="6,266.40,276.58,15.27,8.97" target="#b9">[10]</ref>, German wordlists found on the Internet, and German words in the CLEF-2001 German collection. In our implementation, we considered only the case where a compound is the concatenation of component words, and the case where the linking element s is present. Note that the number of possible decompositions of a compound is determined by what is in the base dictionary. For example, when the word mittagessen (lunch) is not in the base dictionary, the compound mittagessenzeit (lunch time) would be split into three component words mittag (noon), essen (meal), and zeit (time).</p><p>It is not always desirable to split up German compounds into their component words. Consider again the compound Erdbeben. In this case, it is probably better not to split up the compound. But in other cases like Gemüseexporteure (vegetable exporters), Fußballweltmeisterschaft (World Soccer Championship), splitting up the compounds probably is desirable since the use of the component words might retrieval additional relevant documents which are otherwise likely to be missed if only the compounds are used. In fact, we noticed that the compound Gemüseexporteure does not occur in the CLEF-2001 German document collection.</p><p>In general, it is conceivable that breaking up compounds is helpful. The same phrase may be spelled out in words sometimes, but as one compound other times. When a user formulate a German query, the user may not know if a phrase should appear as multi-word phrase or as one compound. An example is the German equivalent of the English phrase "European Football Cup", in the title of topic 113, the German equivalent is spelled as one compound Fussballeuropameisterschaft, but in the description field, it is Europameisterschaft im Fußball, yet in the narrative field, it is Fußballeuropameisterschaft. This example brings out two points in indexing German texts. First, it should be helpful to split compounds into component words. Second, normalizing the spelling of ss and ß should be helpful. Two more such examples are Scheidungsstatistiken and Präsidentschaftskandidaten. The German equivalent of "divorce statistics" is Scheidungsstatistiken in the title field of topic 115, but Statistiken über die Scheidungsraten in the description field. The German equivalent of "presidency candidates" is Präsidentschaftskandidaten in title field of topic 135, but Kandidat für das Präsidentenamt in the description field of the same topic. The German equivalent for "Nobel price winner for literature" is Literaturnobelpreisträger, in the "Der Spiegel" German collection, we find variants of Literatur-Nobelpreisträger, Literaturnobelpreis-Trgerin. Literaturnobelpreis sometimes appears as "Nobelpreis für Literatur".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Test Collection</head><p>The document collection for the multilingual IR task consists of documents in five languages: English, French, German, Italian, and Spanish. The collection has about 750,000 documents which are newspaper articles published in 1994 except that part of the Der Spiegel was published in 1995. The distribution of documents among the five document languages is presented in Table <ref type="table" coords="6,241.56,667.18,3.74,8.97" target="#tab_6">5</ref>. A set of 50 topics was developed and released in more than 10 languages, including Dutch, English, French, German, Italian, and Spanish. A topic has three parts: 1) title, a short description of information need; 2) description, a sentence-long description of information need; and 3) narrative, specifying document relevance criteria. More details about the test collection are presented in <ref type="bibr" coords="6,448.67,703.06,15.30,8.97" target="#b12">[13]</ref>. The multilingual IR task at CLEF 2002 was concerned with searching the collection consisting of English, French, German, Italian, and Spanish documents for relevant documents, and returning a combined, ranked list of documents in any document language in response to a query. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Results</head><p>All retrieval runs reported in this paper used only the title and description fields in the topics. The ids and average precision values of the official runs are presented in bold face, other runs are unofficial ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Monolingual retrieval experiments</head><p>In this section we present the results of monolingual retrieval. We created a stopwords list for each document language. In indexing, the stopwords were removed from both documents and topics. Additional words such as relevant and document were removed from topics. The words in all six languages were stemmed using Muscat stemmers downloaded from http://open.muscat.com. For automatic query expansion, the top-ranked 10 terms from the top-ranked 10 documents after the initial search were combined with the original query to create the expanded query. For Dutch and German monolingual runs, the compounds were split into their component words, and only their component words were retained in document and topic indexing. All the monolingual runs included automatic query expansion via the relevance feedback procedure described in section 3. For the German monolingual runs, compounds were decomposed into their component words by applying the decompounding procedure described above. Only component words of the decomposed compounds were kept in document and topic indexing. Table <ref type="table" coords="7,233.40,694.18,4.98,8.97" target="#tab_9">7</ref> presents the performance of German monolingual retrieval with three different features which are decompounding, stemming, and query expansion. The features are implemented in the order of decompounding, stemming, and query expansion. For example, when decompounding and stemming are present, the compounds are split into component words first, then the components are stemmed. The table shows when any one of the three features is present, the average precision improves from 4.94% to 19.73% over  Table <ref type="table" coords="8,106.68,629.92,4.98,8.97" target="#tab_10">8</ref> presents the German words in the title or desc fields of the topics that were split into component words using the decompounding procedure described in section 4. The column labeled component words shows the component words of the decomposed compounds. The German word eurofighter was split into euro and fighter since both component words are in the base dictionary, and the word eurofighter is not. Including the word eurofigher in the base dictionary will prevent it from being split into component words. The word geographos was decomposed into geog, rapho, and s for the same reason that the component words are in the base dictionary. Two topic words, lateinamerika and zivilbevölkerung, were not split into component words because both are present in our base dictionary which is far from being perfect. For the same reason, the preisträgers was not decomposed into preis and trägers. An ideal base dictionary should contain all and only the words that should not be further split into smaller component words. Our current decompounding procedure does not split words in the base dictionary into smaller component words. When the two compounds, lateinamerika and zivilbevölkerung, are removed from the base dictionary, lateinamerika is split into latein and amerika, and zivilbevölkerung into zivil and bevölkerung. The topic word südjemen was not split into süd and jemen because our base dictionary does not contain words that are three-letter long or shorter. The majority of the errors in decompounding are caused by the incompleteness of the base dictionary or the presence of compound words in the base dictionary.</p><p>We used a Dutch stopword list of 1326 words downloaded from http://clef.iei.pi.cnr.it:2002/ for Dutch monolingual retrieval. After removing stopwords, the Dutch words were stemmed using the muscat Dutch stemmer. For Dutch decompounding, we used a Dutch wordlist of 223,557 words <ref type="foot" coords="9,360.84,156.16,3.49,6.28" target="#foot_0">1</ref> . From this wordlist we created a Dutch base dictionary of 210,639 by manually breaking up the long words that appear to be compounds. It appears that many Dutch compound words remain in the base dictionary. Like the German base dictionary, an ideal Dutch base dictionary should include all and only the words that should not be further decomposed into smaller component words. The Dutch words in the topics or desc fields of the topics were split into component words using the same procedure as for German decompounding. Like German decompounding, the words in the Dutch base dictionary are not decomposed. The source wordlist files contain a list of country names, which should have beed added to the   Europe. The Dutch equivalent of mad cow diseases is gekkekoeienziekte in the topic, but never occurs in the Dutch collection. Without decompounding, the precision for this topic is 0.1625, and with decompounding, the precision increased to 0.3216. The precision for topic 90 which vegetable exporters is 0.0128 without decompounding. This topic contains two compound words, Groentenexporteurs and diepvriesgroenten. The former one which is perhaps the most important term for this topic never occurs in the Dutch document collection. After decompounding, the precision for this topic increased to 0.3443. Topic 55 contains two important compound words, Alpenverkeersplan and Alpeninitiatief. Both never occur in the Dutch document collection. The precision for this topic is 0.0746 without decompounding, and increased to 0.2137 after decompounding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Cross-language Retrieval Experiments</head><p>A major factor affecting the end performance of cross-language retrieval and multilingual retrieval is the quality of translation resources. In this section, we evaluate the effectiveness of three different translation resources: automatic machine translation systems, parallel corpora, and bilingual dictionaries. Two of the issues in translating topics are 1) determining the number of translations to retain when multiple candidate translations are available; and 2) assigning weights to the selected translations <ref type="bibr" coords="10,283.02,252.76,10.58,8.97" target="#b7">[8]</ref>. When machine translation systems are used to translate topics, these two issues are resolved automatically by the machine translation systems, since they provides only one translation for each word. However, when bilingual dictionaries or parallel corpora are used to translate topics, often for a source word, there may be several alternative translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">CLIR Using MT</head><p>In this section, we evaluate two machine translation systems, online Babelfish translation<ref type="foot" coords="10,425.10,332.20,3.49,6.28" target="#foot_1">2</ref> and L&amp;H Power Translator Pro, version 7.0, for translating topics in CLIR. We used both translators to translate the 50 English topics into French, Italian, German, and Spanish. For each language, both sets of translations were preprocessed in the same way. were used in cross-language retrieval from English to French, German, Italian and Spanish, the translation from L&amp;H Translator and the translation from Babelfish were combined by topic. The term frequencies in the combined topics were reduced by half so that the combined topics were comparable in length to the source English topics. Then the combined translations were used to search the document collection for relevant documents as in monolingual retrieval. For example, for the English-to-Italian run bky2bienit, we first translated the source English topics into Italian using L&amp;H Translator and Babelfish. The Italian translations produced by L&amp;H Translator and the Italian translations produced by Babelfish were combined by topic. Then the combined, translated Italian topics with term frequencies reduced by half were used to search the Italian document collections. The bky2bienfr, bky2biende, bky2bienes CLIR runs from English were all produced in the same way as the bky2bienit run. For English or French to German cross-language retrieval runs, the words in title or desc fields of the translated German topics were decompounded. For all cross-language runs, words were stemmed after removing stopwords like in monolingual retrieval. The English-to-French run bky2bienfr2 was produced by merging the bky2bienfr run and the bky2bienfr5 run which used parallel corpora as the sole translation resource. More discussion about the use of parallel corpora will be presented below.</p><p>All the cross-language runs applied blind relevance feedback. The top-ranked 10 terms from the top-ranked 10 documents after the initial search were combined with the initial query to formulate an expanded query. The results presented in table <ref type="table" coords="11,173.52,265.30,9.96,8.97" target="#tab_14">12</ref> show that query expansion improved the average precision for the official runs from 10.85% to 29.36%. The L&amp;H Translator performed better than Babelfish for cross-language retrieval from English to French, German, Italian and Spanish. Combining the translations from L&amp;H Translator and Babalfish performed slightly better than using only the translations from L&amp;H translator.</p><p>We notices a number of error in translating English to Italian using Babelfish. For example, the English text Super G which was translated into Superg, U.N. and U.S.-Russian were not translated. While the phrase Southern Yemen in the desc field was correctly translated into Südyemen, the same phrase in the title field became SüdcYemen. Decompounding is helpful in monolingual retrieval, it is also helpful in cross-language retrieval to German from no decompounding decompounding source target translator average precision average precision change English German L&amp;H Translator 0.2776 0.3009 8.4% English German Babelfish 0.2554 0.2906 13.78% French German Babelfish 0.2774 0.3092 11.46%</p><p>Table <ref type="table" coords="11,94.74,442.24,8.49,8.97" target="#tab_4">13</ref>: Effectiveness of decompounding in cross-language retrieval to German. All runs were performed without stemming and query expansion.</p><p>other languages such as English. An English phrase of two words may be translated into a German phrase of two words, or into a compound. For examples, in topic 111, the English phrase computer animation in title became ComputerAnimation, and Computer Animation in desc. In topic 109, the English phrase Computer Security became Computer-Sicherheit in the title, but the same phrase in lower case in desc became Computersicherheit.</p><p>Table <ref type="table" coords="11,95.70,535.90,9.96,8.97" target="#tab_4">13</ref> shows the performances of three cross-language retrieval to German with and without decompounding. The improvement in average precision ranges from 8.4% to 13.78%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">English-French CLIR Using Parallel Corpora</head><p>We created a French-English bilingual lexicon from the Canadian Hansard (the recordings of the debates of the House for the period of 1994 to 2001). The texts are in English and French. We first aligned the Hansard corpus at the sentence level using the length-based algorithm proposed by Gale and Church <ref type="bibr" coords="11,392.10,616.96,10.58,8.97" target="#b6">[7]</ref>, resulting in about two million aligned French/English sentence pairs. To speed up the training (i.e, estimating word translation probabilities), we extracted and used only the sentence pairs that contain at least one English topic word in CLEF-2001 topics. A number of preprocessing steps were carried out prior to the training. First, we removed the English stopwords from the English sentences, and French stopwords from the French sentences. Secondly, we changed the variants of a word into its base form. For English, we used a morphological analyzer described in <ref type="bibr" coords="11,426.30,676.72,10.58,8.97" target="#b4">[5]</ref>. For French, we used a French morphological analyzer named DICO. Each of the packages contains a list of words together with their morphological analyses. Thirdly, we discarded the sentence pairs in which one of the sentence has 40 or more words after removing stopwords, and the sentence pairs in which the length ratio of the English sentence over the French sentence is below .7 or above 1.5. The average length ratio of English text over French text is approximately 1.0. Since sentence alignment is not perfect, some mis-alignments are unavoidable. Hence there may be sentence pairs in which the length ratios that deviate far from the average length ratio. After the preprocessing, only 706,210 pairs of aligned sentences remained. The remaining aligned sentence pairs were fed to GIZA++ for estimating English-to-French word translation probabilities. GIZA++ toolkit is an extension to the EGYPT toolkit <ref type="bibr" coords="12,485.82,97.96,11.66,8.97" target="#b0">[1]</ref> which was based on the statistical machine translation models described in <ref type="bibr" coords="12,352.50,109.90,10.61,8.97" target="#b1">[2]</ref>. Readers are referred to <ref type="bibr" coords="12,469.08,109.90,16.60,8.97" target="#b10">[11]</ref> for more details on GIZA++. The whole training phase took about 24 hours on a Sun Microsystem Sparc server machine. Table <ref type="table" coords="12,94.86,133.78,9.96,8.97" target="#tab_5">14</ref> shows the first three French translations produced by GIZA++ for some of the words in the English topics. Table <ref type="table" coords="12,95.64,487.84,8.49,8.97" target="#tab_5">14</ref>: English to French word translation probabilities estimated from parallel corpora using a statistical machine translation toolkit.</p><p>The French translations are ranked in descending order by the probability of translating from an English word into French words. In translating an English word into French, we selected only one French word, the one of the highest translation probability, as the translation. The English topics were translated into French word-by-word, then the translated French topics were used in producing the English-to-French run labeld bky2bienfr5 in table <ref type="table" coords="12,476.64,557.56,8.30,8.97" target="#tab_14">12</ref>. Without query expansion, the parallel corpus-based English-French CLIR performance was slightly better than that of using Babelfish, but slightly lower than that of using L&amp;H translator.</p><p>The CLEF 2002 English topics contain a number of polysemous words such as cup, fall, interest, lead, race, right, rock, star, and the like. The word fall in the context of fall in sale of cars in topic 106 has the meaning of declining. However, the most likely French translation for fall as table <ref type="table" coords="12,368.04,617.32,9.96,8.97" target="#tab_5">14</ref> shows is automne, meaning autumn in English. The word race in ski race in topic 102 or in race car in topic 121 has the meaning of contest or competition in speed. Again the French word of the highest translation probability is race, meaning human race in English. The corrent French translation for the sense of race in ski race or car race should be course. The word star in topic 129 means a plant or celestial body, while in topic 123 in the context of pop star, it means a famous performer. The correct translation for star in topic 129 should be étoile, instead of the most likely translation star, which is the correct French word for the sense of star in pop star. The word rock in topic 130 has the same sense as rock in rock music, not the sense of stone. The correct translation for rock in topic 130 should be rocke. In the same topic, the word lead in lead singer means someone in the leading role, not the metal. These examples show that taking the French word of the highest translation probability as the translation for an English word is overly simplified. Choosing the right French translations would require word sense disambiguation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">CLIR using bilingual dictionary</head><p>For the only English-to-Dutch run bky2biennl, the English topics were translated into Dutch by looking up each English topic word, excluding stopwords, in the online English-Dutch dictionary Babylon<ref type="foot" coords="13,432.72,103.06,3.49,6.28" target="#foot_2">3</ref> . All the Dutch words in the dictionary lookup results were retained except for Dutch stopwords. The Dutch compound words were split into component words. If translating an English topic word resulted in Dutch words, then all translated Dutch words of the English word received the same weight U , i.e., the translated Dutch words were weighted uniformly. The average precision of the English-to-Dutch run is 0.3199, which is much lower than 0.4847 for Dutch monolingual retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Multilingual Retrieval Experiments</head><p>In this section, we describe our multilingual retrieval experiments using the English topics (only title and description fields were indexed). As mentioned in the cross-language experiments section above, we translated the English topics into the other four document languages which are French, German, Italian, and Spanish using Babelfish and L&amp;H Translator. A separate index was created for each of the five document languages. For the multilingual retrieval runs, we merged five ranked lists of documents, one resulted from English monolingual retrieval and four resulted from cross-language retrieval from English to the other four document languages, to produce a unified ranked list of documents regardless of language.</p><p>A fundamental difference between merging in monolingual retrieval or cross-language retrieval and merging in multilingual retrieval is that in monolingual or cross-language retrieval, documents for individual ranked lists are from the same collection, while in multilingual retrieval, the documents for individual ranked lists are from different collections. For monolingual or cross-language retrieval, if we assume that documents appearing on more than one ranked list are more likely to be relevant than the ones appearing on a single ranked list, then we should rank the documents appearing on multiple ranked lists in higher position in the merged ranked list of documents. A simple way to accomplish this is to sum the probability of relevance for the documents appearing on multiple ranked lists while the probabilities of relevance for the documents appearing on a single list remain the same. After summing up the probabilities, the documents are re-ranked in descending order by combined probability of relevance. In multilingual retrieval merging, since the documents on the individual ranked lists are all different, we cannot use multiple appearances of a document in the ranked lists as evidence to promote its rank in the final ranked list. The problem of merging multiple ranked lists of documents in multilingual retrieval is closely linked to estimating probability of relevance. If the estimates of probability of relevance are accurate and well calibrated, then one can simply combine the individual ranked lists and then re-rank the combined list by the raw probability of relevance. In practice, estimating relevance probabilities is a hard problem.</p><p>We looked at the estimated probabilities of relevance produced using the ranking formula described in section 2 for the CLEF 2001 topics to see if there is a linear relationship between the number of relevant documents and the number of documents whose estimated probabilities of relevance are above some threshold. Figure <ref type="figure" coords="13,476.40,498.58,4.98,8.97">1</ref> shows the scatter plot of the number of retrieved documents whose estimated relevance probabilities are above 0.37 versus the number relevant documents for the same topic. Each dot in the figure represents one French topic. The ranked list of documents was produced using the 50 French topics of CLEF 2001 to search against the French collection with query expansion. The top-ranked 10 terms from top-ranked 10 documents in the initial search were merged with initial query to create the expanded query. The threshold of 0.37 was chosen so that the total number of documents for all 50 topics whose estimated relevance probabilities are above the threshold is close to the total number of relevant documents for the same set of topics. If the estimated probabilities are good, the dots in the figure would appear along the diagonal line. The figure shows there is no linear relationship between the number of retrieved documents whose relevance probabilities are above the threshold and the number of relevant documents for the same topic. This implies one cannot use the raw relevance probabilities to directly estimate the number of relevant documents for a topic in a test document collection.</p><p>There are a few simple ways to merge ranked lists of documents from different collections. Here we will evaluate two of them. The first method is to combine all ranked lists, sort the combined list by the raw relevance score, then take the top 1000 documents per topic. The second method is to normalize the relevance score for each topic, dividing all relevance scores by the relevance score of the top most ranked document for the same topic. Table <ref type="table" coords="13,120.77,689.86,9.96,8.97" target="#tab_6">15</ref> presents the multilingual retrieval performance with different merging strategies. The multilingual runs were produced by merging from five runs: bky2moen (English-English, 0.5602), bky2bienfr (English-French, 0.4773), bky2biende (English-German, 0.4479), bky2bienit (English-Italian, 0.4090), and bky2bienes (English-Spanish, 0.4567). The run bky2muen1 was produced by ranking the documents by the unnormalized relevance probabilities after combining the individual runs. And the run bky2muen2 was produced in the same way except that the relevance probabilities were normalized before merging. For each topic, the relevance probabilities of the documents was divided by the relevance probability of the highest-ranked document for the same topic. The simplest direct merging outperformed the score normalizing merging strategy. We did two things to make the relevance probabilities of documents from different language collections comparable to each other. Firstly, as mentioned in section 6.2.3, after concatenating the topic translations from two translators, we reduced the term frequencies by half so that the translated topics are close to the source English topics in length. Secondly, in query expansion, we took the same number of terms (i.e, 10) from the same number of top-ranked documents (i.e, 10) after the initial search for all five individual runs that were used to produce the multilingual runs.</p><p>In the remainder of this section, we present a procedure for computing the optimal performance that could possibly be achieved under the constraint that the relative ranking of the documents in the individual ranked lists is preserved. This procedure assumes that the relevances of documents are known, thus it is not useful to to predict ranks of documents in the final ranked list for multilingual retrieval. However, knowing the upper-bound performance for a set of ranked lists of documents and the related document relevances is useful in measuring the performance of different merging strategies. We will use an example to explain the procedure. Let us assume we are going to merge three runs labeled , ¡ and , as shown in table <ref type="bibr" coords="14,346.08,675.58,8.30,8.97" target="#b15">16</ref>. The relevant documents are marked with an '*'. We want to find a combined ranked list such that the average precision is maximized without changing the relative rank order of the documents on the same ranked list. First we transform the individual runs shown in table <ref type="bibr" coords="14,103.68,711.40,9.96,8.97" target="#b15">16</ref>  implemented in four steps.</p><p>Step 1: Let the active set consist of the first set in the individual lists that contains at least one relevant document. For the example presented in table 17, the initial active set is (0,1) U , (2,1) 1¡ !U ,¡ c ,¡ Ri , (1,3) l bU , c , ei , t #</p><p>Step 2: Choose the element in the active set with the smallest number of irrelevant documents. If there are two or more elements with the smallest number of irrelevant documents, then choose the element that also contains the largest number of relevant documents. If there are two or more elements with the same smallest number of irrelevant documents and the same largest number of relevant documents in the current active set, randomly choose one of them. Append the selected element to the final ranked list. If the next set appearing immediately after the selected element contains at least one relevant document, then add the next set to the current active set. That is, sort the active set by as the major order in increasing order, and by h as the minor order in decreasing order, then take out the first element and put it in the final ranked list.</p><p>Step 3: Repeat step 2 until the current active set is empty.</p><p>Step 4: If the final ranked list has less than 1000 documents, append more irrelevant documents drew from any individual list to the final ranked list.</p><p>The optimal ranking after reordering the sets is presented in table <ref type="table" coords="15,344.76,493.36,9.96,8.97" target="#tab_10">18</ref> set Optimal ranking 1 (0,1) U 2</p><p>(1,3) l bU , c , ei , Et 3</p><p>(1,1) c , i 4</p><p>(2,1) h¡ gU ,¡ c ,¡ Ri 5</p><p>(1,0) t 6</p><p>(1,0) h¡ Rt Table <ref type="table" coords="15,267.90,608.74,8.49,8.97" target="#tab_10">18</ref>: Optimal ranking.</p><p>The upper-bound average precision for the set of runs used for producing our official multilingual runs is 0.5177 with overall recall of 6392/8068. The performances of the direct merging and score-normalizing merging are far below the upper-bound performance that could possibly be achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We have presented a technique for incorporating blind relevance feedback into a document ranking formula based on logistic regression analysis. The improvement in average precision brought by query expansion via blind relevance feedback ranges from 6.42% to 19.42% for monolingual retrieval runs, and from 10.85% to 29.36% for cross-language retrieval runs. We have also presented a procedure to decompose German compounds and Dutch compounds. German decompounding improved the the average precision of German monolingual retrieval by 11.47%. Decompounding increased the average precision for cross-language retrieval to German from English or French. The increase ranges from 8.4% to 11.46%. For Dutch monolingual retrieval, decompounding increased the average precision by 4.10%, which is much lower than the improvement of 13.49% on CLEF 2001 test set. In summary, both blind relevance feedback and decompounding in German or Dutch have been shown to be effective in monolingual and cross-language retrieval. The amount of improvement of performance by decompounding varies from one set of topics to another. Three different translation resources, machine translators, parallel corpora, and bilingual dictionaries, were evaluated on cross-language retrieval. We found that the English-French CLIR performance of using parallel corpora was competitive with that of using commercial machine translators. Two different merging strategies in multilingual retrieval were evaluated. The simplest strategy of merging individual ranked lists of documents by unnormalized relevance score worked better than the one first normalizing the relevance score. To make the relevance scores of the documents from different collections as closely comparable as possible, we selected the same number of terms from the same number of top-ranked documents after the initial search for query expansion in all the runs that were combined to produce the unified ranked lists of documents in multiple languages. We used two machine translators to translate English topics to French, German, Italian and Spanish, and combined by topic the translations from the two translators. We reduced the term frequencies in the combined translated topics by half so that the combined translated topics are close in length to the source English topics. We presented an algorithm for generating the optimal ranked list of documents when the document relevances are known. The optimal performance can then be used to measure the performances of different merging strategies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,219.10,387.90,40.18,5.60;3,241.10,394.70,27.70,5.60;3,316.00,388.00,6.60,5.60;3,280.80,401.80,63.20,8.60"><head></head><label></label><figDesc>#u wv yx hz %{ ¦| W} ẽ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,361.30,336.00,13.00,6.80;4,345.80,343.50,40.70,6.80;4,390.60,334.54,122.39,8.97;4,70.86,346.54,280.80,8.97;4,245.40,374.80,27.46,5.60;4,289.90,368.10,64.00,5.60;4,286.20,381.70,11.50,5.60;4,319.60,381.70,23.10,5.60;4,512.82,372.22,11.62,8.97"><head>8 .</head><label>8</label><figDesc>The terms extracted from the top-ranked documents are ranked by their odds ratio which is given by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="14,70.86,361.00,453.54,8.97;14,70.86,372.94,100.47,8.97;14,110.88,395.80,23.53,8.97;14,172.50,395.80,193.26,8.97;14,393.30,395.80,22.13,8.97;14,447.90,395.80,36.52,8.97;14,110.88,408.16,92.07,8.97;14,242.82,408.16,35.70,8.97;14,298.80,408.16,60.37,8.97;14,393.30,408.16,82.00,8.97;14,110.88,420.52,92.07,8.97;14,242.82,420.52,35.70,8.97;14,298.80,420.52,176.50,8.97;14,70.86,442.24,453.53,8.97;14,70.86,454.24,378.15,8.97;14,70.86,70.87,364.03,274.97"><head>Figure 1 :Table 15 :</head><label>115</label><figDesc>Figure 1: Number of retrieved documents with relevance probability over .37 versus the number of relevant documents for the same topic. run id topic language topic fields merging strategy recall precision bky2muen1 English title,desc Direct merging 5880/8068 0.3762 bky2muen2 English title,desc Normalized merging 5765/8068 0.3570Table15: Multilingual retrieval performance for different merging strategies. The five runs from which the the multilingual runs were produced are bky2moen, bky2bienfr, bky2biende,bky2bienit,bky2bienes.</figDesc><graphic coords="14,70.86,70.87,364.03,274.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,70.86,725.38,453.55,20.91"><head>Table 1</head><label>1</label><figDesc>presents a word contingency table, where h is the number of documents in the collection, the number of top-ranked</figDesc><table coords="4,196.92,72.46,195.50,45.63"><row><cell cols="3">relevant irrelevant</cell><cell></cell></row><row><cell>indexed</cell><cell cols="2">h 5 f</cell><cell>h</cell></row><row><cell>non-indexed m -</cell><cell>n -h</cell><cell>-m +</cell><cell>n -h</cell></row><row><cell>m</cell><cell>n -m</cell><cell></cell><cell>n</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,195.96,130.42,203.34,89.01"><head>Table 1 :</head><label>1</label><figDesc>A contingency table for a word.</figDesc><table coords="4,195.96,162.22,203.34,57.21"><row><cell cols="3">Initial Query Selected Terms Expanded Query</cell></row><row><cell>k 3U (1.0)</cell><cell></cell><cell>k 3U (1.0)</cell></row><row><cell>k wc (2.0)</cell><cell>k wc (2*0.5)</cell><cell>k wc (3.0)</cell></row><row><cell>k i (1.0)</cell><cell>k i (1*0.5)</cell><cell>k i (1.5)</cell></row><row><cell></cell><cell>k yt (0.5)</cell><cell>k yt (0.5)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,70.86,232.18,453.57,85.59"><head>Table 2 :</head><label>2</label><figDesc>Query expansion.documents after the initial search that are presumed relevant, the number of documents among the topranked documents that contain the term k , and h the number of documents in the collection that contain the term k . Then we see from the above contingency table that the probability of finding the term k in a relevant document is 8</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,177.54,444.22,346.88,55.65"><head>Table 3 .</head><label>3</label><figDesc>The last decomposition has the smallest number of</figDesc><table coords="5,177.54,466.60,240.20,33.27"><row><cell></cell><cell cols="2">Decompositions</cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>fuss</cell><cell>ball</cell><cell>europa</cell><cell>meisterschaft</cell></row><row><cell>2</cell><cell cols="3">fussball europa meisterschaft</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,70.86,512.68,453.58,125.37"><head>Table 3 :</head><label>3</label><figDesc>Decompositions of compound fussballeuropameisterschaft.</figDesc><table coords="5,191.16,592.78,212.94,45.27"><row><cell></cell><cell cols="2">Decompositions</cell><cell></cell><cell>log p(D)</cell></row><row><cell>1</cell><cell>winter</cell><cell>s</cell><cell>ports</cell><cell>-43.7002</cell></row><row><cell>2</cell><cell>winter</cell><cell>sports</cell><cell></cell><cell>-20.0786</cell></row><row><cell>3</cell><cell cols="2">winters ports</cell><cell></cell><cell>-28.3584</cell></row></table><note coords="5,70.86,546.46,453.58,8.97;5,70.86,558.46,453.54,8.97;5,70.86,570.40,453.55,8.97"><p><p><p>component words, so the German compound fussballeuropameisterschaft is split into fussball, europa and meisterschaft. Table</p>4</p>presents another example which shows the decompositions of German compound wintersports with respect to a base dictionary containing port, ports, s, sport, sports, winter, winters and other words. The</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="5,192.60,650.80,210.15,8.97"><head>Table 4 :</head><label>4</label><figDesc>Decompositions of compound wintersports.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="7,181.32,72.82,232.62,176.55"><head>Table 5 :</head><label>5</label><figDesc>Part of the CLEF 2002 document sets.</figDesc><table coords="7,181.32,72.82,232.62,154.83"><row><cell cols="2">Language Name</cell><cell>No. of</cell><cell>Size</cell></row><row><cell></cell><cell></cell><cell cols="2">documents (MB)</cell></row><row><cell>English</cell><cell>Los Angeles Times</cell><cell>113,005</cell><cell>425</cell></row><row><cell>French</cell><cell>Le Monde</cell><cell>44,013</cell><cell>157</cell></row><row><cell></cell><cell>SDA French</cell><cell>43,178</cell><cell>86</cell></row><row><cell>German</cell><cell cols="2">Frankfurter Rundschau 139,715</cell><cell>320</cell></row><row><cell></cell><cell>Der Spiegel</cell><cell>13,979</cell><cell>63</cell></row><row><cell></cell><cell>SDA German</cell><cell>71,677</cell><cell>144</cell></row><row><cell>Italian</cell><cell>La Stampa</cell><cell>58,051</cell><cell>193</cell></row><row><cell></cell><cell>SDA Italian</cell><cell>50,527</cell><cell>85</cell></row><row><cell>Spanish</cell><cell>EFE</cell><cell>215,738</cell><cell>509</cell></row><row><cell>Dutch</cell><cell>RC Handelsblad</cell><cell>84,121</cell><cell>299</cell></row><row><cell></cell><cell>Algemeen Dagblad</cell><cell>106,483</cell><cell>241</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="7,70.86,445.48,453.63,221.79"><head>Table 6 :</head><label>6</label><figDesc>Table 6  presents the monolingual retrieval results for six document languages. The last column labeled change shows the improvement of average precision with blind relevance feedback over without it. As table6shows, query expansion increased the average precision of the monolingual runs for all six languages, the improvement ranging from 6.42% for Spanish to 19.42% for French. There are no relevant Italian documents for topic 120, and no relevant English documents for Monolingual IR performance.</figDesc><table coords="7,70.86,516.22,397.21,151.05"><row><cell></cell><cell></cell><cell cols="2">without expansion</cell><cell cols="2">with expansion</cell><cell></cell></row><row><cell>run id</cell><cell>language</cell><cell>recall</cell><cell>precision</cell><cell>recall</cell><cell cols="2">precision change</cell></row><row><cell cols="2">bky2moen English</cell><cell>765/821</cell><cell>0.5084</cell><cell>793/821</cell><cell>0.5602</cell><cell>10.19%</cell></row><row><cell cols="2">bky2monl Dutch</cell><cell>1633/1862</cell><cell>0.4446</cell><cell>1734/1862</cell><cell>0.4847</cell><cell>9.02%</cell></row><row><cell cols="2">bky2mofr French</cell><cell>1277/1383</cell><cell>0.4347</cell><cell>1354/1383</cell><cell>0.5191</cell><cell>19.42%</cell></row><row><cell cols="2">bky2mode German</cell><cell>1696/1938</cell><cell>0.4393</cell><cell>1807/1938</cell><cell>0.5234</cell><cell>19.14%</cell></row><row><cell>bky2moit</cell><cell>Italian</cell><cell>994/1072</cell><cell>0.4169</cell><cell>1024/1072</cell><cell>0.4750</cell><cell>13.94%</cell></row><row><cell cols="2">bky2moes Spanish</cell><cell>2531/2854</cell><cell>0.5016</cell><cell>2673/2854</cell><cell>0.5338</cell><cell>6.42%</cell></row><row><cell cols="3">topics 93, 96, 101, 110, 117, 118, 127 and 132.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="8,70.86,131.56,453.56,451.99"><head>Table 7 :</head><label>7</label><figDesc>German monolingual retrieval performance. The total number of German relevant documents for 50 topics is 1938. the baseline performance when none of the features is present. When two of the three features are included in retrieval, the improvement in precision ranges from 26.89% to 30.47%. And when all three features are present, the average precision is 51.18% better than the baseline performance. It is interesting to see the three features are complementary. That is, the improvement brought by each individual feature is not diminished by the presence of the other two features. Without decompounding, stemming alone improved the average precision by 4.94%. However with decompounding, stemming improved the average precision from 0.3859 to 0.4393, an increase of 13.84%. Stemming became more effective because of decompounding. Decompounding alone improved the average precision by 11.47% for German monolingual retrieval.</figDesc><table coords="8,81.84,291.68,431.59,291.87"><row><cell></cell><cell>compounds</cell><cell>component</cell><cell>words</cell><cell></cell><cell>compounds</cell><cell>component</cell><cell>words</cell></row><row><cell>1</cell><cell>absatzkrise</cell><cell>absatz</cell><cell>krise</cell><cell>2</cell><cell>atemwege</cell><cell>atem</cell><cell>wege</cell></row><row><cell>3</cell><cell>autoindustrie</cell><cell>auto</cell><cell>industrie</cell><cell>4</cell><cell>automobilindustrie</cell><cell>automobil</cell><cell>industrie</cell></row><row><cell>5</cell><cell>bandleaders</cell><cell>band</cell><cell>leaders</cell><cell>6</cell><cell>bronchialasthma</cell><cell>bronchial</cell><cell>asthma</cell></row><row><cell>7</cell><cell>bürgerkrieg</cell><cell>bürger</cell><cell>krieg</cell><cell>8</cell><cell>computeranimation</cell><cell>computer</cell><cell>animation</cell></row><row><cell>9</cell><cell>computeranimationen</cell><cell>computer</cell><cell>animationen</cell><cell cols="2">10 computersicherheit</cell><cell>computer</cell><cell>sicherheit</cell></row><row><cell cols="2">11 durchbrüche</cell><cell>durch</cell><cell>brüche</cell><cell cols="2">12 eigentumsrechte</cell><cell>eigentums</cell><cell>rechte</cell></row><row><cell cols="2">13 eurofighter</cell><cell>euro</cell><cell>fighter</cell><cell cols="2">14 europameisterschaft</cell><cell>europa</cell><cell>meisterschaft</cell></row><row><cell cols="2">15 filmfestspielen</cell><cell>film</cell><cell>festspielen</cell><cell cols="2">16 filmindustrie</cell><cell>film</cell><cell>industrie</cell></row><row><cell cols="2">17 fischereiquoten</cell><cell>fischerei</cell><cell>quoten</cell><cell cols="2">18 fremdsprachigen</cell><cell>fremd</cell><cell>sprachigen</cell></row><row><cell cols="2">19 fremdwörter</cell><cell>fremd</cell><cell>wörter</cell><cell cols="2">20 goldmedaille</cell><cell>gold</cell><cell>medaille</cell></row><row><cell cols="2">21 handynutzung</cell><cell>handy</cell><cell>nutzung</cell><cell cols="2">22 interessenkonflikt</cell><cell>interessen</cell><cell>konflikt</cell></row><row><cell cols="2">23 interessenkonflikts</cell><cell>interessen</cell><cell>konflikts</cell><cell cols="2">24 menschenrechte</cell><cell>menschen</cell><cell>rechte</cell></row><row><cell cols="2">25 mobiltelefone</cell><cell>mobil</cell><cell>telefone</cell><cell cols="2">26 nahrungskette</cell><cell>nahrungs</cell><cell>kette</cell></row><row><cell cols="2">27 netzwerken</cell><cell>netz</cell><cell>werken</cell><cell cols="2">28 nordamerika</cell><cell>nord</cell><cell>amerika</cell></row><row><cell cols="2">29 nordamerikanische</cell><cell>nord</cell><cell cols="3">amerikanische 30 pelzindustrie</cell><cell>pelz</cell><cell>industrie</cell></row><row><cell cols="2">31 präsidentschaftskandidaten</cell><cell>präsidentschafts</cell><cell>kandidaten</cell><cell cols="2">32 premierministers</cell><cell>premier</cell><cell>ministers</cell></row><row><cell cols="2">33 scheidungsraten</cell><cell>scheidungs</cell><cell>raten</cell><cell cols="2">34 scheidungsstatistiken</cell><cell>scheidungs</cell><cell>statistiken</cell></row><row><cell cols="2">35 sicherheitspolitik</cell><cell>sicherheits</cell><cell>politik</cell><cell cols="2">36 spionagefall</cell><cell>spionage</cell><cell>fall</cell></row><row><cell cols="2">37 spionagefalles</cell><cell>spionage</cell><cell>falles</cell><cell cols="2">38 sternensystemen</cell><cell>sternen</cell><cell>systemen</cell></row><row><cell cols="2">39 verkaufszahlen</cell><cell>verkaufs</cell><cell>zahlen</cell><cell cols="2">40 volksbefragung</cell><cell>volks</cell><cell>befragung</cell></row><row><cell cols="2">41 winterspielen</cell><cell>winter</cell><cell>spielen</cell><cell cols="2">42 wintersports</cell><cell>winter</cell><cell>sports</cell></row><row><cell cols="2">43 wirtschaftsembargos</cell><cell>wirtschafts</cell><cell>embargos</cell><cell cols="2">44 wirtschaftspolitik</cell><cell>wirtschafts</cell><cell>politik</cell></row><row><cell cols="2">45 zeitplan</cell><cell>zeit</cell><cell>plan</cell><cell cols="2">46 zurücktreten</cell><cell>zurück</cell><cell>treten</cell></row><row><cell cols="2">47 einheitswährung</cell><cell>einheit</cell><cell>s</cell><cell cols="2">währung</cell><cell></cell><cell></cell></row><row><cell cols="2">48 fischfangquoten</cell><cell>fisch</cell><cell>fang</cell><cell cols="2">quoten</cell><cell></cell><cell></cell></row><row><cell cols="2">49 fussballeuropameisterschaft</cell><cell>fussball</cell><cell>europa</cell><cell cols="2">meisterschaft</cell><cell></cell><cell></cell></row><row><cell cols="2">50 geographos</cell><cell>geog</cell><cell>rapho</cell><cell>s</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">51 literaturnobelpreisträgers</cell><cell>literatur</cell><cell>nobel</cell><cell cols="2">preisträgers</cell><cell></cell><cell></cell></row><row><cell cols="2">52 schönheitswettbewerbe</cell><cell>schönheit</cell><cell>s</cell><cell cols="2">wettbewerbe</cell><cell></cell><cell></cell></row><row><cell cols="2">53 schönheitswettbewerben</cell><cell>schönheit</cell><cell>s</cell><cell cols="2">wettbewerben</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="8,108.66,596.08,374.84,8.97"><head>Table 8 :</head><label>8</label><figDesc>German words in title or desc fields of the topics that are split into component words.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="9,70.86,250.52,453.56,269.73"><head>Table 9 :</head><label>9</label><figDesc>Dutch words in title or desc fields of the topics that are split into component words.</figDesc><table coords="9,83.52,250.52,428.17,269.73"><row><cell></cell><cell cols="2">compounds</cell><cell>component</cell><cell>words</cell><cell></cell><cell cols="2">compounds</cell><cell>component</cell><cell>words</cell></row><row><cell>1</cell><cell cols="2">rijkspolitie</cell><cell>rijks</cell><cell>politie</cell><cell>2</cell><cell cols="2">belangenverstrengeling</cell><cell>belangen</cell><cell>verstrengeling</cell></row><row><cell>3</cell><cell cols="2">sterrenstelsels</cell><cell>sterren</cell><cell>stelsels</cell><cell>4</cell><cell>bontsector</cell><cell></cell><cell>bont</cell><cell>sector</cell></row><row><cell>5</cell><cell>nobelprijs</cell><cell></cell><cell>nobel</cell><cell>prijs</cell><cell>6</cell><cell cols="2">verkoopaantallen</cell><cell>verkoop</cell><cell>aantallen</cell></row><row><cell>7</cell><cell cols="2">grungerock</cell><cell>grunge</cell><cell>rock</cell><cell>8</cell><cell cols="2">spionagezaak</cell><cell>spionage</cell><cell>zaak</cell></row><row><cell>9</cell><cell>frankrijk</cell><cell></cell><cell>frank</cell><cell>rijk</cell><cell cols="3">10 echtscheidingscijfers</cell><cell>echtscheidings</cell><cell>cijfers</cell></row><row><cell cols="3">11 oproepkaart</cell><cell>oproep</cell><cell>kaart</cell><cell cols="3">12 autofabrikanten</cell><cell>auto</cell><cell>fabrikanten</cell></row><row><cell cols="3">13 handelsembargo</cell><cell>handels</cell><cell>embargo</cell><cell cols="3">14 internationale</cell><cell>inter</cell><cell>nationale</cell></row><row><cell cols="2">15 duitsland</cell><cell></cell><cell>duit</cell><cell>s land</cell><cell cols="3">16 computerbeveiliging</cell><cell>computer</cell><cell>beveiliging</cell></row><row><cell cols="3">17 filmindustrie</cell><cell>film</cell><cell cols="4">industrie 18 veiligheidsbeleid</cell><cell>veiligheids</cell><cell>beleid</cell></row><row><cell cols="3">19 netwerktoegang</cell><cell>veiligheids</cell><cell>beleid</cell><cell cols="3">20 filmfestival</cell><cell>film</cell><cell>festival</cell></row><row><cell cols="3">21 omzetcrisis</cell><cell>omzet</cell><cell>crisis</cell><cell cols="3">22 computeranimatie</cell><cell>computer</cell><cell>animatie</cell></row><row><cell cols="3">23 tijdschema</cell><cell>tijd</cell><cell>schema</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell></cell><cell>6</cell><cell>7</cell><cell>8</cell></row><row><cell>features</cell><cell>none</cell><cell>decomp</cell><cell>stem</cell><cell>expan</cell><cell cols="2">decomp+stem</cell><cell>decomp+expan</cell><cell cols="2">stem+expan decomp+stem+expan</cell></row><row><cell>avg prec</cell><cell>0.4021</cell><cell>0.4186</cell><cell>0.4281</cell><cell>0.4669</cell><cell>0.4446</cell><cell></cell><cell>0.4721</cell><cell>0.4770</cell><cell>0.4847</cell></row><row><cell>recall</cell><cell>1562</cell><cell>1623</cell><cell>1584</cell><cell>1614</cell><cell>1633</cell><cell></cell><cell>1727</cell><cell>1702</cell><cell>1734</cell></row><row><cell>change</cell><cell cols="2">baseline +4.10%</cell><cell cols="2">+6.47% +16.12%</cell><cell cols="2">+10.57%</cell><cell>+17.41%</cell><cell>+18.63</cell><cell>+20.54%</cell></row></table><note coords="9,70.86,416.68,453.52,8.97;9,70.86,428.62,453.54,8.97;9,70.86,440.56,453.56,8.97;9,70.86,452.50,453.56,8.97"><p><p><p>Dutch base dictionary. The Dutch words frankrijk and duitsland were split into component words because they are not in the base dictionary. For the same reason, the word internationale was decomposed. It appears compound words in Dutch are not as common as in German. Like in German indexing, when a compound was split into component words, only the component words were retained in the index. Table</p>10</p>presents the performance of</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="9,70.86,532.72,453.59,126.19"><head>Table 10 :</head><label>10</label><figDesc>Dutch monolingual retrieval performance on CLEF-2002 test set. The total number of Dutch relevant documents for the 50 topics of CLEF 2002 is 1862.Dutch monolingual retrieval under various conditions. With no stemming and expansion, Dutch decompounding improved the average precision by 4.10%. Together the three features improved the average precision by 20.54% over the base performance when none of the features is implemented.</figDesc><table coords="9,81.60,622.16,432.01,36.75"><row><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell></row><row><cell>features</cell><cell>none</cell><cell>decomp</cell><cell>stem</cell><cell>expan</cell><cell>decomp+stem</cell><cell>decomp+expan</cell><cell cols="2">stem+expan decomp+stem+expan</cell></row><row><cell>avg prec</cell><cell>0.3239</cell><cell>0.3676</cell><cell>0.3587</cell><cell>0.3471</cell><cell>0.4165</cell><cell>0.3822</cell><cell>0.3887</cell><cell>0.4372</cell></row><row><cell>change</cell><cell cols="2">baseline +13.49%</cell><cell>+10.74%</cell><cell cols="2">+7.16% +28.59%</cell><cell>+18.00%</cell><cell>+20.01%</cell><cell>+34.98%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="9,70.86,671.38,453.59,65.37"><head>Table 11 :</head><label>11</label><figDesc>Dutch monolingual retrieval performance on CLEF-2001 test set. The total number of Dutch relevant documents for the 50 topics of CLEF 2001 is 1224.</figDesc><table /><note coords="9,82.20,715.78,442.19,8.97;9,70.86,727.78,453.55,8.97"><p>For comparison, table 11 presents the Dutch monolingual performance on the CLEF 2001 test set. Decompounding alone improved the average precision by 13.49%. Topic 88 of CELF 2001 is about mad cow diseases in</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="10,70.86,369.64,453.58,360.81"><head>Table 12 :</head><label>12</label><figDesc>Table 12  presents the CLIR retrieval performances for all the official runs and additional runs. The</figDesc><table coords="10,347.64,392.56,92.50,20.91"><row><cell>without</cell><cell>with</cell></row><row><cell cols="2">expansion expansion</cell></row></table><note coords="10,111.66,665.74,412.73,8.97;10,70.86,677.68,64.06,8.97;10,70.86,709.54,453.58,8.97;10,70.86,721.48,453.56,8.97"><p><p><p>Performance of cross-language retrieval runs. The ids and average precision values for the official runs are in bold face. ids and average precision values for the official runs are in bold face. Last column in table</p><ref type="bibr" coords="10,453.00,709.54,9.96,8.97" target="#b11">12</ref> </p>shows the improvement of average precision with query expansion over without it. When both L&amp;H Translator and Babelfish</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16" coords="14,70.86,711.40,454.71,32.91"><head>Table 16 :</head><label>16</label><figDesc>into the form shown in table 17 by grouping the consecutive irrelevant and relevant documents. Each entry in table 17 has the form wh 1o Individual ranks. Relevant documents are marked with * ranked in position from n to , inclusive. is the number of irrelevant documents in the set, and h is the number of relevant documents in the set. For example, the entry (2,1) 1¡ gU ,¡ c ,¡ Ri means the set 1¡ gU ,¡ c ,¡ i has two irrelevant documents, ¡ !U and ¡ c , and one relevant document, ¡ i . After the transformation, the procedure can be 1¡ !U ,¡ c ,¡ Ri<ref type="bibr" coords="15,343.68,237.88,11.45,8.97" target="#b0">(1,</ref><ref type="bibr" coords="15,355.13,237.88,7.64,8.97" target="#b2">3)</ref> x U , ec , i ,</figDesc><table coords="14,240.50,726.00,44.70,5.60"><row><cell>o</cell><cell>U h q9 q9 h9 q wo</cell></row></table><note coords="14,290.20,722.80,235.37,9.56;14,70.86,734.70,198.24,9.60;14,276.06,735.34,248.37,8.97"><p><p>x , where o is the id of the document ranked in the n th position in the original ranking. 1o wo U h q9 h9 q9 q wo</p>x denotes a set of consecutive irrelevant and relevant documents</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17" coords="15,209.34,283.54,173.47,8.97"><head>Table 17 :</head><label>17</label><figDesc>Ranked lists after transformation.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="9,85.20,745.76,232.78,7.17"><p>downloaded from ftp://archive.cs.ruu.nl/pub/UNIX/ispell/words.dutch.gz</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="10,85.20,740.84,160.37,7.17"><p>publicly available at http://babelfish.altavista.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="13,85.20,745.10,117.31,7.17"><p>available at http://www.babylon.com</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgments</head><p>We would like to thank <rs type="person">Vivien Petras</rs> for improving the German base dictionary. This research was supported by <rs type="funder">DARPA</rs> under research grant <rs type="grantNumber">N66001-00-1-8911</rs> (PI: <rs type="person">Michael Buckland</rs>) as part of the <rs type="programName">DARPA Translingual Information Detection, Extraction, and Summarization Program</rs> <rs type="institution">(TIDES)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_M622eU5">
					<idno type="grant-number">N66001-00-1-8911</idno>
					<orgName type="program" subtype="full">DARPA Translingual Information Detection, Extraction, and Summarization Program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="16,92.46,436.56,309.79,8.07" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="16,166.98,436.56,106.96,8.07">Statistical machine translation</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>JHU workshop</orgName>
		</respStmt>
	</monogr>
	<note>final report</note>
</biblStruct>

<biblStruct coords="16,92.46,447.00,431.97,8.07;16,92.46,457.98,264.85,8.07" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="16,336.18,447.00,188.25,8.07;16,92.46,457.98,74.46,8.07">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">J D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,173.76,457.98,92.03,8.07">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="263" to="312" />
			<date type="published" when="1993-06">June 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,92.46,468.42,431.98,8.07;16,92.46,479.40,431.92,8.07;16,92.46,490.32,431.92,8.07;16,92.46,501.30,121.99,8.07" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="16,129.72,468.42,245.36,8.07">Multilingual information retrieval using english and chinese queries</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,178.50,479.40,345.88,8.07;16,92.46,490.32,152.23,8.07">Evaluation of Cross-Language Information Retrieval Systems: Second Workshop of the Cross-Language Evaluation Forum, CLEF-2001</title>
		<title level="s" coord="16,454.14,490.32,70.25,8.07;16,92.46,501.30,77.16,8.07">Springer Computer Scinece Series LNCS</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<meeting><address><addrLine>Darmstadt, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-09">September 2001. 2002</date>
			<biblScope unit="volume">2406</biblScope>
			<biblScope unit="page" from="44" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,92.46,511.74,431.96,8.07;16,92.46,522.72,431.95,8.07" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="16,243.12,511.74,281.30,8.07;16,92.46,522.72,63.80,8.07">Full text retrieval based on probabilistic equations with coefficients fitted by logistic regression</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">S</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,251.22,522.72,172.36,8.07">The Second Text REtrieval Conference (TREC-2)</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1994-03">March 1994</date>
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,92.46,533.16,431.97,8.07;16,92.46,544.14,113.41,8.07" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="16,266.88,533.16,243.57,8.07">A freely available wide coverage morphological analyzer for english</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">Z</forename><surname>Daniel Karp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yves</forename><surname>Schabes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Egedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,92.46,544.14,86.01,8.07">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,92.46,554.58,238.81,8.07" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="16,122.70,554.58,87.30,8.07">The Structure of German</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Clarendon Press</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,92.46,565.08,431.95,8.07;16,92.46,576.00,63.73,8.07" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="16,204.30,565.08,191.10,8.07">A program for aligning sentences in bilingual corpora</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">A</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,402.24,565.08,91.97,8.07">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="75" to="102" />
			<date type="published" when="1993-03">March 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,92.46,586.50,403.09,8.07" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="16,177.60,586.50,132.96,8.07">Cross-language information retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Grefenstette</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,92.46,596.94,431.95,8.07;16,92.46,607.92,326.29,8.07" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="16,140.70,596.94,221.04,8.07">Relevance feedback and other query modification techniques</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,92.46,607.92,190.10,8.07">Information Retrieval: Data Structures &amp; Algorithms</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Frakes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</editor>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="241" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,92.46,618.36,432.02,8.07;16,92.46,629.34,244.15,8.07" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="16,233.94,618.36,290.54,8.07;16,92.46,629.34,80.23,8.07">A freely available morphological analyzer, disambiguator and context sensitive lemmatizer for german</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Lezius</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Rapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wettler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,189.78,629.34,61.31,8.07">COLING-ACL&apos;98</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="743" to="748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,92.46,639.78,431.95,8.07;16,92.46,650.76,179.47,8.07" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="16,179.46,639.78,257.11,8.07">A comparison of alignment models for statistical machine transl ation</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,456.54,639.78,39.27,8.07">COLING00</title>
		<meeting><address><addrLine>Saarbrücken, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-08">August 2000</date>
			<biblScope unit="page" from="1086" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,92.46,661.20,431.96,8.07;16,92.46,672.18,193.27,8.07" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="16,160.32,661.20,199.65,8.07">Cross Language Information Retrieval and Evaluation</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,368.52,661.20,151.70,8.07">Proceedings of the CLEF 2000 Workshop</title>
		<title level="s" coord="16,92.46,672.18,148.44,8.07">Springer Computer Scinece Series LNCS</title>
		<meeting>the CLEF 2000 Workshop</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2069</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,92.46,682.62,412.75,8.07" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
		<title level="m" coord="16,155.22,682.62,163.96,8.07">Working Notes of the CLEF 2001 Workshop 3</title>
		<meeting><address><addrLine>Darmstadt, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-09">September. September 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,92.46,693.06,431.94,8.07;16,92.46,704.04,150.13,8.07" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="16,212.76,693.06,130.85,8.07">Relevance weighting of search terms</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">S</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,350.82,693.06,173.58,8.07;16,92.46,704.04,25.90,8.07">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="page" from="129" to="146" />
			<date type="published" when="1976-06">May-June 1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,92.46,714.48,360.13,8.07" xml:id="b14">
	<monogr>
		<title level="m" coord="16,227.22,714.48,174.04,8.07">The Seventh Text Retrieval Conference (TREC-7)</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,92.46,724.98,356.29,8.07" xml:id="b15">
	<monogr>
		<title level="m" coord="16,227.22,724.98,170.20,8.07">The Eighth Text Retrieval Conference (TREC-8)</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
