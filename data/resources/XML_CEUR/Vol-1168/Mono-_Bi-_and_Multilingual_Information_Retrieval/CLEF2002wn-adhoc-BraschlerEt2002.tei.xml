<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,220.40,74.58,154.56,12.53">Eurospider at CLEF 2002</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,198.08,105.52,67.98,9.07"><forename type="first">Martin</forename><surname>Braschler</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Eurospider Information Technology AG</orgName>
								<address>
									<addrLine>Schaffhauserstrasse 18</addrLine>
									<postCode>CH-8006</postCode>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,272.96,105.52,56.75,9.07"><forename type="first">Anne</forename><surname>Göhring</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Eurospider Information Technology AG</orgName>
								<address>
									<addrLine>Schaffhauserstrasse 18</addrLine>
									<postCode>CH-8006</postCode>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,337.03,105.52,60.16,9.07"><forename type="first">Peter</forename><surname>Schäuble</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Eurospider Information Technology AG</orgName>
								<address>
									<addrLine>Schaffhauserstrasse 18</addrLine>
									<postCode>CH-8006</postCode>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,220.40,74.58,154.56,12.53">Eurospider at CLEF 2002</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B76DC03F8259E888192FC619A321BD3C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For the CLEF 2002 campaign, Eurospider participated in the multilingual and German monolingual tasks. Our main focus was on trying new merging strategies for our multilingual experiments. In this paper, we describe the setup of our system, the characteristics of our experiments, and give a first analysis of some of the results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the following, we describe our experiments carried out for CLEF 2002. Much of the work for this year's campaign builds again on the foundation laid in the previous two CLEF campaigns <ref type="bibr" coords="1,443.83,268.72,11.76,9.07" target="#b0">[1]</ref>  <ref type="bibr" coords="1,460.63,268.72,10.71,9.07" target="#b2">[2]</ref>. Eurospider participated both in the main multilingual track and in the German monolingual track. The main focus of the work was on the multilingual experiments, continuing our successful practice of combining multiple approaches to cross-language retrieval into one system. Using a combination approach makes our system more robust against deficiencies of any single CLIR approach. The main effort for this year was spent on the problem of merging multiple result lists, such as obtained either from searches on different subcollections (representing the different languages of the multilingual collection) or from searches using different retrieval methods. We feel that merging remains an unsolved problem after the first two CLEF campaigns. We introduced a new merging method based on term selection after retrieval, and we also implemented a slightly updated version of our simple interleaving merging strategy. The remainder of this paper is structured as follows: we first discuss the system setup that we used for all experiments. This is followed by a closer look at the multilingual experiments, including details of this year's new merging strategies. The next section describes our submissions for the German monolingual track. The paper closes with conclusions and an outlook.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System setup</head><p>All experiments used a system consisting of the core software that is included in the "relevancy" system developed by Eurospider Information Technology AG. This core system is the same that is used in all commercial products of Eurospider. Prototypical enhancements included in the CLEF system are mainly in the components for multilingual information access (MLIA).</p><p>As an additional experiment, we collaborated this year with Université de Neuchâtel in order to produce a special multilingual run. The results for this run are based both on output from the Eurospider system and on output produced by the Neuchâtel group. More details will be given later in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Indexing:</head><p>The following table gives an overview of indexing methods used for the respective languages: Ranking: the system was configured to use straight Lnu.ltn weighting, as described in <ref type="bibr" coords="1,433.27,688.48,10.71,9.07" target="#b4">[3]</ref>. An exception was made for some of the monolingual runs, which either used BM25 weighting <ref type="bibr" coords="1,389.83,700.00,11.76,9.07" target="#b5">[4]</ref> or a mix of both Lnu.ltn and BM25. All runs used a blind feedback loop, expanding the query by the top 10 terms from the 10 best ranked documents.</p><p>We decided to make indexing and weighting of all languages as symmetric as possible. We did not use different weighting schemes for different languages, or different policies with regard to query expansion. By choosing this approach, we wanted to both mirror a realistic, scalable approach as well as avoid overtraining to characteristics of the CLEF multilingual collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Submitted Runs:</head><p>The following table summarizes the main characteristics of our official experiments: Lnu.ltn + BM25</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multilingual retrieval</head><p>As for last year, we again spent our main effort on the multilingual track. The goal of this track in CLEF is to select a topic language, and use the queries in that language to retrieve documents in five different languages from a multilingual collection. A single result list has to be returned, potentially containing documents in all languages.</p><p>A system working on such a task needs to bridge the gap between the language of the search requests and the languages used in the document collection. For translation, we have successfully used combination approaches, integrating more than one translation method, in the past two campaigns. In 2001, we attempted our most ambitious combination yet <ref type="bibr" coords="2,180.55,713.44,10.71,9.07" target="#b2">[2]</ref>, combining three forms of query translation (similarity thesaurus <ref type="bibr" coords="2,458.95,713.44,11.76,9.07" target="#b6">[5]</ref> [6], machine translation and machine-readable dictionaries) with document translation (through machine translation). This year, we shifted the focus of our experiments somewhat, and used a slightly simpler combination approach, discarding the machine-readable dictionaries from the system. The experiments used either a combination of document translation DT and two forms of query translation QT (machine translation MT and similarity thesaurus ST) or document translation only.</p><p>More than last year, we concentrated on the problem of producing the final, multilingual result list that is to be returned by the system in answer to a search request. Our combination approach, as indeed most approaches used in the CLEF 2000 and 2001 campaigns, produces various intermediate search results, either due to using different translation methods, or due to handling only a subset of the five languages at a time. These intermediate search results need then be "merged" to produce the multilingual result list necessary for submission to CLEF.</p><p>It seems to be generally agreed among the community of active participants in CLEF that merging is an important problem in designing truly multilingual retrieval systems, but as pointed out in the discussions at the CLEF 2001 workshop, not much progress has been made on this topic in the 2001 campaign.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Merging</head><p>In our previous participations to CLEF, we have stuck to two very simple merging strategies: rank-based merging, and interleaving.</p><p>For merging, we essentially distinguish two scenarios:</p><p>1. Both retrieval results were calculated on the same search space. The two result lists will essentially be a reordering of each other (with some extra items appearing at the bottom of the lists). 2. The sets of documents in the result lists are disjoint. This is the case if the runs were produced through retrieval on disjoint search spaces, e.g. one search on the English part of the multilingual CLEF collection, and another search on the French part.</p><p>Some merging strategies apply to both scenarios, whereas some strategies can only be used for scenario one. Rank-based merging can only be applied if the search spaces are shared among the lists to be merged.</p><p>Interleaving is a more general strategy and can be used for both scenarios.</p><p>The main difficulty in merging is the lack of comparability of scores across different result lists. Result lists obtained from different collections, or through different weightings, are not directly comparable. The retrieval status value RSV that the weighting scheme attaches to every document is only used for sorting the list, and is only valid in the context of the query, weighting and collection used.</p><p>Both merging strategies described below address the problem by not using the RSV scores at all in determining the new rank of a document in the merged result list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Rank-based Merging</head><p>For rank-based merging, calculation of a new RSV value for the merged list is based on the ranks of the documents in the original result lists. To calculate the new RSV of a document, its ranks in all the result lists are added. Clearly, the strategy only applies if the search space of all runs is shared, and therefore a substantial "overlap" in the documents retrieved for the individual runs exists. Since we feel that there is more importance in a rank difference between highly ranked documents than in a similar difference among lower ranked documents, we introduced a logarithmic dampening of the rank value, thus boosting the influence of highly ranked documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Interleaving</head><p>As an alternative that applies in both merging scenarios (same search space and disjoint search space), we have in the past used interleaving: the merged result list is produced by taking one document in turn from all individual lists. If the collections are not disjoint, duplicates will have to be eliminated after merging, for example by keeping the most highly ranked instance of a document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">New merging strategies</head><p>We introduced two new merging strategies for this year's experiments. The first is a slight update of the interleaving strategy. The second is more elaborate, and presents an attempt to guess how well a query "hit" a specific subcollection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Collection size-based interleaving</head><p>One main deficiency of interleaving as described above is that all result lists are handled equally, taking the same number of documents from each. It is extremely difficult to determine the number of relevant documents to be expected in the individual subcollections, but we have observed that the ratio of relevant items is quite stable across the different languages in CLEF. Consequently, we have used a simple update to the straight interleaving method: since the subcollections of the CLEF multilingual collection vary considerably in size, we take the portion of documents taken from any one result list to be proportional to the size of the corresponding subcollection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Feedback merging</head><p>The second new strategy aims to predict the amount of relevant information contributed by each subcollection for a specific search request. It does this by carrying out an initial retrieval step, and then analyzing the top ranked documents from the result set, building an "ideal" query to retrieve that set of documents. This query is then compared to the original query, determining the overlap as an indication for the degree to which the concepts of the original query are represented in the retrieval result. The better such representation, the higher is the estimate of relevant documents. The result lists are then finally merged in proportion to these estimates. The biggest advantage of this method is its query dependence: whereas all the other methods described above use fixed ratios for merging the different result sets, this method determines an "optimal" ratio per query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Results</head><p>The results for our officially submitted multilingual experiments are detailed in the following, and a first analysis is given. As can be seen from Table <ref type="table" coords="4,186.80,666.64,3.78,9.07" target="#tab_2">3</ref>, there is very little difference between runs EIT2MDC3 and EIT2MDF3, which differ only in the merging strategy employed. Additionally to having essentially equivalent average precision values, the two runs also differ only very slightly when compared on a query-by-query basis. Clearly, the merging strategy based on feedback, which we newly introduced for this year, had little impact, even though it allows different merging ratios for different queries. When looking at the ratios which were actually used for merging, we see that the new method indeed chooses fairly different ratios for individual queries. Why this does not result into more difference is not immediately clear to us and will require further analysis.</p><p>The two runs based on full queries (including the narrative field), EIT2MNU1 and EIT2MNF3, also show little variation in performance as measured by average precision. Closer analysis shows some striking differences however: EIT2MNF3, which is based on a combination of query translation and document translation, outperforms EIT2MNU1 by more than 20% for 14 queries, whereas the reverse is only true for 4 of the queries. Also, EIT2MNF3 retrieves more than 8% more relevant documents than run EIT2MNU1. We believe that this shows that the combination approach boosts reliability of the system by retrieving extra items, but that we have not found the ideal combination strategy this year that would have maximized this potential.</p><p>The potential to retrieve more relevant items by using combination approaches is also demonstrated by our final multilingual entry, EAN2MDF4, which was produced by merging output from the Eurospider system with results kindly provided to us by Université de Neuchâtel (Prof. J. Savoy). This run retrieves the most relevant items among all our multilingual entries. Compared to median performance, all five multilingual runs perform very well. All runs have around 80% of the queries performing on or above the median, with the Eurospider/Neuchâtel combined run outperforming the median in more than 90% of the cases. The "virtual median performance", obtained by an artificial run that mirrors the median performance among all submissions for every query, is outperformed by more than 50% for all experiments. The best run obtains slightly below 75% of the "virtual best performance", which is obtained by an artificial run combining the best entries for every query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Monolingual retrieval</head><p>For monolingual retrieval, we restricted ourselves to the German document collection. We fine-tuned our submissions compared to last year, and experimented with two different weighting schemes.</p><p>In contrast to last year, we used blind feedback, which was found to be beneficial in CLEF 2001 by several groups. Our German runs used the Spider German stemmer coupled with splitting of German compound nouns (decompounding). We chose the most aggressive splitting method available in the system, in order to split a maximum number of compound nouns. The results show very little difference between EIT2GDB1 and EIT2GDL1, which used the BM25 and Lnu.ltn weighting scheme, respectively. Query-by-query analysis confirms little impact from choosing between the two alternatives. Not surprisingly, merging the two runs (into EIT2GDM1) leads again to very similar performance. On an absolute basis, the runs all perform well, with all runs having around 60%-75% of queries with performance above the median. All runs also outperform the "virtual median performance". For German monolingual, this seems to be a harder benchmark, as "virtual median performance" is around 65% of "virtual best performance", whereas for the multilingual track it was only roughly 45% of the "optimum".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Outlook</head><p>Our main focus this year were experiments on the problem of merging multiple result lists coming from the different language-specific subcollections in the multilingual task. We introduced a new method based on feedback merging, which showed little impact in practice. We will now evaluate the results more closely to determine why the new strategy hardly affected retrieval behavior.</p><p>As in last year, we again used a combination translation approach for the multilingual experiments. The results confirm last year's good performance. We can again conclude that combination approaches are robust; 80-90% of all queries performed on or above median performance in our systems. This good performance was achieved even though we avoided special configuration for individual languages, in order to more accurately reflect situations were only few details about the collections to be searched are known in advance.</p><p>For the monolingual experiments, we compared the impact of choosing between two of the most popular highperformance weighting schemes: BM25 and Lnu.ltn. We could not detect a meaningful difference, either in overall performance or when comparing individual queries. Consequently, combining the two weightings gives no clear advantage over using either one individually.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,75.19,592.96,441.44,81.07"><head>Table 1 .</head><label>1</label><figDesc>Overview of indexing setup for all languages.</figDesc><table coords="1,75.19,605.20,441.44,68.83"><row><cell cols="3">Language Stopwording Stemming</cell><cell>Convert diacritical chars</cell></row><row><cell>English</cell><cell>yes</cell><cell>English "Porter-like" stemmer</cell><cell>-</cell></row><row><cell>French</cell><cell>yes</cell><cell>French Spider stemmer</cell><cell>no</cell></row><row><cell>German</cell><cell>yes</cell><cell cols="2">German Spider stemmer, including decompounding yes</cell></row><row><cell>Italian</cell><cell>yes</cell><cell>Italian Spider stemmer</cell><cell>no</cell></row><row><cell>Spanish</cell><cell>yes</cell><cell>Spanish Spider stemmer</cell><cell>yes</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,75.19,176.80,445.84,393.31"><head>Table 2 .</head><label>2</label><figDesc>Main characteristics of official experiments.</figDesc><table coords="2,75.19,189.04,445.84,381.07"><row><cell>Run tag</cell><cell>Track</cell><cell>Topic</cell><cell>Topic</cell><cell>Run type</cell><cell cols="2">Translation Merging</cell><cell>Expansion Weighting</cell></row><row><cell></cell><cell></cell><cell>lang.</cell><cell>fields</cell><cell></cell><cell></cell><cell>strategy</cell></row><row><cell cols="3">EIT2MNU1 Multilingual DE</cell><cell cols="3">TDN Automatic Documents</cell><cell>N/A</cell><cell>Blind</cell><cell>Lnu.ltn</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(MT)</cell><cell></cell><cell>Feedback</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10/10</cell></row><row><cell cols="3">EIT2MNF3 Multilingual DE</cell><cell cols="3">TDN Automatic Queries</cell><cell cols="2">Term Sel Blind</cell><cell>Lnu.ltn</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(ST+MT),</cell><cell></cell><cell>Feedback</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Documents</cell><cell></cell><cell>10/10</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(MT)</cell><cell></cell></row><row><cell cols="3">EIT2MDF3 Multilingual DE</cell><cell>TD</cell><cell cols="2">Automatic Queries</cell><cell cols="2">Term Sel Blind</cell><cell>Lnu.ltn</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(ST+MT),</cell><cell></cell><cell>Feedback</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Documents</cell><cell></cell><cell>10/10</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(MT)</cell><cell></cell></row><row><cell cols="3">EIT2MDC3 Multilingual DE</cell><cell>TD</cell><cell cols="2">Automatic Queries</cell><cell>Interleave</cell><cell>Blind</cell><cell>Lnu.ltn</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(ST+MT),</cell><cell>coll</cell><cell>Feedback</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Documents</cell><cell></cell><cell>10/10</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(MT)</cell><cell></cell></row><row><cell cols="3">EAN2MDF4 Multilingual EN</cell><cell>TD</cell><cell cols="2">Automatic Queries</cell><cell>Logrank</cell><cell>Blind</cell><cell>Lnu.ltn +</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(MT+Uni</cell><cell></cell><cell>Feedback</cell><cell>Uni</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Neuchâtel)</cell><cell></cell><cell>10/10+Uni</cell><cell>Neuchâtel</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Neuchâtel</cell></row><row><cell cols="2">EIT2GDB1 German</cell><cell>DE</cell><cell>TD</cell><cell cols="2">Automatic N/A</cell><cell>N/A</cell><cell>Blind</cell><cell>BM25</cell></row><row><cell></cell><cell>Monolingual</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Feedback</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10/10</cell></row><row><cell cols="2">EIT2GDL1 German</cell><cell>DE</cell><cell>TD</cell><cell cols="2">Automatic N/A</cell><cell>N/A</cell><cell>Blind</cell><cell>Lnu.ltn</cell></row><row><cell></cell><cell>Monolingual</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Feedback</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10/10</cell></row><row><cell cols="2">EITGDM1 German</cell><cell>DE</cell><cell>TD</cell><cell cols="2">Automatic N/A</cell><cell>Logrank</cell><cell>Blind</cell><cell>Lnu.ltn +</cell></row><row><cell></cell><cell>Monolingual</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Feedback</cell><cell>BM25</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10/10</cell></row><row><cell cols="2">EITGNM1 German</cell><cell>DE</cell><cell cols="3">TDN Automatic N/A</cell><cell>Logrank</cell><cell>Blind</cell></row><row><cell></cell><cell>Monolingual</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Feedback</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10/10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,70.88,441.28,426.56,115.87"><head>Table 3 :</head><label>3</label><figDesc>Key performance figures for the multilingual experiments</figDesc><table coords="4,70.88,453.52,426.56,103.63"><row><cell>Run tag</cell><cell>Average Precision</cell><cell>Precision @ 10 docs</cell><cell>Relevant retrieved</cell></row><row><cell>EIT2MNU1</cell><cell>0.3539</cell><cell>0.6560</cell><cell>5188</cell></row><row><cell>EIT2MNF3</cell><cell>0.3554</cell><cell>0.6520</cell><cell>5620</cell></row><row><cell>EIT2MDC3</cell><cell>0.3400</cell><cell>0.5860</cell><cell>5368</cell></row><row><cell>EIT2MDF3</cell><cell>0.3409</cell><cell>0.6040</cell><cell>5411</cell></row><row><cell>EAN2MDF4</cell><cell>0.3480</cell><cell>0.6260</cell><cell>5698</cell></row><row><cell cols="2">Total number of relevant documents: 8068</cell><cell></cell><cell></cell></row><row><cell cols="2">"Virtual best performance": 0.4834</cell><cell></cell><cell></cell></row><row><cell cols="2">"Virtual median performance": 0.2193</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,75.19,571.12,407.04,81.07"><head>Table 4 :</head><label>4</label><figDesc>Comparison of performance against median for multilingual experiments</figDesc><table coords="4,75.19,583.36,407.04,68.83"><row><cell>Run Tag</cell><cell>Best</cell><cell cols="2">Above Median Median</cell><cell cols="2">Below Median Worst</cell></row><row><cell>EIT2MNU1</cell><cell>4</cell><cell>34</cell><cell>2</cell><cell>9</cell><cell>1</cell></row><row><cell>EIT2MNF3</cell><cell>5</cell><cell>34</cell><cell>3</cell><cell>8</cell><cell>0</cell></row><row><cell>EIT2MDC3</cell><cell>1</cell><cell>36</cell><cell>3</cell><cell>10</cell><cell>0</cell></row><row><cell>EIT2MDF3</cell><cell>2</cell><cell>37</cell><cell>1</cell><cell>10</cell><cell>0</cell></row><row><cell>EAN2MDF4</cell><cell>1</cell><cell>45</cell><cell>0</cell><cell>4</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,70.87,386.08,426.56,103.87"><head>Table 5 :</head><label>5</label><figDesc>Key performance figures for the monolingual experiments</figDesc><table coords="5,70.87,398.32,426.56,91.63"><row><cell>Run tag</cell><cell>Average precision</cell><cell>Precision @ 10</cell><cell>Relevant retrieved</cell></row><row><cell>EIT2GDB1</cell><cell>0.4482</cell><cell>0.5160</cell><cell>1692</cell></row><row><cell>EIT2GDL1</cell><cell>0.4561</cell><cell>0.5420</cell><cell>1704</cell></row><row><cell>EIT2GDM1</cell><cell>0.4577</cell><cell>0.5320</cell><cell>1708</cell></row><row><cell>EIT2GNM1</cell><cell>0.5148</cell><cell>0.5940</cell><cell>1843</cell></row><row><cell cols="2">Total number of relevant documents: 1938</cell><cell></cell><cell></cell></row><row><cell cols="2">Virtual best performance: 0.6587</cell><cell></cell><cell></cell></row><row><cell cols="2">Virtual median performance: 0.4244</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="5,75.19,503.92,406.80,69.07"><head>Table 6 :</head><label>6</label><figDesc>Comparison of performance against median for monolingual experiments</figDesc><table coords="5,75.19,516.16,406.80,56.83"><row><cell>Run Tag</cell><cell>Best</cell><cell cols="2">Above Median Median</cell><cell cols="2">Below Median Worst</cell></row><row><cell>EIT2GDB1</cell><cell>1</cell><cell>28</cell><cell>2</cell><cell>19</cell><cell>0</cell></row><row><cell>EIT2GDL1</cell><cell>3</cell><cell>27</cell><cell>5</cell><cell>15</cell><cell>0</cell></row><row><cell>EIT2GDM1</cell><cell>1</cell><cell>30</cell><cell>5</cell><cell>14</cell><cell>0</cell></row><row><cell>EIT2GNM1</cell><cell>6</cell><cell>32</cell><cell>4</cell><cell>8</cell><cell>0</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank <rs type="person">Jacques Savoy</rs> from <rs type="affiliation">Université de Neuchâtel</rs> for providing us with his runs that we used for merging in the experiment labeled "EAN2MDF4".</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="6,85.27,273.28,439.08,9.07;6,70.87,284.80,68.87,9.07" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,201.91,273.28,245.36,9.07">Experiments with the Eurospider Retrieval System for CLEF</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schäuble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,474.79,273.28,24.56,9.07">CLEF</title>
		<imprint>
			<biblScope unit="page" from="140" to="148" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,139.74,284.80,384.89,9.07;6,70.87,296.32,453.76,9.07;6,70.87,307.84,316.32,9.07" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,221.35,284.80,303.28,9.07;6,70.87,296.32,142.34,9.07">Cross-Language Information Retrieval and Evaluation, Workshop of the Cross-Language Evaluation Forum</title>
	</analytic>
	<monogr>
		<title level="m" coord="6,222.07,296.32,46.46,9.07">CLEF 2000</title>
		<title level="s" coord="6,493.99,296.32,30.64,9.07;6,70.87,307.84,108.73,9.07">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000-09">September 2000. 2001</date>
			<biblScope unit="volume">2069</biblScope>
		</imprint>
	</monogr>
	<note>Revised Papers</note>
</biblStruct>

<biblStruct coords="6,85.27,319.36,439.28,9.07;6,70.87,330.88,155.99,9.07" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,271.27,319.36,253.28,9.07">Experiments with the Eurospider Retrieval System for CLEF</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ripplinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schäuble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,98.71,330.88,24.56,9.07">CLEF</title>
		<imprint>
			<biblScope unit="page" from="102" to="110" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,226.86,330.88,297.49,9.07;6,70.87,342.40,453.76,9.07;6,70.87,353.68,453.36,9.07;6,70.87,365.20,279.84,9.07" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,70.87,342.40,453.76,9.07;6,70.87,353.68,71.78,9.07">Evaluation of Cross-Language Information Retrieval Systems, Second Workshop of the Cross-Language Evaluation Forum</title>
	</analytic>
	<monogr>
		<title level="m" coord="6,151.03,353.68,45.98,9.07">CLEF 2001</title>
		<title level="s" coord="6,456.55,353.68,67.68,9.07;6,70.87,365.20,72.25,9.07">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Martin</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Kluck</surname></persName>
		</editor>
		<meeting><address><addrLine>Darmstadt, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">September 3-4, 2001. 2002</date>
			<biblScope unit="volume">2406</biblScope>
		</imprint>
	</monogr>
	<note>Revised Papers</note>
</biblStruct>

<biblStruct coords="6,85.27,376.72,433.92,9.07;6,519.19,374.62,5.16,5.83;6,70.87,388.24,409.56,9.07" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,240.55,376.72,168.54,9.07">Pivoted Document Length Normalization</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,429.43,376.72,89.76,9.07;6,519.19,374.62,5.16,5.83;6,70.87,388.24,325.96,9.07">Proceedings of the 19 th ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 19 th ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,85.27,399.76,439.08,9.07;6,70.87,411.28,453.60,9.07;6,70.87,422.80,43.08,9.07" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,444.55,399.76,74.57,9.07">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,70.87,411.28,255.31,9.07">Proceedings of the Third Text REtrieval Conference (TREC.3)</title>
		<title level="s" coord="6,334.15,411.28,103.68,9.07">NIST Special Publication</title>
		<meeting>the Third Text REtrieval Conference (TREC.3)</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">500</biblScope>
			<biblScope unit="page" from="109" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,85.27,434.32,438.96,9.07;6,70.87,445.84,374.52,9.07" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,159.43,434.32,132.61,9.07">Concept Based Query Expansion</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Frei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,311.59,434.32,212.64,9.07;6,70.87,445.84,210.52,9.07">Proceedings of the 16th ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 16th ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="160" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,85.27,457.36,439.12,9.07;6,70.87,468.88,453.52,9.07;6,70.87,480.40,134.04,9.07" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,269.83,457.36,254.56,9.07;6,70.87,468.88,28.29,9.07">Cross-language information retrieval in a multilingual legal domain</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sheridan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schäuble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,119.83,468.88,404.56,9.07;6,70.87,480.40,35.10,9.07">Proceedings of the First European Conference on Research and Advanced Technology for Digital Libraries</title>
		<meeting>the First European Conference on Research and Advanced Technology for Digital Libraries</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="253" to="268" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
