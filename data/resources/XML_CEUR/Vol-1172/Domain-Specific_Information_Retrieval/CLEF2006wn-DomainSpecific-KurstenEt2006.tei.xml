<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,86.64,74.11,421.23,12.65;1,162.00,90.19,270.96,12.65">Monolingual Retr ieval Exper iments with a DomainSpecific Document Cor pus at the Chemnitz Technical Univer sity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,236.40,121.17,49.63,9.16"><forename type="first">Jens</forename><surname>Kürsten</surname></persName>
							<email>jens.kuersten@s2000.tuchemnitz.de</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">Chemnitz University of Technology</orgName>
								<address>
									<addrLine>Media Informatics Strasse der Nationen 62</addrLine>
									<postCode>09107</postCode>
									<settlement>Chemnitz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,293.04,121.17,65.68,9.16"><forename type="first">Maximilian</forename><surname>Eibl</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">Chemnitz University of Technology</orgName>
								<address>
									<addrLine>Media Informatics Strasse der Nationen 62</addrLine>
									<postCode>09107</postCode>
									<settlement>Chemnitz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,86.64,74.11,421.23,12.65;1,162.00,90.19,270.96,12.65">Monolingual Retr ieval Exper iments with a DomainSpecific Document Cor pus at the Chemnitz Technical Univer sity</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">16E188BEE887B9DAFBAB0D02C4E42231</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Infor mation Stor age and Retr ieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval I.5 [Patter n Recognition]: I.5.3 Clustering General Ter ms Measurement</term>
					<term>Performance</term>
					<term>Experimentation</term>
					<term>Domain Specific Retrieval</term>
					<term>GIRTCorpus Pseudo relevance feedback</term>
					<term>Local clustering</term>
					<term>Data fusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article describes the first participation of the Media Informatics Section of the Chemnitz Technical University at the Cross Language Evaluation Forum. A first experimental prototype is described which implements several different methods of optimizing search results. The configuration of the prototype is tested with the GIRT corpus. The results of the DomainSpecific Monolingual German task suggest that combining the suffix stripping stemming and the decompounding approach is very useful. Also, a local document clustering approach used to improve pseudo relevance feedback seems to be quite beneficial. Nevertheless, the evaluation of the English task using the same configuration suggests that the qualities of the results are highly speech dependent.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In our first participation in the Cross Language Evaluation Forum we focused on the development of a retrieval system which provides a good baseline for the Monolingual GIRT tasks. Furthermore, we tried to improve the baseline results with several optimisation approaches. The outline of this paper is organized as follows. Section 2 introduces the concepts used to achieve good baseline results and section 3 describes the concepts used to optimise the baseline results. Section 4 deals with the configuration of our submitted runs and section 5 recapitulates the results. In section 6 the results are discussed with respect to our expectations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Improving baseline results using the CLEF training data</head><p>At first, we had to design a system which implements the well known algorithms of information retrieval. We used the Apache Lucene API <ref type="bibr" coords="1,246.48,685.17,11.75,9.16" target="#b0">[1]</ref> as core to simplify matters. The developed system consists of the following components: (a) filters for indexing: lowercase filter, stop word filter and stem filter, (b) several algorithms for query construction and (c) an evaluation tool, which allows comparison of different configurations based on the training data. The weighting scheme is embedded in the Lucene core and it is defined in its documentation.</p><p>In our primary experiments with the training data provided by the CLEF campaign we concentrated on the DomainSpecific Monolingual German task. Unfortunately, the results were quite bad results in those experiments. Thus, we searched the respective components for possible improvements. We decided to improve the query construction process and to try different German stemmers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Constructing improved queries</head><p>In a first step only the topic terms (title and description) in all fields of the GIRT corpus were searched. After evaluating our results we analysed the corpus and we observed that the title, abstract, controlledterm and classificationtext fields should be used for searching, because the other fields do not contain information which can successfully be used for indexing and searching. A slight improvement was achieved by adapting the query construction process to search the topic terms only within the fields abovementioned. As we wanted to participate with automatic runs, we have to consider the CLEF guideline <ref type="bibr" coords="2,368.64,220.05,10.53,9.16" target="#b1">[2]</ref>. It states that all fields of the GIRT corpus can be used for automatic retrieval, which is still satisfied. Also, the query construction had to be improved. There are many ways to combine the search terms within a query in the correct manner. For example, one can build phrases, i.e. that two or more terms have to appear besides each other and in the correct order. But phrases are not the only way to specify or generalise a search for two or more terms which could have a semantic connection. Other methods to combine terms in a query formulation procedure are span queries, fuzzy queries and range queries. We experimented with phrase queries without a significant success and with span queries with a slight improvement of precision. In our initial experiments we also observed that the topics should be analysed before they are used for query construction. The problem with the topics is that they contain certain terms that are not beneficial for searching the GIRT collection. Thus, we implemented a topic analyzer to remove those terms automatically. We had to be careful by realizing this, since we did not want to eliminate terms that are important for a specific topic. As the described topic analyzer works without any kind of human intervention, we are still constructing our queries automatically according to the CLEF guideline. As shown in table 1, we achieved a significant improvement of both precision and recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Differ ent stemming algor ithms for Ger man</head><p>We compared two different stemming approaches for German. The first one is a part of the Snowball project <ref type="bibr" coords="2,102.72,565.89,11.75,9.16" target="#b2">[3]</ref> and the second one was implemented at Chemnitz University of Technology by Stefan Wagner <ref type="bibr" coords="2,510.24,565.89,10.53,9.16" target="#b3">[4]</ref>.</p><p>With the development of the second stemmer, we aimed at the improvement of the effectiveness of our retrieval results. Therefore, we used a stemming approach, which is quite different from well known suffix stripping algorithms. In our approach every term is divided into a number of syllables. These syllables are treated by the system as tokens. Each token then can consist of syllables again. Different implementations of the decompounding approach were tested to detect which one would achieve the best retrieval results. Our decompounding algorithm has two options: (1) remove flexion and (2) avoid overstemming. With the first option we try to remove flexions from the original term and with the second one we try to remove some letters to avoid the problem of overstemming.</p><p>Tables 2, 3 and 4 compare the results of the two stemming approaches and their algorithms on the basis of the CLEF training data from 2003 to 2005. As one can see in the result tables both stemming approaches outperform the nonstemmed approach. There is also a major improvement with our decompounding approach, which is not very significant at precision. But concerning the recall values we achieve an increase of 11 3 Concepts to optimise the monolingual baseline r esults</p><p>In this section we will introduce and describe the concepts we used to optimise our baseline results. Three different optimisation approaches and their possible combinations were tested. In the first subsection we take a look at the well known query expansion approach. Thereafter, we describe how we tried to use clustering to get further improvements. Finally, we show which fusion algorithms were used to combine the results of the two stemming algorithms from section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Query expansion</head><p>Query expansion is one of the most widely used techniques to improve precision and/or recall of information retrieval systems. There are many different approaches in the query expansion field. The main distinction of them is whether manual (user) intervention is needed or not. In our experiments for automatic retrieval we used the pseudo relevance feedback approach. Pseudo relevance feedback is a form of unsupervised learning, where terms from the top k documents of an initial retrieval are used to reformulate the original query.</p><p>Unfortunately, the application of pseudo relevance feedback is not useful in all cases. Many researchers tried to figure out how the reformulation of the query has to be done and which of the available features have to be used for it. Thus, various approaches exist in this field. Again, we tried to keep things simple and implemented a pseudo relevance feedback strategy as follows.</p><p>We also used the top k documents of an initial retrieval run. From those k documents, where k = 10 turned out to be a good choice, we only took terms that appeared at least n times in our k documents. Thereby, it can be ensured that only terms from those documents are used that are term correlated and related to the topic anyway. In our experiments we found out that n = 3 seems to be a good value for our query expansion strategy.</p><p>In further experiments with our feedback algorithm we tried to improve the performance by using the feedback algorithm repeatedly, but we observed that only using it twice could be useful. Higher numbers of iterations did not improve retrieval performance; in fact they even deteriorated performance. The results of our successful experiments with query expansion are summarized in table 5 and compared to a retrieval run without query expansion (first row). We only show the results of the CLEF training data from 2005 to keep clearness. As one can see, we got a strong performance increase with our feedback strategy. The results of the tests with the training data from the other years as well as those with the stemming algorithm from section 2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Local clustering</head><p>This section describes how we applied clustering in the query expansion procedure. We also mention the algorithms implemented for clustering. The goal of the appliance of clustering was again the improvement of precision and/or recall.</p><p>At first, a short review of the different clustering techniques is given. Principally, one has to distinguish between nonhierarchical and hierarchical algorithms for clustering. We tested both concepts to find out what advantages and disadvantages they have in an IR application. The nonhierarchical methods can be further subdivided into single pass partitioning and reallocation algorithms. The nonhierarchical algorithms are all heuristic in nature because they require a priori decisions, e.g. about cluster size and cluster number. For that reason, these algorithms are only approximating clusters. Nevertheless, the most commonly used non hierarchical clustering algorithm called kmeans was also implemented.</p><p>Although many researchers have been concentrating their forces on hierarchical clustering, some papers on kmeans like algorithms have appeared in the last few years. For example Steinbach et al. compared some of the most widely used approaches with an algorithm they developed <ref type="bibr" coords="4,352.56,360.45,10.70,9.16" target="#b4">[5]</ref>. A short overview on nonhierarchical clustering is also given in <ref type="bibr" coords="4,180.72,371.97,10.53,9.16" target="#b5">[6]</ref>, but the main part of the paper is on hierarchical clustering. In <ref type="bibr" coords="4,457.44,371.97,11.75,9.16" target="#b6">[7]</ref> Peter Willett provided a review of research on hierarchical document clustering. As abovementioned, we also implemented hierarchical clustering. We applied the LanceWilliams update formula <ref type="bibr" coords="4,371.52,395.01,11.52,9.16" target="#b5">[6]</ref> to get the possibility to compare different hierarchical algorithms. To avoid confusion we do not explain in further detail how our clustering algorithms were implemented.</p><p>As mentioned in the beginning of this section we tried to apply clustering in the query expansion procedure. Therefore, we tried the following configurations: (a) document clustering: using the top k documents returned from clustering the top n documents of an initial retrieval, (b) term clustering: using the terms returned from clustering the terms of the top n documents of an initial retrieval, (c) document clustering + PRF: combining (a) with our query expansion from section 3.1 and (d) term clustering + PRF: combining (b) with our query expansion from section 3.1. For document clustering our experiments showed, that clustering the top n = 50 documents of an initial retrieval provides the best results.</p><p>The term clustering approach is quite different from the traditional document clustering, because the objects to be clustered are not the top k documents themselves, but the terms of those top k documents. To get more reliable termcorrelations, we used the top k = 20 documents for term clustering. A brief description of some methods for term clustering is given in <ref type="bibr" coords="4,252.00,544.53,10.53,9.16" target="#b7">[8]</ref>. The results show that local document clustering can achieve a small improvement at precision and recall, when it is combined with the pseudo relevance feedback method described in section 3.1. Furthermore, it is obvious that term clustering did not improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Merging</head><p>This section will give a short review on the result list fusion approaches we implemented to combine the different indexing (stemming) schemes described in section 2.2. The motivation of applying fusion of different result lists is based on the following two effects. First, the relevant documents retrieved by different retrieval systems should have a high rank and a high overlap. Second, the nonrelevant documents of the result lists obtained by different systems are supposed to have a low rank and a low overlap. It is assumed that those hypotheses hold on the result lists we want to fuse.</p><p>Merging has also been in the interest of many researchers in the IR field. In the report of Fox and Shaw <ref type="bibr" coords="5,70.80,231.57,11.75,9.16" target="#b8">[9]</ref> methods for result list fusion have been compared. In their result summarizing the RSV values performed best. Savoy <ref type="bibr" coords="5,120.72,243.09,16.79,9.16" target="#b9">[10]</ref> also compared some data fusion operators and he additionally suggested the zscore operator, which performs best in his study. Another fusion operator called normbytopk was introduced by Lin and Chen in their report on merging mechanisms for multilingual information retrieval <ref type="bibr" coords="5,400.80,266.13,15.26,9.16" target="#b10">[11]</ref>. Beside the most widely known round robin and raw score fusion operators we implemented the fusion mechanisms depicted in table <ref type="table" coords="5,509.28,277.65,3.66,9.16" target="#tab_5">7</ref>. name formula explanation  Generally speaking, the results of our experiments were as expected. The result list fusion approach for the DomainSpecific Monolingual German task clearly outperformed our other experiments. Moreover, the local clustering of the top documents in combination with the PRF approach achieves slight better results than PRF alone or PRF in combination with local term clustering.</p><formula xml:id="formula_0" coords="5,122.64,313.17,348.64,100.21">RSV k retrieval status value of document k in list i sum RSV å ´ k k i RSV 1 a i a weighting factor for list i Norm max ) ( 1 å ´ k i k i Max RSV a Max i maximum retrieval status value of list i Norm RSV å - - ´ k i i i k i Min Max Min RSV</formula><p>In the results of the DomainSpecific Monolingual English task one can see that also PRF with document clustering performs best and PRF with term clustering worked worst. Besides, the differences in the results were very small. Additionally, we have to reinvestigate our experiments for the Monolingual English task, because the gap between those results and the results of the Monolingual German task indicates, that there might be an unknown issue with those experiments, when we keep in mind that the used corpora are pseudo parallel.</p><p>Concluding our experiments one can say, that for German text retrieval, combining the suffix stripping stemming and the decompounding approach is very useful and further experiments with other languages should be done to determine if one can achieve similar results with them. Also, the local document clustering approach for improvement of PRF seems to be quite beneficial, but again further experiments, especially with different corpora, should be carried out to detect, if the improvement works only within this (domainspecific) environment or not.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,185.52,407.74,3.48,6.33;5,277.44,402.32,4.00,10.91;5,216.24,402.32,4.00,10.91;5,277.92,385.28,4.00,10.91;5,215.52,385.28,4.00,10.91;5,194.40,392.48,7.57,11.54;5,326.88,383.01,18.60,9.83;5,371.28,383.01,100.16,9.16;5,371.28,394.53,52.72,9.16;5,326.88,417.57,24.84,9.83;5,371.28,417.57,83.12,9.16;5,371.28,429.09,52.72,9.16;5,122.64,417.57,28.96,9.16;5,180.96,426.01,12.83,16.36;5,295.20,427.52,6.59,10.91;5,249.84,419.84,6.59,10.91;5,205.44,427.52,6.59,10.91;5,185.52,418.50,3.48,6.69;5,309.12,430.74,1.93,6.69;5,268.80,440.10,1.93,6.69;5,285.84,423.06,1.93,6.69;5,243.84,423.06,3.48,6.69;5,201.84,430.74,1.93,6.69;5,240.24,436.40,29.33,11.54;5,258.48,419.36,27.36,11.54;5,223.20,419.36,22.10,11.54;5,185.52,442.30,3.48,6.33;5,311.52,427.52,4.00,10.91;5,288.24,419.84,4.00,10.91;5,218.64,419.84,4.00,10.91;5,213.60,427.52,4.00,10.91;5,303.12,427.04,5.93,11.54;5,194.16,427.04,7.57,11.54;5,317.28,428.85,2.52,9.16;5,179.28,461.73,17.76,9.16;5,267.12,472.98,1.93,6.69;5,282.96,456.18,1.93,6.69;5,249.84,456.18,1.93,6.69;5,206.40,463.62,1.93,6.69;5,238.56,469.46,29.10,11.31;5,263.76,452.66,19.32,11.31;5,222.72,452.66,38.38,11.31;5,211.68,460.57,6.46,10.69;5,200.40,460.10,5.81,11.31;5,326.88,441.09,26.76,9.83;5,371.28,441.09,84.48,9.16;5,371.28,452.61,52.00,9.16;5,122.64,483.09,36.00,9.16;5,122.64,494.61,21.12,9.16;5,180.96,491.29,12.83,16.36;5,251.28,505.66,3.82,6.33;5,205.44,492.80,6.59,10.91;5,185.52,483.78,3.48,6.69;5,257.28,505.38,3.48,6.69;5,239.76,505.38,9.00,6.69;5,248.64,488.34,3.48,6.69;5,201.84,496.02,1.93,6.69;5,219.12,501.68,22.10,11.54;5,228.00,484.64,22.10,11.54;5,185.52,507.58,3.48,6.33;5,262.32,492.80,4.00,10.91;5,213.60,492.80,4.00,10.91;5,194.16,492.32,7.57,11.54;5,326.88,483.09,136.88,9.83;5,371.28,494.61,71.04,9.16;5,371.28,506.13,73.84,9.16"><head></head><label></label><figDesc>retrieval status value of the topk documents in list i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,70.80,300.69,453.60,115.61"><head>Table 1</head><label>1</label><figDesc>The results of our experiments are shown in table 1.</figDesc><table coords="2,146.64,335.09,302.08,57.56"><row><cell>query construction</cell><cell cols="2">mean average precision average recall</cell></row><row><cell>standard</cell><cell>0.3373</cell><cell>0.6731</cell></row><row><cell>spans</cell><cell>0.2880</cell><cell>0.3632</cell></row><row><cell>standard + spans</cell><cell>0.3515</cell><cell>0.6523</cell></row><row><cell>standard + spans + topic analyzer</cell><cell>0.3822</cell><cell>0.6920</cell></row></table><note coords="2,170.40,406.61,287.20,9.69"><p>-optimised query construction tested with the CLEF 2003 training data</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,70.80,703.89,453.36,20.68"><head>Table 4</head><label>4</label><figDesc></figDesc><table coords="3,151.20,84.29,293.03,293.00"><row><cell>stemming algorithm</cell><cell cols="2">mean average precision average recall</cell></row><row><cell>none</cell><cell>0.3114</cell><cell>0.6122</cell></row><row><cell>plain german decompounder</cell><cell>0.3751 (+20.5%)</cell><cell>0.7789 (+27.2%)</cell></row><row><cell>plain german decompounder</cell><cell>0.4173 (+34.0%)</cell><cell>0.7950 (+29.9%)</cell></row><row><cell>+ overstemming</cell><cell></cell><cell></cell></row><row><cell>plain german decompounder</cell><cell>0.4089 (+31.3%)</cell><cell>0.7794 (+27.3%)</cell></row><row><cell>+ remove flexion</cell><cell></cell><cell></cell></row><row><cell>+ overstemming</cell><cell></cell><cell></cell></row><row><cell>snowball german2</cell><cell>0.3822 (+22.7%)</cell><cell>0.6920 (+13.0%)</cell></row><row><cell cols="3">Table 2 -stemming algorithms tested with the CLEF 2003 training data</cell></row><row><cell>stemming algorithm</cell><cell cols="2">mean average precision average recall</cell></row><row><cell>none</cell><cell>0.2741</cell><cell>0.5526</cell></row><row><cell>plain german decompounder</cell><cell>0.3440 (+25.5%)</cell><cell>0.7054 (+27.7%)</cell></row><row><cell>+ overstemming</cell><cell></cell><cell></cell></row><row><cell>snowball german2</cell><cell>0.3444 (+25.6%)</cell><cell>0.6302 (+14.0%)</cell></row><row><cell cols="3">Table 3 -stemming algorithms tested with the CLEF 2004 training data</cell></row><row><cell>stemming algorithm</cell><cell cols="2">mean average precision average recall</cell></row><row><cell>none</cell><cell>0.2971</cell><cell>0.5966</cell></row><row><cell>plain german decompounder</cell><cell>0.4048 (+36.3%)</cell><cell>0.7923 (+32.8%)</cell></row><row><cell>+ overstemming</cell><cell></cell><cell></cell></row><row><cell>snowball german2</cell><cell>0.3811 (+28.3%)</cell><cell>0.6674 (+11.9%)</cell></row></table><note coords="2,508.46,703.89,15.69,9.16;2,70.80,715.41,310.44,9.16;3,186.24,391.25,255.28,9.69"><p>.9% (2004), 14.9% (2003) and 18.7% (2005) compared to the snowball algorithm. -stemming algorithms tested with the CLEF 2005 training data</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,108.24,84.21,401.88,92.33"><head>Table 5 -</head><label>5</label><figDesc>2 were similar. # docs for PRF min term occurrences iterations mean average precision average recall pseudo relevance feedback (PRF) tested with the CLEF 2005 training data</figDesc><table coords="4,133.68,119.73,353.51,33.16"><row><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.3811</cell><cell>0.6674</cell></row><row><cell>10</cell><cell>3</cell><cell>1</cell><cell>0.4556 (+19.5%)</cell><cell>0.8091 (+21.2%)</cell></row><row><cell>10</cell><cell>3</cell><cell>2</cell><cell>0.4603 (+20.8%)</cell><cell>0.8154 (+22.2%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,70.80,555.65,453.68,150.92"><head>Table 6</head><label>6</label><figDesc>lists the results of our four clustering configurations by testing each of them with the CLEF training data from 2005. Clustering is compared with our baseline results (first row) and the simple PRF procedure (second row). The value "10 + x" in the first column of the table means, that the top k = 10 documents from the initial retrieval were taken and we added those documents returned from clustering, which were not already in this set of 10 documents, i.e. that the number of documents for PRF can vary between 10 and 20.</figDesc><table coords="4,102.24,625.01,389.27,81.56"><row><cell cols="4"># docs for PRF # terms for PRF clustered obj. mean average precision</cell><cell>average recall</cell></row><row><cell>0</cell><cell>0</cell><cell></cell><cell>0.3811</cell><cell>0.6674</cell></row><row><cell>10</cell><cell>0</cell><cell></cell><cell>0.4603 (+20.8%)</cell><cell>0.8154 (+22.2%)</cell></row><row><cell>0</cell><cell>20</cell><cell>terms (b)</cell><cell>0.4408 (+15.7%)</cell><cell>0.8024 (+20.2%)</cell></row><row><cell>10</cell><cell>20</cell><cell>terms (d)</cell><cell>0.4394 (+15.3%)</cell><cell>0.7968 (+19.4%)</cell></row><row><cell>10</cell><cell>Unlimited</cell><cell>docs (a)</cell><cell>0.4603 (+20.8%)</cell><cell>0.7949 (+19.1%)</cell></row><row><cell>10 + x</cell><cell>Unlimited</cell><cell>docs (c)</cell><cell>0.4726 (+24.0%)</cell><cell>0.8225 (+23.2%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,103.20,720.53,388.48,9.69"><head>Table 6</head><label>6</label><figDesc></figDesc><table /><note coords="4,136.08,720.53,355.60,9.69"><p>-comparison of PRF with/without clustering algorithms on the CLEF 2005 training data</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="5,70.80,529.65,454.88,197.08"><head>Table 7 -</head><label>7</label><figDesc>fusion operators based on RSVWe tested the merging strategies with the two results lists obtained by the two different stemming algorithms of table 4 from section 2.2. All the tested fusion operators improved the precision of our primary results as can be seen in table<ref type="bibr" coords="5,203.52,575.49,3.78,9.16" target="#b7">8</ref>. Furthermore, one can observe that zscore performed best and norm RSV performed almost as good as zscore. Another interesting observation is that the simple round robin approach also achieved very good results. The only fusion operator that performed poorly compared to the others was raw score.</figDesc><table coords="5,182.40,633.17,230.32,93.56"><row><cell cols="3">fusion operator mean average precision average recall</cell></row><row><cell>round robin</cell><cell>0.4296</cell><cell>0.7875</cell></row><row><cell>raw score</cell><cell>0.4070</cell><cell>0.7938</cell></row><row><cell>sum RSV</cell><cell>0.4267</cell><cell>0.7938</cell></row><row><cell>norm max</cell><cell>0.4256</cell><cell>0.7867</cell></row><row><cell>norm RSV</cell><cell>0.4311</cell><cell>0.8057</cell></row><row><cell>norm by topk</cell><cell>0.4297</cell><cell>0.7878</cell></row><row><cell>zscore</cell><cell>0.4324</cell><cell>0.8050</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="5,163.44,740.69,268.24,9.69"><head>Table 8 -</head><label>8</label><figDesc>fusion operators tested with the CLEF 2005 training data 5 Results of submitted runsIn this section we present the results of our official experiments. The results for the DomainSpecific Monolingual German task are shown in table9and those for the DomainSpecific Monolingual English task are illustrated in table<ref type="bibr" coords="7,145.68,323.49,8.40,9.16" target="#b9">10</ref>. In each row of the tables the value of the best performing configuration is highlighted bold and the worst performing configuration italic.</figDesc><table coords="7,135.36,358.13,324.48,345.69"><row><cell>topic no.</cell><cell cols="4">TUCMIgirtde1 TUCMIgirtde2 TUCMIgirtde3 TUCMIgirtde4</cell></row><row><cell>151</cell><cell>0.5735</cell><cell>0.5696</cell><cell>0.5726</cell><cell>0.7766</cell></row><row><cell>152</cell><cell>0.6139</cell><cell>0.4973</cell><cell>0.7000</cell><cell>0.7657</cell></row><row><cell>153</cell><cell>0.7631</cell><cell>0.7734</cell><cell>0.7631</cell><cell>0.7169</cell></row><row><cell>154</cell><cell>0.6928</cell><cell>0.7061</cell><cell>0.6784</cell><cell>0.8012</cell></row><row><cell>155</cell><cell>0.4345</cell><cell>0.4875</cell><cell>0.3405</cell><cell>0.4926</cell></row><row><cell>156</cell><cell>0.6792</cell><cell>0.7088</cell><cell>0.6901</cell><cell>0.6911</cell></row><row><cell>157</cell><cell>0.4380</cell><cell>0.3876</cell><cell>0.4364</cell><cell>0.5263</cell></row><row><cell>158</cell><cell>0.5201</cell><cell>0.5731</cell><cell>0.5320</cell><cell>0.4022</cell></row><row><cell>159</cell><cell>0.6628</cell><cell>0.6344</cell><cell>0.6516</cell><cell>0.6226</cell></row><row><cell>160</cell><cell>0.5431</cell><cell>0.6343</cell><cell>0.5809</cell><cell>0.6261</cell></row><row><cell>161</cell><cell>0.7532</cell><cell>0.7678</cell><cell>0.7569</cell><cell>0.6954</cell></row><row><cell>162</cell><cell>0.5232</cell><cell>0.5312</cell><cell>0.5082</cell><cell>0.6260</cell></row><row><cell>163</cell><cell>0.2978</cell><cell>0.3061</cell><cell>0.2978</cell><cell>0.0816</cell></row><row><cell>164</cell><cell>0.0067</cell><cell>0.0028</cell><cell>0.0060</cell><cell>0.0551</cell></row><row><cell>165</cell><cell>0.8447</cell><cell>0.8573</cell><cell>0.8771</cell><cell>0.8938</cell></row><row><cell>166</cell><cell>0.5773</cell><cell>0.6009</cell><cell>0.5917</cell><cell>0.6184</cell></row><row><cell>167</cell><cell>0.5025</cell><cell>0.5154</cell><cell>0.4986</cell><cell>0.4845</cell></row><row><cell>168</cell><cell>0.4517</cell><cell>0.5130</cell><cell>0.4636</cell><cell>0.4842</cell></row><row><cell>169</cell><cell>0.1584</cell><cell>0.1539</cell><cell>0.0906</cell><cell>0.1957</cell></row><row><cell>170</cell><cell>0.5882</cell><cell>0.6004</cell><cell>0.5931</cell><cell>0.6154</cell></row><row><cell>171</cell><cell>0.4377</cell><cell>0.4584</cell><cell>0.4383</cell><cell>0.5699</cell></row><row><cell>172</cell><cell>0.5005</cell><cell>0.4388</cell><cell>0.5023</cell><cell>0.5697</cell></row><row><cell>173</cell><cell>0.5981</cell><cell>0.5859</cell><cell>0.5634</cell><cell>0.6630</cell></row><row><cell>174</cell><cell>0.4873</cell><cell>0.4802</cell><cell>0.4861</cell><cell>0.4796</cell></row><row><cell>175</cell><cell>0.1571</cell><cell>0.2212</cell><cell>0.2202</cell><cell>0.1810</cell></row><row><cell>aver. prec.</cell><cell>0.5122</cell><cell>0.5202</cell><cell>0.5136</cell><cell>0.5454</cell></row><row><cell># best topics</cell><cell>2</cell><cell>9</cell><cell>0</cell><cell>14</cell></row><row><cell># worst topics</cell><cell>8</cell><cell>5</cell><cell>5</cell><cell>7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="7,90.72,717.65,413.76,9.69"><head>Table 9 -</head><label>9</label><figDesc>topic by topic precision of submitted runs for the DomainSpecific Monolingual German task</figDesc><table coords="8,135.36,72.77,324.48,345.69"><row><cell>topic no.</cell><cell cols="4">TUCMIgirten1 TUCMIgirten2 TUCMIgirten3 TUCMIgirten4</cell></row><row><cell>151</cell><cell>0.5331</cell><cell>0.5438</cell><cell>0.5301</cell><cell>0.5592</cell></row><row><cell>152</cell><cell>0.5817</cell><cell>0.5919</cell><cell>0.5948</cell><cell>0.5760</cell></row><row><cell>153</cell><cell>0.4269</cell><cell>0.4660</cell><cell>0.4269</cell><cell>0.4166</cell></row><row><cell>154</cell><cell>0.7248</cell><cell>0.7148</cell><cell>0.7148</cell><cell>0.7237</cell></row><row><cell>155</cell><cell>0.4936</cell><cell>0.4828</cell><cell>0.4624</cell><cell>0.4936</cell></row><row><cell>156</cell><cell>0.5248</cell><cell>0.5368</cell><cell>0.5254</cell><cell>0.5237</cell></row><row><cell>157</cell><cell>0.1952</cell><cell>0.1361</cell><cell>0.2038</cell><cell>0.2100</cell></row><row><cell>158</cell><cell>0.2468</cell><cell>0.1731</cell><cell>0.1369</cell><cell>0.2944</cell></row><row><cell>159</cell><cell>0.7857</cell><cell>0.7675</cell><cell>0.7851</cell><cell>0.7824</cell></row><row><cell>160</cell><cell>0.1626</cell><cell>0.2807</cell><cell>0.2076</cell><cell>0.1395</cell></row><row><cell>161</cell><cell>0.5146</cell><cell>0.5102</cell><cell>0.5132</cell><cell>0.5182</cell></row><row><cell>162</cell><cell>0.2902</cell><cell>0.2590</cell><cell>0.2816</cell><cell>0.2805</cell></row><row><cell>163</cell><cell>0.1485</cell><cell>0.1634</cell><cell>0.1559</cell><cell>0.1435</cell></row><row><cell>164</cell><cell>0.0273</cell><cell>0.0251</cell><cell>0.0262</cell><cell>0.0276</cell></row><row><cell>165</cell><cell>0.5431</cell><cell>0.5313</cell><cell>0.5158</cell><cell>0.5585</cell></row><row><cell>166</cell><cell>0.4630</cell><cell>0.4757</cell><cell>0.4457</cell><cell>0.4703</cell></row><row><cell>167</cell><cell>0.0634</cell><cell>0.0604</cell><cell>0.0792</cell><cell>0.0714</cell></row><row><cell>168</cell><cell>0.1224</cell><cell>0.0949</cell><cell>0.1075</cell><cell>0.1238</cell></row><row><cell>169</cell><cell>0.1162</cell><cell>0.1719</cell><cell>0.1033</cell><cell>0.1159</cell></row><row><cell>170</cell><cell>0.2547</cell><cell>0.2455</cell><cell>0.2547</cell><cell>0.2454</cell></row><row><cell>171</cell><cell>0.4122</cell><cell>0.4309</cell><cell>0.4140</cell><cell>0.4314</cell></row><row><cell>172</cell><cell>0.0792</cell><cell>0.1002</cell><cell>0.0911</cell><cell>0.0835</cell></row><row><cell>173</cell><cell>0.3773</cell><cell>0.4202</cell><cell>0.3810</cell><cell>0.3770</cell></row><row><cell>174</cell><cell>0.5668</cell><cell>0.5511</cell><cell>0.5668</cell><cell>0.5580</cell></row><row><cell>175</cell><cell>0.1199</cell><cell>0.1499</cell><cell>0.1003</cell><cell>0.1197</cell></row><row><cell>aver. prec.</cell><cell>0.3510</cell><cell>0.3553</cell><cell>0.3450</cell><cell>0.3538</cell></row><row><cell># best topics</cell><cell>6</cell><cell>9</cell><cell>4</cell><cell>9</cell></row><row><cell># worst topics</cell><cell>2</cell><cell>9</cell><cell>8</cell><cell>7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="8,70.80,432.29,435.12,41.66"><head>Table 10 -</head><label>10</label><figDesc>topic by topic precision of submitted runs for the DomainSpecific Monolingual English task 6 Discussion of results and conclusions</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_0" coords="8,92.40,690.80,55.79,10.91"><p>Refer ences</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This section provides an overview about the configuration of the runs we submitted. We participated in the DomainSpecific Monolingual tasks for German and English. Our runs are described in detail in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Domainspecific Monolingual Retrieval -Ger man</head><p>We submitted four runs for this task and their individual configurations were as follows:</p><p>(1) TUCMIgirtde1 All of our experiments used the weighting scheme embedded in the Lucene core, which is a tf*idf weighting scheme (see <ref type="bibr" coords="6,168.24,498.93,10.44,9.16" target="#b0">[1]</ref>). With our submitted experiments we hoped to confirm our observations from our experiments with the training data. Mainly, we hoped that the query expansion with clustering would be superior to the query expansion approach without clustering. Moreover, we expected quite good results from our last experiment where the result lists of the two different stemming approaches (see section 2.2) were fused.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Domainspecific Monolingual Retrieval -English</head><p>We also submitted four runs for this task and their individual configurations were as follows.</p><p>(1) TUCMIgirten1 With our submission for this task we also wanted to confirm that the query expansion with clustering performs better than the other query expansion approaches. The last experiment was submitted to test whether the simple query expansion performs better than the iterative 2step method.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,106.08,715.89,224.76,9.16;8,106.08,727.41,220.72,9.16;8,106.08,738.93,96.24,9.16" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="8,299.04,715.89,27.26,9.16">Lucene</title>
		<ptr target="http://lucene.apache.org" />
		<imprint>
			<date type="published" when="2006">19982006. August 10, 2006</date>
			<publisher>The Apache Software Foundation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,106.08,72.69,418.44,9.16;9,106.08,84.21,220.72,9.16;9,106.08,95.73,360.72,9.16" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="9,171.12,72.69,349.10,9.16">Guidelines for Participation in CLEF 2006 AdHoc and DomainSpecific Tracks</title>
		<author>
			<persName coords=""><surname>Clef</surname></persName>
		</author>
		<ptr target="http://www.clefcampaign.org/DELOS/CLEF/Protect/guidelines06.htm" />
		<imprint>
			<date type="published" when="2006-08-10">2006. August 10, 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,106.08,107.25,179.88,9.16;9,106.08,118.77,220.72,9.16;9,106.08,130.29,132.72,9.16" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="9,197.04,107.25,85.10,9.16">The Snowball Project</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Porter</surname></persName>
		</author>
		<ptr target="http://www.snowball.tartarus.org" />
		<imprint>
			<date type="published" when="2001-08-10">2001. August 10, 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,106.08,141.81,205.08,9.16;9,106.08,153.33,220.72,9.16;9,106.08,164.85,198.00,9.16" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="9,202.56,141.81,103.66,9.16">A German Decompounder</title>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Wagner</surname></persName>
		</author>
		<ptr target="http://wwwuser.tuchemnitz.de/~wags/cv/clr.pdf" />
		<imprint>
			<date type="published" when="2005-08-10">2005. August 10, 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,106.08,176.37,418.32,9.16;9,106.08,187.89,258.96,9.16" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="9,360.48,176.37,163.92,9.16;9,106.08,187.89,44.18,9.16">A Comparison of Document Clustering Techniques</title>
		<author>
			<persName coords=""><forename type="first">Michael;</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">George;</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Vipin</surname></persName>
		</author>
		<idno>#00034</idno>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
		<respStmt>
			<orgName>University of Minnesota</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="9,106.08,199.17,418.20,9.16;9,106.08,210.69,347.20,9.16" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,218.16,199.17,91.05,9.16">Clustering Algorithms</title>
		<author>
			<persName coords=""><forename type="first">Edie</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,172.08,210.69,221.83,9.16">Information Retrieval -Data Structures and Algorithms</title>
		<editor>
			<persName><forename type="first">William</forename><forename type="middle">B ;</forename><surname>Frakes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ricardo</forename><surname>Baezayates</surname></persName>
		</editor>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1992">1992. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,106.08,222.21,418.48,9.16;9,106.08,233.73,165.36,9.16" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,202.08,222.21,203.91,9.16">Recent trends in hierarchic document clustering</title>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Willett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,414.96,222.21,109.60,9.16;9,106.08,233.73,52.24,9.16">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">577597</biblScope>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,106.08,245.25,418.32,9.16;9,106.08,256.77,30.00,9.16" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="9,325.44,245.25,120.30,9.16">Modern Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Ricardo;</forename><surname>Baezayates</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ribeironeto</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Berthier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Pearson Addison Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,106.08,268.29,411.84,9.16;9,517.92,266.23,6.60,5.89;9,106.08,279.81,288.96,9.16" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,285.36,268.29,138.88,9.16">Combination of Multiple searches</title>
		<author>
			<persName coords=""><forename type="first">Edward</forename><forename type="middle">A ;</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,432.24,268.29,85.68,9.16;9,517.92,266.23,6.60,5.89;9,106.08,279.81,143.47,9.16">Proceedings of the 2 nd Text Retrieval Conference (TREC2)</title>
		<meeting>the 2 nd Text Retrieval Conference (TREC2)</meeting>
		<imprint>
			<publisher>NIST Special Publication</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page">500215</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,106.08,291.33,418.20,9.16;9,106.08,302.85,183.84,9.16" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,211.92,291.33,308.46,9.16">Data Fusion for Effective European Monolingual Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Jacques</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,106.08,302.85,183.84,9.16">Working Notes for the CLEF 2004 Workshop</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,106.08,314.37,418.20,9.16;9,106.08,325.65,183.84,9.16" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,278.64,314.37,241.74,9.16">Merging Mechanisms in Multilingual Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Wencheng;</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hsinhis</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,106.08,325.65,183.84,9.16">Working Notes for the CLEF 2002 Workshop</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
