<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,122.40,148.63,357.99,15.51">Domain Specific Retrieval: Back to Basics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,269.76,182.13,63.65,9.96"><forename type="first">Ray</forename><forename type="middle">R</forename><surname>Larson</surname></persName>
							<email>ray@sims.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,122.40,148.63,357.99,15.51">Domain Specific Retrieval: Back to Basics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EFC92C288A89187F19ECC1125B76DD37</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval Algorithms</term>
					<term>Performance</term>
					<term>Measurement Cheshire II</term>
					<term>Logistic Regression</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we will describe Berkeley's approach to the Domain Specific (DS) track for CLEF 2006. This year we are not using the tools for thesaurus-based query expansion and de-compounding for German that were developed over the past many years and used very successfully in earlier Berkeley entries in this track. Our intent has been to incorporate those tools into the Cheshire system, but we were unable to complete the development in time for use in the officially submitted runs. This year Berkeley submitted 12 runs, including one for each subtask of the DS track. These include 3 Monolingual runs for English, German, and Russian, 7 Bilingual runs (3 X2EN, 1 X2DE, and 3 X2RU), and 2 Multilingual runs. For many DS sub-tasks our runs were the best performing runs, but sadly they were also the only runs for a number of subtasks. In the sub-tasks where there were other entries, our relative performance was above the mean performance in 2 sub-tasks and just below the mean in another.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper discusses the retrieval methods and evaluation results for Berkeley's participation in the Domain Specific track. Our submitted runs this year are intended to establish a new baseline for comparision in future Domain Specific evaluations for the Cheshire system, and did not use the techniques of thesaurus-based query expansion or German decompounding used in previous years. This year we used only basic probabilistic retrieval methods for DS tasks. We hope, in future years, (assuming that the task will continue in future years) to be able to use those, or refinements of those techniques via the Cheshire II or Cheshire3 systems.</p><p>This year Berkeley submitted 12 runs, including one for each subtask of the DS track. These include 1 Monolingual run for each of English, German, and Russian for a total of 3 Monolingual runs, and 7 Bilingual runs <ref type="bibr" coords="1,207.29,721.53,35.82,9.96">(3 X2EN</ref>, 1 X2DE, and 3 X2RU), and 2 Multilingual runs. This paper first very briefly describes the retrieval methods used, including our blind feedback method for text, which are discussed in greater detail in our ImageCLEF paper. We then describe our submissions for the various DS sub-tasks and the results obtained. Finally we present conclusions and discussion of future approaches to this track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Retrieval Algorithms</head><p>As we have discussed in our other papers for the ImageCLEF and GeoCLEF tracks in this volume, basic form and variables of the Logistic Regression (LR) algorithm used for all of our submissions were originally developed by Cooper, et al. <ref type="bibr" coords="2,277.02,225.81,9.89,9.96" target="#b2">[3]</ref>. To formally the LR method, the goal of the logistic regression method is to define a regression model that will estimate (given a set of training data), for a particular query Q and a particular document D in a collection the value P (R | Q, D), that is, the probability of relevance for that Q and D. This value is then used to rank the documents in the collection which are presented to the user in order of decreasing values of that probability. To avoid invalid probability values, the usual calculation of P (R | Q, D) uses the "log odds" of relevance given a set of S statistics, s i , derived from the query and database, giving a regression formula for estimating the log odds from those statistics:</p><formula xml:id="formula_0" coords="2,235.08,329.90,277.90,30.50">log O(R | Q, D) = b 0 + S i=1 b i s i (1)</formula><p>where b 0 is the intercept term and the b i are the coefficients obtained from the regression analysis of a sample set of queries, a collection and relevance judgements. The final ranking is determined by the conversion of the log odds form to probabilities:</p><formula xml:id="formula_1" coords="2,232.44,413.33,136.40,25.00">P (R | Q, D) = e log O(R|Q,D) 1 + e log O(R|Q,D)</formula><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">TREC2 Logistic Regression Algorithm</head><p>For all of our Domain Specific submissions this year we used a version of the Logistic Regression (LR) algorithm that has been used very successfully in Cross-Language IR by Berkeley researchers for a number of years <ref type="bibr" coords="2,179.40,494.61,12.27,9.96" target="#b0">[1]</ref> and which is also used in our GeoCLEF and Domain Specific submissions.</p><p>For the Domain Specific track we used the Cheshire II information retrieval system implementation of this algorithm. One of the current limitations of this implementation is the lack of decompounding for German documents and query terms in the current system. As noted in our other CLEF notebook papers, the Logistic Regression algorithm used was originally developed by Cooper et al. <ref type="bibr" coords="2,151.38,554.49,10.55,9.96" target="#b1">[2]</ref> for text retrieval from the TREC collections for TREC2. The basic formula is:</p><formula xml:id="formula_2" coords="2,184.08,575.10,233.60,147.36">log O(R|C, Q) = log p(R|C, Q) 1 -p(R|C, Q) = log p(R|C, Q) p(R|C, Q) = c 0 + c 1 * 1 |Q c | + 1 |Qc| i=1 qtf i ql + 35 + c 2 * 1 |Q c | + 1 |Qc| i=1 log tf i cl + 80 -c 3 * 1 |Q c | + 1 |Qc| i=1 log ctf i N t + c 4 * |Q c |</formula><p>where C denotes a document component (i.e., an indexed part of a document which may be the entire document) and Q a query, R is a relevance variable, c k are the k coefficients obtained though the regression analysis.</p><formula xml:id="formula_3" coords="3,90.00,111.33,422.65,61.98">p(R|C, Q) is the probability that document component C is relevant to query Q, p(R|C, Q) the probability that document component C is not relevant to query Q, which is 1.0 - p(R|C, Q) |Q c |</formula><p>More details of this algorithm and the coefficients used with it may be found in our Image-CLEF notebook paper where the same algorithm and coefficients were used. In addition to this primary algorithm we used a version that performs "blind feedback" during the retrieval process. The method used is described in detail in our ImageCLEF notebook paper. Our blind feedback approach uses the 10 top-ranked documents from an initial retrieval using the LR algorithm above, and selects the top 10 terms from the content of those documents, using a version of the Robertson and Sparck Jones probabilistic term relevance weights <ref type="bibr" coords="3,330.39,393.93,9.89,9.96" target="#b4">[5]</ref>. Those ten terms are merged with the original query and new term frequency weights are calculated, and the revised query submitted to obtain the final ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approaches for Domain Specific</head><p>In this section we describe the specific approaches taken for our submitted runs for the Domain Specific track. First we describe the indexing and term extraction methods used, and then the search features we used for the submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Indexing and Term Extraction</head><p>Although the Cheshire II system uses the XML structure of documents and extracts selected portions of the record for indexing and retrieval, for the submitted runs this year we used only a single one of these indexes that contains the entire content of the document.  did not use the Entry Vocabulary Indexes (search term recommender) that were used by Berkeley in previous years (see <ref type="bibr" coords="4,189.99,328.29,10.22,9.96" target="#b3">[4]</ref>), this certainly had an impact on our results, seen in a large drop in average precision when compared to similar runs using these query expansion strategies. We hope to be able to enable and test some of these strategies further using the new retrieval system in time for presentation at the meeting.</p><p>For all indexing we used language-specific stoplists to exclude function words and very common words from the indexing and searching. The German language runs, however, did not use decompounding in the indexing and querying processes to generate simple word forms from compounds (actually we tried, but there was a bug that failed to match any compounds in our runs). This is another aspect of our indexing for this year's Domain Specific task that reduced our results relative to last year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Search Processing</head><p>Searching the Domain Specific collection used Cheshire II scripts to parse the topics and submit the title and description elements from the topics to the "topic" index containing all terms from the documents. For the monolingual search tasks we used the topics in the appropriate language (English, German, or Russian), and for bilingual tasks the topics were translated from the source language to the target language using SYSTRAN (via Babelfish at Altavista.com) or PROMT via the PROMT web interface. We believe that other translation tools provide a more accurate representation of the topics for some languages (like the L&amp;H P.C. translator used in our GeoCLEF entries) but that was not available to us for our official runs for this track this year. Wherever possible we used both of these MT systems and submitted the resulting translated queries as separate runs. However, some translations are only available on one system or the other (e.g., German Rightarrow Russian is only available in PROMT). All of our runs for this track used the TREC2 algorithm as described above with blind feedback using the top 10 terms from the 10 top-ranked documents in the initial retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results for Submitted Runs</head><p>The summary results (as Mean Average Precision) for the submitted bilingual and monolingual runs for both English and German are shown in Table <ref type="table" coords="4,324.67,692.25,3.90,9.96" target="#tab_2">2</ref>, the Recall-Precision curves for these runs are also shown in Figure <ref type="figure" coords="4,198.63,704.25,5.03,9.96" target="#fig_0">1</ref> (for monolingual and multilingual) and Figures <ref type="figure" coords="4,414.36,704.25,5.03,9.96">2</ref> and<ref type="figure" coords="4,441.46,704.25,5.03,9.96">3</ref> (for bilingual). In Figures <ref type="figure" coords="4,138.81,716.25,7.79,9.96" target="#fig_0">1,</ref><ref type="figure" coords="4,150.44,716.25,3.90,9.96">2</ref>, and 3 the names are abbrevated to the letters and numbers of the full name in Table <ref type="table" coords="4,116.86,728.13,5.03,9.96" target="#tab_2">2</ref> describing the languages and translation system used. For example, in Figure <ref type="figure" coords="4,461.45,728.13,5.03,9.96">3</ref> DERU+P corresponds to BERK BI DERU T2FB P in Table <ref type="table" coords="4,314.33,740.13,3.90,9.96" target="#tab_2">2</ref>.  <ref type="table" coords="5,132.94,522.57,5.03,9.96" target="#tab_2">2</ref> shows all of our submitted runs for the Domain Specific track. Precision and recall curves for the runs are shown in Figures <ref type="figure" coords="5,281.98,534.57,5.03,9.96" target="#fig_0">1</ref> and<ref type="figure" coords="5,312.92,534.57,5.03,9.96">2</ref> for the Monolingual and Multilingual and Bilingual tasks respectively. Although Berkeley's results for these tasks are high, compared to the overall average MAP for all participants, not much can be claimed for this since there were very few submissions for the Domain Specific track this year, and for many tasks (Multilingual, Monolingual Russian, Bilingual X⇒Russian, and Bilingual X⇒English) Berkeley was the only group that submitted runs. A few observations concerning translation are worth mentioning. First is that where we have comparable runs using different translations, all of the processing except for the topic translation was identical. Thus, it is obvious that for translations from German to English, Babelfish does a better job, and for English to Russian PROMT does a better job. We could not get PROMT (online version) to successfully translate the Russian topics to English apparently due to invalid character codes in the input topics (which were not detected by Babelfish).</p><p>It is perhaps more interesting to compare our results with last year's result, where Berkeley was also quite successful (see <ref type="bibr" coords="5,221.51,689.97,10.22,9.96" target="#b3">[4]</ref>). This year we see considerable improvement in all tasks when compared to the Berkeley1 submissions for 2005 (which used the same system as this year, but with different algorithms). However, none of this years runs for German Monolingual or Bilingual tasks approached the performance seen for last year's Berkeley2 runs. Because we did no decompounding for German, nor did we do query expansion using EVMs this year, it seems a reasonable assumption  The results of these comparisons with our 2005 Domain Specific results are shown in Table <ref type="table" coords="6,90.00,559.89,3.90,9.96" target="#tab_3">3</ref>. The only exception to the large percentage improvements seen by our best 2006 runs over the Berkeley1 2005 runs is found in Bilingual German⇒Russian. We suspect that there were translation problems for the German topics using PROMT, but we lack sufficient language skills in each language to understand what all these problems were. But we did observe that a large number of German terms were not translated, and that many spurious additional characters appeared in the translated texts. Another difference worth noting is that the Berkeley2 group used the L&amp;H PowerTranslator software for it's English to X translations, while this year we used only Babelfish and PROMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Given the small number of submissions for the Domain Specific track this year, we wonder about the viability of the track for the future. Berkeley's runs this year did not build on the successes of the Berkeley2 group from 2005, but instead worked only to establish a new baseline set of results for future for retrieval processing without query expansion using EVMs or thesaurus information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,105.00,283.89,392.70,9.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Berkeley Domain Specific Monolingual Runs(left) and Multilingual Runs (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,96.12,283.89,410.43,9.96"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Berkeley Domain Specific Bilingual Runs -To English (left) and To German (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,90.00,163.05,385.61,129.66"><head></head><label></label><figDesc>is the number of matching terms between a document component and a query, qtf i is the within-query frequency of the ith matching term, tf i is the within-document frequency of the ith matching term, ctf i is the occurrence frequency in a collection of the ith matching term, ql is query length (i.e., number of terms in a query like |Q| for non-feedback situations), cl is component length (i.e., number of terms in a component), and N</figDesc><table /><note coords="3,98.04,286.46,3.00,6.26;3,106.44,282.45,269.67,9.96"><p>t is collection length (i.e., number of terms in a test collection).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,90.00,590.04,422.69,164.86"><head>Table 1 :</head><label>1</label><figDesc>Cheshire II Indexes for Domain Specific 2006Table1lists the indexes created for the Domain Specific database and the document elements from which the contents of those indexes were extracted. The "Used" column in Table1indicates whether or not a particular index was used in the submitted Domain Specific runs. This year we</figDesc><table coords="3,97.20,590.04,408.62,84.65"><row><cell>Name</cell><cell>Description</cell><cell>Content Tags</cell><cell>Used</cell></row><row><cell>docno</cell><cell>Document ID</cell><cell>DOCNO</cell><cell>no</cell></row><row><cell>title</cell><cell>Article Title</cell><cell>TITLE</cell><cell>no</cell></row><row><cell>topic</cell><cell>All Content Words</cell><cell>DOC</cell><cell>yes</cell></row><row><cell>date</cell><cell>Date</cell><cell>DATE</cell><cell>no</cell></row><row><cell cols="2">geoname Geographic names</cell><cell>GEOGR-AREA, COUNTRY-CODE</cell><cell>no</cell></row><row><cell>subject</cell><cell cols="2">Controlled Vocabulary CONTROLLED-TERM-x, CLASSIFICATION-TEXT-x</cell><cell>no</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,116.40,110.88,369.52,370.73"><head>Table 2 :</head><label>2</label><figDesc>Submitted Domain Specific Runs</figDesc><table coords="6,116.40,110.88,369.52,370.73"><row><cell>Run Name</cell><cell cols="2">Description</cell><cell></cell><cell>translation</cell><cell>MAP</cell></row><row><cell>BERK BI ENDE T2FB B</cell><cell cols="3">Bilingual English⇒German</cell><cell>Babelfish</cell><cell>0.23658</cell></row><row><cell>BERK BI DEEN T2FB B</cell><cell cols="3">Bilingual German⇒English</cell><cell>Babelfish</cell><cell>0.33013*</cell></row><row><cell>BERK BI DEEN T2FB P</cell><cell cols="3">Bilingual German⇒English</cell><cell>PROMT</cell><cell>0.31763</cell></row><row><cell>BERK BI RUEN T2FB B</cell><cell cols="3">Bilingual Russian⇒English</cell><cell>Babelfish</cell><cell>0.32282*</cell></row><row><cell>BERK BI DERU T2FB P</cell><cell cols="4">Bilingual German⇒Russian PROMT</cell><cell>0.10826*</cell></row><row><cell>BERK BI ENRU T2FB B</cell><cell cols="3">Bilingual English⇒Russian</cell><cell>Babelfish</cell><cell>0.11554</cell></row><row><cell>BERK BI ENRU T2FB P</cell><cell cols="3">Bilingual English⇒Russian</cell><cell>PROMT</cell><cell>0.16482**</cell></row><row><cell>BERK MO DE T2FB</cell><cell cols="3">Monolingual German</cell><cell>none</cell><cell>0.39170</cell></row><row><cell>BERK MO EN T2FB</cell><cell cols="3">Monolingual English</cell><cell>none</cell><cell>0.41357</cell></row><row><cell>BERK MO RU T2FB</cell><cell cols="3">Monolingual Russian</cell><cell>none</cell><cell>0.25422**</cell></row><row><cell cols="4">BERK MU DE T2FB B CMBZ Multilingual from German</cell><cell cols="2">PROMT and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Babelfish</cell><cell>0.04674</cell></row><row><cell cols="4">BERK MU EN T2FB B CMBZ Multilingual from English</cell><cell>Babelfish</cell><cell>0.07534**</cell></row><row><cell>Task Description</cell><cell>2005</cell><cell>2005</cell><cell>2006</cell><cell>Pct. Diff</cell><cell>Pct. Diff</cell></row><row><cell></cell><cell>Berk1</cell><cell>Berk2</cell><cell>MAP</cell><cell cols="2">from Berk1 from Berk2</cell></row><row><cell>Bilingual English⇒German</cell><cell cols="3">0.1477 0.4374 0.2366</cell><cell>+60.19</cell><cell>-45.91</cell></row><row><cell>Bilingual German⇒English</cell><cell cols="3">0.2398 0.4803 0.3301</cell><cell>+37.66</cell><cell>-31.27</cell></row><row><cell>Bilingual Russian⇒English</cell><cell>0.2358</cell><cell>-</cell><cell>0.3228</cell><cell>+36.90</cell><cell></cell></row><row><cell cols="4">Bilingual German⇒Russian 0.1717 0.2331 0.1083</cell><cell>-36.92</cell><cell>-53.54</cell></row><row><cell>Bilingual English⇒Russian</cell><cell cols="3">01364 0.1810 0.1648</cell><cell>+20.82</cell><cell>-8.95</cell></row><row><cell>Monolingual German</cell><cell cols="3">0.2314 0.5144 0.3917</cell><cell>+69.27</cell><cell>-23.85</cell></row><row><cell>Monolingual English</cell><cell cols="3">0.3291 0.4818 0.4136</cell><cell>+25.68</cell><cell>-14.16</cell></row><row><cell>Monolingual Russian</cell><cell cols="3">0.2409 0.3038 0.2542</cell><cell>+5.52</cell><cell>-16.33</cell></row><row><cell>Multilingual from German</cell><cell>0.0294</cell><cell>-</cell><cell>0.0467</cell><cell>+58.84</cell><cell>-</cell></row><row><cell>Multilingual from English</cell><cell>0.0346</cell><cell>-</cell><cell>0.0753</cell><cell>+117.63</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,90.00,494.13,352.87,51.72"><head>Table 3 :</head><label>3</label><figDesc>Comparison of MAP with 2005 Berkeley1 and Berkeley2 that the lack of those is what led to this relative worse performance.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>This obviously hurt our comparable overall performance in tasks with submissions from elsewhere. We did, however, see a marked improvement in performance for our specific system (Cheshire II was also used for Berkeley1 last year) with the improvements to our probabilistic retrieval algorithms developed after last year's submissions. We suspect that with the further addition of decompounding for German, and the use of EVMs and thesaurus expansion we can match or exceed the performance of the Berkeley2 runs last year.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="7,105.47,225.93,407.32,9.96;7,105.48,237.81,350.28,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,248.63,225.93,264.16,9.96;7,105.48,237.81,168.89,9.96">Multilingual information retrieval using machine translation, relevance feedback and decompounding</title>
		<author>
			<persName coords=""><forename type="first">Aitao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fredric</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,283.89,237.81,92.84,9.96">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="149" to="182" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.47,257.73,407.32,9.96;7,105.48,269.73,407.25,9.96;7,105.48,281.73,53.80,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,283.19,257.73,229.59,9.96;7,105.48,269.73,195.51,9.96">Full Text Retrieval based on Probabilistic Equations with Coefficients fitted by Logistic Regression</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">S</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,320.70,269.73,159.76,9.96">Text REtrieval Conference (TREC-2)</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.47,301.65,407.20,9.96;7,105.48,313.53,407.11,9.96;7,105.48,325.53,407.43,9.96;7,105.48,337.53,101.53,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,374.80,301.65,137.88,9.96;7,105.48,313.53,106.48,9.96">Probabilistic retrieval based on staged logistic regression</title>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">S</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fredric</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><forename type="middle">P</forename><surname>Dabney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,232.80,313.53,279.79,9.96;7,105.48,325.53,180.21,9.96">15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>Copenhagen, Denmark; New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1992">June 21-24. 1992</date>
			<biblScope unit="page" from="198" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.47,357.45,407.18,9.96;7,105.48,369.33,407.04,9.96;7,105.48,381.33,407.37,9.96;7,105.48,393.21,22.88,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,310.49,357.45,202.17,9.96;7,105.48,369.33,268.79,9.96">Domain-specific CLIR of english, german and russian using fusion and subject metadata for query expansion</title>
		<author>
			<persName coords=""><forename type="first">Vivien</forename><surname>Petras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fredric</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ray</forename><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,394.81,369.33,117.71,9.96;7,105.48,381.33,83.31,9.96">Cross-Language Evaluation Forum: CLEF 2005</title>
		<title level="s" coord="7,305.59,381.33,178.14,9.96">Lecture Notes in Computer Science LNCS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4022</biblScope>
			<biblScope unit="page" from="226" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.47,413.25,407.15,9.96;7,105.48,425.13,328.09,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,281.13,413.25,158.03,9.96">Relevance weighting of search terms</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Sparck</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,450.06,413.25,62.57,9.96;7,105.48,425.13,181.49,9.96">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="page" from="129" to="146" />
			<date type="published" when="1976-06">May-June 1976</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
