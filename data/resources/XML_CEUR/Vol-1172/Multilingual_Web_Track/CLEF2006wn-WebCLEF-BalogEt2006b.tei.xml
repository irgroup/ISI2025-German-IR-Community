<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
				<funder ref="#_nKneQBD">
					<orgName type="full">Netherlands Organisation for Scientific Research (NWO)</orgName>
				</funder>
				<funder ref="#_GUCsqbJ #_VfbWC39 #_Yh6E2bm #_bAmTH3f #_EWNVP73 #_BtJ8ejR #_hYTfvaP #_n9mxFPG #_5z9vnUY #_Vmyv7zw #_NkG4ghY">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_UhA8ve7 #_zmNhmC9">
					<orgName type="full">NWO</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,219.53,182.75,68.03,8.74"><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
							<email>kbalog@science.uva.nl</email>
						</author>
						<author>
							<persName coords="1,307.48,182.75,75.99,8.74"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The University of Amsterdam</orgName>
								<address>
									<postCode>2006</postCode>
									<settlement>WebCLEF</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">ISLA</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">93E61197D7F833514339C3FA47F0F1CF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Managment]: Languages-Query Languages Measurement, Performance, Experimentation Web retrieval, Multilingual retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our aim for our participation in WebCLEF 2006 was to investigate the robustness of information retrieval techniques to crosslingual retrieval, such as compact document representations, and query reformulation techniques. Our focus was on the mixed monolingual task. Apart from the proper preprocessing and transformation of various encodings, we did not apply any language-specific techniques. Instead, the target domain meta field was used in some of our runs. A standard combSUM combination using Min-Max normalization was used to combine runs, based on a separate content and title indexes of documents. We found that the combination is effective only for the human generated topics. Query reformulation techniques can be used to improve retrieval performance, as witnessed by our best scoring configuration, however these techniques are not yet beneficial to all different kinds of topics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The world wide web is a natural setting for cross-lingual information retrieval, since the web content is essentially multilingual. On the other hand, web data is much noisier than traditional collections, eg. newswire or newspaper data, which originated from a single source. Also, the linguistic variety in the collection makes it harder to apply language-dependent processing methods, eg. stemming algorithms. Moreover, the size of the web only allows for methods that scale well.</p><p>We investigate a range of approaches to crosslingual web retrieval using the test suite of the CLEF 2006 WebCLEF track, featuring a stream of known-item topics in various languages. The topics are a mixture of human generated (manually) and automatically generated topics. Our focus is on the mixed monolingual task. Our aim for our participation in WebCLEF 2006 was to investigate the robustness of information retrieval techniques, such as compact document representations (titles or incoming anchor-texts), and query reformulation techniques.</p><p>The remainder of the paper is organized as follows. In Section 2 we describe our retrieval system as well as the specific approaches we applied. In Section 3 we describe the runs that we submitted, while the results of those runs are detailed in Section 4. We conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head><p>Our retrieval system is based on the Lucene engine <ref type="bibr" coords="2,316.26,190.70,9.97,8.74" target="#b3">[4]</ref>.</p><p>For our ranking, we used the default similarity measure of Lucene, i.e., for a collection D, document d and query q containing terms t i :</p><formula xml:id="formula_0" coords="2,180.26,234.42,242.48,26.35">sim(q, d) = t∈q tf t,q • idf t norm q • tf t,d • idf t norm d • coord q,d • weight t ,</formula><p>where tf t,X = freq(t, X),</p><formula xml:id="formula_1" coords="2,250.15,309.07,116.61,22.31">idf t = 1 + log |D| freq(t, D)</formula><p>,</p><formula xml:id="formula_2" coords="2,230.62,338.92,115.11,36.32">norm d = |d|, coord q,d = |q ∩ d| |q| ,<label>and</label></formula><formula xml:id="formula_3" coords="2,237.33,385.17,128.22,22.03">norm q = t∈q tf t,q • idf t 2 .</formula><p>We did not apply any stemming nor did we use a stopword list. We applied case-folding and normalized marked characters to their unmarked counterparts, i.e., mapping á to a, ö to o, ae to ae, î to i, etc. The only language specific processing we did was a transformation of the multiple Russian and Greek encodings into an ASCII transliteration.</p><p>We extracted the full text from the documents, together with the title and anchor fields, and created three separate indexes:</p><p>• Content: Index of the full document text.</p><p>• Title: Index of all &lt;title&gt; fields.</p><p>• Anchors: Index of all incoming anchor-texts.</p><p>We performed three base runs using the separate indexes. We evaluated the base runs using the WebCLEF 2005 topics, and decided to use only the content and title indexes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Run combinations</head><p>We experimented with the combination of content and title runs, using standard combination methods as introduced by Fox and Shaw <ref type="bibr" coords="2,273.69,630.81,9.97,8.74" target="#b0">[1]</ref>: combMAX (take the maximal similarity score of the individual runs); combMIN (take the minimal similarity score of the individual runs); combSUM (take the sum of the similarity scores of the individual runs); combANZ (take the sum of the similarity scores of the individual runs, and divide by the number of non-zero entries); combMNZ (take the sum of the similarity scores of the individual runs, and multiply by the number of non-zero entries); and combMED (take the median similarity score of the individual runs).</p><p>Fox and Shaw <ref type="bibr" coords="2,169.50,702.54,10.52,8.74" target="#b0">[1]</ref> found combSUM to be the best performing combination method. Kamps and de Rijke <ref type="bibr" coords="2,129.89,714.50,10.52,8.74" target="#b1">[2]</ref> conducted extensive experiments with the Fox and Shaw combination rules for nine european languages, and demonstrated that combination can lead into significant improvements. Moreover, they proved that the effectiveness of combining retrieval strategies differs between English and other European languages. In Kamps and de Rijke <ref type="bibr" coords="3,367.29,123.98,9.97,8.74" target="#b1">[2]</ref>, combSUM emerges as the best combination rule, confirming Fox and Shaw's findings.</p><p>Similarity score distributions may differ radically across runs. We apply the combination methods to normalized similarity scores. That is, we follow Lee <ref type="bibr" coords="3,369.80,159.84,10.51,8.74" target="#b2">[3]</ref> and normalize retrieval scores into a [0, 1] range, using the minimal and maximal similarity scores (min-max normalization):</p><formula xml:id="formula_4" coords="3,263.53,191.54,249.48,22.31">s = s -min max -min ,<label>(1)</label></formula><p>where s denotes the original retrieval score, and min (max ) is the minimal (maximal) score over all topics in the run.</p><p>For the WebCLEF 2005 topics the best performance was achieved using the combSUM combination rule, which is in conjunction with the findings in <ref type="bibr" coords="3,336.36,259.97,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="3,350.18,259.97,7.01,8.74" target="#b1">2]</ref>, therefore we used that method for our WebCLEF 2006 submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Query reformulation</head><p>In addition to our run combination experiments, we conducted experiments to measure the effect of phrase and query operations. We tested query-rewrite heuristics using phrases and word n-grams.</p><p>Phrases In this straightforward mechanism we simply add the topic terms as a phrase to the topic. For example, for the topic WC0601907, with title "diana memorial fountain", the query becomes: diana memorial fountain "diana memorial fountain". Our intuition is that rewarding documents that contain the whole topic as a phrase, not only the individual terms, would be beneficial to retrieval performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N-grams</head><p>In our approach every word n-gram from the query is added to the query as a phrase with weight n. This means that longer phrases get bigger boost. Using the previous topic as an example, the query becomes: diana memorial fountain "diana memorial" 2 "memorial fountain" 2 "diana memorial fountain" 3 , where the number in the upper index denotes the weight attached to the phrase (the weight of the individual terms is 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Runs</head><p>We submitted five runs to the mixed monolingual task: Baseline Base run using the content only index.</p><p>Comb Combination of the content and title runs, using the CombSUM method.</p><p>CombMeta The Comb run, using the target domain meta field. We restrict our search to the corresponding domain.</p><p>CombPhrase The CombMeta run, using the Phrase query reformulation technique.</p><p>CombNboost The CombMeta run, using the N-grams query reformulation technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Table <ref type="table" coords="3,118.28,697.82,4.98,8.74" target="#tab_0">1</ref> lists our results in terms of mean reciprocal rank. Runs are listed along the left-hand side, while the labels indicate either all topics (all ) or various subsets: automatically generated (auto)-further subdivided into automatically generated using the unigram generator (auto-u) and automatically generated using the bigram generator (auto-b)-and manual (manual ), which is further subdivided into new manual topics and old manual topics.</p><p>Significance testing was done using the two-tailed Wilcoxon Matched-pair Signed-Ranks Test, where we look for improvements at a significance level of 0.05 ( 1 ), 0.001 ( <ref type="formula" coords="4,428.73,122.40,3.97,6.12">2</ref>), and 0.0001 ( 3 ). Signficant differences noted in Table <ref type="table" coords="4,251.42,135.93,4.98,8.74" target="#tab_0">1</ref> are with respect to the Baseline run. Combination of the content and title runs (Comb) increased performance only for the manual topics. The use of the title tag does not help when the topics are automatically generated. Instead of employing a language detection method, we simply used the target domain meta field. The CombMeta run improved the retrieval performance significantly for all subsets of topics. Our query reformulation techniques show mixed, but promising results. The best overall score was achieved when the topic, as a phrase, was added to the query (CombPhrase). The comparison of CombPhrase vs CombMeta reveals that they achieved similar scores for all subsets of topics, except for the automatic topics using the bigram generator, where the query reformulation technique was clearly beneficial. The n-gram query reformulation technique (CombNboost) further improved the results of the auto-b topics, but hurt accuracy on all other subsets, especially on the manual topics. The CombPhrase run demonstrates that even a very simple query reformulation technique can be used to improve retrieval scores. However, we need to further investigate how to automatically detect whether it is beneficial to use such techniques (and if yes, which technique to apply) for a given a topic.</p><p>Comparing the various subsets of topics, we see that the automatic topics have proven to be more difficult than the manual ones. Also, the new manual topics seem to be more appropriate for known-item search than the old manual topics. There is a clear ranking among the various subsets of topics, and this ranking is independent from the applied methods: man -n man -o auto -u auto -b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Our aim for our participation in WebCLEF 2006 was to investigate the robustness of information retrieval techniques to crosslingual web retrieval. The only language-specific processing we applied was the transformation of various encodings into an ASCII transliteration. We did not apply any stemming nor did we use a stopword list. We indexed the collection by extracting the full text and the title fields from the documents. A standard combSUM combination using Min-Max normalization was used to combine the runs based on the content and title indexes. We found that the combination is effective only for the human generated topics, using the title field did not improve performance when the topics are automatically generated. Significant improvements (+15% MRR) were achieved by using the target domain meta field. We also investigated the effect of query reformulation techniques. We found, that even very simple methods can improve retrieval performance, however these techniques are not yet beneficial to retrieval for all subsets of topics. Although it may be too early to talk about a solved problem, effective and robust web retrieval techniques seem to carry over to the mixed monolingual setting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,110.63,167.26,381.25,91.41"><head>Table 1 :</head><label>1</label><figDesc>Submission results (Mean Reciprocal Rank)</figDesc><table coords="4,110.63,189.47,381.25,69.19"><row><cell>runID</cell><cell>all</cell><cell>auto</cell><cell>auto-u</cell><cell>auto-b</cell><cell cols="2">manual man-n</cell><cell>man-o</cell></row><row><cell>Baseline</cell><cell>0.1694</cell><cell>0.1253</cell><cell>0.1397</cell><cell>0.1110</cell><cell>0.3934</cell><cell>0.4787</cell><cell>0.3391</cell></row><row><cell>Comb</cell><cell>0.1685 1</cell><cell>0.1208 3</cell><cell>0.1394 3</cell><cell>0.1021</cell><cell>0.4112</cell><cell>0.4952</cell><cell>0.3578</cell></row><row><cell>CombMeta</cell><cell>0.1947 3</cell><cell>0.1505 3</cell><cell cols="2">0.1670 3 0.1341 3</cell><cell cols="3">0.4188 3 0.5108 1 0.3603 1</cell></row><row><cell cols="3">CombPhrase 0.2001 3 0.1570 3</cell><cell>0.1639 3</cell><cell>0.1500 3</cell><cell cols="3">0.4190 0.5138 0.3587</cell></row><row><cell cols="2">CombNboost 0.1954 3</cell><cell cols="2">0.1586 3 0.1595 3</cell><cell cols="2">0.1576 3 0.3826</cell><cell>0.4891</cell><cell>0.3148</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Acknowledgments</head><p><rs type="person">Krisztian Balog</rs> was supported by the <rs type="funder">Netherlands Organisation for Scientific Research (NWO)</rs> under project numbers <rs type="grantNumber">220-80-001</rs>, <rs type="grantNumber">600.-065.-120</rs> and <rs type="grantNumber">612.000.106</rs>. Maarten de Rijke was supported by <rs type="funder">NWO</rs> under project numbers <rs type="grantNumber">017.001.190</rs>, <rs type="grantNumber">220-80-001</rs>, <rs type="grantNumber">264-70-050</rs>, <rs type="grantNumber">354-20-005</rs>, <rs type="grantNumber">600.-065.-120</rs>, <rs type="grantNumber">612-13-001</rs>, <rs type="grantNumber">612.000.106</rs>, <rs type="grantNumber">612.066.302</rs>, <rs type="grantNumber">612.069.006</rs>, <rs type="grantNumber">640.001.501</rs>, and <rs type="grantNumber">640.002.501</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_nKneQBD">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_GUCsqbJ">
					<idno type="grant-number">600.-065.-120</idno>
				</org>
				<org type="funding" xml:id="_UhA8ve7">
					<idno type="grant-number">612.000.106</idno>
				</org>
				<org type="funding" xml:id="_zmNhmC9">
					<idno type="grant-number">017.001.190</idno>
				</org>
				<org type="funding" xml:id="_VfbWC39">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_Yh6E2bm">
					<idno type="grant-number">264-70-050</idno>
				</org>
				<org type="funding" xml:id="_bAmTH3f">
					<idno type="grant-number">354-20-005</idno>
				</org>
				<org type="funding" xml:id="_EWNVP73">
					<idno type="grant-number">600.-065.-120</idno>
				</org>
				<org type="funding" xml:id="_BtJ8ejR">
					<idno type="grant-number">612-13-001</idno>
				</org>
				<org type="funding" xml:id="_hYTfvaP">
					<idno type="grant-number">612.000.106</idno>
				</org>
				<org type="funding" xml:id="_n9mxFPG">
					<idno type="grant-number">612.066.302</idno>
				</org>
				<org type="funding" xml:id="_5z9vnUY">
					<idno type="grant-number">612.069.006</idno>
				</org>
				<org type="funding" xml:id="_Vmyv7zw">
					<idno type="grant-number">640.001.501</idno>
				</org>
				<org type="funding" xml:id="_NkG4ghY">
					<idno type="grant-number">640.002.501</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,105.50,224.47,407.50,8.74;5,105.50,236.43,407.50,8.74;5,105.50,248.38,149.30,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,201.19,224.47,145.03,8.74">Combination of multiple searches</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,369.10,224.47,143.91,8.74;5,105.50,236.43,77.47,8.74">The Second Text REtrieval Conference (TREC-2)</title>
		<imprint>
			<publisher>NIST Special Publication</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="500" to="215" />
		</imprint>
		<respStmt>
			<orgName>National Institute for Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.50,268.31,407.50,8.74;5,105.50,280.26,407.51,8.74;5,105.50,292.22,100.80,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,236.54,268.31,276.46,8.74;5,105.50,280.26,100.97,8.74">The effectiveness of combining information retrieval strategies for European languages</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,227.90,280.26,280.09,8.74">Proceedings of the 2004 ACM Symposium on Applied Computing</title>
		<meeting>the 2004 ACM Symposium on Applied Computing</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1073" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.50,312.14,407.51,8.74;5,105.50,324.10,407.51,8.74;5,105.50,336.06,407.50,8.74;5,105.50,348.01,334.17,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,156.81,312.14,335.39,8.74">Combining multiple evidence from different properties of weighting schemes</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1145/215206.215358</idno>
		<ptr target="http://doi.acm.org/10.1145/215206.215358" />
	</analytic>
	<monogr>
		<title level="m" coord="5,105.50,324.10,407.51,8.74;5,105.50,336.06,180.15,8.74">SIGIR &apos;95: Proceedings of the 18th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="180" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.50,367.94,316.79,9.02" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="5,143.26,367.94,110.72,8.74">The Lucene search engine</title>
		<author>
			<persName coords=""><surname>Lucene</surname></persName>
		</author>
		<ptr target="http://lucene.apache.org/" />
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
