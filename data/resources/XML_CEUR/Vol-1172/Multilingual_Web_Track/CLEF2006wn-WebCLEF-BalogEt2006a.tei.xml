<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,180.70,148.86,241.61,15.15">Overview of WebCLEF 2006</title>
				<funder ref="#_6AxJkaH #_AaMM7JX #_vKj992Z #_4fSQBBs #_4UxeAMx #_STp4CmG">
					<orgName type="full">NWO</orgName>
				</funder>
				<funder ref="#_8FEPnb5">
					<orgName type="full">Netherlands Organisation for Scientific Research (NWO)</orgName>
				</funder>
				<funder ref="#_9336a2Q #_6z3Urk6 #_WGGX4VY #_mxVKtkj #_yc8uHcz #_syF2fdw #_BF9fAWJ #_aav6SBk #_8yXzDuh #_ZTvsBEH #_NaZRbNc #_VZwwvzz #_UQMTnD6">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,128.37,182.75,68.03,8.74"><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
							<email>kbalog@science.uva.nl</email>
							<affiliation key="aff0">
								<orgName type="laboratory">ISLA</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,220.79,182.75,63.82,8.74"><forename type="first">Leif</forename><surname>Azzopardi</surname></persName>
							<email>leif.azzopardi@cis.strath.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer and Information Sciences</orgName>
								<orgName type="institution">University of Strathclyde</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,309.00,182.75,54.43,8.74"><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
							<email>kamps@science.uva.nl</email>
							<affiliation key="aff0">
								<orgName type="laboratory">ISLA</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Archive and Information Studies</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,394.17,182.75,75.99,8.74"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">ISLA</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,180.70,148.86,241.61,15.15">Overview of WebCLEF 2006</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9BF869E594BD555AB8AD733FA7982A33</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries Measurement, Performance, Experimentation Web retrieval, Known-item retrieval, Multilingual retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We report on the CLEF 2006 WebCLEF track devoted to crosslingual web retrieval. We provide details about the retrieval tasks, the used topic set, and the results of WebCLEF participants. WebCLEF 2006 used a stream of known-item topics consisting of: (i) manual topics (including a selection of WebCLEF 2005 topics, and a set of new topics) and (ii) automatically generated topics (generated using two techniques). Our main findings are the following. First, the results over all topics show that current CLIR systems are quite effective, retrieving on average the target page in the top few ranks. Second, when we break down the scores over the manually constructed and the generated topics, we see that the manually constructed topics result in higher performance. Third, the resulting scores on automatic topics give, at least, a solid indication of performance, and can hence be an attractive alternative in situations where manual topics are not readily available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The world wide web presents one of the greatest challenges for cross-language information retrieval <ref type="bibr" coords="1,121.63,654.44,9.96,8.74" target="#b4">[5]</ref>. Content on the world wide web is essentially multilingual, and web users are often polyglots. The European web space is a case in point: the majority of European speak at least one language other than their mother-tongue, and the Internet is a frequent reason to use a foreign language <ref type="bibr" coords="1,131.52,690.30,9.96,8.74" target="#b3">[4]</ref>. The challenge of crosslingual web retrieval is addressed, head-on, by WebCLEF <ref type="bibr" coords="1,498.99,690.30,9.97,8.74" target="#b8">[9]</ref>.</p><p>The crosslingual web retrieval track uses an extensive collection of spidered web sites of European governments, baptized EuroGOV <ref type="bibr" coords="1,269.81,714.21,9.97,8.74" target="#b6">[7]</ref>. The retrieval task at WebCLEF 2006 is based on a stream of known-item topics in a range of languages. This task, which is labeled mixed-monolingual retrieval, was pioneered at the WebCLEF 2005 <ref type="bibr" coords="1,297.12,738.12,9.97,8.74" target="#b7">[8]</ref>. Participants of WebCLEF 2005 expressed the wish to be able to iron out issues with the systems they built during last year's campaign, since for many it was their first attempt at web IR with lots of languages, encoding issues, different formats, and noisy data. The continuation of this known-item retrieval task at WebCLEF 2006 allows veteran participants to take stock and make meaningful comparisons of their results over years. To facilitate this, we decided to include a selection of WebCLEF 2005 topics in the topic set (also available for training purposes), as well as a set of new known-item topics. Furthermore, we decided to experiment with the automatic generation of known-item topics <ref type="bibr" coords="2,415.12,183.75,9.96,8.74" target="#b1">[2]</ref>. By contrasting the human topics with the automatically generated topics, we hope to gain insight in the validity of the automatically generated topics, especially in a multilingual environment. Our main findings are the following. First, the results over all topics show that current CLIR systems are quite effective, retrieving on average the target page in the top few ranks. Second, when we break down the scores over the manually constructed and the generated topics, we see that the manually constructed topics result in higher performance. Third, the resulting scores on automatic topics give, at least, a solid indication of performance, and can hence be an attractive alternative in situations where manual topics are not readily available.</p><p>The remainder of this paper is structured as follows. Section 2 gives the details of the method for automatically generating known-item topics. Next, in Section 3, we discuss the details of the track set-up: the retrieval task, document collection, and topics of request. Section 4 reports the runs submitted by participants, and Section 5 discusses the results of the official submissions. Finallly, in Section 6 we discuss our findings and draw some initial conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Automatic Topic Construction</head><p>This year we experimented with the automatic generation of known-item topics. The main advantage of automatically generating queries is that for any given test collection numerous queries can be produced at minimal cost <ref type="bibr" coords="2,235.35,417.85,9.97,8.74" target="#b1">[2]</ref>. In the WebCLEF setting this could be especially rewarding, since manual development of topics on all the different languages would require human resources we do not dispose of.</p><p>To create simulated queries, we model the following behavior of a known-item searcher. We assume that the user wants to retrieve a particular document that they have seen before in the collection, because some need has arisen calling for this document. The user then tries to reconstruct or recall terms, phrases and features that would help identify this document, which they pose as a query.</p><p>The basic algorithm we use for generating queries was introduced by Azzopardi and de Rijke <ref type="bibr" coords="2,501.76,513.49,9.97,8.74" target="#b1">[2]</ref>, and is based on an abstraction of the actual querying process, as follows:</p><p>• Initialize an empty query q = {} • Select the document d to be the known-item with probability p(d)</p><p>• Select the query length k with probability p(k)</p><p>• Repeat k times:</p><p>-Select a term t from the document model of d with probability p(t|θ d ) -Add t to the query q.</p><p>• Record d and q to define the known-item/query pair.</p><p>By repeatedly performing this algorithm we can create many queries. Before doing so, the probability distributions p(d), p(k) and p(t|θ d ) need to be defined. By using different probability distributions we can characterize different types and styles of queries that a user may submit.</p><p>Azzopardi and de Rijke <ref type="bibr" coords="2,214.30,702.78,10.51,8.74" target="#b1">[2]</ref> conducted experiments using various term sampling methods in order to simulate different styles of queries. In one case, they set the probability of selecting a term from the document model to a uniform distribution, where p(t|θ d ) was set to zero for all terms that did not occur in the document, whilst all other terms were assigned an equal probability. Compared to other types of queries, they found that using a uniform selection produced queries which were the most similar to real queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unigram</head><p>In the construction of a set of queries for the EuroGOV collection, we also use uniform sampling, but include query noise and then phrase extraction into the process to create more realistic queries. To include some noise to the process of generating a query, our model for sampling query terms is broken into two parts: sampling from the document (in our case uniformly) and sampling terms at random (i.e., noise). Figure <ref type="figure" coords="3,303.90,516.76,4.98,8.74">1</ref> shows the sampling process; where a term is drawn from the unigram document model with some probability λ, or it is drawn from the noise model with probability 1 -λ . Consequently, as λ tends to zero, we assume that the user has almost perfect recollection of the original document. Conversely, as λ tends to one, we assume that the user's memory of the document degrades to the point that they know the document exists but they have no idea as to the terms other than randomly selecting terms (from the collection). We used λ = 0.1 for topic generation. This model was used for our first setting, called auto-uni.</p><p>We further extended the process of sampling terms from a document. Once a term has been sampled from the document, we assume that there is some probability that the subsequent term will be drawn. For instance given the sentence, ". . . Information Retrieval Agent . . . ", if the first term sampled is "Retrieval", then the subsequent term selected will be "Agent". This was included to provide some notion of phrase extraction to the process of selecting query terms. The process is depicted in Figure <ref type="figure" coords="3,217.14,660.22,3.88,8.74">2</ref>. This model was used for our second setting, called auto-bi, where we either add the subsequent term with p = 0.7, or sample a new term independently from the document with p = 0.3.</p><p>We indexed each domain within the EuroGOV collection separately, using the Lemur language modeling toolkit <ref type="bibr" coords="3,163.74,708.04,9.97,8.74" target="#b5">[6]</ref>. We experimented with two different styles of queries, and for each of them we generated 30 queries per top level domain. For both settings, the query length k was selected using a Poisson distribution where the mean was set to 3. Two restrictions were placed on sampled query terms: (i) the size of a term needed to be greater than 3, and (ii) the terms should not contain any numeric characters. Finally, the document prior p(d) was also set to a uniform distribution.</p><p>Our initial results motivate further work with more sophisticated query generators. A natural next step would be to take structure and document priors into account.</p><p>3 The WebCLEF 2006 Tasks</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Document Collection</head><p>For the purposes of the WebCLEF track the EuroGOV corpus was developed <ref type="bibr" coords="4,443.52,311.39,9.96,8.74" target="#b6">[7]</ref>. EuroGOV is a crawl of European government-related sites, where collection building is less restricted by intellectual property rights. It is a multilingual web corpus, which contains over 3.5 million pages from 27 primary domains, covering over twenty languages. There is no single language that dominates the corpus, and its linguistic diversity provides a natural setting for multilingual web search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Topics</head><p>The topic set for WebCLEF 2006 consists of a stream of 1,940 known-item topics, consisting of both manual and automatically generated topics. As is shown in Table <ref type="table" coords="4,372.50,429.40,3.88,8.74" target="#tab_0">1</ref>, 195 manual topics were reused from WebCLEF 2005, and 125 new manual topics were constructed. For the generated topics, we focused on 27 primary domains and generated 30 topics using the auto-uni query generation, and another 30 topics using the auto-bi query generation (see Section 2 for details), amounting to 810 automatic topics for each of the methods.</p><p>After the runs had been evaluated, we observed that the performance achieved on the automatic topics are frequently very poor. We found that in several cases none of the participants found any relevant page within the top 50 returned results. These are often mixed-language topics, a result of language diversity within a primary domain, or they proved to be too hard for any other reason.</p><p>In our post-submission analysis we decided to zoom in on a subset of topics and removed any topics that did not meet the following criterion: "whether any participant found the targetted page within the top 50." Table <ref type="table" coords="4,231.15,572.86,4.98,8.74" target="#tab_0">1</ref> presents the number of original, deleted and remaining topics. 820 out of the 1, 940 original topics were removed. Most of the removed topics are automatic (803), but there are also a few manual ones (17). The remaining topic set contains 1,120 topics, and is referred as the new topic set.</p><p>We decided to re-evaluate the submitted runs using this new topic set. Since it is a subset of the original topic collection, participants did not have to make any efforts. Submitted runs were re-evaluated using a restricted version of the (original) qrels that correspond to the new topic set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Retrieval Task</head><p>WebCLEF 2006 saw the continuation of the Mixed Monolingual task of WebCLEF 2005 <ref type="bibr" coords="4,478.02,690.87,9.97,8.74" target="#b7">[8]</ref>. The mixed-monolingual task is meant to simulate a user searching for a known-item page in a European language. The mixed-monolingual task uses the title field of the topics to create a set of monolingual known-item topics.</p><p>Our emphasis this year is on the mixed monolingual task. The manual topics in the topic set contain an English translation of the query. Hence, using only the manual topics, experiments with a Multilingual task are possible. The multilingual task is meant to simulate a user looking for a certain known-item page in a particular European language. The user, however, uses English to formulate her query. The multilingual task used the English translations of the original topic statements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Submission</head><p>For each task, participating teams were allowed to submit up to 5 runs. The results had to be submitted in TREC format. For each topic a ranked list of no more than 50 results should be returned. For each topic at least 1 result must be returned. Participants were also asked to provide a list of the metadata fields they used, and a brief description of the methods and techniques employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Evaluation</head><p>The WebCLEF 2006 topics were known-item topics where a unique URL is targetted (unless there are page-duplicates in the collection, or near duplicates). Hence, we opted for a precision measure. The main metric used for evaluation was mean reciprocal rank (MRR). The reciprocal rank is, indeed, calculated as 1 divided by the rank at which the (first) relevant page is found. The mean reciprocal rank is obtained by, indeed, averaging the reciprocal ranks of a set of topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Submitted Runs</head><p>There were 8 participating teams that managed to submit official runs to WebCLEF 2006: buap; depok; hildesheim; hummingbird; isla; reina; rfia; and ucm. For details of the respective retrieval approaches to crosslingual web retrieval, we refer to the participants papers.</p><p>Table <ref type="table" coords="5,132.09,450.62,4.98,8.74" target="#tab_1">2</ref> lists the runs submitted to WebCLEF 2006: 35 for the mixed-monolingual task, and 1 for the bilingual task. We also indicate the use of topic metadata, either the topic's language (TL), the targetted page's language (PL), or the targetted page's domain (PD). The mean reciprocal rank (MRR) is reported over both the original and the new topic set. The official results of WebCLEF 2006 were based on the original topic set containing 1,940 topics. As detailed in Section 3.2 above, we have pruned the topic set by removing topics for which none of the participants retrieved the target page, resulting in 1,120 topics. In Appendix A, we provide scores for various breakdowns for both the original topic set and the new topic set.</p><p>The task description stated that for each topic, at least 1 result must be returned. However, several runs did not fulfill this condition. The best results for each team were achieved using 1 or more metadata fields. Knowledge of the page's primary domain (shown in the PD column in Table <ref type="table" coords="5,117.40,582.13,4.43,8.74" target="#tab_1">2</ref>) seemed moderately effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>This year our focus is on the Mixed-Monolingual task. A large number of topics were made available, consisting old manual, new manual, and automatically generated topics. Evaluation results showed that the performance achieved on the automatic topics are frequently very poor, and we made a new topic set where we removed topics for which none of the participants found any relevant page within the top 50 returned results. All the results presented in this section correspond to the new topic set consisting of 1,120 topics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Mixed-Monolingual</head><p>We look at each team's best scoring run, independent of whether it was a baseline run or used some of the topic metadata. Table <ref type="table" coords="7,241.89,454.23,4.98,8.74" target="#tab_2">3</ref> presents the scores of the participating teams. We report the results over the whole new qrel set (all ), and over the automatic and manual subsets of topics. What is striking is that the automatic topics proved to be more difficult than manual ones. This may be due in part to the fact that the manual topics cover 11 languages, but the generated topics cover all 27 domains in EuroGOV including the more difficult domains and languages. Another important factor may be the imperfections in the generated topics. Apart from the lower scores, the auto topics also dominate the manual topics in number. Therefore we also used the average of the auto and manual scores for ranking participants. Defining an overall ranking of teams is not straightforward, since one team may outperform another on the automatic topics, but perform worse on the manual ones. Still, we observe that participants can be unambiguously assigned into one out of three bins based on either the all or the average scores: the first bin consisting of hummingbird and isla; the second bin of depok, hildesheim, rfia, and ucm; and the third bin of buap and reina.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation on Automatic Topics</head><p>Automatic topics were generated using two different methods, as described in Section 2 above. The participating teams' scores did not show significant variance between the difficulty of topics, using the the two generators. Table <ref type="table" coords="7,244.53,667.88,4.98,8.74" target="#tab_3">4</ref> provides details of the best runs when evaluation is restricted to automatically generated topics only. Note that the scores included in Table <ref type="table" coords="7,279.42,691.79,4.98,8.74" target="#tab_3">4</ref> are measured on the new topic set. Notice, by the way, that there is very little difference between the number of topics within the new topic set for the two automatic topic subsets (auto-uni and auto-bi in Table <ref type="table" coords="7,369.80,715.70,3.88,8.74" target="#tab_0">1</ref>).</p><p>In general, the two query generation methods perform very similarly, and it is system specific whether one type of automatic topics is preferred over the other. Our initial results with auto- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation on Manual Topics</head><p>The manual topics include 183 old and 120 new queries. Old topics were randomly sampled from last year's topics, while new topics were developed by Universidad Complutense de Madrid (UCM) and the track organizers. The new topics cover only languages for which expertise was available: Dutch, English, German, Hungarian, and Spanish.</p><p>In case of the old manual topics we witnessed improvements for all teams that took part in WebCLEF 2005, compared to their last year's scores. Moreover, we found that most participating systems performed better on the new manual topics, compared to the old ones. A possible explanation is the nature of the topics, namely the new topics may be more appropriate for know-item search. Also, language coverage of the new manual topics could play a role.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparing Rankings</head><p>We use Kendall's tau to determine correlations between the rankings of runs resulting from different topic sets. First, we find weak (0.2-0.4) to moderate (0.4-0.6) positive correlations between ranking of runs resulting from automatic topics, and rankings of runs resulting from manual topics, only new manual topics, and only old manual topics; see Table <ref type="table" coords="8,337.36,496.57,3.88,8.74" target="#tab_5">6</ref>. The rankings resulting from the topics generated with the "auto-bi" method are somewhat more correlated with the manual rankings than the ranking resulting from the topics generated with the "auto-uni" method. A very strong positive correlation (0.8-1.0) is found between the ranking of runs obtained using new manual topics and the ranking of runs resulting from using old manual topics. Note that the new topic set we introduced does not affect the relative ranking of systems, thus the correlation scores we reported here are exactly the same for the original and for the new topic sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Multilingual Runs</head><p>Our main focus this year was on the monolingual task, but we allowed submissions for multilingual experiments within the mixed-monolingual setup. The manual topics (both old and new ones) are provided with English titles. The automatically generated topics do not have English translations. We received only one multilingual submission, from the University of Hildesheim. The evaluation of the multilingual run is restricted to the manual topics in the topic set, Table <ref type="table" coords="9,454.54,241.98,4.98,8.74" target="#tab_1">2</ref> summarizes the results of that run. A detailed breakdown over the different topic types is provided in Appendix A (Tables <ref type="table" coords="9,168.65,265.89,4.98,8.74" target="#tab_6">7</ref> and<ref type="table" coords="9,196.32,265.89,4.43,8.74" target="#tab_7">8</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>The world-wide-web is a natural reflection of the language diversity in the world, both in terms of web content as well as in terms of web users. Effective cross-language information retrieval (CLIR) techniques have clear potential for improving the search experience of such users. The WebCLEF track at CLEF 2006 attempts to realize some of this potential, by investigating knownitem retrieval in a multilingual setting. Known-item retrieval is a typical search task on the web <ref type="bibr" coords="9,499.72,368.48,9.96,8.74" target="#b2">[3]</ref>. This year's track focused on mixed monolingual search, in which the topic set is a stream of knownitem topics in various languages. This task was pioneered at WebCLEF 2005 <ref type="bibr" coords="9,424.81,392.39,9.97,8.74" target="#b7">[8]</ref>. The collection is based on the spidered content of web sites of European governments. This year's topic set covered all 27 primary domains in the collection, and contained both manually constructed search topics and automatically generated topics. Our main findings for the mixed-monolingual task are the following. First, the results over all topics show that current CLIR systems are quite effective. These systems retrieve, on average, the target page in the top few ranks. This is particularly impressive when considering that the topics of WebCLEF 2006 covered no less than 27 European primary domains. Second, when we break down the scores over the manually constructed and the generated topics, we see that the manually constructed topics result in higher performance. The manual topics consisted of both a set of newly constructed topics, and a selection of WebCLEF 2005 topics. For veteran participants, we can compare the scores over years, and we see progress for the old manual topics. The new manual topics (which were not available for training) seem to confirm this progress.</p><p>Building a cross-lingual test collection is a complex endeavor. Information retrieval evaluation requires substantial manual effort by topic authors and relevance assessors. In a cross-lingual setting this is particularly difficult, since the language capabilities of topic authors should sufficiently reflect the linguistic diversity of the used document collection. Alternative proposals to traditional topics and relevance assessments, such as term relevance sets, still require human effort (albeit only a fraction) and linguistic capacities by the topic author. <ref type="foot" coords="9,336.90,606.01,3.97,6.12" target="#foot_0">1</ref> This prompted us to experiment with techniques for automatically generating known-item search requests. The automatic construction of known-item topics has been applied earlier in a monolingual setting <ref type="bibr" coords="9,392.25,631.50,9.97,8.74" target="#b1">[2]</ref>. At WebCLEF 2006, two refined versions of the techniques were applied in a mixed-language setting. The general set-up of the the WebCLEF 2006 track can be viewed as an experiment with automatically constructing topics. Recall that the topic set contained both manual and automatic topics. This allows us to critically evaluate the performance on the automatic topics with the manual topics, although the comparison is not necessarily fair given that the manual and automatic subsets of topics differ both in number and in the domains they cover. Our general conclusion on the automatic topics is a mixed one: On the one hand, our results show that there are still some substantial differences between the automatic topics and manual topics, and it is clear that automatic topics cannot simply substitute manual topics. Yet on the other hand, the resulting scores on automatic topics give, at least, a solid indication of performance, and can hence be an attractive alternative in situations where manual topics are not readily available.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,189.49,246.81,224.03,8.74"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: The process of auto-uni query generation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,90.00,108.98,423.00,78.62"><head>Table 1 :</head><label>1</label><figDesc>Number of topics in the original topic set, and in the new topic set where we only retain topics for which at least one of the participants retrieved the relevant page.</figDesc><table coords="4,130.54,142.20,341.92,45.40"><row><cell></cell><cell>all</cell><cell cols="6">auto auto-uni auto-bi manual manual-o manual-n</cell></row><row><cell cols="3">original 1,940 1,620</cell><cell>810</cell><cell>810</cell><cell>320</cell><cell>195</cell><cell>125</cell></row><row><cell>new</cell><cell>1,120</cell><cell>817</cell><cell>415</cell><cell>402</cell><cell>303</cell><cell>183</cell><cell>120</cell></row><row><cell>deleted</cell><cell>820</cell><cell>803</cell><cell>395</cell><cell>408</cell><cell>17</cell><cell>12</cell><cell>5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,90.00,143.30,423.01,572.77"><head>Table 2 :</head><label>2</label><figDesc>Summary of all runs submitted to WebCLEF 2006. The 'metadata usage' columns indicate usage of topic metadata: topic language (TL), page language (PL), page domain (PD). Mean Reciprocal Rank (MRR) scores are reported for both the original and the new topic set. For each team, its best scoring non-metadata run is in italics, and its best scoring metadata run is in boldface. Scores reported at the Multilingual section are based only on the manual topics.</figDesc><table coords="6,346.37,212.79,132.04,8.74"><row><cell>Metadata usage</cell><cell>topics</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,90.00,108.98,423.00,149.21"><head>Table 3 :</head><label>3</label><figDesc>Best overall results using the new topic set. The results are reported on all topics, the automatic and manual subsets of topics, and average is calculated from the auto and manual scores.</figDesc><table coords="7,119.92,152.61,363.17,105.57"><row><cell>Group id</cell><cell>Run</cell><cell>all</cell><cell cols="3">auto manual average</cell></row><row><cell>isla</cell><cell>combPhrase</cell><cell cols="2">0.3464 0.3145</cell><cell>0.4411</cell><cell>0.3778</cell></row><row><cell cols="2">hummingbird humWC06dpcD</cell><cell cols="2">0.2390 0.1396</cell><cell>0.5068</cell><cell>0.3232</cell></row><row><cell>depok</cell><cell>UI2DTF</cell><cell cols="2">0.1589 0.0923</cell><cell>0.3386</cell><cell>0.2154</cell></row><row><cell>rfia</cell><cell>ERFinal</cell><cell cols="2">0.1768 0.1556</cell><cell>0.2431</cell><cell>0.1993</cell></row><row><cell>hildesheim</cell><cell>UHiBase /5-10</cell><cell cols="2">0.1376 0.0685</cell><cell>0.3299</cell><cell>0.1992</cell></row><row><cell>ucm</cell><cell cols="3">webclef-run-all-2006-def-ok-2 0.1505 0.1103</cell><cell>0.2591</cell><cell>0.1847</cell></row><row><cell>buap</cell><cell>allpt40bi</cell><cell cols="2">0.0272 0.0080</cell><cell>0.0790</cell><cell>0.0435</cell></row><row><cell>reina</cell><cell>USAL mix hp</cell><cell cols="2">0.0241 0.0075</cell><cell>0.0689</cell><cell>0.0382</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,155.45,272.31,292.10,126.44"><head>Table 4 :</head><label>4</label><figDesc>Best runs using the automatic topics in the new topic set.</figDesc><table coords="7,157.18,293.97,288.65,104.78"><row><cell>Group id</cell><cell>Run</cell><cell cols="3">auto auto-uni auto-bi</cell></row><row><cell>isla</cell><cell>combNboost</cell><cell>0.3145</cell><cell>0.3114</cell><cell>0.3176</cell></row><row><cell>rfia</cell><cell>ERFinal</cell><cell>0.1556</cell><cell>0.1568</cell><cell>0.1544</cell></row><row><cell cols="2">hummingbird humWC06dpcD</cell><cell>0.1396</cell><cell>0.1408</cell><cell>0.1384</cell></row><row><cell>ucm</cell><cell cols="2">webclef-run-all-2006 0.1103</cell><cell>0.1128</cell><cell>0.1077</cell></row><row><cell>depok</cell><cell>UI2DTF</cell><cell>0.0923</cell><cell>0.1024</cell><cell>0.0819</cell></row><row><cell>hildesheim</cell><cell>UHiBase</cell><cell>0.0685</cell><cell>0.0640</cell><cell>0.0731</cell></row><row><cell>buap</cell><cell>allpt40bi</cell><cell>0.0080</cell><cell>0.0061</cell><cell>0.0099</cell></row><row><cell>reina</cell><cell>USAL mix hp</cell><cell>0.0075</cell><cell>0.0126</cell><cell>0.0022</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,90.00,108.98,423.00,172.27"><head>Table 5 :</head><label>5</label><figDesc>Best manual runs using the new topic set.</figDesc><table coords="8,90.00,130.64,423.00,150.61"><row><cell>Group id</cell><cell>Run</cell><cell>manual</cell><cell>old</cell><cell>new</cell></row><row><cell cols="2">hummingbird humWC06dpcD</cell><cell cols="2">0.5068 0.4936</cell><cell>0.5269</cell></row><row><cell>isla</cell><cell>combPhrase</cell><cell>0.4411</cell><cell cols="2">0.3822 0.5310</cell></row><row><cell>depok</cell><cell>UI2DTF</cell><cell>0.3386</cell><cell>0.2783</cell><cell>0.4307</cell></row><row><cell>hildesheim</cell><cell>UHi1-5-10</cell><cell>0.3299</cell><cell>0.2717</cell><cell>0.4187</cell></row><row><cell>ucm</cell><cell>webclef-run-all-2006-def-ok-2</cell><cell>0.2591</cell><cell>0.2133</cell><cell>0.3289</cell></row><row><cell>rfia</cell><cell>DPSinDiac</cell><cell>0.2431</cell><cell>0.1926</cell><cell>0.3201</cell></row><row><cell>buap</cell><cell>allpt40bi</cell><cell>0.0790</cell><cell>0.0863</cell><cell>0.0679</cell></row><row><cell>reina</cell><cell>USAL mix hp</cell><cell>0.0689</cell><cell>0.0822</cell><cell>0.0488</cell></row><row><cell cols="5">matically generated queries are promising, but still a large portion of these topics are not realistic.</cell></row><row><cell cols="5">This motivates us to work further on more advanced query generation methods.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,103.99,549.26,391.71,200.56"><head>Table 6 :</head><label>6</label><figDesc>Kendall tau rank correlation, two-sided p-value.</figDesc><table coords="8,103.99,570.92,391.71,178.90"><row><cell></cell><cell>all</cell><cell>auto</cell><cell cols="5">auto-uni auto-bi manual manual-new manual-old</cell></row><row><cell>all</cell><cell>τ</cell><cell>0.8182</cell><cell>0.7726</cell><cell>0.8125</cell><cell>0.5935</cell><cell>0.6292</cell><cell>0.5707</cell></row><row><cell></cell><cell>p</cell><cell>0.0000</cell><cell>0.0000</cell><cell>0.0000</cell><cell>0.0000</cell><cell>0.0000</cell><cell>0.0000</cell></row><row><cell>auto</cell><cell>τ</cell><cell></cell><cell>0.9412</cell><cell>0.9688</cell><cell>0.4108</cell><cell>0.4575</cell><cell>0.3945</cell></row><row><cell></cell><cell>p</cell><cell></cell><cell>0.0000</cell><cell>0.0000</cell><cell>0.0006</cell><cell>0.0001</cell><cell>0.0010</cell></row><row><cell>auto-uni</cell><cell>τ</cell><cell></cell><cell></cell><cell>0.9097</cell><cell>0.3717</cell><cell>0.4183</cell><cell>0.3619</cell></row><row><cell></cell><cell>p</cell><cell></cell><cell></cell><cell>0.0000</cell><cell>0.0019</cell><cell>0.0005</cell><cell>0.0025</cell></row><row><cell>auto-bi</cell><cell>τ</cell><cell></cell><cell></cell><cell></cell><cell>0.4029</cell><cell>0.4762</cell><cell>0.3800</cell></row><row><cell></cell><cell>p</cell><cell></cell><cell></cell><cell></cell><cell>0.0008</cell><cell>0.0000</cell><cell>0.0016</cell></row><row><cell>manual</cell><cell>τ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9123</cell><cell>0.9642</cell></row><row><cell></cell><cell>p</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0000</cell><cell>0.0000</cell></row><row><cell cols="2">manual-new τ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8769</cell></row><row><cell></cell><cell>p</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0000</cell></row><row><cell>manual-old</cell><cell>τ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>p</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="11,90.00,130.93,423.00,598.18"><head>Table 7 :</head><label>7</label><figDesc>Original topic set: breakdown of submission results over topic types (MRR) for all runs submitted to WebCLEF 2006. For each team, its best scoring run is in boldface.</figDesc><table coords="11,96.19,164.53,410.61,564.58"><row><cell>RUN</cell><cell>ALL</cell><cell></cell><cell>AUTO</cell><cell></cell><cell></cell><cell>MANUAL</cell><cell></cell></row><row><cell></cell><cell>topics</cell><cell>all</cell><cell>uni</cell><cell>bi</cell><cell>all</cell><cell>old</cell><cell>new</cell></row><row><cell>buap</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>allpt40bi</cell><cell cols="7">0.0157 0.0040 0.0031 0.0049 0.0750 0.0810 0.0657</cell></row><row><cell>depok</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>UI1DTA</cell><cell>0.0404</cell><cell>0.0234</cell><cell>0.0296</cell><cell>0.0173</cell><cell>0.1263</cell><cell>0.1099</cell><cell>0.1522</cell></row><row><cell>UI2DTF</cell><cell cols="7">0.0918 0.0466 0.0525 0.0406 0.3216 0.2611 0.4168</cell></row><row><cell>UI3DTAF</cell><cell>0.0253</cell><cell>0.0142</cell><cell>0.0116</cell><cell>0.0168</cell><cell>0.0819</cell><cell>0.0644</cell><cell>0.1094</cell></row><row><cell>UI4DTW</cell><cell>0.0116</cell><cell>0.0025</cell><cell>0.0020</cell><cell>0.0030</cell><cell>0.0583</cell><cell>0.0284</cell><cell>0.1053</cell></row><row><cell>hildesheim</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>UHi1-5-10</cell><cell>0.0718</cell><cell>0.0242</cell><cell>0.0231</cell><cell cols="2">0.0253 0.3134</cell><cell cols="2">0.2550 0.4051</cell></row><row><cell>UHi510</cell><cell>0.0718</cell><cell>0.0242</cell><cell>0.0231</cell><cell cols="2">0.0253 0.3134</cell><cell cols="2">0.2550 0.4051</cell></row><row><cell>UHiBase</cell><cell cols="4">0.0795 0.0346 0.0328 0.0363</cell><cell cols="2">0.3076 0.2556</cell><cell>0.3893</cell></row><row><cell>UHiBrf1</cell><cell>0.0677</cell><cell>0.0220</cell><cell>0.0189</cell><cell>0.0251</cell><cell>0.3000</cell><cell>0.2485</cell><cell>0.3812</cell></row><row><cell>UHiBrf2</cell><cell>0.0676</cell><cell>0.0221</cell><cell>0.0188</cell><cell>0.0253</cell><cell>0.2989</cell><cell>0.2464</cell><cell>0.3816</cell></row><row><cell>UHiTitle</cell><cell>0.0724</cell><cell>0.0264</cell><cell>0.0245</cell><cell>0.0283</cell><cell>0.3061</cell><cell>0.2542</cell><cell>0.3876</cell></row><row><cell>UHiMu (multilingual)</cell><cell>0.0489</cell><cell>0.0083</cell><cell>0.0063</cell><cell>0.0102</cell><cell>0.2553</cell><cell>0.2146</cell><cell>0.3192</cell></row><row><cell>hummingbird</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>humWC06</cell><cell>0.1133</cell><cell>0.0530</cell><cell>0.0572</cell><cell>0.0488</cell><cell>0.4194</cell><cell>0.3901</cell><cell>0.4657</cell></row><row><cell>humWC06dp</cell><cell>0.1209</cell><cell>0.0528</cell><cell>0.0555</cell><cell>0.0501</cell><cell>0.4664</cell><cell>0.4471</cell><cell>0.4967</cell></row><row><cell>humWC06dpc</cell><cell>0.1169</cell><cell>0.0472</cell><cell>0.0481</cell><cell>0.0464</cell><cell>0.4703</cell><cell>0.4553</cell><cell>0.4939</cell></row><row><cell>humWC06dpcD</cell><cell cols="7">0.1380 0.0704 0.0721 0.0687 0.4814 0.4633 0.5099</cell></row><row><cell>humWC06p</cell><cell>0.1180</cell><cell>0.0519</cell><cell>0.0556</cell><cell>0.0482</cell><cell>0.4538</cell><cell>0.4252</cell><cell>0.4988</cell></row><row><cell>isla</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>baseline</cell><cell>0.1694</cell><cell>0.1253</cell><cell>0.1397</cell><cell>0.1110</cell><cell>0.3934</cell><cell>0.3391</cell><cell>0.4787</cell></row><row><cell>comb</cell><cell>0.1685</cell><cell>0.1208</cell><cell>0.1394</cell><cell>0.1021</cell><cell>0.4112</cell><cell>0.3578</cell><cell>0.4952</cell></row><row><cell>combmeta</cell><cell>0.1947</cell><cell cols="2">0.1505 0.1670</cell><cell>0.1341</cell><cell cols="2">0.4188 0.3603</cell><cell>0.5108</cell></row><row><cell>combNboost</cell><cell cols="2">0.1954 0.1586</cell><cell cols="2">0.1595 0.1576</cell><cell>0.3826</cell><cell>0.3148</cell><cell>0.4891</cell></row><row><cell>combPhrase</cell><cell>0.2001</cell><cell>0.1570</cell><cell>0.1639</cell><cell cols="2">0.1500 0.4190</cell><cell cols="2">0.3587 0.5138</cell></row><row><cell>reina</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>usal base</cell><cell>0.0100</cell><cell>0.0028</cell><cell>0.0044</cell><cell>0.0011</cell><cell>0.0468</cell><cell>0.0550</cell><cell>0.0340</cell></row><row><cell>usal mix</cell><cell>0.0137</cell><cell>0.0038</cell><cell>0.0065</cell><cell>0.0011</cell><cell>0.0640</cell><cell>0.0747</cell><cell>0.0472</cell></row><row><cell>USAL mix hp</cell><cell cols="7">0.0139 0.0038 0.0065 0.0011 0.0655 0.0771 0.0472</cell></row><row><cell>usal mix hp</cell><cell>0.0139</cell><cell>0.0038</cell><cell>0.0065</cell><cell>0.0011</cell><cell>0.0655</cell><cell>0.0771</cell><cell>0.0472</cell></row><row><cell>usal mix hp ok</cell><cell>0.0139</cell><cell>0.0038</cell><cell>0.0065</cell><cell>0.0011</cell><cell>0.0655</cell><cell>0.0771</cell><cell>0.0472</cell></row><row><cell>rfia</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DPSinDiac</cell><cell>0.0982</cell><cell>0.0721</cell><cell>0.0736</cell><cell cols="3">0.0706 0.2309 0.1808</cell><cell>0.3098</cell></row><row><cell>ERConDiac</cell><cell>0.1006</cell><cell>0.0771</cell><cell>0.0795</cell><cell>0.0746</cell><cell>0.2203</cell><cell>0.1693</cell><cell>0.3006</cell></row><row><cell>ERFinal</cell><cell cols="4">0.1021 0.0785 0.0803 0.0766</cell><cell>0.2220</cell><cell cols="2">0.1635 0.3140</cell></row><row><cell>ERSinDiac</cell><cell cols="4">0.1021 0.0785 0.0803 0.0766</cell><cell>0.2220</cell><cell cols="2">0.1635 0.3140</cell></row><row><cell>ucm</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>webclef-run-all-2006-def-ok-2</cell><cell cols="7">0.0870 0.0556 0.0578 0.0534 0.2461 0.2002 0.3183</cell></row><row><cell>webclef-run-all-2006-def-ok</cell><cell>0.0870</cell><cell>0.0556</cell><cell>0.0578</cell><cell>0.0534</cell><cell>0.2461</cell><cell>0.2002</cell><cell>0.3183</cell></row><row><cell>webclef-run-all-2006-ok-conref</cell><cell>0.0870</cell><cell>0.0556</cell><cell>0.0578</cell><cell>0.0534</cell><cell>0.2461</cell><cell>0.2002</cell><cell>0.3183</cell></row><row><cell>webclef-run-all-2006</cell><cell>0.0870</cell><cell>0.0556</cell><cell>0.0578</cell><cell>0.0534</cell><cell>0.2461</cell><cell>0.2002</cell><cell>0.3183</cell></row><row><cell>webclef-run-all-OK-definitivo</cell><cell>0.0870</cell><cell>0.0556</cell><cell>0.0578</cell><cell>0.0534</cell><cell>0.2461</cell><cell>0.2002</cell><cell>0.3183</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="12,90.00,130.93,423.00,598.18"><head>Table 8 :</head><label>8</label><figDesc>New topic set: breakdown of submission results over topic types (MRR) for all runs submitted to WebCLEF 2006. For each team, its best scoring run is in boldface.</figDesc><table coords="12,96.19,164.53,410.61,564.58"><row><cell>RUN</cell><cell>ALL</cell><cell></cell><cell>AUTO</cell><cell></cell><cell></cell><cell>MANUAL</cell><cell></cell></row><row><cell></cell><cell>topics</cell><cell>all</cell><cell>uni</cell><cell>bi</cell><cell>all</cell><cell>old</cell><cell>new</cell></row><row><cell>buap</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>allpt40bi</cell><cell cols="7">0.0272 0.0080 0.0061 0.0099 0.0790 0.0863 0.0679</cell></row><row><cell>depok</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>UI1DTA</cell><cell>0.0699</cell><cell>0.0465</cell><cell>0.0578</cell><cell>0.0348</cell><cell>0.1330</cell><cell>0.1171</cell><cell>0.1572</cell></row><row><cell>UI2DTF</cell><cell cols="7">0.1589 0.0923 0.1024 0.0819 0.3386 0.2783 0.4307</cell></row><row><cell>UI3DTAF</cell><cell>0.0439</cell><cell>0.0281</cell><cell>0.0226</cell><cell>0.0339</cell><cell>0.0862</cell><cell>0.0686</cell><cell>0.1130</cell></row><row><cell>UI4DTW</cell><cell>0.0202</cell><cell>0.0049</cell><cell>0.0038</cell><cell>0.0060</cell><cell>0.0613</cell><cell>0.0302</cell><cell>0.1088</cell></row><row><cell>hildesheim</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>UHi1-5-10</cell><cell>0.1243</cell><cell>0.0480</cell><cell>0.0451</cell><cell cols="2">0.0510 0.3299</cell><cell cols="2">0.2717 0.4187</cell></row><row><cell>UHi510</cell><cell>0.1243</cell><cell>0.0480</cell><cell>0.0451</cell><cell cols="2">0.0510 0.3299</cell><cell cols="2">0.2717 0.4187</cell></row><row><cell>UHiBase</cell><cell cols="4">0.1376 0.0685 0.0640 0.0731</cell><cell cols="2">0.3238 0.2724</cell><cell>0.4023</cell></row><row><cell>UHiBrf1</cell><cell>0.1173</cell><cell>0.0436</cell><cell>0.0369</cell><cell>0.0505</cell><cell>0.3159</cell><cell>0.2648</cell><cell>0.3939</cell></row><row><cell>UHiBrf2</cell><cell>0.1171</cell><cell>0.0438</cell><cell>0.0367</cell><cell>0.0510</cell><cell>0.3147</cell><cell>0.2625</cell><cell>0.3943</cell></row><row><cell>UHiTitle</cell><cell>0.1254</cell><cell>0.0524</cell><cell>0.0479</cell><cell>0.0570</cell><cell>0.3222</cell><cell>0.2709</cell><cell>0.4005</cell></row><row><cell>UHiMu (multilingual )</cell><cell>0.0846</cell><cell>0.0164</cell><cell>0.0124</cell><cell>0.0205</cell><cell>0.2686</cell><cell>0.2286</cell><cell>0.3297</cell></row><row><cell>hummingbird</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>humWC06</cell><cell>0.1962</cell><cell>0.1051</cell><cell>0.1116</cell><cell>0.0984</cell><cell>0.4416</cell><cell>0.4156</cell><cell>0.4812</cell></row><row><cell>humWC06dp</cell><cell>0.2092</cell><cell>0.1047</cell><cell>0.1084</cell><cell>0.1009</cell><cell>0.4910</cell><cell>0.4764</cell><cell>0.5132</cell></row><row><cell>humWC06dpc</cell><cell>0.2023</cell><cell>0.0937</cell><cell>0.0939</cell><cell>0.0935</cell><cell>0.4952</cell><cell>0.4852</cell><cell>0.5104</cell></row><row><cell>humWC06dpcD</cell><cell cols="7">0.2390 0.1396 0.1408 0.1384 0.5068 0.4936 0.5269</cell></row><row><cell>humWC06p</cell><cell>0.2044</cell><cell>0.1030</cell><cell>0.1086</cell><cell>0.0971</cell><cell>0.4777</cell><cell>0.4530</cell><cell>0.5154</cell></row><row><cell>isla</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>baseline</cell><cell>0.2933</cell><cell>0.2485</cell><cell>0.2726</cell><cell>0.2237</cell><cell>0.4141</cell><cell>0.3614</cell><cell>0.4946</cell></row><row><cell>comb</cell><cell>0.2918</cell><cell>0.2394</cell><cell>0.2720</cell><cell>0.2058</cell><cell>0.4329</cell><cell>0.3812</cell><cell>0.5117</cell></row><row><cell>combmeta</cell><cell>0.3370</cell><cell cols="2">0.2985 0.3259</cell><cell>0.2701</cell><cell cols="2">0.4409 0.3839</cell><cell>0.5278</cell></row><row><cell>combNboost</cell><cell cols="2">0.3384 0.3145</cell><cell cols="2">0.3114 0.3176</cell><cell>0.4028</cell><cell>0.3355</cell><cell>0.5054</cell></row><row><cell>combPhrase</cell><cell>0.3464</cell><cell>0.3112</cell><cell>0.3199</cell><cell cols="2">0.3023 0.4411</cell><cell cols="2">0.3822 0.5310</cell></row><row><cell>reina</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>usal base</cell><cell>0.0174</cell><cell>0.0055</cell><cell cols="2">0.0087 0.0023</cell><cell>0.0493</cell><cell>0.0586</cell><cell>0.0351</cell></row><row><cell>usal mix</cell><cell>0.0237</cell><cell>0.0075</cell><cell>0.0126</cell><cell>0.0022</cell><cell>0.0674</cell><cell>0.0796</cell><cell>0.0488</cell></row><row><cell>USAL mix hp</cell><cell cols="3">0.0241 0.0075 0.0126</cell><cell cols="4">0.0022 0.0689 0.0822 0.0488</cell></row><row><cell>usal mix hp</cell><cell>0.0241</cell><cell>0.0075</cell><cell>0.0126</cell><cell>0.0022</cell><cell>0.0689</cell><cell>0.0822</cell><cell>0.0488</cell></row><row><cell>usal mix hp ok</cell><cell>0.0241</cell><cell>0.0075</cell><cell>0.0126</cell><cell>0.0022</cell><cell>0.0689</cell><cell>0.0822</cell><cell>0.0488</cell></row><row><cell>rfia</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DPSinDiac</cell><cell>0.1700</cell><cell>0.1429</cell><cell>0.1436</cell><cell cols="3">0.1422 0.2431 0.1926</cell><cell>0.3201</cell></row><row><cell>ERConDiac</cell><cell>0.1742</cell><cell>0.1528</cell><cell>0.1552</cell><cell>0.1503</cell><cell>0.2320</cell><cell>0.1804</cell><cell>0.3106</cell></row><row><cell>ERFinal</cell><cell cols="4">0.1768 0.1556 0.1568 0.1544</cell><cell>0.2337</cell><cell cols="2">0.1743 0.3244</cell></row><row><cell>ERSinDiac</cell><cell cols="4">0.1768 0.1556 0.1568 0.1544</cell><cell>0.2337</cell><cell cols="2">0.1743 0.3244</cell></row><row><cell>ucm</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>webclef-run-all-2006-def-ok-2</cell><cell cols="7">0.1505 0.1103 0.1128 0.1077 0.2591 0.2133 0.3289</cell></row><row><cell>webclef-run-all-2006-def-ok</cell><cell>0.1505</cell><cell>0.1103</cell><cell>0.1128</cell><cell>0.1077</cell><cell>0.2591</cell><cell>0.2133</cell><cell>0.3289</cell></row><row><cell>webclef-run-all-2006-ok-conref</cell><cell>0.1505</cell><cell>0.1103</cell><cell>0.1128</cell><cell>0.1077</cell><cell>0.2591</cell><cell>0.2133</cell><cell>0.3289</cell></row><row><cell>webclef-run-all-2006</cell><cell>0.1505</cell><cell>0.1103</cell><cell>0.1128</cell><cell>0.1077</cell><cell>0.2591</cell><cell>0.2133</cell><cell>0.3289</cell></row><row><cell>webclef-run-all-OK-definitivo</cell><cell>0.1505</cell><cell>0.1103</cell><cell>0.1128</cell><cell>0.1077</cell><cell>0.2591</cell><cell>0.2133</cell><cell>0.3289</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="9,105.24,734.11,407.75,6.99;9,90.00,743.58,317.26,6.99"><p>Recall that term relevance sets (T-rels) consisting of a set of terms likely to occur in relevant documents, and a set of irrelevant terms (especially disambiguation terms avoiding false-positives)<ref type="bibr" coords="9,395.97,743.58,8.47,6.99" target="#b0">[1]</ref>.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments Thanks to <rs type="institution">Universidad Complutense de Madrid (UCM)</rs> for providing additional Spanish topics. <rs type="person">Krisztian Balog</rs> was supported by the <rs type="funder">Netherlands Organisation for Scientific Research (NWO)</rs> under project numbers <rs type="grantNumber">220-80-001</rs>, <rs type="grantNumber">600.065.120</rs> and <rs type="grantNumber">612.000.106</rs>. <rs type="person">Jaap Kamps</rs> was supported by <rs type="funder">NWO</rs> under project numbers <rs type="grantNumber">612.066.302</rs>, <rs type="grantNumber">612.066.513</rs>, <rs type="grantNumber">639.072.601</rs>, and <rs type="grantNumber">640.001.501</rs>; and by the <rs type="programName">E.U. IST programme</rs> of the <rs type="programName">6th FP for RTD</rs> under project <rs type="projectName">MultiMATCH</rs> contract <rs type="grantNumber">IST-033104</rs>. <rs type="person">Maarten de Rijke</rs> was supported by <rs type="funder">NWO</rs> under project numbers <rs type="grantNumber">017.001.190</rs>, <rs type="grantNumber">220-80-001</rs>, <rs type="grantNumber">264-70-050</rs>, <rs type="grantNumber">354-20-005</rs>, <rs type="grantNumber">600.065.120</rs>, <rs type="grantNumber">612-13-001</rs>, <rs type="grantNumber">612.000.106</rs>, <rs type="grantNumber">612.066.302</rs>, <rs type="grantNumber">612.069.006</rs>, <rs type="grantNumber">640.001.501</rs>, <rs type="grantNumber">640.-002.501</rs>, and and by the <rs type="programName">E.U. IST programme</rs> of the <rs type="programName">6th FP for RTD</rs> under project <rs type="projectName">MultiMATCH</rs> contract <rs type="grantNumber">IST-033104</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_8FEPnb5">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_9336a2Q">
					<idno type="grant-number">600.065.120</idno>
				</org>
				<org type="funding" xml:id="_6AxJkaH">
					<idno type="grant-number">612.000.106</idno>
				</org>
				<org type="funding" xml:id="_AaMM7JX">
					<idno type="grant-number">612.066.302</idno>
				</org>
				<org type="funding" xml:id="_vKj992Z">
					<idno type="grant-number">612.066.513</idno>
				</org>
				<org type="funding" xml:id="_4fSQBBs">
					<idno type="grant-number">639.072.601</idno>
				</org>
				<org type="funding" xml:id="_4UxeAMx">
					<idno type="grant-number">640.001.501</idno>
					<orgName type="program" subtype="full">E.U. IST programme</orgName>
				</org>
				<org type="funded-project" xml:id="_STp4CmG">
					<idno type="grant-number">IST-033104</idno>
					<orgName type="project" subtype="full">MultiMATCH</orgName>
					<orgName type="program" subtype="full">6th FP for RTD</orgName>
				</org>
				<org type="funding" xml:id="_6z3Urk6">
					<idno type="grant-number">017.001.190</idno>
				</org>
				<org type="funding" xml:id="_WGGX4VY">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_mxVKtkj">
					<idno type="grant-number">264-70-050</idno>
				</org>
				<org type="funding" xml:id="_yc8uHcz">
					<idno type="grant-number">354-20-005</idno>
				</org>
				<org type="funding" xml:id="_syF2fdw">
					<idno type="grant-number">600.065.120</idno>
				</org>
				<org type="funding" xml:id="_BF9fAWJ">
					<idno type="grant-number">612-13-001</idno>
				</org>
				<org type="funding" xml:id="_aav6SBk">
					<idno type="grant-number">612.000.106</idno>
				</org>
				<org type="funding" xml:id="_8yXzDuh">
					<idno type="grant-number">612.066.302</idno>
				</org>
				<org type="funding" xml:id="_ZTvsBEH">
					<idno type="grant-number">612.069.006</idno>
				</org>
				<org type="funding" xml:id="_NaZRbNc">
					<idno type="grant-number">640.001.501</idno>
				</org>
				<org type="funding" xml:id="_VZwwvzz">
					<idno type="grant-number">640.-002.501</idno>
					<orgName type="program" subtype="full">E.U. IST programme</orgName>
				</org>
				<org type="funded-project" xml:id="_UQMTnD6">
					<idno type="grant-number">IST-033104</idno>
					<orgName type="project" subtype="full">MultiMATCH</orgName>
					<orgName type="program" subtype="full">6th FP for RTD</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,105.50,334.70,407.50,8.74;10,105.50,346.65,407.50,8.74;10,105.50,358.61,407.50,8.74;10,105.50,370.56,49.27,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,336.73,334.70,176.26,8.74;10,105.50,346.65,59.19,8.74">Scaling ir-system evaluation using term relevance sets</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amitay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Lempel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Soffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,189.78,346.65,323.22,8.74;10,105.50,358.61,232.52,8.74">Proceedings of the 27th annual international ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 27th annual international ACM SIGIR conference on Research and Development in Information Retrieval<address><addrLine>New York USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,388.81,407.51,8.74;10,105.50,400.76,407.51,8.74;10,105.50,412.72,391.68,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,247.18,388.81,246.13,8.74">Automatic construction of known-item finding test beds</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,105.50,400.76,407.51,8.74;10,105.50,412.72,140.59,8.74">Proceedings of the 29th annual international ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 29th annual international ACM SIGIR conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="603" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,430.96,314.10,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,156.33,430.96,114.01,8.74">A taxonomy of web search</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Broder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,278.99,430.96,57.90,8.74">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3" to="10" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,449.21,407.50,8.74;10,105.50,461.17,404.58,9.02" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,176.43,449.21,129.52,8.74">Europeans and their languages</title>
		<ptr target="http://ec.europa.eu/public_opinion/archives/ebs/ebs_243_en.pdf" />
	</analytic>
	<monogr>
		<title level="j" coord="10,313.09,449.21,98.48,8.74">Special Eurobarometer</title>
		<imprint>
			<biblScope unit="volume">243</biblScope>
			<date type="published" when="2006">2006</date>
			<publisher>European Commision</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,479.41,407.51,8.74;10,105.50,491.37,145.58,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,263.45,479.41,244.43,8.74">Cross language information retrieval: a research roadmap</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kando</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,105.50,491.37,57.90,8.74">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="72" to="80" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,509.61,407.51,8.74;10,105.50,522.28,149.22,8.30" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="10,145.19,509.61,303.50,8.74">The Lemur toolkit for language modeling and information retrieval</title>
		<author>
			<persName coords=""><surname>Lemur</surname></persName>
		</author>
		<ptr target="http://www.lemurproject.org/" />
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,539.81,407.51,8.74;10,105.50,551.77,407.51,8.74;10,105.50,563.72,407.51,8.74;10,105.50,575.68,407.51,8.74;10,105.50,587.63,187.16,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,321.36,539.81,191.65,8.74;10,105.50,551.77,26.64,8.74">EuroGOV: Engineering a multilingual Web corpus</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sigurbjörnsson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,213.67,563.72,299.34,8.74;10,105.50,575.68,211.51,8.74">Accessing Multilingual Information Repositories: 6th Workshop of the Cross-Language Evaluation Forum (CLEF 2005)</title>
		<title level="s" coord="10,394.49,575.68,118.51,8.74;10,105.50,587.63,30.49,8.74">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,605.88,407.51,8.74;10,105.50,617.83,407.51,8.74;10,105.50,629.79,407.50,8.74;10,105.50,641.74,407.51,8.74;10,105.50,653.70,109.09,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,320.90,605.88,102.62,8.74">Overview of WebCLEF</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sigurbjörnsson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,140.57,629.79,372.43,8.74;10,105.50,641.74,137.93,8.74">Accessing Multilingual Information Repositories: 6th Workshop of the Cross-Language Evaluation Forum (CLEF 2005)</title>
		<title level="s" coord="10,318.60,641.74,150.56,8.74">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2005">2005. 2006</date>
			<biblScope unit="volume">4022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,671.94,407.50,9.02;10,90.00,706.89,12.19,12.62;10,118.33,706.89,274.14,12.62;10,90.00,731.75,423.01,8.74;10,90.00,743.71,195.19,8.74" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="10,156.97,671.94,112.59,8.74">Cross-lingual web retrieval</title>
		<author>
			<persName coords=""><surname>Webclef</surname></persName>
		</author>
		<ptr target="http://ilps.science.uva.nl/WebCLEF/" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>A Breakdown of Scores over Topic Types We provide a breakdown of scores over the different topic types, both for the original topic set in Table 7 and for the new topic set in Table 8</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
