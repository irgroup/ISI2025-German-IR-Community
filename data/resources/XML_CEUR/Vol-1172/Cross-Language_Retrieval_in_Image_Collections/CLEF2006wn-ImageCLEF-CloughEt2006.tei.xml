<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,98.52,148.53,406.28,15.61;1,145.08,170.49,312.89,15.61">Overview of the ImageCLEF 2006 photographic retrieval and object annotation tasks</title>
				<funder ref="#_fX5g5vQ">
					<orgName type="full">MUSCLE NoE</orgName>
				</funder>
				<funder ref="#_DZKmhXf">
					<orgName type="full">DFG (Deutsche Forschungsgemeinschaft)</orgName>
				</funder>
				<funder ref="#_BvrFG4m">
					<orgName type="full">Victoria University</orgName>
				</funder>
				<funder ref="#_gFp4Xhc">
					<orgName type="full">American National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_XAfWtPH">
					<orgName type="full">Swiss National Science Foundation (FNS)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,180.24,203.97,54.15,9.96"><forename type="first">Paul</forename><surname>Clough</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sheffield University</orgName>
								<address>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,244.92,203.97,81.57,9.96"><forename type="first">Michael</forename><surname>Grubinger</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Victoria University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,336.96,203.97,78.67,9.96"><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,227.52,218.01,64.93,9.96"><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Vienna University</orgName>
								<address>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,302.88,218.01,68.12,9.96"><forename type="first">Henning</forename><surname>MÃ¼ller</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University and Hospitals of Geneva</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,98.52,148.53,406.28,15.61;1,145.08,170.49,312.89,15.61">Overview of the ImageCLEF 2006 photographic retrieval and object annotation tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DE7D88BCEA3489EF074CF6393497D65B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Management]: Languages-Query Languages Measurement, Performance, Experimentation Image retrieval, image classification, performance evaluation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the general photographic retrieval and object annotation tasks of the ImageCLEF 2006 evaluation campaign. These tasks provide both the resources and the framework necessary to perform comparative laboratory-style evaluation of visual information systems for image retrieval and automatic image annotation. Both tasks offer something new for 2006 and attracted a large number of submissions: 12 groups participating in ImageCLEFphoto and 3 in the automatic annotation task. This paper summarises components used in the benchmark, including the collections, the search and annotation tasks, the submissions from participating groups, and results.</p><p>The general photographic retrieval task, ImageCLEFphoto, used a new collection -the IAPR-TC12 Benchmark -of 20,000 colour photographs with semi-structured captions in English and German. This new collection replaces the St Andrews collection of historic photographs used for the previous three years. For ImageCLEFphoto groups submitted mainly text-only runs. However, 31% of runs involved some kind of visual retrieval technique, typically combined with text through the merging of image and text retrieval results. Bilingual text retrieval was performed using two target languages: English and German, with 59% of runs bilingual. Highest monolingual of English was shown to be 74% for Portuguese-English and 39% of German for English-German. Combined text and retrieval approaches were seen to give, on average, higher retrieval results (+54%) than using text (or image) retrieval alone. Similar to previous years, the use of relevance feedback (most commonly in the form of pseudo relevance feedback) to enable query expansion was seen to improve the text-based submissions by an average of 39%. Topics have been categorised and analysed with respect to various attributes including an estimation of their "visualness" and linguistic complexity.</p><p>The general automatic object annotation task used a hand collected dataset of 81,211 images from 268 classes provided by LTUtech. Given training data, participants were required to classify previously unseen images. The error rate of submissions for this task was high (ranging from 77.3% to 93.2%) resulting in a large proportion of test images being misclassified by any of the proposed classification methods. The task can therefore be said to have been very challenging for participants.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The evaluation of text information retrieval has benefited from the use of standardised benchmarks and evaluation events, performed since the 1960s <ref type="bibr" coords="2,310.10,270.69,9.91,9.96" target="#b2">[2]</ref>. With TREC<ref type="foot" coords="2,384.00,269.21,3.97,6.97" target="#foot_0">1</ref> (Text REtrieval Conference <ref type="bibr" coords="2,90.00,282.57,15.49,9.96" target="#b11">[11]</ref>) a standard was set that has been used as the model for evaluation events in related fields. One such event is CLEF<ref type="foot" coords="2,172.92,293.21,3.97,6.97" target="#foot_1">2</ref> (Cross Language Evaluation Forum) and within CLEF, the retrieval of images from multilingual collections: ImageCLEF. Over the past 2-3 years, ImageCLEF has expanded to deal with multiple domains (most noticeably the retrieval of medical images) and aspects of retrieval such as the automatic annotation of images with text descriptors. In this paper, we describe three tasks at ImageCLEF 2006: the general photographic retrieval task (ImageCLEFphoto), a general visual retrieval task, and a general image annotation (or classification) task. Section 2 describes the first general retrieval task, section 3 the visual retrieval task aimed more specifically at evaluating purely visual retrieval systems, and section 4 describes the automatic annotation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The ImageCLEFphoto photographic retrieval task 2.1 General Overview</head><p>This task is similar to the classic TREC ad-hoc retrieval task: simulation of the situation in which a system knows the set of documents to be searched, but cannot anticipate the particular topic that will be investigated (i.e. topics are not known to the system in advance). The goal of ImageCLEFphoto 2006 is: given a multilingual statement describing a user information need, find as many relevant images as possible from the given document collection. After three years of image retrieval evaluation using the St. Andrews database <ref type="bibr" coords="2,370.08,525.09,9.91,9.96" target="#b3">[3]</ref>, a new database was used in this year's task: the IAPR TC-12 Benchmark <ref type="bibr" coords="2,298.68,537.09,10.00,9.96" target="#b5">[5]</ref>, created under Technical Committee 12 (TC-12) of the International Association of Pattern Recognition (IAPR <ref type="foot" coords="2,381.24,547.61,3.97,6.97" target="#foot_2">3</ref> ). This collection differs from the St Andrews collection used in previous campaigns in two major ways: (1) it contains mainly colour photographs (the St Andrews collection was primarily black and white) and ( <ref type="formula" coords="2,455.96,572.97,4.25,9.96">2</ref>) it contains semi-structured captions in English and German (the St Andrews collection used only English).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Document Collection</head><p>The IAPR TC-12 Benchmark contains 20,000 photos taken from locations around the world and comprises a varying cross-section of still natural images. Figure <ref type="figure" coords="2,372.13,643.17,4.98,9.96" target="#fig_0">1</ref> illustrates a number of sample images from a selection of categories. The majority of images have been provided by viventura <ref type="foot" coords="2,505.80,653.69,3.97,6.97" target="#foot_3">4</ref> , an independent travel company that organises adventure and language trips to South-America. Travel guides accompany the tourists and maintain a daily online diary including photographs of trips made and general pictures of each location including accommodation, facilities and ongoing social projects. The collection contains many different images of similar visual content, but varying illumination, viewing angle and background. This makes it a challenge for the successful application of visual analysis techniques. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sports.</head><p>Landscapes. People. Animals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cities.</head><p>Actions. Architecture.</p><p>Each image in the collection has a corresponding semi-structured caption consisting of the following seven fields: (1) a unique identifier, (2) a title, (3) a free-text description of the semantic and visual contents of the image, (4) notes for additional information, <ref type="bibr" coords="3,390.73,479.61,12.76,9.96" target="#b5">(5)</ref> the provider of the photo and fields describing <ref type="bibr" coords="3,183.25,491.49,12.76,9.96" target="#b6">(6)</ref> where and <ref type="bibr" coords="3,248.66,491.49,12.76,9.96" target="#b7">(7)</ref> when the photo was taken. These fields exist in English and German, with a Spanish version currently being verified. Figure <ref type="figure" coords="3,388.44,503.49,4.98,9.96" target="#fig_1">2</ref> shows a sample image with its corresponding English annotation. These annotations are stored in a database allowing subsets of the collection to be created for benchmarking based on specifying particular parameters (e.g. which caption fields to use).</p><p>One of these parameters is annotation quality: in order to provide a more realistic scenario, the annotation files have been generated with a varying degree of annotation "completeness":</p><p>â¢ 70% of the annotations contain title, description, notes, location and date.</p><p>â¢ 10% of the annotations contain title, location and date.</p><p>â¢ 10% of the annotations contain location and date.</p><p>â¢ 10% of the images are not annotated (or have empty tags respectively).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Query Topics</head><p>Participants were given 60 topics, created using a custom-built topic creation and administration system to "achieve a natural, balanced topic set accurately reflecting real word user statements of information needs" <ref type="bibr" coords="4,176.29,275.25,10.57,9.96" target="#b9">[9]</ref> (pp.1069). The following information was considered in the topic creation process:</p><p>Number of topics. In order to increase the reliability of results, a total of 60 topics was provided to participants.</p><p>Log file Analysis. To make the task realistic, topics were derived from analysing a log file<ref type="foot" coords="4,102.72,333.65,3.97,6.97" target="#foot_4">5</ref> from a web-based interface to the IAPR TC-12 collection which is used by employees and customers of viventura. A total of 40 topics were taken directly from the log file (semantically equivalent but perhaps with slight syntactic modification, e.g. "lighthouse sea" to "lighthouses at the sea") and 10 topics derived from entries in the log file (e.g. "straight roads in Argentina" changed to "straight roads in the USA"). The remaining 10 topics were not taken directly from the log file but created to test various aspects of text and image retrieval (e.g. "black and white photos of Russia").</p><p>Geographic Constraints. Corresponding to the findings from previous log file analyses (see, e.g. <ref type="bibr" coords="4,108.84,430.65,14.73,9.96" target="#b12">[12]</ref>), many search requests exhibit geographic constraints and this was found to be similar with the IAPR TC-12 collection. Thus, 24 of the topics were created with a geographic constraint (e.g. "tourist accommodation near Lake Titicaca" specifies a location and spatial operator near); 20 of the topics specifying a geographic feature or a permanent man-made object (e.g. "group standing in salt pan") and the remaining topics having no geography (e.g. "photos of female guides").</p><p>Visual Features. All topics were classified according to how "visual" they were considered to be. An average rating between 1-5<ref type="foot" coords="4,252.48,512.93,3.97,6.97" target="#foot_5">6</ref> was obtained for each topic from three experts in the field of image analysis, and the retrieval score from a baseline content-based image retrieval (CBIR) system <ref type="foot" coords="4,119.76,536.81,3.97,6.97" target="#foot_6">7</ref> . A total of 30 topics are classed as "semantic" (levels 1 and 2) for which visual approaches are highly unlikely to improve results; 20 topics are "neutral" (level 3) for which visual approaches may or may not improve results and 10 are "visual" topics for which content-based approaches are most likely to improve retrieval results.</p><p>Topic Difficulty. A topic complexity measure was used to categorise topics according to their linguistic complexity <ref type="bibr" coords="4,184.09,597.93,10.00,9.96" target="#b6">[6]</ref>. A total of 31 topics were chosen to be rather easy topics (levels 1 and 2), 25 topics were medium-hard topics (level 3), and 4 topics were difficult (levels 4 and 5).</p><p>Size of Target Set. Topic creators aimed for a target set size between 20 and 100 relevant images and thus had to further modify some of the topics (broadening or narrowing the concepts). The minimum was chosen in order to be able to use P(20) as a performance measure, whereas the upper limit of relevant images should limit the retrieval of relevant images by chance and to keep the relevance judgment pools to a manageable size.</p><p>Annotation Quality. Another dimension considered was the distribution of the topics in regards to the level of annotation quality of relevant images for the particular queries. In other words, 18 topics were provided in which all relevant images have complete annotations, 10 topics with 80% -100% of the relevant images having complete annotations, further 19 topics with 60% -80% of the relevant images with complete annotations, and 13 topics with less than 60% of the relevant images with complete annotations.</p><p>Attributes of Text Retrieval. Various aspects of text retrieval on a more semantic level were considered too, concentrating on vocabulary mismatches, general versus specific concepts, word disambiguation and abbreviations.</p><p>Participant Feedback. In last year's break-out session, participants suggested we provide groups of similar topics in order to facilitate the analysis of weak performing queries. This year saw groups of up to five similar topics (e.g. "tourist groups / destinations / Machu Picchu in bad weather").</p><p>Each original topic comprised a title (a short sentence or phrase describing the search request in a few words), and a narrative (a description of what constitutes a relevant or non-relevant image for each request). In addition, three image examples were provided with each topic in order to test relevance feedback (both manual and automatic) and query-by-example searches. The topic titles were then translated into 15 languages including German, French, Spanish, Italian, Portuguese, Dutch, Russian, Japanese, and Simplified and Traditional Chinese. All translations were provided by at least one native speaker and verified by at least another native speaker. Unlike in past campaigns, however, the topic narratives were neither translated nor evaluated this year. A list of all topics can be found in Table <ref type="table" coords="5,242.79,338.49,3.90,9.96" target="#tab_4">5</ref>. In addition, 30 purely visual topics were provided in a visual subtask to attract more visual groups. These visual topics are, in fact, a modified subset of the 60 original topics in which nonvisual features like geographic constraints or proper names were removed. Only three example images and no textual information like topic titles or narrative descriptions were provided. Section 3 provides more details about this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Relevance Assessments</head><p>Relevance assessments were carried out by the two topic creators<ref type="foot" coords="6,382.92,128.33,3.97,6.97" target="#foot_7">8</ref> using a custom-built online tool. The top 40 results from all submitted runs were used to create image pools giving an average of 1,045 images (max: 1468; min: 575) to judge per topic. The topic creators judged all images in the topic pools and also used interactive search and judge (ISJ) to supplement the pools with further relevant images. The ISJ was based on purely text searches. The assessments were based on a ternary classification scheme: (1) relevant, (2) partially relevant, and (3) not relevant. Based on these judgments, only those images judged relevant by both assessors were considered for the set of relevant images (qrels).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Participating Groups and Methods</head><p>A record number of 36 groups registered for ImageCLEFphoto this year, with exactly one third of them submitting a total of 157 runs (all of which were evaluated). This is similar to last year <ref type="bibr" coords="6,111.84,283.65,83.76,9.96">(11 groups in 2005)</ref>, although fewer runs (349 in 2005). Table <ref type="table" coords="6,386.88,283.65,4.98,9.96" target="#tab_1">2</ref> shows an overview of these participating groups and the number of runs submitted. New groups submitting in 2006 include Berkeley, RWTH, CINDI, TUC and CELI. All groups (with the exception of RWTH) submitted a monolingual English run with the most popular languages appearing as Italian, Japanese and Simplified Chinese. A brief description of the methods of the submitted runs is provided for each group (listed alphabetically by their group ID). Participants were also asked to categorise their submissions according to the following: query language, annotation language (English or German), type (automatic or manual), use of feedback or automatic query expansion, and modality (text only, image only or combined). Table <ref type="table" coords="6,201.86,601.53,4.98,9.96" target="#tab_3">4</ref> shows the overall results according to runs categorised by these dimensions. Most submissions made use of the image metadata, with 8 groups submitting bilingual runs and 11 groups monolingual runs. For many participants, the main focus of their submission was combining visual and text features (11 groups text-only and 7 groups combined text and image) and/or using some kind of relevance feedback to provide query expansion (8 groups using some kind of feedback).</p><p>Berkeley. The School of Information Management and Systems of the University of California in Berkeley, USA, submitted seven runs. All runs were text only: 4 monolingual English, 2 monolingual German, one bilingual English-German. Berkeley submitted 3 runs using feedback and 3 runs using title + narrative. The retrieval algorithm used was a form of logistic regression as used in TREC2 with blind relevance feedback method (10 highest weighting terms from top 10 documents). Translation was using Babelfish and expanding queries using the metadata of relevant images was found to work well. An interesting result was that using query expansion without any translation of terms worked surprisingly well for the bilingual run.</p><p>CEA-LIC2M. The CEA-LIC2M group from Fontenay aux Roses Cedex in France submitted five runs without using feedback or query expansion techniques. The group submitted 2 visual, 2 text, 1 mixed, 2 monolingual English and 1 bilingual French-English run. Separate initial queries were performed using the text and visual components of the topics, and then merged a-posteriori. Documents and queries are processed using a linguistic analyser to extract "concepts". Performing visual retrieval on each query image and merging results appeared to provide better results than visual retrieval with all three example images simultaneously.</p><p>CELI. The participants from CELI srl of Torino, Italy, submitted 9 text-only, automatic runs without feedback: 1 monolingual English, 8 bilingual, Italian-English and 6 runs with different query expansion techniques. Translation is achieved using bilingual dictionaries and a disambiguation approach based on Latent Semantic Analysis was implemented. Using a Boolean AND operator of the translations was found to provide higher results than using an OR operator. Results for P10 and P20 were shown to give similar results across runs compared to a more variable MAP result. The use of query expansion was shown to increase retrieval effectiveness to bridge the gap between the uncontrolled language of the query and the controlled language of the metadata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CINDI.</head><p>The CINDI group from Concordia University in Montreal, Canada, submitted 3 monolingual English runs, 2 text only, 1 mixed, 2 automatic, 1 manual, 2 with feedback (manual), 1 without feedback, 2 with query expansion and 1 without query expansion. The use of manual relevance feedback and the integration of text and image achieved the best performance for this group.</p><p>DCU. Dublin City University in Dublin, Ireland, submitted 40 automatic runs, 14 mixed, 26 text-only, 27 with feedback and 13 without feedback. DCU submitted 6 monolingual and 34 bilingual runs exploring 10 different query languages and both annotation languages. Text retrieval is performed using the BM25 weighting operator, and visual features matched using the Jeffrey Divergence function. Image retrieval on individual images was performed and merged using the CombMAX operator. Text and visual runs were fused using the weighted CombSUM operator. The results showed that fused text and image retrieval consistently outperformed text-only methods. The use of pseudo relevance feedback was also shown to improve the effectiveness of the text retrieval model.</p><p>IPAL. IPAL Singapore submitted 13 automatic runs (monolingual only): 6 visual, 4 mixed and 3 text only. Various indexing methods were tested and the XIOTA system used for text retrieval. The group used pseudo relevance feedback and an interesting feature of this was using feedback from one modality to influence the other (e.g. the result of image ranking used to drive query expansion through documents). Results indicate that the combination of text and image retrieval leads to better performance. They submitted a further 4 runs to the visual-only subtask.</p><p>NII. The National Institute of Informatics from Tokyo, Japan, submitted 6 text-only, automatic runs without feedback or query expansion, concentrating on all possibilities of three languages: English, German and Japanese: 1 monolingual English, 1 monolingual German and four bilingual runs. NII used the Lemur toolkit for text retrieval (unigram language modelling algorithm), Babelfish for translation, and a visual feature-based micro-clustering algorithm was trialled for the linking of near identical images annotated in different languages. This clustering approach did not improve retrieval effectiveness.</p><p>Miracle. The Miracle group of the Daedalus University in Madrid, Spain, submitted 30 automatic runs: 28 text only, 2 mixed and 10 runs involving query expansion based on Wordnet. The group used only the English annotations and generated 18 monolingual English runs and 12 bilingual runs (Russian, Polish, Japanese and simplified Chinese). A total of 8 runs used narrative descriptions only, 9 runs used both title and narratives and the remaining used the titles only. The most effective approach was shown to be the indexing of nouns from the image captions with no other processing.</p><p>NTU. The National Taiwan University from Taipei, Taiwan, submitted 30 automatic runs: 10 text only, 20 mixed, 12 with feedback and 18 without feedback. A total of 2 monolingual English, 2 monolingual German, 1 visual run and 25 bilingual runs (using English annotations only) exploring 10 different languages were submitted. NTU showed that the use of visual features could improve text-only retrieval based on the image annotations. A novel word-image ontology approach did not perform as well as retrieval with the image captions. Systran was used to provide translation and the initial query images were found to improve ad-hoc retrieval.</p><p>RWTH. The Human Language Technology and Pattern Recognition Group from the RWTH Aachen University in Aachen, Germany, submitted a total number of 4 entirely visual runs: 2 for the standard ad-hoc task, and 2 to the visual retrieval sub-task. Visual-only retrieval did not perform well in either task.</p><p>SINAI. The University of JaÃ©n, Spain, submitted 12 automatic text-only runs, 8 runs with query expansion, using English annotations only. The group submitted 4 monolingual runs and 8 bilingual runs (Dutch, French, German, Italian, Portuguese and Spanish). A number of different MT systems were used for translation and the Lemur toolkit implementation of Okapi used as the retrieval model. TUC. Technische UniversitÃ¤t Chemnitz from Germany submitted four automatic monolingual English runs: 3 text only and 1 mixed; 3 with feedback (and query expansion) and 1 without. Combining/merging independent visual and text runs appear to give highest retrieval effectiveness, together with the use of text-based query expansion. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.1">Analysis of System Runs</head><p>Results for submitted runs were computed using the latest version of TREC EVAL <ref type="foot" coords="9,450.00,146.81,3.97,6.97" target="#foot_8">9</ref> . Submissions were evaluated using uninterpolated (arithmetic) Mean Average Precisions MAP and Precision at rank 20 (P20) because most online image retrieval engines like Google, Yahoo and Altavista display 20 images by default. Further measures considered include Geometric Mean Average Precision (GMAP) to test robustness <ref type="bibr" coords="9,256.09,195.93,14.60,9.96" target="#b10">[10]</ref>, and the Binary Preference (bpref) measure which is a good indicator for the completeness of relevance judgments <ref type="bibr" coords="9,353.04,207.93,9.91,9.96" target="#b1">[1]</ref>. Using Kendall's Tau to compare system ranking between measures, we have found significant correlations at the 0.001 level between all measures above 0.74. This requires further investigation, but it would appear that the measure used to rank systems does affect the system ranking. Table <ref type="table" coords="9,131.88,255.69,4.98,9.96" target="#tab_2">3</ref> shows the runs which achieved the highest MAP for each language pair. Of these runs, 83% use feedback of some kind (typically pseudo relevance feedback) and a similar proportion use both visual and textual features for retrieval. It is noticeable that submissions from NTU and DCU dominate the results (see participant's workshop papers for further information about their runs). It is interesting to note that English monolingual outperforms the German monolingual (19% lower) and the highest bilingual to English run was Portuguese-English which performed 74% of monolingual , but the highest bilingual to German run was English to German which performed only at only 39% of monolingual. Also, unlike previous years, the top-performing bilingual runs have involved Portuguese, traditional Chinese and Russian as the source language showing an improvement of the retrieval methods using these languages. Table <ref type="table" coords="9,132.36,588.57,4.98,9.96" target="#tab_3">4</ref> shows results by different dimensions and shows that on average: monolingual results are higher than bilingual, retrieval using English annotations is higher than German, combined text and image retrieval is higher than text or image only, and retrieval with feedback gives higher results than without (we are currently determining statistical significance of these results). This trend has continued for the past three years (combined media and feedback runs performing the highest). Absolute retrieval results are lower than previous years and we attribute this to the choice of topics, a more visually challenging photographic collection and there being incomplete annotations provided with the collection. All groups have shown that combining visual features from the image and semantic knowledge derived from the captions offers optimum retrieval for many of the topics. In general, feedback (typically in the form of query expansion based on pseudo relevance feedback) also appears to work well on short captions (including results from previous years) and is likely due to the limited vocabulary exhibited by the captions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.2">Analysis of Topics</head><p>Table <ref type="table" coords="10,118.68,309.09,4.98,9.96" target="#tab_4">5</ref> shows the average P20 and MAP scores across all runs for each topic (together with the number of relevant images per topic). There are considerable differences between topics, e.g. "photos of radio telescopes" (topic 57) has an average MAP of 0.5161; whereas "tourist accommodation near Lake Titicaca" (topic 9) has an average MAP of 0.0027. Reasons for these differences are likely due to the discriminating power of query terms in the collection, the complexity of topics (e.g. topic 9 involves a location and fuzzy spatial operator which will not be handled appropriately unless necessary support is given for spatial queries), the level of semantic knowledge required to retrieve relevant images (this will limit the success of purely visual approaches), and translation success (e.g. whether proper names have been successfully handled). Based on all results, we find the following trends according to average MAP (standard deviation):</p><p>â¢ Log file Analysis. For topics taken from the log file MAP=0.1296 (0.0928); topics derived from the log file MAP=0.1155 (0.0625) and topics not taken from the log file MAP=0.2191 (0.1604). It is likely that most topics not derived from the log file are more "visual" and perhaps therefore simpler to execute.</p><p>â¢ Geographic Constraints. Topics specifying specific locations and spatial operators MAP=0.1146 (0.0872); topics specifying general locations or man-made objects MAP=0.1785 (0.1111) and topics with no geography MAP=0.1313 (0.1219). Most groups did not use geographic retrieval methods.</p><p>â¢ Visual Features. For topics where it is estimated visual techniques will not improve results (levels 1 and 2) MAP=0.1179 (0.1041); for topics where visual retrieval could improve results (level 3) MAP=0.1318 (0.0940) and topics where visual techniques are expected to improve results (levels 4 and 5) MAP=0.2250 (0.1094). More visual topics are likely to perform better given many participants made use of combined visual and textual approaches.</p><p>â¢ Topic Difficulty. Topics rated as linguistically easy (complexity levels 1 and 2) MAP=0.1794 (0.1191); topics rated as challenging MAP=0.1107 (0.0728) and topics rated as difficult MAP=0.0234 (0.0240).</p><p>â¢ Annotation Quality. Topics with all relevant images having annotations MAP=0.1668 (0.1356); topics with 80-99% of relevant images having annotations MAP=0.1290 (0.0653); topics with 60-79% of relevant images having annotations MAP=0.1353 (0.1002) and topics with 0-59% of relevant images having complete annotations MAP=0.1198 (0.1027). The use of non-text approaches is the likely cause of successful retrieval for topics with relevant images containing incomplete annotations.</p><p>We are currently investigating the effects of various retrieval strategies (e.g. use of visual and textual features) on results for different topics which will be reported in further work. We expect that the use of visual techniques will improve topics which can be considered "more visual" (e.g. "sunset over water" is more visual than "pictures of female guides" which one could consider more semantic) and that topics which are considered "more difficult" linguistically (e.g. "bird flying" is linguistically simpler than "pictures taken on Ayers Rock") will require more complex language processing techniques.</p><p>3 The ImageCLEFphoto visual retrieval sub-task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">General Overview</head><p>The ImageCLEFphoto visual retrieval sub-task offers a challenge that is similar to the general ImageCLEFphoto task: given a user information need described by three sample images, find as many relevant images as possible from a given document collection using content-based image retrieval only.</p><p>The main goal of this task is to investigate the current status quo of CBIR as regards general photographic collections, or in other words, how well CBIR techniques can, at this stage of research, handle realistic user queries on general still-natural images (in contrast to very specific tasks); it was created to further attract more visually orientated groups to ImageCLEFphoto, which was predominated by participating groups using text-orientated approaches in previous years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Document Collection and Query Topics</head><p>The same document collection was used as with the ImageCLEFphoto task, namely the 20,000 colour photos of the IAPR TC-12 collection, without the corresponding image captions.</p><p>The topic creators selected 30 topics (also from the ImageCLEFphoto task) that were as collection-independent as possible, removing geographic constraints (e.g. "black and white photos" instead of "black and white photos from Russia") and other, non-visual constraints (e.g. "child wearing baseball cap" instead of "godson wearing baseball cap") in order to make them more visual (narrative descriptions for the relevance assessments was adjusted accordingly). Yet, the participants were only allowed to use three images representative for the textual description of each topic 10 . These 30 topics were further classified into three evenly sized groups according to how visual they were estimated to be (the same approach as described in the Visual Features paragraph of section 2.3).</p><p>Based on these findings, the topics were categorized into 10 easy topics that should do well with CBIR techniques (level &gt; 3), 10 hard topics that will be quite difficult for CBIR (level â¤ 2), and 10 medium topics that should lie in between these two categories (2 &lt; level â¤ 3). Table <ref type="table" coords="11,507.96,531.69,4.98,9.96" target="#tab_5">6</ref> displays the title of the visual queries together with the average value of the individual expert judgments and the aforementioned categorisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Participating Groups and Methods</head><p>Two out of 12 groups that participated at the general ImageCLEFphoto task also submitted a total of six runs for the visual subtask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IPAL.</head><p>The IPAL group from Singapore submitted four slightly different runs in which only visual similarities are used: the query images and all the images of the collection were indexed with feature reduction using Latent Semantic Indexing, and the images were then ranked according to their distances to the query images.</p><p>RWTHi6. The RWTHi6 group from the RWTH University Aachen, Germany, submitted two runs to the visual sub-task: one using invariant and tamura texture feature histograms which are compared using JSD, weighing IFH twice as strong as texture features based on the assumption that colour information is more important than texture information for databases of general photographs; the other one using 2048 bin histograms of image patches in colour which are compared according to their colour and texture using JSD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Relevance Judgments and Results</head><p>The relevance judgments were performed as described in Section 2.4: the top 40 results from the six submitted runs were used to create image pools giving an average of 171 images (max: 190; min: 83) to judge per topic. The topic creators judged all images in the topic pools and also heavily used interactive search and judge (ISJ) to supplement the pools with further relevant images.</p><p>Most runs had quite promising results for precision values at a low cut-off (P20 = 0.285 for the best run, compare the results shown in Table <ref type="table" coords="12,304.58,289.17,3.88,9.96" target="#tab_6">7</ref>). However, it is felt that this is due to the fact that some relevant images in the database are visually very similar to the query images, rather than algorithms really understanding what one is searching for. The retrieved images at higher ranks seemed to be quite random and further relevant images were only found by chance, which is also reflected by the quite low MAP scores (0.101 for the best run) and further backs up the aforementioned assumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Discussion</head><p>Many image retrieval systems have recently achieved decent results in retrieval tasks of quite specific domains or in tasks which are purely tailored to the current level of CBIR. The low results of the visual sub-task, however, show that content-based image retrieval is a far cry from actually bridging the semantic gap for visual information retrieval from databases of general, real-life photographs.</p><p>It has to be further investigated with the participants why only two (out of 36 registered) groups actually submitted their results. On the one hand, some groups mentioned in their feedback that they couldn't submit due to lack of time; the generally low results for this task might have also discouraged several groups from submitting their results. On the other hand, there were twice as many groups that submitted purely content-based runs to the main ImageCLEFphoto task; the question might arise whether this visual task has been promoted sufficiently enough and it should further be discussed with participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Object Annotation Task</head><p>After the big success of the automatic medical annotation task from last year <ref type="bibr" coords="12,438.24,581.49,9.91,9.96" target="#b7">[7]</ref>, which clearly showed the need for evaluation challenges in computer vision, and several demands for a similar task in a less specific domain by participants, a plan for a non-medical automatic image classification or annotation task was created. In contrast to the medical task, images to be labeled are of everyday objects and hence do not require The aim of this newly created image annotation task is to identify objects shown in images and label the image accordingly. In contrast to the PASCAL visual object classes challenge<ref type="foot" coords="12,263.76,651.77,7.93,6.97" target="#foot_9">11</ref>  <ref type="bibr" coords="12,275.40,653.25,10.57,9.96" target="#b4">[4]</ref> where several two-class experiments are performed, i.e. independent prediction of presence or absence of various object classes, here several object classes are tackled jointly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Database &amp; Task Description</head><p>LTUtech<ref type="foot" coords="13,128.04,128.33,7.93,6.97" target="#foot_10">12</ref> kindly provided their hand collected dataset of images from 268 classes. Each image of this dataset shows one object in a rather clean environment, i.e. the images show the object and some mostly homogeneous background.</p><p>To facilitate participation in the first year, the number of classes taken into account is considerably lowered to 21 classes. The classes 1) "ashtrays", 2) "backpacks", 3) "balls", 4) "banknotes", 5) "benches", 6) "books", 7) "bottles", 8) "cans", 9) "calculators", 10) "chairs", 11) "clocks", 12) "coins", 13) "computer equipment ", 14) "cups and mugs", 15) "hifi equipment ", 16) "cutlery(knives, forks and spoons)", 17) "plates", 18) "sofas", 19) "tables", 20) "mobile phones", and 21) "wallets" are used. Removing all images that do not belong to one of these classes leads to a database of 81211 images. To create a new set of test data, 1100 new images of objects from these classes were taken. In these images, the objects are in a more "natural setting", i.e. there is more background clutter than in the training images. To simplify the classification task, it is specified in advance that each test image belongs to only one of the 21 classes. Multiple objects of the same class may appear in an image. Objects not belonging to any of the 21 classes may appear as background clutter.</p><p>The training data was released together with 100 randomly sampled test images with known classification to allow for tuning of the systems. At a later date, the remaining 1000 test images were published without their classification as test data.</p><p>The distribution of the classes is not uniform in either of these datasets. An overview of the distribution of the classes is given in Table <ref type="table" coords="13,275.66,356.85,4.98,9.96" target="#tab_7">8</ref> and Figure <ref type="figure" coords="13,333.28,356.85,4.98,9.96">4</ref> gives an example from the training data and from the test data for each of the classes. From these images it can be seen that the task is hard, as the test data contains far more clutter than the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Participating Groups &amp; Methods</head><p>In total, 20 groups registered and 3 of these submitted a total of 8 runs. Here for each group a very short description of the methods of the submitted runs is provided. The groups are listed alphabetically by their group id, which is later used in the results section to refer to the groups.</p><p>CINDI. The CINDI group from Concordia University in Montreal, Canada submitted 4 runs.</p><p>For their experiments they use MPEG7 edge direction histograms and MPEG7 color layout descriptors which are classified by a nearest neighbor classifier and by different combinations of support vector machines. They expect their run SVM-Product to be their best submission.</p><p>DEU. This group from the Department of Computer Engineering of the Dokuz Eylul University in Tinaztepe, Turkey submitted 2 runs. For their experiments they use MPEG7 edge direction histograms and MPEG7 color layout descriptors respectively. For classification, a nearest prototype approach is taken.</p><p>RWTHi6. The Human Language Technology and Pattern Recognition Group from the RWTH Aachen University in Aachen, Germany submitted 2 runs. For image representation they use a bag-of-features approach and for classification a discriminatively trained maximum entropy (loglinear) model is used. The runs differ with respect to the histogram bins and vector quantization methods chosen.</p><p>MedGIFT. The medGIFT group of the University and Hospitals of Geneva submitted three runs to the medical automatic annotation task. One was entirely based on tf/idf weighting of the GNU Image Finding Tool (GIFT) and thus acted as a baseline using only collection frequencies of features with no learning on the training data supplied. The other submission is a combination of several separate runs by voting. The single results were quite different, so the combination-run is expected to be the best submission. The runs were submitted after the evaluation ended and are thus not ranked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>The results of the evaluation are given in Table <ref type="table" coords="15,300.38,169.53,3.90,9.96" target="#tab_8">9</ref>: the runs are sorted by the error rate. Overall, the error rates are very high due to the very hard task: they range from 77.3% to 93.2%, i.e. a large part of the test images could not be classified correctly by any of the methods. Table <ref type="table" coords="15,507.98,193.53,4.98,9.96" target="#tab_8">9</ref> gives details how many images could be classified correctly by how many classifiers. There is no test image that was classified correctly by all classifiers, but 411 images were misclassified by all submitted runs and 301 images could be classified correctly by only one classifier.</p><p>Here too, a combination of classifiers can improve the results: Combining the first two methods by summing up normalized confidences leads to an error rate of 76.7%. Combining the three best submissions leads to an error rate of 75.8%. Adding further submissions could not improve the performance further, and combining all submissions leads to an error rate of 78.8%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussion</head><p>Considering that the error rates of the submitted runs are high and that nearly half of these images could not be classified correctly by any of the submitted methods, it can be said that the the task was very challenging. One aspect that contributes to this outcome is certainly that the training images mainly contain very little clutter and that the test images are images of the objects in their "natural" environment. None of the groups specially addressed this issue although it would be exepcted to lead to improvements. Furthermore, the results show that discriminatively trained methods outperform other methods as in the medical automatic annotation task (although only a small improvement is seen and is probably not statistically significant).</p><p>The object annotation task and the medical automatic annotation task of ImageCLEF 06 [8] are very similar, but differ in some critical aspects:</p><p>â¢ Both tasks provide a relatively large training set and a disjunct test set. Thus, in both cases it is possible to learn a relatively reliable model for the training data (this is somewhat proven for the medical annotation task)</p><p>â¢ Both tasks are multi-class/one object per image classification tasks. Here they differ from the PASCAL visual classes challenge which addresses a set of object vs. non object tasks where several objects (of equal or unequal type) may be contained in an image.</p><p>â¢ The medical annotation task has only gray scale images, whereas the object task has mainly color images. This is probably most relevant for the selection of descriptors.</p><p>â¢ The images from the test and the training set are from the same distribution for the medical task, whereas for the object task, the training images are rather clutter-free and the test images contain a significant amount of clutter. This is probably relevant and should be addressed when developing methods for the non-medical task. Unfortunately, the participating methods did not address this issue which probably has a significant impact on the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>ImageCLEF continues to provide resources to the retrieval and computational vision communities to facilitate standardised laboratory-style testing of (predominately text-based) image retrieval systems. The main division of effort thus far in ImageCLEF has been between medical and nonmedical information systems. These fields have helped to attract different groups to ImageCLEF (and CLEF) over the past 2-3 years and thereby broaden the audience of this evaluation campaign.</p><p>For the retrieval task, the first 2 evaluation events were based on cross-language retrieval from a cultural heritage collection: the St Andrews historic collection of photographic images. This provided certain challenges for both the text and visual retrieval communities, most noticeably the style of language used in the captions and the types of pictures in the collection: mainly black-andwhite of varying levels of quality and visual degradation. For the automatic annotation/object classification task the addition of the LTU dataset has provided a more general challenge to researchers than medical images. For 2006, the retrieval task moved to a new collection based on feedback from ImageCLEF participants in 2005-2006 and the availability of the IAPR-TC12 Benchmark<ref type="foot" coords="16,424.80,181.73,7.93,6.97" target="#foot_11">13</ref> . Designed specifically as a benchmark collection, it is well-suited for use in ImageCLEF with captions in multiple languages and high-quality colour photographs covering a range of topics. This type of collection -personal photographs -is likely to become of increasing interest to researchers with the growth of the desktop search market and popularity of tools such as FlickR 14 .</p><p>Like in previous years, the ImageCLEFphoto task has shown the usefulness of combining visual and textual features derived from the images themselves and associated image captions. It is noticeable that, although some topics are more "visual" than others and likely to benefit more from visual techniques, the majority of topics seem to benefit from a combination of text and visual approaches and participants continue to deal with issues involved in combining this evidence. In addition, the use of relevance feedback to facilitate, for example, query expansion in text retrieval continues to improve the results of many topics in collections used so far, likely due to the nature of the text associated with images: typically a controlled vocabulary that lends itself to blind relevance feedback.</p><p>The object annotation task has shown that current approaches to image classification and/or annotation have problems with test data that is not from the same distribution as the provided training data. Given the current high interest in object recognition and annotation in the computer vision community it is to be expected that big improvements are achievable in the area of automatic image annotation in the near future. It is planned to use image annotation techniques as a preprocessing step for a multi-modal information retrieval system: given an image, create an annotation and use the image and the generated annotation to query a multi-modal information retrieval system, which is likely to improve the results given the much better performance of combined runs in the photographic retrieval task.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,173.88,178.05,255.25,9.96;3,121.44,198.38,85.04,113.39"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sample images from the IAPR TC-12 collection.</figDesc><graphic coords="3,121.44,198.38,85.04,113.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,230.39,546.21,142.22,9.96;3,266.75,576.61,238.11,128.75"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Sample image caption.</figDesc><graphic coords="3,266.75,576.61,238.11,128.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,209.16,369.33,184.81,9.96;5,159.72,389.58,283.47,172.74"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Topic with three sample images.</figDesc><graphic coords="5,159.72,389.58,283.47,172.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,143.76,362.25,312.42,175.44"><head>Table 1 :</head><label>1</label><figDesc>Participating groups.</figDesc><table coords="6,143.76,383.85,312.42,153.84"><row><cell>Group ID</cell><cell>Institution</cell><cell>Runs</cell></row><row><cell>Berkeley</cell><cell>University of California, Berkeley, USA</cell><cell>7</cell></row><row><cell cols="2">CEA-LIC2M Fontenay aux Roses Cedex, France</cell><cell>5</cell></row><row><cell>CELI</cell><cell>CELI srl, Torino, Italy</cell><cell>9</cell></row><row><cell>CINDI</cell><cell>Concordia University, Montreal, Canada</cell><cell>3</cell></row><row><cell>DCU</cell><cell>Dublin City University, Dublin, Ireland</cell><cell>40</cell></row><row><cell>IPAL</cell><cell>IPAL, Singapore</cell><cell>9(+4)</cell></row><row><cell>NII</cell><cell>National Institute of Informatics, Tokyo, Japan</cell><cell>6</cell></row><row><cell>Miracle</cell><cell>Daedalus University, Madrid, Spain</cell><cell>30</cell></row><row><cell>NTU</cell><cell>National Taiwan University, Taipei, Taiwan</cell><cell>30</cell></row><row><cell>RWTH</cell><cell>RWTH Aachen University, Aachen, Germany</cell><cell>2(+2)</cell></row><row><cell>SINAI</cell><cell>University of JaÃ©n, JaÃ©n, Spain</cell><cell>12</cell></row><row><cell>TUC</cell><cell>Technische UniversitÃ¤t Chemnitz, Germany</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,147.00,508.77,309.25,240.47"><head>Table 2 :</head><label>2</label><figDesc>Ad-hoc experiments listed by query and annotation language.</figDesc><table coords="8,174.00,530.40,251.93,218.85"><row><cell>Query Language</cell><cell cols="3">Annotation # Runs # Participants</cell></row><row><cell>English</cell><cell>English</cell><cell>49</cell><cell>11</cell></row><row><cell>Italian</cell><cell>English</cell><cell>15</cell><cell>4</cell></row><row><cell>Japanese</cell><cell>English</cell><cell>10</cell><cell>4</cell></row><row><cell>Simplified Chinese</cell><cell>English</cell><cell>10</cell><cell>3</cell></row><row><cell>French</cell><cell>English</cell><cell>8</cell><cell>4</cell></row><row><cell>Russian</cell><cell>English</cell><cell>8</cell><cell>3</cell></row><row><cell>German</cell><cell>English</cell><cell>7</cell><cell>3</cell></row><row><cell>Spanish</cell><cell>English</cell><cell>7</cell><cell>3</cell></row><row><cell>Portuguese</cell><cell>English</cell><cell>7</cell><cell>3</cell></row><row><cell>Dutch</cell><cell>English</cell><cell>4</cell><cell>2</cell></row><row><cell cols="2">Traditional Chinese English</cell><cell>4</cell><cell>1</cell></row><row><cell>Polish</cell><cell>English</cell><cell>3</cell><cell>1</cell></row><row><cell>Visual</cell><cell>English</cell><cell>1</cell><cell>1</cell></row><row><cell>German</cell><cell>German</cell><cell>8</cell><cell>4</cell></row><row><cell>English</cell><cell>German</cell><cell>6</cell><cell>3</cell></row><row><cell>French</cell><cell>German</cell><cell>3</cell><cell>1</cell></row><row><cell>Japanese</cell><cell>German</cell><cell>1</cell><cell>1</cell></row><row><cell>Visual</cell><cell>(none)</cell><cell>6</cell><cell>3</cell></row><row><cell>Visual Topics</cell><cell>(none)</cell><cell>6</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,96.00,394.05,415.74,178.05"><head>Table 3 :</head><label>3</label><figDesc>System with highest MAP for each language.</figDesc><table coords="9,96.00,421.25,415.74,150.85"><row><cell>Language (Annotation)</cell><cell>Group</cell><cell>Run ID</cell><cell>MAP</cell><cell>P20</cell><cell>GMAP</cell><cell>bpref</cell></row><row><cell>English (English)</cell><cell>CINDI</cell><cell>Cindi Exp RF</cell><cell>0.385</cell><cell>0.530</cell><cell>0.282</cell><cell>0.874</cell></row><row><cell>German (German)</cell><cell>NTU</cell><cell>DE-DE-AUTO-FB-TXTIMG-T-WEprf</cell><cell>0.311</cell><cell>0.335</cell><cell>0.132</cell><cell>0.974</cell></row><row><cell>Portuguese (English)</cell><cell>NTU</cell><cell>PT-EN-AUTO-FB-TXTIMG-T-WEprf</cell><cell>0.285</cell><cell>0.403</cell><cell>0.177</cell><cell>0.755</cell></row><row><cell>T. Chinese (English)</cell><cell>NTU</cell><cell>ZHS-EN-AUTO-FB-TXTIMG-TOnt-WEprf</cell><cell>0.279</cell><cell>0.464</cell><cell>0.154</cell><cell>0.669</cell></row><row><cell>Russian (English)</cell><cell>NTU</cell><cell>RU-EN-AUTO-FB-TXTIMG-T-WEprf</cell><cell>0.279</cell><cell>0.408</cell><cell>0.153</cell><cell>0.755</cell></row><row><cell>Spanish (English)</cell><cell>NTU</cell><cell>SP-EN-AUTO-FB-TXTIMG-T-WEprf</cell><cell>0.278</cell><cell>0.407</cell><cell>0.175</cell><cell>0.757</cell></row><row><cell>French (English)</cell><cell>NTU</cell><cell>FR-EN-AUTO-FB-TXTIMG-T-WEprf</cell><cell>0.276</cell><cell>0.416</cell><cell>0.158</cell><cell>0.750</cell></row><row><cell>Visual (English)</cell><cell>NTU</cell><cell>AUTO-FB-TXTIMG-WEprf</cell><cell>0.276</cell><cell>0.448</cell><cell>0.107</cell><cell>0.657</cell></row><row><cell>S. Chinese (English)</cell><cell>NTU</cell><cell>ZHS-EN-AUTO-FB-TXTIMG-T-WEprf</cell><cell>0.272</cell><cell>0.392</cell><cell>0.168</cell><cell>0.750</cell></row><row><cell>Japanese (English)</cell><cell>NTU</cell><cell>JA-EN-AUTO-FB-TXTIMG-T-WEprf</cell><cell>0.271</cell><cell>0.402</cell><cell>0.170</cell><cell>0.746</cell></row><row><cell>Italian (English)</cell><cell>NTU</cell><cell>IT-EN-AUTO-FB-TXTIMG-T-WEprf</cell><cell>0.262</cell><cell>0.398</cell><cell>0.143</cell><cell>0.722</cell></row><row><cell>German (English)</cell><cell>DCU</cell><cell>combTextVisual DEENEN</cell><cell>0.189</cell><cell>0.258</cell><cell>0.070</cell><cell>0.683</cell></row><row><cell>Dutch (English)</cell><cell>DCU</cell><cell>combTextVisual NLENEN</cell><cell>0.184</cell><cell>0.234</cell><cell>0.063</cell><cell>0.640</cell></row><row><cell>English (German)</cell><cell>DCU</cell><cell>combTextVisual ENDEEN</cell><cell>0.122</cell><cell>0.175</cell><cell>0.036</cell><cell>0.524</cell></row><row><cell>Polish (English)</cell><cell>Miracle</cell><cell>miratctdplen</cell><cell>0.108</cell><cell>0.139</cell><cell>0.005</cell><cell>0.428</cell></row><row><cell>French (German)</cell><cell>DCU</cell><cell>combTextVisual FRDEEN</cell><cell>0.104</cell><cell>0.147</cell><cell>0.002</cell><cell>0.245</cell></row><row><cell>Visual (none)</cell><cell>RWTHi6</cell><cell>RWTHi6-IFHTAM</cell><cell>0.063</cell><cell>0.182</cell><cell>0.022</cell><cell>0.366</cell></row><row><cell>Japanese (German)</cell><cell>NII</cell><cell>mcp.bl.jpn tger td.skl dir</cell><cell>0.032</cell><cell>0.051</cell><cell>0.001</cell><cell>0.172</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,102.24,118.29,395.38,150.83"><head>Table 4 :</head><label>4</label><figDesc>MAP scores for each result dimension.</figDesc><table coords="10,102.24,138.00,395.38,131.13"><row><cell>Dimension</cell><cell>Type</cell><cell cols="2"># Runs # Groups</cell><cell>Mean (Ï)</cell><cell cols="2">Median Highest</cell></row><row><cell>Query Language</cell><cell>bilingual</cell><cell>93</cell><cell cols="2">8 0.144 (0.074)</cell><cell>0.143</cell><cell>0.285</cell></row><row><cell></cell><cell>monolingual</cell><cell>57</cell><cell cols="2">11 0.154 (0.090)</cell><cell>0.145</cell><cell>0.385</cell></row><row><cell></cell><cell>visual</cell><cell>7</cell><cell cols="2">3 0.074 (0.090)</cell><cell>0.047</cell><cell>0.276</cell></row><row><cell>Annotation</cell><cell>English</cell><cell>133</cell><cell cols="2">11 0.152 (0.082)</cell><cell>0.151</cell><cell>0.385</cell></row><row><cell></cell><cell>German</cell><cell>18</cell><cell cols="2">4 0.121 (0.070)</cell><cell>0.114</cell><cell>0.311</cell></row><row><cell></cell><cell>none</cell><cell>6</cell><cell cols="2">2 0.041 (0.016)</cell><cell>0.042</cell><cell>0.063</cell></row><row><cell>Modality</cell><cell>Text Only</cell><cell>108</cell><cell cols="2">11 0.129 (0.062)</cell><cell>0.136</cell><cell>0.375</cell></row><row><cell></cell><cell>Text + Image</cell><cell>43</cell><cell cols="2">7 0.199 (0.077)</cell><cell>0.186</cell><cell>0.385</cell></row><row><cell></cell><cell>Image Only</cell><cell>6</cell><cell cols="2">2 0.041 (0.016)</cell><cell>0.042</cell><cell>0.063</cell></row><row><cell cols="2">Feedback/Expansion without</cell><cell>85</cell><cell cols="2">11 0.128 (0.055)</cell><cell>0.136</cell><cell>0.334</cell></row><row><cell></cell><cell>with</cell><cell>72</cell><cell cols="2">8 0.165 (0.090)</cell><cell>0.171</cell><cell>0.385</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="18,119.40,181.53,361.61,506.62"><head>Table 5 :</head><label>5</label><figDesc>ImageCLEFphoto topics and average score across all submissions.</figDesc><table coords="18,119.40,202.61,361.61,485.53"><row><cell>ID</cell><cell>Topic Title</cell><cell>#Rel</cell><cell>AVG P20</cell><cell>AVG MAP</cell></row><row><cell>1</cell><cell>accommodation with swimming pool</cell><cell>35</cell><cell>0.3157</cell><cell>0.2208</cell></row><row><cell>2</cell><cell>church with more than two towers</cell><cell>27</cell><cell>0.0451</cell><cell>0.0550</cell></row><row><cell>3</cell><cell>religious statue in the foreground</cell><cell>32</cell><cell>0.1466</cell><cell>0.0812</cell></row><row><cell>4</cell><cell>group standing in front of mountain landscape in Patagonia</cell><cell>68</cell><cell>0.0136</cell><cell>0.0070</cell></row><row><cell>5</cell><cell>animal swimming</cell><cell>64</cell><cell>0.1340</cell><cell>0.0537</cell></row><row><cell>6</cell><cell>straight road in the USA</cell><cell>84</cell><cell>0.1380</cell><cell>0.1104</cell></row><row><cell>7</cell><cell>group standing in salt pan</cell><cell>49</cell><cell>0.3472</cell><cell>0.2083</cell></row><row><cell>8</cell><cell>host families posing for a photo</cell><cell>74</cell><cell>0.3198</cell><cell>0.2174</cell></row><row><cell>9</cell><cell>tourist accommodation near Lake Titicaca</cell><cell>13</cell><cell>0.0031</cell><cell>0.0027</cell></row><row><cell>10</cell><cell>destinations in Venezuela</cell><cell>36</cell><cell>0.3043</cell><cell>0.3013</cell></row><row><cell>11</cell><cell>black and white photos of Russia</cell><cell>65</cell><cell>0.1386</cell><cell>0.1252</cell></row><row><cell>12</cell><cell>people observing football match</cell><cell>31</cell><cell>0.1201</cell><cell>0.1097</cell></row><row><cell>13</cell><cell>exterior view of school building</cell><cell>72</cell><cell>0.2037</cell><cell>0.0907</cell></row><row><cell>14</cell><cell>scenes of footballers in action</cell><cell>34</cell><cell>0.2929</cell><cell>0.2629</cell></row><row><cell>15</cell><cell>night shots of cathedrals</cell><cell>23</cell><cell>0.3432</cell><cell>0.2924</cell></row><row><cell>16</cell><cell>people in San Francisco</cell><cell>54</cell><cell>0.2235</cell><cell>0.1714</cell></row><row><cell>17</cell><cell>lighthouses at the sea</cell><cell>27</cell><cell>0.3420</cell><cell>0.2751</cell></row><row><cell>18</cell><cell>sport stadium outside Australia</cell><cell>49</cell><cell>0.1870</cell><cell>0.1178</cell></row><row><cell>19</cell><cell>exterior view of sport stadia</cell><cell>57</cell><cell>0.1636</cell><cell>0.0922</cell></row><row><cell>20</cell><cell>close-up photograph of an animal</cell><cell>73</cell><cell>0.0559</cell><cell>0.0115</cell></row><row><cell>21</cell><cell>accommodation provided by host families</cell><cell>70</cell><cell>0.1963</cell><cell>0.1386</cell></row><row><cell>22</cell><cell>tennis player during rally</cell><cell>92</cell><cell>0.4377</cell><cell>0.4589</cell></row><row><cell>23</cell><cell>sport photos from California</cell><cell>75</cell><cell>0.1525</cell><cell>0.0662</cell></row><row><cell>24</cell><cell>snowcapped buildings in Europe</cell><cell>62</cell><cell>0.1068</cell><cell>0.0901</cell></row><row><cell>25</cell><cell>people with a flag</cell><cell>63</cell><cell>0.1861</cell><cell>0.1086</cell></row><row><cell>26</cell><cell>godson with baseball cap</cell><cell>79</cell><cell>0.1256</cell><cell>0.0664</cell></row><row><cell>27</cell><cell>motorcyclists racing at the Australian Motorcycle Grand Prix</cell><cell>30</cell><cell>0.2827</cell><cell>0.3025</cell></row><row><cell>28</cell><cell>cathedrals in Ecuador</cell><cell>41</cell><cell>0.2599</cell><cell>0.1195</cell></row><row><cell>29</cell><cell>views of Sydney's world-famous landmarks</cell><cell>40</cell><cell>0.1741</cell><cell>0.1837</cell></row><row><cell>30</cell><cell>room with more than two beds</cell><cell>25</cell><cell>0.0312</cell><cell>0.0290</cell></row><row><cell>31</cell><cell>volcanos around Quito</cell><cell>58</cell><cell>0.1491</cell><cell>0.0519</cell></row><row><cell>32</cell><cell>photos of female guides</cell><cell>26</cell><cell>0.1401</cell><cell>0.1065</cell></row><row><cell>33</cell><cell>people on surfboards</cell><cell>50</cell><cell>0.2000</cell><cell>0.1330</cell></row><row><cell>34</cell><cell>group pictures on a beach</cell><cell>77</cell><cell>0.2608</cell><cell>0.1092</cell></row><row><cell>35</cell><cell>bird flying</cell><cell>88</cell><cell>0.5704</cell><cell>0.3001</cell></row><row><cell>36</cell><cell>photos with Machu Picchu in the background</cell><cell>105</cell><cell>0.3765</cell><cell>0.2393</cell></row><row><cell>37</cell><cell>sights along the Inka-Trail</cell><cell>92</cell><cell>0.1910</cell><cell>0.0738</cell></row><row><cell>38</cell><cell>Machu Picchu and Huayna Picchu in bad weather</cell><cell>23</cell><cell>0.1077</cell><cell>0.0852</cell></row><row><cell>39</cell><cell>people in bad weather</cell><cell>72</cell><cell>0.0333</cell><cell>0.0097</cell></row><row><cell>40</cell><cell>tourist destinations in bad weather</cell><cell>104</cell><cell>0.0623</cell><cell>0.0157</cell></row><row><cell>41</cell><cell>winter landscape in South America</cell><cell>135</cell><cell>0.0367</cell><cell>0.0090</cell></row><row><cell>42</cell><cell>pictures taken on Ayers Rock</cell><cell>45</cell><cell>0.2478</cell><cell>0.2622</cell></row><row><cell>43</cell><cell>sunset over water</cell><cell>40</cell><cell>0.2210</cell><cell>0.1472</cell></row><row><cell>44</cell><cell>mountains on mainland Australia</cell><cell>160</cell><cell>0.1750</cell><cell>0.1093</cell></row><row><cell>45</cell><cell>South American meat dishes</cell><cell>41</cell><cell>0.2096</cell><cell>0.1222</cell></row><row><cell>46</cell><cell>Asian women and/or girls</cell><cell>41</cell><cell>0.2710</cell><cell>0.1291</cell></row><row><cell>47</cell><cell>photos of heavy traffic in Asia</cell><cell>35</cell><cell>0.0645</cell><cell>0.0392</cell></row><row><cell>48</cell><cell>vehicle in South Korea</cell><cell>33</cell><cell>0.0750</cell><cell>0.0704</cell></row><row><cell>49</cell><cell>images of typical Australian animals</cell><cell>99</cell><cell>0.1123</cell><cell>0.0810</cell></row><row><cell>50</cell><cell>indoor photos of churches or cathedrals</cell><cell>36</cell><cell>0.2988</cell><cell>0.1866</cell></row><row><cell>51</cell><cell>photos of goddaughters from Brazil</cell><cell>29</cell><cell>0.0355</cell><cell>0.0634</cell></row><row><cell>52</cell><cell>sports people with prizes</cell><cell>29</cell><cell>0.1392</cell><cell>0.0901</cell></row><row><cell>53</cell><cell>views of walls with unsymmetric stones</cell><cell>44</cell><cell>0.2941</cell><cell>0.2257</cell></row><row><cell>54</cell><cell>famous television (and telecommunication) towers</cell><cell>18</cell><cell>0.1210</cell><cell>0.1418</cell></row><row><cell>55</cell><cell>drawings in Peruvian deserts</cell><cell>81</cell><cell>0.2361</cell><cell>0.0958</cell></row><row><cell>56</cell><cell>photos of oxidised vehicles</cell><cell>28</cell><cell>0.0877</cell><cell>0.0676</cell></row><row><cell>57</cell><cell>photos of radio telescopes</cell><cell>10</cell><cell>0.3006</cell><cell>0.5161</cell></row><row><cell>58</cell><cell>seals near water</cell><cell>56</cell><cell>0.4216</cell><cell>0.2222</cell></row><row><cell>59</cell><cell>creative group pictures in Uyuni</cell><cell>24</cell><cell>0.0627</cell><cell>0.0532</cell></row><row><cell>60</cell><cell>salt heaps in salt pan</cell><cell>28</cell><cell>0.4040</cell><cell>0.2952</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="19,134.76,157.89,333.64,360.59"><head>Table 6 :</head><label>6</label><figDesc>The visual topics and the three categories: easy, medium and hard.</figDesc><table coords="19,171.36,179.64,257.19,338.85"><row><cell>ID Topic Title</cell><cell>Level</cell></row><row><cell>82 sunset over water</cell><cell>4.75</cell></row><row><cell>66 black and white photos</cell><cell>4.25</cell></row><row><cell>88 drawings in deserts</cell><cell>4.00</cell></row><row><cell>71 tennis player on tennis court</cell><cell>3.75</cell></row><row><cell>78 bird flying</cell><cell>3.25</cell></row><row><cell>85 photos of dark-skinned girls</cell><cell>3.25</cell></row><row><cell>86 views of walls with asymmetric stones</cell><cell>3.25</cell></row><row><cell>68 night shots of cathedrals</cell><cell>3.25</cell></row><row><cell>64 straight road</cell><cell>3.25</cell></row><row><cell>72 snowcapped buildings</cell><cell>3.25</cell></row><row><cell>67 scenes of footballers in action</cell><cell>3.00</cell></row><row><cell>74 motorcyclists riding on racing track</cell><cell>3.00</cell></row><row><cell>76 people on surfboards</cell><cell>3.00</cell></row><row><cell>63 animal swimming</cell><cell>2.75</cell></row><row><cell>69 lighthouses at the sea</cell><cell>2.75</cell></row><row><cell>77 group pictures on a beach</cell><cell>2.75</cell></row><row><cell>81 winter landscape</cell><cell>2.75</cell></row><row><cell>90 salt heaps in salt pan</cell><cell>2.75</cell></row><row><cell>65 group standing in salt pan</cell><cell>2.50</cell></row><row><cell>84 indoor photos of churches or cathedrals</cell><cell>2.25</cell></row><row><cell>79 photos with Machu Picchu in the background</cell><cell>2.00</cell></row><row><cell cols="2">80 Machu Picchu and Huayna Picchu in bad weather 2.00</cell></row><row><cell>62 group in front of mountain landscape</cell><cell>2.00</cell></row><row><cell>70 close-up photograph of an animal</cell><cell>2.00</cell></row><row><cell>83 images of typical Australian animals</cell><cell>1.75</cell></row><row><cell>87 television and telecommunication towers</cell><cell>1.75</cell></row><row><cell>89 photos of oxidised vehicles</cell><cell>1.75</cell></row><row><cell>73 child wearing baseball cap</cell><cell>1.50</cell></row><row><cell>75 exterior view of churches or cathedrals</cell><cell>1.50</cell></row><row><cell>61 church with more than two towers</cell><cell>1.25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="19,156.48,616.53,286.93,94.91"><head>Table 7 :</head><label>7</label><figDesc>The visual results.</figDesc><table coords="19,156.48,636.24,286.93,75.21"><row><cell cols="2">RK RUN ID</cell><cell>MAP</cell><cell>P20</cell><cell cols="2">BPREF GMAP</cell></row><row><cell>1</cell><cell>RWTHi6-IFHTAM</cell><cell cols="2">0.1010 0.2850</cell><cell>0.4307</cell><cell>0.0453</cell></row><row><cell>2</cell><cell>RWTHi6-PatchHisto</cell><cell cols="2">0.0706 0.2217</cell><cell>0.3831</cell><cell>0.0317</cell></row><row><cell>3</cell><cell cols="3">IPAL-LSA3-VisualTopics 0.0596 0.1717</cell><cell>0.3360</cell><cell>0.0281</cell></row><row><cell>4</cell><cell cols="3">IPAL-LSA2-VisualTopics 0.0501 0.1800</cell><cell>0.3093</cell><cell>0.0218</cell></row><row><cell>5</cell><cell cols="3">IPAL-LSA1-VisualTopics 0.0501 0.1650</cell><cell>0.3123</cell><cell>0.0236</cell></row><row><cell>6</cell><cell>IPAL-MF-VisualTopics</cell><cell cols="2">0.0291 0.1417</cell><cell>0.2374</cell><cell>0.0119</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="20,169.20,400.17,264.77,9.96"><head>Table 8 :</head><label>8</label><figDesc>Overview of the data of the object annotation task.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="20,148.80,435.69,305.54,141.60"><head>Table 9 :</head><label>9</label><figDesc>Results from the object annotation task sorted by error rate.</figDesc><table coords="20,184.44,447.45,230.98,129.84"><row><cell cols="2">rank Group ID Runtag</cell><cell>Error rate</cell></row><row><cell>1 RWTHi6</cell><cell>SHME</cell><cell>77.3</cell></row><row><cell>2 RWTHi6</cell><cell>PatchHisto</cell><cell>80.2</cell></row><row><cell>3 cindi</cell><cell>Cindi-SVM-Product</cell><cell>83.2</cell></row><row><cell>4 cindi</cell><cell>Cindi-SVM-EHD</cell><cell>85.0</cell></row><row><cell>5 cindi</cell><cell>Cindi-SVM-SUM</cell><cell>85.2</cell></row><row><cell>6 cindi</cell><cell>Cindi-Fusion-knn</cell><cell>87.1</cell></row><row><cell>7 DEU-CS</cell><cell>edgehistogr-centroid</cell><cell>88.2</cell></row><row><cell cols="2">-medGIFT fw-bwpruned</cell><cell>90.5</cell></row><row><cell cols="2">-medGIFT baseline</cell><cell>91.7</cell></row><row><cell>8 DEU-CS</cell><cell>colorlayout-centroid</cell><cell>93.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="20,90.00,605.01,423.00,141.60"><head>Table 10 :</head><label>10</label><figDesc>The number of test images that were correctly classified by the specified number of runs number of number of runs in which</figDesc><table coords="20,233.28,628.65,148.08,117.96"><row><cell>images</cell><cell>correctly classified</cell></row><row><cell>411</cell><cell>0</cell></row><row><cell>301</cell><cell>1</cell></row><row><cell>120</cell><cell>2</cell></row><row><cell>69</cell><cell>3</cell></row><row><cell>54</cell><cell>4</cell></row><row><cell>30</cell><cell>5</cell></row><row><cell>13</cell><cell>6</cell></row><row><cell>2</cell><cell>7</cell></row><row><cell>0</cell><cell>8</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,105.24,698.51,88.19,7.35"><p>http://trec.nist.gov/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,105.24,707.99,122.02,7.35"><p>http://www-clef-campaign.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,105.24,717.47,83.99,7.35"><p>http://www.iapr.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,105.24,726.95,96.59,7.35"><p>http://www.viventura.de</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="4,105.24,712.70,325.36,7.97"><p>Log file taken between 1st February and 15th April 2006 containing 980 unique queries.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="4,105.24,722.18,407.63,7.97;4,90.00,731.66,395.42,7.97"><p>We asked experts in the field to rate these topics according to the following scheme: (1) CBIR will produce very bad or random results, (2) bad results, (3) average results, (4) good results and (5) very good results.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="4,105.24,741.14,224.54,7.97"><p>The FIRE system was used based on using all query images.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="6,105.24,742.10,251.22,7.97"><p>One of the topic generators is part of the viventura travel company.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="9,105.24,740.15,213.12,7.35"><p>http://trec.nist.gov/trec eval/trec eval.7.3.tar.gz</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_9" coords="12,105.24,696.59,190.26,7.35"><p>http://www.pascal-network.org/challenges/VOC/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_10" coords="13,105.24,741.47,92.39,7.35"><p>http://www.ltutech.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_11" coords="16,105.24,736.94,393.03,7.97"><p>One of the biggest factors influencing what collections are used and provided by ImageCLEF is copyright.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank the CLEF campaign for supporting the ImageCLEF initiative. Furthermore, special thanks go to viventura, the IAPR and <rs type="institution">LTUtech</rs> for providing their image databases for this years' tasks, and to <rs type="person">Tobias Weyand</rs> for creating the web interface for submissions.</p><p>This work was partially funded by the <rs type="funder">DFG (Deutsche Forschungsgemeinschaft)</rs> under contracts <rs type="grantNumber">NE-572/6</rs> and <rs type="grantNumber">Le-1108/4</rs>, the <rs type="funder">Swiss National Science Foundation (FNS)</rs> under contract <rs type="grantNumber">205321-109304/1</rs>, the <rs type="funder">American National Science Foundation (NSF)</rs> with grant <rs type="grantNumber">ITR-0325160</rs>, an <rs type="grantName">International Postgraduate Research Scholarship</rs> (IPRS) by <rs type="funder">Victoria University</rs>, and the <rs type="programName">EU Sixth Framework Program</rs> with the <rs type="projectName">SemanticMining</rs> project (<rs type="grantNumber">IST NoE 507505</rs>) and the <rs type="funder">MUSCLE NoE</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_DZKmhXf">
					<idno type="grant-number">NE-572/6</idno>
				</org>
				<org type="funding" xml:id="_XAfWtPH">
					<idno type="grant-number">Le-1108/4</idno>
				</org>
				<org type="funding" xml:id="_gFp4Xhc">
					<idno type="grant-number">205321-109304/1</idno>
				</org>
				<org type="funded-project" xml:id="_BvrFG4m">
					<idno type="grant-number">ITR-0325160</idno>
					<orgName type="grant-name">International Postgraduate Research Scholarship</orgName>
					<orgName type="project" subtype="full">SemanticMining</orgName>
					<orgName type="program" subtype="full">EU Sixth Framework Program</orgName>
				</org>
				<org type="funding" xml:id="_fX5g5vQ">
					<idno type="grant-number">IST NoE 507505</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="14,155.52,188.01,55.17,9.96;14,270.60,180.33,62.12,9.96;14,400.92,188.25,38.13,9.96;14,151.92,261.33,62.37,9.96;14,279.84,261.33,43.33,9.96;14,385.44,252.21,69.32,9.96;14,160.32,318.57,47.48,9.96;14,267.36,331.05,66.09,9.96;14,398.64,318.57,38.25,9.96;14,158.04,391.89,50.01,9.96;14,274.44,398.37,50.24,9.96;14,394.92,399.09,46.05,9.96;14,150.36,465.09,65.35,9.96;14,279.60,469.41,43.77,9.96;14,399.48,471.45,40.93,9.96;14,164.64,540.21,51.09,9.96;14,260.28,526.05,82.54,9.96;14,388.68,536.13,48.45,9.96;14,159.36,601.17,44.13,9.96;14,275.04,599.25,49.65,9.96;14,391.44,599.97,53.73,9.96;14,90.00,660.57,28.20,9.96" xml:id="b0">
	<monogr>
		<title level="m" coord="14,155.52,188.01,55.17,9.96;14,270.60,180.33,62.12,9.96;14,400.92,188.25,38.13,9.96;14,151.92,261.33,62.37,9.96;14,279.84,261.33,43.33,9.96;14,385.44,252.21,69.32,9.96;14,160.32,318.57,47.48,9.96;14,267.36,331.05,66.09,9.96;14,398.64,318.57,38.25,9.96;14,158.04,391.89,50.01,9.96;14,274.44,398.37,50.24,9.96;14,394.92,399.09,46.05,9.96;14,150.36,465.09,65.35,9.96;14,279.60,469.41,43.77,9.96;14,399.48,471.45,40.93,9.96;14,164.64,540.21,51.09,9.96;14,260.28,526.05,82.54,9.96;14,388.68,536.13,48.45,9.96;14,159.36,601.17,44.13,9.96;14,275.04,599.25,49.65,9.96;14,391.44,599.97,53.73,9.96;14,90.00,660.57,28.20,9.96">1 -Ashtrays 2 -Backpacks 3 -Balls 4 -Banknotes 5 -Bench 6 -Bookshelves 7 -Bottles 8 -Calculators 9 -Cans 10 -Chairs 11 -Clocks 12 -Coins 13 -Computer 14 -Cups 15 -HiFi 16 -Knives 17 -MobilePhones 18 -Plates 19 -Sofas 20 -Tables 21 -Wallets Figure</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="16,110.52,638.97,402.52,9.96;16,110.52,650.97,402.53,9.96;16,110.52,662.85,402.51,9.96;16,110.52,674.85,25.92,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="16,282.03,638.97,212.87,9.96">Retrieval evaluation with incomplete information</title>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,110.52,650.97,402.53,9.96;16,110.52,662.85,181.46,9.96">SIGIR &apos;04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,110.52,694.41,402.57,9.96;16,110.52,706.29,402.62,9.96;16,110.52,718.29,144.75,9.96" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="16,207.26,694.41,305.83,9.96;16,110.52,706.29,166.81,9.96">Report on the testing and analysis of an investigation into the comparative efficiency of indexing systems</title>
		<author>
			<persName coords=""><forename type="first">Cyril</forename><forename type="middle">W</forename><surname>Cleverdon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1962-09">September 1962</date>
			<pubPlace>Cranfield, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Aslib Cranfield Research Project</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="17,110.52,111.33,402.95,9.96;17,110.52,123.33,402.52,9.96;17,110.52,135.21,402.62,9.96;17,110.52,147.21,402.49,9.96;17,110.52,159.21,252.90,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="17,345.08,111.33,168.39,9.96;17,110.52,123.33,180.10,9.96">Overview of the CLEF cross-language image retrieval track (ImageCLEF) 2004</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,372.36,135.21,140.78,9.96;17,110.52,147.21,322.61,9.96">Multilingual Information Access for Text, Speech and Images: Result of the fifth CLEF evaluation campaign</title>
		<title level="s" coord="17,441.12,147.21,71.89,9.96;17,110.52,159.21,77.08,9.96">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julio</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><surname>Magnini</surname></persName>
		</editor>
		<meeting><address><addrLine>Bath, England</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,110.52,179.13,402.72,9.96;17,110.52,191.01,402.49,9.96;17,110.52,203.01,402.63,9.96;17,110.52,215.01,402.48,9.96;17,110.52,226.89,402.69,9.96;17,110.52,238.89,402.49,9.96;17,110.52,250.89,402.27,9.96;17,110.52,262.77,402.43,9.96;17,110.52,274.77,402.51,9.96;17,110.52,286.65,274.21,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="17,253.00,250.89,202.10,9.96">The 2005 pascal visual object classes challenge</title>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luc</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Moray</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olivier</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Navneet</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gyuri</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Dorko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><surname>Duffner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">D R</forename><surname>Eichhorn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mario</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christophe</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tom</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Frederic</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Markus</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jorma</forename><surname>Koskela</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diane</forename><surname>Laaksonen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bastian</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hongying</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hermann</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernt</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cordelia</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edgar</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Seemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amos</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sandor</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bill</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilkay</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ville</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianguo</forename><surname>Viitaniemi</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,476.04,250.89,36.75,9.96;17,110.52,262.77,402.43,9.96;17,110.52,274.77,248.74,9.96">Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment (PASCAL Workshop 05)</title>
		<title level="s" coord="17,439.82,274.77,73.21,9.96;17,110.52,286.65,89.85,9.96">Lecture Notes in Artificial Intelligence</title>
		<meeting><address><addrLine>Southampton, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3944</biblScope>
			<biblScope unit="page" from="117" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,110.52,306.57,402.63,9.96;17,110.52,318.57,402.51,9.96;17,110.52,330.57,402.46,9.96;17,110.52,342.45,285.39,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="17,439.54,306.57,73.61,9.96;17,110.52,318.57,317.64,9.96">The IAPR-TC12 benchmark: A new evaluation resource for visual information systems</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Deselears</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,456.12,318.57,56.91,9.96;17,110.52,330.57,402.46,9.96;17,110.52,342.45,113.41,9.96">International Workshop OntoImage&apos;2006 Language Resources for Content-Based Image Retrieval, held in conjunction with LREC&apos;06</title>
		<meeting><address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-05">May 2006</date>
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,110.52,362.37,402.47,9.96;17,110.52,374.37,402.33,9.96;17,110.52,386.37,201.86,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="17,346.38,362.37,166.61,9.96;17,110.52,374.37,139.18,9.96">Linguistic estimation of topic difficulty in cross-language image retrieval</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Clement</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,268.68,374.37,244.17,9.96;17,110.52,386.37,50.39,9.96">CLEF 2005: Overview of the Cross Language Evaluation Forum 2005</title>
		<imprint>
			<date type="published" when="2006-09">September 2006</date>
		</imprint>
	</monogr>
	<note>page to appear</note>
</biblStruct>

<biblStruct coords="17,110.52,406.29,402.49,9.96;17,110.52,418.17,402.51,9.96;17,110.52,430.17,351.40,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="17,495.87,406.29,17.15,9.96;17,110.52,418.17,225.10,9.96">The Use of medGIFT and easyIR for ImageCLEF 2005</title>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antoine</forename><surname>Geissbuhler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johan</forename><surname>Marty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Lovis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Ruch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,360.24,418.17,152.79,9.96;17,110.52,430.17,100.43,9.96">Proceedings of the Cross Language Evaluation Forum 2005</title>
		<meeting>the Cross Language Evaluation Forum 2005<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>LNCS</publisher>
			<date type="published" when="2006-09">September 2006</date>
		</imprint>
	</monogr>
	<note>page in press</note>
</biblStruct>

<biblStruct coords="17,110.52,450.09,402.51,9.96;17,110.52,462.09,402.38,9.96;17,110.52,473.97,173.43,9.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="17,110.52,462.09,319.27,9.96">Overview of the imageclefmed 2006 medical retrieval and annotation tasks</title>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,449.52,462.09,63.38,9.96;17,110.52,473.97,21.21,9.96">CLEF working notes</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09">September 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,110.52,493.89,402.51,9.96;17,110.52,505.89,392.06,9.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="17,236.05,493.89,234.92,9.96">Cross language system evaluation: The clef campaigns</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,479.88,493.89,33.15,9.96;17,110.52,505.89,279.75,9.96">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1067" to="1072" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,110.52,525.81,372.85,9.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="17,197.66,525.81,131.44,9.96">The trec robust retrieval track</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,337.56,525.81,57.91,9.96">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="20" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,110.52,545.73,402.59,9.96;17,110.52,557.73,402.40,9.96;17,110.52,569.61,69.97,9.96" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donna</forename><surname>Harmann</surname></persName>
		</author>
		<title level="m" coord="17,290.32,545.73,222.79,9.96;17,110.52,557.73,43.64,9.96;17,175.80,557.73,167.49,9.96">Overview of the seventh Text REtrieval Conference (TREC-7)</title>
		<meeting><address><addrLine>Gaithersburg, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-11">November 1998</date>
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
	<note>The Seventh Text Retrieval Conference</note>
</biblStruct>

<biblStruct coords="17,110.52,589.53,402.59,9.96;17,110.52,601.53,402.42,9.96;17,110.52,613.53,200.31,9.96" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="17,399.83,589.53,113.28,9.96;17,110.52,601.53,38.03,9.96">Geomodification in query rewriting</title>
		<author>
			<persName coords=""><forename type="first">Vivian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eugene</forename><surname>Stipp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rosie</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,172.32,601.53,340.62,9.96;17,110.52,613.53,50.14,9.96">GIR &apos;06: Proceedings of the Workshop on Geographic Information Retrieval, SIGIR 2006</title>
		<imprint>
			<date type="published" when="2006-08-10">August 10 2006</date>
		</imprint>
	</monogr>
	<note>page to appear</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
