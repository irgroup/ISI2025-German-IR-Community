<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,95.40,98.73,411.96,15.51;1,198.84,120.69,205.29,15.51">IPAL Knowledge-based Medical Image Retrieval in ImageCLEFmed 2006</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,109.44,154.07,71.05,9.96"><forename type="first">Caroline</forename><surname>Lacoste</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IPAL French-Singaporean Joint Lab (I2R</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">NUS</orgName>
								<orgName type="institution" key="instit3">UJF)</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,188.20,154.07,91.92,9.96"><forename type="first">Jean-Pierre</forename><surname>Chevallet</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IPAL French-Singaporean Joint Lab (I2R</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">NUS</orgName>
								<orgName type="institution" key="instit3">UJF)</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,287.70,154.07,60.33,9.96"><forename type="first">Joo-Hwee</forename><surname>Lim</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IPAL French-Singaporean Joint Lab (I2R</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">NUS</orgName>
								<orgName type="institution" key="instit3">UJF)</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,356.27,154.07,43.61,9.96"><forename type="first">Xiong</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IPAL French-Singaporean Joint Lab (I2R</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">NUS</orgName>
								<orgName type="institution" key="instit3">UJF)</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,408.07,154.07,80.33,9.96"><forename type="first">Daniel</forename><surname>Raccoceanu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IPAL French-Singaporean Joint Lab (I2R</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">NUS</orgName>
								<orgName type="institution" key="instit3">UJF)</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,166.08,168.11,37.05,9.96"><forename type="first">Diem</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IPAL French-Singaporean Joint Lab (I2R</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">NUS</orgName>
								<orgName type="institution" key="instit3">UJF)</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,206.37,168.11,44.50,9.96"><forename type="first">Thi</forename><surname>Hoang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IPAL French-Singaporean Joint Lab (I2R</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">NUS</orgName>
								<orgName type="institution" key="instit3">UJF)</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,259.25,168.11,83.04,9.96"><forename type="first">Roxana</forename><surname>Teodorescu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IPAL French-Singaporean Joint Lab (I2R</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">NUS</orgName>
								<orgName type="institution" key="instit3">UJF)</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,350.11,168.11,86.56,9.96"><forename type="first">Nicolas</forename><surname>Vuillenemot</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IPAL French-Singaporean Joint Lab (I2R</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">NUS</orgName>
								<orgName type="institution" key="instit3">UJF)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,95.40,98.73,411.96,15.51;1,198.84,120.69,205.29,15.51">IPAL Knowledge-based Medical Image Retrieval in ImageCLEFmed 2006</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D820FAE0527B8D8299798D4516A33456</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing-Indexing methods, Thesauruses</term>
					<term>H.3.3 Information Search and Retrieval-Retrieval Models, Information filtering</term>
					<term>H.2 [Database Management]: H.2.4 System-Multimedia Database Measurement, Performance, Experimentation Indexing methods, Thesauruses, Retrieval Models, Information filtering, Multimedia Database</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the contribution of IPAL group on the CLEF 2006 medical retrieval task (i.e. ImageCLEFmed). The main idea of our group is to incorporate medical knowledge in the retrieval system within a multimodal fusion framework. For text, this knowledge is in the Unified Medical Language System (UMLS) sources. For images, this knowledge is in semantic features that are learned from examples within structured learning framework. We propose to represent both image and text using UMLS concepts. The use of UMLS concepts allows the system to work at a higher semantic level and to standardize the semantic index of medical data, facilitating the communication between visual end textual indexing and retrieval. The results obtained with UMLS-based approaches show the potential of this conceptual indexing, especially when using a semantic dimension filtering, and the benefit of working within a fusion framework, leading to the best results of ImageCLEFmed 2006. We also test a visual retrieval system based on manual query design and visual task fusion. Even if it provides the best visual results, this purely visual retrieval provides poor results in comparison to the best textual approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Besides the ever-growing amount of medical data produced everyday, medical image retrieval systems have a large potential in medical applications. The three main applications concern medical diagnosis, teaching, and research. For the clinical decision making process, it can be beneficial to find other images of the same modality, of the same anatomic region, and of the same disease <ref type="bibr" coords="1,123.93,681.59,14.58,9.96" target="#b15">[16]</ref>. Hence, medical CBIR systems can assist doctors in diagnosis by retrieving images with known pathologies that are similar to a patient's image(s). In teaching and research, visual retrieval methods could help researchers, lecturers, and student find relevant images from large repositories. Visual features not only allow the retrieval of cases with patients having similar diagnoses but also cases with visual similarity but different diagnoses.</p><p>Current CBIR systems <ref type="bibr" coords="2,209.07,97.31,15.58,9.96" target="#b20">[21]</ref> generally use primitive features such as color or texture <ref type="bibr" coords="2,477.86,97.31,15.46,9.96" target="#b16">[17,</ref><ref type="bibr" coords="2,497.16,97.31,11.60,9.96" target="#b17">18]</ref>, or logical features such as object and their relationships <ref type="bibr" coords="2,334.95,109.31,15.46,9.96" target="#b24">[25,</ref><ref type="bibr" coords="2,353.53,109.31,7.79,9.96" target="#b3">4]</ref> to represent images. Because they do not use medical knowledge, such systems provide poor results in the medical domain. More specifically, the description of an image by low-level or medium-level features is not sufficient to capture the semantic content of a medical image. This loss of information is called the semantic gap. In specialized systems, this semantic gap can be reduced leading to good retrieval results <ref type="bibr" coords="2,90.00,169.07,15.46,9.96" target="#b10">[11,</ref><ref type="bibr" coords="2,108.34,169.07,12.70,9.96" target="#b19">20,</ref><ref type="bibr" coords="2,124.04,169.07,6.95,9.96" target="#b5">6]</ref>. Indeed, the more a retrieval application is specialized for a limited domain, the smaller the gap can be narrowed by using domain knowledge.</p><p>Among the limited research efforts of medical CBIR, classification or clustering driven feature selection and weighting has received much attention as general visual cues often fail to be discriminative enough to deal with more subtle, domain-specific differences and more objective ground truth in the form of disease categories is usually available <ref type="bibr" coords="2,341.30,228.83,10.55,9.96" target="#b7">[8,</ref><ref type="bibr" coords="2,354.85,228.83,11.60,9.96" target="#b14">15]</ref>. In reality, pathology bearing regions tend to be highly localized <ref type="bibr" coords="2,231.96,240.83,9.98,9.96" target="#b7">[8]</ref>. Hence, local features such as those extracted from segmented dominant image regions approximated by best fitting ellipses have been proposed <ref type="bibr" coords="2,449.83,252.71,14.58,9.96" target="#b11">[12]</ref>. However, it has been recognized that pathology bearing regions cannot be segmented out automatically for many medical domains <ref type="bibr" coords="2,192.77,276.59,14.58,9.96" target="#b19">[20]</ref>. Hence it is desirable to have a medical CBIR system that represents images in terms of semantic features, that can be learned from examples (rather than handcrafted with a lot of expert input) and do not rely on robust region segmentation.</p><p>The semantic gap can also be reduced by exploiting all sources of information. In particular, mixing text and image information generally increases the retrieval performance <ref type="bibr" coords="2,466.94,324.47,9.98,9.96" target="#b6">[7]</ref>. In <ref type="bibr" coords="2,499.33,324.47,9.98,9.96" target="#b1">[2]</ref>, statistical methods are used for modeling the occurrence of document keywords and visual characteristics. The proposed system is sensitive to the quality of the segmentation of the images. Other initiatives to combine image and text analysis study the use of Latent Semantic Analysis (LSA) techniques <ref type="bibr" coords="2,138.20,372.23,15.46,9.96" target="#b23">[24,</ref><ref type="bibr" coords="2,156.42,372.23,11.60,9.96" target="#b25">26]</ref>. In <ref type="bibr" coords="2,187.96,372.23,14.58,9.96" target="#b23">[24]</ref>, the author applied the LSA method to features extracted from the two media. The conclusion of this study is that combining the image and the text through the LSA method is not always efficient. The usefulness of LSA is also not conclusive in <ref type="bibr" coords="2,432.70,396.23,14.58,9.96" target="#b25">[26]</ref>. Conversely, a simple late fusion of visual and textual indexes provides generally good results.</p><p>In this paper, we present our work on medical image retrieval that is mainly based on the incorporation of medical knowledge in the system within a fusion framework. For text, this knowledge is in the Unified Medical Language System (UMLS) sources produced by NML<ref type="foot" coords="2,487.44,442.63,3.95,6.97" target="#foot_0">1</ref> . For images, this knowledge is in semantic features that are learned from examples and do not rely on robust region segmentation. In order to manage large and complex sets of visual entities (i.e., high content diversity) in the medical domain, we developed a structured learning framework that facilitates modular design and extract medical visual semantics. We developed two complementary visual indexing approaches within this framework: a global indexing to access image modality, and a local indexing to access semantic local features. This local indexing does not rely on region segmentation but builds upon patch-based semantic detector <ref type="bibr" coords="2,358.48,527.75,14.58,9.96" target="#b12">[13]</ref>.</p><p>To benefit efficiently from both modalities, we propose to represent both image and text using UMLS concepts in our principal retrieval system. The use of UMLS concepts allows our system to work at a higher semantic level and to standardize the semantic index of medical data, facilitating the communication between visual end textual indexing and retrieval. We propose several fusion approaches and a visual modality filtering is designed to remove visually aberrant images according to the query modality concept(s).</p><p>Besides this UMLS-based system, we also investigate the potential of a closed visual retrieval system where all queries are fixed and manually designed (i.e. several examples are manually selected to represent each query).</p><p>Textual, visual, and mixed approaches derived from these two systems are evaluated on the medical task of CLEF 2006 (i.e. imageCLEFmed). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">UMLS-based Textual Retrieval</head><p>UMLS is a good candidate as a knowledge base for medical image and text indexing. It is more than a terminology base because terms are associated with concepts. There exists also different type of links. The base is large (more than 50,000 concepts, 5.5 million of terms in 17 languages), and is maintained by specialists with two updates a year. Unfortunately, UMLS is a merger of different sources (thesaurus, terms lists), and is neither complete, nor consistent. In particular, the links among concepts are not equally distributed. UMLS is a "meta thesaurus", i.e. a merger of existing thesaurus. It is not an ontology, because there is no formal description of concepts, but its large set of terms and variation restricted to medical domain only, enable us to experiment a full scale conceptual indexing system. In UMLS, all concepts are assigned to at least one semantic type from the Semantic Network. This provides consistent categorization of all concepts in the meta-thesaurus at the relatively general level represented in the Semantic Network. This partially solves the problem of merging existing thesaurus hierarchy during the merging process. Despite the large set of terms and terms variation available in UMLS, it still cannot cover all possible (potentially infinite) term variation. So we need a concept identification tool that manages terms variation. For English texts, we use MetaMap <ref type="bibr" coords="3,316.19,446.99,15.75,9.96" target="#b0">[1]</ref> provided by NLM. We have developed a similar tool developed for French and German documents. This concept extraction tools do not provide any disambiguation. We partially overcome this problem by manually ordering them by thesaurus sources: we prefer source that strongly belong to medicine. For example, this enables the identification of "x-ray" as radiography and not as the physical phenomenon (the wave) which seldom appears in our documents. Concepts extraction is limited to noun phrase (i.e. verbs are not treated).</p><p>The extracted concepts are then organized in conceptual vectors, like a conventional vector space IR model. We then use the same weighting scheme provided by our XIOTA indexing system <ref type="bibr" coords="3,90.00,554.63,9.98,9.96" target="#b4">[5]</ref>.</p><p>We tested six retrieval approaches based on this conceptual indexing and an approach -corresponding to run IPAL Textual TDF -based on an indexing using MeSH<ref type="foot" coords="3,405.84,577.15,3.95,6.97" target="#foot_1">2</ref> terms.</p><p>Each conceptual text retrieval approach uses a Vector Space Model (VSM) for representing each document and a cosine similarity measure to compare the query index to the database medical report. The tf • idf measure is used to weight the concepts. The mapping text-concept is separate for three languages, query vectors of concepts of three languages are fusioned, and used for interrogation using the three indexing separately. Then, the three relevance status values are fusioned together.</p><p>One major criticism we have against VSM is the lack of structure of the query. VSM is known to perform well using long textual queries but ignoring query structure. The ImageCLEFmed 2006 queries are rather short. Moreover, it seems obvious to us that it is the complete query that should be solved and not only part of it. After query examination, we found out that queries are implicitly structured according to some semantic types (e.g. anatomy, pathology, modality). We call this the "semantic dimensions" of the query. Omitting correct answer to any of these dimensions may lead to incorrect answers. Unfortunately VSM does not provide a way to ensure answers to each dimension.</p><p>To solve this problem, we decided to add a semantic dimension filtering step to the VSM, in order to explicitly taking into account the query dimension structure. This extra filtering step retains answers that incorporate at least one dimension. We use semantic structure on concepts provided by UMLS. Semantic dimension of a concept is defined by its UMLS semantic type, grouped into semantic groups: Anatomy, Pathology and Modality. Only a conceptual indexing and a structured meta-thesaurus like UMLS enable us to do such a semantic dimension filtering (DF). This filtering discards noisy answers regarding to the dimension query semantic structure. The corresponding run is "IPAL Textual CDF". We also test a similar dimension filtering based MESH terms (run "IPAL Textual TDF"). In this case, the association between MeSH terms and a dimension had to be done manually. According to Table <ref type="table" coords="4,355.25,252.71,3.90,9.96" target="#tab_0">1</ref>, using UMLS concepts more than terms improves the results of 2 Mean Average Precision (MAP) points (i.e. from 21% to 23%).</p><p>Another solution to take into account query semantic structure is to re-weight answers according to dimensions (DW). Here, Relevance Status Value output from VSM is multiplied by the number of concepts matched with the query according to the dimensions. This simple reweighting scheme strongly emphasizes the presence of maximum number of concepts related to semantic dimensions. This re-weighting step implicitly do the previous dimension filtering (DF), as the relevance value is multiplied by 0. According to our results in Table <ref type="table" coords="4,400.54,336.47,5.03,9.96" target="#tab_0">1</ref> this DW approach -corresponding to the run "IPAL Textual CDW" -produces the best results of 2006 ImageCLEFmed with 26% of MAP. This result outperforms any other classical textual indexing reported in Im-ageCLEFmed 2006. Hence we have shown here the potential of conceptual indexing.</p><p>In run "IPAL Textual CPRF", we tested Pseudo-Relevance Feedback (PRF). From the result of late fusion of text-image retrieval results, three top relevant documents retrieved are taken and all concepts of these documents are added into query for query expansion. Then, a dimension filtering is applied. In fact, this run should have been classified in the mixed runs as we also use the image information to have a better precision in the three first images. This PRF approach improves slightly the results obtained with a simple dimension filtering. This is principally due to the fact errors can be present in the three first documents, even with the best mixed retrieval result. Using a manual Relevance Feedback (RF), we obtained a MAP of 25% that is 2 points higher than the result obtained with a simple dimension filtering. In this last run -named "IPAL Textual CRF" -a maximum of 4 top relevant documents were chosen by human judgment over 20 first retrieved image. All the concepts from these documents are added into the query for query expansion.</p><p>We also tested document expansion using the UMLS semantic network. Based on UMLS hierarchical relationships, each database concept is expanded by concepts positioned at a higher level in the UMLS hierarchy and connected to this concept with respect to the semantic relation "is a". The expanded concepts have a higher position than document concept in UMLS hierarchy. For example a document indexed by the concept "molar teeth" would be also indexed by the more general concept "teeth". This document would be thus retrieved if the user ask for a teeth photography. This expansion does not seem relevant according to the Table <ref type="table" coords="4,458.07,587.51,5.03,9.96" target="#tab_0">1</ref> as the run "IPAL Textual CDE" -that uses a document expansion technique and a dimension filtering -is 4 points below the simple dimension filtering.</p><p>3 Visual Retrieval</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">UMLS-based visual indexing and retrieval</head><p>In order to manage large and complex sets of visual entities in the medical domain, we developed a structured learning framework to facilitate modular design and learning of medical semantics from images. This framework allows to index images using VisMed terms, that are typical semantic tokens characterized by a visual appearance in medical image regions. Each VisMed term is expressed in the medical domain as a combination of UMLS concepts. In this way, we have a common language to index both image and text, which facilitates the communication between visual and textual indexing and retrieval. We developed two complementary indexing approaches within this statistical learning framework:</p><p>• a global indexing to access image modality (chest X-ray, gross photography of an organ, microscopy, etc.);</p><p>• a local indexing to access semantic local features that are related to modality, anatomy, and pathology concepts.</p><p>After a presentation of both approaches in Sections 3.1.1 and 3.1.2, retrieval procedures and experimental results are given in Section 3.1.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Global UMLS Indexing</head><p>The global UMLS indexing is based on a two level hierarchical classifier according to mainly modality concepts. This modality classifier is learned from about 4000 images separated in 32 classes: 22 grey level modalities, and 10 color modalities. Each indexing term is characterized by a UMLS modality concept (e.g. chest X-ray, gross photography of an organ) and, sometimes, a spatial concept (e.g. axial, frontal, etc), or a color percept (color, grey). The training images come from the CLEF database (about 2500 examples), from the IRMA<ref type="foot" coords="5,369.48,385.63,3.95,6.97" target="#foot_2">3</ref> database (about 300 examples), and from the web (about 1200 examples). The training images from ImageCLEFmed database was obtained from modality concept extraction using medical reports. A manual filtering step on this extraction process to remove irrelevant examples had to be performed. We plan to automate this filtering in the near future.</p><p>The first level of the classifier corresponds to a classification for grey level versus color images. Indeed, some ambiguity can appear due to the presence of colored images, or the slightly blue or green appearance of X-ray images. This first classifier uses the first three moments in the HSV color space computed on the entire image. The second level corresponds to the classification of modality UMLS concepts given that the image is in the grey or the color cluster. For the grey level cluster, we use grey level histogram (32 bins), texture features (mean and variance of Gabor coefficients for 5 scales and 6 orientations), and thumbnails (grey values of 16x16 resized image). For the color cluster, we have adopted HSV histogram (125 bins), Gabor texture features, and thumbnails. Zero-mean normalization <ref type="bibr" coords="5,259.68,542.51,10.55,9.96" target="#b8">[9]</ref> was applied to each feature . For each SVM classifier, we adopted a RBF kernel:</p><formula xml:id="formula_0" coords="5,266.76,564.55,241.98,11.80">exp(-γ|x -y| 2 ) (<label>1</label></formula><formula xml:id="formula_1" coords="5,508.74,566.39,4.23,9.96">)</formula><p>where γ = 1 2σ 2 and with a modified city-block distance:</p><formula xml:id="formula_2" coords="5,245.88,606.99,267.10,31.13">|x -y| = 1 F F f =1 |x f -y f | N f<label>(2)</label></formula><p>where x = {x 1 , ..., x F } and y = {y 1 , ..., y F } are feature vectors, x f , y f are feature vectors of type f, N f is the feature vector dimension, and F is the number of feature types: F = 1 for the grey versus color classifier, F = 3 for the conditional modality classifiers: color, texture, thumbnails.</p><p>We use γ = 1 in all our experiments. This just-in-time feature fusion within the kernel combines the contribution of color, texture, and spatial features equally <ref type="bibr" coords="6,362.88,73.43,14.58,9.96" target="#b13">[14]</ref>.</p><p>The probability of a modality MOD i for an image z is given by:</p><formula xml:id="formula_3" coords="6,189.24,106.67,323.74,22.21">P (MOD i |z) = P (MOD i |z, C)P (C|z) if MOD i ∈ C P (MOD i |z, G)P (G|z) if MOD i ∈ G<label>(3)</label></formula><p>where C and G denote the color and the grey level clusters respectively, and the conditional probability P (MOD i |z, V ) is given by:</p><formula xml:id="formula_4" coords="6,241.56,173.34,271.42,26.71">P (c|z, V ) = exp Dc(z) j∈V exp Dj (z)<label>(4)</label></formula><p>where D c is the signed distance to the SVM hyperplane that separate class c from the other classes of the cluster V . After learning -using SVM-Light software<ref type="foot" coords="6,293.28,232.99,3.95,6.97" target="#foot_3">4</ref> [10, 22] -, each database image z is indexed according to modality given its low-level features z f . The indexes are the probability values given by Equation (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Local UMLS Indexing</head><p>To better capture the medical image content, we propose to extend the global modeling and classification with local patch classification of local visual and semantic tokens (LVM terms). Each LVM indexing term is expressed as a combination of Unified Medical Language System (UMLS) concepts from Modality, Anatomy, and Pathology semantic types. A Semantic Patch Classifier was designed to classify a patch according to the 64 LVM terms. In these experiments, we have adopted color and texture features from patches (i. e. small image blocks) and a classifier based on SVMs and the softmax function <ref type="bibr" coords="6,277.67,374.27,10.55,9.96" target="#b2">[3]</ref> given by Equation ( <ref type="formula" coords="6,380.32,374.27,3.87,9.96" target="#formula_4">4</ref>). The color features are the three first moments of the Hue, the Saturation, and the Value of the patch. The texture features are the mean and variance of Gabor coefficients using 5 scales and 6 orientations. Zero-mean normalization <ref type="bibr" coords="6,152.58,410.15,10.43,9.96" target="#b8">[9]</ref> is applied to both the color and texture features. We adopted a RBF kernel with modified city-block distance given by Equation (2). The training dataset is composed of 3631 patches extracted from 1033 images mostly coming from the web (921 images coming from the web and 112 images from the ImageCLEFmed collection ∼ 0.2%).</p><p>After learning, the LVM indexing terms are detected during image indexing from image patches without region segmentation to form semantic local histograms. Essentially, an image is tessellated into overlapping image blocks of size 40x40 pixels after size standardization. Each patch is then classified into one of the 64 LVM terms using the Semantic Patch Classifier. An image containing P overlapping patches is then characterized by the set of P LVM histograms and their respective location in the image. An histogram aggregation per block gives the final image index : M × N LVM histograms. Each bin of the histogram of a given block B corresponds to the probability of a LVM term presence in this block. This probability is computed as follows:</p><formula xml:id="formula_5" coords="6,222.12,562.07,286.63,25.57">P (VMT i |B) = z |z ∩ B| P (VMT i |z) z |z ∩ B| (<label>5</label></formula><formula xml:id="formula_6" coords="6,508.75,568.91,4.23,9.96">)</formula><p>where B is a block of a given image, z denotes a path of the same image, |z ∩ B| is the area of the intersection between z and B, and P (VMT i |z) is given by Equation (4). To facilitate spatial aggregation and matching of image with different aspect ratios ρ, we design 5 tiling templates, namely M × N = 3 × 1, 3 × 2, 3 × 3, 2 × 3, and 1 × 3 grids resulting in 3, 6, 9, 6, and 3 probability vectors per image respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Visual retrieval using UMLS-based visual indexing</head><p>We propose three retrieval methods from query by example(s) based on the two UMLS-based visual indexing. When several images are given in the query, the similarity between a database image z with the query is given by the maximum value among the similarities between z and each query image.</p><p>The first method -corresponding to run "IPAL Visual MC" -is based on the global indexing scheme according the modality. An image is represented by a semantic histogram, each bin corresponding to a modality probability. The distance between two images is given by the Manhattan distance (i.e. city-block distance) between the two semantic histograms.</p><p>The second method -corresponding to run "IPAL Visual SPC" -is based on the local UMLS visual indexing. An image is then represented by M × N semantic histograms. Given two images represented as different grid patterns, we propose a flexible tiling (FlexiTile) matching scheme to cover all possible matches <ref type="bibr" coords="7,204.13,211.31,14.58,9.96" target="#b12">[13]</ref>. The distance between a query image and a database image is then the mean of block by block distances on all the possible matches. The distance between two blocks is given by the Manhattan distance between the two LocVisMed histograms.</p><p>The last visual retrieval method -corresponding to run "IPAL Visual SPC+MC" -is the fusion of the two first approaches. This approach combines thus two complementary sources of information, the first concerning the general aspect of the image (global indexing according to modality), the second concerning semantic local features with spatial information (local UMLS indexing). The similarity to a query is given by the mean of the similarity to a query according each index.</p><p>The 2006 CLEF medical task was particularly difficult this year for purely visual approaches. Indeed, the queries were at a hight semantic level for a general retrieval system. As a proof, the best automatic visual result was less than 8% of MAP. Mixing the local and global indexing, gives us the third place with 6% of MAP as showed in Table <ref type="table" coords="7,330.91,354.83,5.03,9.96" target="#tab_1">2</ref>  <ref type="foot" coords="7,336.12,353.47,3.95,6.97" target="#foot_4">5</ref> . We believe than we can improve these results using also the textual query in the retrieval process. Indeed, besides the usual similaritybased queries, our semantic indexing allow semantic-based query. Tests are in course on 2005 and 2006 medical tasks, providing promishing results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Manual Query Construction and Visual Task Fusion</head><p>To see how far we can go with a purely visual approach, we propose here a closed visual system based on manual query construction and visual task fusion. This work is similar to what we did in ImageCLEFmed 2005 <ref type="bibr" coords="7,210.67,583.55,16.94,9.96" target="#b22">[23]</ref>. We fused retrieval results generated by systems using multiplefeature representations and multiple retrieval systems. More specifically, we used three types of feature representations, i.e., "blob", "icon" and "blob+icon" and two retrieval systems, "SVM" and "Dist". For each topic, we manually chose about 50 similar images which is used to form a training set for SVM and to construct the query. All images are then represented by these features respectively and passing either retrieval system.</p><p>In this year's attempts, we have submitted 10 runs based on the fusion of six sub-runs. The generated sub-runs are denoted D1,D2,D3,D4, D5 and D6. D1,D2 and D3 use "Dist" retrieval system but different features (D1 using "icon", D2 using "blob", D3 using "blob+icon"), D4 and D5 use "SVM" with different features (D4 using "blob", D2 using "icon"), and D6 uses the UMLSbased system presented in Section 3.1 (D6 corresponds to the run "IPAL Visual MC" that is based on global UMLS indexing).</p><p>Different from the work done in 2005, we also use the probability estimation of each image about its modality that is given by Equation (3). Some of these sub-runs (D1-D6) are linearly combined together to produce a score for each image. Each score may then be multiplied by the probability and all the results are sorted to yield the final retrieval ranking lists. The runs "IPAL CMP D1D2D4D5D6", "IPAL CMP D1D2D3D4D5", "IPAL CMP D1D2D3D4D5D6" 6 , and "IPAL CMP D1D2D4D5" used the probability estimations. We also applied a color filter to remove those images whose number of color channels are less than that of the query images, except for runs "IPAL D1D2D4D5D6" and "IPAL D1D2D4D5". The performance of these runs is given in Table <ref type="table" coords="8,128.98,228.83,3.90,9.96" target="#tab_2">3</ref>. From these results, we can find that:</p><p>• Applying a probability estimation of each image about its modality and imaging anatomy helps (compare "IPAL CMP D1D2D4D5D6" (MAP=0.1596) and "IPAL cfD1D2D4D5D6" (MAP=0.155));</p><p>• Color filtering generally improves performance, but not significantly;</p><p>• Using D6 in the combination, the performance is improved. Compare "IPAL cfD1D2D4D5D6" (MAP=0.155) versus "IPAL cfD1D2D4D5" (MAP = 0.1461) and "IPAL D1D2D4D5D6" (MAP=0.1551) versus "IPAL D1D2D4D5" (MAP=0.1461).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">UMLS-based Mixed Retrieval</head><p>We propose three types of fusion between text and images:</p><p>• a simple late fusion (run "IPAL Cpt Im");</p><p>• a fusion that uses a visual filtering according to modality concept(s) (runs "IPAL ModFDT Cpt Im", "IPAL ModFST Cpt Im", "IPAL ModFDT TDF Im", and "IPAL ModFDT Cpt");</p><p>• an early fusion of UMLS-based visual and textual indexes (run "IPAL MediSmart 1" and "IPAL MediSmart 2"). The first fusion method is a late fusion of visual and textual similarity measures. The similarity between a mixed query Q = (Q I , Q T ) (Q I : image(s), Q T : text) and a couple composed of an image and the associated medical report (I, R) is then given by:</p><formula xml:id="formula_7" coords="9,180.12,260.87,332.86,28.93">λ(Q, I, R) = α λ V (Q I , I) max z∈DI λ V (Q I , z) + (1 -α) λ T (Q T , R) max z∈DT λ T (Q T , z)<label>(6)</label></formula><p>where λ V (Q I , I) denotes the visual similarity between the visual query Q I and an image I, λ T (Q T , R) denotes the textual similarity between the textual query Q T and the medical report R, D I denotes the image database, and D T denotes the text database. The factor α allows the control of the weight of the textual similarity with respect to the image similarity. After some experimentations on imageCLEFmed 2005, we choose α = 0.7. In order to compare similarities in the same range, each similarity is divided by the corresponding maximal similarity value on the entire database. The result of the corresponding run, ""IPAL Cpt Im", given in Table <ref type="table" coords="9,483.28,373.07,5.03,9.96" target="#tab_3">4</ref> show the good complementarity of the visual and textual indexing: from 26% for the textual retrieval and 6% for the visual retrieval, the mixed retrieval provides 31% of MAP. The best results on imageCLEFmed 2006 in terms of MAP and R-precision (i.e. precision after R retrieved images, where R is the number of relevant images) were obtained with this simple late fusion.</p><p>The second type of fusion exploits directly the UMLS index of images. Indeed, it is based on a direct matching between concepts extracted from the textual query and conceptual image indexes. This direct matching is done automatically with the use of the Unified Medical Language System. More specifically, a comparison between the query concepts related to modality and the image modality index is done in order to remove all aberrant images. The decision rule is the following: an image I is admissible for a query modality MOD Q only if:</p><formula xml:id="formula_8" coords="9,250.44,514.43,262.54,10.33">P (MOD Q |I) &gt; τ (MOD Q )<label>(7)</label></formula><p>where τ (MOD Q ) is a threshold defined for the modality MOD Q . This decision rule defines a set of admissible images for a given modality MOD Q : {I ∈ D I :</p><formula xml:id="formula_9" coords="9,356.40,548.39,111.23,10.33">P (MOD Q |I) &gt; τ (MOD Q )}.</formula><p>The final result is then the intersection of this set and the ordered set of images retrieved by any system. This modality filter is particularly interesting for filtering textual retrieval results. Indeed, several images of different modalities can be associated to the same medical report. The ambiguity is thus removed when using a visual modality filtering. We test this approach with, first, a fixed threshold for all modality τ (MOD Q ) = 0.15 ("ModFST") based on experimental tests on 2005 CLEF medical task, and, second, an adaptive threshold for each modality according a confidence degree given to the classifier according this modality ("ModFDT"). The adaptive thresholding performs slightly better than the constant thresholding (compare "IPAL ModFDT Cpt Im" and "IPAL ModFST Cpt Im" in Table <ref type="table" coords="9,244.69,655.91,3.88,9.96" target="#tab_3">4</ref>). In fact, we have over-estimated these thresholds for most modalities. Indeed, when this modality filtering is applied to the late fusion results the results decrease of 2 points (compare "IPAL ModFDT Cpt Im" and "IPAL Cpt Im"). That means that this filtering not only removes aberrant images but also relevant images. This filtering nevertheless increases the results of the purely textual retrieval approach from 26% to 27% (see "IPAL ModFDT Cpt" in Table <ref type="table" coords="10,229.93,97.31,5.03,9.96" target="#tab_3">4</ref> and "IPAL Textual CDW" in Table <ref type="table" coords="10,397.31,97.31,3.88,9.96" target="#tab_0">1</ref>). Moreover, this filtering is relevant if the user -which is often the case -is more interested in the precision in the first retrieved images that in the mean average precision. Indeed, the Figure <ref type="figure" coords="10,401.61,121.19,5.03,9.96" target="#fig_0">1</ref> shows that the adaptive modality filtering on the late fusion results ("IPAL ModFDT Cpt Im") and even directly on the textual results ("IPAL ModFDT Cpt") provides a better precision than the late fusion results ("IPAL Cpt Im") for the first retrieved documents (until 30 when applied on textual results, until 50 when applied on the mixed results). We also submitted two runs concerning the early fusion of UMLS-based visual and textual indexes. A Semantic level fuzzyfication algorithm takes into account the frequency, the localization, the confidence and the source of the information <ref type="bibr" coords="10,314.95,512.39,14.58,9.96" target="#b18">[19]</ref>. Unfortunately, errors were found after the submission. Using the corrected algorithm, we obtain 24% of mean average precision on ImageCLEFmed 2006. We have to note that the dimension filtering and re-weighting used in the other IPAL mixed runs are not applied here, which explains in part the difference of precision. In fact, this result is higher than the results obtained with runs that do not use this dimension filtering (20% for mixed retrieval, 23% for textual retrieval). We currently develop clustering techniques to improve the retrieval results. A fuzzy min-max boosted K-means clustering approach gives promishing results on CASImage database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have proposed a medical image retrieval system that represents both texts and images at a very high semantic level using concepts from the Unified Medical Language System. Textual, visual, and mixed approaches derived from this system were evaluated on ImageCLEFmed 2006. A structured framework was proposed to bridge the semantic gap between low-level image features and the semantic UMLS concepts. A closed visual system based on manual query construction and visual task fusion was also tested to go as far as possible using a purely visual approach. From the results on ImageCLEF 2006, we can conclude that the textual approaches capture more easily the semantics of the medical queries, providing better results than purely visual retrieval approaches. Indeed, the best visual approach in 2006 -that corresponds to a result of our closed system -only provides 16% of MAP against 26% of MAP for the best textual results. Moreover, the results show the potential of conceptual indexing, especially when using a semantic dimension filtering: we obtained the best textual and mixed results in imageCLEF 2006 using our UMLS-based system. The benefit of working in a fusion framework has been demonstrated. Firstly, visual retrieval results are enhanced by the fusion of global and local similarities. Secondly, mixing textual and visual information improves significantly the system performance. Besides precision in the first documents increases when using a visual modality filtering, allowing 68% of mean precision on the 10 first documents and 62% of mean precision for the 30 first documents on the 30 queries of ImageCLEF 2006. We are currently investigating the potential of an early fusion scheme using appropriate clustering methods. In the near future, we plan to use the LVM terms from local indexing for semantics-based retrieval (i.e. cross-modal retrieval: processing textual query on LVM-based image indexes). A visual filtering based on local information could also be derived from the semantic local indexing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="10,126.36,465.11,350.09,9.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Precision for N retrieved documents on ImageCLEFmed 2006 queries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,194.40,68.39,214.25,155.52"><head>Table 1 :</head><label>1</label><figDesc>Results of textual runs</figDesc><table coords="3,194.40,90.35,214.25,133.56"><row><cell></cell><cell cols="2">(a) Automatic runs</cell><cell></cell></row><row><cell>Rank</cell><cell>run ID</cell><cell>MAP</cell><cell>R-prec</cell></row><row><cell cols="4">1/31 IPAL Textual CDW 26.46% 30.93%</cell></row><row><cell cols="4">2/31 IPAL Textual CPRF 22.94% 28.43%</cell></row><row><cell>3/31</cell><cell>IPAL Textual CDF</cell><cell cols="2">22.70% 29.04%</cell></row><row><cell>5/31</cell><cell>IPAL Textual TDF</cell><cell cols="2">20.88% 24.05%</cell></row><row><cell cols="2">10/31 IPAL Textual CDE</cell><cell cols="2">18.56% 25.03%</cell></row><row><cell></cell><cell cols="2">(b) relevance feedback run</cell><cell></cell></row><row><cell>Rank</cell><cell>run ID</cell><cell>MAP</cell><cell>R-prec</cell></row><row><cell>1/1</cell><cell cols="3">IPAL Textual CRF 25.34% 29.76%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,190.32,421.43,222.44,77.88"><head>Table 2 :</head><label>2</label><figDesc>Results of automatic visual runs</figDesc><table coords="7,190.32,441.23,222.44,58.08"><row><cell>Rank</cell><cell>run ID</cell><cell>MAP</cell><cell>R-prec</cell></row><row><cell>1/11</cell><cell>CINDI Fusion Visual</cell><cell cols="2">07.53% 13.11%</cell></row><row><cell cols="4">3/11 IPAL Visual SPC+MC 06.41% 10.69%</cell></row><row><cell>4/11</cell><cell>IPAL Visual MC</cell><cell cols="2">05.66% 09.12%</cell></row><row><cell>6/11</cell><cell>IPAL Visual SPC</cell><cell cols="2">04.84% 08.47%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,175.56,257.63,251.92,149.64"><head>Table 3 :</head><label>3</label><figDesc>Results of manual visual runs</figDesc><table coords="8,175.56,277.43,251.92,129.84"><row><cell>Rank</cell><cell>run ID</cell><cell>MAP</cell><cell>R-prec</cell></row><row><cell>1/10</cell><cell>IPAL CMP D1D2D4D5D6</cell><cell cols="2">15.96% 19.39%</cell></row><row><cell>2/10</cell><cell>IPAL CMP D1D2D3D4D5</cell><cell cols="2">15.84% 19.22%</cell></row><row><cell cols="4">3/10 IPAL CMP D1D2D3D4D5D6 15.79% 19.62%</cell></row><row><cell>4/10</cell><cell>IPAL D1D2D4D5D6</cell><cell cols="2">15.51% 20.58%</cell></row><row><cell>5/10</cell><cell>IPAL cfD1D2D4D5D6</cell><cell cols="2">15.50% 20.47%</cell></row><row><cell>6/10</cell><cell>IPALcf D1D2D3D4D5D6</cell><cell cols="2">15.20% 20.19%</cell></row><row><cell>7/10</cell><cell>IPAL CMP D1D2D4D5</cell><cell cols="2">14.63% 19.94%</cell></row><row><cell>8/10</cell><cell>IPAL cfD1D2D4D5</cell><cell cols="2">14.61% 19.98%</cell></row><row><cell>9/10</cell><cell>IPAL D1D2D4D5</cell><cell cols="2">14.61% 19.98%</cell></row><row><cell>10/10</cell><cell>IPALcf D1D2D3D4D5</cell><cell cols="2">14.17% 19.57%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,185.88,68.39,231.33,113.76"><head>Table 4 :</head><label>4</label><figDesc>Results of automatic mixed runs</figDesc><table coords="9,185.88,88.07,231.33,94.08"><row><cell>Rank</cell><cell>run ID</cell><cell>MAP</cell><cell>R-prec</cell></row><row><cell>1/37</cell><cell>IPAL Cpt Im</cell><cell cols="2">30.95% 34.59%</cell></row><row><cell>2/37</cell><cell cols="3">IPAL ModFDT Cpt Im 28.78% 33.52%</cell></row><row><cell>3/37</cell><cell>IPAL ModFST Cpt Im</cell><cell cols="2">28.45% 33.17%</cell></row><row><cell cols="4">4/37 IPAL ModFDT TDF Im 27.30% 37.74%</cell></row><row><cell>5/37</cell><cell>IPAL ModFDT Cpt</cell><cell cols="2">27.22% 37.57%</cell></row><row><cell>17/37</cell><cell>IPAL MediSmart 1</cell><cell cols="2">6.49% 10.12%</cell></row><row><cell>30/37</cell><cell>IPAL MediSmart 2</cell><cell>4.20%</cell><cell>6.57%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,105.24,678.88,209.84,7.61"><p>National Library of Medicine -http://www.nlm.nih.gov/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,105.24,681.28,407.41,7.61;3,90.00,690.64,201.08,7.61"><p>Medical Subject Headings, MeSH, is the controlled vocabulary thesaurus of the U.S. National Library of Medicine. It is included in the meta-thesaurus UMLS.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,105.24,693.82,190.05,6.43"><p>http://phobos.imib.rwth-aachen.de/irma/index_en.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="6,105.24,664.45,121.82,7.35"><p>http://svmlight.joachims.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="7,105.24,662.44,407.44,7.61;7,90.00,671.80,103.23,7.61"><p>The Mean Average Precision and the Recall-precision computed in imageCLEFmed for the run IPAL Visual SPC+MC was</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_5" coords="7,196.95,671.80,315.69,7.61;7,90.00,681.28,28.05,7.61"><p><ref type="bibr" coords="7,196.95,671.80,4.41,7.61" target="#b5">6</ref>.34% and 10.48% respectively because we only submitted -by error -the 25 first queries.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6" coords="9,105.24,686.92,407.28,7.61;9,90.00,696.40,82.35,7.61"><p>"IPAL CMP D1D2D3D4D5D6" corresponds to "IPAL CMP D1D2D3D4D5D" in ImageCLEFmed where the last letter was missing</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,110.51,319.43,402.15,9.96;11,110.52,331.43,401.87,9.96;11,110.52,343.31,137.00,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,164.82,319.43,347.84,9.96;11,110.52,331.43,34.62,9.96">Effective mapping of biomedical text to the UMLS metathesaurus: The MetaMap program</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aronson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,173.00,331.43,339.40,9.96;11,110.52,343.31,48.25,9.96">Proceedings of the Annual Symposium of the American Society for Medical Informatics</title>
		<meeting>the Annual Symposium of the American Society for Medical Informatics</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="17" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.51,363.35,402.31,9.96;11,110.52,375.23,345.10,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,471.81,363.35,41.01,9.96;11,110.52,375.23,80.88,9.96">Matching words and pictures</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,199.97,375.23,166.30,9.96">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1107" to="1135" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.51,395.15,383.40,9.96" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="11,172.63,395.15,175.91,9.96">Neural Networks for Pattern Recognition</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Clarendon Press</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.51,415.07,402.16,9.96;11,110.52,427.07,402.13,9.96;11,110.52,439.07,290.65,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,347.09,415.07,165.58,9.96;11,110.52,427.07,289.12,9.96">Blobworld: Image segmentation using expectation-maximisation and its applications to image querying</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Carson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,412.66,427.07,99.99,9.96;11,110.52,439.07,184.08,9.96">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1026" to="1038" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.51,458.99,402.23,9.96;11,110.52,470.87,402.17,9.96;11,110.52,482.87,260.00,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,209.29,458.99,303.46,9.96;11,110.52,470.87,240.89,9.96">X-IOTA: An open XML framework for IR experimentation application on multiple weighting scheme tests in a bilingual corpus</title>
		<author>
			<persName coords=""><forename type="first">Jean-Pierre</forename><surname>Chevallet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,359.83,470.87,152.86,9.96;11,110.52,482.87,128.21,9.96">Lecture Notes in Computer Science (LNCS), AIRS&apos;04 Conference</title>
		<meeting><address><addrLine>Beijing</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3211</biblScope>
			<biblScope unit="page" from="263" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.51,502.79,402.15,9.96;11,110.52,514.67,402.55,9.96;11,110.52,526.67,22.88,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,298.42,502.79,214.24,9.96;11,110.52,514.67,85.74,9.96">Knowledge-based image retrieval with spatial and temporal constructs</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">C</forename><surname>Alfonso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">T</forename><surname>Ricky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,206.32,514.67,247.46,9.96">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="872" to="888" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.51,546.59,402.19,9.96;11,110.52,558.59,402.27,9.96;11,110.52,570.47,273.61,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,274.20,558.59,238.59,9.96;11,110.52,570.47,16.68,9.96">The CLEF 2005 automatic medical image annotation task</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Desealers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffery</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="11,135.69,570.47,192.60,9.96">Springer Lecture Notes in Computer Science</title>
		<imprint/>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct coords="11,110.51,590.39,402.17,9.96;11,110.52,602.39,402.13,9.96;11,110.52,614.39,244.38,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,419.64,590.39,93.04,9.96;11,110.52,602.39,255.45,9.96">Unsupervised feature selection applied to content-based retrieval of lung images</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Dy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Kak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">S</forename><surname>Broderick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Aisen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,376.93,602.39,135.71,9.96;11,110.52,614.39,147.75,9.96">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="373" to="378" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.51,634.31,402.26,9.96;11,110.52,646.19,402.22,9.96;11,110.52,658.19,63.74,9.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,274.80,634.31,237.97,9.96;11,110.52,646.19,31.54,9.96">Content-based image retrieval with relevance feedback in mars</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mehrotra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,166.41,646.19,313.70,9.96">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="815" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.50,678.11,375.84,9.96" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="11,170.69,678.11,246.19,9.96">Learning to Classify Text using Support Vector Machines</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Kluwer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.50,61.43,402.17,9.96;12,110.52,73.43,401.98,9.96;12,110.52,85.31,76.44,9.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,433.91,61.43,78.76,9.96;12,110.52,73.43,143.82,9.96">Fast and effective retrieval of medical tumor shapes</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Korn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Sidiropoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Protopapas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,263.27,73.43,244.70,9.96">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="889" to="904" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.50,105.35,402.17,9.96;12,110.52,117.23,100.66,9.96" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,211.61,105.35,237.13,9.96">Content-based image retrieval in medical applications</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,460.17,105.35,52.51,9.96;12,110.52,117.23,15.56,9.96">Methods Inf Med</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="354" to="361" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.50,137.15,402.28,9.96;12,110.52,149.15,402.23,9.96;12,110.52,161.03,53.80,9.96" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,251.89,137.15,260.90,9.96;12,110.52,149.15,95.17,9.96">VisMed: a visual vocabulary approach for medical image indexing and retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-P</forename><surname>Chevallet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,228.15,149.15,251.51,9.96">Proceedings of the Asia Information Retrieval Symposium</title>
		<meeting>the Asia Information Retrieval Symposium</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="84" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.50,180.95,402.30,9.96;12,110.52,192.95,275.44,9.96" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,222.17,180.95,286.31,9.96">Discovering recurrent image semantics from class discrimination</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,110.52,192.95,206.15,9.96">EURASIP Journal of Applied Signal Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.50,212.87,402.20,9.96;12,110.52,224.87,401.97,9.96;12,110.52,236.75,110.28,9.96" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,171.67,212.87,246.50,9.96">Semantic based biomedical image indexing and retrieval</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,263.40,224.87,249.10,9.96;12,110.52,236.75,37.21,9.96">Trends and Advances in Content-Based Image and Video Retrieval</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Kriegel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Veltkamp</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.50,256.67,402.31,9.96;12,110.52,268.67,402.14,9.96;12,110.52,280.67,202.32,9.96" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,367.98,256.67,144.83,9.96;12,110.52,268.67,338.24,9.96">A review of content-based image retrieval systems in medical applications -clinical benefits and future directions</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Michoux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bandon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Geissbuhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,455.85,268.67,56.82,9.96;12,110.52,280.67,132.87,9.96">International Journal of Medical Informatics</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.50,300.59,402.28,9.96;12,110.52,312.47,402.13,9.96;12,110.52,324.47,401.98,9.96;12,110.52,336.47,179.91,9.96" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,239.19,312.47,273.46,9.96;12,110.52,324.47,43.27,9.96">QBICproject: querying images by content, using color, texture, and shape</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Niblack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Equitz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Flickner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">H</forename><surname>Glasman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Petkovic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Yanker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Taubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,269.90,324.47,238.05,9.96">Storage and Retrieval for Image and Video Databases</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Niblack</surname></persName>
		</editor>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="1908">1908. 1993</date>
			<biblScope unit="page" from="173" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.50,356.39,402.16,9.96;12,110.52,368.27,355.42,9.96" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,298.06,356.39,214.61,9.96;12,110.52,368.27,80.61,9.96">Photobook: Tools for content-based manipulation of image databases</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,199.84,368.27,181.76,9.96">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="233" to="254" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.50,388.19,402.38,9.96;12,110.52,400.19,402.03,9.96;12,110.52,412.19,245.75,9.96" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,391.43,388.19,121.46,9.96;12,110.52,400.19,216.58,9.96">A semantic fusion approach between medical images and reports using UMLS</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Racoceanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Teodorescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vuillemenot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,353.23,400.19,159.32,9.96;12,110.52,412.19,166.27,9.96">Proceedings of the Asia Information Retrieval Symposium (Special Session)</title>
		<meeting>the Asia Information Retrieval Symposium (Special Session)<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.50,432.11,402.28,9.96;12,110.52,443.99,402.24,9.96;12,110.52,455.99,331.20,9.96" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="12,139.89,443.99,372.87,9.96;12,110.52,455.99,36.55,9.96">Using human perceptual categories for content-based retrieval from a medical image database</title>
		<author>
			<persName coords=""><forename type="first">Chi-Ren</forename><surname>Shyu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christina</forename><surname>Pavlopoulou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Avinash</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carla</forename><forename type="middle">E</forename><surname>Kak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lynn</forename><forename type="middle">S</forename><surname>Brodley</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Broderick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,155.84,455.99,188.55,9.96">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="119" to="151" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.50,475.91,402.34,9.96;12,110.52,487.91,401.84,9.96;12,110.52,499.79,141.88,9.96" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="12,421.13,475.91,91.72,9.96;12,110.52,487.91,160.64,9.96">Content-based image retrieval at the end of the early years</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,279.43,487.91,232.93,9.96;12,110.52,499.79,48.01,9.96">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1349" to="1380" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.50,519.71,348.76,9.96" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="12,203.68,519.71,180.73,9.96">The Nature of Statistical Learning Theory</title>
		<author>
			<persName coords=""><forename type="first">Vladimir</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.50,539.63,402.30,9.96;12,110.52,551.63,402.11,9.96;12,110.52,563.63,338.42,9.96" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="12,465.43,539.63,47.37,9.96;12,110.52,551.63,313.97,9.96">Combining multilevel visual features for medical image retrieval in ImageCLEF 2005</title>
		<author>
			<persName coords=""><forename type="first">Xiong</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qiu</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tian</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xu</forename><surname>Changsheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ong</forename><surname>Sim-Heng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Foong</forename><surname>Kelvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,444.79,551.63,67.84,9.96;12,110.52,563.63,143.57,9.96">Cross Language Evaluation Forum 2005 workshop</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09">September 2005</date>
			<biblScope unit="page">73</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.50,583.55,402.07,9.96;12,110.52,595.43,93.85,9.96" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="12,177.04,583.55,171.98,9.96">Image retrieval : Content versus context</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Westerveld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,368.74,583.55,143.84,9.96;12,110.52,595.43,63.15,9.96">Recherche d&apos;Information Assistee par Ordinateur</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.50,615.35,402.29,9.96;12,110.52,627.35,402.50,9.96;12,110.52,639.23,22.88,9.96" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="12,433.91,615.35,78.89,9.96;12,110.52,627.35,258.52,9.96">CORE: a contentbased retrieval engine for multimedia information systems</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Desai Narasimhalu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">M</forename><surname>Mehtre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">P</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,381.75,627.35,85.97,9.96">Multimedia Systems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="25" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.50,659.27,402.14,9.96;12,110.52,671.15,374.85,9.96" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="12,223.57,659.27,289.08,9.96;12,110.52,671.15,126.45,9.96">Narrowing the semantic gap -improved text-based web document retrieval using visual features</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Grosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,245.40,671.15,147.61,9.96">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="189" to="200" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
