<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,143.32,115.86,328.49,12.93;1,141.10,133.79,332.89,12.93">IPAL Inter-Media Pseudo-Relevance Feedback Approach to ImageCLEF 2006 Photo Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,153.89,170.99,64.34,9.96"><forename type="first">Nicolas</forename><surname>Maillot</surname></persName>
							<email>nmaillot@i2r.a-star.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IPAL French-Singaporean Joint Lab Institute for Infocomm Research (I2R</orgName>
								<orgName type="institution">Centre National de la Recherche Scientifique (CNRS)</orgName>
								<address>
									<addrLine>21 Heng Mui Keng Terrace</addrLine>
									<postCode>119613</postCode>
									<settlement>Singapore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,225.82,170.99,91.91,9.96"><forename type="first">Jean-Pierre</forename><surname>Chevallet</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IPAL French-Singaporean Joint Lab Institute for Infocomm Research (I2R</orgName>
								<orgName type="institution">Centre National de la Recherche Scientifique (CNRS)</orgName>
								<address>
									<addrLine>21 Heng Mui Keng Terrace</addrLine>
									<postCode>119613</postCode>
									<settlement>Singapore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,325.42,170.99,46.15,9.96"><forename type="first">Vlad</forename><surname>Valea</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IPAL French-Singaporean Joint Lab Institute for Infocomm Research (I2R</orgName>
								<orgName type="institution">Centre National de la Recherche Scientifique (CNRS)</orgName>
								<address>
									<addrLine>21 Heng Mui Keng Terrace</addrLine>
									<postCode>119613</postCode>
									<settlement>Singapore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,398.68,170.99,62.49,9.96"><forename type="first">Joo</forename><forename type="middle">Hwee</forename><surname>Lim</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IPAL French-Singaporean Joint Lab Institute for Infocomm Research (I2R</orgName>
								<orgName type="institution">Centre National de la Recherche Scientifique (CNRS)</orgName>
								<address>
									<addrLine>21 Heng Mui Keng Terrace</addrLine>
									<postCode>119613</postCode>
									<settlement>Singapore</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,143.32,115.86,328.49,12.93;1,141.10,133.79,332.89,12.93">IPAL Inter-Media Pseudo-Relevance Feedback Approach to ImageCLEF 2006 Photo Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">54CEABDB04A52F7585E22D3F666C1AA1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing -Indexing methods</term>
					<term>Thesauruses; H.3.3 Information Search and Retrieval -Clustering</term>
					<term>Relevance feedback</term>
					<term>Retrieval models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This document describes the participation to ImageCLEF 2006 photographic retrieval task of the IPAL lab (Singaporean French collaboration) hosted at Institute for Infocomm Research, Singapore. This paper provides a description of the way results has been produced. The text/image database used is IAPR <ref type="bibr" coords="1,322.60,328.63,9.20,8.97" target="#b0">[1]</ref>. We have tested a cooperative use of a text retrieval and an image retrieval engine. We show in particular how inter-media re-ranking and pseudo-relevance feedback have been used for producing the results. We have also tested Latent Semantic Analysis (LSA) approach on visual runs. WordNet thesaurus has been used for pre-processing textual annotations within spell checking corrections. Our approach is completely automatic. A description of the runs submitted to the competition is also given.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the most interesting issues in multimedia information retrieval is to use different modalities (e.g. text, image) in a cooperative way.</p><p>In this experiment, our goal is to study inter-media pseudo-relevance feedback (between text and image) and so to explore how the output of an image retrieval system can be used for expanding textual queries. This is motivated by the hypothesis that two images with a very strong visual similarity should share some common semantics.</p><p>We are also interested in studying how appearance-based re-ranking techniques can be used to enhance the output of a text retrieval system. This is motivated by the fact that high-level concepts have most of the time a large variety of visual appearances. In some cases, it can be useful for the end-user to obtain images of a concept which have a well-defined appearance. This document is structured as follows. Section 2 gives a description of the system used to produce results submitted by IPAL at ImageCLEF 2006 Photo task. Section 3 provides a description of the most important runs submitted. Section 4 provides an analysis of the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>Our results have been produced by the cooperative use of a image indexing and retrieval system and a text indexing and retrieval system. An overview of the complete retrieval system can be found in fig. <ref type="figure" coords="2,334.82,262.30,3.87,9.96" target="#fig_4">4</ref>. The system developed contains pseudo-relevance feedback and re-ranking capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Text Indexing and Retrieval</head><p>Our goal on the text runs is to experiment a mixture of knowledge and statistical information to solve very precise short query. Knowledge comes from WordNet <ref type="bibr" coords="2,134.76,353.56,10.49,9.96" target="#b1">[2]</ref> and from the corpus it-self (for Geographical Named Entities). Statistical information comes only from the corpus.</p><p>Text indexing and retrieval were mainly achieved by the XIOTA system <ref type="bibr" coords="2,134.76,390.00,9.94,9.96" target="#b2">[3]</ref>. Before indexing, we have experimented four levels of linguistic treatment: morpho-syntactic, noun-phrase, named entity and conceptual. The morpho-syntax consists in transforming the original text into normalized word stems with partof-speech information (POS). More information are usually available like number (singular and plural), and the stemmed form. Noun-phrase consists in grouping a word sequence that has a unique meaning like "swimming pool". In some case it includes the change of the part of speech. For example, at the morpho-syntax level, "swimming pool" is recognized as a verb (swim) followed by a noun. But at the noun phrase level, the composed noun is recognized, and identified as a unique term. Finally, at the conceptual level, all terms are replaced by a concept reference. For this last step sense disambiguation is mandatory.</p><p>Morpho-Syntax Texts have first been preprocessed in order to recognize part of speech and to correct spelling. The following steps have been followed in sequence:</p><p>-XML correction: As XIOTA relies on a correct XML data flow, an automatic XML correction is applied for correcting some closing tags. -Part of Speech: Files are then passed through a part of speech tagger (Tree-Tagger<ref type="foot" coords="2,181.33,623.88,3.97,4.84" target="#foot_0">1</ref>  <ref type="bibr" coords="2,190.23,624.73,10.27,9.96" target="#b3">[4]</ref>). A correction is applied to suppress tagging from documents identifiers.</p><p>-Unknown proper nouns: when the tagger recognizes proper nouns, it provides a unique normalize version. When the tagger does not recognize the proper noun, we assume the normalize form does not change. Other forms of unrecognized terms are supposed to be misspelled words.</p><p>-Word normalization: it consists in removing every accent, and also removing some rare character coding errors, assuming char coding is ISO-8859-1 Latin 1. This is mainly effective for foreign geographical proper nouns (e.g. Spanish). -Spelling corrections: we make the assumption that every terms not tagged as a proper noun and unknown is misspelled (about 700 terms, ex: "buidings", "toursits"). This is false for some terms not recognized by the POS tagger because they are joint like "belltower", "blowtube"."snowcover", or because of the hyphen like "winter-jaket", "cloud-imposed". This list of unknown words is passed through aspell<ref type="foot" coords="3,285.52,272.46,3.97,4.84" target="#foot_1">2</ref> to associate a possible correct form. When aspell proposes several choices, the first one is selected.</p><p>We think that the spelling correction is important to ensure correct index in the case of short documents. Possible misspellings are detected thanks to the part of speech step. Queries are processed in the same way. All other processing including text indexing, start from the analyzed and corrected text collection. Namely, the basic vector space indexing performs only a POS filtering before building vector indexes.</p><p>Noun Phrase We have used WordNet to detect noun phase. Candidates were selected using POS template:</p><p>noun (singular) + noun (singular) (e.g. "baseball cap") -noun (singular) + noun (plural) (e.g. "tennis players") -proper noun + proper noun (e.g. "South America") -verb VG + noun (e.g. "swimming pool") If the two following conditions hold: the template and the presence in Word-Net, we replace the word couple by one term with the correct stemmed version. It means that the two terms will be treated as only one indexing term. We have not used cooccurrence statistics because the corpus is too small. Named Entity In this tourist image set, the location of the scene in the picture is important. That is why we have decided to detect geographic Named Entity. We have used two information sources: WordNet and the corpus itself. In fact, we have extracted information from the LOCATION tag to build a list of geographic names. Then we have tagged the rest of the corpus using first WordNet and then this list. Filtering is based on the proper noun POS, and on the lexical WordNet category "noun.location". If the location is not found in WordNet, then the location list is used. We have used this information to split the query and force the geographic information matching.</p><p>Concept Concept indexing seems a nice way to solve the term mismatch because all term variations is replace by one unique concept. Unfortunately the problem remains in the concept detection, because it need a disambiguation step. For this experiment, we have used the WordNet sense frequency when available. This information provides a sort of statistic on the more frequent sense of a term. Otherwise we have filtered the most frequent semantic category (lexname), and choose the most frequent one. This choice is correct most of the time for this corpus. This step produces a new corpus with WordNet concept references that enables conceptual indexing and retrieval.  The low-level features extracted on patches are the following:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Image Indexing and Retrieval</head><p>-Texture features used by our system are Gabor features <ref type="bibr" coords="4,406.29,564.28,9.94,9.96" target="#b4">[5]</ref>. The resulting feature vector is of dimension 60. -Color Features. Color is characterized by RGBL histograms (32 bins for each component). The resulting feature vector is of dimension 128.</p><p>For a patch p k i , the numerical feature vector extracted from</p><formula xml:id="formula_0" coords="4,134.77,618.70,345.78,24.42">p k i is noted fe(p k i ) ∈ R n . In this case, n = 60 + 128.</formula><p>We also define a similarity measure based on regions obtained by image segmentation <ref type="bibr" coords="4,195.36,655.55,9.94,9.96" target="#b5">[6]</ref>. An example of image segmentation can be found in fig. <ref type="figure" coords="4,455.86,655.55,3.87,9.96" target="#fig_2">2</ref>. Let R i = {r k i }, resp. R j = {r l j }, be the set of regions obtained from segmentation of image i, resp. j. This similarity measure dR(i, j) is defined as following:</p><formula xml:id="formula_1" coords="5,200.75,349.05,211.37,27.76">dR(i, j) = r k i ∈Ri min{L 2 (f e(r k i ), f e(r l j ))} r l j ∈Rj card(R i )</formula><p>For a region r k i , the numerical feature vector extracted from</p><formula xml:id="formula_2" coords="5,134.77,383.48,345.78,24.42">r k i is noted fe(r k i ) ∈ R n .</formula><p>Additional low-level features extracted on regions are their size and the position of their centroids. This implies that in this case, n = 60+128+1+2.</p><p>We have also used Local Features to characterize fine details. Note that in this case, patches are not considered. We use bags of Sift 3 <ref type="bibr" coords="5,412.63,432.31,10.49,9.96" target="#b6">[7]</ref> as explained in <ref type="bibr" coords="5,146.08,444.27,9.94,9.96" target="#b7">[8]</ref>. A visual vocabulary is built by clustering techniques (k-means). Sift features are extracted on the whole images database. Key-points are obtained by scale-space extrema localization after Difference of Gaussian (DoG) computation. Then, the k-means algorithm is used to build the visual vocabulary. The number of clusters is set to 150. Once the visual vocabulary has been built, a bag of visterms can be associated with each image of the database. The cosine distance is used to compute the distance between two bags of visterms. The bag of visterm associated with the image i is noted b i ∈ R 150 .</p><p>Similarity Function. The visual similarity between two images i and j, δ I (i, j), is defined as following:</p><formula xml:id="formula_3" coords="5,167.55,598.62,280.18,27.83">δ I (i, j) = α × np k=1 L 2 (fe(p k i ), fe(p k j )) n p + β × b i • b j |b i ||b j | + γdR(i, j)</formula><p>In our experiments α = 0.4, β = 0.4, and γ = 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Inter-Media Pseudo-Relevance Feedback</head><p>User Feedback is a basic way to solve the classic IR term mismatch problem between query and documents. User relevance is used to select relevant top retrieved document which indexing terms are injected into the initial query. This query expansion can be done automatically assuming that the k top ranked documents are relevant: this is called "pseudo-relevance feedback". Pseudo-relevance feedback has been tested for example in <ref type="bibr" coords="6,311.53,196.78,10.49,9.96" target="#b8">[9]</ref> as "local feedback" with other local term concurrence technique.</p><p>We are concerned about mixed mode queries (text + image) and interested in solving this queries using the two modalities. Other works like <ref type="bibr" coords="6,411.91,232.65,15.46,9.96" target="#b9">[10]</ref> pipeline the retrieval on one modality (text), to the other (image). Pseudo-relevance feedback for multimedia document has also been studied in <ref type="bibr" coords="6,355.86,256.56,14.58,9.96" target="#b10">[11]</ref>.</p><p>Our approach is to query both modality in parallel and to apply pseudorelevance feedback from one modality to the other. For example, the result of the image ranking drives text query expansion through documents. This information is then used to expand the textual query. We call this Inter-Media Pseudo-Relevance Feedback. As the queries contain both image and text, querying can be initiated whether by the text modality or by the image modality. Figure <ref type="figure" coords="6,475.31,328.29,4.97,9.96" target="#fig_3">3</ref> illustrates this principle for text query expansion based on the image modality. In this case, retrieval is achieved in 3 main steps. <ref type="bibr" coords="6,376.78,607.73,12.70,9.96" target="#b0">(1)</ref> The initial query is used as an input of the image retrieval engine. The text contained in the query is not involved in the image retrieval process. <ref type="bibr" coords="6,340.69,631.64,12.70,9.96" target="#b1">(2)</ref> The textual annotations associated with the top k documents retrieved by the image retrieval engine are then used for query expansion purposes. (3) After this expansion, the resulting text query is processed by the text retrieval engine to produce the final set of ranked documents. In our experiments, we have set k = 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Re-Ranking</head><p>We have also integrated re-ranking mechanisms based on the visual appearance (see fig. <ref type="figure" coords="7,173.47,204.07,4.42,9.96" target="#fig_4">4</ref>) with the same hypothesis: the top k documents retrieved by text retrieval are relevant. The the k associated relevant images are used to form a class of images which hopefully corresponds to the concept represented by the query.</p><p>The goal of re-ranking is to change the rank of the images which are visually similar to the images retrieved by text retrieval. Re-ranking is used as a postprocessing step (4) of the pseudo-relevance feedback described in section 2.4.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Implementation</head><p>Both image and text retrieval systems are implemented in C++. For text, basic IR function are part of XIOTA system, dedicated scripts are written in Perl or shell scripts. The image retrieval system heavily relies on the LTI-LIB <ref type="foot" coords="7,432.59,585.48,3.97,4.84" target="#foot_2">4</ref> computer vision library which includes image processing algorithms (e.g. feature extraction, segmentation), machine learning algorithms, and matrix algebra functionalities. This library has a very clean object-oriented design and is very well documented.</p><p>3 Description of the runs submitted 3.1 IPAL-PW P stands for Part of Speech and W for Single word. Text are first processed as explained in section 2.2. Term filtering is done on part-of-speech. Only nouns, proper nouns, abbreviations, adjectives and verbs are kept. Stemming is provided by the POS tagger. For this run, only the document fields TITLE, DESCRIP-TION and LOCATION are used. The weighting is the tf.idf, and ranking is computed with the cosine distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">IPAL-PW-PFB3</head><p>This run results from the use of the pseudo-relevance feedback described in section 2.4. PFB3 stands for pseudo-relevance feedback involving the three top images retrieved by image retrieval. It is an extra step of the text indexing run IPAL-PW. The textual annotations associated with three images are used for expansion of the text query. It is important to note that the query is expanded from the document index with tf weighting. The query weighting is performed after the merge (pseudo feedback). Then it is equivalent to a merge of the original text from the document into the query text. If we consider the use of short query implies being under the Information Retrieval (IR) "subsumption matching" paradigm where relevant document is supposed to imply the query; building a query by merging document is closer to the IR "similarly matching" paradigm, where a relevant document is supposed to be closed to the query. It is also important to note that Image Retrieval Systems are quite always under the "similarly matching" paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">IPAL-PW-PFB3-RR60 and IPAL-PW-PFB3-RR300</head><p>The re-ranking process described in section 2.5 has been used to produce these runs. Re-ranking was applied on the 60, resp. 300 top images in the documents retrieved by the text retrieval engine for run IPAL-PW-FB3-RR60, resp. IPAL-PW-FB3-RR300. Re-ranking is not applied on the whole set of retrieved images. The reason for that is that images which have a low ranking, share little semantics with query. Even if they are visually similar to the query, they should not be assigned a high rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">IPAL-WN</head><p>WN stands for Indexing using WordNet concepts. Concepts are extracted from WordNet and used to expand documents and queries. Concepts and original terms are kept in the vector because of the low reliability of concept disambiguation. Hence we have not tested a real full conceptual indexing. Before query re-weighting, a classic tf.idf weighting scheme is applied to all documents and queries. Query is then split on noun and proper noun. Weighting is then linearly rescaled to maximum 1 on these sub queries. This enables to emphasis the maximum terms in the answer as every term has the same weighting scale. As a consequence, weighting scheme is no more exact tf.idf. Nevertheless, we still use the cosine distance for ranking. Geographical Named Entities are also localized and solved apart in sub queries in the same way. Finally, top documents are those who equally match nouns with concepts, proper nouns and geographical named entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">IPAL-WN-MF-LF</head><p>This run results from a late fusion (by a weighted-sum) of the output of the text retrieval engine and the output of the image retrieval engine. MF stands for mixed features (described in section 2.3). The principle of late fusion is depicted in fig. <ref type="figure" coords="9,162.96,284.34,3.87,9.96" target="#fig_5">5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">IPAL-EI</head><p>EI stands for Equal Importance of all Nouns and Proper Nouns and Noun Phrase. Noun Phrases are computed using WordNet. This run tests the importance of Noun Phrase against other nouns. As tree-tagger does not recognize composed nouns (Noun Phrase), WordNet is used to detect composed nouns with only two nouns (e.g. tennis player, baseball cap, swimming pool) (see 2.2). Each name and proper noun produces a sub query which weight is normalized to 1 in the same way as IPAL-WN. Then sub query results are merged. Then, top documents are those who equally match nouns, proper nouns and noun phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">IPAL-LSA</head><p>This run results from Latent Semantic Analysis (LSA) <ref type="bibr" coords="9,371.35,643.60,15.46,9.96" target="#b11">[12]</ref> of the image patches. The role LSA is to reduce the effects of synonymy and polysemy by dimension reduction of a term-document matrix. The resulting reduced space is called the latent space. This run does not use the same features as described in section 2.3.</p><p>Indexing is performed as following:</p><p>1. Each image is split in 16 non-overlapping patches.</p><p>2. From each patch, RGBL histogram (128 bins = 32 * 4) and edge histogram features are extracted. 3. Patches are clustered using k-means clustering algorithm (k=4000). The cluster centroids are also computed. 4. Term-document matrix is computed A = (a ij ) with i = 1, m and j = 1, n , where a ij is the number of patches of image j belonging to cluster i. tf-idf is computed from this term-document matrix. In our case, the size of the term-document matrix is 4000 × 20000. 5. Singular Value Decomposition is applied to the term-document matrix A = U SV t . Image coordinates matrix SV t and a transformation matrix U t are obtained.</p><p>Retrieval is achieved as following:</p><p>1. Images in the query are split in 16 non-overlapping patches.</p><p>2. From each patch, RGBL histogram (128 bins = 32 * 4) and edge histogram features are extracted. 3. Distance to closest clusters centroids are computed and each patch of each image in the query is assigned to the corresponding cluster. The query (Q) has the same form as a column in the term-document matrix. Tf-Idf is performed on Q. 4. The query is projected into latent semantic space by multiplication with the transformation matrix U t , Q proj = U t Q. 5. Distance between the query (Q proj ) and all images in the database (columns of SV t ) is computed and the images are ranked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">IPAL-MF</head><p>This run was produced by the use of features described in section 2.3. The similarity distance used between two images i and j is δ I (i, j).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Result Analysis</head><p>Mean Average Prevision resulting from each run is summarized in table <ref type="table" coords="10,451.31,571.87,3.87,9.96" target="#tab_1">1</ref>. IPAL-PW-PFB3 has produced our best Mean Average Precision. In this case, textual information extracted from the 3 top images retrieved by the image retrieval engine are used for text query expansion.</p><p>One unexpected result is the degradation of the results (compared to IPAL-PW-PFB3) when applying the appearance based re-ranking algorithm. As expected, mean average precision is lower for the run IPAL-PW-PFB3-RR300 than for the run IPAL-PW-PFB3-RR60. The difference between the two runs is of 7.9%. Run IPAL-PW-PFB3-RR300 shows that when the number of images considered by re-ranking increases, MAP decreases.</p><p>For textual run only, the use of WordNet concepts decrease the MAP. We have not used any of the semantic links provided by WordNet (like hypernym between "bird" and "animal") and we have notice some problem in sense disambiguation (like "church" not recognized as a building). This may explain the lake of improvement. The role of noun phrases seems also not really crucial as IPAL-EI is lower than single terms indexing IPAL-PW. Giving equal importance to single and composed terms is hence a bad idea.</p><p>For visual only runs, Latent Semantic Analysis (LSA) leads to slightly better results compared to retrieval based on visual similarity in the feature space. However, mean average precision for visual runs remains very low. Precision at 10 documents (P10) is 0.1417 for run IPAL-LSA and 0.1050 for run IPAL-MF. Precision at 20 documents (P20) is 0.1075 for run IPAL-LSA and 0.0883 for run IPAL-MF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Our approach to this year competition was based on a cooperative approach between an image retrieval and a text retrieval system. These experiments show that the combined use of a text retrieval and an image retrieval systems leads to better performance but only for inter media pseudo relevance feedback and not for late fusion. One surprising aspect of these results is that re-ranking based on visual appearance reduces mean average precision.</p><p>The IAPR image database is challenging. Many concepts are represented with a large variety of appearances. Query by content using a few images cannot lead to satisfactory results by using only appearance-based techniques. Indeed, a few samples a given concept cannot capture its conceptual essence.</p><p>MAP remains low and is probably still too low to be used in practical conditions. A lot of work has to be done to improve the quality of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Future Work</head><p>We believe that machine learning techniques should be used to obtain an conceptual abstraction of the query images. In this case, the issue is to train the concept detectors. Providing manually a sufficient number of image samples is extremely tedious and does not really scale-up to a large number of concepts. We believe that textual annotations could help building training sets easily and to help raising low-level image features at a semantic level. One of our short-term goals is to apply Latent Semantic Analysis on both image and text modalities. In this case, the size of the resulting term-document matrix is potentially huge and technical problems related to memory management will be encountered. We plan to integrate advanced image interpretation techniques based on prior knowledge on categories of scenes of interest (e.g. indoor, outdoor). We are also interested in adding semi-automatic and ontology-driven feedback and re-ranking.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,134.77,260.71,345.47,9.96;4,134.77,272.66,345.46,9.96;4,134.77,284.62,345.45,9.96;4,134.77,296.58,345.50,9.96;4,134.77,308.53,345.32,10.39;4,134.77,320.48,41.69,9.96;4,176.50,319.50,4.23,6.26;4,176.50,325.70,2.81,6.26;4,185.70,320.48,57.46,10.39;4,243.67,320.48,236.71,9.96;4,134.77,332.78,10.08,10.05;4,147.56,332.44,310.59,9.96"><head>Feature</head><label></label><figDesc>Extraction. The feature extraction process is based on a tessellation of the images of the database. Each image is split into patches (fig. 1). The visual indexing process is based on patch extraction on all the images of the collection followed by feature extraction on the resulting patches. Let I be the set of the N images in the document collection. First, each image i ∈ I is split into n p patches p k i (1 ≤ k ≤ n p ). Patch extraction on the whole collection results in n p × N patches. Fig. 1 shows the result of patch extraction for one image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,134.77,491.84,345.54,8.97;4,134.77,502.79,80.93,8.97;4,266.26,366.25,82.77,110.36"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Each image of the database has been split in patches. In this case the image is split in 5x5 patches.</figDesc><graphic coords="4,266.26,366.25,82.77,110.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,134.77,250.99,345.55,8.97;5,134.77,261.95,345.51,8.97;5,134.77,272.91,190.67,8.97;5,306.74,116.92,70.60,94.14"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An image (a) segmented by the Meanshift segmentation algorithm [6]. The result is a set of regions (b). Obtaining regions that correspond to semantic entities is very challenging and remains an open-problem.</figDesc><graphic coords="5,306.74,116.92,70.60,94.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,134.77,542.83,345.48,8.97;6,134.77,553.79,345.52,8.97;6,134.77,564.75,37.72,8.97"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Overview of pseudo-relevance feedback. The textual annotations associated with the top images retrieved by the image retrieval engine are used for query expansion purposes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,160.10,468.36,294.88,8.97"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Re-ranking comes as a post-process of pseudo-relevance feedback.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="9,245.25,436.35,124.77,8.97"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Principle of late fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="11,134.77,117.73,345.56,148.05"><head>Table 1 .</head><label>1</label><figDesc>Mean Average Precision (MAP) of submitted runs. The best run is produced by pseudo-relevance mechanisms involving the 3 first images retrieved by the image retrieval engine.</figDesc><table coords="11,147.46,117.73,317.27,114.72"><row><cell>Run ID</cell><cell>Run Type</cell><cell>Mean Average Precision</cell></row><row><cell>IPAL-PW-PFB3</cell><cell>Mixed</cell><cell>0.3337</cell></row><row><cell>IPAL-PW-PFB3-RR60</cell><cell>Mixed</cell><cell>0.2206</cell></row><row><cell>IPAL-PW-PFB3-RR300</cell><cell>Mixed</cell><cell>0.1409</cell></row><row><cell>IPAL-WN-MF-LF</cell><cell>Mixed</cell><cell>0.0568</cell></row><row><cell>IPAL-PW</cell><cell>Text</cell><cell>0.1619</cell></row><row><cell>IPAL-WN</cell><cell>Text</cell><cell>0.1428</cell></row><row><cell>IPAL-EI</cell><cell>Text</cell><cell>0.1362</cell></row><row><cell>IPAL-LSA</cell><cell>Visual</cell><cell>0.0321</cell></row><row><cell>IPAL-MF</cell><cell>Visual</cell><cell>0.0173</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,656.30,256.95,8.97"><p>http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,144.73,656.30,119.04,8.97"><p>http://aspell.sourceforge.net/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="7,144.73,656.30,115.15,8.97"><p>http://ltilib.sourceforge.net/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.95,320.89,337.39,8.97;12,151.52,331.85,328.76,8.97;12,151.52,342.80,328.74,8.97;12,151.52,353.76,20.08,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,365.82,320.89,114.52,8.97;12,151.52,331.85,198.61,8.97">The iapr benchmark: A new evaluation resource for visual information systems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,371.79,331.85,108.49,8.97;12,151.52,342.80,228.07,8.97">LREC 06 OntoImage 2006: Language Resources for Content-Based Image Retrieval</title>
		<meeting><address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct coords="12,142.95,364.37,337.37,8.97;12,151.52,375.34,23.00,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,207.89,364.37,156.74,8.97">Wordnet: a lexical database for english</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,373.28,364.37,64.22,8.97">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.95,385.95,337.36,8.97;12,151.52,396.90,328.80,8.97;12,151.52,407.86,189.31,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,214.81,385.95,213.54,8.97">X-iota: An open xml framework for ir experimentation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Chevallet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,333.17,396.90,24.68,8.97">AIRS</title>
		<title level="s" coord="12,424.52,396.90,55.80,8.97;12,151.52,407.86,80.90,8.97">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Myaeng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3411</biblScope>
			<biblScope unit="page" from="263" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.95,418.47,337.34,8.97;12,151.52,429.43,328.74,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,202.02,418.47,221.98,8.97">Probabilistic part-of-speech tagging using decision trees</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,445.11,418.47,35.18,8.97;12,151.52,429.43,296.08,8.97">Proceedings of International Conference on New Methods in Language Processing</title>
		<meeting>International Conference on New Methods in Language Processing</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.95,440.05,337.35,8.97;12,151.52,451.00,101.74,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,251.27,440.05,224.94,8.97">Texture features for browsing and retrieval of image data</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,151.52,451.00,24.17,8.97">PAMI</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="837" to="842" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.95,461.61,337.36,8.97;12,151.52,472.57,139.94,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,262.43,461.61,217.87,8.97;12,151.52,472.57,30.32,8.97">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,189.72,472.57,24.17,8.97">PAMI</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="603" to="619" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.95,483.18,337.37,8.97;12,151.52,494.14,136.71,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,208.96,483.18,233.31,8.97">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,454.33,483.18,25.99,8.97;12,151.52,494.14,63.74,8.97">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.95,504.76,337.36,8.97;12,151.52,515.72,328.79,8.97;12,151.52,526.67,168.74,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,395.22,504.76,85.09,8.97;12,151.52,515.72,88.44,8.97">Visual categorization with bags of keypoints</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Willamowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,261.60,515.72,218.70,8.97;12,151.52,526.67,78.18,8.97">ECCV International Workshop on Statistical Learning in Computer Vision</title>
		<meeting><address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.95,537.28,337.37,8.97;12,151.52,548.24,328.77,8.97;12,151.52,559.21,328.80,8.97;12,151.52,570.16,71.32,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,233.35,537.28,229.26,8.97">Query expansion using local and global document analysis</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,151.52,548.24,328.77,8.97;12,151.52,559.21,213.04,8.97">SIGIR &apos;96: Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="4" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.60,580.77,337.70,8.97;12,151.52,591.73,328.76,8.97;12,151.52,602.69,25.55,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,388.23,580.77,92.06,8.97;12,151.52,591.73,125.19,8.97">Toward cross-language and cross-media image retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">I</forename><surname>Oumohmed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mignotte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Y</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,297.50,591.73,178.03,8.97">Working Notes for the CLEF 2004 Workshop</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.60,613.30,337.71,8.97;12,151.52,624.26,265.85,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,291.78,613.30,188.53,8.97;12,151.52,624.26,16.78,8.97">Multimedia search with pseudo-relevance feedback</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,190.65,624.26,158.93,8.97">Intl Conf on Image and Video Retrieval</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.60,634.87,337.69,8.97;12,151.52,645.83,156.89,8.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,318.36,634.87,158.14,8.97">Introduction to latent semantic analysis</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">W</forename><surname>Foltz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Laham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,151.52,645.83,79.30,8.97">Discourse Processes</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="259" to="284" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
