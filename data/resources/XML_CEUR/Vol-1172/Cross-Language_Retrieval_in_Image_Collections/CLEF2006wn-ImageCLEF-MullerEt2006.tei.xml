<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,103.92,98.53,395.36,15.61;1,174.48,120.49,254.08,15.61">Overview of the ImageCLEFmed 2006 medical retrieval and annotation tasks</title>
				<funder ref="#_tg6UGV6">
					<orgName type="full">Swiss National Science Foundation (FNS)</orgName>
				</funder>
				<funder ref="#_pJgrgXW">
					<orgName type="full">DFG (Deutsche Forschungsgemeinschaft)</orgName>
				</funder>
				<funder ref="#_mDp52ZM">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_M5BD9nz">
					<orgName type="full">American National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_5y2y4bF">
					<orgName type="full">MUSCLE NoE</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,174.60,153.97,68.00,9.96"><forename type="first">Henning</forename><surname>Müller</surname></persName>
							<email>henning.mueller@sim.hcuge.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Medical Informatics</orgName>
								<orgName type="institution">University and Hospitals of Geneva</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,253.20,153.97,78.79,9.96"><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Dep</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,342.48,153.97,78.74,9.96"><forename type="first">Thomas</forename><surname>Lehmann</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Medical Informatics</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,203.16,168.01,54.28,9.96"><forename type="first">Paul</forename><surname>Clough</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Oregon Health and Science University (OHSU)</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,267.84,168.01,53.91,9.96"><forename type="first">Eugene</forename><surname>Kim</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Oregon Health and Science University (OHSU)</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,332.16,168.01,63.26,9.96"><forename type="first">William</forename><surname>Hersh</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Oregon Health and Science University (OHSU)</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Sheffield University</orgName>
								<address>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,103.92,98.53,395.36,15.61;1,174.48,120.49,254.08,15.61">Overview of the ImageCLEFmed 2006 medical retrieval and annotation tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3613ED980BD20341D1687FCAD0F291A8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Management]: Languages-Query Languages Measurement, Performance, Experimentation Image Retrieval, Performance Evaluation, Image Classification, Medical Imaging</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the medial image retrieval and the medical annotation tasks of ImageCLEF 2006. These tasks are described in a separate paper from the other task to reduce the size of the overview papaer.These two medical tasks are described separately with respect to the goals, databases used, topics created and distributed among participants, results and techniques used. The best performing techniques are described in more detail to provide better insights about successful strategies. Some ideas for future tasks are also presented.</p><p>The ImageCLEFmed medical image retrieval task had 12 participating groups and received 100 submitted runs. Most runs were automatic, with only a few manual or interactive. Purely textual runs were in the majority compared to purely visual, runs but most runs were mixed, i.e., using visual and textual information. None of the manual or interactive techniques were significantly better than those used for the automatic runs. The best-performing systems used visual and textual techniques combined, but combinations of visual and textual features often did not improve a system's performance. Purely visual systems only performed well on the visual topics.</p><p>The medical automatic annotation used a larger database in 2006, with 10'000 training images and 116 classes, up from 57 in 2005. Twelve participating groups submitted 27 runs. Despite the much larger number of classes, results were almost as good as in 2005 and a clear improvement in performance could be shown. The bestperforming system of 2005 would have only received a position in the upper middle part in 2006.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>ImageCLEF<ref type="foot" coords="2,143.16,133.69,3.97,4.85" target="#foot_0">1</ref>  <ref type="bibr" coords="2,151.08,134.53,10.57,9.96" target="#b2">[3]</ref> started within CLEF<ref type="foot" coords="2,257.52,133.69,3.97,4.85" target="#foot_1">2</ref> (Cross Language Evaluation Forum) in 2003. A medical image retrieval task was added in 2004 to explore domain-specific multilingual information retrieval as well as multi-modal retrieval (combining visual and textual features for retrieval). Since 2005, a medical retrieval and a medical image annotation task are parts of ImageCLEF.</p><p>This paper concentrates on the two medical tasks, whereas a second paper <ref type="bibr" coords="2,424.69,182.29,10.45,9.96" target="#b1">[2]</ref> describes the new object classification and the photographic retrieval tasks. More detailed information can also be found on the task web pages for ImageCLEFmed <ref type="foot" coords="2,305.52,205.33,3.97,4.85" target="#foot_2">3</ref> and the medical annotation task <ref type="foot" coords="2,455.64,205.33,3.97,4.85" target="#foot_3">4</ref> . A detailed analysis of the 2005 medical image retrieval task is available in <ref type="bibr" coords="2,367.56,218.17,10.00,9.96" target="#b7">[8]</ref>. <ref type="bibr" coords="2,90.00,248.51,8.08,13.01" target="#b1">2</ref> The Medical Image Retrieval Task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">General Overview</head><p>In 2006, the medical retrieval task was run for the third year, and for the second year in a row with the same dataset of over 50'000 images from four distinct collections. One of the most interesting findings for 2005 was the variable performance of systems based on whether the topics had been classified as amenable to visual, textual, or mixed retrieval methods. For this reason, we developed 30 topics for 2006, with 10 each in the categories of being amenable to visual, textual, or mixed retrieval methods.</p><p>The scope of the topic development was slightly enlarged by using the log files of a medical media search engine of the Health on the Net (HON) foundation. Analysis of these logs showed a great number of general topics not covering the entire four axes defined in 2005:</p><p>• Anatomic region shown in the image;</p><p>• Image modality (e.g. x-ray, CT, MRI, gross pathology, etc.);</p><p>• Pathology or disease shown in the image;</p><p>• Abnormal visual observation (e.g. enlarged heart).</p><p>The process of relevance judgments was similar to 2005 and, for the evaluation of the results, the trec eval package was used, since it is the standard in information retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Registration and participation</head><p>In 2006, a record number of 47 groups registered for ImageCLEF and among these, 37 also registered for the medical image retrieval task. Groups came from four continents and from a total of 16 countries.</p><p>Unfortunately, many of the registered group did not send in results In the end, 12 groups from 8 countries submitted results. Each entry below describes briefly the techniques used for their submissions.</p><p>• Concordia University, Canada. The CINDI group from Concordia University, Montreal, Canada submitted a total of four runs, one purely textual, one purely visual, and two combined runs. Text retrieval was based on Apache Lucene. For visual information a combination of global and local features were used and compared using the Euclidean distance.</p><p>Most of the submissions used relevance feedback.</p><p>• Microsoft Research, China. Microsoft Research China submitted one purely visual run using a combination of various features accounting for color, texture, and blocks.</p><p>• Institute for Infocomm Research I2R-IPAL, Singapore. IPAL submitted 26 runs, the largest number of any group. Textual and visual runs were prepared in cooperation with I2R. For visual retrieval patches of image regions were applied and manually classified into semantically valid categories and mapped to Unified Medical Language System (UMLS). For the textual analysis, the three languages were separately mapped to UMLS terms and then applied to retrieval. Several classifiers based on SVMs and other classical approaches were used and combined.</p><p>• University Hospitals of Freiburg, Germany. The Freiburg group submitted a total of 9 runs mainly using textual retrieval. Interlingua and the original language were used (morphosaurus and Lucene). Queries were preprocessed by removing the "show me" test. Runs differed in query language and combination with GIFT settings.</p><p>• Jaen University (SINAI), Spain. The SINAI group submitted 12 runs, three of them using only textual information and nine using a textual retrieval system and addind provided data from the GIFT image retrieval system. The runs differed in settings for "information gain" and the weighting of textual and visual information.</p><p>• Oregon Health and Science University (OHSU), USA. OHSU performed manual modification of queries and then attempted to augment output by fusing results from visual runs. One set of runs from OHSU established a baseline using the text of the topics as given. Another set of runs then manually modified the topic text removing common words and adding synonyms. For both sets of runs, there were submissions in each of the three individual languages (English, French, German) plus a merged run with all three and another run with the English topics expanded with automatic translation using the Babelfish translator.</p><p>The manual modification of the queries improved performance substantially, though still below other groups' automated methods. The best results came from the English-only queries, followed by the automatically translated and the merged queries. One additional run assessed fusing data from a visual run with the merged queries. This decreased MAP but did improve precision at high levels of retrieval output, e.g., precision at 10 and 30 images.</p><p>• I2R Medical Analysis Lab, Singapore. Their submission was together with the IPAL group from the same lab.</p><p>• MedGIFT, University and Hospitals of Geneva, Switzerland. The University and Hospitals of Geneva relied on two retrieval systems for their submission. The visual part was performed with the medGIFT retrieval system. The textual retrieval used a mapping of the query and document text towards concepts in the MeSH (Medical Subject Headings) terminology. Then, matching was performed with a frequency-based weighting methods using easyIR. All results were automatic runs using visual, textual and mixed features. Separate runs were submitted for the three languages.</p><p>• RWTH Aachen University -Computer Science, Germany. RWTHi6 submitted a total of nine runs, all using the FIRE retrieval system and a variety of features describing color, texture, and global appearance in different ways. For one of the runs, the queries and the qrels of last year were used as training data to obtain weights for the combination of features using maximum entropy training. One run was purely textual, three runs were purely visual, and the remaining five runs used textual and visual information. All runs were fully-automatic runs without any user interaction or manual tuning.</p><p>• RWTH Aachen University -Medical Informatics, Germany. RWTHmi submitted two purely visual runs without any user interaction. Both runs used a combination of various global appearance features compared using invariant distance measures and texture features. The runs differed in the weights for the features used.</p><p>• State University New York, Buffalo, USA. SUNY submitted fourruns, two purely textual and two using textual and visual information. Parameters for their system were tuned using the ImageCLEF 2005 topics, and automatic relevance feedback was used in different variations.</p><p>• LITIS Lab, INSA Rouen, France. The INSA group from Rouen submitted one run using visual and textual information. For the textual information the MeSH dictionaries were used and the images were represented by various features accounting for global and local information. Most of the topics were treated fully automatic, and only four topics were treated with manual interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Databases</head><p>In 2006, the same dataset was used as in 2005 containing four distinct sets of images. The Casimage <ref type="foot" coords="4,104.52,266.17,3.97,4.85" target="#foot_4">5</ref> dataset was made available to participants <ref type="bibr" coords="4,297.01,267.01,14.60,9.96" target="#b13">[14]</ref>, containing almost 9'000 images of 2'000 cases <ref type="bibr" coords="4,90.00,279.01,14.60,9.96" target="#b14">[15]</ref>. Images present in Casimage include mostly radiology modalities, but also photographs, Pow-erPoint slides and illustrations. Cases are mainly in French, with around 20% being in English and 5% without annotation. We also used the PEIR<ref type="foot" coords="4,328.56,302.05,3.97,4.85" target="#foot_5">6</ref> (Pathology Education Instructional Resource) database with annotation based on the HEAL<ref type="foot" coords="4,323.40,314.05,3.97,4.85" target="#foot_6">7</ref> project (Health Education Assets Library, mainly Pathology images <ref type="bibr" coords="4,201.49,326.77,10.23,9.96" target="#b0">[1]</ref>). This dataset contains over 33.000 images with English annotations, with the annotation being on a per image and not a per case basis as in Casimage. The nuclear medicine database of MIR, the Mallinkrodt Institute of Radiology<ref type="foot" coords="4,379.80,349.81,3.97,4.85" target="#foot_7">8</ref>  <ref type="bibr" coords="4,387.48,350.77,14.60,9.96" target="#b15">[16]</ref>, was also made available to us for ImageCLEFmed. This dataset contains over 2.000 images mainly from nuclear medicine with annotations provided per case and in English. Finally, the PathoPic<ref type="foot" coords="4,412.92,373.81,3.97,4.85" target="#foot_8">9</ref> collection (Pathology images <ref type="bibr" coords="4,122.87,386.53,10.74,9.96" target="#b4">[5]</ref>) was included in our dataset. It contains 9.000 images with extensive annotation on a per image basis in German. Part of the German annotation is translated into English. As such, we were able to use a total of more than 50.000 images, with annotations in three different languages.</p><p>Through an agreement with the copyright holders, we were able to distribute these images to the participating research groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Query topics</head><p>The query topics were based on two surveys performed in Portland and Geneva <ref type="bibr" coords="4,432.38,480.73,10.45,9.96" target="#b6">[7,</ref><ref type="bibr" coords="4,445.58,480.73,11.62,9.96" target="#b11">12]</ref>. In addition to this, a log file of a media search engine HON<ref type="foot" coords="4,293.64,491.77,7.93,4.85" target="#foot_9">10</ref> was used to create topics. Based on the surveys, topics for ImageCLEFmed were developed along the following axes:</p><p>• Anatomic region shown in the image;</p><p>• Image modality (x-ray, CT, MRI, gross pathology, etc.);</p><p>• Pathology or disease shown in the image;</p><p>• Abnormal visual observation (e.g. enlarged heart).</p><p>Still, as the HON log-files indicated rather general topics than the fairly specific ones used in 2005, we used real queries from these log-files in 2006. We could not use the most frequent queries, since they were too general, e.g. heart, lung, etc., but rather those that satisfied at least two of the Show me chest CT images with nodules. Zeige mir CT Bilder der Lunge mit Knötchen. Montre-moi des CTs du thorax avec nodules.  defined axes and that appeared frequently. After identifying over 50 of such candidate topics, we grouped them into three classes based upon as estimation of what retrieval techniques to which they would be most retrievable -visual, mixed, or textual. Another goal was to cover frequent diseases and have a balanced variety of imaging modalities and anatomic regions corresponding to the database that contains many pathology images.</p><p>After choosing ten queries for each of the three categories, we searched query images on the web manually. In 2005, images were taken partly from the collection. Although they were cropped most of the time, having images from another collection made the visual task more challenging, as these images could be from other modalities and have completely different characteristics concerning texture, luminosity, etc. This year we created 10 topics for each of the 3 groups for a total of 30 topics.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Relevance Judgements</head><p>For relevance judging, pools were built from all images for a given topic ranked in the top 30 retrieved. This gave pools of anywhere from 647 to 1187 images, with a mean of 910 per topic. Relevance judgments were performed by seven US physicians enrolled in the OHSU biomedical informatics graduate program. Eleven of the 30 topics were judged in duplicate, with two judged by three different judges. Each topic had a designated "original" judge from the seven.</p><p>A total of 27,306 relevance judgements were made. (These were primary judgments; ten topics had duplicate judgments that we will analyze later.) The judgments were turned into a qrels file, which was then used to calculate results with trec eval. We used Mean Average Precision (MAP) as the primary evaluation measure. We note, however, that its orientation to recall (over precision) may may not be appropriate for many image retrieval tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Submissions and Results</head><p>A total of 12 groups participated in ImageCLEFmed 2006 from eight different countries (Canada, China, France, Germany, Singapore, Spain, Switzerland, and the United States). These groups collectively submitted 100 runs, with each group submitting anywhere from 1 to 26 runs.</p><p>We defined two categories for the submitted runs: one for the interaction used (automatic -no human intervention, manual -human modification of the query before the output of the system is seen, and interactive -human modification of the query after the output of the system is seen ) and one for the data used for retrieval (visual, textual, or a mixture). The majority of the submitted runs were automatic. There were fewer visual runs than there were textual and mixed runs.</p><p>Figure <ref type="figure" coords="6,136.33,640.09,4.98,9.96" target="#fig_4">4</ref> gives an overview of the number of relevant images per topic and of the performance that this topic obtained on average (MAP). It can be seen that the variation in this case was substantial. Some topics had several hundred relevant images in the collection, whereas other only had very few. Likewise, performance could be extremely good for a few topics and extremely bad for others. There does not appear to be a direct connection between number of relevant images for a topics and the average performance that systems obtain.</p><p>In Figure <ref type="figure" coords="7,149.77,400.57,4.98,9.96" target="#fig_5">5</ref> is a comparison of several performance measurements for all submitted runs. In particular when looking at early precision (P(30)) these variations were quite large, but slowly disappear for later precision (P(100)). On the other hand, these measures do seem to correlate fairly well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.1">Automatic retrieval</head><p>The category of automatic runs was by far the most common category for results submissions. A total of 79 of the 100 submitted runs were in this category. In Table <ref type="table" coords="7,414.00,492.61,4.98,9.96" target="#tab_0">1</ref> the best run of each participating system per category is shown as is in the following tables. Showing all 100 runs would have results in information difficult to read.</p><p>We can see that the best submitted automatic run was a mixed run and that other mixed runs had very good results. Nonetheless, several of the very good results were textual only, so a generalization does not seem completely possible. Visual systems had a fairly low overall performance, although for the first ten visual topics, their performance was very good.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.2">Manual retrieval</head><p>Figure <ref type="figure" coords="7,121.33,608.65,4.98,9.96" target="#fig_1">2</ref> shows the submitted manual runs. With the small numbers of these runs, generalization is difficult.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.3">Interactive retrieval</head><p>Table <ref type="table" coords="7,117.72,664.93,4.98,9.96" target="#tab_2">3</ref> shows the submitted interactive runs. The first run had good performance but was still not better than the best automatic run of the same group. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Conclusions</head><p>The best overall run by the IPAL institute is an automatic run using visual and textual features.</p><p>From the submitted runs, we can say that interactive and manual runs do not perform better than the automatic runs. This may be partly due to the fact that most groups submitted many more automatic runs than other runs. The automatic approach appears to be less time-consuming and most research groups have more experience in optimizing these runs. Visual features seem to be mainly good for the visual topics but fail to help for the semantic features. Text-only runs perform very well and only a few mixed runs manage to be better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Medical Automatic Annotation Task</head><p>Automatic image annotation is a classification task, where a given image is automatically labeled with a text describing its contents. In restricted domains, the annotation may be just a class from a constrained set of classes, or it may be an arbitrary narrative text describing the contents of the images. Last year, the medical automatic annotation task was performed in ImageCLEF to compare state-of-the-art approaches to automatic image annotation and classification and to make a first step toward using automatically annotated images in a multi-modal retrieval system <ref type="bibr" coords="9,90.00,277.93,14.60,9.96" target="#b12">[13]</ref>. This year's medical automatic annotation task builds on top of last year, with 1,000 new images to be classified were collected and the number of classes is more than doubled, resulting in a harder task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Database &amp; Task Description</head><p>The complete database consists of 11,000 fully classified radiographs taken randomly from medical practice at the RWTH Aachen University Hospital. A total of 9,000 of these were released together with their classification as training data, with another 1,000 also published with their classification as validation data to allow the groups for tuning their classifiers in a standardized manner. One thousand additional images were released at a later date without their classification as test data. These 1,000 images had to be classified using the 10,000 images (9,000 training + 1,000 validation) as training data.</p><p>The complete database of 11,000 images was subdivided into 117 classes according to the complete IRMA code annotation <ref type="bibr" coords="9,234.85,443.65,14.60,9.96" target="#b10">[11]</ref>. The IRMA code is a multi-axial scheme assessing anatomy, biosystem, creation and direction of imaging. Currently, this code is available in English and German, but could easily be translated to other languages. It is planned to use the result of such automatic annotation experiments for further, textual image retrieval tasks in the future.</p><p>Example images from the database together with their class numbers are shown in Figure <ref type="figure" coords="9,505.22,491.41,3.90,9.96" target="#fig_6">6</ref>. The classes in the database are not uniformly distributed, for example, class 111 has a 19.3% share of the complete dataset, class 108 has a 9.2% share of the database, while six classes have only 1 0 / 00 or less.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Participating Groups &amp; Methods</head><p>In total, 28 groups registered and 12 of these submitted runs. For each group, a brief description of the methods of the submitted runs is provided. The groups are listed alphabetically by their group id, which is later used in the results section to refer to the groups.</p><p>CINDI. The CINDI group from Concordia University in Montreal, Canada submitted 5 runs using a variety of features including MPEG-7 Edge Histogram Descriptor, MPEG-7 Color Layout Descriptor, invariant shape moments, downscaled images, and semi-global features. Some of the experiments combine these features with a principal component analysis (PCA). The dimensionality of the feature vectors is up to 580. For four of the runs, a support vector machine (SVM) is used for classification with different multi-class voting schemes. In one run, the nearest neighbor decision rule is applied. The group expects the run cindi-svm-sum to be their best submission. MSRA. The Web Search and Mining Group from Microsoft Research Asia submitted two runs. One run uses a combination of gray-block features, block-wavelet features, features accounting for binarized images, and an edge histogram. In total, a 397-dimensional feature vector is used. The other run uses a bag of features approach with vector quantization, where a histogram of quantized vectors is computed region-wise on the images. In both runs, SVM is used for classification. The group did not identify which of these they expect to be better. MU I2R. The Media Understanding Group of the Institute for Infocomm Research, Singapore submitted one run. In this run, a two-stage medical image annotation method was applied. In the first stage, the images are reduced to 32×32 pixels (1024 dimensional vector) and classified using a support vector machine. In the second stage, those decisions for which the support vector machine was unsure were refined using a classifier that was trained on a subset of the training images. In addition to down-scaled images, SIFT (scale invariant feature transformation) features and principal components of features were used for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NCTU DBLAB.</head><p>The DBLAB of the National Chiao Tung University in Hsinchu, Taiwan submitted one run using tree image features, Gabor texture features, coherence moment and related vector layout as image descriptors. The classification was done using a nearest neighbor classifier.</p><p>OHSU. The Department of Medical Informatics &amp; Clinical Epidemiology of the Oregon Health and Science University in Portland, OR, USA submitted 4 runs. For image representation, a variety of descriptors was tested including 16×16 pixel versions of the images, and partly localized gray level cooccurrence matrix (GLCM) features resulting in a feature vector of up to 380 components. For classification, a multilayer perceptron were used and settings were optimized using the development set.</p><p>RWTHi6. The Human Language Technology and Pattern Recognition Group from the RWTH Aachen University in Aachen, Germany submitted three runs. One uses the image distortion model (IDM) that was used for the best run of last year, and the other a sparse histogram of image patches and absolute position. The IDM run is based on a nearest neighbor classifier, while the other runs use SVM or a maximum entropy classifier. The feature vectors for the IDM experiments have less than 1024 components and the sparse histograms have 65536 bins. The group expects the run SHME to be their best submission.</p><p>RWTHmi. The Image Retrieval in Medical Applications (IRMA) group, Department of Medical Informatics, RWTH Aachen University Hospital in Aachen, Germany submitted two runs using cross-correlation on 32×32 images with explicit translation shifts, IDM for X×32 images, global texture features as proposed by Tamura, and global texture features as proposed by Castelli et al. based on fractal concepts resulting in an approximately 2500-dimensional feature vector. For classification, a nearest neighbor classifier was used. For the run RWTHmi-opt weights for these features were optimized on the development set, and for the run RWTHmi-baseline the default parameters of the IRMA system were used.</p><p>UFR. The Pattern Recognition and Image Processing group from the University of Freiburg in Freiburg, Germany submitted two runs using gradient-like features extracted over interest points. Gradients over multiple directions and scale are calculated and used as a local feature vector. The features are clustered to form a code book of size 20 and a cluster co-occurrence matrix is computed over multiple distance ranges and multiple angle ranges (since rotation invariance is not desired), resulting in a 4-D array per image which is flattened and used as final, approximately 160000-dimensional, feature vector. Classification is done using multi-class SVM in a one-vs-rest approach with a histogram intersection kernel.</p><p>ULG. The Systems and Modeling group of the Institute Montefiore from Liège, Belgium extracts a large number of possibly overlapping, squared sub-windows of random sizes and at random positions from training images. Then, an ensemble model composed by twenty randomized trees is automatically built based on size-normalized versions of the sub-windows. It is operated directly on their pixel values to predict classes of sub-windows. Given this sub-window classifier, a new image is classified via sub-windows and combining the classification decisions. The feature vectors are 576-dimensional. The group expects the run ULG-SYSMOD-RANDOM-SUBWINDOWS-EX to be their best submission.</p><p>UTD. The Data Mining Laboratory group of the University of Texas at Dallas, Richardson, TX, USA submitted one run. The images are scaled to 16×16 pixels, and their dimensionality is reduced by PCA, resulting in a maximally 256-dimensional feature vector. Then, a weighted k-nearest neighbor algorithm is applied for classification.</p><p>MedGIFT. The medGIFT group of the University and Hospitals of Geneva submitted three runs to the medical automatic annotation task. One was entirely based on tf/idf weighting of the GNU Image Finding Tool (GIFT) and thus acted as a baseline using only collection frequencies of features with no learning on the training data supplied. For the second run features are weighted with an additional factor, learned from the supplied training data. For these submissions a 5-NN was used as classifier. The third submission is a combination of several separate runs by voting. The combined results are quite different, so the combination-run is expected to be the best submission. The runs were submitted after the evaluation ended and are thus not ranked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>The results from the evaluation are shown in Table <ref type="table" coords="12,326.90,119.53,3.90,9.96" target="#tab_3">4</ref>. The error rates ranged from 16.2% to 34.1%. Based on the training data, a system guessing the most frequent group for all 1,000 test images would result in a 80.5% error rate, since 195 radiographs of the test set were from class 111, which was the biggest class in the training data. A more realistic baseline is given by a nearest neighbor classifier using Euclidean distance to compare the images scaled to 32×32 pixels <ref type="bibr" coords="12,90.00,179.41,10.00,9.96" target="#b8">[9]</ref>. This classifier yields an error rate of 32.1%. The average confusion matrix of all submitted runs is shown in Figure <ref type="figure" coords="12,194.42,191.29,3.90,9.96" target="#fig_7">7</ref>. Obviously, a diagonal structure is reached. Thus on the average, many images were classified correctly, but it can also be seen that some classes have high inter-class similarity: e.g. classes 108 to 111 are often confused. In total, many images from other classes were classified to be from class 111, which was the class with the highest amount of training data. Obviously, not all classes were equally difficult. A tendency that classes with only few training instances were harder to classify than classes with a large amount of training data could be seen; which was to be expected and had been reported in the literature earlier <ref type="bibr" coords="12,410.40,263.05,10.00,9.96" target="#b5">[6]</ref>.</p><p>Given the confidence files of all runs, we tried to combine the classifiers by the sum rule. Therefore, all confidence files were normalized such that the confidences could be interpreted as a-posteriori probabilities p(c|x) where c was the class and x the observation. Unlike last years results, where this technique could not improve the results, clear improvements were possible combining several classifiers <ref type="bibr" coords="12,214.79,322.81,14.60,9.96" target="#b9">[10]</ref>: Using the top 3 ranked classifiers in combination, an error rate of 14.4% was obtained. The best result was obtained combining the top 7 ranked classifiers. Note, that here no additional parameters were tuned but the classifiers were combined weighted equally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussion</head><p>The most interesting observation of this year's evaluation can be seen when comparing the results with the results of last year: The RWTHi6-IDM <ref type="bibr" coords="12,314.41,405.01,10.57,9.96" target="#b3">[4]</ref> system that performed best in last years task (error rate: 12.1%) obtained an error rate of 20.4% this year. This increase in error rate can be explained by the larger number of classes and thus more similar classes that can easily be confused. On the other hand, 10 methods clearly outperformed this result this year, nine of these use SVMs as a classifier (ranks 2-10) and one using a discriminatively trained log-linear model (rank 1). Thus, it can clearly be stated that the performance of image annotation techniques strongly improved over the last year, and that techniques that were initially developed in the field of object recognition and detection are very well suited for the automatic annotation of medical radiographs.</p><p>Another interesting observation drawn from the combination of classifiers was that in contrast to last year, where a combination of arbitrary classifiers from the evaluation did not lead to an improvement over the best submission, this year a clear improvement was obtained by combining several submissions. A reason for this might be the improved performance of the submissions or the higher diversity among the submitted methods.</p><p>To give an approximate idea of runtime and memory requirements of the various methods we give the dimensionality of the feature vectors used by the groups. Naturally, the dimension of the feature vectors alone does not say very much about runtime, because the used models for classification have a high impact on runtime and memory consumption, too. However, a trend that high dimensional feature vectors lead to good results can clearly be seen in the results as the best three methods use feature vectors of very high dimensionality (65,536 and 160,000 respectively) and implicitly transform these into even higher dimensional feature spaces by the use of kernel methods.  For the medical retrieval task, none of the manual or interactive techniques were significantly better than those used for the automatic runs. The best-performing systems used visual and textual techniques combined but several times a combination of visual and textual features did not improve a system's performance. Thus, combinations for multi-modal retrieval need to done carefully. Purely visual systems only performed well on the visual topics.</p><p>For the automatic annotation task, discriminative methods outperformed methods based on nearest neighbor classification and the top-performing methods were based on the assumption that images consist of images parts which can be modelled more or less independently.</p><p>One goal for future tasks is to motivate groups to work more on interactive or manual runs than automated retrieval. With proper manpower, such runs should be better than even optimized automatic runs. Another future goal is to motivate an increasing number of subscribed groups to participate. Collections are planned to become larger as well to stay realistic. Some groups already complained about too large datasets, so a smaller second dataset might be an option for these groups to at least submit some results and compare them with the other techniques.</p><p>For the automatic annotation task, a future goal is to use textual labels with varying annotation precision rather than a simple class-based annotation scheme and to consider semi-automatic annotation methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,220.80,202.69,161.44,9.96;5,310.32,224.18,124.67,93.50"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example for a visual topic.</figDesc><graphic coords="5,310.32,224.18,124.67,93.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,220.20,367.93,162.64,9.96;5,168.00,224.18,138.87,93.50"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example for a mixed topic.</figDesc><graphic coords="5,168.00,224.18,138.87,93.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,122.76,519.37,319.56,9.96;5,220.44,646.69,162.12,9.96;5,203.52,658.69,196.11,9.96;5,202.56,670.57,198.02,9.96"><head></head><label></label><figDesc>Figures 1, 2, 3 show examples for a visual, a mixed and a semantic topic. Show me x-ray images of bone cysts. Zeige mir Röntgenbilder von Knochenzysten. Montre-moi des radiographies de kystes d'os.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,214.44,692.53,174.15,9.96;5,182.64,548.66,109.57,93.50"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example for a semantic topic.</figDesc><graphic coords="5,182.64,548.66,109.57,93.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,149.16,328.45,304.69,9.96;6,96.72,58.86,409.62,255.10"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Evaluation results and number of relevant images per topic.</figDesc><graphic coords="6,96.72,58.86,409.62,255.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="7,91.68,356.77,419.67,9.96;7,90.00,58.88,455.06,283.40"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Evaluation results for the best runs of each system in each category, ordered by MAP.</figDesc><graphic coords="7,90.00,58.88,455.06,283.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="10,90.00,214.69,423.03,9.96;10,90.00,226.69,270.37,9.96;10,339.24,144.20,75.27,56.60"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Example images from the IRMA database together with their class numbers. the bottom row emphasized the intra-class variety of the IRMA database.</figDesc><graphic coords="10,339.24,144.20,75.27,56.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="14,90.00,576.85,423.13,9.96;14,90.00,588.73,423.01,9.96;14,90.00,600.73,423.12,9.96;14,90.00,612.61,133.93,9.96;14,90.00,139.36,423.00,423.00"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Average confusion matrix over all runs of the medical automatic annotation task. Dark points denote high entries, white points denote zero. On the x-axis, the correct class is given and on the y-axis the class to which images have been classified is given. For visualization purposes values are in logarithmic scale.</figDesc><graphic coords="14,90.00,139.36,423.00,423.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,150.00,102.73,295.23,234.96"><head>Table 1 :</head><label>1</label><figDesc>Overview of the automatic runs.</figDesc><table coords="8,150.00,112.57,295.23,225.12"><row><cell>Run identifier</cell><cell cols="3">visual textual MAP</cell><cell>R-Prec</cell></row><row><cell>IPAL Cpt Im</cell><cell>x</cell><cell>x</cell><cell>0.3095</cell><cell>0.3459</cell></row><row><cell>IPAL Textual CDW</cell><cell></cell><cell>x</cell><cell>0.2646</cell><cell>0.3093</cell></row><row><cell>GE 8EN.treceval</cell><cell></cell><cell>x</cell><cell>0.2255</cell><cell>0.2678</cell></row><row><cell>UB-UBmedVT2</cell><cell>x</cell><cell>x</cell><cell>0.2027</cell><cell>0.2225</cell></row><row><cell>UB-UBmedT1</cell><cell></cell><cell>x</cell><cell>0.1965</cell><cell>0.2256</cell></row><row><cell>UKLFR origmids en en</cell><cell></cell><cell>x</cell><cell>0.1698</cell><cell>0.2127</cell></row><row><cell>RWTHi6-EnFrGePatches</cell><cell>x</cell><cell>x</cell><cell>0.1696</cell><cell>0.2078</cell></row><row><cell>RWTHi6-En</cell><cell></cell><cell>x</cell><cell>0.1543</cell><cell>0.1911</cell></row><row><cell>OHSU baseline trans</cell><cell></cell><cell>x</cell><cell>0.1264</cell><cell>0.1563</cell></row><row><cell>GE vt10.treceval</cell><cell>x</cell><cell>x</cell><cell>0.12</cell><cell>0.1703</cell></row><row><cell>SINAI-SinaiOnlytL30</cell><cell></cell><cell>x</cell><cell>0.1178</cell><cell>0.1534</cell></row><row><cell>CINDI Fusion Visual</cell><cell>x</cell><cell></cell><cell>0.0753</cell><cell>0.1311</cell></row><row><cell>MSRA WSM-msra wsm</cell><cell>x</cell><cell></cell><cell>0.0681</cell><cell>0.1136</cell></row><row><cell>IPAL Visual SPC+MC</cell><cell>x</cell><cell></cell><cell>0.0634</cell><cell>0.1048</cell></row><row><cell>RWTHi6-SimpleUni</cell><cell>x</cell><cell></cell><cell>0.0499</cell><cell>0.0849</cell></row><row><cell>SINAI-SinaiGiftT50L20</cell><cell>x</cell><cell>x</cell><cell>0.0467</cell><cell>0.095</cell></row><row><cell>GE-GE gift</cell><cell>x</cell><cell></cell><cell>0.0467</cell><cell>0.095</cell></row><row><cell>UKLFR mids en all co</cell><cell>x</cell><cell>x</cell><cell>0.0167</cell><cell>0.0145</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,150.00,435.49,293.55,55.68"><head>Table 2 :</head><label>2</label><figDesc>Overview of the manual runs.</figDesc><table coords="8,150.00,444.85,293.55,46.32"><row><cell>Run identifier</cell><cell cols="3">visual textual MAP</cell><cell>R-Prec</cell></row><row><cell>OHSUeng</cell><cell></cell><cell>x</cell><cell>0.2132</cell><cell>0.2554</cell></row><row><cell>IPAL CMP D1D2D4D5D6</cell><cell>x</cell><cell></cell><cell>0.1596</cell><cell>0.1939</cell></row><row><cell>INSA-CISMef</cell><cell>x</cell><cell>x</cell><cell>0.0531</cell><cell>0.0719</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,150.00,588.97,295.23,67.56"><head>Table 3 :</head><label>3</label><figDesc>Overview of the interactive runs.</figDesc><table coords="8,150.00,598.33,295.23,58.20"><row><cell>Run identifier</cell><cell cols="3">visual textual MAP</cell><cell>R-Prec</cell></row><row><cell>IPAL Textual CRF</cell><cell></cell><cell>x</cell><cell>0.2534</cell><cell>0.2976</cell></row><row><cell>OHSU-OHSU m1</cell><cell>x</cell><cell>x</cell><cell>0.1563</cell><cell>0.187</cell></row><row><cell>CINDI Text Visual RF</cell><cell>x</cell><cell>x</cell><cell>0.1513</cell><cell>0.1969</cell></row><row><cell>CINDI Visual RF</cell><cell>x</cell><cell></cell><cell>0.0957</cell><cell>0.1347</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="13,90.00,182.29,423.05,404.64"><head>Table 4 :</head><label>4</label><figDesc>Results of medical automatic annotation task. If a group submitted several runs, the run that was expected to be their best is marked with '*'</figDesc><table coords="13,109.92,205.93,379.89,381.00"><row><cell></cell><cell>rank Group</cell><cell>Runtag</cell><cell>Error rate [%]</cell></row><row><cell>*</cell><cell>1 RWTHi6</cell><cell>SHME</cell><cell>16.2</cell></row><row><cell>*</cell><cell>2 UFR</cell><cell>UFR-ns-1000-20x20x10</cell><cell>16.7</cell></row><row><cell></cell><cell>3 RWTHi6</cell><cell>SHSVM</cell><cell>16.7</cell></row><row><cell></cell><cell cols="2">4 MedIC-CISMeF local+global-PCA335</cell><cell>17.2</cell></row><row><cell></cell><cell cols="2">5 MedIC-CISMeF local-PCA333</cell><cell>17.2</cell></row><row><cell></cell><cell>6 MSRA</cell><cell>WSM-msra-wsm-gray</cell><cell>17.6</cell></row><row><cell>*</cell><cell cols="2">7 MedIC-CISMeF local+global-PCA450</cell><cell>17.9</cell></row><row><cell></cell><cell>8 UFR</cell><cell>UFR-ns-800-20x20x10</cell><cell>17.9</cell></row><row><cell></cell><cell>9 MSRA</cell><cell>WSM-msra-wsm-patch</cell><cell>18.2</cell></row><row><cell></cell><cell cols="2">10 MedIC-CISMeF local-PCA150</cell><cell>20.2</cell></row><row><cell></cell><cell>11 RWTHi6</cell><cell>IDM</cell><cell>20.4</cell></row><row><cell cols="2">*  12 RWTHmi</cell><cell>opt</cell><cell>21.5</cell></row><row><cell></cell><cell>13 RWTHmi</cell><cell>baseline</cell><cell>21.7</cell></row><row><cell cols="2">*  14 CINDI</cell><cell>cindi-svm-sum</cell><cell>24.1</cell></row><row><cell></cell><cell>15 CINDI</cell><cell>cindi-svm-product</cell><cell>24.8</cell></row><row><cell></cell><cell>16 CINDI</cell><cell>cindi-svm-ehd</cell><cell>25.5</cell></row><row><cell></cell><cell>17 CINDI</cell><cell>cindi-fusion-KNN9</cell><cell>25.6</cell></row><row><cell></cell><cell>18 CINDI</cell><cell>cindi-svm-max</cell><cell>26.1</cell></row><row><cell cols="2">*  19 OHSU</cell><cell>OHSU-iconGLCM2-tr</cell><cell>26.3</cell></row><row><cell></cell><cell>20 OHSU</cell><cell>OHSU-iconGLCM2-tr-de</cell><cell>26.4</cell></row><row><cell></cell><cell>21 NCTU</cell><cell>dblab-nctu-dblab2</cell><cell>26.7</cell></row><row><cell></cell><cell>22 MU</cell><cell>I2R-refine-SVM</cell><cell>28.0</cell></row><row><cell></cell><cell>23 OHSU</cell><cell>OHSU-iconHistGLCM2-t</cell><cell>28.1</cell></row><row><cell cols="2">*  24 ULG</cell><cell>SYSMOD-RANDOM-SUBWINDOWS-EX</cell><cell>29.0</cell></row><row><cell></cell><cell>25 DEU</cell><cell>DEU-3NN-EDGE</cell><cell>29.5</cell></row><row><cell></cell><cell>-medGIFT</cell><cell>combination</cell><cell>29.7</cell></row><row><cell></cell><cell>26 OHSU</cell><cell>OHSU-iconHist-tr-dev</cell><cell>30.8</cell></row><row><cell></cell><cell>-medGIFT</cell><cell>fw-bwpruned</cell><cell>31.7</cell></row><row><cell></cell><cell>27 UTD</cell><cell>UTD</cell><cell>31.7</cell></row><row><cell></cell><cell>-medGIFT</cell><cell>baseline</cell><cell>32.0</cell></row><row><cell></cell><cell>28 ULG</cell><cell>SYSMOD-RANDOM-SUBWINDOWS-24</cell><cell>34.1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,105.24,657.99,130.53,7.35"><p>http://ir.shef.ac.uk/imageclef/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,105.24,667.47,122.02,7.35"><p>http://www.clef-campaign.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,105.24,677.07,105.10,7.35"><p>http://ir.ohsu.edu/images</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,105.24,686.55,329.50,7.35"><p>http://www-i6.informatik.rwth-aachen.de/~deselaers/imageclef06/medicalaat.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="4,105.24,647.67,100.91,7.35"><p>http://www.casimage.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="4,105.24,657.15,105.10,7.35"><p>http://peir.path.uab.edu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="4,105.24,666.63,113.50,7.35"><p>http://www.healcentral.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="4,105.24,676.11,134.73,7.35"><p>http://gamma.wustl.edu/home.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="4,105.24,685.71,185.46,7.35"><p>http://alf3.urz.unibas.ch/pathopic/intro.htm</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9" coords="4,105.24,695.19,75.60,7.35"><p>http://www.hon.ch/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10" coords="15,105.24,666.51,96.59,7.35"><p>http://www.ltutech.com/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank the CLEF campaign for supporting the ImageCLEF initiative. Furthermore, the authors would like to thank <rs type="person">LTUtech 11</rs> for providing the database for the non-medical automatic annotation task and to <rs type="person">Tobias Weyand</rs> for creating the web interface for submissions.</p><p>This work was partially funded by the <rs type="funder">DFG (Deutsche Forschungsgemeinschaft)</rs> under contracts <rs type="grantNumber">NE-572/6</rs> and <rs type="grantNumber">Le-1108/4</rs>, the <rs type="funder">Swiss National Science Foundation (FNS)</rs> under contract <rs type="grantNumber">205321-109304/1</rs>, the <rs type="funder">American National Science Foundation (NSF)</rs> with grant <rs type="grantNumber">ITR-0325160</rs>, and the <rs type="programName">EU Sixth Framework Program</rs> with the <rs type="projectName">SemanticMining</rs> project (<rs type="grantNumber">IST NoE 507505</rs>) and the <rs type="funder">MUSCLE NoE</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_pJgrgXW">
					<idno type="grant-number">NE-572/6</idno>
				</org>
				<org type="funding" xml:id="_tg6UGV6">
					<idno type="grant-number">Le-1108/4</idno>
				</org>
				<org type="funding" xml:id="_M5BD9nz">
					<idno type="grant-number">205321-109304/1</idno>
				</org>
				<org type="funded-project" xml:id="_mDp52ZM">
					<idno type="grant-number">ITR-0325160</idno>
					<orgName type="project" subtype="full">SemanticMining</orgName>
					<orgName type="program" subtype="full">EU Sixth Framework Program</orgName>
				</org>
				<org type="funding" xml:id="_5y2y4bF">
					<idno type="grant-number">IST NoE 507505</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="15,110.52,467.65,402.52,9.96;15,110.52,479.65,243.38,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="15,334.27,467.65,178.77,9.96;15,110.52,479.65,55.63,9.96">Introducing HEAL: The health education assets library</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">S</forename><surname>Candler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Uijtdehaage</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Dennis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,174.36,479.65,82.04,9.96">Academic Medicine</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="249" to="253" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,110.52,499.57,402.63,9.96;15,110.52,511.57,402.51,9.96;15,110.52,523.45,44.54,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="15,390.24,499.57,122.91,9.96;15,110.52,511.57,213.23,9.96">Overview of the ImageCLEF 2006 photo retrieval and object annotation tasks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,346.08,511.57,89.37,9.96">CLEF working notes</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09">Sep. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,110.52,543.37,402.95,9.96;15,110.52,555.37,402.52,9.96;15,110.52,567.25,402.62,9.96;15,110.52,579.25,402.49,9.96;15,110.52,591.25,252.90,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="15,345.08,543.37,168.39,9.96;15,110.52,555.37,180.10,9.96">Overview of the CLEF cross-language image retrieval track (ImageCLEF) 2004</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,372.36,567.25,140.78,9.96;15,110.52,579.25,322.61,9.96">Multilingual Information Access for Text, Speech and Images: Result of the fifth CLEF evaluation campaign</title>
		<title level="s" coord="15,441.12,579.25,71.89,9.96;15,110.52,591.25,77.08,9.96">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julio</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><surname>Magnini</surname></persName>
		</editor>
		<meeting><address><addrLine>Bath, England</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,110.52,611.17,402.44,9.96;15,110.52,623.05,402.59,9.96;15,110.52,635.05,402.44,9.96;15,110.52,647.05,306.49,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="15,488.78,611.17,24.18,9.96;15,110.52,623.05,402.59,9.96;15,110.52,635.05,26.47,9.96">FIRE in ImageCLEF 2005: Combining content-based image retrieval with textual information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,157.44,635.05,286.87,9.96">Workshop of the Cross-Language Evaluation Forum (CLEF 2005)</title>
		<title level="s" coord="15,452.28,635.05,60.68,9.96;15,110.52,647.05,88.72,9.96">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09">September 2005</date>
		</imprint>
	</monogr>
	<note>page in press</note>
</biblStruct>

<biblStruct coords="16,110.52,61.33,402.57,9.96;16,110.52,73.33,402.97,9.96;16,110.52,85.21,22.93,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="16,432.25,61.33,80.84,9.96;16,110.52,73.33,297.57,9.96">Webbasierte Lernwerkzeuge für die Pathologie -web-based learning tools for pathology</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Glatz-Krieger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Glatz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gysel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Dittler</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mihatsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,415.56,73.33,39.75,9.96">Pathologe</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="394" to="399" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,110.52,105.25,402.42,9.96;16,110.52,117.13,277.24,9.96" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="16,354.72,105.25,158.23,9.96;16,110.52,117.13,170.22,9.96">The Elements of Statistical Learning: Data Mining, Inference, and Prediction</title>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,110.52,137.05,402.59,9.96;16,110.52,149.05,402.29,9.96;16,110.52,160.93,338.55,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="16,468.87,137.05,44.25,9.96;16,110.52,149.05,238.80,9.96">A qualitative task analysis of biomedical image use and retrieval</title>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffery</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Gorman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Ruch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,369.60,149.05,143.21,9.96;16,110.52,160.93,125.10,9.96">ImageCLEF/MUSCLE workshop on image retrieval evaluation</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09">September 2005</date>
			<biblScope unit="page" from="11" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,110.52,180.85,402.53,9.96;16,110.52,192.85,402.57,9.96;16,110.52,204.85,282.50,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="16,110.52,192.85,303.23,9.96">Imageclefmed: A text collection to advance biomedical image retrieval</title>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffery</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianji</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Gorman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Ruch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,421.32,192.85,91.77,9.96;16,110.52,204.85,159.55,9.96">Journal of the American Medical Informatics Association</title>
		<imprint>
			<date type="published" when="2006-10">September/October, 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,110.52,224.77,402.42,9.96;16,110.52,236.65,402.57,9.96;16,110.52,248.65,172.59,9.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="16,348.16,224.77,164.78,9.96;16,110.52,236.65,122.28,9.96">Classification of medical images using non-linear distortion models</title>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Gollan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,257.40,236.65,222.73,9.96">Proc. BVM 2004, Bildverarbeitung für die Medizin</title>
		<meeting>BVM 2004, Bildverarbeitung für die Medizin<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-03">March 2004</date>
			<biblScope unit="page" from="366" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,110.52,268.57,402.25,9.96;16,110.52,280.57,175.83,9.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="16,160.58,268.57,105.71,9.96">On combining classifiers</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,275.88,268.57,236.90,9.96;16,110.52,280.57,48.08,9.96">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="226" to="239" />
			<date type="published" when="1998-03">March 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,110.52,300.49,402.40,9.96;16,110.52,312.37,402.74,9.96;16,110.52,324.37,90.97,9.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="16,110.52,312.37,243.21,9.96">The irma code for unique classification of medical images</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kohnen</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Berthold B Wein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,373.56,312.37,73.23,9.96">Proceedings SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">5033</biblScope>
			<biblScope unit="page" from="440" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,110.52,344.29,402.51,9.96;16,110.52,356.29,402.44,9.96;16,110.52,368.17,402.51,9.96;16,110.52,380.17,57.50,9.96" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="16,205.57,356.29,250.55,9.96">Health care professionals&apos; image use and search behaviour</title>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christelle</forename><surname>Despont-Gros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffery</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Lovis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antoine</forename><surname>Geissbuhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,477.12,356.29,35.84,9.96;16,110.52,368.17,268.03,9.96">Proceedings of the Medical Informatics Europe Conference (MIE 2006)</title>
		<meeting>the Medical Informatics Europe Conference (MIE 2006)<address><addrLine>Maastricht, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-08">August 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,110.52,400.09,402.50,9.96;16,110.52,412.09,402.51,9.96;16,110.52,423.97,351.40,9.96" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="16,495.87,400.09,17.15,9.96;16,110.52,412.09,225.10,9.96">The Use of medGIFT and easyIR for ImageCLEF 2005</title>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antoine</forename><surname>Geissbuhler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johan</forename><surname>Marty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Lovis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Ruch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,360.24,412.09,152.79,9.96;16,110.52,423.97,100.43,9.96">Proceedings of the Cross Language Evaluation Forum 2005</title>
		<meeting>the Cross Language Evaluation Forum 2005<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>LNCS</publisher>
			<date type="published" when="2006-09">September 2006</date>
		</imprint>
	</monogr>
	<note>page in press</note>
</biblStruct>

<biblStruct coords="16,110.52,443.89,402.51,9.96;16,110.52,455.89,402.43,9.96;16,110.52,467.89,215.30,9.96" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="16,110.52,455.89,331.08,9.96">A reference data set for the evaluation of medical image retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antoine</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean-Paul</forename><surname>Vallée</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Francois</forename><surname>Terrier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antoine</forename><surname>Geissbuhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,453.84,455.89,59.10,9.96;16,110.52,467.89,130.58,9.96">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="295" to="305" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,110.52,487.81,402.53,9.96;16,110.52,499.69,402.43,9.96;16,110.52,511.69,152.30,9.96" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="16,175.93,499.69,283.28,9.96">Casimage project -a digital teaching files authoring environment</title>
		<author>
			<persName coords=""><forename type="first">Antoine</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martina</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Natalia</forename><surname>Dfouni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean-Paul</forename><surname>Vallée</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Osman</forename><surname>Ratib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,468.24,499.69,44.70,9.96;16,110.52,511.69,74.75,9.96">Journal of Thoracic Imaging</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,110.52,531.61,402.56,9.96;16,110.52,543.49,330.50,9.96" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="16,396.85,531.61,116.24,9.96;16,110.52,543.49,93.39,9.96">An internet-based nuclear medicine teaching file</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">R</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">H</forename><surname>Vreeland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,211.44,543.49,122.12,9.96">Journal of Nuclear Medicine</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1520" to="1527" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
