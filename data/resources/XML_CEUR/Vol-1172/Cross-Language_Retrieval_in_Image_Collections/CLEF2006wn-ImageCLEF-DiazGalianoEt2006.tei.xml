<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,185.60,146.21,231.78,18.08">SINAI at ImageCLEF 2006</title>
				<funder ref="#_WTYk9J2">
					<orgName type="full">Spanish Government</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,153.22,181.27,80.31,10.46"><forename type="first">M</forename><forename type="middle">C</forename><surname>Díaz-Galiano</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Departamento de Informática Grupo Sistemas Inteligentes de Acceso a la Información Campus Las Lagunillas</orgName>
								<orgName type="institution">University of Jaén</orgName>
								<address>
									<addrLine>Ed. A3</addrLine>
									<postCode>E-23071</postCode>
									<settlement>Jaén</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,241.43,181.27,103.03,10.46"><forename type="first">M</forename><forename type="middle">A</forename><surname>García-Cumbreras</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Departamento de Informática Grupo Sistemas Inteligentes de Acceso a la Información Campus Las Lagunillas</orgName>
								<orgName type="institution">University of Jaén</orgName>
								<address>
									<addrLine>Ed. A3</addrLine>
									<postCode>E-23071</postCode>
									<settlement>Jaén</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,352.61,181.27,92.68,10.46"><forename type="first">M</forename><forename type="middle">T</forename><surname>Martín-Valdivia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Departamento de Informática Grupo Sistemas Inteligentes de Acceso a la Información Campus Las Lagunillas</orgName>
								<orgName type="institution">University of Jaén</orgName>
								<address>
									<addrLine>Ed. A3</addrLine>
									<postCode>E-23071</postCode>
									<settlement>Jaén</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,222.54,195.22,71.68,10.46"><forename type="first">A</forename><surname>Montejo-Raez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Departamento de Informática Grupo Sistemas Inteligentes de Acceso a la Información Campus Las Lagunillas</orgName>
								<orgName type="institution">University of Jaén</orgName>
								<address>
									<addrLine>Ed. A3</addrLine>
									<postCode>E-23071</postCode>
									<settlement>Jaén</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,302.38,195.22,78.07,10.46"><forename type="first">L</forename><forename type="middle">A</forename><surname>Ureña-López</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Departamento de Informática Grupo Sistemas Inteligentes de Acceso a la Información Campus Las Lagunillas</orgName>
								<orgName type="institution">University of Jaén</orgName>
								<address>
									<addrLine>Ed. A3</addrLine>
									<postCode>E-23071</postCode>
									<settlement>Jaén</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,185.60,146.21,231.78,18.08">SINAI at ImageCLEF 2006</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DB1B9C2E76AEE8D677BE21ACC153F4D3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software Visual and text retrieval, Information Gain, Indexing, Machine Translators</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes SINAI team participation in the ImageCLEF campaign. The SINAI research group participated in both the ad hoc task and the medical task. The experiments accomplished in both tasks result from very different approaches.</p><p>For the adhoc task the main IR system used is the same as that of the 2005 Im-ageCLEF adhoc task. The improvement of the adhoc system is a new Machine Translation system that works with several translators and implements several heuristics. We have participated in the English monolingual task and in six bilingual tasks for the languages: Dutch, French, German, Italian, Portuguese and Spanish. The results obtained shown that the English monolingual results are good (0,2234 is our best result) and there is a loss of precision with the bilingual runs and some languages like German or Spanish works better than others, because of the translations.</p><p>For the medical task, this year we carried out new and very different experiments to imageCLEFmed2005 ones. First of all, we have processed the set of collections using Information Gain (IG) to determine which are the best tags that should be considered in the indexing process. These tags are those supposed to provide the most relevant and non-redundant information, and have been selected automatically according to our information-based strategy along with the data and relevance assessments from last year. This year, our goal was to analyze how tag selection may contribute to the quality of final results. In order to select reduced set of tags we have computed IG. 11 different collections were generated according to the percentage of tags with highest IG value. Finally, only results related to experiments with selections over the 20%, 30% and 40% of available tags were submitted, since they reported best performance on 2005 data.</p><p>Experiments using only textual query and using textual mixing with visual query have been submitted. For visual query we have used the GIFT lists provide by the organization. Surprisingly, the system performs better on the text retrieval alone than mixed textual and visual retrieval.</p><p>On the other hand, we try show that information filtering through tag selection using information gain improves retrieval results without the need of a manual selection, but the obtained results are no conclusive. Unfortunately, the results obtained are not as successful as desired. Due to a computing processing mistake all our mixed runs obtain the same results than the visual GIFT baseline (0.0467). At the moment of writing of this paper we are modifying our system in order to solve this problem.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This is the second participation of the SINAI research group at the ImageCLEF campaign. We have participated in the ad hoc task and the medical task.</p><p>As a cross-language retrieval task, multilingual image retrieval based on query translation can achieve high performance, more than monolingual retrieval. The ad hoc task involves to retrieve relevant images using the text associated to each image query.</p><p>The goal of the medical task is to retrieve relevant images based on an image query <ref type="bibr" coords="2,479.57,243.47,9.96,10.46" target="#b0">[1]</ref>. For this, organizers supply a multilingual and visual collection and a set of queries (images and a short text in English, French and German are associated). We first preprocess the collection using Information Gain (IG). This year, our main goal is to compare the effect of select different tags from the collection using this measure. We have attempted to choose those tags, providing the best information in order to improve the result obtained. We have generated several collections with different number of tags depending on their IG. Finally, we have only submitted runs on 3 different collections (at 20%,30% and 40%) because they reported the best results for the Image-CLEFmed2005 data. For each collection, we first compare the results obtained using only textual query against results obtained combining textual and visual information. Finally, we have used different methods to merge visual and textual results.</p><p>Next section describes the ad hoc experiments. In Section 3, we explain the experiments for the medical task. Finally, conclusions and future work are presented in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Ad Hoc Task</head><p>The goal of the ad hoc task is, given a multilingual query, to find as many relevant images as possible, from an image collection.</p><p>The proposal of the ad hoc task is to compare results with and without pseudo-relevant feedback, with or without query expansion, using different methods of query translation or using different retrieval models and weighting functions <ref type="bibr" coords="2,309.10,489.52,9.96,10.46" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Experiments Description</head><p>In our experiments we have used seven languages: Dutch, English, French, German, Italian, Portuguese and Spanish.</p><p>Because in 2005 the results were quite good, this year we have used the same IR system and the same strategies, but introducing a new translation module. This module combines some Machine Translators and implements some heuristics.</p><p>The Machine Translators used have been (in brackets the translator by default for each language):</p><p>• Epals (German and Portuguese)</p><p>• Prompt (Spanish)</p><p>• Reverso (French)</p><p>• Systran (Dutch and Italian) Some heuristics are, for instance, the use of the translation made by the translator by default, a combination with the translations of every translator, or a combination of the words with a higher punctuation (two points if it appears in the default translation and one point if it appears in all of the other translations). The dataset is a new collection: IAPR. The IAPR TC-12 image collection consists of 20,000 images taken from locations around the world and comprising a varying cross-section of still natural images. It includes pictures of a range of sports, actions, photographs of people, animals, cities, landscapes and many other aspects of contemporary life.</p><p>The collections have been preprocessed, using stopwords and the Porter's stemmer.</p><p>The collection dataset has been indexed using LEMUR IR system. It is a toolkit that supports indexing of large-scale text databases, the construction of simple language models for documents, queries, or subcollections, and the implementation of retrieval systems based on language models as well as a variety of other retrieval models. The toolkit is being developed as part of the Lemur Project, a collaboration between the Computer Science Department at the University of Massachusetts and the School of Computer Science at Carnegie Mellon University.</p><p>One parameter for each experiment is the weighting function, such as Okapi or TFIDF. Another is the use or not of PRF (pseudo-relevance feedback ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Results and Discussion</head><p>As parameters all the results are obtained using the title and narrative text, when possible. In the English monolingual task and in the German-English bilingual task we have combined the use or not of pseudo-relevance feedback and the weighting function (Okapi or Tfidf).</p><p>In table <ref type="table" coords="3,143.76,529.59,3.87,10.46">1</ref>, we can see the English monolingual results. The results obtained show that the pseudo-relevance feedback is too important when Okapi is used as weighing function. The results with Tfidf and with Okapi without PRF are very poor.</p><p>Table <ref type="table" coords="3,131.31,565.46,4.98,10.46" target="#tab_0">2</ref> show a summary of experiments submitted and results obtained for the German-English bilingual runs. In this case we have combine the same parameters than in the monolingual task.</p><p>The results obtained show that there is a loss of MAP between the best monolingual experiment and this bilingual, around a 28%. Even though, the other results in the English monolingual task are quite worse compared to the German bilingual ones.</p><p>Finally, table <ref type="table" coords="3,165.87,625.24,4.98,10.46" target="#tab_1">3</ref> show a summary of experiments submitted and results obtained for the other five bilingual runs.</p><p>The results obtained show that in general there is a loss of precision compared to the English monolingual results. The Spanish result is around a 17% worse. The other languages decrease the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Medical Task</head><p>The main goal of medical ImageCLEF task is to improve the retrieval of medical images from heterogeneous and multilingual document collections containing images as well as text. For the medical task, we have used the list of retrieved images by GIFT<ref type="foot" coords="4,406.94,234.08,3.97,7.32" target="#foot_0">1</ref> which was supplied by the organizers of this track. Last year, our efforts concentrated in manipulating the text descriptions associated with these images and mixing the partial results lists with the GIFT lists <ref type="bibr" coords="4,364.16,271.02,9.96,10.46" target="#b2">[3]</ref>. However, this year our experiments focus in preprocessing the collection using Information Gain (IG) in order to improve the quality of results and to automate the tag selection process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preprocessing the Collection</head><p>In order to generate the textual collection we have used the ImageCLEFmed.xml file that links collections with their images and annotations. It has external links to the images and the associated annotations in XML files. It contains relative paths, from the root directory, to all the related files.</p><p>The entire collection consists of 4 datasets (CASImage, Pathopic, Peir and MIR) containing about 50,000 images. Each subcollection is organized into cases that represent a group of related images and annotations. At every case a group of images and an optional annotation is given. Each image is part of a case and has optional associated annotations, which encloses metadata and/or a textual annotation. All of the images and annotations are stored in separate files. Image-CLEFmed.xml only contains the connections between collections, cases, images, and annotations.</p><p>The collection annotations are in XML format. The majority of the annotations are in English but a significant number is also in French (in the CASImage collection) and German (in the Pathopic collection), with ew cases not contain any annotation at all. The quality of the texts varies across collections and even within the same collection.</p><p>For the MIR subset, specifically designed regular expressions have been applied in order to get different segments of information, due to the lack of predefined XML tags. In this way, information such as identificator string, authors, date and so on has been extracted from within the corpus.</p><p>We generate a textual document per image, where the identifier number of document is the name of the image and the text of document is the XML annotation associated to this image. If there were several images of the same case, then the text was copied several times.</p><p>We have used English language for the document collection as well as for the queries. Thus, French annotations in CASImage collection were translated into English and then were incorporated to the collection. Pathopic collection has annotations in both English and German languages. We only used English annotations in order to generate the Pathopic documents, discarding German annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Information Gain and Tag Selection</head><p>Last year, almost all tags were used to generate the final corpus. Only those labels that seemed not to provide any information were removed, like the LANGUAGE tag. But this year these tags have been selected according to the amount of information theoretically supplied. For this, we have used the information gain measure as a method to select the best tags in the collection.</p><p>The main goal was to determine whether the results obtained from a corpus where tags have been reduced by discarding those with low IG may show higher performance levels. The aim is to eliminate those tags that do not provide further information or that introduce noise, therefore degradating results.</p><p>At the beginning, experiments with only 10%, 20%, 30%, ..., 100% of those labels with highest associated IG were performed, using 2005 data for evaluation. Once results were analyzed, most accurated results were obtained with 20%, 30% and 40% of the total of available tags, being these ones the collections used in the submitted experiments for the 2006 campaign.</p><p>The method applied consists in computing the information gain for every tag at every subcollection. Since each subcollection (CASImage, Pathopic, Peir and MIR) has a different set of tags, the information gain was calculated using each subcolletion as scope, isolating each one from the others. Let C be the set of cases, E the value set for the E tag, then the formula applied is as follows:</p><formula xml:id="formula_0" coords="5,239.12,277.91,273.88,10.46">IG(C|E) = H(C) -H(C|E) (1)</formula><p>where</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IG(C|E) is the information gain for the E tag, H(C) is the entropy and H(C|E) is the relative entropy</head><p>In order to calculate this value, we compute the entropy of the set of cases C as:</p><formula xml:id="formula_1" coords="5,164.89,379.33,348.11,32.39">H(C) = - |C| i=1 p(c i ) log 2 p(c i ) = - |C| i=1 1 |C| log 2 1 |C| = -log 2 1 |C|<label>(2)</label></formula><p>And the entropy of the set of cases C conditioned by the tag E would be:</p><formula xml:id="formula_2" coords="5,151.67,438.47,361.33,34.21">H(C|E) = |E| j=1 |C e j | |C| - |Ce j | i=1 1 |C e j | log 2 1 |C e j | = - |E| j=1 |C e j | |C| log 2 1 |C e j |<label>(3)</label></formula><p>where C ej is the subset of cases in C having the tag E set to the value e j (this value is a combination of words where order does not matter) Therefore, we can conclude the final equation for the computation of the information gain supplied by a given tag E over the set of cases C as follows:</p><formula xml:id="formula_3" coords="5,207.37,564.04,305.63,32.39">IG(C|E) = -log 2 1 |C| + |E| j=1 |C ej | |C| log 2 1 |C e j |<label>(4)</label></formula><p>For every tag in every collection, its information gain is computed. Then, the tags selected to compose the final collection are those showing high values of IG. Once the document collection was generated, experiments were conducted with the LEMUR<ref type="foot" coords="5,339.89,625.65,3.97,7.32" target="#foot_1">2</ref> retrieval information system, applying the Kl-divergence weighting scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experiment Description</head><p>Our main goal is to investigate the effectiveness of filtering tags using IG in the text collection. For this, we have accomplished several experiments using the ImageCLEFmed2005 in order to determinate the best tag percentage. First, we have carried out experiments with 10%, 20%...100% of tags and we have evaluated the results with the relevance assessments of the 2005 collection. Based on the result obtained, we have only submitted runs with 20%, 30% y 40% of tags for the 2006 collection because these corpus reported the best results. Thus for each experiment, we have submitted 3 runs (one per corpus generated at: 20%, 30% and 40% of all available tags).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment</head><p>We wanted also to compare the obtained results when we only use the text associated to the query topic and the results when we merge visual and textual information. For this, first experiment has been performed as baseline case. This experiment simply consists of taking the text associated to each query as a new textual query. Then, each textual query is submitted to the LEMUR system. The resulting list is directly the baseline run.</p><p>The remain experiments start from the ranked lists provided by the GIFT tool. The organization provides list of relevant images generated by GIFT for each query. For each list/query we have used an automatic textual query expansion using the associated text to the top ranked images from GIFT lists. Thus, we have added the text associated to the first four images from the GIFT list to the original textual query in order to generate a new textual query. Then, the new textual query is submitted to the LEMUR system and we obtain a new ranked list. Thus, for each original query we have 2 partial lists: one (expanded) text list and one GIFT list. The last step consists of merging these partial resulting lists using some strategy in order to obtain one final list (FL) with relevant images ranked by relevance. The merging process was done given different weight of importance to the visual (VL) and textual lists (TL):</p><formula xml:id="formula_4" coords="6,215.50,463.41,297.49,10.46">F L = V L * α + T L * β, with α + β = 1<label>(5)</label></formula><p>In order to set these parameters we have again launched some experiments with the 2005 collection varying α and β in the range [0,1] with step 0.1 (i.e., 0, 0.1, 0.2,...,0.9 and 1). After analyzing the results, we have submitted runs with β set to 0.5, 0.6 and 0.7 for the 2006 collection.</p><p>These 3 experiments and the baseline experiment (that only uses textual information of the query) have been accomplished over the 3 different corpus generated with 20%, 30% and 40% of tags. All textual experiments have been carried out with LEMUR using Pseudo Relevance Feedback and the Kl-divergence weighting scheme, as pointed out previously. In summary, we have submitted 12 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results</head><p>The total runs submitted at ImageCLEFmed2006 for text only were 31 and for mixed retrieval were 37.</p><p>Table <ref type="table" coords="6,132.06,635.22,4.98,10.46" target="#tab_2">4</ref> shows the results for text only retrieval with the SINAI system. Unfortunately, due to a computing processing mistake all our mixed runs obtain the same results than the visual GIFT baseline (0.0467). At the moment of writing of this paper we are modifying our system in order to solve this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Further Work</head><p>In this paper, we have presented the experiments carried out in our participation in the ImageCLEF campaign.</p><p>For the adhoc task, we have tried a new Machine Translation module. The application of some heuristics improves the bilingual results, but it is necessary to study the queries with poorest results, in order to improve them. Our next work will be the improvement of the results in the IR phase, applying new techniques for query expansion (using thesauri or web information) and the investigation in other heuristics for the Machine Translation module.</p><p>For the medical task, we have tried to apply Information Gain in order to improve the results. Unfortunately, the performance obtained has been very poor. In addition, for mixed runs our system has a computing mistake and result obtained are no conclusive. However, we consider that the Information Gain is a good idea and a widely used method to filter information without the need of a manual tag selection. Thus, our next step will focus on improving the visual lists and the merging process.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,126.60,109.34,349.81,173.28"><head>Table 2 :</head><label>2</label><figDesc>Summary of results for the German-English bilingual adhoc runs</figDesc><table coords="3,126.60,109.34,349.81,151.42"><row><cell>Experiment</cell><cell cols="4">Initial Query Expansion Weight MAP</cell><cell>Rank</cell></row><row><cell cols="2">sinaiEnEnFbOkapiExp1 title + narr</cell><cell>with</cell><cell cols="3">Okapi 0.2234 9/49</cell></row><row><cell cols="2">sinaiEnEnFbOkapiExp2 title + narr</cell><cell>without</cell><cell cols="3">Okapi 0.0845 38/49</cell></row><row><cell cols="2">sinaiEnEnFbOkapiExp3 title + narr</cell><cell>with</cell><cell>Tfidf</cell><cell cols="2">0.0846 37/49</cell></row><row><cell cols="2">sinaiEnEnFbOkapiExp4 title + narr</cell><cell>without</cell><cell>Tfidf</cell><cell cols="2">0.0823 39/49</cell></row><row><cell cols="5">Table 1: Summary of results for the English monolingual adhoc runs</cell></row><row><cell>Experiment</cell><cell cols="5">Initial Query Expansion Weight MAP Rank</cell></row><row><cell cols="2">sinaiDeEnFbOkapiExp1 title + narr</cell><cell>with</cell><cell cols="2">Okapi 0.1602</cell><cell>4/8</cell></row><row><cell cols="2">sinaiDeEnFbOkapiExp2 title + narr</cell><cell>without</cell><cell cols="2">Okapi 0.1359</cell><cell>7/8</cell></row><row><cell cols="2">sinaiDeEnFbOkapiExp3 title + narr</cell><cell>with</cell><cell>Tfidf</cell><cell>0.1489</cell><cell>5/8</cell></row><row><cell cols="2">sinaiDeEnFbOkapiExp4 title + narr</cell><cell>without</cell><cell>Tfidf</cell><cell>0.1369</cell><cell>6/8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,480.26,739.78,32.74,10.46"><head>Table 3 :</head><label>3</label><figDesc>Summary of results for the others five bilingual adhoc runs are formulated with sample images and a sort of textual description explaining the research goal.</figDesc><table coords="3,480.26,739.78,32.74,10.46"><row><cell>Queries</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,136.21,109.34,330.59,81.09"><head>Table 4 :</head><label>4</label><figDesc>Performance of official runs in Medical Image Retrieval (text only)</figDesc><table coords="6,342.83,109.34,74.57,10.46"><row><cell>Precision Rank</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,105.24,729.54,130.43,8.37"><p>http://www.gnu.org/software/gift/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,105.24,728.12,112.73,8.37"><p>http://www.lemurproject.org/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5">Acknowledgements</head><p>This work has been partially supported by a grant from the <rs type="funder">Spanish Government</rs>, project <rs type="projectName">R2D2</rs> (<rs type="grantNumber">TIC2003-07158-C04-04</rs>)</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_WTYk9J2">
					<idno type="grant-number">TIC2003-07158-C04-04</idno>
					<orgName type="project" subtype="full">R2D2</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,105.50,351.58,407.52,10.46;7,105.50,363.54,407.51,10.46;7,105.50,375.49,269.16,10.46" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,105.50,363.54,336.35,10.46">Overview of the ImageCLEF 2006 photo retrieval and object annotation tasks</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,461.03,363.54,51.97,10.46;7,105.50,375.49,238.72,10.46">Proceedings of the Cross Language Evaluation Forum (CLEF 2006)</title>
		<meeting>the Cross Language Evaluation Forum (CLEF 2006)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,395.41,407.51,10.46;7,105.50,407.37,407.50,10.46;7,105.50,419.32,213.39,10.46" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,472.29,395.41,40.72,10.46;7,105.50,407.37,286.77,10.46">Overview of the ImageCLEFmed 2006 medical retrieval and annotation tasks</title>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,407.55,407.37,105.46,10.46;7,105.50,419.32,182.95,10.46">Proceedings of the Cross Language Evaluation Forum (CLEF 2006)</title>
		<meeting>the Cross Language Evaluation Forum (CLEF 2006)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,439.25,407.51,10.46;7,105.50,451.20,407.50,10.46;7,105.50,463.16,168.43,10.46" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,217.13,451.20,119.38,10.46">SINAI at ImageCLEF 2005</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Martín-Valdivia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>García-Cumbreras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Díaz-Galiano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">C</forename><surname>Ureña-López</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Montejo-Raez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,357.98,451.20,155.02,10.46;7,105.50,463.16,137.98,10.46">Proceedings of the Cross Language Evaluation Forum (CLEF 2005)</title>
		<meeting>the Cross Language Evaluation Forum (CLEF 2005)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
