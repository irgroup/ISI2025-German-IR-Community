<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,155.40,69.74,301.18,11.84;1,228.66,84.86,154.68,11.84">Medical Image Retrieval and Automated Annotation: OHSU at ImageCLEF 2006</title>
				<funder ref="#_2E2Fjcn">
					<orgName type="full">US National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,278.16,111.82,55.58,8.48"><forename type="first">William</forename><surname>Hersh</surname></persName>
							<email>hersh@ohsu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Medical Informatics &amp; Clinical Epidemiology Oregon Health &amp;</orgName>
								<orgName type="institution">Science University Portland</orgName>
								<address>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,253.62,122.68,104.77,8.48"><forename type="first">Jayashree</forename><surname>Kalpathy-Cramer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Medical Informatics &amp; Clinical Epidemiology Oregon Health &amp;</orgName>
								<orgName type="institution">Science University Portland</orgName>
								<address>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,279.18,133.48,53.62,8.48"><forename type="first">Jeffery</forename><surname>Jensen</surname></persName>
							<email>jensejef@ohsu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Medical Informatics &amp; Clinical Epidemiology Oregon Health &amp;</orgName>
								<orgName type="institution">Science University Portland</orgName>
								<address>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,155.40,69.74,301.18,11.84;1,228.66,84.86,154.68,11.84">Medical Image Retrieval and Automated Annotation: OHSU at ImageCLEF 2006</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E36099F6D72C3F7E2B47E8280CEC84BE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries Image retrieval, Performance, Image annotation, Experimentation Manual query modification, Data fusion, Classification, Neural networks 1. Image Retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Oregon Health &amp; Science University participated in both the medical retrieval and medical annotation tasks of ImageCLEF 2005. Our efforts in the retrieval task focused on manual modification of query statements and fusion of results from textual and visual retrieval techniques. Our results showed that manual modification of queries does improve retrieval performance, while data fusion of textual and visual techniques improves precision but lowers recall. However, since image retrieval may be a precision-oriented task, these data fusion techniques could be of value for many users. In the annotation task, we assessed a variety of learning techniques and obtained classification accuracy of up to 74% with test data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a. Introduction</head><p>The mission of information retrieval research at Oregon Health &amp; Science University (OHSU) is to better understand the needs and optimal implementation of systems for users in biomedical tasks, including research, education, and clinical care. The goals of the OHSU experiments in the medical image retrieval task of ImageCLEF were to assess manual modification of topics with and without visual retrieval techniques. We manually modified the topics to generate queries, and then used what we thought would be the best run (which in retrospect was not) for combination with visual techniques, similar to the approach we took in ImageCLEF 2005 <ref type="bibr" coords="1,92.22,628.78,10.05,8.48" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b. System Description</head><p>Our retrieval system was based on the open-source search engine, Lucene <ref type="bibr" coords="1,372.31,672.10,9.98,8.48" target="#b1">[2]</ref>, which is part of the Apache Jakarta distribution. We have used Lucene in other retrieval evaluation forums, such as the Text Retrieval Conference (TREC) Genomics Track <ref type="bibr" coords="1,235.33,693.70,10.24,8.48" target="#b2">[3,</ref><ref type="bibr" coords="1,247.86,693.70,6.83,8.48" target="#b3">4]</ref>. Documents in Lucene are indexed by parsing of individual words and weighting of those words with an algorithm that sums for each query term in each document the product of the term frequency (TF), the inverse document frequency (IDF), the boost factor of the term, the normalization of the document, the fraction of query terms in the document, and the normalization of the weight of the query terms, for each term in the query. The score of document d for query q consisting of terms t is calculated as follows: As Lucene is a code library and set of routines for IR functionality, it does not have a standard user interface. We have therefore also created a search interface for Lucene that is tailored to the ImageCLEF medical retrieval test collection structure <ref type="bibr" coords="2,166.99,245.02,11.02,8.48" target="#b4">[5]</ref> and the ability to use the MedGIFT search engine for visual retrieval on single images <ref type="bibr" coords="2,92.22,255.88,10.03,8.48" target="#b5">[6]</ref>. We did not use the user interface for these experiments, though we plan to undertake interactive user experiments in the future.</p><formula xml:id="formula_0" coords="2,374.16,124.65,41.30,9.30">) ( *</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>c. Runs Submitted</head><p>We submitted three general categories of runs:</p><p>• Automatic textual -submitting the topics as phrased in the official topics file directly into Lucene. We submitted each of the three languages in separate runs, along with a run that combined all three languages into a single query string and another run that included the output from the Babelfish translator (http://babelfish.altavista.com/). • Manual textual -manually editing of the official topic files by one of the authors (WRH). The editing mostly consisted of removing function and other common words. Similar to the automatic runs, we constructed query files in each of the three languages, along with a run that combined all three languages into a single query string and a final run that included the output from the Babelfish translator. The manually modified query strings are listed in Table <ref type="table" coords="2,378.21,408.70,3.54,8.48" target="#tab_1">1</ref>. • Interactive mixed -a combination of textual and visual techniques, described in greater detail below.</p><p>The mixed textual and visual run was implemented as a serial process, where the results of what we thought would be our best textual run were passed through a set of visual retrieval steps. This run started by using the top 2000 retrieved images of the OHSU_all textual run. These results were combined with the top 1000 results distributed from the medGIFT (visual) system. Only those images that were in both lists were chosen. These were ordered by the textual ranking, with typically 8 to 300 images in common.</p><p>A neural network-based scheme using a variety of low level, global image features was used to create the visual part of the retrieval system. The retrieval system was created in MATLAB using Netlab <ref type="bibr" coords="2,425.40,517.54,10.24,8.48" target="#b6">[7,</ref><ref type="bibr" coords="2,437.93,517.54,6.82,8.48" target="#b7">8]</ref>. We used a multilayer perceptron architecture to create the the two-class classifiers to determine if a color image was a 'microscopic' image or 'gross pathology.' It was a two layer structure, with a hidden layer of approximately 50-150 nodes. A variety of combinations of the image features were used as inputs. All inputs to the neural network (the image feature vectors) were normalized using the training set to have a mean of zero and variance of 1.</p><p>Our visual system then analyzed the sample images associated with each sub-task. If the query image was deemed to be a color image by the system, the set of top 2000 textual images was processed and those that were deemed to be color were moved to the top of the list. Within that, the ranking was based on the ranking of the textual results. A neural network was created to process color images to determine if they were microscopic or gross pathology/photograph. The top 2000 textual results were processed through this network and the appropriate type of image (based on the query image) received a higher score. Relevance feedback was used to improve the training for the network <ref type="bibr" coords="3,184.23,689.50,7.38,8.48" target="#b8">[9]</ref><ref type="bibr" coords="3,191.61,689.50,3.69,8.48" target="#b9">[10]</ref><ref type="bibr" coords="3,195.29,689.50,11.07,8.48" target="#b10">[11]</ref>. Low level texture features based on grey-level co-occurrence matrices (GLCM) were used as input to the neural network <ref type="bibr" coords="3,246.86,700.30,14.92,8.48" target="#b11">[12,</ref><ref type="bibr" coords="3,264.07,700.30,11.22,8.48" target="#b12">13]</ref>. We also created neural networks for a few classes of radiographic images, based on the system that we had used for the automatic annotation class (described in detail in the next section). Images identified as being of the correct class received a higher score.</p><p>The primary goal of these visual techniques was to move the relevant images higher on the ordered list of retrieved images, thus leading to higher precision. However, we would be limited in the recall to only those images that had already been retrieved by the textual search. Thus, even in the ideal case, where all the relevant images were moved to the top of the list, the MAP would be limited by the number of relevant images that were retrieved by the textual search (recall of the textual search).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>d. Results and Analysis</head><p>The characteristics of the submitted OHSU runs are listed in Table <ref type="table" coords="4,345.27,187.60,3.54,8.48" target="#tab_2">2</ref>, with various results shown in Figure <ref type="figure" coords="4,494.19,187.60,3.54,8.48" target="#fig_0">1</ref>. The automatic textual runs were our lowest scoring runs. The best of these runs was the English-only run passed through the Babelfish translator, which obtained a MAP of 0.1264. The remaining runs all performed poorly, with all MAP results under 0.08. The manual textual runs performed somewhat better. Somewhat surprising to us, the best of these runs was the English-only run (OHSUeng). This was our best run of all, with a MAP of 0.2132. It outperformed an English-only run with terms from automatic translation added (OHSUeng_trans, with a MAP of 0.1906) as well as a run with queries of topic statements from all languages with a MAP of 0.1673).</p><p>The MAP for our interactive-mixed run, OHSU_m1, was 0.1563. As noted above, this run was based on modification of OHSUall, which had a MAP of 0.1673. At a first glance, it appears that performance was worsened with the addition of visual techniques, due to the lower MAP. However, as seen in Figure <ref type="figure" coords="4,470.01,306.58,3.54,8.48" target="#fig_0">1</ref>, and similar to our results from 2005, the average precision at various numbers of images retrieved was higher, especially at the top of the retrieval list. This confirmed our finding from 2005 that visual techniques used to modify textual runs diminish recall-oriented measures like MAP but improve precision at the very top of output list, which may be useful to real users. There was a considerable variation in performance on different topics. For most topics, the addition of visual techniques improved early precision, but for some, the reverse was true.  We also looked at MAP for the tasks separated by their perceived nature of the question (one favoring visual, semantic, or mixed techniques). For the visual and mixed queries, the incorporation of visual techniques improved MAP. However, for semantic queries, there was a serious degradation in MAP by the addition of the visual steps in the retrieval process. This, however, is driven by only one query, number 27, where MAP for OHSU_all was 0.955, while for OHSU_m1 was 0.024. Excluding this query, MAP for OHSU_m1 was 0.161 while that of OHSU_all was 0.140, indicating a slight improvement for the addition of visual techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>e. Conclusions</head><p>Our runs demonstrated that manual modification of topic statements makes a large performance difference, although our results are not as good as some groups that did automatic processing of the text of topics. Our results also showed that visual retrieval techniques provide benefit at the top of the retrieval output, as demonstrated by higher precision at various output levels, but are detrimental to recall, as shown by lower MAP. However, for most image retrieval tasks, precision may be more important than recall, so visual techniques may be of value in real-world image retrieval systems. Additional research on how real users query image retrieval systems could shed light on which system-oriented evaluation measures are most important. Also suggested by our runs is that system performance is dependent upon the topic type. In particular, visual retrieval techniques degrade the performance of topics that are most amenable to textual retrieval techniques. This indicates that systems that can determine the query type may be able to improve performance with that information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Automated Image Annotation</head><p>The goal of this task was to correctly classify 1000 radiographic medical images into 116 categories. The images differed in the "modality, body orientation, body region, and biological system examined," according to the track Web site. The task organizers provided a set of 9,000 training images that were classified into these 116 classes. In addition, another set of classified images (numbering 1000) was provided as a development set. The suggested procedure was to create a classifier based on the training images. The development set could then be used to test the effectiveness of the classifier. One could then combine the training and development tests to create a larger database to create the final classifier for the test images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a. Introduction</head><p>For the automated image annotation task, we used a combination of low-level image features and a neural network based classifier. Our results (error rate of 26.3% for our best run) were in the middle of the range of results obtained for all groups, indicating to us the potential capabilities of these techniques as well as some areas of improvement for further experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b. System Description</head><p>A neural network-based scheme using a variety of low-level, largely global image features was used to create the classifier, which was implemented in MATLAB using Netlab. A variety of feature vectors were then tested with the results. For our first efforts in the medical image automatic annotation domain, we started with low-level, commonly used, global, texture and histogram features. In addition, we tried to capture a sense of spatial differences between images classes.</p><p>Images were first padded to create a 512x512 image, with the original image centered within this new image. White (255) and black (0) pixels were tested for the padding. This was done since we had noted that the aspect ratio of the image can provide information useful for classification. All images were resized to 256x256 pixels using bilinear extrapolation.</p><p>A variety of features described below were tested on the development set. These features were combined in different ways to try to improve the classification ability of the system, with the final submissions were based on the three best combinations of image features. The features included:</p><p>• Icon: A 16x16 pixel 'icon' of the image was created by resizing the image using bilinear extrapolation. This vector of dimension 256 was fed directly into the input of the neural network • GLCM: Four gray level co-occurrence matrices (GLCM) <ref type="bibr" coords="6,342.72,504.82,41.08,8.48">[ Haralick]</ref> matrices with offsets of 1 pixel, 0, 45, 90 and 135 degrees were created for the image after rescaling the image to 16 levels. GLCM statistics of contrast, correlation, energy, homogeneity and entropy were calculated for each matrix. A 20 dimensional vector was created for each image by concatenating the 5 dimensional vector obtained by each of the four matrices. • GLCM2: In order to capture the spatial variation of the images in a coarse manner, the resized image (256x256) was partitioned into 5 squares of size 128x128 pixels (top left, top right, bottom left, bottom right, centre). A gray level correlation matrix was created for each partition. A 20 dimensional vector was created for each partition. Subsequently, the 5 vectors from each of the partitions were concatenated to created feature vector of dimension 100. • Hist: A 32-bin histogram was created for each image and counts were used as the input • DCT: A global discrete cosine transform was created for each image. The upper left (10x10) vectors were concatenated and used as inputs</p><p>We used a multilayer perceptron architecture to create the multi-class classifier <ref type="bibr" coords="6,390.01,658.30,10.20,8.48" target="#b6">[7,</ref><ref type="bibr" coords="6,402.50,658.30,6.82,8.48" target="#b7">8]</ref>. It was a two layer structure, with a hidden layer of approximately 200-400 nodes. A variety of combinations of the above image features were used as inputs. All inputs to the neural network (the image feature vectors) were normalized using the training set to have a mean of zero and variance of 1. The architecture was optimized using the training and development sets provided.</p><p>The network architecture, primarily the number of hidden nodes, needed to be optimized for each set of input feature vectors, since the length of the feature vectors varied from 32 to 356. The training set was used to create the classifier, typically with the accuracy increasing with an increase in the number of hidden nodes. It was relatively easy to achieve 100% classification accuracy on the training set. However, there were issues with overfitting if too many hidden nodes were used (see Figure <ref type="figure" coords="7,316.95,122.68,3.41,8.48" target="#fig_1">2</ref>). We used empirical methods to optimize the network for each set of feature vectors by using a network architecture that resulted in the highest classification accuracy for the development set. For instance, for the feature vectors consisting of iconHist features, we would use 300 hidden nodes, while for iconGLCM, we would use a network consisting of 200 hidden nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>c. Runs Submitted</head><p>We submitted four runs, iconGLCM2 using just the training set for creating the net, iconGLCM2 using the development and training set for creating the net, icongHist, and iconHistGLCM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>d. Results and Analysis</head><p>The best results for the development set were obtained using a 356 dimensional normalized input vector consisting of the icon (16x16) concatenated with the GLCM. The classification rate on the training set was 80%. The next best result was obtained using a 288 dimensional normalized input vector consisting of the icon (16x16) concatenated with Hist. The classification rate on the development set was 78%. Most other runs including just the icon or DCT or GLCM2 gave about 70-75% classification accuracy, as seen in Table <ref type="table" coords="7,428.59,295.72,3.54,8.48" target="#tab_4">4</ref>. However, the results obtained on the test set were lower than those of the development set.  Analyzing the data, it appeared that a few classes were primarily responsible for the differences seen between the development set and test set (see Table <ref type="table" coords="8,241.33,227.32,3.41,8.48" target="#tab_5">5</ref>). Class 108 had the most significant difference seen, which was about 2.4% of the 6% difference seen in iconGLCM2. Most of the misclassification of class 108 was into class 111, visually a very similar class. Observing the confusion matrices in general for all the runs, the most misclassifications were between classes 108/111 and 2/56.</p><p>Following our availability of the results, we performed additional experiments aiming to improve the classification between these sets of visually similar classes. We created two new additional classifiers to distinguish between class 2 and 56, and between class 108 and 111. We merged images labeled by the original classifier as class 2 and 56, and class 108 and 111 and then applied the new classifiers on these newly merged classes. Using this hierarchical classification, we improved our classification accuracy by about 4% (to 79%) overall for the test set. This seems like a promising approach to improve the classification ability of our system.</p><p>One of the issues with the database is that the number of training images in each of the classes is quite varied. Another issue is that there are some classes that are visually quite similar while other classes that have quite a bit of within class variation. These issues were proved to be a little challenging for our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>e. Conclusions</head><p>Using a neural network approach and primarily low level global features, we obtained moderate results in the ImageCLEFmed automatic annotation task. The best results were obtained by using a feature vector consisting of a 16x16 icon and grey-level co-occurrence features. A multi-layer perceptron architecture was used for the neural network. In the future, we plan to explore using a hierarchical set of classifiers to improve the classification between visually similar classes (for instance, different views of the same anatomical organ). This might also work well with the IRMA classification system. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,92.22,359.50,404.90,8.48;5,92.22,370.24,212.77,8.48"><head>Figure 1 -</head><label>1</label><figDesc>Figure 1 -MAP and precision at various retrieval levels for all OHSU runs and the run with the best overall MAP from ImageCLEFmed 2006, IPAL-IPAL_Cpt_Im.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,92.22,647.86,249.23,8.48"><head>Figure 2 -</head><label>2</label><figDesc>Figure 2 -Images classified correctly vs. number of hidden nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,92.22,68.56,427.50,563.96"><head>Table 1 -</head><label>1</label><figDesc>Manually modified queries for OHSU manual textual runs.</figDesc><table coords="3,92.22,90.76,427.50,541.76"><row><cell cols="2">Topic English</cell><cell>German</cell><cell>French</cell></row><row><cell>1</cell><cell>oral cavity including teeth and</cell><cell>Mundhöhle mit Zähnen und</cell><cell>cavité buccale incluant des dents</cell></row><row><cell></cell><cell>gum tissue</cell><cell>Zahnfleisch</cell><cell>et du tissu des gencives</cell></row><row><cell>2</cell><cell>frontal head MRI</cell><cell>MR Frontalaufnahmen des</cell><cell>IRM frontal du crâne</cell></row><row><cell></cell><cell></cell><cell>Kopfes</cell><cell></cell></row><row><cell>3</cell><cell>knee x-ray</cell><cell>Röntgenbilder des Knies</cell><cell>radiographies du genou</cell></row><row><cell>4</cell><cell>x-ray of a tibia with a fracture</cell><cell>Röntgenbilder einer</cell><cell>radiographies du tibia avec</cell></row><row><cell></cell><cell></cell><cell>gebrochenen Tibia</cell><cell>fracture</cell></row><row><cell>5</cell><cell>x-ray of a hip joint with</cell><cell>Röntgenbilder eines Hüftgelenks</cell><cell>radiographies d'articulation de la</cell></row><row><cell></cell><cell>prosthesis</cell><cell>mit Prothese</cell><cell>hanche avec une prothèse</cell></row><row><cell>6</cell><cell>hand x-ray</cell><cell>Röntgenbilder einer Hand</cell><cell>des radiographies de la main</cell></row><row><cell>7</cell><cell>ultrasound with a triangular</cell><cell>Ultraschallbilder mir</cell><cell>des échographies de résultats</cell></row><row><cell></cell><cell>result</cell><cell>dreieckigem Ergebnis</cell><cell>triangulaires</cell></row><row><cell>8</cell><cell>PowerPoint slides</cell><cell>von Powerpoint Folien</cell><cell>des images de diapositives</cell></row><row><cell></cell><cell></cell><cell></cell><cell>PowerPoint</cell></row><row><cell>9</cell><cell>EEG or ECG</cell><cell>EEG oder EKG</cell><cell>EEG ou ECG</cell></row><row><cell>10</cell><cell>chest CT with nodules</cell><cell>CT der Lunge mit Knötchen</cell><cell>CTs du thorax avec nodules</cell></row><row><cell>11</cell><cell>ultrasound with gallstones</cell><cell>Ultraschallbilder mit</cell><cell>échographies de calculs biliaires</cell></row><row><cell></cell><cell></cell><cell>Gallensteinen</cell><cell></cell></row><row><cell>12</cell><cell>chest x-ray with tuberculosis</cell><cell>Röntgenbilder der Lunge mit</cell><cell>radiographies de la poitrine avec</cell></row><row><cell></cell><cell></cell><cell>Tuberkulose</cell><cell>une tuberculose</cell></row><row><cell>13</cell><cell>CT with a brain infarction</cell><cell>CT eines Gehirnschlages</cell><cell>CT avec un infarctus cérébral</cell></row><row><cell>14</cell><cell>MRI of the brain with a blood</cell><cell>MR des Gehirns mit</cell><cell>IRM du cerveau avec un caillot</cell></row><row><cell></cell><cell>clot</cell><cell>Blutgerinnsel</cell><cell>sanguin</cell></row><row><cell>15</cell><cell>x-ray of vertebral osteophytes</cell><cell>Röntgenbilder von vertebralen</cell><cell>radiographies d'ostéophytes</cell></row><row><cell></cell><cell></cell><cell>Osteophyten</cell><cell>vertébraux</cell></row><row><cell>16</cell><cell>ultrasound of a foetus or fetus</cell><cell>Ultraschallbilder eines Fötus</cell><cell>échographies d'un foetus</cell></row><row><cell>17</cell><cell>abdominal CT of an aortic</cell><cell>CT des Abdomens mit einem</cell><cell>CTs abdominaux d'un anévrisme</cell></row><row><cell></cell><cell>aneurysm</cell><cell>Aneurismus der Aorta</cell><cell>aortique</cell></row><row><cell>18</cell><cell>blood smears that include</cell><cell>Blutabstriche mit</cell><cell>échantillons de sang incluant des</cell></row><row><cell></cell><cell>polymorphonuclear neutrophils</cell><cell>polymophonuklearer</cell><cell>neutrophiles</cell></row><row><cell></cell><cell></cell><cell>Neutrophils</cell><cell>polymorphonucléaires</cell></row><row><cell>19</cell><cell>multinucleated giant cells</cell><cell>mehrkernige riesenzellen</cell><cell>cellules géantes multinucléées</cell></row><row><cell>20</cell><cell>lung tissue</cell><cell>Lungengewebe lung</cell><cell>tissu pulmonaire</cell></row><row><cell>21</cell><cell>infected wound</cell><cell>infizierten Wunde wound</cell><cell>plaie infectée wound</cell></row><row><cell>22</cell><cell>tumours or tumors</cell><cell>Tumoren</cell><cell>tumeurs</cell></row><row><cell>23</cell><cell>CT or x-ray of heart</cell><cell>CT oder Röntgenbilder des</cell><cell>CT ou des radiographies qui</cell></row><row><cell></cell><cell></cell><cell>Herzens</cell><cell>montrent le coeur</cell></row><row><cell>24</cell><cell>muscle cells</cell><cell>Muskelzellen</cell><cell>cellules musculaires</cell></row><row><cell>25</cell><cell>tissue from the cerebellum</cell><cell>Kleinhirngewebe kleinhirn</cell><cell>tissu du cervelet</cell></row><row><cell>26</cell><cell>x-ray of bone cysts</cell><cell>Röntgenbilder von</cell><cell>radiographies de kystes d'os</cell></row><row><cell></cell><cell></cell><cell>Knochenzysten</cell><cell></cell></row><row><cell>27</cell><cell>Budd-Chiari malformation</cell><cell>Budd-Chiari Verformung</cell><cell>malformation de Budd-Chiari</cell></row><row><cell>28</cell><cell>parvovirus infection</cell><cell>Parvovirusinfektion parvovirus</cell><cell>infection parvovirale</cell></row><row><cell></cell><cell></cell><cell>infection</cell><cell></cell></row><row><cell>29</cell><cell>bacterial meningitis</cell><cell>bakteriellen Hirnhautentzündung</cell><cell>méningite bactérienne</cell></row><row><cell></cell><cell></cell><cell>meningitis</cell><cell>meningitis bacterial</cell></row><row><cell>30</cell><cell>findings with Alzheimer's</cell><cell>Fällen mit einer Alzheimer</cell><cell>observations avec la maladie</cell></row><row><cell></cell><cell>Disease</cell><cell>Diagnose</cell><cell>d'Alzeimer</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,92.22,403.96,433.36,166.05"><head>Table 2 -</head><label>2</label><figDesc>Characteristics of OHSU runs.</figDesc><table coords="4,92.22,426.16,433.36,143.85"><row><cell>Run_ID</cell><cell>Type</cell><cell>Description</cell></row><row><cell>OHSU_baseline_trans</cell><cell>Auto-Text</cell><cell>Baseline queries in English translated automatically</cell></row><row><cell>OHSU_english</cell><cell>Auto-Text</cell><cell>Baseline queries in English only</cell></row><row><cell>OHSU_baseline_notrans</cell><cell>Auto-Text</cell><cell>Baseline queries in all languages</cell></row><row><cell>OHSU_german</cell><cell>Auto-Text</cell><cell>Baseline queries in German only</cell></row><row><cell>OHSU_french</cell><cell>Auto-Text</cell><cell>Baseline queries in French only</cell></row><row><cell>OHSUeng</cell><cell>Manual-Text</cell><cell>Manually modified queries in English only</cell></row><row><cell>OHSUeng_trans</cell><cell>Manual-Text</cell><cell>Manually modified queries in English translated automatically</cell></row><row><cell>OHSU-OHSUall</cell><cell>Manual-Text</cell><cell>Manually modified queries in all three languages</cell></row><row><cell>OHSUall</cell><cell>Manual-Text</cell><cell>Manually modified queries in all three languages</cell></row><row><cell>OHSUger</cell><cell>Manual-Text</cell><cell>Manually modified queries in German only</cell></row><row><cell>OHSUfre</cell><cell>Manual-Text</cell><cell>Manually modified queries in French only</cell></row><row><cell>OHSU-OHSU_m1</cell><cell>Interactive-Mixed</cell><cell>Manually modified queries filtered with visual methods</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,92.22,619.06,214.88,75.81"><head>Table 3 -</head><label>3</label><figDesc>MAP by query type for mixed and textual runs.</figDesc><table coords="5,96.90,641.32,187.21,53.55"><row><cell>Query Type</cell><cell></cell><cell>MAP</cell></row><row><cell></cell><cell>OHSU_m1</cell><cell>OHSUall</cell></row><row><cell>Visual</cell><cell>0.139</cell><cell>0.128</cell></row><row><cell>Mixed</cell><cell>0.182</cell><cell>0.148</cell></row><row><cell>Semantic</cell><cell>0.149</cell><cell>0.226</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,92.22,68.56,252.00,133.88"><head>Table 4 -</head><label>4</label><figDesc>Classification rates for OHSU automatic annotation runs.</figDesc><table coords="8,96.90,92.44,241.32,110.00"><row><cell>Feature vector</cell><cell>Classification rate</cell><cell></cell></row><row><cell></cell><cell>Development</cell><cell>Test</cell></row><row><cell>DCT</cell><cell>71</cell><cell>-</cell></row><row><cell>icon</cell><cell>74</cell><cell>-</cell></row><row><cell>iconDCT</cell><cell>75</cell><cell>-</cell></row><row><cell>iconHist</cell><cell>78</cell><cell>69</cell></row><row><cell>iconGLCM</cell><cell>78</cell><cell>-</cell></row><row><cell>iconGLCMHist</cell><cell>78</cell><cell>72</cell></row><row><cell>iconGLCM2</cell><cell>80</cell><cell>74</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,92.22,519.46,361.28,96.87"><head>Table 5 -</head><label>5</label><figDesc>Differences for select classes.</figDesc><table coords="8,104.28,544.00,349.22,72.33"><row><cell>Class</cell><cell cols="2">Development set</cell><cell></cell><cell>Test set</cell><cell>Difference in error count</cell></row><row><cell></cell><cell>Count</cell><cell># correct</cell><cell cols="2">Count2 #correct2</cell><cell></cell></row><row><cell>108</cell><cell>93</cell><cell>78</cell><cell>92</cell><cell>54</cell><cell>23</cell></row><row><cell>61</cell><cell>21</cell><cell>21</cell><cell>20</cell><cell>16</cell><cell>4</cell></row><row><cell>44</cell><cell>10</cell><cell>7</cell><cell>10</cell><cell>2</cell><cell>5</cell></row><row><cell>12</cell><cell>23</cell><cell>21</cell><cell>22</cell><cell>16</cell><cell>4</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was funded in part by Grant <rs type="grantNumber">ITR-0325160</rs> of the <rs type="funder">US National Science Foundation</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_2E2Fjcn">
					<idno type="grant-number">ITR-0325160</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,95.76,137.80,3.54,8.48;9,126.14,137.80,373.93,8.48;9,126.12,148.66,389.42,8.48;9,126.12,159.46,361.76,8.48" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,213.84,137.80,286.23,8.48;9,126.12,148.66,196.12,8.48">Manual query modification and data fusion for medical image retrieval. 6th Workshop of the Cross-Language Evaluation Forum</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hersh</forename><forename type="middle">W</forename></persName>
		</author>
		<ptr target="http://medir.ohsu.edu/~hersh/imageclef-OHSU-05.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="9,329.18,148.66,42.34,8.48">CLEF 2005</title>
		<title level="s" coord="9,378.04,148.66,137.50,8.48;9,126.12,159.46,27.26,8.48">Springer Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct coords="9,95.76,170.26,3.54,8.48;9,126.14,170.26,357.72,8.48" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Gospodnetic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hatcher</surname></persName>
		</author>
		<title level="m" coord="9,243.66,170.26,62.34,8.48">Lucene in Action</title>
		<meeting><address><addrLine>Greenwich, CT</addrLine></address></meeting>
		<imprint>
			<publisher>Manning Publications</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,95.77,181.06,3.55,8.48;9,126.16,181.06,362.25,8.48;9,126.12,191.92,373.34,8.48;9,126.12,202.72,301.80,8.48;9,126.12,213.52,219.81,8.48" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,286.50,181.06,201.91,8.48;9,126.12,191.92,179.89,8.48">Feature generation, feature selection, classifiers, and conceptual drift for biomedical document triage</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">T</forename><surname>Bhuptiraju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hersh</forename><forename type="middle">W</forename></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec13/papers/ohsu-hersh.geo.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="9,311.82,191.92,187.64,8.48;9,126.12,202.72,40.50,8.48">The Thirteenth Text Retrieval Conference: TREC 2004. 2004</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<publisher>National Institute of Standards and Technology</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,95.76,224.38,3.54,8.48;9,126.14,224.38,387.92,8.48;9,126.12,235.18,389.90,8.48;9,126.12,245.98,366.36,8.48;9,126.12,256.84,30.82,8.48" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,264.54,224.38,249.52,8.48;9,126.12,235.18,92.17,8.48">A comparison of techniques for classification and ad hoc retrieval of biomedical documents</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hersh</forename><forename type="middle">Wr</forename></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec14/papers/ohsu-geo.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="9,224.82,235.18,209.62,8.48">The Fourteenth Text Retrieval Conference -TREC 2005</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<publisher>National Institute for Standards &amp; Technology</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,95.75,267.64,3.53,8.48;9,126.10,267.64,388.81,8.48;9,126.12,278.44,322.23,8.48;9,126.12,289.30,164.43,8.48" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,193.68,267.64,317.63,8.48">Advancing biomedical image retrieval: development and analysis of a test collection</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</author>
		<ptr target="http://www.jamia.org/cgi/reprint/M2082v1" />
	</analytic>
	<monogr>
		<title level="j" coord="9,126.12,278.44,215.41,8.48">Journal of the American Medical Informatics Association</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>Epub ahead of print</note>
</biblStruct>

<biblStruct coords="9,95.76,300.10,3.54,8.48;9,126.12,300.10,376.28,8.48;9,126.12,310.90,389.16,8.48;9,126.12,321.70,129.75,8.48" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,186.18,300.10,316.21,8.48;9,126.12,310.90,106.89,8.48">The use of MedGIFT and EasyIR for ImageCLEF 2005. 6th Workshop of the Cross-Language Evaluation Forum</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,240.06,310.90,42.29,8.48">CLEF 2005</title>
		<title level="s" coord="9,288.88,310.90,167.15,8.48">Springer Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct coords="9,95.77,332.56,3.55,8.48;9,126.12,332.56,328.11,8.48" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="9,174.42,332.56,154.25,8.48">Neural Networks for Pattern Recognition</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Clarendon Press</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,95.75,343.36,3.53,8.48;9,126.08,343.36,371.38,8.48" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="9,171.30,343.36,162.58,8.48">Netlab: Algorithms for Pattern Recognition</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">T</forename><surname>Nabney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>London, England</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,95.76,354.16,3.54,8.48;9,126.12,354.16,365.82,8.48;9,126.12,365.02,380.74,8.48;9,126.12,375.81,110.59,8.48" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,169.68,354.16,322.26,8.48;9,126.12,365.02,58.21,8.48">Comparing probabilistic and neural relevance feedback in an interactive information retrieval system</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,190.56,365.02,288.61,8.48">Proceedings of the 1994 IEEE International Conference on Neural Networks</title>
		<meeting>the 1994 IEEE International Conference on Neural Networks<address><addrLine>Orlando, Florida</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="3426" to="3430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,100.14,386.61,3.96,8.48;9,126.11,386.61,379.99,8.48;9,126.12,397.47,252.69,8.48" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,217.02,386.61,153.42,8.48">A novel BP-based image retrieval system</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno>1557-1560</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,376.62,386.61,129.48,8.48;9,126.12,397.47,77.29,8.48">IEEE International Symposium on Circuits and Systems</title>
		<meeting><address><addrLine>Kobe, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,100.15,408.27,3.96,8.48;9,126.11,408.27,336.73,8.48;9,126.12,419.07,186.63,8.48" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,202.38,408.27,260.46,8.48;9,126.12,419.07,59.77,8.48">A hybird image retrieval system with user&apos;s relevance feedback using neurocomputing</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,192.42,419.07,42.52,8.48">Informatica</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="271" to="279" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,100.14,429.93,3.96,8.48;9,126.11,429.93,389.43,8.48;9,126.12,440.73,16.50,8.48" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,180.18,429.93,176.81,8.48">Statistical and structural approaches to texture</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,362.88,429.93,90.64,8.48">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1979">1979</date>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="786" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,100.14,451.53,3.96,8.48;9,126.11,451.53,365.54,8.48;9,126.12,462.39,376.80,8.48;9,126.12,473.19,376.03,8.48;9,126.12,483.99,61.72,8.48" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,300.06,451.53,191.58,8.48;9,126.12,462.39,166.23,8.48">Supervised machine learning based medical image annotation and retrieval in ImageCLEFmed</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">C</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bhattacharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,318.12,462.39,184.80,8.48;9,126.12,473.19,72.65,8.48">6th Workshop of the Cross-Language Evaluation Forum</title>
		<title level="s" coord="9,205.27,473.19,167.12,8.48">Springer Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
	<note>CLEF 2005. in press</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
