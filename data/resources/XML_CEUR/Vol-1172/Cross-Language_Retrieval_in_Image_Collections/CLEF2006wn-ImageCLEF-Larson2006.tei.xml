<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,121.92,148.63,358.97,15.51;1,199.08,170.59,204.57,15.51">Text Retrieval and Blind Feedback for the ImageCLEF Photo Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,269.76,203.97,63.65,9.96"><forename type="first">Ray</forename><forename type="middle">R</forename><surname>Larson</surname></persName>
							<email>ray@sims.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,121.92,148.63,358.97,15.51;1,199.08,170.59,204.57,15.51">Text Retrieval and Blind Feedback for the ImageCLEF Photo Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5B5A0D399490158BD793C33415FF2898</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.7 Digital Libraries Algorithms, Performance, Measurement Cheshire II, Logistic Regression, Data Fusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we will describe Berkeley's approach to the ImageCLEF "Photo" task for CLEF 2006. This year is the first time that we have participated in ImageCLEF, and we chose to primarily establish a baseline for the Cheshire II system for this task, while we had originally hoped to use GeoCLEF methods for this task, in the end time constraints led us to restrict our submissions to the basic required runs for the task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper discusses the retrieval methods and evaluation results for Berkeley's participation in the ImageCLEF Photo task. Our submitted runs this year are intended to establish a simple baseline for comparision in future ImageCLEF tasks. This year we used only text-based retrieval methods for Imageclef, totally ignoring the images themselves (and the reference images specified in the queries). We hope, in future years, to be able to use combined text and image processing approaches similar to those used in some earlier work combining Berkeley's BlobWorld image retrieval system with the Cheshire II system (see <ref type="bibr" coords="1,306.65,623.85,10.22,9.96" target="#b6">[7]</ref>).</p><p>This year Berkeley submitted 7 runs, of which 4 where English Monolingual, 2 German Monolingual, and 1 Bilingual English to German.</p><p>This paper first describes the retrieval methods used, including our blind feedback method for text, followed by a discussion of our official submissions and the results obtained from other methods after official submissions had been closed. Finally we present some discussion of the results and our conclusions.</p><p>The basic form and variables of the Logistic Regression (LR) algorithm used for all of our submissions was originally developed by Cooper, et al. <ref type="bibr" coords="2,320.08,145.17,9.98,9.96" target="#b4">[5]</ref>. As originally formulated, the LR model of probabilistic IR attempts to estimate the probability of relevance for each document based on a set of statistics about a document collection and a set of queries in combination with a set of weighting coefficients for those statistics. The statistics to be used and the values of the coefficients are obtained from regression analysis of a sample of a collection (or similar test collection) for some set of queries where relevance and non-relevance has been determined. More formally, given a particular query and a particular document in a collection P (R | Q, D) is calculated and the documents or components are presented to the user ranked in order of decreasing values of that probability. To avoid invalid probability values, the usual calculation of P (R | Q, D) uses the "log odds" of relevance given a set of S statistics, s i , derived from the query and database, such that:</p><formula xml:id="formula_0" coords="2,235.08,273.98,277.90,30.50">log O(R | Q, D) = b 0 + S i=1 b i s i (1)</formula><p>where b 0 is the intercept term and the b i are the coefficients obtained from the regression analysis of the sample collection and relevance judgements. The final ranking is determined by the conversion of the log odds form to probabilities:</p><formula xml:id="formula_1" coords="2,232.44,354.77,280.54,25.12">P (R | Q, D) = e log O(R|Q,D) 1 + e log O(R|Q,D)<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">TREC2 Logistic Regression Algorithm</head><p>For all of our ImageCLEF submissions this year we used a version of the Logistic Regression (LR) algorithm that has been used very successfully in Cross-Language IR by Berkeley researchers for a number of years <ref type="bibr" coords="2,157.46,436.29,12.27,9.96" target="#b2">[3]</ref> and which is also used in our GeoCLEF and Domain Specific submissions. For the ImageCLEF task we used the Cheshire II information retrieval system implementation of this algorithm. One of the current limitations of this implementation is the lack of decompounding for German documents and query terms in the current system. As noted in our other CLEF notebook papers, the Logistic Regression algorithm used was originally developed by Cooper et al. <ref type="bibr" coords="2,486.49,484.17,10.55,9.96" target="#b3">[4]</ref> for text retrieval from the TREC collections for TREC2. The basic formula is:</p><formula xml:id="formula_2" coords="2,184.08,514.98,233.60,147.36">log O(R|C, Q) = log p(R|C, Q) 1 -p(R|C, Q) = log p(R|C, Q) p(R|C, Q) = c 0 + c 1 * 1 |Q c | + 1 |Qc| i=1 qtf i ql + 35 + c 2 * 1 |Q c | + 1 |Qc| i=1 log tf i cl + 80 -c 3 * 1 |Q c | + 1 |Qc| i=1 log ctf i N t + c 4 * |Q c |</formula><p>where C denotes a document component (i.e., an indexed part of a document which may be the entire document) and Q a query, R is a relevance variable, c k are the k coefficients obtained though the regression analysis.</p><formula xml:id="formula_3" coords="2,90.00,705.57,422.65,41.31">p(R|C, Q) is the probability that document component C is relevant to query Q, p(R|C, Q) the probability that document component C is not relevant to query Q, which is 1.0 - p(R|C, Q) |Q c |</formula><p>If stopwords are removed from indexing, then ql, cl, and N t are the query length, document length, and collection length, respectively. If the query terms are re-weighted (in feedback, for example), then qtf i is no longer the original term frequency, but the new weight, and ql is the sum of the new weight values for the query terms. Note that, unlike the document and collection lengths, query length is the "optimized" relative frequency without first taking the log over the matching terms.</p><p>The coefficients were determined by fitting the logistic regression model specified in log O(R|C, Q) to TREC training data using a statistical software package. The coefficients, c k , used for our official runs are the same as those described by Chen <ref type="bibr" coords="3,320.47,366.45,13.49,9.96" target="#b0">[1]</ref>. These were: c 0 = -3.51, c 1 = 37.4, c 2 = 0.330, c 3 = 0.1937 and c 4 = 0.0929. Further details on the TREC2 version of the Logistic Regression algorithm may be found in Cooper et al. <ref type="bibr" coords="3,320.93,390.33,9.98,9.96" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Blind Relevance Feedback</head><p>In addition to the direct retrieval of documents using the TREC2 logistic regression algorithm described above, we have implemented a form of "blind relevance feedback" as a supplement to the basic algorithm. The algorithm used for blind feedback was originally developed and described by Chen <ref type="bibr" coords="3,115.43,472.41,9.98,9.96" target="#b1">[2]</ref>. Blind relevance feedback has become established in the information retrieval community due to its consistent improvement of initial search results as seen in TREC, CLEF and other retrieval evaluations <ref type="bibr" coords="3,179.66,496.41,9.98,9.96" target="#b5">[6]</ref>. The blind feedback algorithm is based on the probabilistic term relevance weighting formula developed by Robertson and Sparck Jones <ref type="bibr" coords="3,358.69,508.29,9.89,9.96" target="#b7">[8]</ref>.</p><p>Blind relevance feedback is typically performed in two stages. First, an initial search using the original topic statement is performed, after which a number of terms are selected from some number of the top-ranked documents (which are presumed to be relevant). The selected terms are then weighted and then merged with the initial query to formulate a new query. Finally the reweighted and expanded query is submitted against the same collection to produce a final ranked list of documents. Obviously there are important choices to be made regarding the number of top-ranked documents to consider, and the number of terms to extract from those documents. For ImageCLEF this year, having no prior data to guide us, we chose to use the top 10 terms from 10 top-ranked documents. The terms were chosen by extracting the document vectors for each of the 10 and computing the Robertson and Sparck Jones term relevance weight for each document. This weight is based on a contingency table where the counts of 4 different conditions for combinations of (assumed) relevance and whether or not the term is, or is not in a document. Table <ref type="table" coords="3,478.63,651.81,5.03,9.96" target="#tab_1">1</ref> shows this contingency table.</p><p>The relevance weight is calculated using the assumption that the first 10 documents are relevant and all others are not. For each term in these documents the following weight is calculated: The 10 terms (including those that appeared in the original query) with the highest w t are selected and added to the original query terms. For the terms not in the original query, the new "term frequency" (qtf i in Equation 3 above) is set to 0.5. Terms that were in the original query, but are not in the top 10 terms are left with their original qtf i . For terms in the top 10 and in the original query the new qtf i is set to 1.5 times the original qtf i for the query. The new query is then processed using the same LR algorithm as shown in Equation 3 and the ranked results returned as the response for that topic.</p><formula xml:id="formula_4" coords="3,255.24,707.90,257.74,30.29">w t = log Rt R-Rt Nt-Rt N -Nt-R+Rt (3) Relevant Not Relevant In doc R t N t -R t N t Not in doc R -R t N -N t -R + R t N -N t R N -R N</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approaches for ImageCLEF</head><p>In this section we describe the specific approaches taken our official submitted runs for the ImageCLEF photo task. First we describe the indexing and term extraction methods used, and then the search features we used for the submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Indexing and Term Extraction</head><p>Although the Cheshire II system uses the XML structure of documents and extracts selected portions of the record for indexing and retrieval, for the submitted runs this year we used only a single one of these indexes that contains the entire content of the document. Table <ref type="table" coords="4,133.42,572.85,5.03,9.96" target="#tab_2">2</ref> lists the indexes created for the ImageCLEF database and the document elements from which the contents of those indexes were extracted. The "Used" column in Table <ref type="table" coords="4,466.48,584.73,5.03,9.96" target="#tab_2">2</ref> indicates whether or not a particular index was used in the submitted ImageCLEF runs. Although we had hoped to use more elements (especially geographic names) we ran out of time due to work on other CLEF tracks. We hope to be able to do some additional runs for discussion at the Meeting.</p><p>For all indexing we used language-specific stoplists to exclude function words and very common words from the indexing and searching. The German language runs, however, did not use decompounding in the indexing and querying processes to generate simple word forms from compounds (actually we tried, but there was a bug that failed to match any compounds in our runs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Search Processing</head><p>Searching the ImageCLEF collection used Cheshire II scripts to parse the topics and submit the title or title and narrative from the topics to the "topic" index containing all terms from the documents. For the monolingual search tasks we used the topics in the appropriate language (English or German), and for bilingual tasks the topics were translated from the source language to the target language using SYSTRAN (via Babelfish at Altavista). We believe that other translation tools provide a more accurate representation of the topics (like the L&amp;H P.C. translator used in our GeoCLEF entries. However since the runs and submission were done from a hotel room over a highly variable wireless connection we did not have access to all of the tools we would have normally used. We also did a German to English translation, but the quality of translation was so poor (partially because only short titles were available) that we decided not to submit that run.</p><p>We tried two main approaches for searching, the first used only the topic text from the title element, the second included both titles and the narrative elements. In all cases the "topic" index mentioned above was used, and probabilistic searches were carried out. Two forms of the TREC2 logistic regression algorithm were used. One used the basic algorithm as described above, and the other used the TREC2 algorithm with blind feedback using the top 10 terms from the 10 top-ranked documents in the initial retrieval.</p><p>Our official runs did not make use of the example images in queries, but after our official submissions we realized that this information could be used in a text-only system as well. This involved retrieving the associated metadata records from the example images included in the queries and using their titles and location information to expand the basic query. We tested this in unofficial "POST" runs that are also described below following the discussion of the official </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results for Submitted Runs</head><p>The summary results (as Mean Average Precision) for the official submitted bilingual and monolingual runs for both English and German are shown in Table <ref type="table" coords="6,367.37,591.93,3.90,9.96" target="#tab_4">3</ref>, the Recall-Precision curves for these runs are also shown in Figures <ref type="figure" coords="6,250.55,603.93,79.41,9.96">1 (for monolingual</ref>) and 2 (for bilingual). In Figures <ref type="figure" coords="6,480.30,603.93,5.03,9.96">1</ref> and<ref type="figure" coords="6,507.64,603.93,5.03,9.96">2</ref> the name are abbrevated to the final letters and numbers of the full name in Table <ref type="table" coords="6,450.93,615.81,3.90,9.96" target="#tab_4">3</ref>. Table <ref type="table" coords="6,490.14,615.81,5.03,9.96" target="#tab_5">4</ref> has a number of unofficial runs described in the next section, and Figures <ref type="figure" coords="6,395.79,627.81,5.03,9.96">3</ref> and<ref type="figure" coords="6,423.37,627.81,5.03,9.96">4</ref> show the Precision and Recall curves for those runs (with some of the runs from Figures <ref type="figure" coords="6,405.84,639.69,5.03,9.96">1</ref> and<ref type="figure" coords="6,435.34,639.69,5.03,9.96">2</ref> for comparison. (Note also the differences of the Y scale in these figures).</p><p>Table <ref type="table" coords="6,132.46,663.69,5.03,9.96" target="#tab_4">3</ref> shows all of our submitted runs for the ImageCLEF Photo task. Precision and recall curves for the runs are shown in Figures <ref type="figure" coords="6,268.89,675.57,5.03,9.96">1</ref> and<ref type="figure" coords="6,296.47,675.57,3.90,9.96">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Conclusions</head><p>The officially submitted runs described above, when compared to participants using combined text and image methods, are not particularly distinguished. The German monolingual and bilingual  runs suffer from the lack of decompounding in this version of the system. For the purpose of establishing a baseline performance for our system the runs are adequate. One interesting observation for is that, although the MAP for runs with blind feedback was higher than matching runs without it, the precision at low recall levels was lower than runs without feedback. We suspect that this is an effect of the small size of the metadata records available for this task when compared to the full-text in other collections. Use of the narrative for English topics allowed us to further improve performance in the official runs, although the same pattern for feedback versus no feedback performance as mentioned above was observed, indicative that the pattern is likely a function of the record size and not query size. Although our official runs for ImageCLEF Photo 2006 are in no way outstanding, they do provide a "foot in the door" and a baseline for tuning our future performance for this task.</p><p>Quite a different picture appears in our "POST" runs, shown in Table <ref type="table" coords="7,428.65,541.77,5.03,9.96" target="#tab_5">4</ref> and in Figures <ref type="figure" coords="7,507.68,541.77,5.03,9.96">3</ref> and<ref type="figure" coords="7,109.55,553.77,3.90,9.96">4</ref>. All of these runs use expansion of the queries based on the "TITLE" and "LOCATION" elements of the metadata annotations associated with images included in the "image" element of the queries (e.g. rather like image processing approaches, but using only text). All of these runs, had they been official runs, would have ranked at or near the top of the evaluation pools. (The similarity in between our MAP results and some others would seem to indicate that others were using a similar method of query expansion, perhaps in addition to image processing).</p><p>In Table <ref type="table" coords="7,143.85,625.53,5.03,9.96" target="#tab_6">5</ref> we compare the best performing runs, all using blind relevance feedback, for matching tasks in our official and unofficial runs. For Bilingual English to German we see a dramatic 146% improvement in MAP using this query expansion method. For the comparable Monolingual tasks we see improvements ranging from almost 50% to over 80%. This is strong indication that expanding queries using the metadata of relevant images is a very good strategy for the ImageCLEF Photo task.</p><p>As an further experiment, we decided to try this expansion method for Bilingual retrieval without any translation of the original topic, that is, we used the title in the original language (in this case French) and did our expansion using appropriate metadata for the target language (English). The result from this experiment is included in Table <ref type="table" coords="7,359.94,733.17,5.03,9.96" target="#tab_5">4</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,140.28,283.89,322.18,9.96"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Berkeley Monolingual Runs -English (left) and German (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,125.04,283.89,352.75,9.96"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Berkeley Monolingual POST Runs -English (left) and German (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,90.00,111.33,385.61,129.78"><head></head><label></label><figDesc>is the number of matching terms between a document component and a query, qtf i is the within-query frequency of the ith matching term,</figDesc><table coords="3,90.00,151.17,385.61,89.94"><row><cell>tf i is the within-document frequency of the ith matching term,</cell></row><row><cell>ctf i is the occurrence frequency in a collection of the ith matching term,</cell></row><row><cell>ql is query length (i.e., number of terms in a query like |Q| for non-feedback situations),</cell></row><row><cell>cl is component length (i.e., number of terms in a component), and</cell></row><row><cell>N t is collection length (i.e., number of terms in a test collection).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,178.68,168.33,245.53,9.96"><head>Table 1 :</head><label>1</label><figDesc>Contingency table for term relevance weighting</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,190.68,454.38,221.66,94.59"><head>Table 2 :</head><label>2</label><figDesc>Cheshire II Indexes for ImageCLEF 2006</figDesc><table coords="4,190.68,454.38,221.66,71.99"><row><cell>Name</cell><cell>Description</cell><cell cols="2">Content Tags Used</cell></row><row><cell>docno</cell><cell>Document ID</cell><cell>DOCNO</cell><cell>no</cell></row><row><cell>title</cell><cell>Article Title</cell><cell>TITLE</cell><cell>no</cell></row><row><cell>topic</cell><cell cols="2">All Content Words DOC</cell><cell>yes</cell></row><row><cell>date</cell><cell>Date of Image</cell><cell>DATE</cell><cell>no</cell></row><row><cell cols="3">geoname Image Place names LOCATION</cell><cell>no</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,106.56,222.45,389.33,132.44"><head>Table 3 :</head><label>3</label><figDesc>Submitted ImageCLEF Runs</figDesc><table coords="7,106.56,256.14,389.33,98.75"><row><cell>Run Name</cell><cell>Description</cell><cell>Query</cell><cell>Feedback</cell><cell>MAP</cell></row><row><cell>POST BI DEEN T EXP-TL</cell><cell cols="2">Bilingual German⇒English Title</cell><cell>Y</cell><cell>0.3114</cell></row><row><cell>POST BI ENDE T EXP-TL</cell><cell cols="2">Bilingual English⇒German Title</cell><cell>Y</cell><cell>0.2738</cell></row><row><cell cols="3">POST BI ENDE TN EXP-TL Bilingual English⇒German Title+Narr</cell><cell>Y</cell><cell>0.2645</cell></row><row><cell>POST BI FREN T EXP-TL</cell><cell>Bilingual French⇒English</cell><cell>Title</cell><cell>Y</cell><cell>0.2971</cell></row><row><cell>POST MO DE T EXP-TL</cell><cell>Monolingual German</cell><cell>Title</cell><cell>Y</cell><cell>0.2906</cell></row><row><cell>POST MO EN T EXP-TL</cell><cell>Monolingual English</cell><cell>Title</cell><cell>Y</cell><cell>0.3147</cell></row><row><cell>POST MO EN TN EXP-TL</cell><cell>Monolingual English</cell><cell>Title+Narr</cell><cell>Y</cell><cell>0.3562</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,114.36,368.01,373.96,9.96"><head>Table 4 :</head><label>4</label><figDesc>Unofficial POST ImageCLEF Runs (using image metadata query expansion)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="7,367.37,733.17,145.45,9.96"><head>Table 5 :</head><label>5</label><figDesc>as run "POST BI FREN T EXP-Comparison of Official and Unofficial POST ImageCLEF Runs TL". If that run had been submitted as an official run, it would have been a 186.5% improvement in MAP over the best official French to English submitted for the ImageCLEF Photo task.</figDesc><table coords="8,156.72,111.18,289.61,71.63"><row><cell>Description</cell><cell>Query</cell><cell cols="3">Official POST Percent</cell></row><row><cell></cell><cell></cell><cell>MAP</cell><cell>MAP</cell><cell>Improv.</cell></row><row><cell cols="2">Bilingual English⇒German Title+Narr</cell><cell>0.1072</cell><cell>0.2645</cell><cell>146.74</cell></row><row><cell>Monolingual German</cell><cell>Title</cell><cell>0.158</cell><cell>0.2906</cell><cell>83.92</cell></row><row><cell>Monolingual English</cell><cell>Title</cell><cell>0.1824</cell><cell>0.3147</cell><cell>72.53</cell></row><row><cell>Monolingual English</cell><cell>Title+Narr</cell><cell>0.2392</cell><cell>0.3562</cell><cell>48.91</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,105.47,303.93,407.33,9.96;8,105.48,315.81,407.16,9.96;8,105.48,327.81,406.97,9.96;8,105.48,339.81,407.13,9.96;8,105.48,351.69,141.70,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,166.38,303.93,297.92,9.96">Multilingual information retrieval using english and chinese queries</title>
		<author>
			<persName coords=""><forename type="first">Aitao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,421.06,315.81,91.58,9.96;8,105.48,327.81,406.97,9.96;8,105.48,339.81,83.04,9.96">Evaluation of Cross-Language Information Retrieval Systems: Second Workshop of the Cross-Language Evaluation Forum, CLEF-2001</title>
		<title level="s" coord="8,429.03,339.81,83.58,9.96;8,105.48,351.69,89.57,9.96">Springer Computer Scinece Series LNCS</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Martin</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Kluck</surname></persName>
		</editor>
		<meeting><address><addrLine>Darmstadt, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-09">September 2001. 2002</date>
			<biblScope unit="volume">2406</biblScope>
			<biblScope unit="page" from="44" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.47,371.61,407.24,9.96;8,105.48,383.61,94.70,9.96" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="8,166.86,371.61,213.38,9.96">Cross-Language Retrieval Experiments at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Aitao</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002. 2003</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">2785</biblScope>
			<biblScope unit="page" from="28" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.47,403.53,407.32,9.96;8,105.48,415.53,350.28,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,248.63,403.53,264.16,9.96;8,105.48,415.53,168.89,9.96">Multilingual information retrieval using machine translation, relevance feedback and decompounding</title>
		<author>
			<persName coords=""><forename type="first">Aitao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fredric</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,283.89,415.53,92.84,9.96">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="149" to="182" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.47,435.45,407.32,9.96;8,105.48,447.33,407.25,9.96;8,105.48,459.33,53.80,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,283.19,435.45,229.59,9.96;8,105.48,447.33,195.51,9.96">Full Text Retrieval based on Probabilistic Equations with Coefficients fitted by Logistic Regression</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">S</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,320.70,447.33,159.76,9.96">Text REtrieval Conference (TREC-2)</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.47,479.25,407.20,9.96;8,105.48,491.25,407.11,9.96;8,105.48,503.13,407.43,9.96;8,105.48,515.13,101.53,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,374.79,479.25,137.88,9.96;8,105.48,491.25,106.48,9.96">Probabilistic retrieval based on staged logistic regression</title>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">S</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fredric</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><forename type="middle">P</forename><surname>Dabney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,232.79,491.25,279.79,9.96;8,105.48,503.13,180.21,9.96">15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>Copenhagen, Denmark; New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1992">June 21-24. 1992</date>
			<biblScope unit="page" from="198" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.47,535.05,407.30,9.96;8,105.48,546.93,270.43,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,175.48,535.05,333.42,9.96">Probabilistic retrieval, component fusion and blind feedback for xml retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,117.95,546.93,47.20,9.96">INEX 2005</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3977</biblScope>
			<biblScope unit="page" from="225" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.47,566.85,407.18,9.96;8,105.48,578.85,407.03,9.96;8,105.48,590.85,407.25,9.96;8,105.48,602.73,150.90,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,262.42,566.85,250.23,9.96;8,105.48,578.85,186.01,9.96">Information access for a digital library: Cheshire II and the Berkeley environmental digital library</title>
		<author>
			<persName coords=""><forename type="first">Ray</forename><forename type="middle">R</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chad</forename><surname>Carson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,416.25,578.85,96.25,9.96;8,105.48,590.85,311.15,9.96">Knowledge: Creation, Organization and Use: Proceedings of the 62nd ASIS Annual Meeting</title>
		<editor>
			<persName><forename type="first">Larry</forename><surname>Woods</surname></persName>
		</editor>
		<meeting><address><addrLine>Medford, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Information Today</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="515" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.47,622.65,407.15,9.96;8,105.48,634.65,328.09,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,281.13,622.65,158.03,9.96">Relevance weighting of search terms</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Sparck</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,450.05,622.65,62.57,9.96;8,105.48,634.65,181.49,9.96">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="page" from="129" to="146" />
			<date type="published" when="1976-06">May-June 1976</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
