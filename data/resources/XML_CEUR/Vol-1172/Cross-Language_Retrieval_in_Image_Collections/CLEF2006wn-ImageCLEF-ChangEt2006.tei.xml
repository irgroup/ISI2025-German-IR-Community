<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,85.67,74.31,423.87,12.63;1,121.31,90.27,352.52,12.63">Approaches of Using a Word-Image Ontology and an Annotated Image Corpus as Intermedia for Cross-Language Image Retrieval</title>
				<funder ref="#_dewGnkW">
					<orgName type="full">National Science Council, Taiwan</orgName>
				</funder>
				<funder ref="#_xSvq7xX">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,224.39,128.33,67.49,8.96"><forename type="first">Yih-Chen</forename><surname>Chang</surname></persName>
							<email>ycchang@nlg.csie.ntu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Taiwan University Taipei</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.26,128.33,59.70,8.96"><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
							<email>hhchen@csie.ntu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Taiwan University Taipei</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,85.67,74.31,423.87,12.63;1,121.31,90.27,352.52,12.63">Approaches of Using a Word-Image Ontology and an Annotated Image Corpus as Intermedia for Cross-Language Image Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1B16D2D1983059FAB3FD80AEBA76CCAF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ACM Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval---Retrieval models</term>
					<term>Relevance feedback Cross language image retrieval</term>
					<term>cross-media translation</term>
					<term>image annotation</term>
					<term>ontology</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Two kinds of intermedia are explored in ImageCLEFphoto2006. The approach of using a word-image ontology maps images to fundamental concepts in an ontology and measure the similarity of two images by using the kind-of relationship of the ontology. The approach of using an annotated image corpus maps images to texts describing concepts in the images, and the similarity of two images is measured by text counterparts using BM25. The official runs show that visual query and intermedia are useful. Comparing the runs using textual query only with the runs merging textual query and visual query, the latter improved 71%~119% of the performance of the former. Even in the situation which example images were removed from the image collection beforehand, the performance was still improved about 21%~43%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, many methods <ref type="bibr" coords="1,204.83,431.81,152.02,8.96" target="#b1">(Clough, Sanderson, &amp; Müller, 2005</ref>; Clough, Müller, Deselaers, Grubinger, Lehmann, Jensen, &amp; Hersh, 2006) have been proposed to explore visual information to improve the performance of cross-language image retrieval. The challenging issue is the semantic differences among visual and textual information. For example, using t h e v i s u a l i n f o r ma t i o n " r e d c i r c l e "may retrieve noise images containing " red flower" , " red balloon" , " red ball" , a n d s oon, rather than the desired ones containing " s u n " . The semantic difference between visual concept " r e d c i r c l e " a n d textual symbol " s u n " is called a semantic gap.</p><p>Some approaches conducted text-and content-based image retrieval separately and then merged the results of two runs <ref type="bibr" coords="1,119.87,512.20,95.53,8.96" target="#b0">(Besançon, et al., 2005;</ref><ref type="bibr" coords="1,218.15,512.20,70.81,8.96" target="#b4">Jones, et al., 2005</ref>; Lin, Chang &amp; Chen, forthcoming). Content-based image retrieval may suffer from the semantic gap problem and report noise images. That may have negative effects on the final performance. Some other approaches (Lin, Chang &amp; Chen, forthcoming) learned the relationships between visual and textual information and used the relationships for media transformation. The final retrieval performance depends on the relationship mining.</p><p>In this paper, we use two intermedia approaches to capture the semantic gap. The main characteristic of these approaches is that human knowledge is imbedded in the intermedia and can be used to compute the semantic similarity of images. A word-image ontology and an annotated image corpus are explored and compared. Section 2 specifies how to build and use the word-image ontology. Section 3 deals with the uses of the annotated corpus. Sections 4 and 5 show and discuss the official runs in ImageCLEFphoto2006.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">An Approach of Using a Word-Image Ontology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Building the Ontology</head><p>A word-image ontology is a word ontology aligned with the related images on each node. Building such an ontology manually is time consuming. In ImageCLEFphoto2006, the image collection has 20,000 colored images. There are 15,998 images containing English captions in &lt;TITLE&gt; and &lt;DESCRIPTION&gt; fields. The vocabularies include more than 8,000 different words, thus an ontology with only hundreds words is not enough.</p><p>Instead of creating a new ontology from the beginning, we extend WordNet, the well-known word ontology, to a word-image ontology. In WordNet, different senses and relations are defined for each word. For simplicity, we only consider the first two senses and kind-of relations in the ontology. Because our experiments in ImageCLEF2004 <ref type="bibr" coords="2,156.23,119.09,116.60,8.96">(Lin, Chang &amp; Chen, 2006)</ref> showed that verbs and adjectives are less appropriate to be represented as visual features, we only used nouns here.</p><p>Before aligning images and words, we selected those nouns in both WordNet and image collection based on their TF-IDF scores. For each selected noun, we used Google image search to retrieval 60 images from the web. The returned images may encounter two problems: (1) they may have unrelated images, and (2) the related images may not be pure enough, i.e., the foci may be in the background or there may be some other things in the images. Zinger (2005) tried to deal with this problem by using visual features to cluster the retrieved images and filtering out those images not belonging to any clusters. Unlike Zinger (2005), we employed textual features. For each retrieved image, Google will return a short snippet. We filter out those images whose snippets do not exactly match the query terms. The basic idea is: " t h e mo r e t h i n g s a snippet mentions, the more complex the image is." Finally, we get a word-image ontology with 11,723 images in 2,346 nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Using the Ontology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Similarity Scoring</head><p>Each image contains several fundamental concepts specified in the word-image ontology. The similarity of two images is measured by the sum of the similarity of the fundamental concepts. In this paper we use kind-of relations to compute semantic distance between fundamental concepts A and B in the word-image ontology. At first, we find the least common ancestor (LCA) of A and B. The distance between A and B is the length of the path from A through LCA to B. When computing the semantic distance of nodes A and B, the more the nodes should be traversed from A to B, the larger the distance is. In addition to the path length, we also consider the weighting of links in a path shown as follows.</p><p>(1) When computing the semantic distance of a node A and its child B, we consider the number of children of A. The more children A has, the larger the distance between A and B is. In an extreme case, if A has only one child B, then the distance between A and B is 0. Let #children(A) denote the number of children of A, and base(A) denote the basic distance of A and its children. We define base(A) to be log(#children(A)).</p><p>(2) When computing the semantic distance of a node A and its brother, we consider the level it locates. Assume B is a child of A. If A and B have the same number of brothers, then the distance between A and its brothers is larger than that between B and its brothers. Let level(A) be the depth of node A. Assume the level of root is 0. The distance between node A and its child, denoted by dist(A), is defined to be ) (</p><formula xml:id="formula_0" coords="2,453.71,477.20,70.78,15.62">) ( A base c A level  .</formula><p>Here C is a constant between 0 and 1. In this paper, C is set to 0.9. If the shortest path between two different nodes N 0 and</p><formula xml:id="formula_1" coords="2,321.47,503.90,126.00,11.84">N m is N 0 , N 1 , …, N LCA , …, N m-1</formula><p>, N m , we define the distance between N 0 and N m to be:</p><formula xml:id="formula_2" coords="2,96.59,529.17,162.94,25.08">) ( ) ( ) , ( 1 - 1 0     m i i LCA m N dist N dist N N dist</formula><p>The larger the distance of two nodes is, the less similar the nodes are.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Mapping into the Ontology</head><p>Before counting the semantic distance between two given images, we need to map the two images into nodes of the ontology. In other words, we have to find the fundamental concepts the two images consist of. A CBIR system is adopted. It regards an image as a visual query and retrieves the top k fundamental images (i.e., fundamental concepts) in the word-image ontology. In such a case, we have two sets of nodes S 1 ={n 11 , n 12 , n 13 , …, n 1k } and S 2 ={n 21 , n 22 , n 23 , …, n 2k }, which correspond to the two images, respectively. We define the following formula to compute the semantic distance:</p><formula xml:id="formula_3" coords="2,96.71,669.32,278.06,24.47">k j where n n dist S S stance SemanticDi j k i i ,..., 1 )), , ( min( ) , ( 2 1 1 2 1    </formula><p>Given a query with m example images, we regard each example image Q as an independent visual query, and compute the semantic distance between Q and images in the collection. Note that we determine what concepts are composed of an image in the collection before retrieval. After m image retrievals, each image in the collection has been assigned m scores based on the above formula. We choose the best score for each image and sort all the images to create a rank list. Finally, the top 1000 images in the rank list will be reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">An Approach of Using an Annotated Image Corpus</head><p>An annotated image corpus is a collection of images along with their text annotations. The text annotation specifies the concepts and their relationships in the images. To measure the similarity of two images, we have to know how many common concepts there are in the two images. An annotated image corpus can be regarded as a reference corpus. We submit two CBIRs to the reference corpus for the two images to be compared. The corresponding text annotations of the retrieved images are postulated to contain the concepts embedded in the two images. The similarity of text annotations measures the similarity of the two images indirectly.</p><p>The image collection in ImageCLEFphoto2006 can be considered as a reference annotated image corpus. Using image collection itself as intermedia has some advantages. It is not necessary to map images in the image collection to the intermedia. Besides, the domain can be more restricted to peregrine pictures. In the experiments, the &lt;DESCRIPTION&gt;, &lt;NOTE&gt;, and &lt;LOCATION&gt; fields in English form the annotated corpus.</p><p>To use the annotated image corpus as intermedia to compute similarity between example images and images in image collection, we need to map these images into intermedia. Since we use the image collection itself as intermedia, we only need to map example images in this work. An example image is considered as a visual query and submitted to retrieve images in intermedia by a CBIR system. The corresponding text counterparts of the top returned k images form a long text query and it is submitted to an Okapi system to retrieval images in the image collection. BM25 formula measures the similarity between example images and images in image collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In the formal runs, we submitted 25 cross-lingual runs for eight different query languages. All the queries with different source languages were translated into target language (i.e., English) using SYSTRAN system. We considered several issues, including (1) using different intermedia approaches (i.e., the text-image ontology and the annotated image corpus), and (2) with/without using visual query. In addition, we also submitted 4 monolingual runs which compared (1) the annotation in English and in German, and (2) using or not using visual query and intermedia. At last, we submitted a run using visual query and intermedia only. The details of our runs are described as follows: These runs are regarded as baselines and are compared with the runs using both textual and visual information.</p><p>(2) 2 monolingual and text query only runs: NTU-EN-EN-AUTO-NOFB-TXT, and NTU-DE-DE-AUTO-NOFB-TXT. These two runs serve as the baselines to compare with cross-lingual runs with text query only, and to compare with the runs using both textual and visual information. (3) 1 visual query only run with the approach of using an annotated image corpus: NTU-AUTO-FB-TXTIMG-WEprf. This run will be merged with the runs using textual query only, and is also a baseline to compare with the runs using both visual and textual queries. (5) 8 cross-lingual runs, using both textual and visual queries with the approach of using word-image ontology: NTU-PT-EN-AUTO-NOFB-TXTIMG-T-IOntology,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NTU-RU-EN-AUTO-NOFB-TXTIMG-T-IOntology, NTU-ES-EN-AUTO-NOFB-TXTIMG-T-IOntology, NTU-FR-EN-AUTO-NOFB-TXTIMG-T-IOntology, NTU-ZHS-EN-AUTO-NOFB-TXTIMG-T-IOntology, NTU-JA-EN-AUTO-NOFB-TXTIMG-T-IOntology, NTU-ZHT-EN-AUTO-NOFB-TXTIMG-T-IOntology, and</head><p>NTU-IT-EN-AUTO-NOFB-TXTIMG-T-IOntology. These runs merge textual query only runs in <ref type="bibr" coords="4,297.83,153.53,10.69,8.96" target="#b0">(1)</ref>, and visual query runs with weights 0.9 and 0.1.</p><p>(6) 2 monolingual runs, using both textual and visual queries with the approach of an annotated corpus: NTU-EN-EN-AUTO-FB-TXTIMG, and NTU-DE-DE-AUTO-FB-TXTIMG These two runs using both textual and visual queries. The monolingual run in (2) and the visual run in (3) are merged with equal weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Discussions</head><p>Table <ref type="table" coords="4,96.47,275.45,4.98,8.96" target="#tab_0">1</ref> shows experimental results of official runs in ImageCLEFphoto2006. We compare performance of the runs using textual query only, and the runs using both textual and visual queries (i.e., Text Only vs. Text + Annotation and Text + Ontology). In addition, we also compare the runs using word-image ontology and the runs using annotated image corpus (i.e., Text + Ontology vs. Text + Annotation). The runs whose performance is better than that of baseline (i.e., Text Only) will be marked in bold. The results show all runs using annotated image corpus are better than the baseline. In contrast, only two runs using word-image ontology are better.  Since the example images in this task are in the image collection, the CBIR system always correctly maps the example images into themselves at mapping step. We made some extra experiments to examine the performance of our intermedia approach. In the experiments, we took out the example images from the image collection when mapping example images into intermedia. Table <ref type="table" coords="5,334.67,256.13,4.98,8.96" target="#tab_1">2</ref> shows the experiment results. Although the performance of Table <ref type="table" coords="5,160.54,267.53,4.98,8.96" target="#tab_1">2</ref> is lower than that of Table <ref type="table" coords="5,277.76,267.53,3.76,8.96" target="#tab_0">1</ref>, the runs using the approach of annotated image corpus are still better than the runs using textual query only.  Table <ref type="table" coords="5,120.47,654.53,4.98,8.96" target="#tab_2">3</ref> shows the experiment results of monolingual runs. Using both textual and visual queries are still better than runs using textual query only. The performance of the runs by taking out the example images from collection beforehand is still better than the runs use textual query only. Comparing with Tables <ref type="table" coords="5,460.55,677.45,4.98,8.96" target="#tab_0">1</ref> and<ref type="table" coords="5,485.75,677.45,3.76,8.96" target="#tab_1">2</ref>, we can find some cross-lingual runs that use textual and visual queries are even better than monolingual run that use textual query only. That means visual information is very important when doing cross language image retrieval.</p><p>Table <ref type="table" coords="5,120.95,712.01,4.98,8.96" target="#tab_3">4</ref> shows the experiment of runs using visual query only. When example images were kept in the image collection, we can always map the example images into the right images. Therefore, the translation from visual information into textual information will be more correctly. The experiment shows the performance of visual query runs is better than that of textual query runs when the transformation is correct. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>The experiments show visual query and intermedia approaches are useful. Comparing the runs using textual query only with the runs merging textual query and visual query, the latter improved 71%~119% of performance of the former. Even in the situation which example images are removed from the image collection, the performance can still be improved about 21%~43%. We find visual query in image retrieval is important. The performance of the runs using visual query only can be even better than the runs using textual only if we translate visual information into textual one correctly. In this year the word-image ontology built automatically still contain much noise. We will investigate how to filter out the noise and explore different methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,95.03,433.97,185.53,8.96;3,119.03,448.49,279.21,8.96;3,119.03,459.89,284.73,8.96;3,119.03,471.41,278.01,8.96;3,119.03,482.93,299.01,8.96"><head>( 1 )</head><label>1</label><figDesc>8 cross-lingual and text query only runs: NTU-PT-EN-AUTO-NOFB-TXT, NTU-RU-EN-AUTO-NOFB-TXT, NTU-ES-EN-AUTO-NOFB-TXT, NTU-ZHT-EN-AUTO-NOFB-TXT, NTU-FR-EN-AUTO-NOFB-TXT, NTU-JA-EN-AUTO-NOFB-TXT, NTU-IT-EN-AUTO-NOFB-TXT, and NTU-ZHS-EN-AUTO-NOFB-TXT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,95.03,625.85,424.57,8.96;3,119.15,640.37,368.73,8.96;3,119.15,651.77,367.89,8.96;3,119.15,663.29,374.01,8.96;3,119.15,674.81,383.01,8.96;3,119.15,686.33,405.25,8.96;3,119.15,697.73,29.61,8.96"><head>( 4 )</head><label>4</label><figDesc>8 cross-lingual runs, using both textual and visual queries with the approach of an annotated corpus: NTU-PT-EN-AUTO-FB-TXTIMG-T-WEprf, NTU-RU-EN-AUTO-FB-TXTIMG-T-WEprf, NTU-ES-EN-AUTO-FB-TXTIMG-T-WEprf, NTU-FR-EN-AUTO-FB-TXTIMG-T-WEprf, NTU-ZHS-EN-AUTO-FB-TXTIMG-T-WEprf, NTU-JA-EN-AUTO-FB-TXTIMG-T-WEprf, NTU-ZHT-EN-AUTO-FB-TXTIMG-T-Weprf, and NTU-IT-EN-AUTO-FB-TXTIMG-T-Weprf. These runs merge the textual query only runs in (1) and visual query only run in (3) with equal weight.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,76.31,391.30,40.11,8.10;4,180.00,391.30,24.78,8.10;4,229.31,391.30,36.18,8.10;4,319.31,391.30,121.42,8.10;4,180.00,409.90,24.78,8.10;4,229.31,409.66,69.02,8.10;4,319.31,409.66,161.17,8.10;4,180.00,428.14,24.78,8.10;4,229.31,428.14,59.46,8.10;4,319.31,428.14,186.66,8.10;4,76.31,447.70,28.50,8.10;4,180.00,447.70,24.78,8.10;4,229.31,447.70,36.18,8.10;4,319.31,447.70,124.30,8.10;4,180.00,466.42,24.78,8.10;4,229.31,466.18,69.02,8.10;4,319.31,466.18,161.76,8.10;4,180.00,484.66,24.78,8.10;4,229.31,484.66,59.46,8.10;4,319.31,484.66,189.54,8.10;4,76.31,504.22,28.62,8.10;4,180.00,504.22,24.78,8.10;4,229.31,504.22,36.18,8.10;4,319.31,504.22,122.26,8.10;4,180.00,522.94,24.78,8.10;4,229.31,522.70,69.02,8.10;4,319.31,522.70,159.84,8.10;4,180.00,541.18,24.78,8.10;4,229.31,541.18,59.46,8.10;4,319.31,541.18,187.50,8.10;4,76.31,560.74,25.02,8.10;4,180.00,560.74,24.78,8.10;4,229.31,560.74,36.18,8.10;4,319.31,560.74,122.86,8.10;4,180.00,579.46,24.78,8.10;4,229.31,579.22,69.02,8.10;4,319.31,579.22,162.48,8.10;4,180.00,597.70,24.78,8.10;4,229.31,597.70,59.46,8.10;4,319.31,597.70,187.98,8.10;4,76.31,617.26,69.27,8.10;4,180.00,617.26,24.78,8.10;4,229.31,617.26,36.18,8.10;4,319.31,617.26,128.86,8.10;4,180.00,635.85,24.78,8.10;4,229.31,635.62,69.02,8.10;4,319.31,635.62,166.20,8.10;4,180.00,654.46,24.78,8.10;4,229.31,654.22,59.46,8.10;4,319.31,654.22,193.98,8.10;4,76.31,673.78,31.96,8.10;4,180.00,673.78,24.78,8.10;4,229.31,673.78,36.18,8.10;4,319.31,673.78,121.78,8.10;4,180.00,692.38,24.78,8.10;4,229.31,692.14,69.02,8.10;4,319.31,692.14,159.24,8.10;4,180.00,710.62,24.77,8.10;4,229.31,710.62,59.46,8.10;4,319.31,710.62,187.02,8.10;5,193.19,76.73,205.28,8.96;5,76.31,95.38,70.95,8.10;5,180.00,95.38,24.78,8.10;5,229.31,95.38,36.18,8.10;5,319.31,95.38,128.62,8.10;5,180.00,114.10,24.78,8.10;5,229.31,113.85,69.02,8.10;5,319.31,113.85,165.84,8.10;5,180.00,132.58,24.78,8.10;5,229.31,132.34,59.46,8.10;5,319.31,132.34,193.74,8.10;5,76.31,151.90,22.98,8.10;5,180.00,151.90,24.78,8.10;5,229.31,151.90,36.18,8.10;5,319.31,151.90,119.50,8.10;5,180.00,170.62,24.78,8.10;5,229.31,170.38,69.02,8.10;5,319.31,170.38,156.96,8.10;5,180.00,188.85,24.78,8.10;5,229.31,188.85,59.46,8.10;5,319.31,188.85,184.74,8.10"><head>1 .</head><label>1</label><figDesc>NTU-ES-EN-AUTO-FB-TXTIMG-T-Weprf 0.1554 Text + Ontology NTU-ES-EN-AUTO-NOFB-TXTIMG--EN-AUTO-FB-TXTIMG-T-Weprf 0.1396 Text + Ontology NTU-JA-EN-AUTO-NOFB-TXTIMG-T-IOntologyTable Performance of Official Runs (Continued) NTU-IT-EN-AUTO-NOFB-TXTIMG-T-IOntology</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,76.31,337.30,40.11,8.10;5,180.00,337.30,24.78,8.10;5,229.31,337.30,36.18,8.10;5,319.31,337.30,121.42,8.10;5,180.00,356.02,24.78,8.10;5,229.31,355.78,69.02,8.10;5,319.31,355.78,180.58,8.10;5,76.31,375.34,28.50,8.10;5,180.00,375.34,24.78,8.10;5,229.31,375.34,36.18,8.10;5,319.31,375.34,124.30,8.10;5,180.00,394.06,24.78,8.10;5,229.31,393.82,69.02,8.10;5,319.31,393.82,181.18,8.10;5,76.31,413.38,28.62,8.10;5,180.00,413.38,24.78,8.10;5,229.31,413.38,36.18,8.10;5,319.31,413.38,122.26,8.10;5,180.00,431.98,24.78,8.10;5,229.31,431.74,69.02,8.10;5,319.31,431.74,179.26,8.10;5,76.31,451.30,25.02,8.10;5,180.00,451.30,24.78,8.10;5,229.31,451.30,36.18,8.10;5,319.31,451.30,122.86,8.10;5,180.00,470.02,24.78,8.10;5,229.31,469.78,69.02,8.10;5,319.31,469.78,181.90,8.10;5,76.31,489.34,69.27,8.10;5,180.00,489.34,24.78,8.10;5,229.31,489.34,36.18,8.10;5,319.31,489.34,128.86,8.10;5,180.00,508.06,24.78,8.10;5,229.31,507.82,69.02,8.10;5,319.31,507.82,185.62,8.10;5,76.31,527.38,31.96,8.10;5,180.00,527.38,24.78,8.10;5,229.31,527.38,36.18,8.10;5,319.31,527.38,121.78,8.10;5,180.00,545.98,24.78,8.10;5,229.31,545.74,69.02,8.10;5,319.31,545.74,178.66,8.10;5,76.31,565.30,70.95,8.10;5,180.00,565.30,24.78,8.10;5,229.31,565.30,36.18,8.10;5,319.31,565.30,128.62,8.10;5,180.00,584.02,24.78,8.10;5,229.31,583.78,69.02,8.10;5,319.31,583.78,185.26,8.10;5,76.31,603.34,22.98,8.10;5,180.00,603.34,24.78,8.10;5,229.31,603.34,36.18,8.10;5,319.31,603.34,119.50,8.10;5,180.00,622.06,24.78,8.10;5,229.31,621.82,69.02,8.10;5,319.31,621.82,176.38,8.10"><head></head><label></label><figDesc>NTU-ZHS-EN-AUTO-FB-TXTIMG-T-Weprf-NoE Japanese 0.1431 Text Only NTU-JA-EN-AUTO-NOFB-TXT 0.1702 Text + Annotations NTU-JA-EN-AUTO-FB-TXTIMG-T-Weprf-NTU-IT-EN-AUTO-FB-TXTIMG-T-Weprf-NoE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,85.79,355.97,340.94,23.76"><head>Table 1 .</head><label>1</label><figDesc>Performance of Official Runs</figDesc><table coords="4,85.79,371.62,340.94,8.10"><row><cell>Query Language</cell><cell>MAP</cell><cell>Description</cell><cell>Runs</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,85.79,302.09,412.28,23.76"><head>Table 2 .</head><label>2</label><figDesc>Performance of Runs by Removing Example Images from the Collection (Unofficial Runs)</figDesc><table coords="5,85.79,317.74,340.94,8.10"><row><cell>Query Language</cell><cell>MAP</cell><cell>Description</cell><cell>Runs</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,76.31,73.13,433.80,137.04"><head>Table 3 .</head><label>3</label><figDesc>Performance of Monolingual Image Retrieval</figDesc><table coords="6,76.31,88.90,433.80,121.26"><row><cell>Query Language</cell><cell>MAP</cell><cell>Description</cell><cell>Runs</cell></row><row><cell>English</cell><cell>0.1787</cell><cell>Text Only</cell><cell>NTU-EN-EN-AUTO-NOFB-TXT</cell></row><row><cell>(+example images)</cell><cell>0.2950</cell><cell>Text + Annotations</cell><cell>NTU-EN-EN-AUTO-FB-TXTIMG</cell></row><row><cell>(-example images)</cell><cell>0.2027</cell><cell>Text + Annotations</cell><cell>NTU-EN-EN-AUTO-FB-TXTIMG-NoE (unofficial)</cell></row><row><cell>German</cell><cell>0.1294</cell><cell>Text Only</cell><cell>NTU-DE-DE-AUTO-NOFB-TXT</cell></row><row><cell>(+example images)</cell><cell>0.3109</cell><cell>Text + Annotations</cell><cell>NTU-DE-DE-AUTO-FB-TXTIMG</cell></row><row><cell>(-example images)</cell><cell>0.1608</cell><cell>Text + Annotations</cell><cell>NTU-DE-DE-AUTO-FB-TXTIMG-NoE (unofficial)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,103.43,232.25,401.99,83.16"><head>Table 4 .</head><label>4</label><figDesc>Performance of Visual Query</figDesc><table coords="6,103.43,250.90,401.99,64.50"><row><cell>MAP</cell><cell>Description</cell><cell>Runs</cell></row><row><cell>0.1787</cell><cell>Text Only (monolingual)</cell><cell>NTU-EN-EN-AUTO-NOFB-TXT</cell></row><row><cell>0.2757</cell><cell>Visual Only (+example images)</cell><cell>NTU-AUTO-FB-TXTIMG-Weprf</cell></row><row><cell>0.1174</cell><cell>Visual Only (-example images)</cell><cell>NTU-AUTO-FB-TXTIMG-Weprf-NoE (unofficial)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Research of this paper was partially supported by <rs type="funder">National Science Council, Taiwan</rs>, under the contracts <rs type="grantNumber">NSC94-2213-E-002-076</rs> and <rs type="grantNumber">NSC95-2752-E-001-001-PAE</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_dewGnkW">
					<idno type="grant-number">NSC94-2213-E-002-076</idno>
				</org>
				<org type="funding" xml:id="_xSvq7xX">
					<idno type="grant-number">NSC95-2752-E-001-001-PAE</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,74.31,547.06,450.10,8.10;6,84.47,557.50,440.01,8.10;6,84.47,567.82,440.02,8.10;6,84.47,578.14,32.85,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,308.86,547.06,215.56,8.10;6,84.47,557.50,143.86,8.10">Cross-media feedback strategies: Merging text and image information to improve image retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Besançon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Hède</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">A</forename><surname>Moellic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fluhr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,112.55,567.82,269.95,8.10">Proceedings of 5th Workshop of the Cross-Language Evaluation Forum</title>
		<title level="s" coord="6,390.95,567.82,22.88,8.10">LNCS</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<meeting>5th Workshop of the Cross-Language Evaluation Forum<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page" from="709" to="717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.31,588.46,450.30,8.10" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,266.51,588.46,196.65,8.10">The CLEF 2004 cross language image retrieval track</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<editor>Peters, C.</editor>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,84.47,598.78,440.07,8.10;6,84.47,609.10,232.65,8.10" xml:id="b2">
	<analytic>
	</analytic>
	<monogr>
		<title level="m" coord="6,334.43,598.78,190.11,8.10;6,84.47,609.10,64.15,8.10">Proceedings of 5th Workshop of the Cross-Language Evaluation Forum</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<meeting>5th Workshop of the Cross-Language Evaluation Forum<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.31,619.54,450.25,8.10;6,84.47,629.85,203.34,8.10;6,287.87,627.95,4.68,5.40;6,296.03,629.85,228.51,8.10;6,84.47,640.18,123.45,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,464.27,619.54,60.30,8.10;6,84.47,629.85,133.15,8.10">The CLEF 2005 cross-language image retrieval track</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,224.51,629.85,63.30,8.10;6,287.87,627.95,4.68,5.40;6,296.03,629.85,193.15,8.10">Proceedings of 6 th Workshop of the Cross Language Evaluation Forum</title>
		<title level="s" coord="6,497.15,629.85,27.39,8.10;6,84.47,640.18,97.23,8.10">Lecture Notes in Computer Science</title>
		<meeting>6 th Workshop of the Cross Language Evaluation Forum</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.31,650.50,449.94,8.10;6,84.47,660.82,440.00,8.10;6,84.47,671.26,440.00,8.10;6,84.47,681.58,136.53,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,429.00,650.50,95.26,8.10;6,84.47,660.82,262.25,8.10">Dublin City University at CLEF 2004: Experiments with the ImageCLEF St Andrew&apos;s Collection</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Groves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khasin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lam-Adesina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mellebeek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Way</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,228.95,671.26,264.91,8.10">Proceedings of 5th Workshop of the Cross-Language Evaluation Forum</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<meeting>5th Workshop of the Cross-Language Evaluation Forum<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page" from="653" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.31,691.90,215.11,8.10;6,283.43,690.03,241.00,9.97;6,84.47,700.35,440.07,9.97;6,84.47,712.54,115.65,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,416.75,690.03,107.68,9.97;6,84.47,700.35,201.72,9.97">o r ma t i o n f o r C r o s s -Language Image Retrieval: A Trans-Me d i a Di c t i o n a r y Ap p r o a c h</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,290.63,702.22,148.62,8.10">Information Processing and Management</title>
		<imprint/>
	</monogr>
	<note>forthcoming. I n t e g r a t i n g T e x t u a l a n d V i s u a l I n f. Special Issue on Asia Information Retrieval Research</note>
</biblStruct>

<biblStruct coords="6,74.31,720.98,450.23,9.97;6,84.47,733.30,273.33,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,186.35,720.98,249.72,9.96">r a c t i n g a n On t o l o g y o f P o r t a b l e Ob j e c t s f r o m Wo r d Ne t</title>
	</analytic>
	<monogr>
		<title level="m" coord="6,444.59,722.85,79.95,8.10;6,84.47,733.30,269.54,8.10">Proceedings of the MUSCLE/ImageCLEF Workshop on Image and Video Retrieval Evaluation</title>
		<meeting>the MUSCLE/ImageCLEF Workshop on Image and Video Retrieval Evaluation</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>S .</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
