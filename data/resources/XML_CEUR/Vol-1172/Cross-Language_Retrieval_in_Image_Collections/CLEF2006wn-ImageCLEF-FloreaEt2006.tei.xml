<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,110.34,148.86,382.31,15.15;1,166.46,170.78,270.09,15.15">MedIC/CISMeF at ImageCLEF 2006: Image Annotation and Retrieval Tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,150.91,204.67,36.83,8.74"><forename type="first">F</forename><surname>Florea</surname></persName>
							<email>filip.florea@insa-rouen.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">LITIS Laboratory</orgName>
								<orgName type="institution">INSA de Rouen</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">CISMeF Team</orgName>
								<orgName type="institution" key="instit1">Rouen University Hospital</orgName>
								<orgName type="institution" key="instit2">GCSIS Medical School of Rouen</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,205.01,204.67,47.46,8.74"><forename type="first">A</forename><surname>Rogozan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LITIS Laboratory</orgName>
								<orgName type="institution">INSA de Rouen</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,263.38,204.67,41.27,8.74"><forename type="first">V</forename><surname>Cornea</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">CISMeF Team</orgName>
								<orgName type="institution" key="instit1">Rouen University Hospital</orgName>
								<orgName type="institution" key="instit2">GCSIS Medical School of Rouen</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,314.74,204.67,52.29,8.74"><forename type="first">A</forename><surname>Bensrhair</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LITIS Laboratory</orgName>
								<orgName type="institution">INSA de Rouen</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,394.52,204.67,46.39,8.74"><forename type="first">S</forename><surname>Darmoni</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LITIS Laboratory</orgName>
								<orgName type="institution">INSA de Rouen</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">CISMeF Team</orgName>
								<orgName type="institution" key="instit1">Rouen University Hospital</orgName>
								<orgName type="institution" key="instit2">GCSIS Medical School of Rouen</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,110.34,148.86,382.31,15.15;1,166.46,170.78,270.09,15.15">MedIC/CISMeF at ImageCLEF 2006: Image Annotation and Retrieval Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">62BF22A85504F76541769F5EBA383253</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval Content-based image retrieval</term>
					<term>image categorization</term>
					<term>visual/textual retrieval</term>
					<term>classification</term>
					<term>machine learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the 2006 ImageCLEF cross-language image retrieval track, the MedIC/CISMeF group participated at the two medical-related tasks: the automatic annotation task and the multilingual image retrieval task. For the first task we submitted four runs based on supervised classification of combined texture and statistical image representations, the best result being the fourth rank at only 1% of the winner. The architecture proposed for the second task is reposing on textual-retrieval using terms derived from the MeSH thesaurus, combined with ranking by visual similarity. Due to technical and practical difficulties, the only run we were able to submit was incomplete, resulting in a modest result in the pools. Therefore, the actual capacity of the proposed retrieval architecture could not be evaluated.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>ImageCLEF cross-language image retrieval track was established in 2003 as part of the Cross Language Evaluation Forum (CLEF), a benchmarking multilingual information retrieval campaign held annually since 2000.</p><p>The CISMeF project<ref type="foot" coords="1,196.82,612.56,3.97,6.12" target="#foot_0">1</ref> (French acronym for Catalog and Index of French-language health resources) <ref type="bibr" coords="1,129.21,626.09,10.52,8.74" target="#b0">[1]</ref> is a quality-controlled subject gateway initiated by the Rouen University Hospital. The objective is to describe and index the main French-language health resources (documents on the web) to assist the users (i.e. health professionals, students or general public) in their search for high quality medical information available on the Internet.</p><p>The CISMeF team currently is developing an image information extraction and annotation module, named MedIC (i.e. Medical Image Categorization), to allow direct access to images extracted from health-documents.</p><p>For the 2006 edition of ImageCLEF, we used the MedIC module to participate at the two tasks involving medical images: the medical image annotation task and the multilingual medical image retrieval task. We used different approaches for each of the two tasks, following their specificity and objectives. Thus, an approach based on supervised classification of image visual representations was used for the annotation task, and an approach reposing on bilingual (i.e. English-French) MeSH term text-retrieval combined with visual image similarity for the retrieval task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Medical Image Annotation Task</head><p>When searching for images in non-annotated databases, medical image categorization and annotation can be very useful. Generally the first step is to define the categories (i.e. classes of images) you need to "recognize" and to select a set of image prototypes for each category. Then, each new/unknown image is projected in one of the categories (using some form of image similarity), and thus annotated with the identifier of the category (or the multilingual terms that can be associated to each category). One of the major disadvantages of this approach is the dependence on manually annotated prototypes for each of the categories, especially when treating domains as vast and rich as medicine. A second weakness is its difficulty to treat "unknown" images, an additional category being simply too big to create (basically it should contain everything else). Nevertheless, medical image annotation proved to be significantly accurate when an image sub-domain is considered, and image categories are well defined.</p><p>The image annotation task of ImageCLEF 2006 is using an image dataset provided by the IRMA project, consisting of 10000 fully annotated x-rays taken randomly from medical routine and 1000 x-rays for which classification labels are not available to the participants. The images are subdivided in 116 categories (classes), hierarchically organized according to image modality (x-ray, plain radiography), body orientation, body region, and biological system captured. The annotated dataset is intended to be used for training (9000 images) and validation -system tuning by parameter optimization (1000 images) and the 1000 un-annotated images for tests. Each approach/run is evaluated according to the annotation of the test set.</p><p>Our approach is based on the supervised classification of combined both local and global texture and statistic image representations. Because the resulted number of features can be significant compared to the available data samples (raising estimation problems for the classifiers) we added a dimensionality reduction phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image Resizing, Global and Local Representations</head><p>For these experiments, all images are resized to a fixed dimension of 256×256 pixels. Even though this simplifies several aspects of the problem (e.g. simpler generation of texture filters, normalization of the resulted representation size), we want to point out that loosing the original image aspect ratio introduces some structural and textural deformations, which could result in poor performances when dealing, for example, with image fragments. However, from our observations (in general) and after the examination of the IRMA database we concluded that, most of the times, images of the same category have the same aspect ratio, and thus will finally be deformed in the same way.</p><p>The image features can be extracted both globally and from local representations. The global features are extracted at the image level and have the advantage of being less sensible of small local variations, noise and/or geometrical transformations (e.g. especially rotation). However due to the importance of details in medical imaging, local features are of great importance when representing the medical image content. To take in consideration the spatial disposition of features inside the images we choose to also use a local representation obtained by splitting the original image in 16 equal sub-blocks (of 64x64 pixels). This way each image is represented by a vector of 16 blocks, and from each block features are extracted to describe its content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feature Extraction</head><p>The x-rays used for the annotation task are by nature acquired in gray-levels, and electronically digitized using 8bbp (8 bit per pixel; 2 8 = 256 gray levels). This renders some of the most successfully used (i.e. for image representation) features, like the color, inapplicable here. The texture based features combined with statistical gray-level measures proved to be a well suited global descriptor for medical images <ref type="bibr" coords="3,250.42,178.23,9.97,8.74" target="#b2">[3]</ref>.</p><p>For describing image texture we employed several very different approaches:</p><p>• (COOC) In <ref type="bibr" coords="3,162.17,212.10,10.52,8.74" target="#b4">[5]</ref> the co-occurrence matrix is used to explore the gray-level spatial dependence of the texture. We compute 4 co-occurrence matrixes, one on each direction (horizontal, vertical and diagonals), after a 64 gray-level quantification. From each matrix, 4 features are extracted: energy, entropy, contrast and homogeneity, producing a 16 feature vector.</p><p>• (DF) <ref type="bibr" coords="3,138.15,267.89,15.50,8.74" target="#b15">[16]</ref> made the assumption that textures are fractals for a certain range of magnifications.</p><p>Fractal dimension is not an integer in contrast to the dimension in Euclidean geometry, but a number between 2 and 3. The more the texture is smooth (respectively rough), the more the fractal dimension is close to 2 (respectively 3). We used a modified box-counting texture analysis based on the probability density function described by <ref type="bibr" coords="3,397.57,315.71,9.96,8.74" target="#b6">[7]</ref>. The computing of the fractal dimension generates a single feature.</p><p>• (GB) The belief that simple cells in the human visual cortex can be modelled by Gabor functions <ref type="bibr" coords="3,157.64,359.55,15.50,8.74" target="#b10">[11]</ref> lead to the development of texture features extracted from response to Gabor filters <ref type="bibr" coords="3,142.16,371.51,9.97,8.74" target="#b8">[9]</ref>. The aim is to discriminate coarse textures which have spectral energy concentrated at low spatial frequency, from fine textures which have larger concentrations at high spatial frequency. The Gabor filters computed at λ = 3 scales and φ = 4 orientations, produce a 12 level decomposition from which we extract the mean and standard deviation, resulting a 24 feature vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• (DCT)</head><p>The discrete cosine transform is popular in image coding due to good performance and fast implementation <ref type="bibr" coords="3,226.62,451.21,14.62,8.74" target="#b16">[17]</ref>. <ref type="bibr" coords="3,251.39,451.21,15.49,8.74" target="#b12">[13,</ref><ref type="bibr" coords="3,270.90,451.21,12.73,8.74" target="#b13">14,</ref><ref type="bibr" coords="3,287.64,451.21,12.73,8.74" target="#b14">15]</ref> suggest using a 3 × 3 DCT for texture feature extraction. They furthermore suggest excluding the low-frequency component of the DCT, thus yielding 8 features.</p><p>• (RL) Galloway has proposed a run-length-based technique, which calculates characteristic textural features from gray-level run lengths in different image directions <ref type="bibr" coords="3,437.06,507.00,9.96,8.74" target="#b3">[4]</ref>. A total of 14 features is derived.</p><p>• (Laws) Laws has suggested a set convolution masks for feature extraction <ref type="bibr" coords="3,450.07,538.88,9.97,8.74" target="#b7">[8]</ref>. Using the Laws filter masks for textural energy 28 features are resulted.</p><p>• (MSAR) <ref type="bibr" coords="3,150.31,570.76,15.50,8.74" target="#b9">[10]</ref> proposed the classification of color textures using Multispectral Simultaneous Autoregressive Model (MSAR). The basic idea of a simultaneous autoregressive(SAR) model is to express a gray level of a pixel as a function of the gray levels in its neighborhood. The related model parameters for one image are calculated using a least squares technique and are used as textural features. This approach is similar to the Markov random fields described in <ref type="bibr" coords="3,126.53,630.53,9.96,8.74" target="#b5">[6]</ref>. From this 24 features are resulting.</p><p>In addition we used features derived from gray-level statistical measures (STAT): different estimations of the first order (mean, median and mode), second order (variance and l2 norm), third and forth order (skewness and kurtosis) moments, thus obtaining a 7 feature vector.</p><p>We choose to combine these descriptors because in previous experiments, using feature selection algorithms, we pointed out their complementarity <ref type="bibr" coords="3,310.42,700.27,9.97,8.74" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Dimensionality Reduction</head><p>A significant obstacle in machine learning problems is learning from few data samples in a highdimensional feature space. Unfortunately, most of the time, the number of data samples is given by the context of the application and thus it is difficult to change. Furthermore, with the increase of feature space dimensionality, it becomes impossible to estimate the probability density function (PDF) with a reasonable amount of training data and (very important) computational burden.</p><p>In previous experiments <ref type="bibr" coords="4,215.24,190.19,10.52,8.74" target="#b2">[3]</ref> we used various feature selection techniques, and the best ratio between (later) classification-accuracy and dimensionality-reduction are obtained with the Principal Component Analysis (PCA). PCA is a linear transformation that transforms the data into a new coordinate system, the first new coordinate (called the first principal component) containing the projection with the greatest variance of the data (from any projection possible), the second new coordinate containing the second greatest variance and so on. The dimensionality reduction is done retaining those characteristics of the dataset that contribute most to its variance, by keeping lower-order principal components and ignoring higher-order ones. Generally, the low-order components often contain the "most important" aspects of the data.</p><p>For the ImageCLEF Annotation submissions, the experiments are conducted choosing enough eigenvectors to account for either 95% or 97% of the variance in the original data (on the training set).</p><p>We show in <ref type="bibr" coords="4,159.75,333.65,10.51,8.74" target="#b2">[3]</ref> that some of the texture features (cooccurence, fractal dimension and Gabor wavelets) as well as the statistic features are complementary, all the ten feature selection methods used, selecting subsets of each feature set. The other texture features were considered and implemented after the experiments described in <ref type="bibr" coords="4,299.15,369.51,9.97,8.74" target="#b2">[3]</ref>, but they all behaved similarly, adding a small amount of useful information to the image representation. However most of the information is redundant, imposing some form of dimensionality reduction when using several descriptors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Classification</head><p>For the projection of the test instances (i.e. the feature representation of each test image) in corresponding categories the MedIC module uses several well known supervised classification approaches based on neural networks, decision trees, support vector methods and nearest-neighbor architectures. In previous experiments <ref type="bibr" coords="4,267.49,475.57,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="4,282.46,475.57,7.75,8.74" target="#b1">2]</ref> we noted the best performances obtained by the Support Vector Machine (SVM) classifier and the good performances/classification time of the k-Nearest Neighbor approach. Given that in most application the classification task (especially the learning phase) are expected to be conducted off-line (therefore making time a less important issue), for the experiments of ImageCLEF 2006 we only submitted runs classified with SVM, as they are expected to be better.</p><p>The SVM classification is conducted using different parameters for: kernel = polynomial, radial basis function (RBF) and sigmoid, C = the cost parameter (1 → 10 4 ), γ = the gamma parameter of each kernel (10 -1 → 10 -4 ) and d = the degree of the polynomial kernel (1 → 5). From these, using the 1000 images validation dataset, the best parameters are selected: RBF kernel, γ = 10 -2 , C=10 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Results</head><p>Twelve <ref type="bibr" coords="4,124.08,641.40,17.71,8.74" target="#b11">(12)</ref> groups participated to the Image Annotation Task of ImageCLEF 2006 and submitted a total of 27 runs. The MedIC/CISMeF group submitted four runs. The parameters used for each run are presented in Table <ref type="table" coords="4,234.70,665.31,3.88,8.74" target="#tab_0">1</ref>. For all the four runs a combined descriptor COOC, DF, GB, DCT, RL, Laws, MSAR, STAT is used. This represents (16+1+24+8+14+28+24+7) = 122 features for each extracting window (according to section 2. We obtained the forth rank but we situated third in the hierarchy of groups (only RWTHi6<ref type="foot" coords="5,508.53,283.70,3.97,6.12" target="#foot_1">2</ref> and UFR<ref type="foot" coords="5,126.43,295.65,3.97,6.12" target="#foot_2">3</ref> obtained better scores) and also we obtained the third score (the second and third rank having the same error rate: 16.7%). The best score was obtained by the RWTHi6 with an error rate of 16.2%. That represents an improvement of 1% compared to our best score (local+global PCA335), meaning that, compared to our 828 correctly annotated test images, 10 more images (not necessarily from those we missed) are correctly annotated.</p><p>However it is interesting to note that in all our experiments conducted on the validation set, we obtained better results with on average 4.75%, reaching up to 87.7% correctly annotated validation images (12.3% error rate) with (local+global PCA450). This indicates different difficulties for the validation and test sets, and it will be interesting to compare if different systems reacted in the same way.</p><p>Also, we note that equal test error rates are obtained for two runs: local+global PCA335 and local PCA333, with reduced representations of comparable sizes. This could indicate that adding the globally extracted features is redundant and the first information to be discarded, in the case of local+global PCA335, by a more compacting 95% PCA transform (the same information seems to be captured locally only and preserved with PCA 97%). An inspection of the list of miss-annotated images for each run could show exactly if this assumption is true. Using the same parameters, we obtain similar performances on the validation dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Medical Retrieval Task</head><p>The multilingual medical image retrieval task uses an image dataset containing 50026 images from four image collections: Casimage, MIR, PEIR, and PathoPIC. Each collection contains textual annotation and case descriptions in XML format and various languages: Casimage (Fr-En), MIR (En), PEIR (En), and PathoPIC (Ge-En).</p><p>There are 30 topics for ImageCLEFmed 2006, organized in three categories (with 10 topics for each): Visual, Textual and Mixed. The categories are defined according to the type of approach the participants are expected to use on each.</p><p>For the medical image retrieval task we submitted a single run using an approach based on bilingual (i.e. English-French) MeSH term text-retrieval and visual image similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MeSH dictionaries</head><p>The first step is the extraction of terms, from the textual annotations of the image collection. The MedIC terms are originally based on the French version of the MeSH thesaurus (Medical Subject Headings 4 ) and they are reorganized in several image-dependent categories: image modality, anatomical region, disease, technical acquisition parameters (i.e. view angle), image formats (i.e. JPEG, PPT). Each category has its own dictionary, and contains for each MeSH term declinations like inflected (plural) MeSH terms, synonyms of MeSH terms, inflected synonyms of MeSH terms, abbreviations, initials and others. We can observe an extract of the modalities dictionary, containing the ultrasound declinations (in French): echographie,echographie.N+MeSH+TR+QMesh:fs echographies,echographie.N+MeSH+TR+QMeSH:fp ECHO,echographie.N+MeSH+TR+QMeSH us,echographie.N+MeSH+TR+QMeSH ultrasonographie,echographie.N+MeSH+TR+QMeSH:fs ultrasonographies,echographie.N+MeSH+TR+QMeSH:fp</p><p>The French dictionaries were created by the CISMeF team for experiments on automatic textual indexation, of health-resources (i.e. medical documents) in French. For the experiments presented at ImageCLEF 2006 we constructed corresponding English dictionaries to be able to treat all the textual annotations (only the Casimage collection has French textual annotations). However, due to the size and complexity of this task, the English dictionaries are containing only a small part of the French terms, and all the results from non-French collections are thus seriously influenced.</p><p>Once the terms are extracted form the textual annotations, a second step is the extract the search terms from each topic. This is performed similarly as for the extraction of annotation terms. Of course that using the same incomplete English dictionaries, the extraction of search terms from non-French annotations is as well negatively influenced.</p><p>The extraction of terms is performed using the linguistic INTEX/NOOJ environment <ref type="bibr" coords="6,471.19,383.00,14.62,8.74" target="#b17">[18]</ref>. This methodology is derived from the automatic text indexing approach that CISMeF is developing <ref type="bibr" coords="6,494.74,394.96,14.61,8.74" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Visual similarity</head><p>Once the textual annotations containing all the search terms of each topic are obtained, the relevance of each retrieved image is evaluated according to the mean similarity between each retrieved image and the two (or three) query images (of each topic). The visual similarity between two images is estimated as the L2 distance between feature representations of images. We employed some of the features presented at section 2.2: COOC, RL, DCT and STAT as well as additional color features: mean color, mean saturation and color histograms. The features are extracted from 64x64 image sub-blocks of 256×256 resized images, as for the image annotation experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>A total of 10 groups participated at the multilingual medical image retrieval task of ImageCLEF 2006, with a number of 100 runs. The MedIC/CISMeF group submitted a single run, and was placed on 69-th position, with an 0,0531 Mean Average Precision (MAP). Comparatively, the best score was obtained by the Image Processing &amp; Application Lab (IPAL) <ref type="foot" coords="6,422.35,593.54,3.97,6.12" target="#foot_4">5</ref> , with 0,3095 MAP. We expected this modest score due to several unexpected problems we experienced during the preparation of our run. The most significant problem was that due to last moment technical problems we were able to treat only ∼30% of the 50026 images. Furthermore, the dictionaries we normally use for the extraction of medical terms are in French, and their translation in English (with all the derived forms: plurals, synonyms) was very limited. Knowing that ∼82% of the images have non-French annotations, we actually expected even poorer results. The treatment of less then a third of the collection and with incomplete dictionaries is shown also by the small number of answers our run proposed (for all the 30 topics), 1114, the smallest of all the runs.</p><p>The system is conceived to be automatic, but at the last moment we chosed to manually intervene at the search term extraction phase in four of the topics: 1.1, 1.5, 1.9, 3.3. Therefore we chose to declare the whole run as manual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper we present the methods we used for the ImageCLEF 2006 evaluation. We participated in the medical tasks: the automatic annotation task, where we obtained the fourth rank (also the third score and the third placed group), and the multilingual medical image retrieval task, where our run was significantly less competitive, due technical an practical problems we were not able to overcome until the last moment.</p><p>The results obtained in the annotation task shows that the approach we propose is capable of obtaining very competitive results. A comparison between the correctly annotated images of different systems could be interesting, and could indicate how to combine different architectures to further improve the classification/annotation results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,90.00,689.22,423.01,44.60"><head>Table 1 :</head><label>1</label><figDesc>2). The runs local+global PCA450 and local+global PCA450 are considering one global extraction window and 16 local windows, and thus have an original number of features equal to 122*17 = 2074. From these PCA450 uses a 97% variance PCA to select 450 features, while using 95% produces 335 features. When considering Run details only the 16 local extraction windows, the original number of features becomes 122*16 = 1952 and the PCA 97% and 95% are producing 333, respectively 150 features.</figDesc><table coords="5,99.72,109.40,403.56,94.93"><row><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell></row><row><cell>Run label</cell><cell></cell><cell></cell><cell>Parameters</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">local global</cell><cell>orig.</cell><cell>PCA</cell><cell>final</cell><cell>error</cell><cell>error</cell><cell>rank</cell></row><row><cell>local+global PCA450 local+global PCA335 local PCA333 local PCA150</cell><cell>√ √ √ √</cell><cell>√ √ × ×</cell><cell>no.feat. 2074 2074 1952 1952</cell><cell>var 97% 95% 97% 95%</cell><cell>no.feat 450 335 333 150</cell><cell cols="2">valid. 12.3% 17.9% test 12.7% 17.2% 12.6% 17.2% 15.9% 20.2%</cell><cell>7 4 5 10</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,105.24,740.66,84.27,6.99"><p>http://www.cismef.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,105.24,728.00,354.56,6.99"><p>Human Language Technology and Pattern Recognition Group of the RWTH Aachen University</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,105.24,737.50,360.44,6.99"><p>Chair of Pattern Recognition and Image Processing of the Albert-Ludwigs University of Freiburg</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,105.24,747.01,117.89,6.99"><p>http://www.nlm.nih.gov/mesh/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="6,105.24,743.61,72.93,6.99"><p>http://ipal.imag.fr/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,110.48,284.25,402.53,8.74;7,110.48,296.20,244.60,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,424.54,284.25,88.47,8.74;7,110.48,296.20,90.98,8.74">Cismef: a structured health resource guide</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Darmoni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Baudic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douyére</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Piot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,210.21,296.20,57.20,8.74">Meth Inf Med</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="35" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,316.13,402.52,8.74;7,110.48,328.08,359.55,8.74" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="7,387.27,316.13,125.74,8.74;7,110.48,328.08,102.08,8.74">Medical image categorization with medic and medgift</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Florea</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Rogozan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Geissbuhler</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Darmoni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>MIE)</publisher>
		</imprint>
	</monogr>
	<note>submitted to Medical Inforamtics Europe</note>
</biblStruct>

<biblStruct coords="7,110.48,348.01,402.51,8.74;7,110.48,359.97,402.51,8.74;7,110.48,371.92,402.52,8.74;7,110.48,383.88,402.51,8.74;7,110.48,395.83,402.52,8.74;7,110.48,407.79,266.09,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,337.18,348.01,175.82,8.74;7,110.48,359.97,273.47,8.74">Comparison of feature-selection and classification techniques for medical image modality categorization</title>
		<author>
			<persName coords=""><surname>Florea</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Rogozan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bensrhair</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Darmoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,132.44,383.88,380.56,8.74;7,110.48,395.83,398.20,8.74">IEEE International Conference on Optimization of Electrical and Electronic Equipment (OPTIM2006), Special Session on Image Processing -Technical and Medical Applications</title>
		<meeting><address><addrLine>Brasov, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">May 18-19 2006</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="161" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,427.71,402.52,8.74;7,110.48,439.67,122.57,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,185.16,427.71,184.29,8.74">Texture analysis using graylevel runlengths</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Galloway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,377.38,427.71,135.62,8.74;7,110.48,439.67,43.50,8.74">Computer, Graphics and Image Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="172" to="179" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,459.59,402.51,8.74;7,110.48,471.55,299.68,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,334.19,459.59,174.96,8.74">Texture features for image classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,110.48,471.55,218.22,8.74">IEEE Trans. Systems, Mans and Cybernetics, SMC</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="610" to="621" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,491.47,402.52,8.74;7,110.48,503.43,306.19,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,324.32,491.47,188.69,8.74;7,110.48,503.43,109.97,8.74">Texture classification using features derived from random field models</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Khotanzad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,229.58,503.43,118.26,8.74">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="43" to="50" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,523.35,402.53,8.74;7,110.48,535.31,192.87,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,308.74,523.35,204.27,8.74;7,110.48,535.31,68.72,8.74">Texture description and segmentation through fractal geometry</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Crownover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,188.29,535.31,29.46,8.74">CVGIP</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="150" to="166" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,555.23,402.51,8.74;7,110.48,567.19,123.32,8.74" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="7,165.55,555.23,129.20,8.74">Textured Image Segmentation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">I</forename><surname>Laws</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
		</imprint>
		<respStmt>
			<orgName>University of Southern Califomia School of Engineering</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="7,110.48,587.11,402.52,8.74;7,110.48,599.07,58.70,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,153.13,587.11,194.84,8.74">Image representation using 2d gabor wavelets</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,355.95,587.11,95.27,8.74">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="1996-10">october 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.47,618.99,402.52,8.74;7,110.48,630.95,402.52,8.74;7,110.48,642.90,22.70,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,207.49,618.99,305.51,8.74;7,110.48,630.95,120.62,8.74">Texture classification and segmentation using multiresolution simultaneous autoregressive models</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,240.58,630.95,84.71,8.74">Pattern Recognition</title>
		<idno type="ISSN">:0031- 3203</idno>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="173" to="188" />
			<date type="published" when="1992-02">February 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.47,662.83,402.53,8.74;7,110.48,674.78,110.79,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,166.12,662.83,276.02,8.74">Mathematical description of the response of simple cortical cells</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Marcelja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,449.54,662.83,63.46,8.74;7,110.48,674.78,15.84,8.74">J. Optical Soc. Am</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1297" to="1300" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.47,694.71,402.53,8.74;7,110.48,706.67,394.90,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,300.52,694.71,212.49,8.74;7,110.48,706.67,322.46,8.74">Automatic indexing of health resources in french with a controlled vocabulary for the cismef catalogue: a preliminary study</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nèvèol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rogozan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Darmoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,441.70,706.67,32.96,8.74">Medinfo</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.47,726.59,402.53,8.74;7,110.48,738.55,360.36,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,255.35,726.59,257.66,8.74;7,110.48,738.55,29.81,8.74">On local linear transform and gabor filter representation of texture</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">V</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,161.42,738.55,210.90,8.74">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="627" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.47,112.02,402.53,8.74;8,110.48,123.98,384.03,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,168.30,112.02,237.40,8.74">Exploiting image indexing techniques in DCT domain</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">W</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,430.42,112.02,82.58,8.74;8,110.48,123.98,265.54,8.74">APR International Workshop on Multimedia Information Analysis and Retrieval</title>
		<imprint>
			<date type="published" when="1998">juin 1998</date>
			<biblScope unit="page" from="196" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.47,143.90,402.52,8.74;8,110.48,155.86,298.37,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,372.67,143.90,140.33,8.74;8,110.48,155.86,97.61,8.74">Exploiting image indexing techniques in DCT domain</title>
		<author>
			<persName coords=""><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ting-Chuen</forename><surname>Pong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roland</forename><forename type="middle">T</forename><surname>Chin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,217.48,155.86,84.59,8.74">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1841" to="1851" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.47,175.78,402.52,8.74;8,110.48,187.74,43.73,8.74" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="8,178.58,175.78,183.31,8.74">Fractal-based descriptors of natural scenes</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,370.36,175.78,93.22,8.74">IEEE Trans on PAMI</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="661" to="674" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.47,207.66,402.53,8.74;8,110.48,219.62,372.87,8.74" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="8,375.33,207.66,137.67,8.74;8,110.48,219.62,113.95,8.74">Subband Compression of Images -Principles and Examples</title>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Tor Audun Ramstad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Ole Aase</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Husøy</forename><surname>Håkon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>ELSEVIER Science Publishers BV</publisher>
			<pubPlace>North Holland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.47,239.54,402.52,8.74;8,110.48,251.50,129.17,8.74" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><surname>Silberztein</surname></persName>
		</author>
		<title level="m" coord="8,182.24,239.54,330.75,8.74;8,110.48,251.50,29.42,8.74">Dictionnaires électroniques et analyse automatique de textes: le syst ème INTEX</title>
		<meeting><address><addrLine>Paris</addrLine></address></meeting>
		<imprint>
			<publisher>Masson</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
