<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,111.06,76.00,389.82,16.65">Two-stage SVM for Medical Image Annotation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,220.44,112.10,33.89,11.10"><forename type="first">Bo</forename><surname>Qiu</surname></persName>
							<email>qiubo@i2r.a-star.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">Institute for Infocomm Research (I</orgName>
								<address>
									<addrLine>2 R) 21 Heng Mui Keng Terrace</addrLine>
									<postCode>119613</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,262.85,112.10,81.86,11.10"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Institute for Infocomm Research (I</orgName>
								<address>
									<addrLine>2 R) 21 Heng Mui Keng Terrace</addrLine>
									<postCode>119613</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,353.75,112.10,37.95,11.10"><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<email>tian@i2r.a-star.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">Institute for Infocomm Research (I</orgName>
								<address>
									<addrLine>2 R) 21 Heng Mui Keng Terrace</addrLine>
									<postCode>119613</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,111.06,76.00,389.82,16.65">Two-stage SVM for Medical Image Annotation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">929E630C191C7F7D6AB22C207B5D0308</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing -Abstracting methods</term>
					<term>Dictionaries</term>
					<term>Indexing methods Algorithms</term>
					<term>Performance</term>
					<term>Experimentation Classification</term>
					<term>Medical images</term>
					<term>SVM</term>
					<term>Two-stage</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we proposed a two-stage medical image annotation method in order to achieve higher classification rate. Coarse and fine classifications are performed at the two stages. At the first stage, low resolution pixel maps (32x32) are used to represent the medical images while Support Vector Machines (SVM) are used to classify the resolution reduced pixel maps. Our experiment results showed that with the first stage classification 78.2% of classification accuracy has been achieved for the development dataset (DEV). At the second stage, selected images for which SVM classifiers are too close (judged by a predefined threshold value) are further refined in order to improve classification accuracy. In our experiments on DEV we have found there were about 200 images to be re-classified. Moreover, to eliminate the influence of the great volume unbalance among classes, a new training dataset is formed out of the old one. In this new balanced training dataset, each class has 30 samples at the most and the smallest class has 10 samples. We have further designed the following three classification steps: 1) 20x50 low resolution pixel maps feed into SVMs; 2) SIFT features feed into Euclidean distance classifiers; 3) 16x16 low resolution pixel maps feed into PCA classifiers. In the sequential steps, those classified classes which have little error are recorded (proven by experiments on DEV) and their results are taken as the last classification results instead of the initial results coming from the first SVM process. Finally the results from the SVMs at the first stage, and the refined classification results at the second stage are combined to form the final classification result. Our experimental results showed with the two stage method significant improvement of classification accuracy has been achieved.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Medical image annotation has great potential commercial value. However by now, the relevant techniques still lingers in a basic research stage. To boost the development of it, after the establishment of ImageCLEF, a sub-branch of CLEF (Cross Language Evaluation Forum) workshop (http://www.clef-campaign.org/), Human Language Technology and Pattern Recognition Group of the RWTH Aachen University helped to design a task of medical image annotation.</p><p>It's the second year that ImageCLEF publishes the medical image annotation task. Still at a starting stage, the annotation task is simplified to a classification problem: purely using visual features to classify some unlabeled images into some defined classes which are given by doctors. The total process is demanded to be automatic.</p><p>In ImageCLEF2006 there are 116 classes to be classified (Figure <ref type="figure" coords="1,352.16,621.28,3.44,7.85">1</ref>), which is totally different from 57 classes in ImageCLEF2005. Another difference is that there is an extra DEV provided which includes 1000 labeled images. The common parts of 2005 and 2006 are: 9000 labeled training images, 1000 testing images. And the evaluation method is simple: to minimize the number of misclassified images in the 1000 testing data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1. 116 classes of the task</head><p>In general each solution for the classification problem will include how to choose features and how to construct the corresponding classifiers. By now, there are many features and classifiers having been explored. Wherever, how to find the most efficient combination of the features and the classifier are the key factors to the solution. Features are described in Section 2; classification method is described in Section 3; and our result in joining the ImageCLEF2006 is shown in Section 4; at last the conclusion and future work is shown in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">IMAGE FEATURES</head><p>According to <ref type="bibr" coords="2,111.16,660.69,11.26,8.74" target="#b0">[1]</ref>[2] <ref type="bibr" coords="2,133.67,660.69,11.26,8.74" target="#b2">[3]</ref>, the most frequently used image features are categorized into color, texture, and shape. In our past work <ref type="bibr" coords="2,77.35,672.39,10.66,8.74" target="#b3">[4]</ref>, we tested Low Resolution Pixel-Map (LRPM) which can be regarded as a kind of color feature; besides of LRPM, we also explored contrast, anisotropy, polarity, which are texture features, and Blob, a mid-level feature compared with those frequently used low-level features. All the features are shown in Figure <ref type="figure" coords="2,339.86,696.46,3.49,7.85" target="#fig_1">2</ref>.  In this year, we tested some new features (as shown in Figure <ref type="figure" coords="3,328.46,183.70,3.91,7.85" target="#fig_3">3</ref>) like salient map, salient point, SIFT (Scale Invariant Feature Transform), and stripe. Salient map is an explicit two-dimensional map that encodes the saliency or conspicuity of objects in the visual environment <ref type="bibr" coords="3,107.16,224.13,11.09,8.74" target="#b4">[5]</ref> <ref type="bibr" coords="3,118.25,224.13,11.09,8.74" target="#b5">[6]</ref>. "The purpose of the saliency map is to represent the conspicuity-or "saliency"-at every location in the visual field by a scalar quantity and to guide the selection of attended locations, based on the spatial distribution of saliency." The last map is 16x16 and forms a 256 feature vector with the gray values.</p><p>Instead of global features like LRPM and salient map, in <ref type="bibr" coords="3,311.41,265.23,11.72,8.74" target="#b6">[7]</ref> and <ref type="bibr" coords="3,345.40,265.23,10.64,8.74" target="#b7">[8]</ref>, the term 'salient point' is introduced, which is different from 'interest point' calculated by 'corner detectors' often although both of them are based on local computation. Salient points are related to any visual interesting part of the image whether it is smoothed or corner-like, while they are extracted under multi-resolution representation from wavelets. According to <ref type="bibr" coords="3,381.54,300.33,10.66,8.74" target="#b8">[9]</ref>, choose the 50 top salient points and generate the image patches around the points. Our patches are 13x13 and gray values of those patches are formed into the feature vectors.</p><p>SIFT was introduced by David G. Lowe <ref type="bibr" coords="3,232.27,341.43,15.36,8.74" target="#b9">[10]</ref>, and there are 4 major stages of computation used to generate SIFT features: Scale-space extrema detection, keypoint localization, orientation assignment, keypoint descriptor. SIFT transforms image data into scale-invariant coordinates relative to local features. For each keypoint, 4x4 descriptors are computed from a 16x16 sample array, and for each descriptor, there are 8 orientation bins. Thus a 128 element feature vector is formed.</p><p>Stripe was developed in our recent work <ref type="bibr" coords="3,235.21,394.23,15.37,8.74" target="#b10">[11]</ref>. By dividing an image into different kinds of grids, we can extract features based on the cells of the grids. To make it easily handled, histogram of each cell is calculated and we call the cells 'stripes'. For one image, all its stripes' histogram queues in order and forms a feature vector.  Facing so many features, we can not judge which one or which combination is the best for the solution of the task. They only can be chosen by the experiments. Besides of how to choose image features, another important factor influencing the classification results is how to design the classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CLASSIFICATION METHODOLOGY</head><p>According to <ref type="bibr" coords="3,110.18,643.41,15.36,8.74" target="#b11">[12]</ref>, there have been many classifiers developed. And they can be basically divided into two categories: rulebased classifiers and computational intelligence based classifiers. The former one needs rules defined by designers, while the latter is just a framework made by designers and integrated with a learning algorithm, which includes supervised and unsupervised methods. Unsupervised methods are also called clustering. Supervised methods include Boosting, Decision trees, Neural Networks, Bayesian Networks, SVM, Hidden Markov Field, etc. <ref type="foot" coords="4,367.86,85.16,3.00,5.23" target="#foot_0">1</ref>In our system, SVM is chosen as the classifier. Different from other methods, we use a two stage SVM in the system. In the following, we firstly introduce a basic system using SVM. Then the two stage SVM is described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A Basic Classification System Using SVM</head><p>As shown in Figure <ref type="figure" coords="4,133.64,148.36,3.49,7.85" target="#fig_4">4</ref>, SVM is a kind of supervised method. So the learning process is needed, in which the main target is to tune the parameters. After the parameters are fixed by the cross validation part, the unlabeled images can be input and processed by the classification system. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Two-stage SVM</head><p>The basic classification system using SVM can not reach a high accuracy if the image features are not well extracted. To improve the accuracy of the basic system, we put forward the two-stage SVM. The main idea is to find some rules to postprocess the basic SVM results. So the two-stage SVM can be described as the following steps: 1) Execute the basic SVM classification process; 2) Find some badly classified classes and refine the results coming from step 1).</p><p>In the 2 nd process, how to find the badly classified classes is based on the cross validation part. Owing to the seriously unbalanced problem (the numbers of the 116 classes are not equal but having great difference, the biggest class has more than 2500 samples while the least one has less than 10 samples), most of the wrongly classified samples focus on several classes. Those classes are proved by cross validation part that they include more than 95% wrongly classified samples. Targeting on refining the results of those classes, the 2 nd step is presented in the following paragraph.</p><p>For one image in those chosen 'bad' classes, note that a corresponding distance vector from the basic system is D</p><formula xml:id="formula_0" coords="4,54.00,614.91,503.99,44.24">= (d 1 , d 2 , d 3 , …, d n ). Inside the n elements, the maximal 3 values are d k1 , d k2 , d k3 . Then if 1 2 ( )<label>10000</label></formula><formula xml:id="formula_1" coords="4,263.70,644.91,294.28,14.24">k k d d th 1 - × ≤ ,<label>( 1 )</label></formula><p>the image should be considered to be reclassified. th 1 is a threshold which is decided by the cross validation part. However, for all the images chosen by this way, there are around 1/3 of them are correctly classified, which has been proved by the cross validation experiment. This means the 1/3 needn't to be reclassified. How to identify those images? We find that if the image distance vector satisfies the following condition</p><formula xml:id="formula_2" coords="5,259.92,116.55,298.06,14.24">2 3 ( ) 10000 k k d d th 2 - × &gt; ,<label>( 2 )</label></formula><p>the image will be regarded as a 'good' image and needn't be reclassified. th 2 is also a threshold decided by the cross validation part.</p><p>In a short summary, to find the 'bad' images in the 2 nd step of our two-stage SVM, firstly cross validation helps find the 'bad' classes; secondly the two formulas help judge whether the images should be reclassified or not.</p><p>While keeping the classified results of all the 'good' images, we are going to reclassify the chosen 'bad' images. This refining process includes the following steps:</p><formula xml:id="formula_3" coords="5,68.22,228.15,8.38,8.74">1)</formula><p>Generating a new training dataset out of the old one. This is to eliminate the influence of the great volume unbalance among classes. In this new balanced training dataset, each class has 30 samples at the most and the smallest class has 10 samples.</p><p>2) Feeding 20x50 LRPM into SVM classifiers;</p><p>3) Feeding SIFT features into Euclidean distance classifiers;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4)</head><p>Feeding 16x16 LRPM into PCA classifiers <ref type="bibr" coords="5,267.97,304.65,10.84,8.74" target="#b3">[4]</ref>;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5)</head><p>Choosing the best classifier and features for each class using cross validation;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6)</head><p>Classifying the 'bad' images using the classifiers and features fixed by step 5).</p><p>At last, the results of the reclassified 'bad' images are combined together with those of the 'good' images coming from the first SVM process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">OUR RESULT FOR IMAGECLEF2006 MEDICAL ANNOTATION</head><p>In the case of fulfilling the task of ImageCLEF2006, we use DEV to make the cross validation to train the classifiers and choose image features. Then the 1000 true images are tested based on the trained classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Cross validation</head><p>As shown in Figure <ref type="figure" coords="5,133.16,457.30,3.49,7.85">5</ref>, when using 32x32 LRPM and SVM classifiers, our system reached an average accuracy of 78.2%. For all of the other features as mentioned in Section 2, we can not get a higher accuracy. So only this result is presented.</p><p>In the figure, the first part is 'correctness-class', which means the vertical axis presents the correctness/accuracy, and the horizontal axis presents the 116 classes. From this part we can see which classes have reached a satisfied accuracy. The second part shows the number of wrongly classified samples for each class. In this part we can find which class is the most difficult class. The difficult classes absorbed too many samples which don't belong to them. The third part shows the numbers of the classified images for each class and the fourth part shows the true numbers of images in each class.</p><p>In the whole process of cross validation, the SVM parameters are set and the two thresholds in formula ( <ref type="formula" coords="5,501.78,550.53,3.90,8.74" target="#formula_1">1</ref>) and ( <ref type="formula" coords="5,534.62,550.53,3.90,8.74" target="#formula_2">2</ref>) are chosen: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">True test</head><p>In the first stage of SVM, 32x32 LRPM is used, the same as used in cross validation. The classification result is shown in Figure <ref type="figure" coords="5,79.82,654.34,3.49,7.85">6</ref>. The average accuracy accounting for all the 116 classes is 70.1%, which is calculated based on the published ground truth from the ImageCLEF2006med.</p><p>In the second stage of SVM, we used the features and classifiers as mentioned in Section 3.2. Combined with the results from the first stage, the final result is shown in Figure 7. The average accuracy over the 116 classes is 72%.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION AND FUTURE WORK</head><p>Though we got a common result in DEV, the true results with the 72% accuracy is too far away from expected. The main reason for this may be owing to the bad choice of features. Global features are not good enough to be applied to solve the complex classification problem of medical images. More precise local features have to be considered.</p><p>In the future work, we will focus on more efficient features and relevant classifiers. And more work on analyzing the 116 classes should be done because many classes can not be identified even with our eyes. Where is the key part/feature to distinguish them? It may be a long-term question.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,238.44,168.03,135.12,8.10"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Features used in last year</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="3,235.68,567.27,140.63,8.10;3,224.34,432.48,92.26,108.66"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Features tested in this year</figDesc><graphic coords="3,224.34,432.48,92.26,108.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="4,233.34,374.31,145.32,8.10;4,68.22,389.31,489.76,8.74;4,54.00,401.01,503.99,8.74;4,54.00,412.71,446.55,8.74;4,68.22,430.41,489.82,8.74;4,54.00,442.11,437.35,8.74"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. A basic classification systemSVM classifier can calculate similarity values between each input image and each class. Then for each input image, there will be a distance vector whose elements represent the distance values between the image and each class boundary. The smaller it is, the more difficult to be classified. So the largest value puts the image into the corresponding class.Cross validation part uses some training date to tune the parameters. When using radial basis function (RBF), SVM parameters include the standard variance (σ), and the trade-off between training error and the margin (C)<ref type="bibr" coords="4,477.17,442.11,10.64,8.74" target="#b3">[4]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="6,178.50,377.61,254.99,8.10"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Figure 5. Result of cross validation using DEV (accuracy = 78.2%)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="7,160.14,377.82,291.75,9.02"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Result after refining with two-stage SVM (accuracy = 72%)</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,61.20,707.92,191.12,7.85"><p>http://en.wikipedia.org/wiki/Statistical_classification</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,75.00,509.98,483.04,7.85;7,75.00,521.98,149.55,7.85" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="7,214.66,509.98,188.48,7.85">Content-Based Image Retrieval Systems: A Survey</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Remco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mirela</forename><surname>Veltkamp</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Tanase</surname></persName>
		</author>
		<idno>UU-CS-2000-34</idno>
		<ptr target="http://give-lab.cs.uu.nl/cbirsurvey/" />
		<imprint>
			<date type="published" when="2000-10">Oct. 2000</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="7,75.00,533.73,483.03,8.10;7,75.00,545.73,186.20,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,141.30,533.98,327.80,7.85">Automatic Categorization of Medical Images for Content-based Retrieval and Data Mining</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,475.56,533.73,82.47,8.10;7,75.00,545.73,79.82,8.10">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="143" to="155" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.00,557.98,483.00,7.85;7,75.00,569.98,383.12,7.85" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="7,144.24,557.98,224.51,7.85">A Survey on: Contents Based Search in Image Databases</title>
		<author>
			<persName coords=""><forename type="first">Björn</forename><surname>Johansson</surname></persName>
		</author>
		<idno>LiTH-ISY-R-2215</idno>
		<imprint>
			<date type="published" when="2000-02">Feb., 2000</date>
			<pubPlace>Sweden</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Technical Reports from the Computer Vision Laboratory, Dept. of Electrical Engineering, Linköping University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.00,581.98,483.06,7.85;7,75.00,593.98,253.44,7.85" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,204.43,581.98,235.70,7.85">An Automatic Classification System Applied in Medical Images</title>
		<author>
			<persName coords=""><forename type="first">Bo</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,447.12,581.98,110.95,7.85;7,75.00,593.98,49.82,7.85">IEEE Intl Conf Multimedia &amp; Expo (ICME)</title>
		<meeting><address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07">2006. Jul. 9-12, 2006</date>
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.00,605.73,483.01,8.10;7,75.00,617.73,285.67,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,178.32,605.97,261.53,7.85">A Model of Saliency-Based Visual Attention for Rapid Scene Analysis</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,446.58,605.73,111.43,8.10;7,75.00,617.73,123.90,8.10">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998-11">Nov 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.00,629.73,483.02,8.10;7,75.00,641.97,109.91,7.85" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,135.19,629.97,293.85,7.85">A saliency-based search mechanism for overt and covert shifts of visual attention</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,434.82,629.73,57.25,8.10">Vision Research</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10-12</biblScope>
			<biblScope unit="page" from="1489" to="1506" />
			<date type="published" when="2000-05">May 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.00,653.73,483.00,8.10;7,75.00,665.97,160.17,7.85" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,271.03,653.97,140.13,7.85">Evaluation of Salient Point Techniques</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Loupias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,417.60,653.73,104.28,8.10">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">13-14</biblScope>
			<biblScope unit="page" from="1087" to="1095" />
			<date type="published" when="2003-12">December, 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.00,677.73,483.00,8.10;7,75.00,689.73,177.23,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,236.20,677.97,184.16,7.85">Wavelet-Based Salient Points for Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Loupias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bres</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J-M</forename><surname>Jolion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,427.26,677.73,130.74,8.10;7,75.00,689.73,75.21,8.10">International Conference on Image Processing (ICIP&apos;00)</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.00,701.31,483.01,8.74;8,75.00,74.76,294.02,9.02" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,211.34,701.31,342.53,8.74">Improving a Discriminative Approach to Object Recognition using Image Patches</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,75.00,74.76,74.18,9.02">Proc. DAGM 2005</title>
		<meeting>DAGM 2005<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="volume">3663</biblScope>
			<biblScope unit="page" from="326" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,75.00,87.45,482.99,8.10;8,75.00,99.70,86.27,7.85" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,142.67,87.70,212.13,7.85">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,365.76,87.45,154.50,8.10">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,75.00,111.70,482.95,7.85;8,75.00,123.69,249.62,7.85" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,293.72,111.70,264.23,7.85;8,75.00,123.69,133.38,7.85">Stripe: Image Feature Based on a New Grid Method and Its Application in ImageCLEF, will appeart in AIRS</title>
		<author>
			<persName coords=""><forename type="first">Bo</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Racoceanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chang</forename><forename type="middle">Sheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,233.22,123.69,23.02,7.85">LNCS</title>
		<imprint>
			<biblScope unit="volume">4182</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,75.00,135.69,482.97,7.85;8,75.00,147.69,266.65,7.85" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="8,208.74,135.69,245.41,7.85">Multi Classifier Systems -A Review and Roadmap for Developers</title>
		<author>
			<persName coords=""><forename type="first">Romesh</forename><surname>Ranawana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Palade</forename><surname>Vasile</surname></persName>
		</author>
		<imprint>
			<publisher>IOS Press</publisher>
			<pubPlace>Amsterdam</pubPlace>
		</imprint>
	</monogr>
	<note>to appear in March 2006, Issue of the Journal of Hybrid Intelligent Systems</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
