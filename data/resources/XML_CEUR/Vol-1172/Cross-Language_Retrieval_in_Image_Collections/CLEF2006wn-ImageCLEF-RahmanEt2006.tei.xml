<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,98.76,98.63,405.52,15.61;1,98.76,120.59,405.79,15.61;1,170.88,142.43,261.65,15.61">CINDI at ImageCLEF 2006: Image Retrieval &amp; Annotation Tasks for the General Photographic and Medical Image Collections</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,156.00,176.03,64.34,9.96"><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science &amp; Software Engineering</orgName>
								<orgName type="institution">Concordia University</orgName>
								<address>
									<addrLine>1455 de Maisonneuve Blvd</addrLine>
									<postCode>H3G 1M8</postCode>
									<settlement>Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,229.22,176.03,49.51,9.96"><forename type="first">Varun</forename><surname>Sood</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science &amp; Software Engineering</orgName>
								<orgName type="institution">Concordia University</orgName>
								<address>
									<addrLine>1455 de Maisonneuve Blvd</addrLine>
									<postCode>H3G 1M8</postCode>
									<settlement>Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,286.84,176.03,62.43,9.96"><forename type="first">Bipin</forename><forename type="middle">C</forename><surname>Desai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science &amp; Software Engineering</orgName>
								<orgName type="institution">Concordia University</orgName>
								<address>
									<addrLine>1455 de Maisonneuve Blvd</addrLine>
									<postCode>H3G 1M8</postCode>
									<settlement>Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,356.92,176.03,90.25,9.96"><forename type="first">Prabir</forename><surname>Bhattacharya</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science &amp; Software Engineering</orgName>
								<orgName type="institution">Concordia University</orgName>
								<address>
									<addrLine>1455 de Maisonneuve Blvd</addrLine>
									<postCode>H3G 1M8</postCode>
									<settlement>Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,98.76,98.63,405.52,15.61;1,98.76,120.59,405.79,15.61;1,170.88,142.43,261.65,15.61">CINDI at ImageCLEF 2006: Image Retrieval &amp; Annotation Tasks for the General Photographic and Medical Image Collections</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">91752ABCCDAA8F3E8F5B13D53CE8A22A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.7 Digital Libraries</term>
					<term>I.4.8 [Image Processing and Computer Vision]: Scene Analysis-Object Recognition Algorithms, Machine learning, Performance, Experimentation Content-based image retrieval, Vector space model, Feature extraction, Query expansion, Relevance feedback, Classification, Support vector machine</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents our techniques used and their analysis for the runs made and the results submitted by the CINDI group for the task of the image retrieval and automatic annotation of ImageCLEF 2006. For the ah-hoc image retrieval from both the photographic and medical image collections, we have experimented with cross-modal (image and text) interaction and integration approaches based on the relevance feedback in the form of textual query expansion and visual query point movement with adaptive similarity matching functions. Experimental results show that our approaches performed well compared to initial visual or textual only retrieval without any user interactions or feedbacks. We are ranked first and second and achieved the highest MAP score (0.3850) for the ad-hoc retrieval in the photographic collection (IAPR) among all the submissions. For the automatic annotation tasks for both the medical (IRMA) and object collections (LTU), we have experimented with a classifier combination approach, where several probabilistic multi-class SVM classifiers with features at different levels as inputs are fused with several combination rules to predict the final probability score of each category as image annotation. Analysis of the results of the different runs we made for both the image retrieval and annotation tasks are reported in this paper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For the 2006 ImageCLEF workshop, CINDI research group has participated in four different tasks of ImageCLEF track: an ad-hoc retrieval from a photographic collection, ad-hoc retrieval from a medical collection, and automatic annotation of the medical and the object data sets <ref type="bibr" coords="2,464.64,107.15,10.57,9.96" target="#b0">[1,</ref><ref type="bibr" coords="2,478.56,107.15,6.97,9.96" target="#b1">2]</ref>. This paper presents the methodologies, results and analysis of the runs of each of the tasks separately.</p><p>2 Ad-hoc retrieval from photographic collection Our main goal of the ad-hoc retrieval task is to investigate the effectiveness of combining text and image by involving user in the retrieval loop in the form of relevance feedback. We have experimented with a cross-modal approach of image retrieval which integrates visual information based on purely low-level image content and semantical information from the associated annotated text files. The advantages of both modalities are exploited by involving the users in the retrieval loop for the cross-modal interaction and integration in similarity matching.</p><p>For the text-based retrieval, the keywords from the annotated files are extracted and indexed with the help of the vector space model paradigm <ref type="bibr" coords="2,307.57,257.63,10.00,9.96" target="#b2">[3]</ref>. In order to perform the query expansion on the textual search, additional keywords are extracted for the query based on the positive feedbacks from the user. For the content-based search, a query point movement and an adjustment of the similarity matching functions are performed based on the estimation of the mean and covariance matrix from the feature vectors of the positive feedback images. Finally, a ranked-based ordered list of images is obtained by a pre-filtering approach which integrats the scores from both the text and image search-based result lists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Text retrieval approach</head><p>For the keyword-based search on the annotated text files, we have utilized a simple but effective information retrieval (IR) tool by Raymond Mooney at the Texas University <ref type="bibr" coords="2,419.64,387.59,9.91,9.96" target="#b3">[4]</ref>. However, we have performed several modifications to the original library according to the experimental requirements, such as allowing recursive indexation of the text files that are stored in directories, expanding the stop word list by adding several common words specific to the experimental domain and modifying the term weighting scheme with the query expansion. For the text-based indexing, keywords are extracted from all the associated annotation files by ignoring all the tags as stop words.</p><p>The indexing technique is based on the popular vector space model (VSM) of IR <ref type="bibr" coords="2,465.49,459.23,9.91,9.96" target="#b2">[3]</ref>. In this model, texts and queries are represented as vectors in a N -dimensional space, where N is the number of keywords in the collection. So, each document j can be represented as a vector as:</p><formula xml:id="formula_0" coords="2,250.44,505.07,262.60,10.65">D j =&lt; w 1j , • • • , w N j &gt;<label>(1)</label></formula><p>The element w ij represents the weights of the keyword w i appearing in document j and can be weighted in a variety of ways. One common scheme is term-frequency-inverse document frequency (TF-IDF) weighting. Both global weight and local weight are considered in this approach <ref type="bibr" coords="2,487.22,550.91,9.91,9.96" target="#b2">[3]</ref>. A global weight indicates the overall importance of that component in the feature vector across the whole image collection. A local weight is applied to each element indicating the relative importance of the component within its vector. The local weight is denoted as L i,j = log(f i,j ) + 1, where f i,j is the frequency of occurrence of keyword w i in document j. The global weight is the inverse document frequency and denoted by G i where</p><formula xml:id="formula_1" coords="2,298.20,610.67,182.10,10.33">G i = log(M/M i ) + 1, for i = (1, • • • , , N )</formula><p>, where M i be the number of documents in which w i is found and M is the total number of documents in the collection. Finally, the element w ij is expressed as the product of local and global weight:</p><formula xml:id="formula_2" coords="2,90.00,646.55,108.37,10.33">hence w ij = L i,j * G i [3].</formula><p>The vector space model is based on the assumption that similar documents will be represented by similar vectors in the N -dimensional vector space. In particular, similar documents are expected to have small angles between their corresponding vectors. Hence, the cosine similarity measure is adopted between feature vectors of the query document q and database document j as follows <ref type="bibr" coords="3,499.67,61.43,10.00,9.96" target="#b2">[3]</ref>:</p><formula xml:id="formula_3" coords="3,171.96,81.76,341.08,33.32">S text (q, j) = S text (D q , D j ) = N i=1 w iq * w ij N i=1 (w iq ) 2 * N i=1 (w ij ) 2 (2)</formula><p>where, D q and D j are the query and document vector respectively. The advantage of the VSM includes a ranked result of the retrieved documents (as well as the associated images) which would be useful when we fuse the results from both the keyword and content-based image retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Content-based image retrieval approach</head><p>The performance of a Content-based image retrieval approach(CBIR) system depends on the underlying image representation, usually in the form of a feature vector <ref type="bibr" coords="3,400.32,207.83,10.00,9.96" target="#b4">[5]</ref>. Based on the previous experiments <ref type="bibr" coords="3,147.12,219.71,10.00,9.96" target="#b5">[6]</ref>, we have found that the image features at different levels are complementary in nature and together they could contribute to effectively distinguish the images of different semantic categories. Hence, to generate the feature vectors, we have extracted the low-level global, semi-global and region specific local features for the image representation at different levels of abstraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Feature extraction and similarity matching</head><p>In this work, the MPEG-7 based Edge Histogram Descriptor (EHD) and Color Layout Descriptor (CLD) are extracted for image representation at the global level <ref type="bibr" coords="3,368.51,323.75,10.00,9.96" target="#b6">[7]</ref>. To represent the global shape feature, the spatial distribution of edges are utilized by the EHD descriptor. A histogram with 16 × 5 = 80 bins is obtained, corresponding to a feature vector f EHD , having a dimension of 80 <ref type="bibr" coords="3,499.70,347.63,10.00,9.96" target="#b6">[7]</ref>. The CLD represents the spatial layout of the images in a very compact form <ref type="bibr" coords="3,434.39,359.63,10.00,9.96" target="#b6">[7]</ref>. It is obtained by applying the discrete cosine transform (DCT) on the 2-D array of local representative colors in YCbCr color space. In this work, CLD with 10 Y , 3 Cb and 3 Cr coefficients is extracted to form a 16-dimensional feature vector f CLD . Now, for comparing the query image Q and the target image T in the database based on the global features, a weighted Euclidean distance measure is utilized as</p><formula xml:id="formula_4" coords="3,178.20,440.75,334.83,10.65">DIS global (Q, T ) = ω CLD D CLD (Q, T ) + ω EHD D EHD (Q, T ),<label>(3)</label></formula><p>where</p><formula xml:id="formula_5" coords="3,113.60,460.87,306.27,13.33">, D CLD (Q, T ) = ||f CLD Q -f CLD T || 2 and D EHD (Q, T ) = ||f EHD Q -f EHD T</formula><p>|| 2 are the Euclidean distance measures for CLD and EHD feature vector respectively and ω CLD and ω EHD are weights for each feature distance measure subject to ω CLD + ω EHD = 1 and adjusted as ω CLD = 0.4 and ω EHD = 0.6 in the experiment.</p><p>For semi-global feature vector, a simple grid-based approach is used to divide the images into five overlapping sub-images <ref type="bibr" coords="3,214.69,521.99,9.91,9.96" target="#b5">[6]</ref>. Several moment based color and texture features are extracted from each of the sub-images and later they are combined to form a semi-global feature vector. For moment-based color feature, the first (mean) and second (standard deviation) central moments of each color channel in HSV color space are extracted. Texture features are extracted from the grey level co-occurrence matrix (GLCM) <ref type="bibr" coords="3,297.47,569.87,10.00,9.96" target="#b7">[8]</ref>. GLCM is defined as a sample of the joint probability density of the gray levels of two pixels separated by a given displacement. Second order moments, such as energy, maximum probability, entropy, contrast and inverse difference moment are measured based on the GLCM. Color and texture feature vectors are normalized and combined to form a joint feature vector of 11-dimensions (6 for color and 5 for texture) for each sub-region to finally generate a 55-dimensional (5 × 11) semi-global feature vector f SG . For the semi-global distance measure between Q and T , we also utilized the Euclidean distance measure as</p><formula xml:id="formula_6" coords="3,223.56,663.55,289.48,13.57">DIS semi-global (Q, T ) = ||f SG Q -f SG T || 2<label>(4)</label></formula><p>We have also considered a local region specific feature extraction approach by fragmenting an image automatically into a set of homogeneous regions based on a fast k-means clustering technique. To represent each region with local features, we consider information on weight (i.e, number of pixels) and color-texture as in <ref type="bibr" coords="4,192.49,73.43,10.00,9.96" target="#b5">[6]</ref>. Color feature f c Ri of each region R i is a 3-D vector and is represented by the K-means cluster center, i.e., the average value for each of the three color channels in HSV space of all the image pixels in this region. Texture feature of each region is measured in an indirect way by considering the cross-correlation among color channels due to the off diagonal of the 3 × 3 covariance matrix of the region R i .</p><p>To compute the region specific distance measure between two regions R i and R j of Q and T respectively, we apply the Bhattacharyya distance metric <ref type="bibr" coords="4,342.73,145.19,10.56,9.96" target="#b8">[9]</ref> as follows:</p><formula xml:id="formula_7" coords="4,197.64,168.62,315.40,82.98">D(R i , R j ) = 1 8 (f c Ri -f c Rj ) T (C QR i + C TR j ) 2 -1 (f c Ri -f c Rj ) + 1 2 ln (CQ R i +CT R j ) 2 |C QR i ||C TR j |<label>(5)</label></formula><p>where f c Ri and f c Rj are the region feature vectors, and C QR i and C TR i are the covariance matrices of region R i and R j of query image Q and target image T respectively. Finally, the image-level distance between Q and T is measured as</p><formula xml:id="formula_8" coords="4,182.40,304.36,330.64,28.27">DIS local (Q, T ) = M i=1 w QR i R i (T ) + N j=1 w TR j R j (Q) 2<label>(6)</label></formula><p>where w QR i and w TR j are the weights for region i of image Q and region j of image T respectively. For each region i ∈ M in Q, R i (T ) is defined as the minimum distance between this region and any region j ∈ N in image T and in a similar way R j (Q) is computed. The overall image level similarity is measured by fusing of a weighted combination of individual similarity measures. Once the distance functions are measured as above, they are normalized and converted to similarity measure, which in general is the converse of a distance function. After the similarity measures of each representation are determined as S global (Q, T ), S semi-global (Q, T ), and S local (Q, T ), we aggregate or fuse them into a single similarity matching function as follows:</p><formula xml:id="formula_9" coords="4,149.76,447.59,363.28,10.65">S image (Q, T ) = w g S global (Q, T ) + w sg S semi-global (Q, T ) + w l S local (Q, T )<label>(7)</label></formula><p>Here, w g , w sg and w l are non-negative weighting factors of different feature level similarities with normalization w g + w sg + w l = 1. For the retrieval experiments, they are selected as w g = 0.4, w sg = 0.3 and w l = 0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Cross-modal interaction with relevance feedback</head><p>In a practical multi-modal image retrieval system, the user at first might want to search images with keywords as it is more convenient and semantically more appropriate. However, a short query (e.g., query topic) with few keywords might not be enough to incorporate the user perceived semantics to the retrieval system. Hence, a query expansion process is required to add additional keywords and modify the weight of the keywords in the original query vector. In this paper, a simpler approach of query expansion is considered based on identifying useful terms or keywords from the associated annotation files for the images. The approach of the textual query expansion based on the relevance feedback (RF) is as follows: the user provides the initial query topic and the system extracts from it a set of keywords as the initial textual query vector D q(0) . This query vector is used to retrieve K most similar images from associated text documents based on the cosine similarity measure described in section 2.1. If the user is not satisfied with the result, then the system will allow the user to select a set of relevant or positive images close to the semantics of the initial textual query topic. Next, the system will extract all the keywords from the annotation files associated with the positive feedback images.</p><p>After extracting the additional keywords, the query vector will be adjusted as D q(i) at iteration i by re-weighting its keywords by following the TF-IDF and re-submitted to the system as the query for the next iteration. This process may continue for several iterations until the user is satisfied with the result.</p><p>However, since we have a multi-modal system, it will not be wise to perform query expansion by just using one particular modality (e.g., only text). Visual features of images also play an important part in distinguishing different semantical/visual categories. Therefore, we also need to perform RF with content-based image search for better precision <ref type="bibr" coords="5,370.80,145.19,14.70,9.96" target="#b10">[11]</ref>. In this scenario, like textual query expansion, user might provide the initial image query vector f Q(0) to retrieve K most similar images based on the similarity measure function in equation <ref type="bibr" coords="5,351.12,169.07,11.62,9.96" target="#b6">(7)</ref>. In the next iteration (either from the texual or image-based feedback), user might select a set of relevant images compared to the initial query image. It is assumed that, all the positive feedback images P os(f Q(i) ) at some particular iteration i will belong to the user perceived semantic category and obey the Gaussian distribution to form a cluster in the feature space.</p><p>Let, N Pos be the number of positive feedback images at iteration i and f Tj ∈ ℜ d is be the feature vector that represents j-th image for j ∈ {1, • • • , N Pos }, then the new query point at iteration i is estimated as f Q(i) = 1 NPos NPos j=1 f Tj as the mean vector of positive images and covariance matrix is estimated as</p><formula xml:id="formula_10" coords="5,160.56,266.80,201.40,14.73">C (i) = 1 NPos-1 NPos j=1 (f Tj -f Q(i) )(f Tj -f Q(i) )</formula><p>T . However, singularity issue will arise in covariance matrix estimation if fewer than d + 1 training samples or positive images are available as will be the case in user feedback images. So, we add regularization to avoid singularity in matrices as follows <ref type="bibr" coords="5,181.63,305.15,15.89,9.96" target="#b11">[12]</ref>:</p><formula xml:id="formula_11" coords="5,251.52,315.47,261.51,13.65">Ĉ(i) = αC (i) + (1 -α)I<label>(8)</label></formula><p>for some 0 ≤ α ≤ 1 and I is the d × d identity matrix. After generating the mean vector and covariance matrix of the positive images, we adaptly adjust the Euclidean distance measures of various feature representation with the following Mahalanobis distance measure <ref type="bibr" coords="5,211.92,370.07,10.00,9.96" target="#b8">[9]</ref>:</p><formula xml:id="formula_12" coords="5,201.48,387.59,311.56,15.09">DIS Maha (Q, T ) = (f Q(i) -f T ) T Ĉ-1 (i) (f Q(i) -f T )<label>(9)</label></formula><p>Here, f T denotes the feature vector of target database image T in general for different image representation (e.g., global and semi-global). The Mahalanobis distance differs from the Euclidean distance in that it takes into account the correlations of the data set and is scale-invariant, i.e., it is not dependent on the scale of measurements. If the covariance matrix is the identity matrix then it is the same as the Euclidean distance <ref type="bibr" coords="5,289.08,458.03,9.91,9.96" target="#b8">[9]</ref>. Basically, at each iteration of relevance feedback, we generate several mean vectors and covariance matrices for each of the representation separately and use it in the distance measures. Finally, we obtain a ranked based retrieval by applying the fusion-based similarity function of equation <ref type="bibr" coords="5,130.21,505.79,11.62,9.96" target="#b6">(7)</ref>. So, the above relevance feedback approach performs both the query point movement and similarity matching adjustment at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Integration of the textual and visual results</head><p>We have considered a pre-filtering and merging approach based on the text and image result lists obtained by the text retrieval after query expansion and image retrieval by applying adaptive distance measure (equation ( <ref type="formula" coords="5,216.31,587.51,4.15,9.96" target="#formula_12">9</ref>)) after the relevance feedback. In this multi-modal integration approach, combining the result of the text and image based retrieval is a matter of re-ranking or re-ordering of the images based on a weighted combination of scores from both the modalities. Instead of purely merging the results, we basically perform a pre-filtering step with the text query at first as it can generate results more closely to the user perceived semantics. The steps involved in the proposed interaction and integration approaches are as follows:</p><p>Step 1: Perform an initial text-based search with a query vector D q(0) for a query topic q(0) at iteration i = 0 and rank the associated images based on the ranking of the text (annotation) documents by applying S text of equation <ref type="bibr" coords="5,264.38,695.03,13.24,9.96" target="#b1">(2)</ref>.  Step 2: Consider top K = 30 most similar images from the retrieval interface and obtain user feedback about positive or relevant images (e.g., associated annotation files) for the textual query expansion.</p><p>Step 3: Resubmit the modified query vector D q(i) by re-weighting the keywords at iteration i. Continue the iterations by incrementing i, until the user is satisfied or the system converges.</p><p>Step 4: Perform visual only search on the result list of the first L = 2000 images obtained from step 3 with the initial query image Q(0).</p><p>Step 5: Obtain the user feedback of the relevant images and perform the image only search with the new query Q(i) at iteration i with equation ( <ref type="formula" coords="6,322.86,463.55,4.25,9.96" target="#formula_12">9</ref>) and equation <ref type="bibr" coords="6,393.39,463.55,11.62,9.96" target="#b6">(7)</ref>. Continue the iterations by incrementing i, until the user is satisfied or the system converges.</p><p>Step 6: Aggregate the text and image based scores by fusing the similarity measures as:</p><formula xml:id="formula_13" coords="6,206.76,508.43,306.28,10.65">S(Q, T ) = w text S text (., .) + w image S image (., .)<label>(10)</label></formula><p>where, w text = 0.7 and w image = 0.3 are selected for the experiment.</p><p>Step 7: Finally, rank the images in descending order of similarity values and return the top 1000 images.</p><p>Fig. <ref type="figure" coords="6,126.25,565.31,4.98,9.96" target="#fig_0">1</ref> shows the process flow diagram of the proposed multi-modal intercation and integration approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Analysis of the results</head><p>We have submitted three runs for the ad-hoc retrieval of the IAPR collection as shown in Table <ref type="table" coords="6,90.00,635.27,3.90,9.96" target="#tab_0">1</ref>. In all these runs, English queries and example images are used as our initial source queries. In the first run with ID "Cindi-Text-Eng", we performed only the automatic text-based search without any feedback as our base run. For the second and third runs with ID "Cindi-TXT-EXP" and "Cindi-Exp-RF" respectively, we performed manual feedback in the text only modality and in combination of the text and image modalities (with only one or two iterations for each modality) as discussed in the previous sections. From Table <ref type="table" coords="6,313.68,695.03,3.90,9.96" target="#tab_0">1</ref>, it is clear that the MAP scores are almost doubled in both the cases with the feedback and integration of text and image has achieved the best performance. In fact, these two runs ranked first and second in terms of the MAP score among the 157 submissions in the photographic retrieval task. Our group performed manual submissions using relevance judgement from the user, which along with the integration of the both modalities could be the reason for our good results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Ad-hoc retrieval from medical image collections</head><p>For the ah-hoc image retrieval task in the medical collections (e.g., CaseImage, PEIR, MIR and PathoPic datasets), we have experimented with a similar cross-modal approach performed for photographic retrieval. However, for the text-based indexing and search, we have utilized the Lucene search engine <ref type="bibr" coords="7,183.59,199.79,14.60,9.96" target="#b9">[10]</ref>, an open source project under the Apache software foundation. We have also performed a different query expansion and merging algorithm to obtain a text-based result list and finally merge with the image-based result list to obtain the final ranked result by applying similar weighting scheme as discussed in section 2.4 for photographic retrieval.</p><p>To use the textual information for image retrieval in the medical collections, each image has to be attached to at least one (possibly empty) text document. The text-based indexing is started by extracting keywords from the XML documents by parsing them using Xerces2 Java Parser, an open source project under the Apache software foundation. Every element of the XML document is indexed as a separate field in Lucene. Separate fields make it easier to search for the contents based on criteria or simply searching on all the indexed elements. Before indexing, the stop words (we have added additional domain specific stop words to the list) are removed from the description of the elements. Once the index creation process is completed, the keyword-based searching can be performed using the Lucene API.</p><p>For content-based indexing, we use the same approach as described in section 2 for the photographic collection. However, we have extracted a low-resolution scaled-specific image feature in addition to the global, semi-global and local region-specific features. Since, images in the different medical collections vary in sizes, resizing them into a thumbnail of a fixed size might reduce some noise due to the artifacts presents in the images, although it may introduce distortion. These approaches are extensively used in face or finger-print recognition and have proven to be effective. For the scaled-based feature vector f Scaled , each image is converted to a gray-level image and down scaled to 64 × 64 regardless of the original aspect ratio. Next, the down-scaled image is partitioned further with a 16 × 16 grid to form small blocks of (4 × 4) pixels. The average gray value of each block is measured and concatenated to form a 256-dimensional feature vector. By measuring the average gray value of each block, it can cope with global or local image deformations to some extent and adds robustness with respect to translations and intensity changes.</p><p>We also utilize the Euclidean distance measure to compare Q and T for f Scaled and the fusionbased similarity function is slightly adjusted due to the added scaled-specific feature as follows:</p><formula xml:id="formula_14" coords="7,106.68,531.95,406.36,10.65">S image (Q, T ) = w g S global (Q, T ) + w sc S scaled (Q, T )w sg S semi-global (Q, T ) + w l S local (Q, T )<label>(11)</label></formula><p>For the medical retrieval experiments, the weights are adjusted as w g = 0.4, w sc = 0.2, w sg = 0.25 and w l = 0.15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Query expansion and integration of the results</head><p>A search process can start either by entering some text (query topic) in the text field or by providing a query image (e.g., "query-by-example") to the system. If the user starts a keywordbased search process, then the system will search the index, find the XML documents where those keywords occur based on the similarity matching on the query and document vectors, and finally retrieve the images corresponding to the XML documents. Resulting images are then displayed as sorted in descending order of the similarity scores of the associated XML documents.</p><p>If the user starts the search process with the visual approach (i.e. "query by example"), various low-level image features will be computed on-line and the resulting images will be displayed sorted Figure <ref type="figure" coords="8,224.29,300.47,3.90,9.96">2</ref>: Query expansion and merging approach by similarity score obtained from equation <ref type="bibr" coords="8,277.23,332.39,16.43,9.96" target="#b10">(11)</ref>. After the initial search, user can make use of the relevance feedback system, which can work on both the text and image modalities simulteneously to display the results in the subsequent passes as discussed in section 2.3. After obtaining the initial results, user can select the relevant images as a positive feedback to the system which indicates the type of images the user is looking for. The system then runs two separate queries on the text and image-based systems for the selected feedback images.</p><p>For the query expansion in the text-based system, the system finds the corresponding XML documents from the positive feedback images. Next, it extracts the top n most frequent keywords from each XML documents. Hence, in the next iteration of RF, user will submit seperate new queries to the system using the new keywords found in each document. This will result in m different lists of results where m is equal to the number of documents sent as positive feedback. After getting m separate lists of results, we merge these into a single list and display the text-based result.</p><p>Merging of the results is based on the assumption that if a particular image is occurring in most of the lists, then it should have a higher rank or priority then images less frequent in the lists. So, we upgrade the rank of this image by increasing the average similarity score based on how many lists contain that image. For example, if there are 10 lists of results, and a particular image presents in 8 of the lists, we will add more weight to this image than the image which occurs in 3 lists. More technically, if the image presents in all the list then a boost of 0.3 as additional score will be given to that image score. If the image exists in 50% or above of the lists then a boost of 0.2 and for less than 50%, a boost of 0.1 is given. For the images exist only once among all the lists, no boost is provided but the images are added to the list of the final result with their original score. After this, the list is sorted with the new scores and displayed as the final text-based result list as shown in the Fig. <ref type="figure" coords="8,197.53,607.31,3.90,9.96">2</ref>. The different boosting scores are selected by experimenting on a small sample database, which provided better results.</p><p>When the content-based system receives the list of positive images as relevance feedback, we can perform similar query point movement and similarity measure adjustment techniques as described in section 2.3, to return a new image-based result list on the text-based pre-filtered images. Once we have separate lists of results (e.g., one from the text-based system and another from the imagebased system), we merge the lists using the similar weighting scheme as described in section 2.4 for the photographic image retrieval. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Analysis of the results</head><p>We have submitted three runs for the ad-hoc medical retrieval as shown in Table <ref type="table" coords="9,450.84,200.99,3.90,9.96" target="#tab_1">2</ref>. In all these runs, English queries and example images are used as our initial source queries. In the first run with ID "CINDI-Fusion-Visual", we performed only the automatic visual only search without any feedback. Our group ranked first in this run category (automatic+visual) based on the MAP score (0.0753) out of five different groups and 11 submissions. For the second run with ID "CINDI-Visual-RF", we performed the manual feedback in the image only modality. For this category (e.g., visual only run with RF), only our group has participated this year and achieved better MAP score (0.0957) then without RF as shown in Table <ref type="table" coords="9,343.10,284.63,3.90,9.96" target="#tab_1">2</ref>. For the third run with ID "CINDI-Text-Visual-RF", we performed the manual feedback in both the modalities and merge the result lists as discussed in the previous section. For this category (e.g., mixed with RF), we have achieved a moderate MAP score of 0.1513. From the scores, it is clear that combining both modalities is far better then using only a single modality (e.g., only image).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Automatic annotation tasks</head><p>The aim of the automatic annotation task is to compare the state-of-the-art approaches to image classification and annotation and to quantify their improvements for image retrieval. We investigate a supervised learning-based approach to associate the low-level image features with their high-level semantic categories for the image categorization or annotation of the medical (IRMA) and object (LTU) data sets. Specially, we explore the classifier combination approach of several probabilistic multi-class support vector machine (SVM) classifiers. Instead of using only one integrated feature vector, we utilize the features at the different levels of image representation as inputs to the SVM classifiers and use several classifier combination approaches to predict the final image category as well as probability or membership score of each category as image annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Probabilistic multi-class SVM with pairwise coupling</head><p>SVM is an emerging machine learning technology that has already been used successfully for image retrieval and classification purposes <ref type="bibr" coords="9,247.69,541.07,14.60,9.96" target="#b12">[13]</ref>. It performs classification between two classes by finding a decision surface that is based on the most informative points of the training set. Briefly, one can say that SVM constructs a decision surface between samples of two classes, maximizing the margin between them. SVM was originally designed for binary classification problem. A number of methods have been proposed for extension to multi-class problem to separate L mutually exclusive classes essentially by solving many two-class problems and combining their predictions in various ways <ref type="bibr" coords="9,115.31,612.83,14.60,9.96" target="#b13">[14]</ref>. In the experiments, we utilize a multi-class classification method by combining all pairwise comparisons of binary SVM classifiers, known as one-against-one or pairwise coupling (PWC) <ref type="bibr" coords="9,126.01,636.71,14.60,9.96" target="#b13">[14]</ref>. PWC constructs binary SVM's between all possible pairs of classes. Hence, this method uses L * (L-1)/2 binary classifiers, each of which provides a partial decision for classifying a data point. During the testing of a feature vector f , each of the L * (L -1)/2 classifier votes for one class. The winning class is the one with the largest number of accumulated votes. Although the voting procedure requires just pairwise decisions, it only predicts a class label <ref type="bibr" coords="9,439.44,684.59,14.60,9.96" target="#b15">[16]</ref>. However, to annotate or represent each image with a category specific confidence score, probability estimation is required. In our experiments, the probability estimation approach in <ref type="bibr" coords="10,413.06,232.67,15.49,9.96" target="#b13">[14]</ref> for the multi-class classification by PWC is utilized. In this context, given the observation or feature vector f , the goal is to estimate the posterior probability as</p><formula xml:id="formula_15" coords="10,233.76,275.75,279.27,10.33">p k = P (y = k | f ), k = 1, • • • , L<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multiple SVM classifier combination</head><p>The development of a multiple expert or classifier combination based system has received increasing attention and has been a popular research topic. In general, classifier combination is defined as the instances of the classifiers with different structures trained on distinct feature spaces <ref type="bibr" coords="10,494.76,345.47,14.60,9.96" target="#b14">[15]</ref>. Feature descriptors at different levels of image representation are in diversified forms and often complementary in nature. It is rather unwise to concatenate them together to form a single feature vector as the input for a single classifier. Hence, multiple classifiers are needed to deal with the different features, which results in a general problem of how to combine those classifiers with different features to yield the improved performance.</p><p>In the experiments, we consider expert combination strategies of the SVM classifiers with different low-level features as inputs based on three popular classifier combination rules (e.g., sum, product and max rules) <ref type="bibr" coords="10,198.38,441.11,14.60,9.96" target="#b14">[15]</ref>. Since the outputs of the classifiers are to be used in combination, the confidence or membership scores from the probabilistic SVM's in the range of [0, 1] for each category serve this purpose. In these combination rules, a priori probabilities are assumed to be equal and the decision is made by the following formula in terms of the a posteriori probabilities yielded by the respective classifiers:</p><formula xml:id="formula_16" coords="10,191.04,506.11,321.99,28.73">P combine (y = k | f ) = P combine k L k=1 P combine k , k = 1, • • • , L<label>(13)</label></formula><p>Here, P combine k is the combined output of the classifiers about the likelihood of the sample vector f belonging to the category k ∈ L. P combine k and is obtained by using the following two combination rules <ref type="bibr" coords="10,113.88,566.51,14.60,9.96" target="#b14">[15]</ref>:</p><p>In product rule, it is assumed that the representations used are conditionally statistically independent, where R experts or classifiers are combined as follows</p><formula xml:id="formula_17" coords="10,236.76,608.92,276.28,30.45">P combine k = R m=1 P (y = k | f m )<label>(14)</label></formula><p>where, P (y = k | f m ) denotes the posterior probability of the class k on the input f m for the classifier m. Similarly, for the sum and max rules, it can be stated as follows: </p><formula xml:id="formula_18" coords="10,236.76,677.56,276.28,30.45">P combine k = R m=1 P (y = k | f m )<label>(15)</label></formula><formula xml:id="formula_19" coords="11,236.16,362.32,276.88,18.69">P combine k = R max m=1 P (y = k | f m )<label>(16)</label></formula><p>The sum rule is developed under stricter assumptions than the product rule. In addition to the conditional independence assumption in the product rule, the sum rule assumes that the probability distribution will not deviate significantly from the a priori probabilities <ref type="bibr" coords="11,469.35,410.99,14.60,9.96" target="#b14">[15]</ref>. The multi-class SVM classifiers as experts on different feature descriptors as described in section 2.2 for retrieval, are combined with the above rules and finally classify the image to the category with the highest obtained probability value and annotate the images with the probability or membership scores as shown in the process diagram in Fig. <ref type="figure" coords="11,296.76,458.75,3.90,9.96" target="#fig_1">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis and results of the runs</head><p>To perform SVM-based classification, we utilze the LIBSVM software package <ref type="bibr" coords="11,431.51,505.07,14.60,9.96" target="#b15">[16]</ref>. For the training of both the data sets, RBF kernel functions are utilized with different kernel γ and cost C parameters found out experimentally with 5-fold cross validation (CV).</p><p>We have submitted four runs for the object annotation task as shown in Table <ref type="table" coords="11,453.60,540.95,3.90,9.96" target="#tab_2">3</ref>. First three of these runs are experimented with the proposed multi-class SVM and classifier combination approach with different feature inputs and the last run with ID "Cindi-Fusion-Knn" is experimented with a K-NN (K=9) classifier by using the fusion-based similarity matching function in equation <ref type="bibr" coords="11,90.00,588.71,11.62,9.96" target="#b6">(7)</ref>. Our best run (e.g., "Cindi-SVM-Product") in this task ranked third among all the submissions. Although the accuracy rate is much lower at this moment due to the complexity of the dataset in general.</p><p>For the medical annotation task, we have submitted five runs as shown in Table <ref type="table" coords="11,458.76,624.59,3.90,9.96" target="#tab_3">4</ref>. First four of these runs are experimented with the proposed multi-class SVM and classifier combination approach and the last run with ID "cindi-fusion-KNN9" is experimented with a K-NN (K=9) classifier by using the fusion-based similarity matching function in equation <ref type="bibr" coords="11,428.54,660.47,16.43,9.96" target="#b10">(11)</ref>. Our best run (e.g., "cindi-svm-sum") in this task ranked 13th among all the submissions and 6th among all the groups. This report has examined the image retrieval and annotation approaches of CINDI research group for ImageCLEF 2006. We have participated in all the four sub-tasks and submitted several runs with different combination of methods, features and parameters. We have experimented with a cross-modal interaction and integration approach for the retrieval of the photographic and medical image collections and a supervised classifier combination-based approach for the automatic annotation of the object and medical datasets. The analysis and the results of the runs are discussed in this paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,172.56,222.11,258.17,9.96;6,175.44,59.42,252.27,148.20"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Process flow diagram of the integration approach</figDesc><graphic coords="6,175.44,59.42,252.27,148.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="10,166.68,191.03,269.76,9.96;10,157.44,59.60,288.54,117.06"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Block diagram of the classifier combination process.</figDesc><graphic coords="10,157.44,59.60,288.54,117.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="8,157.44,58.80,287.78,227.30"><head></head><label></label><figDesc></figDesc><graphic coords="8,157.44,58.80,287.78,227.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,112.32,252.47,378.54,89.76"><head>Table 1 :</head><label>1</label><figDesc>Results of the ImageCLEFphoto Retrieval task</figDesc><table coords="6,112.32,270.11,378.54,72.12"><row><cell>Run ID</cell><cell>Language</cell><cell>Mod</cell><cell>A/M</cell><cell>FB</cell><cell>QE</cell><cell>MAP</cell></row><row><cell>Cindi-Text-Eng</cell><cell>Eng</cell><cell>Text</cell><cell cols="2">Automatic Without</cell><cell>No</cell><cell>0.1995</cell></row><row><cell>Cindi-TXT-EXP</cell><cell>Eng</cell><cell>Text</cell><cell>Manual</cell><cell>With</cell><cell cols="2">Yes 0.3749</cell></row><row><cell>Cindi-Exp-RF</cell><cell>Eng</cell><cell>Text+Image</cell><cell>Manual</cell><cell>With</cell><cell cols="2">Yes 0.3850</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,135.36,68.39,332.46,87.72"><head>Table 2 :</head><label>2</label><figDesc>Results of the Medical Retrieval task</figDesc><table coords="9,135.36,83.99,332.46,72.12"><row><cell>Run ID</cell><cell>Topic</cell><cell>System</cell><cell>MAP</cell><cell cols="2">R-prec B-pref</cell></row><row><cell>CINDI-Fusion-Visual</cell><cell>Automatic</cell><cell>Visual</cell><cell cols="2">0.0753 0.1311</cell><cell>0.166</cell></row><row><cell>CINDI-Visual-RF</cell><cell>Feedback</cell><cell>Visual</cell><cell cols="3">0.0957 0.1347 0.1796</cell></row><row><cell>CINDI-Text-Visual-RF</cell><cell>Feedback</cell><cell>Mixed</cell><cell cols="3">0.1513 0.1969 0.2397</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,102.24,68.87,398.56,111.00"><head>Table 3 :</head><label>3</label><figDesc>Performance of the object annotation task (LTU dataset)</figDesc><table coords="11,102.24,86.99,398.56,92.88"><row><cell>Run ID</cell><cell>Feature</cell><cell>Method</cell><cell>Error rate(%)</cell></row><row><cell>Cindi-SVM-Product</cell><cell>CLD+EHD+Semi-global</cell><cell>SVM (Product)</cell><cell>83.2</cell></row><row><cell>Cindi-SVM-SUM</cell><cell>CLD+EHD+Semi-global</cell><cell>SVM (Sum)</cell><cell>85.2</cell></row><row><cell>Cindi-SVM-EHD</cell><cell>EHD</cell><cell>SVM</cell><cell>85.0</cell></row><row><cell>Cindi-Fusion-Knn</cell><cell>CLD+EHD+Semi-global+Local</cell><cell>K-NN</cell><cell>87.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,96.00,205.79,427.12,131.76"><head>Table 4 :</head><label>4</label><figDesc>Performance of the medical annotation task (IRMA dataset)</figDesc><table coords="11,96.00,223.91,427.12,113.64"><row><cell>Run ID</cell><cell>Feature</cell><cell>Method</cell><cell>Error rate(%)</cell></row><row><cell>cindi-svm-product</cell><cell>CLD+EHD+Scaled+Semi-global</cell><cell>SVM (Product)</cell><cell>24.8</cell></row><row><cell>cindi-svm-sum</cell><cell>CLD+EHD+Scaled+Semi-global</cell><cell>SVM (Sum)</cell><cell>24.1</cell></row><row><cell>cindi-svm-max</cell><cell>CLD+EHD+Scaled+Semi-global</cell><cell>SVM (Max)</cell><cell>26.1</cell></row><row><cell>cindi-svm-ehd</cell><cell>EHD</cell><cell>SVM</cell><cell>25.5</cell></row><row><cell cols="2">cindi-fusion-KNN9 CLD+EHD+Scaled+Semi-global+Local</cell><cell>K-NN</cell><cell>25.6</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,110.52,209.03,402.62,9.96;12,110.52,221.03,402.51,9.96;12,110.52,233.03,22.93,9.96" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="12,389.47,209.03,123.68,9.96;12,110.52,221.03,205.90,9.96">Overview of the ImageCLEF 2006 photo retrieval and object annotation tasks</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006-09">Sep., 2006</date>
			<pubPlace>Alicante, Spain</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">CLEF working notes</note>
</biblStruct>

<biblStruct coords="12,110.52,251.27,402.61,9.96;12,110.52,263.27,402.51,9.96;12,110.52,275.15,22.93,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,372.06,251.27,141.07,9.96;12,110.52,263.27,194.65,9.96">Overview of the ImageCLEFmed 2006 medical retrieval and annotation tasks</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,318.00,263.61,90.45,9.18">CLEF working notes</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09">Sep., 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.53,293.51,398.65,9.96" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="12,274.68,293.85,129.43,9.18">Modern Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ribiero-Neto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Addison Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.53,311.75,402.85,9.96;12,110.52,323.75,403.03,9.96;12,110.52,335.63,32.34,9.96" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="12,184.23,311.75,224.27,9.96">Intelligent Information Retrieval and Web Search</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<ptr target="http://www.cs.utexas.edu/users/mooney/ir-course/" />
		<imprint>
			<pubPlace>Austin, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Online Courseware, University of Texas</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.53,353.99,402.49,9.96;12,110.52,365.87,402.50,9.96;12,110.52,377.87,73.81,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,365.04,353.99,147.98,9.96;12,110.52,365.87,115.22,9.96">Content-Based Image Retrieval at the End of the Early Years</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smeulder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,237.84,366.21,220.10,9.18">IEEE Trans. on Pattern Anal. and Machine Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1349" to="1380" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.53,396.23,402.57,9.96;12,110.52,408.11,382.93,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,315.29,396.23,197.81,9.96;12,110.52,408.11,147.68,9.96">A Feature Level Fusion in Similarity Matching to Content-Based Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">C</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bhattacharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,270.00,408.45,192.64,9.18">Proc. 9th Internat Conf. Information Fusion</title>
		<meeting>9th Internat Conf. Information Fusion</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.53,426.47,402.43,9.96;12,110.52,438.35,275.80,9.96" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="12,316.39,426.47,196.57,9.96;12,110.52,438.69,90.59,9.18">Introduction to MPEG-7-Multimedia Content Description Interface</title>
		<editor>B. S. Manjunath, P. Salembier, T. Sikora</editor>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>John Wiley Sons Ltd</publisher>
			<biblScope unit="page" from="187" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.53,456.71,402.44,9.96;12,110.52,468.93,402.31,9.18;12,110.52,480.59,97.95,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,231.92,456.71,156.47,9.96">Texture Analysis in Machine Vision</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,400.92,457.05,112.05,9.18;12,110.52,468.93,394.39,9.18">Chapter Using Texture in Image Similarity and Retrieval, Series on Machine Perception and Artificial Intelligence</title>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.52,498.83,402.72,9.96" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
		<title level="m" coord="12,172.32,499.17,199.87,9.18">Introduction to Statistical Pattern Recognition</title>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
	<note>2nd ed</note>
</biblStruct>

<biblStruct coords="12,110.53,517.19,311.70,9.96" xml:id="b9">
	<monogr>
		<ptr target="http://lucene.apache.org/java/docs/" />
		<title level="m" coord="12,110.53,517.19,90.20,9.96">Lucene search engine</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.53,535.43,402.72,9.96;12,110.52,547.43,293.41,9.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,216.88,535.43,296.37,9.96;12,110.52,547.43,68.01,9.96">Relevance Feedback: A Power Tool for Interactive Content-Based Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,190.32,547.77,152.87,9.18">IEEE Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.53,565.67,402.33,9.96;12,110.52,577.67,138.99,9.96" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,173.91,565.67,149.99,9.96">Regularized Discriminant Analysis</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,336.12,566.01,176.74,9.18;12,110.52,578.01,15.85,9.18">Journal of American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="165" to="175" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.52,595.91,296.41,9.96" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="12,158.04,596.25,115.89,9.18">Statistical Learning Theory</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.53,614.27,402.46,9.96;12,110.52,626.15,338.19,9.96" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,270.94,614.27,242.04,9.96;12,110.52,626.15,77.47,9.96">Probability Estimates for Multi-class Classification by Pairwise Coupling</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">C</forename><surname>Weng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,200.64,626.49,128.81,9.18">J Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="975" to="1005" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.53,644.51,402.42,9.96;12,110.52,656.51,219.39,9.96" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,311.59,644.51,103.34,9.96">On combining classifiers</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hatef</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">P W</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,425.04,644.85,87.91,9.18;12,110.52,656.85,87.39,9.18">IEEE Trans Pattern Anal Machine Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="226" to="239" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.53,674.75,402.59,9.96;12,110.52,686.75,222.04,9.96" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="12,219.44,674.75,203.66,9.96">LIBSVM : a library for support vector machines</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/cjlin/libsvm" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
