<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,139.80,148.73,323.12,15.51;1,92.40,170.69,417.84,15.51;1,228.00,192.53,147.01,15.51">Performing image classification with a frequency-based information retrieval schema for ImageCLEF 2006</title>
				<funder ref="#_2QshAt4">
					<orgName type="full">Swiss National Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,191.04,226.03,66.28,9.96"><forename type="first">Henning</forename><surname>Müller</surname></persName>
							<email>henning.mueller@sim.hcuge.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">University and Hospitals of Geneva</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,265.15,226.03,50.56,9.96"><forename type="first">Tobias</forename><surname>Gass</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University and Hospitals of Geneva</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,323.65,226.03,88.25,9.96"><forename type="first">Antoine</forename><surname>Geissbuhler</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University and Hospitals of Geneva</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,139.80,148.73,323.12,15.51;1,92.40,170.69,417.84,15.51;1,228.00,192.53,147.01,15.51">Performing image classification with a frequency-based information retrieval schema for ImageCLEF 2006</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">74AA0294793B21AFD2110AD5B3869032</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Managment]: Languages-Query Languages Measurement, Performance, Experimentation Image retrieval, Image classification, frequency-based weights</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article describes the participation of the University and Hospitals of Geneva at the ImageCLEF 2006 image classification tasks (medical and non-medical). The techniques applied are based on classical tf/idf weightings of visual features as used in the GIFT (GNU Image Finding Tool) image retrieval engine. Based on the training data, features that appear in images of the same class are weighted higher than features appearing across many classes. These feature weights are added to the classical ft/idf weights, making it a mixture of weightings. Several weightings and learning approaches are applied as well as several quantisations of the features space with respect to grey levels. A surprisingly small number of grey levels leads to best results. Learning can improve the results only slightly and does not obtain as good results as classical image classification approaches. A combination of several classifiers leads to best final results, showing that the applied schemes have independent results. For future work it seems important to study in more detail the important features and feature groups as they are not independent in the GIFT system. Pre-treating of the images (background removal) or allowing for more variation of the images with respect to object size and position might be other approaches to further improve results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>ImageCLEF 1 makes available realistic test collections for the evaluation of retrieval and classification tasks in the context of CLEF 2 (Cross Language Evaluation Forum). A detailed description of the object annotation task and a photographic retrieval task can be found in <ref type="bibr" coords="2,437.09,111.43,9.98,9.96" target="#b0">[1]</ref>. The overview includes a description of the tasks, the submitted results and a ranking of the best systems. A description of a medical image retrieval and automatic image annotation task can be found in <ref type="bibr" coords="2,502.09,135.31,10.55,9.96" target="#b3">[4]</ref> with all the details of submissions. More on the data can also be found on <ref type="foot" coords="2,418.08,146.48,3.95,5.52" target="#foot_2">3</ref> . This article will concentrate on the submission of the University and Hospitals of Geneva for the two image classification tasks. The submissions were not in time for the official evaluation because of a lack of man power but can be compared with these results in the overview articles. Already in 2005, an automatic medical image annotation task top was offered in ImageCLEF <ref type="bibr" coords="2,473.19,195.19,9.98,9.96" target="#b1">[2]</ref>. Best results were obtained by systems using classical image classification techniques <ref type="bibr" coords="2,442.61,207.07,9.98,9.96" target="#b2">[3]</ref>. Approaches based on information retrieval techniques <ref type="bibr" coords="2,274.04,219.07,10.55,9.96" target="#b4">[5]</ref> had lower results but were still among the best five groups, and this without using any learning data. It was expected that a proper use of learning data could improve results significantly, although tf/idf weighting already take into account the distribution of features in the collection. Such a learning approach is attempted and described in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>The methods described in this paper rely heavily on those used in the GIFT<ref type="foot" coords="2,414.24,320.48,3.95,5.52" target="#foot_3">4</ref> (GNU Image Finding Tool) <ref type="bibr" coords="2,116.13,333.31,9.98,9.96" target="#b6">[7]</ref>. The learning approaches applied are based on learning algorithms published in <ref type="bibr" coords="2,476.46,333.31,10.55,9.96" target="#b5">[6]</ref> using the idea to translate the market basket analysis problem to image retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Features used</head><p>GIFT itself uses four different groups of image features, which are described in more detail in <ref type="bibr" coords="2,499.37,391.27,9.98,9.96" target="#b6">[7]</ref>.</p><p>• A global color histogram which is based on the HSV color space and quantised into 18 hues, 3 saturations, 3 values and usually 4 levels of grey.</p><p>• Local color blocks. Each image is recursively partitioned into 4 blocks of equal size, and each block is represented by its mode color.</p><p>• A global texture histogram of the responses to Gabor filters of 3 different scales and 4 directions, which are quantised into 10 bins with the lowest one usually being discarded.</p><p>• Local Gabor block features by applying the filters mentioned above to the smallest blocks created by the recursive partition and using the same quantisation into bins.</p><p>This results in 84'362 possible features where each image contains around 1'500. The Images in the IRMA database are not coloured and thus the number of features is reduced by roughly 50%. Because of this and as a color histogram is usually an effective feature, we decided to increase the color features by extracting not only four levels of grey, but also 8, 16 and 32 levels, resulting in a higher-dimensional space. Such changes in feature space have been used frequently in the medGIFT<ref type="foot" coords="2,133.32,592.76,3.95,5.52" target="#foot_4">5</ref> project.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feature weights</head><p>Several weighting schemes are implemented in GIFT. The basic one used in this paper is the term f requency/inverted document f requency(tf /idf ) weighting which is well known from text retrieval (TR) literature. Given a query image q and a possible result image k, a score is calculated as the sum of all weights of features which are occurring in k.</p><formula xml:id="formula_0" coords="2,231.00,697.03,141.00,20.98">score kq = j (f eature weight j )</formula><p>The weight of each feature is computed by dividing the term frequency(tf ) of the feature by the squared logarithm of the inverted collection frequency(cf ).</p><formula xml:id="formula_1" coords="3,219.96,143.96,162.95,11.96">f eature weight j = tj j * log 2 (1/(cf j ))</formula><p>This results in giving features, which occur very frequently in the collection, a lower weight. These features do not discriminate images very well from each other. An example for such a feature would be black background being present in a very large number of medical images.</p><p>The strategy described above does not use much of the information contained in the training data, only the feature frequencies are exploited and not at all the class memberships of the images. For optimising the retrieval of relevant images, learning from user relevance f eedback was presented in <ref type="bibr" coords="3,148.04,238.99,9.98,9.96" target="#b5">[6]</ref>. Here, we use the described weighting approaches and add with several learning strategies to optimise results for the classification task, where class membership of the entire training data is known.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Strategies</head><p>The former learning approach was to analyse log files of system use and find pairs of images that were marked together in the query process. Afterwards, frequencies can be computed of how often each feature occurs pairs of images. A weight can be calculated by using the information whether or not the images in the pair were both marked as relevant or whether one was marked relevant and the other as notrelevant. This results in desired and non-desired cooccurence of features.</p><p>In the approach described in this paper, we want to train weights in a scope more focused on classification. This means that we do not look at user interaction but rather get relevance data on class memberships of images by looking at the class labels of the training data. Each result image for a query is marked as relevant if the class matches that of the query image and non-relevant otherwise. This allows for a more focused weighting than what real users would do with relevance feedback.</p><p>We then applied several strategies for extracting the pairs of images for such queries. In the first approach, each possible pair of images which occurs at least once is considered relevant. This yields very good results for image retrieval in general as can be seen in <ref type="bibr" coords="3,395.23,462.55,9.98,9.96" target="#b5">[6]</ref>. In the second approach we aim at discriminating positive and negative results in a more direct way. To do this, only the best positive and the worst negative results (images) of a query are taken into account when computing pairs of marked images. In a third approach, we pruned all queries which seemed too easy. This means that if the first N results were already positive, we omitted the entire query from further evaluation. Everything else follows the basic approach. This is based on ideas similar to Support Vector Machines (SVM), where only information on the class boundaries is taken into account and all images that are in the middle of the class would be classified correctly anyways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Computation of additional feature weights</head><p>For each image pair detected beforehand, we calculate the features they have in common and whether the image pair was positive (both images in the same class) or negative (images in different classes). This results in positive and negative cooccurence on a feature level. We used two ways to compute an additional weighting factor for the features:</p><p>• Basic Frequency : In this weighting scheme, each feature is weighted by the number of occurrences in pairs where both images are in the same class, normalised by the number of occurrences of the feature in all pairs.</p><formula xml:id="formula_2" coords="3,178.08,690.67,270.59,24.22">f actor j = |{f j |f j ∈ I a ∧ f j ∈ I b ∧ (I a → I b ) + }| |{f j |f j ∈ I a ∧ f j ∈ I b ∧ ((I a → I b ) + ∨ (I a → I b ) -)}|</formula><p>In the formula, f j is a feature j, I a and I b are two images and (I a → I b ) +/-denotes that I a and I b were marked together positively (+) or negatively (-).</p><p>• Weighted Probabilistic :</p><formula xml:id="formula_3" coords="4,199.92,128.59,226.91,24.34">f actor j = 1 + (2 * pp |{(I a → I b ) + }| ) - np |{(I a → I b ) -}|</formula><p>Here, pp (positive probability) is the probability that the feature j is important for correct classification, whereas np (negative probability) denotes the opposite.</p><p>The additional factors calculated in this way are then simply multiplied with the already existing feature weights using tf/idf for the calculation of similarity scores for all the test images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Classification</head><p>For each query image q, a set of N ∈ {1, 3, 5, 10} result images k with a similarity score S k were returned. The class of each result image were computed and the similarity scores were added up for the corresponding classes. The class with the highest accumulated score was then assigned to the query image. From preliminary experiments it was clearly visible that N = 5 produced the best results. This schema is very similar to a typical K-nearest neighbour (k-NN) classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>This section presents our evaluation results based on the ground truth data supplied by the two ImageCLEF classification tasks. These runs were not officially submitted to the ImageCLEF2006 task because of time constraints. They were submitted to the organisers a few weeks late for comparison with the officially submitted results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Classification on the LTU database</head><p>The non-medical automatic annotation task consisted of 14'035 training images from 21 classes. Data were made available by LookThatUp<ref type="foot" coords="4,275.88,449.12,3.95,5.52" target="#foot_5">6</ref> , and a set of more than 200 classes exists but such a task was regarded too difficult after a few tests. Subsets of images such as computer equipment were formed, mainly with images crawled from the web with a large variety for the contained objects. The task still remained hard with only three groups participating in it. The content of the images was regarded as extremely heterogeneous even for the same classes. Without using any of the described learning methods, using a simple 5-nearest-neighbour classifier, the GIFT had an error rate of 91,7%. Using the learning method with best/worst pruning and the frequency based weighting described above, the error rate decreased to 90,5%. Best results obtained in the competition by an optimised classification system were 77.3%, and the GIFT was not the worst-performing system submitting results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Classification on the IRMA database</head><p>The medical image annotation task was done for the second time in 2006, after a first test in 2005. To augment the complexity, the number of classes was raised from 57 in 2005 to 116 in 2006. 10'000 images were made available as training data, and 1000 images with unknown classes had to be classified. The baseline results of the GIFT with various quantisations of grey levels can be seen in Table <ref type="table" coords="4,150.08,651.67,3.90,9.96" target="#tab_0">1</ref>. They show clearly that more levels of grey do not help the classification, as error rates increase with them.</p><p>In Table <ref type="table" coords="4,147.09,675.55,3.90,9.96">2</ref>, the results of the GIFT using the learning approaches described above can be seen. Surprisingly, the effect of the learning is quite small in comparison to the very good results obtained when aiming at increasing retrieval performance. The only method which improved the error rate at all was the frequency based weighting combined with best/worst pruning of the queries. Even here the difference is statistically not very significant. Table <ref type="table" coords="5,117.58,285.07,3.90,9.96">2</ref>: Error rates on the IRMA database using various weighting strategies and 4 grey levels. S 1 corresponds to using the naive strategy, S 2 to pruning the queries which were found too easy, and S 3 means that only the best positive and worst negative result of each query were taken into account.</p><p>We also combined eight grey levels with the described techniques but the results were always worse and thus not worth describing in more detail. Interestingly, the probabilistic weighting was not affected by the selections of relevant results in the same way as the frequency based weighting. Finally, we accumulated the scores of nearly all runs we performed. This combination results in an error rate of 29,7%, which shows that the approaches make differing errors and are thus theoretically combinable and at least in part independent.</p><p>The best system obtains a classification result of 16.2%, much better than we can obtain with our approach, that is rather good for information retrieval using relevance feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>The provided tasks proved difficult to optimise for a frequency-based image retrieval system such as the GIFT. In comparison to last year's automatic annotation task, the number of classes roughly doubled thus making it more difficult to learn how discriminant features are. The GIFT heavily relies on this analysis, because compared to competitive CBIR systems only low level features not geared towards a specific task are used. We presented and applied a couple of approaches to learn from training data, but given the difficulty of the task could not improve the results significantly. The low level features in GIFT seem to be too sensitive for small changes that occur in such a particular tasks. Possibilities to circumvent this can be on two sides: On the one side it would be possible to normalised images further before they are indexed with the current features. Such a normalisation would include the removal of background as much as possible, the taking into account of the aspect ratio of the images, plus maybe a normalisation of the grey values within a class to have a more constant input. Another part could include changes in the feature space itself. Whereas the block features seem to be working much better with a smaller number of grey levels, the histogram features would rather require a much larger number to contain any good information. More complex features could include shape descriptors after a rough segmentation of the object in the images. Then of course a multi-step approach for classification can be imagined, where images are successively put into smaller and smaller classer and can change features at each stage of the segmentation process. Image classification seems to be hard with an information retrieval approach, but with features of more semantic value this seems to be feasible.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,121.68,110.23,359.43,153.00"><head>Table 1 :</head><label>1</label><figDesc>Error rates on the IRMA database using a varying number of grey levels.</figDesc><table coords="5,164.04,110.23,274.80,153.00"><row><cell cols="3">Number of grey levels Error rate</cell></row><row><cell>4</cell><cell></cell><cell>32,0%</cell></row><row><cell>8</cell><cell></cell><cell>32,1%</cell></row><row><cell>16</cell><cell></cell><cell>34,9%</cell></row><row><cell>32</cell><cell></cell><cell>37,8%</cell></row><row><cell cols="3">Used strategy Frequency weighting Probabilistic weighting</cell></row><row><cell>S 1</cell><cell>35,3%</cell><cell>32,4%</cell></row><row><cell>S 2</cell><cell>33,2%</cell><cell>32,5%</cell></row><row><cell>S 3</cell><cell>31,7%</cell><cell>32,2%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,105.24,735.69,130.32,7.35"><p>http://ir.shef.ac.uk/imageclef/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,105.24,745.17,121.82,7.35"><p>http://www.clef-campaign.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,105.24,728.01,104.93,7.35"><p>http://ir.ohsu.edu/image/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,105.24,737.49,138.71,7.35"><p>http://www.gnu.org/software/gift/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="2,105.24,746.97,134.51,7.35"><p>http://www.sim.hcuge.ch/medgift/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="4,105.24,742.89,96.42,7.35"><p>http://www.ltutech.com/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was partially supported by the <rs type="funder">Swiss National Foundation</rs> (Grant <rs type="grantNumber">205321-109304/1</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_2QshAt4">
					<idno type="grant-number">205321-109304/1</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,105.47,187.99,407.30,9.96;6,105.48,199.99,407.16,9.96;6,105.48,211.99,44.48,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,399.25,187.99,113.52,9.96;6,105.48,199.99,215.12,9.96">Overview of the imageclef 2006 photo retrieval and object annotation tasks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,344.29,199.99,89.88,9.96">CLEF working notes</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09">Sep. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.47,231.91,407.22,9.96;6,105.48,243.79,407.13,9.96;6,105.48,255.79,385.59,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,263.64,243.79,230.89,9.96">The CLEF 2005 cross-language image retrieval track</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">M</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffery</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,105.48,255.79,229.55,9.96">Springer Lecture Notes in Computer Science (LNCS)</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09">September 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.47,275.71,407.14,9.96;6,105.48,287.71,407.26,9.96;6,105.48,299.59,336.17,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,477.23,275.71,35.38,9.96;6,105.48,287.71,403.40,9.96">FIRE in ImageCLEF 2005: Combining content-based image retrieval with textual information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,117.96,299.59,167.55,9.96">Working Notes of the CLEF Workshop</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09">September 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.47,319.51,407.20,9.96;6,105.48,331.51,407.18,9.96;6,105.48,343.51,22.88,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,381.03,319.51,131.64,9.96;6,105.48,331.51,192.59,9.96">Overview of the imageclefmed 2006 medical retrieval and annotation tasks</title>
		<author>
			<persName coords=""><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,321.88,331.51,89.88,9.96">CLEF working notes</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09">Sep. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.47,363.43,407.20,9.96;6,105.48,375.31,407.03,9.96;6,105.48,387.31,196.61,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,495.52,363.43,17.15,9.96;6,105.48,375.31,228.62,9.96">The use of MedGIFT and EasyIR for ImageCLEF 2005</title>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antoine</forename><surname>Geissbuhler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johan</forename><surname>Marty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Lovis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Ruch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,359.91,375.31,152.61,9.96;6,105.48,387.31,40.59,9.96">Working Notes of the 2005 CLEF Workshop</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09">September 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.47,407.23,407.33,9.96;6,105.48,419.23,407.00,9.96;6,105.48,431.11,314.30,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,349.38,407.23,163.42,9.96;6,105.48,419.23,217.67,9.96">Learning from user behavior in image retrieval: Application of the market basket analysis</title>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">Mcg</forename><surname>Squire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thierry</forename><surname>Pun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,329.90,419.23,178.16,9.96">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="65" to="77" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>Special Issue on Content-Based Image Retrieval</note>
</biblStruct>

<biblStruct coords="6,105.47,451.03,407.30,9.96;6,105.48,463.03,407.13,9.96;6,105.48,474.91,407.66,9.96;6,105.48,486.91,48.89,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,421.54,451.03,91.22,9.96;6,105.48,463.03,230.89,9.96">Content-based query of image databases: inspirations from text retrieval</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Mcg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wolfgang</forename><surname>Squire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thierry</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Pun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,478.96,463.03,33.66,9.96;6,105.48,474.91,331.67,9.96">Selected Papers from The 11th Scandinavian Conference on Image Analysis SCIA &apos;99)</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1193" to="1198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,158.57,486.91,137.78,9.96" xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">K</forename><surname>Ersboll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Johansen</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
