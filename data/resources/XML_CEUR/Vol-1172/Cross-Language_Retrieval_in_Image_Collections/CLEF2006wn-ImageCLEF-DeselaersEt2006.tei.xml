<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,164.20,148.86,274.60,15.15;1,193.68,170.78,215.66,15.15">Image Retrieval and Annotation Using Maximum Entropy</title>
				<funder ref="#_3uaJSdv">
					<orgName type="full">DFG (Deutsche Forschungsgemeinschaft)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,182.71,204.67,77.10,8.74"><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology and Pattern Recognition Lehrstuhl für Informatik 6</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,267.45,204.67,64.14,8.74"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology and Pattern Recognition Lehrstuhl für Informatik 6</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,359.65,204.67,60.64,8.74"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology and Pattern Recognition Lehrstuhl für Informatik 6</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,164.20,148.86,274.60,15.15;1,193.68,170.78,215.66,15.15">Image Retrieval and Annotation Using Maximum Entropy</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">10B5B15BD5E40DA534DA40E19B6F440D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.3 Information Search and Retrieval ; I.5 [Pattern Recognition]: I.5.4 Applications content-based image retrieval</term>
					<term>object recognition</term>
					<term>textual information retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we present and discuss our participation in the four tasks of the Image-CLEF 2006 Evaluation. In particular, we present a novel approach to learn feature weights in our content-based image retrieval system FIRE. Given a set of training images with known relevance among each other, the retrieval task is reformulated as a classification task and then the weights to combine a set of features are trained discriminatively using the maximum entropy framework. Experimental results for the medical retrieval task show large improvements over heuristically chosen weights. Furthermore the maximum entropy approach is used for the automatic image annotation tasks in combination with a part-based object model. The best results are achieved in the medical and the object annotation task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image retrieval and automatic classification or annotation of images are highly related research fields. Obviously, image retrieval can be "solved" by image annotation straightforwardly: given a database of images, annotate all of them and use textual information retrieval techniques. Multi modal information retrieval, another highly related field allows to use e.g. visual and textual information to retrieve relevant documents. All these tasks have in common that somehow the semantic gap has to be bridged and that therefore large amounts of data have to be processed. Features and descriptors are extracted from the data and these have to be combined to obtain a satisfying solution.</p><p>In the domain of feature combination, machine learning algorithms are used quite commonly, among them log-linear models that are discriminatively trained under the maximum entropy criterion are very successful <ref type="bibr" coords="1,200.86,685.87,9.96,8.74" target="#b1">[2]</ref>. Maximum entropy, or logistic, models are commonly used in natural language processing <ref type="bibr" coords="1,179.52,697.82,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="1,193.36,697.82,7.01,8.74" target="#b1">2]</ref>, data mining <ref type="bibr" coords="1,263.10,697.82,14.62,8.74" target="#b26">[27]</ref>, and image processing <ref type="bibr" coords="1,380.86,697.82,15.49,8.74" target="#b18">[19,</ref><ref type="bibr" coords="1,399.67,697.82,12.73,8.74" target="#b20">21,</ref><ref type="bibr" coords="1,415.73,697.82,11.62,8.74" target="#b24">25]</ref>.</p><p>In this work, we present how the maximum entropy approach can on the one hand be used for object recognition and classification of images and on the other hand for discriminative training of feature weights in an image retrieval system and thus for learning to combine textual information sources with visual information sources in a unified framework. In particular, we describe how we used maximum entropy training for our submissions to the 2006 ImageCLEF image retrieval and classification/annotation evaluation. The main contribution of this paper is a method to learn feature weights for image retrieval from a given set of queries and relevant documents.</p><p>The remainder of this paper is structured as follows: Section 2 describes the retrieval framework, the application of the maximum entropy approach to feature weight training and the experiments we performed for the two image retrieval tasks in ImageCLEF 2006: medical retrieval and photo/ad-hoc retrieval. Section 3 describes the experiments that were performed for the automatic annotation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Retrieval Tasks</head><p>ImageCLEF 2006 hosted two independent retrieval tasks: The medical retrieval task <ref type="bibr" coords="2,461.16,250.27,15.50,8.74" target="#b27">[28]</ref> and the photo retrieval task <ref type="bibr" coords="2,178.67,262.22,9.97,8.74" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">FIRE -The Flexible Image Retrieval System</head><p>For the retrieval tasks the Flexible Image Retrieval Engine (FIRE) developed in our group was used. FIRE is a research image retrieval system that was designed with extensibility in mind and allows to combine various image descriptors and comparison measures easily.</p><p>Given a set of positive example images Q + and a (possibly empty) set of negative example images Q -a score S(Q + , Q -, X) is calculated for each image X from the database:</p><formula xml:id="formula_0" coords="2,179.43,376.76,333.57,22.68">S(Q + , Q -, X) = q∈Q + S(q, X) + q∈Q - (1 -S(q, X)).<label>(1)</label></formula><p>where S(q, X) is the score of database image X with respect to query q and is calculated as S(q, X) = e -γD(q,X) with γ = 1.0. D(q, X) is a weighted sum of distances calculated as</p><formula xml:id="formula_1" coords="2,218.12,442.88,294.88,30.20">D(q, X) := M m=1 w m • d m (q m , X m ).<label>(2)</label></formula><p>Here, q m and X m are the mth feature of the query image q and the database image X, respectively. d m is the corresponding distance measure and w m is a weighting coefficient. For each d m ,</p><formula xml:id="formula_2" coords="2,100.52,506.51,235.49,11.15">X∈B d m (Q m , X m ) = 1 is enforced by re-normalization.</formula><p>Given a query (Q + , Q -), the images are ranked according to descending score and the K images X with highest scores S(Q + , Q -, X) are returned by the retriever.</p><p>Weights were chosen heuristically based on experiences from earlier experiments; furthermore we used the weights of our run that performed best in the 2005 ImageCLEF medical retrieval evaluation.</p><p>Another way to obtain suitable weights is described in Section 2.3 which requires slight modifications of the decision rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Features</head><p>In the following we describe the image features we used in the evaluation. These features are extracted offline from all database images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appearance-based Image Features.</head><p>The most straight-forward approach is to directly use the pixel values of the images as features. For example, the images might be scaled to a common size and compared using the Euclidean distance. In optical character recognition and for medical data improved methods based on image features usually obtain excellent results <ref type="bibr" coords="2,441.88,709.78,15.49,8.74" target="#b19">[20,</ref><ref type="bibr" coords="2,460.70,709.78,12.73,8.74" target="#b22">23,</ref><ref type="bibr" coords="2,476.75,709.78,11.62,8.74" target="#b23">24]</ref>.</p><p>In this work, we used 32 × 32 versions of the images, these were compared using Euclidean distance. It has been observed, that for classification and retrieval of medical radiographs, this method serves as a not-too-bad baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Color Histograms.</head><p>Color histograms are widely used in image retrieval <ref type="bibr" coords="3,423.59,112.02,10.52,8.74" target="#b4">[5,</ref><ref type="bibr" coords="3,437.35,112.02,12.73,8.74" target="#b12">13,</ref><ref type="bibr" coords="3,453.31,112.02,12.73,8.74" target="#b28">29,</ref><ref type="bibr" coords="3,469.28,112.02,11.63,8.74" target="#b30">31]</ref>. Color histograms are one of the most basic approaches and to show performance improvements, image retrieval systems often are compared to a system using only color histograms. The color space is partitioned and for each partition the pixels with a color within its range are counted, resulting in a representation of the relative frequencies of the occurring colors. In accordance with <ref type="bibr" coords="3,480.12,159.84,14.61,8.74" target="#b28">[29]</ref>, we use the Jeffrey divergence to compare histograms.</p><p>Tamura Features. In <ref type="bibr" coords="3,203.52,197.69,15.50,8.74" target="#b31">[32]</ref> the authors propose six texture features corresponding to human visual perception: coarseness, contrast, directionality, line-likeness, regularity, and roughness. From experiments testing the significance of these features with respect to human perception, it was concluded that the first three features are very important. Thus in our experiments we use coarseness, contrast, and directionality to create a histogram describing the texture <ref type="bibr" coords="3,441.40,245.51,10.51,8.74" target="#b4">[5]</ref> and compare these histograms using the Jeffrey divergence <ref type="bibr" coords="3,292.36,257.47,14.61,8.74" target="#b28">[29]</ref>. In the QBIC system <ref type="bibr" coords="3,409.08,257.47,15.50,8.74" target="#b12">[13]</ref> histograms of these features are used as well.</p><p>Global Texture Descriptor.</p><p>In <ref type="bibr" coords="3,253.37,295.32,10.52,8.74" target="#b4">[5]</ref> a texture feature consisting of several parts is described: Fractal dimension measures the roughness or the crinkliness of a surface. In this work the fractal dimension is calculated using the reticular cell counting method <ref type="bibr" coords="3,379.21,319.23,14.61,8.74" target="#b15">[16]</ref>. Coarseness characterizes the grain size of an image. Here it is calculated depending on the variance of the image. Entropy is used as a measure of disorderedness or information content in an image. The Spatial gray-level difference statistics (SGLD) describes the brightness relationship of pixels within neighborhoods. It is also known as co-occurrence matrix analysis <ref type="bibr" coords="3,316.28,367.05,14.61,8.74" target="#b16">[17]</ref>. . The Circular Moran autocorrelation function measures the roughness of the texture. For the calculation a set of autocorrelation functions is used <ref type="bibr" coords="3,165.72,390.96,14.62,8.74" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Invariant Feature Histograms.</head><p>A feature is called invariant with respect to certain transformations if it does not change when these transformations are applied to the image. The transformations considered here are translation, rotation, and scaling. In this work, invariant feature histograms as presented in <ref type="bibr" coords="3,212.33,452.72,15.49,8.74" target="#b29">[30]</ref> are used. These features are based on the idea of constructing features invariant with respect to certain transformations by integration over all considered transformations. The resulting histograms are compared using the Jeffrey divergence <ref type="bibr" coords="3,437.08,476.63,14.61,8.74" target="#b28">[29]</ref>. Previous experiments have shown that the characteristics of invariant feature histograms and color histograms are very similar and that invariant feature histograms often outperform color histograms <ref type="bibr" coords="3,470.86,500.54,9.97,8.74" target="#b6">[7]</ref>. Thus, in this work color histograms are not used.</p><p>Patch Histograms. In object recognition and detection currently the assumption that objects consist of parts that can be modelled independently is very common, which led to a wide variety of bag-of-features approaches <ref type="bibr" coords="3,220.23,562.30,15.50,8.74" target="#b10">[11,</ref><ref type="bibr" coords="3,239.05,562.30,7.75,8.74" target="#b7">8,</ref><ref type="bibr" coords="3,250.12,562.30,11.63,8.74" target="#b25">26]</ref>.</p><p>Here we follow this approach to generate histograms of image patches for image retrieval. The creation is a 3-step procedure:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Maximum Entropy Training for Image Retrieval</head><p>We propose a novel method based on maximum entropy training using the generalized iterative scaling algorithm (GIS) to obtain feature weightings tuned toward a specific task. The maximum entropy approach is promising here, because it is ideally suited to combine features of different types and it yields good results in other areas like natural language processing <ref type="bibr" coords="4,451.83,166.27,10.52,8.74" target="#b1">[2]</ref> and image recognition <ref type="bibr" coords="4,141.41,178.23,15.50,8.74" target="#b20">[21,</ref><ref type="bibr" coords="4,160.14,178.23,11.63,8.74" target="#b18">19]</ref>. In <ref type="bibr" coords="4,192.40,178.23,14.62,8.74" target="#b18">[19]</ref>, the maximum entropy approach is used for automatic image annotation. The authors partition the image into rectangular parts and consider these patches as "image terms" similar to the usage of words in <ref type="bibr" coords="4,263.55,202.14,9.96,8.74" target="#b1">[2]</ref>.</p><p>We consider the problem of image retrieval to be a classification problem. Given the query image, the images from the database have to be classified to be either relevant (denoted by ⊕) or irrelevant (denoted by ). As classification method we choose log-linear models that are trained using the maximum entropy criterion and the GIS algorithm.</p><p>As features f i for the log-linear models we choose the distances between the m-th feature of the query image Q and the database image X:</p><formula xml:id="formula_3" coords="4,244.53,294.24,113.94,9.65">f i (Q, X) := d i (Q i , X i ).</formula><p>To allow for prior probabilities, we include a constant feature f i=0 (Q, X) = 1. Then, the score is replaced by the posterior probability for class ⊕:</p><formula xml:id="formula_4" coords="4,205.63,346.93,307.37,8.74">S(Q, X) := p(⊕|Q, X)<label>(3)</label></formula><formula xml:id="formula_5" coords="4,252.75,360.88,143.43,31.25">= exp [ i λ ⊕i f i (Q, X)] k∈{⊕, } exp [ i λ ki f i (Q, X)]</formula><p>Given these scores, we return the K images from the database that have the highest score S(Q, X), i.e. the K images that are most likely to be relevant according to the classifier. Note that here in comparison to the score calculation from Equation (1), the w i are replaced by the λ ⊕i and the λ i and an additional renormalization factor is introduced to assure that the probabilities sum up to one. Alternatively, Eq. 3 can easily be transformed to be of the form of Eq. 1 and the w i can be expressed as a function of λ ⊕i and λ i . In addition to considering the first order features alone as they are described above, we propose to use supplementary second order features (i.e. products of distances) as this usually yields superior performance on other tasks. Given a query image Q and a database image X we use the following set of features:</p><formula xml:id="formula_6" coords="4,197.25,519.05,208.50,24.60">f i (Q, X) := d i (Q i , X i ) f i,j (Q, X) := d i (Q i , X i ) • d j (Q j , X j ), i ≥ j,</formula><p>again including the constant feature f i=0 (Q, X) = 1 to allow for prior probabilities. The increased number of features results in more parameters to be trained. In earlier experiments, features of higher degree have been tested and not found to improve the results.</p><p>In the training process, the values of the λ ki are optimized. A sufficiently large amount of training data is necessary to do so. We are given the database T = {X 1 , . . . , X N } of training images with known relevances. For each image X n we are given a set</p><formula xml:id="formula_7" coords="4,90.00,614.14,423.00,21.61">R n = {Y | Y ∈ T is relevant, if X n is the query.}.</formula><p>Because we want to classify the relation between images into the two categories "relevant" or "irrelevant" on the basis of the distances between their features, we choose the following way to derive the training data for the GIS algorithm: The distance vectors D(X n , X m ) = (d 1 (X n1 , X m1 ), . . . , d I (X nI , X mI )) are calculated for each pair of images (X n , X m ) ∈ T × T . That is, we obtain N distance vectors for each of the images X n . These distance vectors are then labeled according to the relevances: Those D(X n , X m ) where X m is relevant with respect to X n , i.e. X m ∈ R n , are labeled ⊕ (relevant) and the remaining ones are labeled with the class label (irrelevant).</p><p>Given these N 2 distance vectors and their classification into "relevant" and "irrelevant" we train the λ ki of the log-linear model from Eq. (3) using the GIS algorithm.</p><p>The GIS algorithm proceeds as follows to determine the free parameters of the model <ref type="bibr" coords="5,497.50,112.02,11.63,8.74" target="#b2">(3)</ref>. First an initial parameter set Λ (0) = {λ (0) ki } is chosen, and then for each iteration t = 1, . . . , T the parameters are updated according to</p><formula xml:id="formula_8" coords="5,201.22,158.04,198.89,127.98">λ (t) ki = λ (t-1) ki + ∆λ (t) ki = λ (t-1) ki + 1 F log N ki Q (t) ki , Q (t) ki := Xn,Xm p Λ (t) (k|X n , X m )f i (X n , X m ), N ⊕i := Xn,Xm∈Rn f i (X n , X m ) N i := Xn,Xm ∈Rn f i (X n , X m )</formula><p>Here, F is a constant that depends on the training data. In some cases, this method is problematic due to the high computational demands. Here, the number of parameters to be estimated is small, i.e. 2I +1, thus performance is not a problem. Due to the low computational demands, this method can also be used to incorporate relevance estimates gathered from user interaction. To do so, the current state of the classifier can be used as a starting point for further training iterations with the training set enlarged by the newly gathered data. This process can e.g. be performed once a day. As the training is performed in an offline manner, the speed of the image retrieval engine is hardly decreased because the calculation of Equation ( <ref type="formula" coords="5,328.63,381.77,4.24,8.74" target="#formula_4">3</ref>) takes barely longer than the calculation of Equation (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Medical Retrieval Task</head><p>We submitted nine runs to the medical retrieval task <ref type="bibr" coords="5,320.16,440.01,14.62,8.74" target="#b27">[28]</ref>, one of these using only text, three using only visual information, and five using visual and textual information. For one of the combined runs we used the above-described maximum entropy training method. To determine the weights, we used the queries and their qrels from last year's medical retrieval task as training data. Table <ref type="table" coords="5,508.02,475.87,4.98,8.74">1</ref> gives an overview of the runs we submitted to the medical retrieval task and the results obtained.</p><p>In Figure <ref type="figure" coords="5,151.21,499.78,4.98,8.74">1</ref> the trained feature weights are visualized after different numbers of maximum entropy training iterations. It can clearly be seen that after 500 iterations the weights hardly differ from uniform weighting and that thus not enough training iterations were performed. After 5000 iterations, there is a clear gain in performance (cp. Table <ref type="table" coords="5,362.75,535.65,4.43,8.74">1</ref>) and the weights are not uniform any more. For example, the weight for feature 1 (English text) has the highest weight. With more iterations, the differences between the particular weights become bigger; after 10.000 iterations no additional gain in performance is yielded anymore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Photo/Ad-Hoc Retrieval Task</head><p>For the photo-and the ad-hoc retrieval task the newly created IAPR TC-12 database <ref type="bibr" coords="5,477.63,617.79,15.50,8.74" target="#b13">[14]</ref> was used, which currently consists of 20,000 general photographs, mainly from a vacation domain. For each of the images a German and an English description exists. The task is described in detail in <ref type="bibr" coords="5,90.00,653.66,9.97,8.74" target="#b2">[3]</ref>.</p><p>Two tasks were defined on this dataset: An ad-hoc task of 60 queries of different semantic and syntactic difficulty, and a photo task of 30 queries, which was based on a subset aiming to investigate the possibilities of purely visual retrieval. Therefore, some semantic constraints were removed from the queries. All queries were formulated by a short textual description and three positive example images.</p><p>Due to short time, we were unable to tune any parameters and just chose to submit two purely visual, full-automatic runs to both of these tasks.</p><p>Table <ref type="table" coords="6,118.21,122.14,3.88,8.74">1</ref>: Summary of our runs submitted to the medical retrieval task. The numbers give the weights (empty means 0) of the features in the experiments and the columns denote: En: English text, Fr : French text, Ge: German text, CH : color histogram, GH : gray histogram, GTF : global texture feature, IH : invariant feature histogram, TH : Tamura Texture Feature histogram, TN : 32x32 thumbnail, PH : patch histogram. The first group of experiments uses only textual information, the second group uses only visual information, the third group uses textual and visual information, and the last group both types of information and the weights are trained using the maximum entropy approach. The last column gives the results of the evaluation. The last three lines are unsubmitted runs that were performed after the evaluation ended.</p><p>run Figure <ref type="figure" coords="6,120.98,716.21,3.88,8.74">1</ref>: Trained weights for the medical retrieval task after different numbers of iterations in the maximum entropy training. On the x-axis, the features are given in the same order as in Table <ref type="table" coords="6,508.02,728.17,4.98,8.74">1</ref> and on the y-axis λ ⊕i -λ i is given. For the runs entitled IFHTAM, we used a combination of invariant feature histograms and Tamura texture histograms. Both histograms are combined by Jeffrey divergence and the invariant feature histograms are weighted by a factor of 2. This combination has been seen to be a very effective combination of features for databases of general photographs like for example the Corel database <ref type="bibr" coords="7,132.61,330.15,9.96,8.74" target="#b6">[7]</ref>. For the runs entitled PatchHisto we used histograms of vector-quantized image patches with 2048 bins.</p><p>In Table <ref type="table" coords="7,147.21,354.06,4.98,8.74" target="#tab_1">2</ref> we summarize the outcomes of the two tasks using the IAPR TC-12 database. The overal MAP values are rather low, but the combination of invariant feature histograms and Tamura texture features clearly outperforms all competing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Automatic Annotation Tasks</head><p>In ImageCLEF 2006, two automatic annotation tasks were arranged. One dealing with the automatic classification of medical radiographs <ref type="bibr" coords="7,280.83,444.69,15.50,8.74" target="#b27">[28]</ref> and one tackling the problem of automatic classification of everyday objects like backpacks, clocks, and plates <ref type="bibr" coords="7,368.99,456.65,9.96,8.74" target="#b2">[3]</ref>. The medical annotation task was very similar to last year's task, but the number of images was slightly raised and the number of classes was raised from 57 to 116. The automatic annotation task was somehow similar to the PASCAL visual object classes challenge <ref type="bibr" coords="7,298.73,492.52,14.62,8.74" target="#b11">[12]</ref>. Here, 20 classes had to be discriminated at once. The following sections describe the methods we applied to these classification tasks and the experiments we performed.</p><p>The task of the medical automatic annotation task and the object annotation tasks are very similar, but differ in some critical aspects:</p><p>• Both tasks provide a relatively large training set and a disjunct test set. Thus, in both cases it is possible to learn a relatively reliable model for the training data (this is somewhat proven for the medical annotation task, and below we also show this for the object annotation task).</p><p>• Both tasks are multi-class/one object per image classification tasks. Here they differ from the PASCAL visual classes challenge which addresses a set of object vs. non object tasks where several objects (of equal or unequal type) may be contained in an image.</p><p>• The medical annotation task has only gray scale images, whereas the object annotation task has mainly color images. This is probably most relevant for the selection of descriptors.</p><p>• The images from the test and the training set are from the same distribution for the medical task, whereas for the object annotation task, the training images are rather clutter-free and the test images contain a significant amount of clutter. This is probably relevant and should be addressed when developing methods for the object annotation task. Unfortunately, our models currently do not address this issue which probably has a significant impact on the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image Distortion Model</head><p>The image distortion model <ref type="bibr" coords="8,212.47,130.41,15.49,8.74" target="#b22">[23,</ref><ref type="bibr" coords="8,230.81,130.41,12.73,8.74" target="#b19">20]</ref> is a zeroth-order image deformation model to compare images pixel-wise. Here, classification is done using the nearest neighbor decision rule: to classify an image, it is compared to all training images in the database and the class of the most similar image is chosen. To compare images, the Euclidean distance can be seen as a very basic baseline, and in earlier works it was shown that image deformation models are a suitable way to improve classification performance significantly e.g. for medical radiographs and for optical character recognition <ref type="bibr" coords="8,141.79,202.14,15.50,8.74" target="#b21">[22,</ref><ref type="bibr" coords="8,160.89,202.14,11.63,8.74" target="#b22">23]</ref>. Here we allow each pixel of the database images to be aligned to the pixels from a 5×5 neighborhood from the image to be classified taking into account the local context from a 3×3 Sobel neighborhood. This method is of particular interest as it outperformed all other methods in automatic annotation task of ImageCLEF 2005 <ref type="bibr" coords="8,231.61,249.96,9.97,8.74" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sparse Patch Histograms &amp; Discriminative Classification</head><p>This approach is based on the widely adopted assumption that objects in images can be represented as a set of loosely coupled parts. In contrast to former models <ref type="bibr" coords="8,363.49,308.19,13.58,8.74" target="#b7">[8,</ref><ref type="bibr" coords="8,380.79,308.19,7.01,8.74" target="#b8">9]</ref>, this method can cope with an arbitrary number of object parts. Here, the object parts are modelled by image patches that are extracted at each position and then efficiently stored in a histogram. In addition to the patch appearance, the positions of the extracted patches are considered and provide a significant increase in the recognition performance.</p><p>Using this method, we create sparse histograms of 65536 (2 16 = 8 4 ) bins, which can either be classified using the nearest neighbor rule and a suitable histogram comparison measure or a discriminative model can be trained for classification. Here, we used a support vector machine with a histogram intersection kernel and a discriminatively trained log-linear maximum entropy model.</p><p>A detailed description of the method is given in <ref type="bibr" coords="8,316.79,427.75,9.97,8.74" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Patch Histograms &amp; Maximum Entropy Classification</head><p>In this approach, we use the histograms of image patches as described in Section 2.2 and maximum entropy training <ref type="bibr" coords="8,163.94,485.98,10.52,8.74" target="#b7">[8,</ref><ref type="bibr" coords="8,177.78,485.98,7.01,8.74" target="#b8">9]</ref>. This method has performed very well in the 2005 annotation task of ImageCLEF <ref type="bibr" coords="8,470.34,497.93,10.52,8.74" target="#b3">[4]</ref> and in the 2005 and 2006 visual object classes challenges of PASCAL <ref type="bibr" coords="8,364.80,509.89,14.62,8.74" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Medical Automatic Annotation Task</head><p>We submitted three runs to the medical automatic annotation task <ref type="bibr" coords="8,392.56,556.17,14.61,8.74" target="#b27">[28]</ref>: one run using the image distortion model RWTHi6-IDM, with exactly the same settings as the according run from last year, which clearly outperformed all competing methods <ref type="bibr" coords="8,339.34,580.08,15.50,8.74" target="#b9">[10]</ref> and two other runs based on sparse histograms of image patches <ref type="bibr" coords="8,220.27,592.03,9.97,8.74" target="#b5">[6]</ref>, where we used a discriminatively trained log-linear maximum entropy model (RWTHi6-SHME) and support vector machines with a histogram intersection kernel (RWTHi6-SHSVM) respectively. Due to time constraints we were unable to submit the method described in Section 3.3, but we give comparison results here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results.</head><p>The results of the evaluation are given in detail in the overview paper. Table <ref type="table" coords="8,483.67,653.79,4.98,8.74" target="#tab_2">3</ref> gives an overview of the results and it can be seen that the runs using the discriminative classifier for the histograms clearly outperform the image distortion model and that in summary our method performed very good on the task.</p><p>The table also gives the result for the method presented in <ref type="bibr" coords="8,373.26,701.61,10.52,8.74" target="#b7">[8,</ref><ref type="bibr" coords="8,387.88,701.61,7.01,8.74" target="#b8">9]</ref>, which we were unable to submit in time. Interestingly, the results of this method are not very good although it is strongly related to the sparse histogram method. Interesting conclusions can be drawn when comparing our results to the results of other groups: the medical informatics division of the RWTH Aachen University (RWTHmi) method uses the image distortion model as a significant part of their method and combines it with various other global image descriptors, which seem not to help the classification. The ULG run is interesting, as it was one of the best performing methods from last year and is also closely related to our unsubmitted run: it uses sparsely extracted sub-images and a discriminative classification framework. The runs of University Freiburg (UFR) and INSA Rouen (MedIC) are included for comparison with the best results from other groups. A more detailed overview of the results can be found in the track overview paper <ref type="bibr" coords="9,158.96,521.62,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="9,172.80,521.62,11.63,8.74" target="#b27">28]</ref>.</p><p>Concluding it can be seen that the approach, where local image descriptors were extracted at every position in the image, outperformed our other approaches, and that probably the modelling of absolute positions is suitable for radiograph recognition. This is because it seems to be a suitable assumption that radiographs are taken under controlled conditions and that thus the geometric layout of images showing the same body region can be assumed to be very similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Object Annotation Task</head><p>We submitted two runs to this task <ref type="bibr" coords="9,250.66,627.67,9.97,8.74" target="#b2">[3]</ref>, one using the method with vector quantized histograms described in Section 3.3 (run-tag PatchHisto) and the other using the method with sparse histograms as described in Section 3.2 (run-tag SHME). These two methods were also used in the PASCAL visual object classes challenge 2006. The third method <ref type="bibr" coords="9,379.07,663.54,15.50,8.74" target="#b17">[18]</ref> we submitted to the PAS-CAL challenge could not be applied to this task due to time and memory constraints.</p><p>Results. Table <ref type="table" coords="9,171.54,701.39,4.98,8.74" target="#tab_3">4</ref> gives the results of the object annotation task. On the average, the error rates are very high. The best two results of 77.3% and 80.2% were achieved with our discriminative classification method. For the submissions of the CINDI group, support vector machines were used and the DEU-CS group used a nearest neighbor classification. Obviously, the results are not satisfactory and large improvements should be possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Outlook</head><p>We have presented our efforts for the ImageCLEF 2006 image retrieval and annotation challenge. In particular, we presented a discriminative method to train weights to combine features in our image retrieval system. This method allows to find weights that clearly outperform a setting with feature weights chosen from experiences from earlier experiments and thus allows us to obtain better results than our best old system. We give an interpretation of the trained weights and show the development of the weights given different number of training iterations.</p><p>The maximum entropy principle was futhermore used for automatic image annotation and very good results were obtained.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,116.61,118.94,369.78,133.28"><head>Table 2 :</head><label>2</label><figDesc>Results from the AdHoc and the Photo task.</figDesc><table coords="7,116.61,131.56,369.78,120.65"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(b) Results from the photo retrieval task with 30</cell></row><row><cell cols="4">(a) Results from the adhoc retrieval task with</cell><cell cols="4">queries. All submissions to this task were sub-</cell></row><row><cell cols="4">60 queries in the category "visual only, full au-</cell><cell cols="4">mitted as full automatic, visual only submissions</cell></row><row><cell cols="2">tomatic, no user interaction".</cell><cell></cell><cell></cell><cell cols="2">without user feedback.</cell><cell></cell><cell></cell></row><row><cell>task</cell><cell>run-tag</cell><cell cols="2">map rank</cell><cell>task</cell><cell>run-tag</cell><cell cols="2">map rank</cell></row><row><cell cols="2">RWTHi6 IFHTAM</cell><cell>0.06</cell><cell>1</cell><cell cols="2">RWTHi6 IFHTAM</cell><cell>0.11</cell><cell>1</cell></row><row><cell cols="3">RWTHi6 PatchHisto 0.05</cell><cell>2</cell><cell cols="3">RWTHi6 PatchHisto 0.08</cell><cell>2</cell></row><row><cell>CEA</cell><cell>mPHic</cell><cell>0.05</cell><cell>3</cell><cell>IPAL</cell><cell>LSA3</cell><cell>0.07</cell><cell>3</cell></row><row><cell>CEA</cell><cell>2mPHit</cell><cell>0.04</cell><cell>4</cell><cell>IPAL</cell><cell>LSA2</cell><cell>0.06</cell><cell>5</cell></row><row><cell>IPAL</cell><cell>LSA</cell><cell>0.03</cell><cell>5</cell><cell>IPAL</cell><cell>LSA1</cell><cell>0.06</cell><cell>4</cell></row><row><cell>IPAL</cell><cell>MF</cell><cell>0.02</cell><cell>6</cell><cell>IPAL</cell><cell>MF</cell><cell>0.04</cell><cell>6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,90.00,118.94,423.00,141.34"><head>Table 3 :</head><label>3</label><figDesc>An overview of the results of the medical automatic annotation task. The first part gives our results (including the error rate of an unsubmitted method for comparison to the results of last year); the second part gives results from other groups that are interesting for comparison</figDesc><table coords="9,170.41,155.10,262.19,105.18"><row><cell>rank run-tag</cell><cell>error rate[%]</cell></row><row><cell>1 RWTHi6 SHME</cell><cell>16.2</cell></row><row><cell>2 RWTHi6 SHSVM</cell><cell>16.7</cell></row><row><cell>11 RWTHi6 IDM</cell><cell>20.5</cell></row><row><cell>-RWTHi6 -[8]</cell><cell>22.4</cell></row><row><cell>2 UFR ns1000-20x20x10</cell><cell>16.7</cell></row><row><cell>4 MedIC-CISMef local+global-PCA335</cell><cell>17.2</cell></row><row><cell>12 RWTHmi rwthmi</cell><cell>21.5</cell></row><row><cell>23 ULG sysmod-random-subwindows-ex</cell><cell>29.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,186.67,284.36,229.65,116.48"><head>Table 4 :</head><label>4</label><figDesc>Results from the object annotation task.</figDesc><table coords="9,186.67,296.06,229.65,104.78"><row><cell cols="2">rank Group ID run-tag</cell><cell>Error rate</cell></row><row><cell>1 RWTHi6</cell><cell>SHME</cell><cell>77.3</cell></row><row><cell>2 RWTHi6</cell><cell>PatchHisto</cell><cell>80.2</cell></row><row><cell>3 CINDI</cell><cell>SVM-Product</cell><cell>83.2</cell></row><row><cell>4 CINDI</cell><cell>SVM-EHD</cell><cell>85.0</cell></row><row><cell>5 CINDI</cell><cell>SVM-SUM</cell><cell>85.2</cell></row><row><cell>6 CINDI</cell><cell>Fusion-knn</cell><cell>87.1</cell></row><row><cell>7 DEU-CS</cell><cell>edgehistogr-centroid</cell><cell>88.2</cell></row><row><cell>8 DEU-CS</cell><cell>colorlayout-centroid</cell><cell>93.2</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,114.91,608.13,398.10,8.74;3,114.91,620.09,241.82,8.74"><p>in the first phase, sub-images are extracted from all training images and the dimensionality is reduced to 40 dimensions using PCA transformation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,114.91,640.01,398.10,8.74;3,114.91,651.97,262.56,8.74"><p>in the second phase, the sub-images of all training images are jointly clustered using the EM algorithm for Gaussian mixtures to form 2000-8000 clusters.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,114.91,671.89,398.10,8.74;3,114.91,683.85,398.09,8.74;3,114.91,695.80,398.10,8.74;3,114.91,707.76,45.39,8.74"><p>in the third phase, all information about each sub-image is discarded except its closest cluster center. Then, for each image a histogram over the cluster identifiers of the respective patches is created, thus effectively coding which "visual words" from the code-book occur in the image.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work was partially funded by the <rs type="funder">DFG (Deutsche Forschungsgemeinschaft)</rs> under contract <rs type="grantNumber">NE-572/6</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_3uaJSdv">
					<idno type="grant-number">NE-572/6</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,110.48,370.17,402.52,8.74;10,110.48,382.13,402.52,8.74;10,110.48,394.08,86.07,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,256.49,370.17,251.98,8.74">Maximum Entropy Models for Named Entity Recognition</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,123.03,382.13,267.29,8.74">7th Conference on Computational Natural Language Learning</title>
		<meeting><address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-05">May 2003</date>
			<biblScope unit="page" from="148" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,412.62,402.52,8.74;10,110.48,424.58,365.64,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,361.33,412.62,151.67,8.74;10,110.48,424.58,126.08,8.74">A Maximum Entropy Approach to Natural Language Processing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">J</forename><surname>Della</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pietra</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,245.41,424.58,113.08,8.74">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="72" />
			<date type="published" when="1996-03">March 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,443.12,402.51,8.74;10,110.48,455.07,402.52,8.74;10,110.48,467.03,144.49,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,421.66,443.12,91.33,8.74;10,110.48,455.07,288.16,8.74">Overview of the Im-ageCLEF 2006 photographic retrieval and object annotation tasks</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,419.93,455.07,88.83,8.74">CLEF working notes</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09">September 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,485.57,402.52,8.74;10,110.48,497.52,402.53,8.74;10,110.48,509.48,402.52,8.74;10,110.48,521.44,111.42,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,110.48,497.52,238.51,8.74">The CLEF 2005 Cross-Language Image Retrieval Track</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,369.35,497.52,143.65,8.74;10,110.48,509.48,139.03,8.74">Workshop of the Cross-Language Evaluation Forum (CLEF 2005)</title>
		<title level="s" coord="10,257.36,509.48,152.80,8.74">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09">September 2005</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct coords="10,110.48,539.98,402.52,8.74;10,110.48,551.93,397.46,8.74" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="10,169.40,539.98,120.47,8.74">Features for Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003-12">December 2003</date>
			<pubPlace>Aachen, Germany</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Human Language Technology and Pattern Recognition Group ; RWTH Aachen University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Diploma thesis</note>
</biblStruct>

<biblStruct coords="10,110.48,570.47,402.52,8.74;10,110.48,582.43,402.52,8.74;10,110.48,594.38,402.52,8.74;10,110.48,606.34,71.98,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,352.38,570.47,160.63,8.74;10,110.48,582.43,146.18,8.74">Sparse Patch-Histograms for Object Classification in Cluttered Images</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hegerath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,278.80,582.43,234.20,8.74;10,110.48,594.38,28.34,8.74">DAGM 2006, Pattern Recognition, 26th DAGM Symposium</title>
		<title level="s" coord="10,213.99,594.38,150.37,8.74">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09">September 2006</date>
			<biblScope unit="volume">4174</biblScope>
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,624.88,402.51,8.74;10,110.48,636.83,402.51,8.74;10,110.48,648.79,351.73,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,274.90,624.88,238.10,8.74;10,110.48,636.83,15.99,8.74">Features for Image Retrieval -A Quantitative Comparison</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,146.31,636.83,255.70,8.74">DAGM 2004, Pattern Recognition, 26th DAGM Symposium</title>
		<title level="s" coord="10,480.18,636.83,32.82,8.74;10,110.48,648.79,116.73,8.74">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Tbingen, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-09">September 2004</date>
			<biblScope unit="volume">3175</biblScope>
			<biblScope unit="page" from="228" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,667.33,402.52,8.74;10,110.48,679.28,402.53,8.74;10,110.48,691.24,185.75,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,280.45,667.33,232.56,8.74;10,110.48,679.28,60.78,8.74">Discriminative Training for Object Recognition using Image Patches</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,191.41,679.28,272.69,8.74">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06">June 2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="157" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,709.78,402.51,8.74;10,110.48,721.73,402.52,8.74;10,110.48,733.69,402.52,8.74;10,110.48,745.64,22.69,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,274.83,709.78,238.17,8.74;10,110.48,721.73,116.61,8.74">Improving a Discriminative Approach to Object Recognition using Image Patches</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,249.15,721.73,258.69,8.74">DAGM 2005, Pattern Recognition, 26th DAGM Symposium</title>
		<title level="s" coord="10,180.58,733.69,151.48,8.74">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-08">August 2005</date>
			<biblScope unit="volume">3663</biblScope>
			<biblScope unit="page" from="326" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,112.02,402.52,8.74;11,110.48,123.98,402.52,8.74;11,110.48,135.93,402.52,8.74;11,110.48,147.89,210.02,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,395.46,112.02,117.54,8.74;11,110.48,123.98,338.89,8.74">FIRE in ImageCLEF 2005: Combining Content-based Image Retrieval with Textual Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,469.82,123.98,43.18,8.74;11,110.48,135.93,238.98,8.74">Workshop of the Cross-Language Evaluation Forum (CLEF 2005)</title>
		<title level="s" coord="11,357.06,135.93,151.65,8.74">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09">September 2005</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct coords="11,110.48,167.20,402.51,8.74;11,110.48,179.15,335.02,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,223.22,167.20,256.25,8.74">Object class recognition using discriminative local features</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Dork</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,488.89,167.20,24.11,8.74;11,110.48,179.15,257.89,8.74">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>submitted</note>
</biblStruct>

<biblStruct coords="11,110.48,198.47,402.52,8.74;11,110.48,210.42,402.52,8.74;11,110.48,222.38,402.52,8.74;11,110.48,234.33,402.52,8.74;11,110.48,246.29,402.52,8.74;11,110.48,258.24,402.53,8.74;11,110.48,270.20,402.52,8.74;11,110.48,282.15,402.52,8.74;11,110.48,294.11,22.69,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,381.12,246.29,131.88,8.74;11,110.48,258.24,94.75,8.74">The 2005 PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Dorko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Duffner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Eichhorn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D R</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koskela</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Laaksonen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Seemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Viitaniemi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,227.28,258.24,285.73,8.74;11,110.48,270.20,394.01,8.74">Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment (PASCAL Workshop 05</title>
		<title level="s" coord="11,183.60,282.15,168.24,8.74">Lecture Notes in Artificial Intelligence</title>
		<meeting><address><addrLine>Southampton, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3944</biblScope>
			<biblScope unit="page" from="117" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,313.42,402.51,8.74;11,110.48,325.37,402.52,8.74;11,110.48,337.33,116.09,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,499.85,313.42,13.14,8.74;11,110.48,325.37,206.84,8.74">Efficient and Effective Querying by Image Content</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Flickner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Niblack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Petkovic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Equitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,325.67,325.37,182.66,8.74">Journal of Intelligent Information Systems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="231" to="262" />
			<date type="published" when="1994-07">July 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,356.64,402.51,8.74;11,110.48,368.60,402.52,8.74;11,110.48,380.55,370.82,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,350.65,356.64,162.34,8.74;11,110.48,368.60,210.61,8.74">The IAPR Benchmark: A New Evaluation Resource for Visual Information Systems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,345.26,368.60,167.74,8.74;11,110.48,380.55,195.99,8.74">LREC 06 OntoImage 2006: Language Resources for Content-Based Image Retrieval</title>
		<meeting><address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-05">May 2006</date>
		</imprint>
	</monogr>
	<note>page in press</note>
</biblStruct>

<biblStruct coords="11,110.48,399.86,402.52,8.74;11,110.48,411.82,402.52,8.74;11,110.48,423.77,387.91,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,143.05,411.82,369.95,8.74;11,110.48,423.77,115.70,8.74">Comparison of Techniques for Measuring Cloud Texture in Remotely Sensed Satellite Meteorological Image Data</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">N</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Renshaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Mugglestone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">F N</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">M</forename><surname>Grant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,235.44,423.77,122.69,8.74">Radar and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="236" to="248" />
			<date type="published" when="1989-10">October 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,443.09,402.52,8.74;11,110.48,455.04,125.95,8.74" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Habercker</surname></persName>
		</author>
		<title level="m" coord="11,180.17,443.09,264.75,8.74">Praxis der Digitalen Bildverarbeitung und Mustererkennung</title>
		<meeting><address><addrLine>Mnchen, Wien</addrLine></address></meeting>
		<imprint>
			<publisher>Carl Hanser Verlag</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,474.35,402.52,8.74;11,110.48,486.31,375.29,8.74" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,329.50,474.35,179.47,8.74">Texture Features for Image Classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,110.48,486.31,236.37,8.74">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="610" to="621" />
			<date type="published" when="1973-11">November 1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,505.62,402.51,8.74;11,110.48,517.58,402.52,8.74;11,110.48,529.53,207.89,8.74" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="11,289.30,505.62,223.69,8.74;11,110.48,517.58,145.63,8.74">Patch-based Object Recognition Using Discriminatively Trained Gaussian Mixtures</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hegerath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,279.76,517.58,227.54,8.74">17th British Machine Vision Conference (BMVC06)</title>
		<meeting><address><addrLine>Edinburgh, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09">September 2006</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct coords="11,110.48,548.84,402.52,8.74;11,110.48,560.80,402.52,8.74;11,110.48,572.75,22.69,8.74" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="11,235.18,548.84,258.33,8.74">Using Maximum Entropy for Automatic Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,110.48,560.80,340.17,8.74">Proceedings of the 3rd International Conference on Image and Video Retrieval</title>
		<meeting>the 3rd International Conference on Image and Video Retrieval</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="24" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,592.07,402.51,8.74;11,110.48,604.02,402.53,8.74;11,110.48,615.98,22.69,8.74" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="11,273.76,592.07,239.23,8.74;11,110.48,604.02,62.10,8.74">Classification of Medical Images using Non-linear Distortion Models</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gollan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,193.93,604.02,138.14,8.74">Bildverarbeitung für die Medizin</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-03">March 2004</date>
			<biblScope unit="page" from="366" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,635.29,402.51,8.74;11,110.48,647.24,402.52,8.74;11,110.48,659.20,140.11,8.74" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="11,280.70,635.29,232.29,8.74;11,110.48,647.24,83.31,8.74">Maximum Entropy and Gaussian Models for Image Object Recognition</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F.-J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,217.28,647.24,198.14,8.74">Pattern Recognition, 24th DAGM Symposium</title>
		<meeting><address><addrLine>Zürich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-09">September 2002</date>
			<biblScope unit="page" from="498" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,678.51,402.51,8.74;11,110.48,690.47,402.52,8.74;11,110.48,702.42,94.67,8.74" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="11,266.23,678.51,246.76,8.74;11,110.48,690.47,49.69,8.74">Classification of Medical Images using Non-linear Distortion Models</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gollan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,182.75,690.47,219.92,8.74">Proc. BVM 2004, Bildverarbeitung für die Medizin</title>
		<meeting>BVM 2004, Bildverarbeitung für die Medizin<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-03">March 2004</date>
			<biblScope unit="page" from="366" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,721.73,402.52,8.74;11,110.48,733.69,402.52,8.74;11,110.48,745.64,245.61,8.74" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="11,277.51,721.73,235.50,8.74;11,110.48,733.69,157.10,8.74">Local Context in Non-linear Deformation Models for Handwritten Character Recognition</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gollan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,293.52,733.69,215.10,8.74">International Conference on Pattern Recognition</title>
		<meeting><address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-08">August 2004</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="511" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.48,112.02,402.51,8.74;12,110.48,123.98,402.52,8.74;12,110.48,135.93,131.65,8.74" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="12,330.36,112.02,182.63,8.74;12,110.48,123.98,112.70,8.74">Adaptation in Statistical Pattern Recognition using Tangent Vectors</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dahmen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,230.24,123.98,278.75,8.74">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="269" to="274" />
			<date type="published" when="2004-02">February 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.48,155.86,402.52,8.74;12,110.48,167.81,402.52,8.74;12,110.48,179.77,294.20,8.74" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="12,293.12,155.86,219.88,8.74;12,110.48,167.81,142.67,8.74">A Maximum Entropy Framework for PArt-Based Texture and Object Recognition</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,279.06,167.81,233.94,8.74;12,110.48,179.77,45.91,8.74">IEEE International Conference on Computer Vision (ICCV 05)</title>
		<meeting><address><addrLine>Bejing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-10">October 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="832" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.48,199.69,402.51,8.74;12,110.48,211.65,402.52,8.74;12,110.48,223.60,38.74,8.74" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="12,336.39,199.69,176.61,8.74;12,110.48,211.65,56.41,8.74">Random Subwindows for Robust Image Classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mare</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Geurts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Piater</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wehenkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,187.70,211.65,275.54,8.74">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="34" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.48,243.53,402.52,8.74;12,110.48,255.48,402.52,8.74;12,110.48,267.44,377.14,8.74" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="12,353.95,243.53,159.05,8.74;12,110.48,255.48,319.56,8.74">Predicting Customer Behavior using Naive Bayes and Maximum Entropy -Winning the Data-Mining-Cup 2004</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mauser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bezrukov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,449.46,255.48,63.54,8.74;12,110.48,267.44,151.83,8.74">Informatiktage 2005 der Gesellschaft fr Informatik</title>
		<meeting><address><addrLine>St. Augustin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-04">April 2005</date>
		</imprint>
	</monogr>
	<note>page in press</note>
</biblStruct>

<biblStruct coords="12,110.48,287.36,402.51,8.74;12,110.48,299.32,402.52,8.74;12,110.48,311.27,102.43,8.74" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="12,407.19,287.36,105.80,8.74;12,110.48,299.32,241.65,8.74">Overview of the Image-CLEFmed 2006 medical retrieval and annotation tasks</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,375.87,299.32,89.96,8.74">CLEF working notes</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09">September 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.48,331.20,402.52,8.74;12,110.48,343.15,402.52,8.74;12,110.48,355.11,216.35,8.74" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="12,349.37,331.20,163.64,8.74;12,110.48,343.15,135.54,8.74">Empirical Evaluation of Dissimilarity Measures for Color and Texture</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Rubner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,266.27,343.15,197.72,8.74">International Conference on Computer Vision</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-09">September 1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1165" to="1173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.48,375.03,402.52,8.74;12,110.48,386.99,305.56,8.74" xml:id="b29">
	<monogr>
		<title level="m" type="main" coord="12,170.95,375.03,235.94,8.74">Feature Histograms for Content-Based Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Siggelkow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<pubPlace>Freiburg, Germany</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Freiburg, Institute for Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="12,110.48,406.91,402.51,8.74;12,110.48,418.87,402.52,8.74;12,110.48,430.82,205.94,8.74" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="12,419.13,406.91,93.86,8.74;12,110.48,418.87,165.34,8.74">Content-Based Image Retrieval at the End of the Early Years</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,283.42,418.87,229.58,8.74;12,110.48,430.82,48.17,8.74">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1349" to="1380" />
			<date type="published" when="2000-12">December 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.48,450.75,402.52,8.74;12,110.48,462.70,348.09,8.74" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="12,278.99,450.75,229.54,8.74">Textural Features Corresponding to Visual Perception</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Yamawaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,110.48,462.70,232.30,8.74">IEEE Transaction on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="460" to="472" />
			<date type="published" when="1978-06">June 1978</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
