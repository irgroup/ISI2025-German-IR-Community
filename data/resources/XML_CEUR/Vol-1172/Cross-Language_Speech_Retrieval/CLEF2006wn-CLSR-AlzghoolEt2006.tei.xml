<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,90.12,91.13,414.89,12.58">University of Ottawa&apos;s participation in the CL-SR task at CLEF 2006</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,233.34,129.24,66.09,9.02"><forename type="first">Muath</forename><surname>Alzghool</surname></persName>
							<email>alzghool@site.uottawa.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technology and Engineering</orgName>
								<orgName type="institution">University of Ottawa</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,318.97,129.24,54.43,9.02"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technology and Engineering</orgName>
								<orgName type="institution">University of Ottawa</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,90.12,91.13,414.89,12.58">University of Ottawa&apos;s participation in the CL-SR task at CLEF 2006</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">504C8E0512CDC439360D3493D8E29597</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software Measurement, Performance, Experimentation Information retrieval, speech recognition transcripts, indexing schemes, automatic translation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the second participation of the University of Ottawa group in CLEF, the Cross-Language Spoken Retrieval (CL-SR) task. We present the results of the submitted runs for the English collection and very briefly for the Czech collection, followed by many additional experiments. We have used two Information Retrieval systems in our experiments: SMART and Terrier were tested with many different weighting schemes for indexing the documents and the queries and with several query expansion techniques (including a new method based on log-likelihood scores for collocations). Our experiments showed that query expansion methods do not help much for this collection. We tested whether the new Automatic Speech Recognition transcripts improve the retrieval results; we also tested combinations of different automatic transcripts (with different estimated word error rates). The retrieval results did not improve, probably because the speech recognition errors happened for the words that are important in retrieval, even in the newer ASR2006 transcripts. By using different system settings, we improved on our submitted result for the required run (English queries, title and description) on automatic transcripts plus automatic keywords. We present crosslanguage experiments, where the queries are automatically translated by combining the results of several online machine translation tools. Our experiments showed that high quality automatic translations (for French) led to results comparable with monolingual English, while the performance decreased for the other languages. Experiments on indexing the manual summaries and keywords gave the best retrieval results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper presents the second participation of the University of Ottawa group in CLEF, the Cross-Language Spoken Retrieval (CL-SR) track. We briefly describe the task <ref type="bibr" coords="1,326.35,553.50,15.37,9.02" target="#b9">[10]</ref>. Then, we present our systems, followed by results for the submitted runs for the English collection and very briefly for the Czech collection. We present results for many additional runs for the English collection. We experiment with many possible weighting schemes for indexing the documents and the queries, and with several query expansion techniques. We test with different speech recognition transcripts to see if the word error rate has an impact on the retrieval performance. We describe cross-language experiments, where the queries are automatically translated from French, Spanish, German and Czech into English, by combining the results of several online machine translation (MT) tools. At the end we present the best results when summaries and manual keywords were indexed.</p><p>The CLEF-2006 CL-SR collection includes 8104 English segments, and 105 topics (queries). Relevance judgments were provided for 63 training topics, and later for 33 test topics. In each document (segment), there are six fields that can be used for the official runs: ASRTEXT2003A, ASRTEXT2004A, ASRTEXT2006A, ASRTEXT2006B, AUTOKEYWORD2004A1, and AUTOKEYWORD2004A2. The first four fields are transcripts produced using Automatic Speech Recognition (ASR) systems developed by the IBM T. J. Watson Research Center in three successive years <ref type="bibr" coords="1,231.66,705.60,86.28,9.02">2003, 2004, and 2006</ref>, with different estimated mean word error rates of 44%, 38%, and 25% respectively.</p><p>Among the 8104 segments covered by the test collection, only 7377 segments have the ASRTEXT2006A field. The ASRTEXT2006B field content is identical to the ASRTEXT2006A field if there is ASR output pro-duced by the 2006 system for the segment, or identical to the ASRTEXT2004A if not. Moreover just 7034 segments have ASRTEXT2003A field. The AUTOKEYWORD2004A1 and AUTOKEYWORD2004A2 field contain a set of thesaurus keywords that were assigned automatically using two different k-Nearest Neighbor (kNN) classifiers using only words from the ASRTEXT2004A field of the segment. Among the 8104 segments covered by the test collection, 8071 and 8090 segments have AUTOKEYWORD2004A1 and AUTOKEYWORD2004A2, respectively</p><p>There is also a Czech collection for this year's CL-SR track; the document collection consists of ASR transcripts for 354 interviews in Czech, together with some manually assigned metadata and some automatically generated metadata, and 115 search topics in two languages (Czech and English). The task for this collection is to return a ranked list of time stamps marking the beginning of sections that are relevant to a topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Overview</head><p>The University of Ottawa Cross-Language Information Retrieval (IR) systems were built with off-the-shelf components. For translating the queries from French, Spanish, German, and Czech into English, several free online machine translation tools were used. Their output was merged in order to allow for variety in lexical choices. All the translations of a title made the title of the translated query; the same was done for the description and narrative fields. For the retrieval part, the SMART <ref type="bibr" coords="2,301.76,291.30,11.53,9.02" target="#b1">[2,</ref><ref type="bibr" coords="2,313.29,291.30,7.69,9.02" target="#b8">9]</ref> IR system and the Terrier <ref type="bibr" coords="2,434.14,291.30,11.53,9.02" target="#b0">[1,</ref><ref type="bibr" coords="2,445.66,291.30,7.69,9.02" target="#b5">6]</ref> IR system were tested with many different weighting schemes for indexing the collection and the queries.</p><p>For translating the topics into English we used several online MT tools. The idea behind using multiple translations is that they might provide more variety of words and phrases, therefore improving the retrieval performance. The seven online MT systems that we used for translating from Spanish, French, and German were: For translation the Czech language topics into English we were able to find only one online MT system: http://intertran.tranexp.com/Translate/result.shtml.</p><p>We combined the outputs of the MT systems by simply concatenating all the translations. All seven translations of a title made the title of the translated query; the same was done for the description and narrative fields. We used the combined topics for all the cross-language experiments reported in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Retrieval</head><p>We used two systems in our participation: SMART and Terrier. SMART was originally developed at Cornell University in the 1960s. SMART is based on the vector space model of information retrieval <ref type="bibr" coords="2,459.43,588.72,10.65,9.02" target="#b1">[2]</ref>. It generates weighted term vectors for the document collection. SMART preprocesses the documents by tokenizing the text into words, removing common words that appear on its stop-list, and performing stemming on the remaining words to derive a set of terms. When the IR server executes a user query, the query terms are also converted into weighted term vectors. Vector inner-product similarity computation is then used to rank documents in decreasing order of their similarity to the user query. The newest version of SMART (version 11) offers many state-ofthe-art options for weighting the terms in the vectors. Each term-weighting scheme is described as a combination of term frequency, collection frequency, and length normalization components <ref type="bibr" coords="2,388.68,670.62,10.65,9.02" target="#b7">[8]</ref>.</p><p>In this paper we employ the notation used in SMART to describe the combined schemes: xxx.xxx. The first three characters refer to the weighting scheme used to index the document collection and the last three characters refer to the weighting scheme used to index the query fields. In SMART, we used mainly the lnn.ntn weighting scheme which performs very well in CLEF-CLSR 2005 <ref type="bibr" coords="2,354.27,717.42,10.85,9.02" target="#b3">[4]</ref>; lnn.ntn means that lnn was used for documents and ntn for queries according to the following formulas: Where tf denote the term frequency of a term t in the document or query, N denotes the number of documents in the collection, and n t denotes the number of documents in which the term t occurs.</p><p>We have also used a query expansion mechanism with SMART, which follows the idea of extracting related words for each word in the topics using the Ngram Statistics Package (NSP) <ref type="bibr" coords="3,379.72,142.80,10.65,9.02" target="#b6">[7]</ref>. We extracted the top 6412 pairs of related words based on log likelihood ratios (high collocation scores in the corpus of ASR transcripts), using a window size of 10 words. We chose log-likelihood scores because they are known to work well even when the text corpus is small. For each word in the topics, we added the related words according to this list. We call this approach to relevance feedback SMARTnsp.</p><p>Terrier was originally developed at University of Glasgow. It is based on Divergence from Randomness models (DFR) where IR is seen as a probabilistic process <ref type="bibr" coords="3,308.46,224.70,10.90,9.02" target="#b0">[1,</ref><ref type="bibr" coords="3,322.38,224.70,7.23,9.02" target="#b5">6]</ref>. We experimented with the In(exp)C2 weighting model, one of Terrier's DFR-based document weighting models. Using the In(exp)C2 model, the relevance score of a document d for a query q is given by the formula:</p><formula xml:id="formula_0" coords="3,72.90,254.90,120.12,28.52">∑ ∈ = q t d t w qtf q d sim ) , ( . ) , (</formula><p>where -qtf is the frequency of term t in the query q, -w(t,d) is the relevance score of a document d for the query term t, given by:</p><formula xml:id="formula_1" coords="3,84.24,331.59,223.93,33.72">) 5 . 0 1 log ( ) ) 1 ( 1 ( ) , ( 2 + + × × + × + = e e e t n N tfn tfn n F d t w</formula><p>where -F is the term frequency of t in the whole collection.</p><p>-N is the number of document in the whole collection.</p><p>-n t is the document frequency of t.</p><p>-n e is given by</p><formula xml:id="formula_2" coords="3,144.42,413.54,110.35,31.85">) ) 1 ( 1 ( F t e N n N n - - × =</formula><p>-tfn e is the normalized within-document frequency of the term t in the document d. It is given by the normalization 2 <ref type="bibr" coords="3,123.16,459.72,10.89,9.02" target="#b0">[1,</ref><ref type="bibr" coords="3,136.60,459.72,7.41,9.02" target="#b2">3]</ref>:</p><formula xml:id="formula_3" coords="3,83.70,483.81,144.00,27.91">) _ 1 ( log l l avg c tf tfn e e × + × =</formula><p>where -c is a parameter, for the submitted run, we fix this parameter to 1.</p><p>-tf is the within-document frequency of the term t in the document d.</p><p>-l is the document length and avg_l is the average document length in the whole collection.</p><p>We estimated the parameter c of the normalization 2 formula by running some experiments on the training data, to get the best values for c depending on the topic fields used. We obtained the following values: c=0.75 for queries using the Title only, c=1 for queries using the Title and Description fields, and c=1 for queries using the Title, Description, and Narrative fields. We select the c value that has a best MAP score according to the training data.</p><p>We have also used a query expansion mechanism in Terrier, which follows the idea of measuring divergence from randomness. In our experiments, we applied the Kullback-Leibler (KL) model for query expansion <ref type="bibr" coords="3,495.03,654.72,10.89,9.02" target="#b3">[4,</ref><ref type="bibr" coords="3,508.64,654.72,11.90,9.02" target="#b9">10]</ref>. It is one of the Terrier DFR-based term weighing models. Using the KL model, the weight of a term t in the topranked documents is given by: -lx is the sum of the length of the top-ranked documents, -F is the term frequency of the query term in the whole collection.</p><p>-token c is the total number of tokens in the whole collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Submitted runs</head><p>Table <ref type="table" coords="4,96.30,311.10,5.01,9.02" target="#tab_1">1</ref> shows the results of the submitted results on the test data (33 queries). The evaluation measure we report is the standard measure computed with the trec_eval script: MAP (Mean Average Precision). The information about what fields of the topic were indexed is given in the column named Fields: T for title only, TD for title + description, TDN for title + description + narrative. For each run we include an additional description of the experimental settings and which document fields were indexed. For the uoEnTDt04A06A and uoEnTDNtMan runs we used the indexing scheme In(exp)C2 from Terrier; and for uoEnTDNsQEx04, uoFrTDNs, and uoSpTDNs we used the indexing scheme lnn.ntn from SMART. We used SMARTnsp query expansion for the uoEnTDNsQEx04 run, KL query expansion for uoEnTDNtMan and uoEnTDt04A06A, and we didn't use any query expansion techniques for uoFrTDNs and uoSpTDNs. We also participated in the task for Czech language. We indexed the Czech topics and ASR transcripts. Table <ref type="table" coords="4,85.89,597.00,5.01,9.02" target="#tab_2">2</ref> shows the results of the submitted runs on the test data (29 topics) for the Czech collection. The evaluation measure we report is the mean General Average Precision (GAP), which rewards retrieval of the right timestamps in the collection. MAP scores could not be used because the speech transcripts were not segmented. A default segmentation was provided: one document was produced for every minute of the interview.</p><p>From our results, that used the default segmentation, we note:</p><p>• The mean GAP is very low for all submitted runs (for all teams).</p><p>• There is a big improvement when we indexed the field ENGLISHMANUKEYWORD relative to the case when we indexed CZECHMANUKEYWORD; this means we loose a lot due to the translation tool used to translate the ENGLISHMANUKEYWORD field into Czech.</p><p>• No improvements if CZECHMANUKEYWORD in added to the ASR field.</p><p>• Terrier's results are slightly better than SMART's for the required run.</p><p>In the rest of the paper we focus only on the Eglish CL-SR collection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison of systems and query expansion methods</head><p>Table <ref type="table" coords="5,96.70,275.46,5.01,9.02" target="#tab_3">3</ref> presents results for the best weighting schemes: for SMART we chose lnn.ntn and for Terrier we chose the In(exp)C2 weighting model, because they achieved the best results on the training data. We present results with and without relevance feedback.</p><p>According to Table <ref type="table" coords="5,162.85,310.56,3.76,9.02" target="#tab_3">3</ref>, we note that:</p><p>• Relevance feedback helps to improve the retrieval results in Terrier for TDN, TD, and T for the training data; the improvement was high for TD and T, but not for TDN. For the test data there is a small improvement.</p><p>• NSP relevance feedback with SMART does not help to improve the retrieval for the training data (except for TDN), but it helps for the test data (small improvement). • SMART results are better than Terrier results for the test data, but not for the training data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison of retrieval using various ASR transcripts</head><p>In order to find the best ASR transcripts to use for indexing the segments, we compared the retrieval results when using the ASR transcripts from the years 2003, 2004, and 2006 or combinations. We also wanted to find out if adding the automatic keywords helps to improve the retrieval results. The results of the experiments using Terrier and SMART are shown in Table <ref type="table" coords="5,234.56,595.20,5.01,9.02" target="#tab_4">4</ref> and Table <ref type="table" coords="5,284.48,595.20,3.76,9.02" target="#tab_5">5</ref>, respectively. We note from the experimental results that:</p><p>• Using Terrier, the best field is ASRTEXT2006B which contains 7377 transcripts produced by the ASR system on 2006 and 727 transcripts produced by the ASR system in 2004, this improvement over using only the ASRTEXT2004A field is very. On the other hand, the best ASR field using SMART is ASRTEXT2004A. • Any combination between two ASRTEXT fields does not help to improve the retrieval.</p><p>• Using Terrier and adding the automatic keywords to ASRTEXT2004A improved the retrieval for the training data but not for the test data. For SMART it helps for both the training and the test data. • In general, adding the automatic keywords helps. Adding them to ASRTEXT2003A or ASRTEXT2006B improved the retrieval results for the training and test data. • For the required submission run English TD, the maximum MAP score was obtained by the combination of ASRTEXT 2004A and 2006A plus autokeywords using Terrier (0.0952) or SMART (0.0932) on the training data; on the test data the combination of ASRTEXT 2004A and autokeywords using SMART obtained the highest value, 0.0725, higher than the value we report in Table <ref type="table" coords="6,409.03,85.38,5.01,9.02" target="#tab_1">1</ref> for the submitted run. All the results in the table are from SMART using the lnn.ntn weighting scheme. Since the result of combined translation for each language was better than when using individual translations from each MT tool on the CLEF 2005 CL-SR data <ref type="bibr" coords="6,276.60,731.76,10.66,9.02" target="#b3">[4]</ref>, we used combined translations in our experiments.</p><p>The retrieval results for French translations were very close to the monolingual English results, especially on the training data. On the test data, the results were much worse when using only the titles of the topics, probably because the translations of the short titles were less precise. For translations from the other languages, the retrieval results deteriorate rapidly in comparison with the monolingual results. We believe that the quality of the French-English translations produced by online MT tools was very good, while the quality was lower for Spanish, German and Czech, successively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Manual summaries and keywords</head><p>Table <ref type="table" coords="7,96.47,317.34,5.01,9.02" target="#tab_7">7</ref> presents the results when only the manual keywords and the manual summaries were used. The retrieval performance improved a lot, for topics in all the languages. The MAP score jumped from 0.0654 to 0.2902 for English test data, TDN, with the In(exp)C2 weighting model in Terrier. The results of cross-language experiments on the manual data show that the retrieval results for combined translation for French and Spanish language were very close to the monolingual English results on training data and test data. For all the experiments on manual summaries and keywords, Terrier's results are better than SMART's. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We experimented with two different systems: Terrier and SMART, with various weighting scheme for indexing the document and query terms. We proposed a new approach for query expansion that uses collocations with high log-likelihood ratio. Used with SMART, the method obtained a small improvement on test data (probably not significant). The KL relevance feedback method produced only small improvements with Terrier on test data. So, query expansion methods do not seem to help for this collection. The improvements of mean word error rates in the ASR transcripts (of ASRTEXT2006A relative to ASRTEXT2004A) did not improve the retrieval results. Also, combining different ASR transcripts (with different error rates) did not seem to help.</p><p>For some experiments, Terrier was better than SMART, for other it was not; therefore we cannot clearly choose one or another IR system for this collection.</p><p>The idea of using multiple translations proved to be good. More variety in the translations would be beneficial. The online MT systems that we used are rule-based systems. Adding translations by statistical MT tools might help, since they could produce radically different translations.</p><p>On the manual data, the best MAP score we obtained is around 29%, for the English test topics. On automatically-transcribed data the best result is around 7.6% MAP score. Since the improvement in the ASR word error rate does not improve the retrieval results, as shown from the experiments in section 4.3, we think that the justification for the difference to the manual summaries is due to the fact that summaries contain different words to represent the content of the segments. In future work we plan to investigate methods of removing or correcting some of the speech recognition errors in the ASR contents and to use speech lattices for indexing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,169.08,98.39,3.09,6.26;4,169.32,81.11,3.09,6.26;4,123.66,89.03,3.09,6.26;4,163.26,91.99,7.29,10.74;4,163.02,74.71,7.29,10.74;4,117.36,82.63,7.29,10.74;4,96.06,82.63,3.32,10.74;4,84.24,82.63,7.96,10.74;4,154.98,89.03,3.48,6.26;4,138.60,82.63,15.25,10.74;4,100.14,82.63,3.97,10.74;4,92.10,82.63,3.97,10.74;4,130.14,78.79,6.54,14.60;4,107.28,78.79,6.54,14.60;4,70.92,108.30,24.51,9.02;4,110.82,137.73,8.72,10.87;4,109.08,120.69,12.08,10.87;4,84.42,128.31,7.38,10.87;4,90.72,134.75,3.13,6.34;4,98.76,124.42,6.62,14.78;4,129.30,129.78,14.51,9.02;4,201.42,144.17,3.13,6.34;4,156.90,134.75,3.13,6.34;4,175.32,137.73,26.16,10.87;4,186.90,120.69,7.38,10.87;4,151.02,124.42,20.54,14.78;4,70.92,154.20,271.30,9.02"><head></head><label></label><figDesc>the frequency of the query term in the top-ranked documents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,70.92,426.81,453.57,143.52"><head>Table 1 .</head><label>1</label><figDesc>Results of the five submitted runs, for topics in English, French, and Spanish. The required run (English, title + description) is in bold.</figDesc><table coords="4,86.64,448.92,420.51,121.42"><row><cell cols="2">Language Run</cell><cell>MAP</cell><cell>Fields</cell><cell>Description</cell></row><row><cell>English</cell><cell>uoEnTDNtMan</cell><cell>0.2902</cell><cell>TDN</cell><cell>Terrier:</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>MANUALKEYWORD + SUMMARY</cell></row><row><cell>English</cell><cell cols="2">uoEnTDNsQEx04 0.0768</cell><cell>TDN</cell><cell>SMART: NSP query expansion</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ASRTEXT2004A + AUTOKEYWORD2004A1, A2</cell></row><row><cell>French</cell><cell>uoFrTDNs</cell><cell>0.0637</cell><cell>TDN</cell><cell>SMART:</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ASRTEXT2004A + AUTOKEYWORD2004A1, A2</cell></row><row><cell>Spanish</cell><cell>uoSpTDNs</cell><cell>0.0619</cell><cell>TDN</cell><cell>SMART:</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ASRTEXT2004A + AUTOKEYWORD2004A1, A2</cell></row><row><cell>English</cell><cell cols="2">uoEnTDt04A06A 0.0565</cell><cell>TD</cell><cell>Terrier: ASRTEXT2004A + ASRTEXT2006A +</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>AUTOKEYWORD2004A1, A2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,70.92,73.35,440.38,154.08"><head>Table 2 .</head><label>2</label><figDesc>Results of the five submitted runs for Czech collection. The required run (English, title + description) is in bold.</figDesc><table coords="5,86.64,84.90,420.69,142.54"><row><cell cols="2">Language Run</cell><cell>GAP</cell><cell>Fields</cell><cell>Description</cell></row><row><cell>Czech</cell><cell cols="3">uoCzEnTDNsMan 0.0039 TDN</cell><cell>SMART: ASRTEXT, CZECHAUTOKEYWORD,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CZECHMANUKEYWORD, ENGLISH</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>MANUKEYWORD, ENGLISHAUTOKEYWORD</cell></row><row><cell>Czech</cell><cell>uoCzTDNsMan</cell><cell cols="2">0.0005 TDN</cell><cell>SMART:</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ASRTEXT, ZECHAUTOKEYWORD,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CZECHMANUKEYWORD</cell></row><row><cell>Czech</cell><cell>uoCzTDNs</cell><cell cols="2">0.0004 TDN</cell><cell>SMART:</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ASRTEXT, CZECHAUTOKEYWORD</cell></row><row><cell>Czech</cell><cell>uoCzTDs</cell><cell cols="2">0.0004 TD</cell><cell>SMART:</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ASRTEXT, CZECHAUTOKEYWORD</cell></row><row><cell>Czech</cell><cell>uoCzEnTDt</cell><cell cols="2">0.0005 TD</cell><cell>Terrier:</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ASRTEXT, CZECHAUTOKEYWORD</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,70.92,406.38,453.57,105.92"><head>Table 3 .</head><label>3</label><figDesc>Results (MAP scores) for Terrier and SMART, with or without relevance feedback, for English topics. In bold are the best scores for TDN, TD, and T.</figDesc><table coords="5,124.38,441.96,337.88,70.34"><row><cell>System</cell><cell>Training</cell><cell></cell><cell></cell><cell>Test</cell><cell></cell><cell></cell></row><row><cell></cell><cell>TDN</cell><cell>TD</cell><cell>T</cell><cell>TDN</cell><cell>TD</cell><cell>T</cell></row><row><cell>1 SMART</cell><cell>0.0954</cell><cell>0.0906</cell><cell>0.0873</cell><cell>0.0766</cell><cell>0.0725</cell><cell>0.0759</cell></row><row><cell>SMARTnsp</cell><cell>0.0923</cell><cell>0.0901</cell><cell>0.0870</cell><cell>0.0768</cell><cell>0.0754</cell><cell>0.0769</cell></row><row><cell>2 Terrier</cell><cell>0.0913</cell><cell>0.0834</cell><cell>0.0760</cell><cell>0.0651</cell><cell>0.0560</cell><cell>0.0656</cell></row><row><cell>TerrierKL</cell><cell>0.0915</cell><cell>0.0952</cell><cell>0.0906</cell><cell>0.0654</cell><cell>0.0565</cell><cell>0.0685</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,70.92,109.02,453.57,250.60"><head>Table 4 .</head><label>4</label><figDesc>Results (MAP scores) for Terrier, with various ASR transcript combinations. In bold are the best scores for TDN, TD, and T.</figDesc><table coords="6,109.14,132.90,373.20,226.72"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Terrier</cell><cell></cell></row><row><cell>Segment fields</cell><cell></cell><cell>Training</cell><cell></cell><cell></cell><cell>Test</cell></row><row><cell></cell><cell>TDN</cell><cell>TD</cell><cell>T</cell><cell>TDN</cell><cell>TD</cell><cell>T</cell></row><row><cell>ASRTEXT 2003A</cell><cell cols="6">0.0733 0.0658 0.0684 0.0560 0.0473 0.0526</cell></row><row><cell>ASRTEXT 2004A</cell><cell cols="2">0.0794 0.0742</cell><cell cols="2">0.0722 0.0670</cell><cell cols="2">0.0569 0.0604</cell></row><row><cell>ASRTEXT 2006A</cell><cell cols="2">0.0799 0.0731</cell><cell cols="2">0.0741 0.0656</cell><cell cols="2">0.0575 0.0576</cell></row><row><cell>ASRTEXT 2006B</cell><cell cols="2">0.0840 0.0770</cell><cell cols="2">0.0776 0.0665</cell><cell cols="2">0.0576 0.0591</cell></row><row><cell>ASRTEXT 2003A+2004A</cell><cell cols="2">0.0759 0.0722</cell><cell cols="2">0.0705 0.0596</cell><cell cols="2">0.0472 0.0542</cell></row><row><cell>ASRTEXT 2004A+2006A</cell><cell cols="2">0.0811 0.0743</cell><cell cols="2">0.0730 0.0638</cell><cell cols="2">0.0492 0.0559</cell></row><row><cell>ASRTEXT 2004A+2006B</cell><cell cols="2">0.0804 0.0735</cell><cell cols="2">0.0732 0.0628</cell><cell cols="2">0.0494 0.0558</cell></row><row><cell>ASRTEXT 2003A+</cell><cell cols="2">0.0873 0.0859</cell><cell cols="2">0.0789 0.0657</cell><cell cols="2">0.0570 0.0671</cell></row><row><cell>AUTOKEYWORD2004A1,A2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ASRTEXT 2004A+</cell><cell cols="6">0.0915 0.0952 0.0906 0.0654 0.0565 0.0685</cell></row><row><cell>AUTOKEYWORD2004A1, A2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ASRTEXT 2006B+</cell><cell cols="2">0.0926 0.0932</cell><cell cols="2">0.0909 0.0717</cell><cell cols="2">0.0608 0.0661</cell></row><row><cell>AUTOKEYWORD2004A1,A2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ASRTEXT 2004A+2006A+</cell><cell cols="2">0.0915 0.0952</cell><cell cols="2">0.0925 0.0654</cell><cell cols="2">0.0565 0.0715</cell></row><row><cell>AUTOKEYWORD2004A1, A2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ASRTEXT 2004A+2006B+</cell><cell cols="2">0.0899 0.0909</cell><cell cols="2">0.0890 0.0640</cell><cell cols="2">0.0556 0.0692</cell></row><row><cell>AUTOKEYWORD2004A1,A2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,70.92,386.46,453.57,319.22"><head>Table 5 .</head><label>5</label><figDesc>Results (MAP scores) for Terrier, with various ASR transcript combinations. In bold are the best scores for TDN, TD, and T. Table6presents results for the combined translation produced by the seven online MT tools, from French, Spanish, and German into English, for comparison with monolingual English experiments (the first line in the table).</figDesc><table coords="6,70.92,410.34,406.01,261.08"><row><cell></cell><cell></cell><cell></cell><cell cols="2">SMART</cell><cell></cell></row><row><cell>Segment fields</cell><cell></cell><cell>Training</cell><cell></cell><cell></cell><cell>Test</cell></row><row><cell></cell><cell>TDN</cell><cell>TD</cell><cell>T</cell><cell>TDN</cell><cell>TD</cell><cell>T</cell></row><row><cell>ASRTEXT 2003A</cell><cell cols="6">0.0625 0.0586 0.0585 0.0508 0.0418 0.0457</cell></row><row><cell>ASRTEXT 2004A</cell><cell cols="6">0.0701 0.0657 0.0637 0.0614 0.0546 0.0540</cell></row><row><cell>ASRTEXT 2006A</cell><cell cols="6">0.0537 0.0594 0.0608 0.0455 0.0434 0.0491</cell></row><row><cell>ASRTEXT 2006B</cell><cell cols="6">0.0582 0.0635 0.0642 0.0484 0.0459 0.0505</cell></row><row><cell>ASRTEXT 2003A+2004A</cell><cell cols="6">0.0685 0.0646 0.0636 0.0533 0.0442 0.0503</cell></row><row><cell>ASRTEXT 2004A+2006A</cell><cell cols="6">0.0686 0.0699 0.0696 0.0543 0.0490 0.0555</cell></row><row><cell>ASRTEXT 2004A+2006B</cell><cell cols="6">0.0686 0.0713 0.0702 0.0542 0.0494 0.0553</cell></row><row><cell>ASRTEXT 2003A + AUTOKEYWORD2004A1,A2</cell><cell cols="6">0.0923 0.0847 0.0839 0.0674 0.0616 0.0690</cell></row><row><cell>ASRTEXT 2004A+ AUTOKEYWORD2004A1,A2</cell><cell cols="6">0.0954 0.0906 0.0873 0.0766 0.0725 0.0759</cell></row><row><cell>ASRTEXT 2006B+ AUTOKEYWORD2004A1,A2</cell><cell cols="6">0.0869 0.0892 0.0895 0.0650 0.0659 0.0734</cell></row><row><cell>ASRTEXT 2004A+ 2006A + AUTOKEYWORD2004A1,A2</cell><cell cols="6">0.0903 0.0932 0.0915 0.0654 0.0654 0.0777</cell></row><row><cell>ASRTEXT 2004A +2006B + AUTOKEYWORD2004A1,A2</cell><cell cols="6">0.0895 0.0931 0.0919 0.0652 0.0655 0.0742</cell></row><row><cell>4.4 Cross-language experiments</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="7,70.92,155.55,395.42,113.86"><head>Table 6 .</head><label>6</label><figDesc>Results of the cross-language experiments, where the indexed fields are ASRTEXT2004A, and AUTOKEYWORD2004A1, A2 using SMART with the weighting scheme lnn.ntn.</figDesc><table coords="7,120.72,183.48,345.62,85.94"><row><cell></cell><cell>Language</cell><cell>Training</cell><cell></cell><cell></cell><cell>Test</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>TDN</cell><cell>TD</cell><cell>T</cell><cell>TDN</cell><cell>TD</cell><cell>T</cell></row><row><cell>1</cell><cell>English</cell><cell>0.0954</cell><cell>0.0906</cell><cell>0.0873</cell><cell>0.0766</cell><cell>0.0725</cell><cell>0.0759</cell></row><row><cell>2</cell><cell>French</cell><cell>0.0950</cell><cell>0.0904</cell><cell>0.0814</cell><cell>0.0637</cell><cell>0.0566</cell><cell>0.0483</cell></row><row><cell>3</cell><cell>Spanish</cell><cell>0.0773</cell><cell>0.0702</cell><cell>0.0656</cell><cell>0.0619</cell><cell>0.0589</cell><cell>0.0488</cell></row><row><cell>4</cell><cell>German</cell><cell>0.0653</cell><cell>0.0622</cell><cell>0.0611</cell><cell>0.0674</cell><cell>0.0605</cell><cell>0.0618</cell></row><row><cell>5</cell><cell>Czech</cell><cell>0.0585</cell><cell>0.0506</cell><cell>0.0421</cell><cell>0.0400</cell><cell>0.0309</cell><cell>0.0385</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="7,70.92,399.48,453.58,170.78"><head>Table 7 .</head><label>7</label><figDesc>Results of indexing the manual keywords and summaries, using SMART with weighting scheme lnn.ntn, and Terrier with (In(exp)C2).</figDesc><table coords="7,108.66,423.36,373.35,146.90"><row><cell></cell><cell>Language and System</cell><cell>Training</cell><cell></cell><cell></cell><cell>Test</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>TDN</cell><cell>TD</cell><cell>T</cell><cell>TDN</cell><cell>TD</cell><cell>T</cell></row><row><cell>1</cell><cell>English SMART</cell><cell>0.3097</cell><cell>0.2829</cell><cell>0.2564</cell><cell>0.2654</cell><cell>0.2344</cell><cell>0.2258</cell></row><row><cell>2</cell><cell>English Terrier</cell><cell>0.3242</cell><cell>0.3227</cell><cell>0.2944</cell><cell>0.2902</cell><cell>0.2710</cell><cell>0.2489</cell></row><row><cell>3</cell><cell>French SMART</cell><cell>0.2920</cell><cell>0.2731</cell><cell>0.2465</cell><cell>0.1861</cell><cell>0.1582</cell><cell>0.1495</cell></row><row><cell>4</cell><cell>French Terrier</cell><cell>0.3043</cell><cell>0.3066</cell><cell>0.2896</cell><cell>0.1977</cell><cell>0.1909</cell><cell>0.1651</cell></row><row><cell>5</cell><cell>Spanish SMART</cell><cell>0.2502</cell><cell>0.2324</cell><cell>0.2108</cell><cell>0.2204</cell><cell>0.1779</cell><cell>0.1513</cell></row><row><cell>6</cell><cell>Spanish Terrier</cell><cell>0.2899</cell><cell>0.2711</cell><cell>0.2834</cell><cell>0.2444</cell><cell>0.2165</cell><cell>0.1740</cell></row><row><cell>7</cell><cell>German SMART</cell><cell>0.2232</cell><cell>0.2182</cell><cell>0.1831</cell><cell>0.2059</cell><cell>0.1811</cell><cell>0.1868</cell></row><row><cell>8</cell><cell>German Terrier</cell><cell>0.2356</cell><cell>0.2317</cell><cell>0.2055</cell><cell>0.2294</cell><cell>0.2116</cell><cell>0.2179</cell></row><row><cell>9</cell><cell>Czech SMART</cell><cell>0.1766</cell><cell>0.1687</cell><cell>0.1416</cell><cell>0.1275</cell><cell>0.1014</cell><cell>0.1177</cell></row><row><cell cols="2">10 Czech Terrier</cell><cell>0.1822</cell><cell>0.1765</cell><cell>0.1480</cell><cell>0.1411</cell><cell>0.1092</cell><cell>0.1201</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,110.30,232.47,414.17,8.10;8,124.92,243.03,399.55,8.10;8,124.92,253.59,20.28,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,259.90,232.47,264.56,8.10;8,124.92,243.03,100.02,8.10">Probabilistic models of information retrieval based on measuring the divergence from randomness</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,233.50,243.03,194.79,8.10">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="357" to="389" />
			<date type="published" when="2002-10">October 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.29,264.15,414.23,8.10;8,124.92,274.71,399.54,8.10;8,124.92,285.27,46.08,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,259.82,264.15,215.05,8.10">Automatic retrieval with locality information using SMART</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,492.50,264.15,32.02,8.10;8,124.92,274.71,205.86,8.10">Proceedings of the First Text REtrieval Conference (TREC-1)</title>
		<meeting>the First Text REtrieval Conference (TREC-1)</meeting>
		<imprint>
			<date type="published" when="1993-03">March 1993</date>
			<biblScope unit="page" from="500" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.29,295.83,414.20,8.10;8,124.92,306.39,319.23,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,313.73,295.83,210.76,8.10;8,124.92,306.39,26.50,8.10">An information-theoretic approach to automatic query expansion</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Bigi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,157.48,306.39,184.34,8.10">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2001-01">January 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.29,316.95,414.20,8.10;8,124.92,327.51,399.55,8.10;8,124.92,338.07,49.39,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,272.92,316.95,251.57,8.10;8,124.92,327.51,82.74,8.10">Using various indexing schemes and multiple translations in the CL-SR task at CLEF 2005</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Inkpen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Alzghool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Islam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,225.47,327.51,99.13,8.10">Proceedings of CLEF 2005</title>
		<title level="s" coord="8,331.80,327.51,132.14,8.10">Lecture Notes in Computer Science</title>
		<meeting>CLEF 2005</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.29,348.63,414.19,8.10;8,124.92,359.19,399.58,8.10;8,124.92,369.75,91.97,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,164.76,359.19,326.13,8.10">Building an Information Retrieval Test Collection for Spontaneous Conversational Speech</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Soergel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">C</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gustman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,508.98,359.19,15.52,8.10;8,124.92,369.75,65.11,8.10">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.29,380.31,408.51,8.10;8,124.92,390.87,354.84,8.10;8,125.16,401.43,122.49,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,393.66,380.31,125.15,8.10;8,124.92,390.87,15.78,8.10">Terrier Information Retrieval Platform</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Johnson</surname></persName>
		</author>
		<ptr target="http://ir.dcs.gla.ac.uk/wiki/Terrier" />
	</analytic>
	<monogr>
		<title level="m" coord="8,156.73,390.87,296.89,8.10">Proceedings of the 27th European Conference on Information Retrieval (ECIR 05)</title>
		<meeting>the 27th European Conference on Information Retrieval (ECIR 05)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.30,411.99,414.30,8.10;8,124.92,422.55,399.56,8.10;8,124.92,433.11,72.64,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,227.25,411.99,243.21,8.10">The design, implementation and use of the ngram statistics package</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,480.50,411.99,44.09,8.10;8,124.92,422.55,365.89,8.10">Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics</title>
		<meeting>the Fourth International Conference on Intelligent Text Processing and Computational Linguistics<address><addrLine>Mexico City, Mexico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.30,443.67,414.15,8.10;8,124.92,454.23,186.82,8.10" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="8,167.15,443.67,357.30,8.10;8,124.92,454.23,17.31,8.10">Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Addison-Wesley Publishing Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.29,464.79,414.16,8.10;8,124.92,475.35,128.55,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,231.34,464.79,184.47,8.10">Term-weighting approaches in automatic retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,422.47,464.79,101.98,8.10;8,124.92,475.35,47.06,8.10">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="513" to="523" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,114.42,486.24,410.04,9.02;8,124.92,497.94,399.55,9.02;8,124.92,509.31,124.85,8.10" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,405.10,486.24,119.36,9.02;8,124.92,497.94,161.71,9.02">Overview of the CLEF-2005 Cross-Language Speech Retrieval Track</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Soergel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,303.85,498.63,97.56,8.10">Proceedings of CLEF 2005</title>
		<title level="s" coord="8,408.05,498.63,116.42,8.10;8,124.92,509.31,16.52,8.10">Lecture Notes in Computer Science</title>
		<meeting>CLEF 2005</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
