<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,163.05,146.21,276.88,18.08;1,223.29,168.13,156.42,18.08">CLEF-2006 CL-SR at Maryland: English and Czech</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,266.42,203.19,70.16,10.46"><forename type="first">Jianqiang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Library and Information Studies</orgName>
								<orgName type="institution">State University of New York at Buffalo</orgName>
								<address>
									<postCode>14260</postCode>
									<settlement>Buffalo</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,263.20,272.93,76.60,10.46"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
							<email>oard@glue.umd.edu</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">College of Information Studies</orgName>
								<orgName type="department" key="dep2">Institute for Advanced Computer Studies</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20740</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,163.05,146.21,276.88,18.08;1,223.29,168.13,156.42,18.08">CLEF-2006 CL-SR at Maryland: English and Czech</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4F2A3F42873BA6CC5BC499F43FABFEA0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.3 Information Search and Retrieval Measurement</term>
					<term>Performance</term>
					<term>Experimentation Speech Retrieval</term>
					<term>Cross-Language Information Retrieval</term>
					<term>Statistical Translation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The University of Maryland participated in the English and Czech tasks. For English, one monolingual run using only fields based on fully automatic transcription (the required condition) and one (otherwise identical) cross-language run using French queries were officially scored. Three contrastive runs in which manually generated metadata fields in the English collection were indexed were also officially scored to explore the applicability of recently developed "meaning matching" approaches to cross-language retrieval of manually indexed interviews. Statistical translation models trained on European Parliament proceedings were found to be poorly matched to this task, yielding 38% and 44% of monolingual mean average precision for indexing based on automatic transcription and manually generated metadata, respectively. Weighted use of alternative translations yielded an apparent (but not statistically significant) 7% improvement over one-best translation when bi-directional meaning matching techniques were employed. Results for Czech were not informative in this first year of that task, perhaps because no accommodations were made for the unique characteristics of Czech morphology.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Previous experiments have shown that limitations in Automatic Speech Recognition (ASR) accuracy were an important factor degrading the retrieval effectiveness for spontaneous conversational speech <ref type="bibr" coords="2,122.55,110.53,10.52,10.46" target="#b1">[2,</ref><ref type="bibr" coords="2,137.35,110.53,7.01,10.46" target="#b3">4]</ref>. In this year's CLEF CL-SR track, ASR text with lower word error rate (hence, better recognition accuracy) was provided for the same set of segmented English interviews used in last year's CL-SR track. Therefore, one of our goals was to determine the degree to which improved ASR accuracy could measurably improve retrieval effectiveness. This year's track also introduced a new task, searching unsegmented Czech interviews. Unlike more traditional retrieval tasks, the objective in this case was to find points in the interviews that mark the beginning of relevant segments (i.e., points at which a searcher might wish to begin replay). A new evaluation metric, Generalized Average Precision (GAP), was defined to evaluate system performance on this task. The GAP measure takes into account the distance between each system-suggested start time in a ranked list and the closest start time found by an assessor-the greater the time between the two points, the lower the contribution of that match to a system score. A more detailed description of GAP and details of how it is computed can be found in the track overview paper. In this study, we were interested in evaluating retrieval based on overlapping passages, a retrieval techniques that has proven to be useful in more traditional document retrieval settings, while hopefully also gaining some insight into the suitability of the new evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Techniques</head><p>In this section, we briefly describe the techniques that we used in our study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Combining Evidence</head><p>Both test collections provide several types of data that are associated with the information to be retrieved, so we tried different pre-indexing combinations of that data. For the English test collection, we combined the ASR text generated with the 2004 system and the ASR text generated with the 2006 system, and we compared the result with that obtained by indexing each alone. Our expectation was that the two ASR engines could produce different errors, and that combining their results might therefore yield better retrieval effectiveness. Thesaurus terms generated automatically using a kNN classifier offer some measure of vocabulary expansion, so we added them to the ASR text combination as well.</p><p>The English test collection also contains a rich set of metadata that was produced by human indexers. Specifically, a set of (on average, five) thesaurus terms were manually assigned, and a three-sentence summary was written for each segment. In addition, the first names of persons that were mentioned in each segment are available, even if the name itself was not stated. We combined all three types of human-generated metadata to create an index for a contrastive condition.</p><p>The Czech document collection contains, in addition to ASR text, manually assigned English thesaurus terms, automatic translations of those terms into Czech, English thesaurus terms that were generated automatically using a kNN classifier that was trained on English segments, and automatic translations of those thesaurus terms into Czech. We tried two ways of combining this data. Czech translations of the automatically generated thesaurus terms were combined with Czech ASR text to produce the required automatic run. We also combined all available keywords (automatic and manual, English and Czech) with the ASR text, hoping that some proper names in English might match proper names in Czech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pruning Bidirectional Translations for Cross-language Retrieval</head><p>In our previous study, we found that using several translation alternatives generated from bidirectional statistical translation could significantly outperform techniques that utilize only the most probable translation, but that using all possible translation alternatives was harmful because statistical techniques generate a large number of very unlikely translations <ref type="bibr" coords="2,413.32,700.20,9.96,10.46" target="#b4">[5]</ref>. The usual process for statistically deriving translation models is asymmetric, so we produce an initial bidirectional model by multiplying the translation probabilities between source words and target words from models trained separately in both directions and then renormalizing. This has the effect of driving the modeled probabilities for translation mappings that are not well supported in both directions to relatively low values. Synonymy knowledge (in this case, from round trip translation using cascaded unidirectional mappings) is then used to aggregate probability mass that would otherwise be somewhat eclectically divided across translation pairings that actually share similar meanings.</p><p>To prune the resulting translations, document-language synonym sets that share the meaning of a query word are then arranged in decreasing order of modeled translation probability and a cumulative probability threshold is used to truncate that list. In our earlier study, we found that a cumulative probability threshold of 0.9 typically results in near-peak cross-language retrieval effectiveness. Therefore, in this study, we tried two conditions: one-best translation (a threshold of zero) and multiple alternatives with a threshold of 0.9. We used the same bilingual corpus that we used in last year's experiment. Specifically, to produce a statistical translation table from French to English, we used the freely available GIZA++ toolkit <ref type="bibr" coords="3,122.65,374.55,10.52,10.46" target="#b2">[3]</ref> <ref type="foot" coords="3,133.17,373.47,3.97,7.32" target="#foot_0">1</ref> to train translation models with the Europarl parallel corpus <ref type="bibr" coords="3,415.64,374.55,9.96,10.46" target="#b0">[1]</ref>. Europarl contains 677,913 automatically aligned sentence pairs in English and French from the European Parliament. We stripped accents from every character and filtered out implausible sentence alignments by eliminating sentence pairs that had a token ratio either smaller than 0.2 or larger than 5; that resulted in 672,247 sentence pairs that were actually used. We started with 10 IBM Model 1 iterations, followed by 5 Hidden Markov Model (HMM) iterations, and ending with 5 IBM Model 4 iterations. The result is a a three-column table that specifies, for each French-English word pair, the normalized translation probability of the English word given the French word. Starting from the same aligned sentence pairs we used the same process to produce a second translation table from French to English.</p><p>Due to the time and resource constraints, we were not able to try a similar technique for Czech this year. All of our processing for Czech was, therefore, monolingual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">English Experiment Results</head><p>The required run for the CLEF-2006 CL-SR track called for use of the title and description fields as a basis for formulating queries. We therefore used all words from those fields as the query (a condition we call "TD") for our five official submissions. Stopwords in each query (as well as in each segment) were automatically removed (after translation) by InQuery, which is the retrieval engine that we used for all of our experiments. Stemming of the queries (after translation) and segments was performed automatically by InQuery using kstem. Statistical significance is reported for p &lt; 0.05 by a Wilcoxon signed rank test for paired samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Officially Scored Runs</head><p>Table <ref type="table" coords="3,118.47,678.83,4.98,10.46" target="#tab_0">1</ref> shows the experiment conditions and the Mean Average Precision (MAP) for the five official runs that we submitted. Not surprisingly, every run based on manually created metadata statistically significantly outperformed every run based on automatic data (ASR text and automatic keywords). run name umd.auto umd.asr04a umd.asr06a umd.asr06b MAP 0.0543 0.0514 0.0517 0.0514 Table <ref type="table" coords="4,137.93,148.10,3.87,10.46">2</ref>: Mean average precision (MAP) for 4 monolingual automatic runs, TD queries.</p><p>As Table <ref type="table" coords="4,145.84,190.44,4.98,10.46" target="#tab_0">1</ref> shows, our one officially scored automatic CLIR run, umd.auto.fr0.9 (with a threshold of 0.9) achieved only 38% of the MAP of the corresponding monolingual MAP, a statistically significant difference. We suspect that domain mismatch between the corpus used for training statistical translation model and the document collection might be a contributing factor, but further investigation of this hypothesis is clearly needed. A locally scored one-best contrastive condition (not shown) yielded about the same results.</p><p>Not surprisingly, combining first names, segment summaries, and manually assigned thesaurus terms produced the best retrieval effectiveness as measured by MAP. Among the three runs with manually created metadata, umd.manu.fr0.9 and umd.manu.fr0 are a pair of comparative crosslanguage runs: umd.manu.fr0.9 had a threshold of 0.9 (which usually led to the selection of multiple translations), while umd.manu.fr0 used only the most probable French translation for each English query word. These settings yielded 44% and 41% of the corresponding monolingual MAP, respectively. The 7% apparent relative increase in MAP with a threshold of 0.9 (compared with one-best translation) was not found to be statistically significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Additional Locally Scored Runs</head><p>In additional to the officially scored monolingual run umd.auto that used four automatically generated fields (ASRTEXT2004A, ASRTEXT2006A, AUTOKEYWORD2004A1, and AUTOKEY-WORD2004A2), we scored three additional runs based on automatically generated data locally: umd.asr04a, umd.asr06a, and umd.asr06b. These runs used only the ASRTEXT2004A, ASR-TEXT2006A, or ASRTEXT2006B fields, respectively (ASRTEXT2006A is empty for some segments; in ASRTEXT2004B those segments are filled in with reportedly less accurate data from ASRTEXT2004A). Table <ref type="table" coords="4,205.67,463.86,4.98,10.46">2</ref> shows the MAP for each of these runs and (again) for umd.auto. Combining the four automatically generated fields yielded a slight apparent improvement in MAP over any run that used only one of those four fields, although the differences are not statistically significant. Interestingly, there is no noticeable difference among umd.asr04a, umd.asr06a and umd.asr06b even when average precision is compared on a topic-by-topic basis, despite the fact that the ASR text produced by the 2006 system is reported to have a markedly lower word error rate than that produced by the 2004 system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Detailed Failure Analysis</head><p>To investigate the factors that could have had a major influence on the effectiveness of runs with automatic data, we conducted a topic-by-topic comparison of average precision. To facilitate that analysis, we produced two additional runs with queries that were formed using words from the title field only, with one run searching the ASRTEXT2006B field only (AUTO) and the other the three metadata fields (MANU). We focused our analysis on those topics that have a MAP of 0.2 or larger in the MANU run for consistency with the failure analysis framework that we have applied previously. With this constraint applied, 15 topics remain for analysis. Figure <ref type="figure" coords="4,478.79,653.61,4.98,10.46">1</ref> shows the topic-by-topic comparison of average precision. As we saw in 2006, the difference in average precision between the two conditions is quite large for most topics.</p><p>Using title-only queries allowed us to look at the contribution of each query word in more detail. Specifically, we looked at the number of segments in which query word appears (a statistic normally referred to as "document frequency"). As Table <ref type="table" coords="4,299.78,713.38,4.98,10.46" target="#tab_1">3</ref> shows, there are some marked differences in the prevalence of query term occurrences between automatically and manually generated fields. In last year's study, poor relative retrieval effectiveness with automatically-generated data was most Figure <ref type="figure" coords="5,121.84,374.50,3.87,10.46">1</ref>: Query-by-query comparison of retrieval effectiveness (average precision) between ASR text and metadata, 15 title queries with average precision of metadata equal to or higher than 0.2. in increasing order of (ASR MAP) / (metadata MAP).</p><p>often associated with a failure to recognizing at least one important query word (often a person or location name). This year, by contrast, we see only two obvious cases that fit that pattern ("varian" and "dp"). This may be because proper names are less common in this year's topic titles. Instead, the pattern that we see is that query words actually appears quite often in ASR segments, and in some cases perhaps too often. Said another way, our problem last year was recall; this year our problem seems to be precision. We'll need to actually read some of the ASR text to see if this supposition is supported, of course. But it is intriguing to note that the failure pattern this year seems to be very different from last year's.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Czech Experiment Results</head><p>The University of Maryland was also one of three teams to participate in the first year of the Czech evaluation. Limited time prevented us from implementing any language processing techniques that were specific to Czech, so all of our runs were based on string matching without any morphological normalization. Automatic segmentation into overlapping passages was provided by the organizers , and we used that segmentation unchanged. Our sole research question for Czech was, therefore, whether the new start-time evaluation metric yielded results that could be used to compare variant systems.</p><p>We submitted three officially scored runs for Czech: umd.all used all available fields (both automatically and manually generated, in both English and Czech), umd.asr used only the Czech ASR text, and umd.akey.asr used both ASR text and the automatic Czech translations of the automatically generated thesaurus terms. Table <ref type="table" coords="5,307.96,698.76,4.98,10.46" target="#tab_2">4</ref> shows the resulting mean Generalized Average Precision (mGAP) values. About all that we can conclude form these results is that they do not serve as a result for making meaningful comparisons. We have identified four possible causes that merit investigation: (1) our retrieval system may indeed be performing poorly, <ref type="bibr" coords="5,483.48,734.63,12.73,10.46" target="#b1">(2)</ref>  evaluation metric may have unanticipated weaknesses, (3) the scripts for computing the metric may be producing erroneous values, or (4) the relevance assessments may contain errors. There is some reason to suspect that the problem may lie with our system design, which contains two known weaknesses. Most obviously, Czech is a highly inflected language in which some degree of morphological normalization is more important than it would be, for example, for English. Good morphological analysis tools are available for Czech, so this problem should be easily overcome.</p><p>The second known problem is more subtle: overlapping segmentation can yield redundant highly ranked segments, but the mGAP scoring process penalized redundancy. Failing to prune redundant segments prior to submission likely resulted in a systematic reduction in our scores. It is not clear whether these two explanations together suffice to explain the very low reported scores this year, but the test collection that is now available from this year's Czech task is exactly the resource that we need to answer that question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Earlier experiments with searching broadcast news yielded excellent results, leading to a plausible conclusion that searching speech was a solved problem. As with all such claims, that is both true and false. Techniques for searching broadcast news are now largely well understood, but searching spontaneous conversational speech based solely on automatically generated transcripts remains a very challenging task. We continue to be surprised by some aspects of our English results, and we are only beginning to understand what happens when we look beyond manually segmented English to unsegmented Czech. Among the things that we don't yet understand are the degree to which the differences we observed this year in English are due to differences in the topics or differences in the ASR, how best to affordable analyze retrieval failures when the blame points more to precision than to recall, whether our unexpectedly poor CLIR results were caused solely by a domain mismatch or principally by some other factor, and whether our Czech test collection and retrieval evaluation metric are properly constructed. In each case, we have so far looked at only one small part of the opportunity space, and much more remains to be done. We look forward to this year's CLEF workshop, where we will have much to discuss!</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,104.17,109.49,394.64,89.71"><head>Table 1 :</head><label>1</label><figDesc>Mean average precision (MAP) for official runs, TD queries.</figDesc><table coords="3,104.17,109.49,394.64,67.40"><row><cell>run name</cell><cell>umd.auto</cell><cell>umd.auto.fr0.9</cell><cell>umd.manu</cell><cell cols="2">umd.manu.fr0.9 umd.manu.fr0</cell></row><row><cell>CL-SR?</cell><cell>monolingual</cell><cell>CL-SR</cell><cell>monolingual</cell><cell>CL-SR</cell><cell>CL-SR</cell></row><row><cell>doc fields</cell><cell>ASR 2004A</cell><cell>ASR 2004A</cell><cell>NAME</cell><cell>NAME</cell><cell>NAME</cell></row><row><cell></cell><cell>ASR 2006A</cell><cell>ASR 2006A</cell><cell>manualkey</cell><cell>manualkey</cell><cell>manualkey</cell></row><row><cell></cell><cell cols="3">autokey04A1A2 autokey04A1A2 SUMMARY</cell><cell>SUMMARY</cell><cell>SUMMARY</cell></row><row><cell>MAP</cell><cell>0.0543</cell><cell>0.0209</cell><cell>0.235</cell><cell>0.1026</cell><cell>0.0956</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,499.17,734.63,13.84,10.46"><head>Table 3 :</head><label>3</label><figDesc>Query word statistics in the document collection. ASR#: the number of ASR segments that the query word appears; Metadata#: the number of segments in at least one of the three metadata fields of which the query word appears. Statistics were obtained after both the queries and the document collection were stemmed.</figDesc><table coords="5,499.17,734.63,13.84,10.46"><row><cell>the</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,100.10,673.26,402.81,10.46"><head>Table 4 :</head><label>4</label><figDesc>Mean generalized average precision (mGAP) for the three official runs, TD queries.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,105.24,734.51,306.99,8.37"><p>http://www-i6.informatik.rwth-aachen.de/Colleagues/och/software/GIZA++.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,105.50,507.00,407.52,10.46;7,105.50,518.95,79.49,10.46" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="7,175.12,507.00,299.83,10.46">Europarl: A multilingual corpus for evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>unpublished draft</note>
</biblStruct>

<biblStruct coords="7,105.50,538.87,407.51,10.46;7,105.50,550.83,407.52,10.46;7,105.50,562.78,407.51,10.46;7,105.50,574.74,407.52,10.46;7,105.50,586.70,327.85,10.46" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,260.54,562.78,252.47,10.46;7,105.50,574.74,118.74,10.46">Building an information retrieval test collection for spontaneous conversational speech</title>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dagobert</forename><surname>Soergel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Doermann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">Craig</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bhuvana</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samuel</forename><surname>Gustman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liliya</forename><surname>Kharevych</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephanie</forename><surname>Strassel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,244.60,574.74,268.42,10.46;7,105.50,586.70,239.53,10.46">Proceedings of the 20th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 20th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="41" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,606.62,407.50,10.46;7,105.50,618.58,142.82,10.46" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,213.90,606.62,167.08,10.46">Improved statistical alignment models</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,406.24,606.62,31.32,10.46">ACL&apos;00</title>
		<meeting><address><addrLine>Hongkong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-10">October 2000</date>
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,638.50,407.51,10.46;7,105.50,650.46,407.50,10.46;7,105.50,662.41,22.69,10.46" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,288.95,638.50,224.06,10.46;7,105.50,650.46,197.64,10.46">Clef-2005 cl-sr at maryland: Document and query expansion using side collections and thesauri</title>
		<author>
			<persName coords=""><forename type="first">Jianqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,327.56,650.46,180.33,10.46">Proceedings of the CLEF 2005 Workshop</title>
		<meeting>the CLEF 2005 Workshop</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,682.33,407.53,10.46;7,105.50,694.29,407.51,10.46;7,105.50,706.25,407.50,10.46" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,288.34,682.33,224.69,10.46;7,105.50,694.29,168.48,10.46">Combining bidirectional translation and synonymy for cross-language information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Jianqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,293.42,694.29,219.59,10.46;7,105.50,706.25,316.46,10.46">Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="202" to="229" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
