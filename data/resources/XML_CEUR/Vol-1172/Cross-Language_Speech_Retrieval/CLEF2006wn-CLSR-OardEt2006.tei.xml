<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,112.92,148.86,377.22,15.15;1,203.59,170.78,195.84,15.15">Overview of the CLEF-2006 Cross-Language Speech Retrieval Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,263.20,204.67,76.61,8.74"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
							<email>oard@glue.umd.edu</email>
						</author>
						<author>
							<persName coords="1,266.42,274.41,70.16,8.74"><forename type="first">Jianqiang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName coords="1,262.33,344.15,78.33,8.74"><forename type="first">Gareth</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
							<email>gareth.jones@computing.dcu.ie</email>
						</author>
						<author>
							<persName coords="1,267.11,413.89,68.77,8.74"><forename type="first">Ryen</forename><forename type="middle">W</forename><surname>White</surname></persName>
							<email>ryenw@microsoft.com</email>
						</author>
						<author>
							<persName coords="1,273.83,483.62,55.34,8.74"><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
							<email>pecina@ufal.mff.cuni.cz</email>
						</author>
						<author>
							<persName coords="1,223.78,553.36,75.19,8.74"><forename type="first">Dagobert</forename><surname>Soergel</surname></persName>
							<email>dsoergel@umd.edu</email>
						</author>
						<author>
							<persName coords="1,321.66,553.36,57.57,8.74"><forename type="first">Xiaoli</forename><surname>Huang</surname></persName>
							<email>xiaoli@umd.edu</email>
						</author>
						<author>
							<persName coords="1,271.18,623.10,60.65,8.74"><forename type="first">Izhak</forename><surname>Shafran</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">College of Information Studies</orgName>
								<orgName type="department" key="dep2">Institute for Advanced Computer Studies</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Library and Information Studies</orgName>
								<orgName type="institution">State University of New York at Buffalo</orgName>
								<address>
									<postCode>14260</postCode>
									<settlement>Buffalo</settlement>
									<region>NY</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Computing Dublin</orgName>
								<orgName type="institution">City University</orgName>
								<address>
									<settlement>Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research One Microsoft</orgName>
								<address>
									<addrLine>Way</addrLine>
									<postCode>98052</postCode>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">MFF UK</orgName>
								<address>
									<addrLine>Malostranske namesti 25, Room 422</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Charles University</orgName>
								<address>
									<postCode>118 00</postCode>
									<settlement>Praha 1</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">College of Information Studies</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department">OGI School of Science &amp; Engineering</orgName>
								<orgName type="institution">Oregon Health and Sciences University</orgName>
								<address>
									<addrLine>20000 NW Walker Rd</addrLine>
									<postCode>97006</postCode>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,112.92,148.86,377.22,15.15;1,203.59,170.78,195.84,15.15">Overview of the CLEF-2006 Cross-Language Speech Retrieval Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7A9FDD466915EB1BC6050B4DACA29173</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.3 Information Search and Retrieval Measurement</term>
					<term>Performance</term>
					<term>Experimentation Speech Retrieval</term>
					<term>Evaluation</term>
					<term>Generalized Average Precision</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The CLEF-2006 Cross-Language Speech Retrieval (CL-SR) track included two tasks: to identify topically coherent segments of English interviews in a known-boundary condition, and to identify time stamps marking the beginning of topically relevant passages in Czech interviews in an unknown-boundary condition. Five teams participated in the English evaluation, performing both monolingual and cross-language searches of ASR transcripts, automatically generated metadata, and manually generated metadata. Results indicate that the 2006 evaluation topics are more challenging than those used in 2005, but that cross-language searching continued to pose no unusual challenges when compared with collections of character-coded text. Three teams participated in the Czech evaluation, but no team achieved results comparable to those obtained with English interviews. The reasons for this outcome are not yet clear.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The 2006 Cross-Language Evaluation Forum (CLEF) Cross-Language Speech Retrieval (CL-SR) track continues last year's effort to support research on ranked retrieval from spontaneous conversational speech. Automatically transcribing spontaneous speech has proven to be considerably more challenging than transcribing the speech of news anchors for the Automatic Speech Recognition (ASR) techniques on which fully-automatic content-based search systems are based.</p><p>The CLEF 2005 CL-SR task focused on searching English interviews. For CLEF 2006, 30 new search topics were developed for the same collection, and an improved ASR transcript with better accuracy for the same set of testimonies was added. This made it possible to validate the retrieval techniques that were shown to be effective with last year's topics, and to further explore the influence of ASR accuracy on the retrieval effectiveness. The CLEF 2006 CL-SR track also added a new task of searching Czech interviews.</p><p>Similar to CLEF 2005, the English task is again based on a known-boundary condition for topically coherent segments. The Czech search task is based on a unknown-boundary condition where participants are required to identify a time stamp for the beginning of each distinct topically relevant passage.</p><p>The first part of this paper describes the English language CL-SR task and summarizes the participants' submitted results. This is followed by a description of the Czech language task with corresponding details of submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">English Task</head><p>The structure of the CLEF 2006 CL-SR English task was identical to that used in 2005. Two English collections were released this year. The first release <ref type="bibr" coords="2,344.62,709.78,68.25,8.74">(March 14, 2006</ref>) contained all material that was now available for training (i.e., both the training and the test topics from last year's CLEF 2005 CL-SR evaluation). There was one small difference from the original 2005 data release: each person's last name that appears in the NAME field (or in the associated XML data files) was reduced into its initial followed by three dots (e.g., "Smith" became "S..."). This collection contains a total of 63 search topics, 8,104 topically coherent segments (the equivalent of "documents" in a classic IR evaluation), and 30,497 relevance judgments.</p><p>The second release (June 5, 2006) included a re-release of all the training materials (unchanged) and an additional 42 candidate evaluation topics (30 new topics, plus 12 other topics for which relevance judgments had not previously been released) and two new fields based on an improved ASR transcript from the IBM T. J. Watson Research Center.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Segments</head><p>Other than the changes described above, the segments used for the CLEF 2006 CL-SR task were identical to those used for CLEF 2005. Two new fields contain ASR transcripts of higher accuracy than were available in 2005 (ASRTEXT2006A and ASRTEXT2006B). The ASRTEXT2006A field contains a transcript generated using the best presently available ASR system, which has a mean word error rate of 25% on held-out data. Because of time constraints, however, only 7,378 segments have text in this field. For the remaining 726 segments, no ASR output was available from the 2006A system at the time the collection was distributed. The ASRTEXT2006B field seeks to avoid this no-content condition by including content identical to the ASRTEXT2006A field when available, and content identical to the ASRTEXT2004A field otherwise. Since ASRTEXT2003A, ASRTEXT2004A, and ASRTEXT2006B contain ASR text that was automatically generated for all 8,104 segments, any (or all) of them can be used for the required run based on automatic data. A detailed description of the structure and fields of the English segment collection is given in last year's track overview paper <ref type="bibr" coords="3,212.76,373.49,14.62,8.74" target="#b12">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Topics</head><p>The limited size of the collection would likely make it impractical to continue to do new topic development for the same set of segments in future years, so we elected to use every previously unreleased topic for the CLEF-2006 CLSR English task. A total of 30 new topics were created for this year's evaluation from actual requests received by the USC Shoah Foundation Institute for Visual History and Education. <ref type="foot" coords="3,223.04,466.01,3.97,6.12" target="#foot_0">1</ref> These were combined with 12 topics that had been developed in previous years, but for which relevance judgments had not been released. This resulted in a set of 42 topics that were candidates for use in the evaluation.</p><p>All topics were initially prepared in English. Translations into Czech, Dutch, French, German, Spanish were created by native speakers of those languages, and the same process was used to prepare French translations of the narrative field for all topics in the training collection (which had not been produced in 2005 due to resource constraints). With the exception of Dutch, all translations were checked for reasonableness by a second native speaker of the language. 2  A total of 33 or the 42 candidate topics were used as a basis for the official 2006 CL-SR evaluation; the remaining 9 topics were rejected because they had either too few known relevant segments (fewer than 5) or too high a density of known relevant segments among the available judgments (over 48%, suggesting that many relevant segments may not have been found). Participating teams were asked to submit results for all 105 available topics (the 63 topics in the 2006 training set and the 42 topics in the 2006 evaluation candidate set) so that new pools could be formed to perform additional judgments on the development set if additional assessment resources become available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evaluation Measure</head><p>As in the CLEF-2005 CL-SR track, we report Mean uninterpolated Average Precision (MAP) as the principal measure of retrieval effectiveness. Version 8.0 of the trec eval program was used to compute this measure. <ref type="foot" coords="4,188.10,152.74,3.97,6.12" target="#foot_2">3</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Relevance Judgments</head><p>Subject matter experts created multi-scale and multi-level relevance assessments in the same manner as was done for the CLEF-2005 CL-SR track <ref type="bibr" coords="4,325.10,212.55,14.62,8.74" target="#b12">[11]</ref>. These were then conflated into binary judgments using the same procedure as was used for CLEF-2005: the union of direct indirect relevance judgments with scores of 2, 3, or 4 (on a 0-4 scale) were treated as topically relevant, and any other case as non-relevant. This resulted in a total of 28,223 binary judgments across the 33 topics, among which 2,450 (8.6%) are relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Techniques</head><p>The following gives a brief description of the methods used by the participants in the English task. Additional details are available in each team's paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1">University of Alicante (UA)</head><p>The University of Alicante used the MINIPAR parser to produce an analysis of syntactic dependencies in the topic descriptions and in the automatically generated portion of the collection. The then used these results in combination with their locally developed IR-n system to produce overlapping passages. Their experiments focused on combining these sources of evidence and on optimizing search effectiveness using pruning techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2">Dublin City University (DCU)</head><p>Dublin City University used two systems based on the Okapi retrieval model. One version used Okapi with their summary-based pseudo relevance feedback method. The other system explored combination of multiple segment fields using the method introduced in <ref type="bibr" coords="4,415.92,478.91,9.97,8.74" target="#b9">[8]</ref>. This system also explored the use of a field-based method for term selection in query expansion with pseudorelevance feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.3">University of Maryland (UMD)</head><p>The University of Maryland team tried two techniques, using the InQuery system in both cases <ref type="bibr" coords="4,499.72,547.10,9.97,8.74" target="#b2">[1]</ref>. Four fields of automatic data were combined to create a segment index. Retrieval results from this index were compared with results from index based on individual automatic data field, showing that combining the four automatic data fields could slightly help, although the observed improvement is not statistically significant. Manual metadata fields were also combined in the same, but no comparative results were reported. In addition, the team also applied the so-called "meaning matching" technique to French-English cross-language retrieval. Although there is some sign showing the technique helps marginally, the CLIR effectiveness is significantly worse than monolingual performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.4">Universidad Nacional de Educacin a Distancia (UNED)</head><p>The UNED team compared the utility of the 2006 ASR with manually generated summaries and manually assigned keywords. A CLIR experiment was performed using Spanish queries with the 2006 ASR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.5">University of Ottawa (UO)</head><p>The University of Ottawa used two information retrieval systems in their experiments: SMART <ref type="bibr" coords="5,90.00,142.36,10.52,8.74" target="#b3">[2]</ref> and Terrier <ref type="bibr" coords="5,154.76,142.36,9.97,8.74" target="#b8">[7]</ref>. The two systems were used with many different weighting schemes for indexing the segments and the queries, and with several query expansion techniques (including a new proposed method based on log-likelihood scores for collocations). For the English collection, different Automatic Speech Recognition transcripts (with different estimated word error rates) were used for indexing the segments, and also several combinations of automatic transcripts. Cross-language experiments were run after the topics were automatically translated into English by combining the results of several online machine translation tools. The manual summaries and manual keywords were used for indexing in the manual run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.6">University of Twente (UT)</head><p>The University of Twente employed a locally developed XML retrieval system that supports Narrowed Extended XPath (NEXI) queries to search the collection. They also prepared Dutch translations of the topics that they used as a basis for CLIR experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">English evaluation results</head><p>Table <ref type="table" coords="5,118.68,340.52,4.98,8.74">1</ref> summarizes the results for all 30 official runs averaged over the 33 evaluation topics, listed in descending order of MAP. Required runs are shown in bold. The best results for the required condition (title plus description queries, automatically generated data, from Dublin City University) of 0.0747 are considerably below (i.e., just 58% of) last year's best results. A similar effect was not observed when manually generated metadata were indexed, however with this year's best result (0.2902) being 93% of last year's best manually generated metadata result. From this we conclude that this year's topic set seems somewhat less well matched with the ASR results, but that the topics are not otherwise generally much harder for information retrieval techniques based on term matching. CLIR also seemed to pose no unusual challenges with this year's topic set, with the best CLIR on automatically generated indexing data (a French run from the University of Ottawa) achieving 83% of the MAP achieved by a comparable monolingual run. Similar effects were observed with manually generated metadata (at 80% of the corresponding monolingual MAP for Dutch queries, from the University of Twente).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Czech Task</head><p>The goal of the Czech task was to automatically identify the start points of topically-relevant passages in interviews. Ranked lists for each topic were submitted by each system in the same form as the CLEF ad hoc task, with the single exception that a system-generated starting point was specified rather than a document identifier. The format for this was "VHF[IntCode].[startingtime]," where "IntCode" is the five-digit interview code (with leading zeroes added) and "startingtime" is the system-suggested replay starting point (in seconds) with reference to the beginning of the interview. Lists were to be ranked by systems in the order that they would suggest for listening to passages beginning at the indicated points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Interviews</head><p>The Czech task was broadly similar to the English task in that the goal was to design systems that could help searchers identify sections of an interview that they might wish to listen to. The processing of the Czech interviews was, however, different from that used for English in three important ways:</p><p>• No manual segmentation was performed. This alters the format of the interviews (which for Czech is time-oriented rather than segment-oriented), it alters the nature of the task (which  <ref type="table" coords="6,118.17,465.39,3.88,8.74">1</ref>: English official runs. Bold runs are required. N = Name (Manual metadata), MK = Manual Keywords (Manual metadata), SUM = Summary (Manual metadata), ASR04 = ASR-TEXT2004A (Automatic) AK1 = AUTOKEYWORD2004A1 (Automatic), AK2 = AUTOKEY-WORD2004A2. See <ref type="bibr" coords="6,179.94,501.26,15.50,8.74" target="#b12">[11]</ref> for descriptions of these fields. (Automatic)</p><p>for Czech is to identify replay start points rather than to select among predefined segments), and it alters the nature of the manually assigned metadata (there are no manually written summaries for Czech and the meaning of a manual thesaurus term assignment for Czech is that discussion of a topic started at that time).</p><p>• The two available Czech ASR transcripts were generated using different ASR systems. In both cases, the acoustic models were trained using 15-minutes snippets from 336 speakers, all of whom are present in the test set as well. However, the language model was created by interpolating two models-an in-domain model from transcripts, and an out-of-domain model from selected portions of Czech National Corpus. For details, see the baseline systems described in <ref type="bibr" coords="6,169.75,659.17,10.52,8.74" target="#b10">[9,</ref><ref type="bibr" coords="6,183.15,659.17,11.63,8.74" target="#b11">10]</ref>. Apart from the improvement in transcription accuracy, the 2006 system differs from the 2004 system in that the transcripts are produced in formal Czech, rather than the colloquial Czech that was produced in 2004. Since the topics were written in formal Czech, the 2006 ASR transcripts may yield better matching. Interview-specific vocabulary priming (adding proper names to the recognizer vocabulary based on names present in a preinterview questionnaire) was not done for either Czech system. Thus, a somewhat higher error rate on named entities might be expected for the Czech systems than for the two English systems (2004 and 2006) in which vocabulary priming was included.</p><p>• ASR is available for both the left and right stereo channels (which usually were recorded from microphones with different positions and orientations).</p><p>Because the task design for Czech is not directly compatible with the design of documentoriented IR systems, we provided a "quickstart" package containing the following:</p><p>• A quickstart script for generating overlapping passages directly from the ASR transcripts.</p><p>The passage duration (in seconds), the spacing between passage start times (also in seconds), and the desired ASR system (2004 or 2006) could be specified. The default settings (180, 60, and 2006) result in 3-minute passages in which one minute on each end overlaps with the preceding or subsequent passage.</p><p>• A quickstart collection created by running the quickstart script with the default settings. This collection contains 11,377 overlapping passages.</p><p>The quickstart collection contains the following automatically generated fields:</p><p>DOCNO The DOCNO field contains a unique document number in the same format as the start times that systems were required to produce in a ranked list. This design allowed the output of a typical IR system to be used directly as a list of correctly formatted (although perhaps not very accurate) start times for scoring purposes.</p><p>ASRSYSTEM specifying the source of the ASR text collection (either "2004" for the colloquial Czech system developed by the University of West Bohemia and Johns Hopkins University in 2004 or "2006" for an updated and possibly more accurate formal Czech system provided by the same research groups in 2006).</p><p>CHANNEL The CHANNEL field specifies which recorded channel (left or right) was used to produce the transcript. The channel that produced the greatest number of total words over the entire transcript (which is usually the channel that produced the best ASR accuracy for words spoken by the interviewee) was automatically selected by default. This automatic selection process was hardcoded in the script, although the script could be modified to generate either or both channels.</p><p>ASRTEXT The ASRTEXT field contains words in order from the transcript selected by ASRSYS-TEM and CHANNEL for a passage beginning at the start time indicated in DOCNO. When the selected transcript contains no words at all from that time period, words are drawn from one alternate source that is chosen in the following priority order: (1) the same ASRSYS-TEM from the other CHANNEL, (2) the same CHANNEL from the other ASRSYSTEM, or (3) the other CHANNEL from the other ASRSYSTEM.</p><p>ENGLISHAUTOKEYWORD The ENGLISHAUTOKEYWORD field contains a set of thesaurus terms that were assigned automatically using a k-Nearest Neighbor (kNN) classifier using only words from the ASRTEXT field of the passage; the top 20 thesaurus terms are included in best-first order. Thesaurus terms (which may be phrases) are separated with a vertical bar character. The classifier was trained using English data (manually assigned thesaurus terms and manually written segment summaries) and run using automatically produced English translations of the 2006 Czech ASRTEXT <ref type="bibr" coords="7,365.84,646.02,9.97,8.74" target="#b7">[6]</ref>. Two types of thesaurus terms are present, but not distinguished: (1) terms that express a subject or concept; (2) terms that express a location, often combined with time in one precombined term <ref type="bibr" coords="7,443.76,669.93,9.97,8.74" target="#b6">[5]</ref>. Because the classifier was trained on the English collection, in which thesaurus terms were assigned with segments, the natural interpretation of an automatically assigned thesaurus term is that the classifier believes the indicated topic is associated with the word spoken in this passage. Note that this differs from the way in which the presence of a manually assigned thesaurus term (described below) should be interpreted.</p><p>CZECHAUTOKEYWORD The CZECHAUTOKEYWORD field contains Czech translations of the ENGLISHAUTOKEYWORD field. These translations were obtained from three sources: (1) professional translation of about 3,000 thesaurus terms, (2) volunteer translation of about 700 thesaurus terms, and (3) a custom-built machine translation system that reused words and phrases from manually translated thesaurus terms to produce additional translations. Some words (e.g., foreign place names) remained untranslated when none of the three sources yielded a usable translation.</p><p>Three additional fields containing data produced by human indexers at the Survivors of the Shoah Visual History Foundation were also available for use in contrastive conditions:</p><p>INTERVIEWDATA The INTERVIEWDATA field contains the first name and last initial for the person being interviewed. This field is identical for every passage that was generated from the same interview.</p><p>ENGLISHMANUKEYWORD The ENGLISHMANUALKEYWORD field contains thesaurus terms that were manually assigned with one-minute granularity from a custom-built thesaurus by subject matter experts at the Survivors of the Shoah Visual History Foundation while viewing the interview. The format is the same as that described for the ENGLISHAU-TOKEYWORD field, but the meaning of a keyword assignment is different. In the Czech collection, manually assigned thesaurus terms are used as onset marks-they appear only once at the point where the indexer recognized that a discussion of a topic or location-time pair had started; continuation and completion of discussion are not marked.</p><p>CZECHMANUKEYWORD The CZECHMANUALKEYWORD field contains Czech translations of the English thesaurus terms that were produced from the ENGLISHMANUALKEY-WORD field using the process described above.</p><p>All three teams used the quickstart collection; no other approaches to segmentation and no other settings for passage length or passage start time spacing were tried.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Topics</head><p>At the time the Czech evaluation topics were released, it was not yet clear which of the available topics were likely to yield a sufficient number of relevant passages in the Czech collection. Participating teams were therefore asked to run 115 topics-every available topic at that time. This included the full 105 topic set that was available this year for English (including all training and all evaluation candidate topics) and 10 adaptations of topics from that set in which geographic restrictions had been removed (as insurance against the possibility that the smaller Czech collection might not have adequate coverage for exactly the same topics).</p><p>All 115 topics had originally been constructed in English and then translated into Czech by native speakers. Since translations into languages other than Czech were not available for the 10 adapted topics, only English and Czech topics were distributed with the Czech collection. No teams used the English topics this year; all official runs this year with the Czech collection were monolingual.</p><p>Two additional topics were created as part of the process of training relevance assessors, and those topics were distributed to participants along with a (possibly incomplete) set of relevance judgments. This distribution occurred too late to influence the design of any participating system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation Measure</head><p>The evaluation measure that we chose for Czech is designed to be sensitive to errors in the start time, but not in the end time, of system-recommended passages. It is computed in the same manner as mean average precision, but with one important difference: partial credit is awarded in a way that rewards system-recommended start times that are close to those chosen by assessors.</p><p>After a simulation study, we chose a symmetric linear penalty function that reduces the credit for a match by 0.1 (absolute) for every 15 seconds of mismatch (either early or late) <ref type="bibr" coords="9,473.42,123.97,9.97,8.74" target="#b5">[4]</ref>. This results in the same computation as the well-known mean Generalized Average Precision (mGAP) measure that was introduced to deal with human assessments of partial relevance <ref type="bibr" coords="9,463.53,147.88,9.97,8.74" target="#b4">[3]</ref>. In our case, the human assessments are binary; it is the degree of match to those assessments that can be partial. Relevance judgments are drawn without replacement so that only the highest ranked match (including partial matches) can be scored for any relevance assessment; other potential matches receive a score of zero. Differences at or beyond a 150 second error are treated as a no-match condition, thus not "using up" a relevance assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Relevance Judgments</head><p>Relevance judgments were completed at Charles University in Prague for a total of 29 Czech topics by subject matter experts who were native speakers of Czech. All relevance assessors had good English reading skills. Topic selection was performed by individual assessors, subject to the following factors:</p><p>• At least five relevant start times in the Czech collection were required in order to minimize the effect of quantization noise on the computation of mGAP.</p><p>• The greatest practical degree of overlap with topics for which relevance judgments were available in the English collection was desirable.</p><p>Once a topic was selected, the assessor iterated between topic research (using external resources) and searching the collection. A new search system was designed to support this interactive search process. The best channel of the Czech ASR and the manually assigned English thesaurus terms were indexed as overlapping passages, and queries could be formed using either or both. Once a promising interview was found, an interactive search within the interview could be performed using either type of term and promising regions were identified using a graphical depiction of the retrieval status value. Assessors could then scroll through the interview using these indications, the displayed English thesaurus terms, and the displayed ASR transcript as cues. They could then replay the audio from any point in order to confirm topical relevance. As they did this, they could indicate the onset and conclusion of the relevant period by designating points on the transcript that were then automatically converted to times with 15-second granularity. <ref type="foot" coords="9,115.21,507.41,3.97,6.12" target="#foot_3">4</ref> Only the start times are used for computation of the mGAP measure, but both start and end times are available for future research.</p><p>Once that search-guided relevance assessment process was completed, the assessors were provided with a set of additional points to check for topical relevance that were computed using a pooling technique similar to that used for English. The top 50 start times from every official run were pooled, duplicates (at one minute granularity) were removed, and the results were inserted into the assessment system as system recommendations. Every system recommendation was checked, although assessors exercised judgment regarding when it would be worthwhile to actually listen to the audio in order to limit the cost of this "highly ranked" assessment process. Relevant passages identified in this way were added to those found using search-guided assessment to produce the final set of relevance judgments (topic 4000 was generalized from a pre-existing topic).</p><p>A total of 1,322 start times for relevant passages were identified, thus yielding an average of 46 relevant passages per topic (minimum 8, maximum 124). Table <ref type="table" coords="9,379.00,664.40,4.98,8.74">2</ref> shows the number of relevant start times for each of the 29 topics, 28 of which are the same as topics used in the English test collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Techniques</head><p>The participating teams all employed existing information retrieval systems to perform monolingual searches of the quickstart collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">University of Maryland (UMD)</head><p>The University of Maryland submitted three official runs in which they tried combining all the fields (Czech ASR text, Czech (manual and automatic) keyword, and the English translations of the keywords) to form a unified passage index using Inquery. They compared the retrieval results based on this index with those based on ASR alone or the combination of automatic keywords and ASR text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">University of Ottawa (UO)</head><p>Three runs were submitted from the University of Ottawa for the Czech task using SMART and one run was submitted using Terrier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.3">University of West Bohemia (UWB)</head><p>The University of West Bohemia was the only team to apply morphological normalization and stopword removal for Czech. A classic TF*IDF model was implemented in Lemur, along with the Lemur implementation of blind relevance feedback. Five runs were submitted for official scoring, and one additional run was scored locally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.4">Results</head><p>With two exceptions, the mean Generalized Average Precision (mGAP) values were between 0.0003 and 0.0005. In a side experiment reported in the UWB paper, random permutation of the possible start times was found to yield a mGAP of 0.0005 in a simulation study. We therefore conclude that none of those runs demonstrated any useful degree of system support for the task.</p><p>Two runs yielded more interesting results. The best official run, from UO, achieved a mGAP of 0.0039, and a run that was locally scored at UWB achieved a mGAP of 0.0015. Interestingly, these are two of the three runs in which the ENGLISHMANUALKEYWORD field was used. A positive influence from that factor would require that untranslated English terms (e.g., place names) match terms that were present in the topic descriptions (either with or without morphological normalization). The UWB paper provides an analysis that suggests that the beneficial effect of using that field may be limited to a single topic.</p><p>The use of overlapping passages in the quickstart collection probably reduced mGAP values substantially because the design of the measure tends to penalize duplication. Specifically, the start time of the highest-ranking passage that matches a passage start time in the relevance judgments will "use up" that judgment. Subsequent passages in which the same matching terms were present would then receive no credit at all (even if they were closer matches). We had  originally intended the quickstart collection to be used only for out-of-the-box sanity checks, with the idea that teams would either modify the quickstart scripts or create new systems outright to explore a broader range of possible system designs. Time pressure and a lack of a suitable training collection precluded that sort of experimentation, however, and the result was that this undesirable effect of passage overlap affected every system. Other possible explanations for the relatively poor results also merit further investigation. This is the first time that mGAP has been used in this way to evaluate actual system results, so it is possible that the measure is poorly designed or that there is a bug in the scoring script. Simulation studies suggest that is not likely to be the case, however. This is also the first time that Czech ASR has been used, and it is the first time that relevance assessment has been done in Czech (using a newly designed system). So there are many possible factors that need to be explored. This year's Czech collection is exactly what we need for such an investigation, so it should be possible to make significant process over the next year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Plans</head><p>The CLEF 2006 CL-SR track extended the previous year's work on the English task by adding new topics, and introduced a new Czech task with a new unknown-boundary evaluation condition. The results of the English task suggest that the evaluation topics this year posed somewhat greater difficulty for systems doing fully automatic indexing. Studying what made these topics more difficult would be an interesting scope for future work. However, the most significant achievement of this year's track was the development of a CL-SR test collection based on a more realistic unknown-boundary condition. Now that we have both that collection and an initial set of system designs, we are in a good position to explore issues of system and evaluation design that clearly have not yet been adequately resolved.</p><p>We expect that it would be possible to continue the CLEF CL-SR track in 2007 if there is sufficient interest. For Czech, it may be possible to obtain relevance judgments for additional topics, perhaps increasing to a total of 50 the number of topics that the track can leave as a legacy for use by future researchers. Developing additional topics for English seems to be less urgent (and perhaps less practical), but we do expect to be able to provide additional automatically generated indexing data (either ASR for additional interviews, word lattices in some form, or both) if there is interest in further work with the English collection. Some unique characteristics of the CL-SR collection may also be of interest to other tracks, including domain-specific retrieval and geoCLEF.</p><p>We look forward to discussing these and other issues when we meet in Alicante!</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,90.00,112.80,411.36,361.33"><head>Table</head><label></label><figDesc></figDesc><table coords="6,98.57,112.80,402.79,337.03"><row><cell>Run name</cell><cell>MAP</cell><cell cols="2">Lang Query</cell><cell>Doc field</cell><cell>Site</cell></row><row><cell>uoEnTDNtMan</cell><cell>0.2902</cell><cell>EN</cell><cell>TDN</cell><cell>MK,SUM</cell><cell>UO</cell></row><row><cell>3d20t40f6sta5flds</cell><cell>0.2765</cell><cell>EN</cell><cell>TDN</cell><cell>ASR06B,AK1,AK2,N,SUM,MK</cell><cell>DCU</cell></row><row><cell>umd.manu</cell><cell>0.2350</cell><cell>EN</cell><cell>TD</cell><cell>N,MK,SUM</cell><cell>UMD</cell></row><row><cell>UTsummkENor</cell><cell>0.2058</cell><cell>EN</cell><cell>T</cell><cell>MK,SUM</cell><cell>UT</cell></row><row><cell>dcuEgTDall</cell><cell>0.2015</cell><cell>EN</cell><cell>TD</cell><cell>ASR06B,AK1,AK2,N,SUM,MK</cell><cell>DCU</cell></row><row><cell>uneden-manualkw</cell><cell>0.1766</cell><cell>EN</cell><cell>TD</cell><cell>MK</cell><cell>UNED</cell></row><row><cell>UTsummkNl2or</cell><cell>0.1654</cell><cell>NL</cell><cell>T</cell><cell>MK,SUM</cell><cell>UT</cell></row><row><cell>dcuFchTDall</cell><cell>0.1598</cell><cell>FR</cell><cell>TD</cell><cell>ASR06B,AK1,AK2,N,SUM.MK</cell><cell>DCU</cell></row><row><cell>umd.manu.fr.0.9</cell><cell>0.1026</cell><cell>FR</cell><cell>TD</cell><cell>N,MK,SUM</cell><cell>UMD</cell></row><row><cell>umd.manu.fr.0</cell><cell>0.0956</cell><cell>FR</cell><cell>TD</cell><cell>N,MK,SUM</cell><cell>UMD</cell></row><row><cell>unedes-manualkw</cell><cell>0.0904</cell><cell>ES</cell><cell>TD</cell><cell>MK</cell><cell>UNED</cell></row><row><cell>unedes-summary</cell><cell>0.0871</cell><cell>ES</cell><cell>TD</cell><cell>SUM</cell><cell>UNED</cell></row><row><cell>uoEnTDNsQEx04A</cell><cell>0.0768</cell><cell>EN</cell><cell>TDN</cell><cell>ASR04,AK1,AK2</cell><cell>UO</cell></row><row><cell>dcuEgTDauto</cell><cell>0.0733</cell><cell>EN</cell><cell>TD</cell><cell>ASR06B,AK1,AK2</cell><cell>DCU</cell></row><row><cell>uoFrTDNs</cell><cell>0.0637</cell><cell>FR</cell><cell>TDN</cell><cell>ASR04,AK1,AK2</cell><cell>UO</cell></row><row><cell>uoSpTDNs</cell><cell>0.0619</cell><cell>ES</cell><cell>TDN</cell><cell>ASR04,AK1,AK2</cell><cell>UO</cell></row><row><cell>uoEnTDt04A06A</cell><cell>0.0565</cell><cell>EN</cell><cell>TD</cell><cell>ASR04,ASR06B,AK1,AK2</cell><cell>UO</cell></row><row><cell>umd.auto</cell><cell>0.0543</cell><cell>EN</cell><cell>TD</cell><cell>ASR04,ASR06B,AK1,AK2</cell><cell>UMD</cell></row><row><cell>UTasr04aEN</cell><cell>0.0495</cell><cell>EN</cell><cell>T</cell><cell>ASR04</cell><cell>UT</cell></row><row><cell>dcuFchTDauto</cell><cell>0.0462</cell><cell>FR</cell><cell>TD</cell><cell>ASR06B,AK1,AK2</cell><cell>DCU</cell></row><row><cell>UA TDN FL ASR06BA1A2</cell><cell>0.0411</cell><cell>EN</cell><cell>TDN</cell><cell>ASR06B,AK1,AK2</cell><cell>UA</cell></row><row><cell>UA TDN ASR06BA1A2</cell><cell>0.0406</cell><cell>EN</cell><cell>TDN</cell><cell>ASR06B,AK1,AK2</cell><cell>UA</cell></row><row><cell>UA TDN ASR06BA2</cell><cell>0.0381</cell><cell>EN</cell><cell>TDN</cell><cell>ASR06B,AK2</cell><cell>UA</cell></row><row><cell>UTasr04aNl2</cell><cell>0.0381</cell><cell>NL</cell><cell>T</cell><cell>ASR04</cell><cell>UT</cell></row><row><cell>UTasr04aEN-TD</cell><cell>0.0381</cell><cell>EN</cell><cell>TD</cell><cell>ASR04</cell><cell>UT</cell></row><row><cell>uneden</cell><cell>0.0376</cell><cell>EN</cell><cell>TD</cell><cell>ASR06B</cell><cell>UNED</cell></row><row><cell>UA TD ASR06B</cell><cell>0.0375</cell><cell>EN</cell><cell>TD</cell><cell>ASR06B</cell><cell>UA</cell></row><row><cell>UA TD ASR06BA2</cell><cell>0.0365</cell><cell>EN</cell><cell>TD</cell><cell>ASR06B,AK2</cell><cell>UA</cell></row><row><cell>unedes</cell><cell>0.0257</cell><cell>ES</cell><cell>TD</cell><cell>ASR06B</cell><cell>UNED</cell></row><row><cell>umd.auto.fr.0.9</cell><cell>0.0209</cell><cell>FR</cell><cell>TD</cell><cell>ASR04,ASR06B,AK1,AK2</cell><cell>UMD</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,90.00,279.09,423.00,32.65"><head>Table 3 :</head><label>3</label><figDesc>Czech official runs. Bold runs are required. CAK = CZECHAUTOKEYWORD (Automatic), EAK = ENGLISHAUTOKEYWORD (Automatic), CMK = CZECHMANUKEYWORD (Manual metadata), EMK = ENGLISHMANUKEYWORD (Manual metadata)</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,105.24,663.91,52.41,6.99"><p>On January 1,</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2006" xml:id="foot_1" coords="3,179.46,663.91,333.54,6.99;3,90.00,673.37,422.99,6.99;3,90.00,682.84,348.86,6.99;3,101.09,690.89,411.90,8.44;3,90.00,701.80,213.13,6.99"><p>the University of Southern California (USC) Shoah Foundation Institute for Visual History and Education was established as the successor to the Survivors of the Shoah Visual History Foundation, which had originally assembled and manually indexed the collection used in the CLEF CL-SR track.2 A subsequent quality assurance check for Dutch revealed only a few minor problems. Both the as-run and the final corrected topics will therefore be released for Dutch.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,105.24,727.93,407.75,6.99;4,90.00,737.39,277.20,6.99"><p>The trec eval program is available from http://trec.nist.gov/trec eval/. The DCU results reported in this paper are based on a subsequent re-submission that corrected a formatting error.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="9,105.24,705.30,407.75,6.99;9,90.00,714.76,422.99,6.99;9,90.00,724.23,422.99,6.99;9,90.00,733.69,422.99,6.99;9,90.00,743.16,40.44,6.99"><p>Several different types of time spans arise when describing evaluation of speech indexing systems. For clarity, we have tried to stick to the following terms when appropriate: manually defined segments (for English indexing), 15minute snippets (for ASR training), 15-second increments (for the start and end time of Czech relevance judgments), relevant passages (identified by Czech relevance assessors), and automatically generated passages (for the quickstart collection).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5">Acknowledgments</head><p>This track would not have been possible without the efforts of a great many people. Our heartfelt thanks go to the dedicated group of relevance assessors in Maryland and Prague, to the Dutch, French and Spanish teams that helped with topic translation, and to <rs type="person">Bill Byrne</rs>, <rs type="person">Martin Cetkovsky</rs>, <rs type="person">Bonnie Dorr</rs>, <rs type="person">Ayelet Goldin</rs>, <rs type="person">Sam Gustman</rs>, <rs type="person">Jan Hajic</rs>, <rs type="person">Jimmy Lin</rs>, <rs type="person">Baolong Liu</rs>, <rs type="person">Craig Murray</rs>, <rs type="person">Scott Olsson</rs>, <rs type="person">Bhuvana Ramabhadran</rs> and <rs type="person">Deborah Wallace</rs> for their help with creating the techniques, software, and data sets on which we have relied.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="10,141.36,112.82,320.27,8.74;10,141.36,127.16,19.93,8.74;10,185.43,127.16,4.98,8.74;10,209.58,127.16,19.93,8.74;10,248.94,127.16,9.96,8.74;10,275.58,127.16,19.93,8.74;10,314.95,127.16,9.96,8.74;10,341.59,127.16,19.93,8.74;10,380.95,127.16,9.96,8.74;10,407.59,127.16,19.93,8.74;10,449.44,127.16,4.98,8.74;10,141.36,139.52,19.93,8.74;10,182.94,139.52,9.96,8.74;10,209.58,139.52,19.93,8.74;10,251.43,139.52,4.98,8.74;10,275.58,139.52,19.93,8.74;10,314.95,139.52,9.96,8.74;10,341.59,139.52,19.93,8.74;10,380.95,139.52,9.96,8.74;10,407.59,139.52,19.93,8.74;10,446.95,139.52,9.96,8.74;10,141.36,151.87,24.91,8.74;10,182.94,151.87,9.96,8.74;10,209.58,151.87,19.93,8.74;10,248.94,151.87,9.96,8.74;10,275.58,151.87,19.93,8.74;10,314.95,151.87,9.96,8.74;10,341.59,151.87,19.93,8.74;10,380.95,151.87,9.96,8.74;10,407.59,151.87,19.93,8.74;10,446.95,151.87,9.96,8.74;10,141.36,164.22,19.93,8.74;10,182.94,164.22,9.96,8.74;10,209.58,164.22,19.93,8.74;10,248.94,164.22,9.96,8.74;10,275.58,164.22,19.93,8.74;10,312.46,164.22,49.06,8.74;10,380.95,164.22,9.96,8.74;10,407.59,164.22,19.93,8.74;10,446.95,164.22,9.96,8.74;10,141.36,176.58,19.93,8.74;10,182.94,176.58,9.96,8.74;10,209.58,176.58,19.93,8.74;10,248.94,176.58,9.96,8.74;10,275.58,176.58,19.93,8.74;10,314.95,176.58,9.96,8.74;10,341.59,176.58,19.93,8.74;10,380.95,176.58,9.96,8.74;10,407.59,176.58,19.93,8.74;10,446.95,176.58,9.96,8.74;10,141.36,188.93,19.93,8.74;10,182.94,188.93,9.96,8.74;10,209.58,188.93,19.93,8.74;10,248.94,188.93,9.96,8.74;10,275.58,188.93,19.93,8.74;10,314.95,188.93,9.96,8.74;10,341.59,188.93,19.93,8.74;10,380.95,188.93,9.96,8.74" xml:id="b0">
	<analytic>
		<idno>1166 8 1181 21 1185 50 1187 26 1225 9 1286 70 1288 9 1310 20 1311 27 1321 27 14312 14 1508 83 1620 35 1630 17 1663 34 1843 52 2198 18 2253 124 3004 43 3005 84 3009 77 3014 87 3015 50 3017 83 3018 26 3020</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,376.24,112.82,85.40,8.74">#rel topid #rel</title>
		<imprint>
			<biblScope unit="page" from="67" to="3025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,212.79,423.00,8.74;12,90.00,256.47,75.52,12.62" xml:id="b1">
	<monogr>
		<title level="m" coord="10,229.38,212.79,279.64,8.74">passages identified for each of the 29 topics in the Czech collection</title>
		<imprint/>
	</monogr>
	<note>Table 2: Number of the relevant</note>
</biblStruct>

<biblStruct coords="12,105.49,281.33,407.51,8.74;12,105.50,293.28,389.82,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,302.59,281.33,124.79,8.74">INQUERY System Overview</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Broglio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,449.12,281.33,63.88,8.74;12,105.50,293.28,301.51,8.74">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="47" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.49,313.21,407.52,8.74;12,105.50,325.16,407.51,8.74;12,105.50,337.12,149.30,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,284.88,313.21,228.13,8.74;12,105.50,325.16,32.17,8.74">Automatic retrieval with locality information using SMART</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,158.70,325.16,265.40,8.74">Proceedings of the First Text REtrieval Conference (TREC-1)</title>
		<meeting>the First Text REtrieval Conference (TREC-1)</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="500" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.49,357.04,407.53,8.74;12,105.50,369.00,388.08,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,247.40,357.04,225.40,8.74">Using graded relevance assessments in IR evaluation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kekalainen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Jarvelin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,480.18,357.04,32.84,8.74;12,105.50,369.00,278.84,8.74">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1120" to="1129" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.49,388.92,407.52,8.74;12,105.50,400.88,407.51,8.74;12,105.50,412.83,407.51,8.74;12,105.50,424.79,22.70,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,217.57,388.92,295.44,8.74;12,105.50,400.88,151.17,8.74">One-sided measures for evaluating ranked retrieval effectiveness with spontaneous conversational speech</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,279.73,400.88,233.27,8.74;12,105.50,412.83,332.23,8.74">Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="673" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.49,444.71,407.50,8.74;12,105.50,456.67,407.51,8.74;12,105.50,468.63,241.15,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,349.25,444.71,163.74,8.74;12,105.50,456.67,239.79,8.74">Leveraging Reusability: Cost-effective Lexical Acquisition for Large-scale Ontology Translation</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">J</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hajic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pecina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,363.33,456.67,149.67,8.74;12,105.50,468.63,211.01,8.74">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.49,488.55,407.50,8.74;12,105.50,500.51,407.51,8.74;12,105.50,512.46,164.86,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,295.41,488.55,144.68,8.74">Cross-language text classification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hajic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,461.02,488.55,51.97,8.74;12,105.50,500.51,407.51,8.74;12,105.50,512.46,92.58,8.74">Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="645" to="646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.49,532.39,407.52,8.74;12,105.50,544.34,407.52,8.74;12,105.50,556.30,117.23,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,455.25,532.39,57.76,8.74;12,105.50,544.34,114.13,8.74">Terrier Information Retrieval Platform</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,241.02,544.34,271.99,8.74;12,105.50,556.30,87.07,8.74">Proceedings of the 27th European Conference on Information Retrieval (ECIR 05)</title>
		<meeting>the 27th European Conference on Information Retrieval (ECIR 05)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.49,576.22,407.51,8.74;12,105.50,588.18,407.50,8.74;12,105.50,600.13,142.86,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,314.27,576.22,198.73,8.74;12,105.50,588.18,24.60,8.74">Simple BM25 Extension to Multiple Weighted Fields</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,137.20,588.18,375.79,8.74;12,105.50,600.13,53.34,8.74">Proceedings of the 13th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 13th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.49,620.06,407.52,8.74;12,105.50,632.01,384.17,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,227.98,620.06,285.03,8.74;12,105.50,632.01,35.84,8.74">Task-Specific Minimum Bayes-Risk Decoding using Learned Edit Distance</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,161.59,632.01,188.29,8.74">Proceedings of INTERSPEECH2004-ICSLP</title>
		<meeting>INTERSPEECH2004-ICSLP</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1945" to="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.47,651.94,402.52,8.74;12,105.50,663.89,407.49,8.74;12,105.50,675.85,72.92,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,237.30,651.94,275.70,8.74;12,105.50,663.89,27.09,8.74">Corrective Models for Speech Recognition of Inflected Languages</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,140.26,663.89,372.73,8.74;12,105.50,675.85,41.06,8.74">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.47,695.77,402.52,8.74;12,105.50,707.73,407.52,8.74;12,105.50,719.68,278.46,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,442.70,695.77,70.30,8.74;12,105.50,707.73,222.01,8.74">Overview of the CLEF-2005 Cross-Language Speech Retrieval Track</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Soergel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,334.98,707.73,178.03,8.74;12,105.50,719.68,247.94,8.74">Proceedings of the CLEF 2005 Workshop on Cross-Language Information Retrieval and Evaluation</title>
		<meeting>the CLEF 2005 Workshop on Cross-Language Information Retrieval and Evaluation</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
