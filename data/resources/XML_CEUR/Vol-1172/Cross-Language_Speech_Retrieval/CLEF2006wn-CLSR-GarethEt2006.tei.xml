<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,340.02,148.86,124.50,15.15;1,121.17,170.78,360.67,15.15;1,248.14,192.69,106.73,15.15">at CLEF 2006: Cross-Language Speech Retrieval (CL-SR) Experiments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,169.39,226.59,79.96,8.74"><forename type="first">Gareth</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
							<email>gjones@computing.dcu.ie</email>
						</author>
						<author>
							<persName coords="1,257.14,226.59,42.61,8.74"><forename type="first">Ke</forename><surname>Zhang</surname></persName>
							<email>kzhang@computing.dcu.ie</email>
						</author>
						<author>
							<persName coords="1,322.44,226.59,111.17,8.74"><forename type="first">Adenike</forename><forename type="middle">M</forename><surname>Lam-Adesina</surname></persName>
							<email>adenike@computing.dcu.ie</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Centre for Digital Video Processing</orgName>
								<orgName type="department" key="dep2">School of Computing Dublin</orgName>
								<orgName type="institution">City University</orgName>
								<address>
									<settlement>Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,340.02,148.86,124.50,15.15;1,121.17,170.78,360.67,15.15;1,248.14,192.69,106.73,15.15">at CLEF 2006: Cross-Language Speech Retrieval (CL-SR) Experiments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">94B97EA12D94901D2917BC49B7E2A063</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Managment]: Languages-Query Languages Measurement, Performance, Experimentation Cross-language spoken document retrieval, Multi Field Document Retrieval, Pseudo relevance feedback</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Dublin City University participation in the CLEF 2006 CL-SR task concentrated on exploring the combination of the multiple fields associated with the documents. This was based on use of the extended BM25F field combination model originally developed for multi-field text documents. Additionally, we again conducted runs with our existing information retrieval methods based on the Okapi model. This latter method required an approach to determining approximate sentence boundaries within the free-flowing automatic transcription provided to enable us to use our summarybased pseudo relevance feedback (PRF). Experiments were conducted only for the English document collection. Topics were translated into English using Systran V3.0 machine translation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Dublin City University participation in the CLEF 2006 CL-SR task concentrated on exploring the combination of the multiple fields associated with the speech documents. It is not immediately clear how best to combine the diverse fields of this document set most effectively in ad hoc information retrieval tasks, such as the CLEF 2006 CL-SR task. Our study is based on using the document field combination extended version of BM25 termed BM25F introduced in <ref type="bibr" coords="1,499.72,706.24,9.96,8.74" target="#b0">[1]</ref>. In addition, we carried out runs using our existing information retrieval methods based on the Okapi model to this data set <ref type="bibr" coords="1,217.95,730.15,9.97,8.74" target="#b1">[2]</ref>. Our official submissions included both English monolingual and French bilingual tasks using automatic only and combined automatic and manual fields. Topics were translated into English using the Systran V3.0 machine translation system. The resulting translated English topics were applied to the English document collection.</p><p>The remainder of this paper is structured as follows: Section 2 summarises the motivation and implementation of the BM25F retrieval model, Section 3 overviews our basic retrieval system and describes our sentence boundary creation technique, Section 4 presents the results of our experimental investigations, and Section 5 concludes the paper with a discussion of our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Field Combination</head><p>The "documents" of the speech collection are based on sections of extended interviews which are segmented into topically related section. The spoken documents are provided with a rich set of data fields, full details of these are given in <ref type="bibr" coords="2,281.14,250.20,9.97,8.74" target="#b2">[3]</ref>. In summary the fields comprise:</p><p>• a transcription of the spoken content of the document generated using an automatic speech recognition (ASR) system,</p><p>• two assigned sets of keywords generated automatically (AKW1,AKW2),</p><p>• one assigned set of manually generated keywords (MKW1),</p><p>• a short three sentence manually written summary of each document,</p><p>• and a manually determined list of the names of all the individuals appearing in the interview.</p><p>Two standard methods of combining multiple document fields for tasks such as this are:</p><p>• to simply merge all the fields into a single document representation and apply standard single document field information retrieval methods,</p><p>• to index the fields separately, perform individual retrieval runs for each field and then to merge the resulting ranked lists by summing in a process of data fusion.</p><p>The topic of field combination for this type of task with ranked information retrieval schemes is explored in <ref type="bibr" coords="2,153.27,475.95,9.96,8.74" target="#b0">[1]</ref>. This paper demonstrated the weaknesses of the simple standard combination methods and proposed an extended version of the standard BM25 term weighting scheme referred to as BM25F which combines multiple fields in a more well founded way.</p><p>The BM25F combination approach uses a simple weighted summation of the multiple fields of the documents to form a single field for each document in the usual way. The importance of each document field for retrieval can be determined empirically in separate runs for each field, the terms appearing in each field are multiplied by a scalar constant representing this importance, and the components of all fields summed to form the overall single field document representation for indexing.</p><p>Once the fields have been combined in a weighted sum standard single field information retrieval methods can be applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System Setup</head><p>The basis of our experimental system is the City University research distribution version of the Okapi system <ref type="bibr" coords="2,154.76,661.96,9.97,8.74" target="#b3">[4]</ref>. The documents and search topics are processed to remove stopwords from a standard list of about 260 words, suffix stripped using the Okapi implementation of Porter stemming <ref type="bibr" coords="2,136.93,685.87,10.52,8.74" target="#b4">[5]</ref> and terms are indexed using a small standard set of synonyms. None of these procedures were adapted for the CLEF 2006 CL-SR test collection.</p><p>Our experiments augmented the standard Okapi retrieval system with two variations of pseudo relevance feedback (PRF) based on extensions of the Robertson selection value (rsv) for expansion term selection. One method is a novel field-based PRF which we are currently developing <ref type="bibr" coords="2,480.58,733.69,9.97,8.74" target="#b5">[6]</ref>, and the other a summary-based method used extensively in our earlier CLEF submissions <ref type="bibr" coords="2,467.09,745.64,9.96,8.74" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Term Weighting</head><p>Document terms were weighted using the Okapi BM25 weighting scheme developed in <ref type="bibr" coords="3,456.47,130.41,10.51,8.74" target="#b3">[4]</ref> calculated as follows,</p><formula xml:id="formula_0" coords="3,165.24,162.73,251.39,23.23">cw(i, j) = cf w(i) × tf (i, j) × (k 1 + 1) k 1 × ((1 -b) + (b × ndl(j))) + tf (i, j)</formula><p>where cw(i, j) represents the weight of term i in document j, cf w(i) = log((N -n(i)+0.5)/(n(i)+ 0.5)), n(i) is the total number of documents containing term i, and N is the total number of documents in the collection, tf (i, j) is the within document term frequency, and ndl(j) = dl(j)/Av.dl is the normalized document length where dl(j) is the length of j. k 1 and b are empirically selected tuning constants for a particular collection. The matching score for each document is computed by summing the weights of terms appearing in the query and the document. The values used for our submitted runs were tuned using the CLEF 2005 training and test topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pseudo-Relevance Feedback</head><p>The main challenge for query expansion is the selection of appropriate terms from the assumed relevant documents. For the CL-SR task our query expansion method operates as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Field-Based PRF</head><p>Query expansion based on the standard Okapi relevance feedback model makes no use of the field structure of multi-field documents. We are currently exploring possible methods of making use of field structure to improvement the quality of expansion term selection. For this current investigation we adopted the following method.</p><p>The fields are merged as described in the previous section and retrieved performed using the initial query. The rsv is then calculated separately for each field of the original document, but where the document position in the ranked retrieval list has been determined using the combined document. The ranked rsv lists for each field are then normalised with respect to the highest scoring term in each list, and then summed to form a single merged rsv list from the expansion terms are selected. The objective of this process is to favour the selection of expansion terms which are ranked highly by multiple fields, rather than those which may obtain a high rsv value based on their association with a minority of the fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Summary-Based PRF</head><p>The method used here is based on our work originally described in <ref type="bibr" coords="3,379.32,547.27,9.97,8.74" target="#b6">[7]</ref>, and modified for the CLEF 2005 CL-SR task <ref type="bibr" coords="3,170.07,559.22,9.96,8.74" target="#b1">[2]</ref>. A summary is made of the ASR transcription of each of the top ranked documents, which are assumed to be relevant for each PRF. Each document summary is then expanded to include all terms in the other metadata fields used in this document index. All non-stopwords in these augmented summaries are then ranked using a slightly modified version of the rsv <ref type="bibr" coords="3,136.13,607.05,9.96,8.74" target="#b3">[4]</ref>. In our modified version of rsv(i), potential expansion terms are selected from the augmented summaries of the top ranked documents, but ranked using statistics from a larger number of assumed relevant ranked documents from the initial run.</p><p>Sentence Selection The summary-based PRF method operates by selecting topic expansion terms from document summaries. However, since the transcriptions of the conversational speech documents generated using automatic speech recognition (ASR) do not contain punctuation, we developed a method of selecting significant document segments to identify documents "summaries". This uses a method derived from Luhn's word cluster hypothesis. Luhn's hypothesis states that significant words separated by not more than 5 non-significant words are likely to be strongly related. Clusters of these strongly related word were identified in the running document transcription by searching for word groups separated by not more than 5 insignificant words.</p><p>Words appearing between clusters are not included in clusters, but can be ignored for the purposes of query expansion since they are by definition stop words.</p><p>The clusters were then awarded a significance score based on two measures:</p><p>Luhn's Keyword Cluster Method: Luhn's method assigns a sentence score for the highest scoring cluster within a sentence <ref type="bibr" coords="4,234.24,168.42,9.97,8.74" target="#b7">[8]</ref>. We adapted this method to assign a cluster score as follows:</p><formula xml:id="formula_1" coords="4,263.55,188.40,54.27,23.89">SS1 = SW 2 T W</formula><p>where SS1 = the sentence score SW = the number of bracketed significant words TW = the total number of bracketed words</p><p>Query-Bias Method This method assigns a score to each sentence based on the number of query terms in the sentence as follows:</p><formula xml:id="formula_2" coords="4,264.75,292.03,51.88,23.89">SS2 = T Q 2 N Q</formula><p>.</p><p>where SS2 = the sentence score TQ = the number of query terms present in the sentence NQ = the number of terms in a query</p><p>The overall score for each sentence (cluster) was then formed by summing these two measures for each sentence. The sentences were then ranked by score with the highest scoring sentences selected as the document summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Investigation</head><p>This section gives results of our experimental investigations for the CLEF 2006 CL-SR task. We first present results for our field combination experiments and then those for experiments using our summary-based PRF method.</p><p>For our formal submitted runs the system parameters were selected by optimising results for the CLEF 2005 CL-SR training and test collection. Our submitted runs for the CLEF 2006 CL-SR task are indicated using a * in the results table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Field Combination Experiments</head><p>Two sets of experiments were carried out using the field combination method. The first uses all the document fields combining manual and automatically generated fields, and the other only the automatically generated fields. We report results for our formal submitted runs using our field-based PRF method and also baseline results without feedback. We also give further results obtained by optimising performance for our systems using the CLEF 2006 CL-SR test set.  The contents of each field were multiplied by the appropriate factor and summed to form the single field document for indexing. Note the unusually high value of k 1 arises due to the change in the tf (i, j) profile resulting from the summation of the document fields <ref type="bibr" coords="5,415.64,454.58,9.96,8.74" target="#b0">[1]</ref>. We conducted monolingual English and bilingual French-English runs. The French topics were translated into English using Systran Version 3.0.</p><p>The results of English monolingual runs are shown in Table <ref type="table" coords="5,370.22,490.45,3.88,8.74" target="#tab_0">1</ref>, and those for French bilingual in Table <ref type="table" coords="5,128.98,502.40,3.88,8.74" target="#tab_1">2</ref>. For both topic sets the top 20 ranked terms were added to the topic for the PRF run with the original topic terms upweighted by 3.0.</p><p>It can be seen from these results that, as is usually the case for cross-language information retrieval, performance for monolingual English is better than bilingual French-English for all measures. A little more surprising is that while the application of the field-based PRF gives a small improvement in the number of relevant documents retrieved, there is little effect on MAP, and precision at high ranked cut off points is generally degraded. PRF methods for this task are the subject of ongoing research, and we will be exploring these results further.</p><p>Further Runs Subsequent to the release of the relevance set for the CLEF 2006 topic set further experiments were conducted to explore the potential for improvement in retrieval performance when the system parameters are optimised. We next show our best results achieved so far using the field combination method. For these runs the fields were weighted as follows:  The results of English monolingual runs are shown in Table <ref type="table" coords="6,370.22,434.96,3.88,8.74" target="#tab_2">3</ref>, and those for French bilingual in Table <ref type="table" coords="6,129.27,446.91,3.88,8.74" target="#tab_3">4</ref>. For monolingual English the top 20 terms were added to the topic for PRF run with the original topic terms upweighted by 33.0 For the French bilingual runs the top 60 terms were added to the topic with the original terms upweighted by 20.0. For these additional runs all test topics were included in all cases.</p><p>Looking at these additional results it can be seen that parameter optimisation gives a good improvement in all measures. It is not immediately clear whether this arises due to the instability of the parameters of our system, or a difference in some feature of the topics between CLEF 2005 and CLEF 2006. We will be investigating this issue further. Performance between monolingual English and bilingual French-English is similar to that observed for the submitted runs. PRF is generally more effective or neutral with the revised parameters, again we plan to conduct further exploration of PRF for multi-field documents is planned to better understand these results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Automatic Only Field Experiments</head><p>Submitted Runs Based on development runs with the CLEF 2005 CL-SR data the system parameters for the submitted automatic only field experiments were set empirically as follows: k 1 = 5.2 and b = 0.2 and the document fields were weighted as follows: ASR2006B × 2; Autokeyword1 × 1; Autokeyword2 × 1.</p><p>These were again summed to form the single field document for indexing. The same French-English topic translations were used for the automatic only field experiments as for the all field experiments.</p><p>The results of English monolingual runs are shown in upweighted by 3.0. From these results it can again be seen that there is the expected reduction in performance between monolingual English and bilingual French-English. The field-based PRF is once again shown generally not to be effective with this dataset. There is a small improvement in the number of relevant documents retrieved, but no there is little positive impact for precision.</p><p>Further Runs Subsequent to the release of the relevance set for the CLEF 2006 topic set further experiments were conducted to explore the potential for improvement in retrieval performance when the system parameters are optimised. We next show our best results achieved so far using the field combination method. For these runs the field weights were identical to those above for the submitted runs, and the Okapi parameters were modified as follows: k 1 = 40.0 and b = 0.3.</p><p>The results of English monolingual runs are shown in Table <ref type="table" coords="7,370.22,456.27,3.88,8.74" target="#tab_6">7</ref>, and those for French bilingual in Table <ref type="table" coords="7,129.27,468.23,3.88,8.74" target="#tab_7">8</ref>. For monolingual English the top 40 terms were added to the topic for PRF run with the original topic terms upweighted by 3.0 For the French bilingual runs the top 60 terms were added to the topic with the original terms upweighted by 3.5. For these additional runs again all test topics were included in all cases.</p><p>These results show an improvement in all metrics relative to the submitted runs. Once again optimising the system parameters results in an improvement effectiveness of the PRF method. Further experiments are planned to explore these results further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Summary-Based PRF Experiments</head><p>For these experiments the document fields were combined into a single field for indexing without application of the field weighting method. The Okapi parameters were again selected using the CLEF 2005 CL-SR training and test collections. The values were set as follows k 1 =1.4 b=0.6. For all our PRF runs, 3 documents were assumed relevant for term selection and document summaries comprised the best scoring 6 clusters. The rsv values to rank the potential expansion terms were estimated based on the top 20 ranked assumed relevant documents. The top 40 ranked expansion terms taken from the clusters were added to the original query in each case. Based on results from our previous experiments in CLEF, the original topic terms are up-weighted by a factor of 3.5 relative to terms introduced by PRF.</p><p>All Field Experiments Table <ref type="table" coords="7,240.45,707.77,4.98,8.74" target="#tab_9">9</ref> shows results for monolingual English retrieval based on all document fields using TDN field topics. Our sumitted run using this method is denoted by the * . Rather surprisingly the baseline result for this system is better than either of those using the field-weighted method shown in Table <ref type="table" coords="7,257.76,743.64,4.98,8.74" target="#tab_0">1</ref> and Table <ref type="table" coords="7,312.32,743.64,3.88,8.74" target="#tab_2">3</ref>. The runs in Tables 1 and Table <ref type="table" coords="7,463.99,743.64,4.98,8.74" target="#tab_2">3</ref> are based on only TD field topics, while those in Table <ref type="table" coords="8,283.26,112.02,4.98,8.74" target="#tab_9">9</ref> use TDN fields. However, the difference is probably to be too big to be explained by this factor alone. We will be investigating the reason for these differences. The summary-based PRF is shown to be effective here, as it has in many previous submissions to CLEF tracks in previous years.</p><p>Automatic Only Field Experiments Tables <ref type="table" coords="8,309.69,173.78,9.96,8.74" target="#tab_10">10</ref> and<ref type="table" coords="8,342.91,173.78,4.98,8.74" target="#tab_9">9</ref> show results for monolingual English retrieval based on only auto document fields using TDN and TD topic fields respectively. Results using TD topics are lower than those for TDN field topics. The summary-based PRF method is again effective for this document index. By contrast to the all field experiments, the results here are rather lower than those in Table <ref type="table" coords="8,248.12,221.60,4.98,8.74" target="#tab_4">5</ref> and Table <ref type="table" coords="8,301.98,221.60,4.98,8.74" target="#tab_6">7</ref> using the field weighted method. We will again be exploring the reasons for the differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This paper has described results for our participation in the CLEF 2006 CL-SR track. Experiments explored use of a field combination method for multi-field documents, and two methods of PRF. Results indicate that further exploration is required of the field combination approach and our new field-based PRF method. Our existing summary-based PRF method is shown to be effective for this task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,90.00,627.12,146.10,8.77;4,90.00,645.50,423.00,8.77;4,90.00,657.49,227.80,9.65"><head>4. 1 . 1</head><label>11</label><figDesc>All Field Experiments Submitted Runs Based on development runs with the CLEF 2005 data the Okapi parameters were set empirically as follows: k 1 = 6.2 and b = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,95.98,663.44,68.91,8.74;5,95.98,675.39,112.39,8.74;5,95.98,687.35,85.54,8.74;5,95.98,699.31,69.46,8.74;5,95.98,711.26,84.99,8.74;5,95.98,723.22,84.99,8.74;5,90.00,739.01,334.15,9.65"><head>Name field × 1 ;</head><label>1</label><figDesc>Manualkeyword field × 5; Summary field × 5; ASR2006B × 1; Autokeyword1 × 1; Autokeyword2 × 1, and the Okapi parameters set empirically as follows: k 1 = 10.5 and b = 0.35.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,90.00,657.49,423.00,94.12"><head>Table 1 :</head><label>1</label><figDesc>Results for monolingual English with all document fields with parameters trained on CLEF 2005 data.</figDesc><table coords="4,317.80,657.49,195.20,8.74"><row><cell>4, and the document fields were weighted as</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,90.00,235.65,423.01,65.13"><head>Table 2 :</head><label>2</label><figDesc>Results for French-English bilingual with all document fields with parameters trained on CLEF 2005 data.</figDesc><table coords="5,193.20,267.33,216.61,33.44"><row><cell>TD</cell><cell cols="2">Recall MAP</cell><cell>P5</cell><cell>P10</cell><cell>P30</cell></row><row><cell>Baseline:</cell><cell>1908</cell><cell cols="4">0.234 0.364 0.342 0.303</cell></row><row><cell>PRF</cell><cell>1929</cell><cell cols="4">0.243 0.364 0.370 0.305</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,90.00,313.90,423.01,67.06"><head>Table 3 :</head><label>3</label><figDesc>Results for monolingual English with all document fields with parameters optimised for the CLEF 2006 topics.</figDesc><table coords="5,194.58,347.51,213.84,33.45"><row><cell>TD</cell><cell cols="2">Recall MAP</cell><cell>P5</cell><cell>P10</cell><cell>P30</cell></row><row><cell>Baseline</cell><cell>1560</cell><cell cols="4">0.172 0.315 0.267 0.231</cell></row><row><cell>PRF</cell><cell>1601</cell><cell cols="4">0.173 0.315 0.267 0.225</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,90.00,394.08,423.01,20.69"><head>Table 4 :</head><label>4</label><figDesc>Results for French-English bilingual with all document fields with parameters optimised for the CLEF 2006 topics.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,90.00,110.82,423.01,111.70"><head>Table 5 :</head><label>5</label><figDesc>Results for monolingual English with only auto document fields with parameters trained on CLEF 2005 data.</figDesc><table coords="6,194.58,110.82,213.84,111.70"><row><cell>TD</cell><cell cols="2">Recall MAP</cell><cell>P5</cell><cell>P10</cell><cell>P30</cell></row><row><cell>Baseline</cell><cell>1290</cell><cell cols="4">0.071 0.163 0.163 0.149</cell></row><row><cell>PRF  *</cell><cell>1361</cell><cell cols="4">0.073 0.152 0.142 0.146</cell></row><row><cell>TD</cell><cell cols="2">Recall MAP</cell><cell>P5</cell><cell>P10</cell><cell>P30</cell></row><row><cell>Baseline</cell><cell>1070</cell><cell cols="4">0.047 0.119 0.113 0.106</cell></row><row><cell>PRF  *</cell><cell>1097</cell><cell cols="4">0.047 0.106 0.094 0.102</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,90.00,235.65,423.00,65.13"><head>Table 6 :</head><label>6</label><figDesc>Results for French-English bilingual with only auto document fields with parameters trained on CLEF 2005 data.</figDesc><table coords="6,194.58,267.33,213.84,33.44"><row><cell>TD</cell><cell cols="2">Recall MAP</cell><cell>P5</cell><cell>P10</cell><cell>P30</cell></row><row><cell>Baseline</cell><cell>1335</cell><cell cols="4">0.080 0.224 0.215 0.169</cell></row><row><cell>PRF</cell><cell>1379</cell><cell cols="4">0.094 0.188 0.206 0.184</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="6,90.00,313.90,423.00,67.06"><head>Table 7 :</head><label>7</label><figDesc>Results for monolingual English with only auto document fields with parameters optimised for the CLEF 2006 topics.</figDesc><table coords="6,194.58,347.51,213.84,33.45"><row><cell>TD</cell><cell cols="2">Recall MAP</cell><cell>P5</cell><cell>P10</cell><cell>P30</cell></row><row><cell>Baseline</cell><cell>1110</cell><cell cols="4">0.050 0.121 0.127 0.123</cell></row><row><cell>PRF</cell><cell>1167</cell><cell cols="4">0.055 0.127 0.142 0.124</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="6,90.00,394.08,423.00,20.69"><head>Table 8 :</head><label>8</label><figDesc>Results for French-English bilingual with only auto document fields with parameters optimised for the CLEF 2006 topics.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="6,90.00,725.82,423.00,20.69"><head>Table 5 ,</head><label>5</label><figDesc>and those for French bilingual in Table6. The top 30 terms were added to the topic for PRF run with the original topic terms</figDesc><table coords="7,211.91,110.82,179.19,33.44"><row><cell>TDN</cell><cell cols="2">Recall MAP P10</cell><cell>P30</cell></row><row><cell>Baseline</cell><cell>1832</cell><cell cols="2">0.246 0.391 0.321</cell></row><row><cell>PRF  *</cell><cell>1895</cell><cell cols="2">0.277 0.439 0.357</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="7,154.57,157.39,289.44,55.11"><head>Table 9 :</head><label>9</label><figDesc>Results for monolingual English with all document fields.</figDesc><table coords="7,211.91,179.06,179.19,33.44"><row><cell>TDN</cell><cell cols="2">Recall MAP P10</cell><cell>P30</cell></row><row><cell>Baseline</cell><cell>633</cell><cell cols="2">0.029 0.069 0.068</cell></row><row><cell>PRF</cell><cell>993</cell><cell cols="2">0.047 0.118 0.107</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="7,136.72,225.63,325.14,55.11"><head>Table 10 :</head><label>10</label><figDesc>Results for monolingual English with only auto document fields.</figDesc><table coords="7,211.91,247.29,179.19,33.44"><row><cell>TD</cell><cell cols="2">Recall MAP P10</cell><cell>P30</cell></row><row><cell>Baseline</cell><cell>627</cell><cell cols="2">0.025 0.069 0.061</cell></row><row><cell>PRF</cell><cell>900</cell><cell cols="2">0.039 0.091 0.089</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="7,136.72,293.86,325.14,8.74"><head>Table 11 :</head><label>11</label><figDesc>Results for monolingual English with only auto document fields.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,105.50,390.91,407.51,8.74;8,105.50,402.87,407.50,8.74;8,105.50,414.82,141.19,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,314.27,390.91,198.74,8.74;8,105.50,402.87,24.60,8.74">Simple BM25 Extension to Multiple Weighted Fields</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,137.20,402.87,375.79,8.74;8,105.50,414.82,53.34,8.74">Proceedings of the 13th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 13th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,434.75,407.50,8.74;8,105.50,446.70,407.51,8.74;8,105.50,458.66,353.82,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,304.22,434.75,208.77,8.74;8,105.50,446.70,214.38,8.74">Dublin City University at CLEF 2005: Cross-Language Speech Retrieval (CL-SR) Experiments</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Lam-Adesina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,328.08,446.70,184.92,8.74;8,105.50,458.66,247.93,8.74">Proceedings of the CLEF 2005: Workshop on Cross-Language Information Retrieval and Evaluation</title>
		<meeting>the CLEF 2005: Workshop on Cross-Language Information Retrieval and Evaluation<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,478.58,407.50,8.74;8,105.50,490.54,407.51,8.74;8,105.50,502.49,353.82,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,442.21,478.58,70.79,8.74;8,105.50,490.54,220.23,8.74">Overview of the CLEF-2005 Cross-Language Speech Retrieval Track</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Soergel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,332.85,490.54,180.15,8.74;8,105.50,502.49,247.93,8.74">Proceedings of the CLEF 2005: Workshop on Cross-Language Information Retrieval and Evaluation</title>
		<meeting>the CLEF 2005: Workshop on Cross-Language Information Retrieval and Evaluation<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,522.42,407.50,8.74;8,105.50,534.37,407.51,8.74;8,105.50,546.33,22.70,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,474.80,522.42,38.20,8.74;8,105.50,534.37,33.92,8.74">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,147.60,534.37,266.54,8.74">Proceedings of the Third Text REtrieval Conference (TREC-3)</title>
		<meeting>the Third Text REtrieval Conference (TREC-3)</meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="109" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,566.25,332.97,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,170.86,566.25,146.32,8.74">An Algorithm for Suffix Stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,324.82,566.25,35.52,8.74">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="10" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,586.18,407.50,8.74;8,105.50,598.13,284.16,8.74" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="8,160.60,586.18,323.94,8.74">Cross-Language Spoken Documenr Retrieval from Oral History Archives</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<pubPlace>Dublin City University</pubPlace>
		</imprint>
		<respStmt>
			<orgName>School of Computing</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">MSc Dissertation</note>
</biblStruct>

<biblStruct coords="8,105.50,618.06,407.50,8.74;8,105.50,630.02,407.51,8.74;8,105.50,641.97,407.50,8.74;8,105.50,653.93,91.69,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,305.45,618.06,207.55,8.74;8,105.50,630.02,135.17,8.74">Applying Summarization Techniques for Term Selection in Relevance Feedback</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Lam-Adesina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,247.97,630.02,265.04,8.74;8,105.50,641.97,330.52,8.74">Proceedings of the Twenty-Fourth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the Twenty-Fourth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New Orleans</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,673.85,407.51,8.74;8,105.50,685.81,145.15,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,161.99,673.85,212.19,8.74">The Automatic Creation of Literature Abstracts</title>
		<author>
			<persName coords=""><forename type="middle">H P</forename><surname>Luhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,382.44,673.85,130.57,8.74;8,105.50,685.81,54.41,8.74">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="165" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
