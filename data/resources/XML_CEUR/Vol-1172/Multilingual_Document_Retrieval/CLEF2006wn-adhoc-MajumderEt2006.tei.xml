<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,131.04,98.73,340.65,15.51;1,175.56,120.69,251.77,15.51">Statistical vs. Rule-Based Stemming for Monolingual French Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,255.12,154.07,88.01,9.96"><forename type="first">Prasenjit</forename><surname>Majumder</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CVPR Unit</orgName>
								<orgName type="institution">Indian Statistical Institute</orgName>
								<address>
									<settlement>Kolkata</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,268.08,168.11,62.11,9.96"><forename type="first">Mandar</forename><surname>Mitra</surname></persName>
							<email>mandar@isical.ac.in</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CVPR Unit</orgName>
								<orgName type="institution">Indian Statistical Institute</orgName>
								<address>
									<settlement>Kolkata</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,255.36,182.03,87.50,9.96"><forename type="first">Kalyankumar</forename><surname>Datta</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept. of EE</orgName>
								<orgName type="institution">Jadavpur University</orgName>
								<address>
									<settlement>Kolkata</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,131.04,98.73,340.65,15.51;1,175.56,120.69,251.77,15.51">Statistical vs. Rule-Based Stemming for Monolingual French Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">602D81A00282EFD5F67D22E7B4B231C7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Managment]: Languages-Query Languages Performance, Experimentation statistical stemming, string distance, clustering, Porter&apos;s algorithm, monolingual information retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our approach to the 2006 Adhoc Monolingual Information Retrieval run for French. The goal of our experiment was to compare the performance of a proposed statistical stemmer with that of a rule-based stemmer, specifically the French version of Porter's stemmer. The statistical stemming approach is based on lexicon clustering, using a novel string distance measure. We submitted three official runs, besides a baseline run that uses no stemming. The results show that stemming significantly improves retrieval performance (as expected) by about 9-10%, and the performance of the statistical stemmer is comparable with that of the rule-based stemmer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We have recently been experimenting with languages that have not been studied much from the IR perspective. These languages are typically resource-poor, in the sense that few language resources or tools are available for them. As a specific example, no comprehensive stemming algorithms are available for these languages. The stemmers that are available for more widely studied languages (e.g. English) usually make use of an extensive set of linguistic rules. Rule based stemmers for most resource-poor languages are either unavailable or lack comprehensive coverage. In earlier work, therefore, we have looked at the problem of stemming for such resource-poor languages, and proposed a stemming approach that is based on purely unsupervised clustering techniques.</p><p>Since the proposed approach does not assume any language-specific information, we expect the approach to work for multiple languages. The motivation behind our experiments at CLEF 2006 was to test this hypothesis. Thus, we focused on mono-lingual retrieval for French (a language which we know nothing about), and tried our statistical stemming approach on French data.</p><p>We give a brief overview of the proposed statistical stemming algorithm in the next section. We outline our experimental setup in Section 3, and discuss the results of the runs that we submitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Statistical Stemmer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">String Distance Measures</head><p>Distance functions map a pair of strings s and t to a real number r, where a smaller value of r indicates greater similarity between s and t. In the context of stemming, an appropriate distance measure would be one that assigns a low distance value to a pair of strings when they are morphologically similar, and assigns a high distance value to morphologically unrelated words. The languages that we have been experimenting with are primarily suffixing in nature, i.e. words are usually inflected by the addition of suffixes, and possible modifications to the tail-end of the word. Thus, for these languages, two strings are likely to be morphologically related if they share a long matching prefix. Based on this intuition, we define a string distance measure D which rewards long matching prefixes, and penalizes an early mismatch.</p><p>Given two strings X = x 0 x 1 . . . x n and Y = y 0 y 1 . . . y n , we first define a Boolean function p i (for penalty) as follows:</p><formula xml:id="formula_0" coords="2,213.12,327.83,170.52,21.84">p i = 0 if x i = y i 0 ≤ i ≤ min(n, n ) 1 otherwise</formula><p>Thus, p i is 1 if there is a mismatch in the i-th position of X and Y . If X and Y are of unequal length, we pad the shorter string with null characters to make the string lengths equal.</p><p>Let the length of the strings be n + 1, and let m denote the position of the first mismatch between X and Y (i.e. x 0 = y 0 , x 1 = y 1 , . . . , x m-1 = y m-1 , but x m = y m ). We now define D as follows:</p><formula xml:id="formula_1" coords="2,170.40,424.88,342.51,28.81">D(X, Y ) = n -m + 1 m × n i=m 1 2 i-m if m &gt; 0, ∞ otherwise<label>(1)</label></formula><p>Note that D does not consider any match once the first mismatch occurs. The actual distance is obtained by multiplying the total penalty by a factor which is intended to reward a long matching prefix, and penalize significant mismatches. For example, for the pair astronomer, astronomically , m = 8, n = 13. Thus, D 3 = 6 8 × ( 1 2 0 + . . . + 1 2 13-8 ) = 1.4766.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Lexicon Clustering</head><p>Using the distance function defined above, we can cluster all the words in a document collection into groups. Each group, consisting of "similar" strings, is expected to represent an equivalence class consisting of morphological variants of a single root word. The words within a cluster can be stemmed to the 'central' word in that cluster. Since the number of natural clusters are unknown apriori, partitive clustering algorithms like k-means are not suitable for our task. Also, the clusters are likely to be of non-convex nature. Graph-theoretic clustering algorithms appear to be the natural choice in this situation because of their ability to detect natural and non-convex clusters in the data. Three variants of graph theoretic clustering are popular in literature, namely, single-linkage, average-linkage, and complete-linkage <ref type="bibr" coords="2,258.15,650.87,9.98,9.96" target="#b1">[2]</ref>. Each of these algorithms are of hierarchical (agglomerative or divisive) nature. In the agglomerative form, the cluster tree (often referred to as a dendogram) consists of individual data points as leaves. The nearest (or most similar) pair(s) of points are merged to form groups, which in turn are successively merged to form progressively larger groups of points. Clustering stops when the similarity between the pair of closest groups falls below a pre-determined threshold. Alternatively, a threshold can be set on the distance value; when the distance between the pair of nearest points exceeds the threshold, clustering stops. The three algorithms mentioned above differ in the way similarity between the groups is defined. We choose the compete-linkage algorithm for our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>We used the Smart <ref type="bibr" coords="3,177.18,164.03,10.55,9.96" target="#b2">[3]</ref> system for all our experiments. We submitted four official runs, including one baseline. For the baseline run (Cbaseline), queries and documents were indexed after eliminating stopwords (using the stopword list provided on the CLEF website<ref type="foot" coords="3,401.52,187.15,3.95,4.77" target="#foot_0">1</ref> ). The &lt;title&gt;, &lt;desc&gt;, and &lt;narr&gt; field of the query were indexed. The Lnu.ltn term-weighting strategy <ref type="bibr" coords="3,456.40,199.91,10.55,9.96" target="#b0">[1]</ref> was used. No stemming was done for the baseline run.</p><p>For the remaining three runs, we used three variants of the statistical stemming method described above. Since our approach is based on hierarchical agglomerative clustering (as described above), the threshold value used in the clustering step is an important parameter of the method. Earlier experiments with English data have shown that 1.5 is a reasonable threshold value. We generated two retrieval runs by setting the threshold to 1.5 and 2.0 respectively (Cd61.5, Cd62.0).</p><p>For the third run, the stemmer was created based on a subset of the data. A lexicon was constructed using only the LeMonde section of the document collection, and this was then clustered as described above to determine the stem classes. Since the lexicon was smaller, the clustering step took less time for this run. The motivation behind this experiment was to study how performance is affected when a subset of the lexicon is used to construct the stemmer in order to save computation time.</p><p>After the relevance judgments for this data set were distributed, we performed two additional experiments: first, we tried setting the clustering threshold to 1.0; and secondly, we used the French version of Porter's stemmer<ref type="foot" coords="3,243.00,378.43,3.95,4.77" target="#foot_1">2</ref> in place of our statistical stemmer. The results obtained for all the official and unofficial runs are given below. The official results for the run labelled Cd61.5 do not agree with the evaluation figures that we obtained by using the distributed relevance judgment data. We therefore report both the official figures and the numbers that we obtained in Table <ref type="table" coords="3,314.48,569.99,3.90,9.96" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>The two most promising runs (Cd61.5 and Porter) were analyzed in greater detail. Paired t-tests show that stemming (using either strategy) results in significant improvements (at a 1% level of confidence) over the baseline (no stemming), but the differences between the rule-based and statistical approaches are not statistically significant. Also, some loss in performance results when the stemmer is generated from a subset of the corpus (run Cld61.5).</p><p>This confirms our hypothesis that the proposed stemming approach, which does not assume any language-specific information, will work for a variety of languages, provided the languages are primarily suffixing in nature.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,136.08,413.87,330.93,118.32"><head>Table 1 :</head><label>1</label><figDesc>Retrieval results obtained using various stemmers</figDesc><table coords="3,136.08,413.87,330.93,96.36"><row><cell>ID</cell><cell>Topic fields</cell><cell>MAP</cell><cell cols="2">Rel. ret. R-Precision</cell></row><row><cell>Cbaseline</cell><cell>T+D+N</cell><cell>0.3196</cell><cell>1,616</cell><cell>31.30%</cell></row><row><cell>Cd61.0</cell><cell>T+D+N</cell><cell>0.3465 (+8.4%)</cell><cell>1,715</cell><cell>34.85%</cell></row><row><cell>Cd61.5 (official)</cell><cell>T+D+N</cell><cell>0.3454 (+8.1%)</cell><cell>1,709</cell><cell>34.35%</cell></row><row><cell>Cd61.5 (obtained)</cell><cell>T+D+N</cell><cell>0.3509 (+9.8%)</cell><cell>1,708</cell><cell>34.26%</cell></row><row><cell>Cd62.0</cell><cell>T+D+N</cell><cell>0.3440 (+7.6%)</cell><cell>1,737</cell><cell>34.78%</cell></row><row><cell>Cld61.5</cell><cell>T+D+N</cell><cell>0.3342 (+4.6%)</cell><cell>1,678</cell><cell>32.81%</cell></row><row><cell>Porter</cell><cell>T+D+N</cell><cell>0.3480 (+8.9%)</cell><cell>1,705</cell><cell>34.71%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,105.24,684.76,116.76,7.97"><p>http://www.unine.ch/info/clef/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,105.24,694.24,331.85,7.97"><p>downloaded from http://www.snowball.tartarus.org/algorithms/french/stemmer.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,105.47,83.27,407.10,9.96;4,105.48,95.27,407.02,9.96;4,105.48,107.15,326.20,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,278.64,83.27,233.94,9.96;4,105.48,95.27,30.27,9.96">Using Query Zoning and Correlation within SMART: TREC5</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,345.89,95.27,166.61,9.96;4,105.48,107.15,213.78,9.96">Proceedings of the Fifth Text REtrieval Conference (TREC-5). NIST Special Publication</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Fifth Text REtrieval Conference (TREC-5). NIST Special Publication</meeting>
		<imprint>
			<date type="published" when="1997-11">November 1997</date>
			<biblScope unit="page" from="500" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,105.47,127.07,407.23,9.96;4,105.48,139.07,89.27,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,297.47,127.07,112.25,9.96">Data clustering: A review</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">N</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,419.16,127.07,89.28,9.96">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="264" to="323" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,105.47,158.99,407.29,9.96;4,105.48,170.99,248.40,9.96" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="4,207.15,158.99,305.61,9.96;4,105.48,170.99,37.21,9.96">The SMART Retrieval System-Experiments in Automatic Document Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
			<publisher>Prentice Hall Inc</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
