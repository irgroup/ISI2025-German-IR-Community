<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,119.94,75.53,372.14,12.58">DCU at CLEF 2006: ROBUST CROSS LANGUAGE TRACK</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,216.66,118.02,102.54,9.02"><forename type="first">Adenike</forename><forename type="middle">M</forename><surname>Lam-Adesina</surname></persName>
							<email>adenike@computing.dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,326.42,118.02,68.92,9.02"><forename type="first">Gareth</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
							<email>gjones@computing.dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,119.94,75.53,372.14,12.58">DCU at CLEF 2006: ROBUST CROSS LANGUAGE TRACK</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1CF90D48771A828E8F61380DE6A7B862</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval -Relevance Feedback</term>
					<term>H.3.7 Digital Libraries Measurement, Performance, Experimentation Robust cross-language information retrieval, Pseudo relevance feedback, Document reranking</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The main focus of the DCU group's participation in the CLEF 2006 Robust Track in CLEF 2006  was not to identify and handle difficult topics in the topic set per se, but rather to explore a new method of re-ranking a retrieved document set. The initial query is used to re-rank documents retrieved using a query expansion method. The intention is to ensure that the query drift that might occur as a result of the addition of expansion terms chosen from irrelevant documents in pseudo relevance feedback (PRF) is minimised. By re-ranking using the initial query, the relevant set is forced to mimic the initial query more closely while not removing the benefits of PRF. Our results show that although our PRF is consistently effective for this task, the application of our re-ranking method generally has little effect on the ranked output.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper describes the DCU experiments for the CLEF 2006 Robust Track. Our official submission included monolingual runs for English and for Spanish, Italian and French where topics and documents had been translated into English and a bilingual run for Spanish using English topics. Unfortunately due to errors in our system we were unable to submit result for monolingual and bilingual German.</p><p>Our general approach was to translate non-English documents and topics into English for use as a pivot language. Collections and topics were translated into English using the Systran Version: 3.0 Machine Translator (Sys). Pseudo Relevance Feedback (PRF) which aims to expand query by selecting potential useful terms from the top retrieved documents to improve retrieval has been shown to be effective in our previous submissions to CLEF <ref type="bibr" coords="1,215.86,608.88,18.34,9.02">[2001]</ref><ref type="bibr" coords="1,234.19,608.88,4.58,9.02">[2002]</ref><ref type="bibr" coords="1,234.19,608.88,4.58,9.02">[2003]</ref><ref type="bibr" coords="1,234.19,608.88,4.58,9.02">[2004]</ref><ref type="bibr" coords="1,238.78,608.88,18.34,9.02">[2005]</ref>, and also in our other research work outside of CLEF. Therefore, we again use this method with our extended PRF method of term selection from document summaries rather than full documents that has been thoroughly tested in our past research work. In addition, for this task we explored the application of a new post-retrieval re-ranking method that we are developing.</p><p>The remainder of this paper is structured as follows: Section 2 covers background to robust information retrieval tasks, Section 3 describes our system setup and the information retrieval (IR) methods used, Section 4 presents our experimental results and section 5 concludes the paper with a discussion of our findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>The robust track was first introduced in the Text Retrieval Conference (TREC) in 2003. The aim was to explore methods of improving retrieval effectiveness for topics that performed poorly using standard generally high performing IR methods, i.e. hard topics. For these topics it is usually the case that although relevant documents exist in the target collection, the topic is not discriminatory enough to find the relevant documents or bring them into the retrieved set of potentially relevant documents. Several approaches have been taken by TREC participants which aim to tackle these hard topics and improve IR effectiveness. This work falls into two main categories: either using a contemporaneous collection (e.g. the web) for query expansion or re-ordering the original ranking of the retrieved relevant documents.</p><p>Kwok. et al. used the web as a contemporaneous collection from which terms were selected for query expansion <ref type="bibr" coords="2,161.81,246.84,10.65,9.02" target="#b0">[1]</ref>. They argued that the reason why PRF is not effective for hard topics is because assumed relevant documents where the expansion are taken from, are usually irrelevant and thus would cause a query drift for hard topics. Therefore they expand the initial query from the web and use the expanded query for retrieval. The list from the initial retrieval step and the expanded query list are then combined into a new list. Results for this approach showed an improvement in IR performance for both normal and hard topics. Interestingly results for runs using short queries were found to be better than those for long queries.</p><p>Amati et al. also found that query expansion from the web resulted in better retrieval for hard topics as long as the queries are short <ref type="bibr" coords="2,241.68,338.82,10.82,9.02" target="#b1">[2]</ref>; for longer queries PRF should be limited to the target collection.</p><p>Piatko et al. used a re-ranking method that aimed to improve the initial ranking of retrieved relevant documents using a method called the minimal matching span <ref type="bibr" coords="2,372.66,361.80,10.61,9.02" target="#b2">[3]</ref>. This method aims to improve the ranking of relevant documents by estimating the minimal length of consecutive sets of document terms containing at least one occurrence of each query term in the set. Documents with high scores have their ranking improved. Results for this method showed an improvement in average precision results compared to not re-ranking. The benefits of this re-ranking method were more visible at the top ranks of the retrieved document set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System Setup</head><p>For our experiments we used the City University research distribution version of the Okapi system retrieval system. Stopwords were removed from both the documents and search topics, and the Okapi implementation of Porter stemming algorithm <ref type="bibr" coords="2,276.34,492.90,11.72,9.02" target="#b3">[4]</ref> was applied to both the document and search terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Term Weighting</head><p>The okapi system is based on the BM25 weighting <ref type="bibr" coords="2,309.03,541.20,11.68,9.02" target="#b4">[5]</ref> scheme where document terms are weighted as follows,</p><formula xml:id="formula_0" coords="2,173.23,575.25,313.59,28.23">) , ( ))) ( ( ) 1 (( * 1 ) 1 1 ( ) , ( ) ( ) , ( j i tf j ndl b b K K j i tf i cfw j i cw + × + - + × × = (1)</formula><p>where cw(i,j) represents the weight of term i in document j, cfw(i) is the standard collection frequency weight, tf(i,j) is the document term frequency, and ndl(j) is the normalized document length. ndl(j) is calculated as ndl(j) = dl(j)/avdl where dl(j) is the length of j and avdl is the average document length for all documents. k1 and b are empirically selected tuning constants for a particular collection. k1 is designed to modify the degree of effect of tf(i,j), while constant b modifies the effect of document length. High values of b imply that documents are long because they are verbose, while low values imply that they are long because they are multi-topic. In our experiments values of k1 and b are estimated based on the CLEF 2003 ad hoc retrieval task data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pseudo-Relevance Feedback</head><p>Short and imprecise queries can affect IR effectiveness. To curtail this negative impact, relevance feedback (RF) via query expansion (QE) is often employed. QE aims to improve initial query statements by addition of terms from user assessed relevant documents. These terms are selected using document statistics and usually describe the information request better. Pseudo-Relevance Feedback (PRF) whereby relevant documents are assumed and used for QE is on average found to give improvement in retrieval performance, although this is usually smaller than that observed for true user-based RF.</p><p>PRF can result in a query drift if expansion terms are selected from assumed relevant document which are in fact not relevant. In our past research work <ref type="bibr" coords="3,333.32,228.42,11.67,9.02" target="#b5">[6]</ref> we discovered that although a top-ranked document might not be relevant, it often contains information that is pertinent to the query. Thus, we developed a new method that select appropriate terms from document summaries. These summaries are constructed in such a way that they contain only sentences that are closely related to the initial query. Our QE method selects terms from summaries of the top 5 ranked documents. The summaries are generated using the method described in <ref type="bibr" coords="3,221.11,285.90,10.64,9.02" target="#b5">[6]</ref>. For all our experiments we used the top 6 ranked sentences as the summary of each document. From this summary we collected all non-stopwords and ranked them using a slightly modified version of the Robertson selection value (rsv) <ref type="bibr" coords="3,356.26,308.94,11.68,9.02" target="#b4">[5]</ref> reproduced below. The top 20 terms were then selected in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>rw(i) r(i) rsv(i) × =</head><p>( 2 ) where r(i) = number of relevant documents containing term i rw(i) is the standard Robertson/Sparck Jones relevance weight <ref type="bibr" coords="3,369.73,383.82,11.68,9.02" target="#b4">[5]</ref> reproduced below</p><formula xml:id="formula_1" coords="3,178.50,404.12,295.31,26.55">) . r(i) )(R . r(i) (n(i) ) . r(i) R n(i) )(N . (r(i) rw(i) 5 0 5 0 5 0 5 0 log + - + - + + - - + =<label>(3)</label></formula><p>where n(i) = the total number of documents containing term i r(i) = the total number of relevant documents term i occurs in R = the total number of relevant documents for this query N = the total number of documents</p><p>In our modified version, potential expansion terms are selected from the summaries of the top 5 ranked documents, and ranked using statistics from assuming that the top 20 ranked documents from the initial run are relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Re-ranking Methodology</head><p>As part of our investigation for the CLEF 2006 robust track we explored the application of a further novel re-ordering of the retrieved document list obtained from our PRF process. This reordering method attempts to ensure that retrieved documents with more matching query terms have their ranking improved, while not discarding the effect of document weighting scheme used. To this end we devised a document re-ranking formula as follows:</p><formula xml:id="formula_2" coords="3,158.34,633.32,294.17,23.30">) (b*nmt/mmt b) ( doc_wgt + - 1 (<label>4</label></formula><formula xml:id="formula_3" coords="3,452.46,639.74,3.35,9.06">)</formula><p>where doc_wgt = the original document matching score b = an empirical value ranging between 0.1 and 0.5 nmt = the number of original topic terms that occur in the document mmt = the mean of the value nmt for a given query over all retrieved documents In this section we describe our parameter selection and present our experimental results for the CLEF 2006 Robust track. Results are given for baseline retrieval without feedback, after the application of our PRF method and after the further application of our re-ranking procedure.</p><p>The CLEF 2006 topics consist of three fields: Title, Description and Narrative. We conducted experiments used the Title and Description (TD) or Title, Description and Narrative (TDN) fields. For all runs we present the precision at both 10 and 30 documents cutoff (P10 and P30), standard TREC average precision results (AvP), the number of relevant documents retrieved out of the total number of relevant in the collection (RelRet), and the change in number of RelRet compared to Baseline runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Selection of System Parameters</head><p>To set appropriate parameters for our runs development runs were carried out using the training topics provided. The topics provided were taken from the CLEF 2003 The Okapi parameters were set as follows k1=1.2 b=0.75. For all our PRF runs, 5 documents were assumed relevant for term selection and document summaries comprised the best scoring 6 sentences in each case. Where the length of sentence was less than 6, half of the total number of sentences were chosen. The rsv values to rank the potential expansion terms were estimated based on the top 20 ranked assumed relevant documents. The top 20 ranked expansion terms taken from these summaries were added to the original query in each case. Based on results from our previous experiments, the original topic terms are upweighted by a factor of 3.5 relative to terms introduced by PRF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results</head><p>Table <ref type="table" coords="4,115.32,375.56,5.04,9.06" target="#tab_0">1</ref> summarises the results of our experiments. Results are shown for the following runs: Baseline -baseline results without PRF using Title, Description and Narrative topic fields (TDN) f20narr -feedback results using the Title, Description and Narrative topic fields. 20 terms are added to the initial query. f20re-ranked -same as F20narr, but documents are re-ranked using the formula (4) above. f20desc -feedback results using the Title and Description sections of query. 20 terms are added to the initial query.</p><p>Comparing the Baseline and f20narr runs it can be seen that application of PRF improves all the performance measures for all runs with the exception of the RelRet for Spanish monolingual where there is a small reduction. By contrast for the Spanish bilingual run there is a much larger improvement in RelRet than is observed for any of the other runs.</p><p>Application of the re-ranking method to the f20narr list produces little change in the ranked output. The only notable change is a further improvement in the RelRet for the Spanish bilingual task. Varying the value of the b factor in equation 4 made only a small difference to the results. We are currently investigating the reasons for this results, and exploring approaches to the re-ranking method which will have a greater impact on the output ranked lists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This paper has presented a summary of our results for the CLEF 2006 Robust Track. The results show that our summary-based PRF method is consistently effective across this topic set. We also explored the use of a novel post-retrieval re-ranking method. Application of this procedure led to very modification in the ranked lists, and we are currently exploring alternative variations on this method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run-ID</head><note type="other">English</note></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,90.00,74.84,432.16,367.56"><head>Table 1 :</head><label>1</label><figDesc>Retrieval results for Baseline, PRF and re-ranked results for the CLEF 2006 Robust track for monolingual English, monolingual French, Spanish and Italian with document and topic translation to English, and Spanish bilingual with document translation to English.</figDesc><table coords="5,159.54,74.84,293.05,321.00"><row><cell></cell><cell></cell><cell cols="4">French Spanish Italian Spanish bi</cell></row><row><cell>Baseline (TDN)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>P10</cell><cell>422</cell><cell>395</cell><cell>485</cell><cell>382</cell><cell>357</cell></row><row><cell>P30</cell><cell>265</cell><cell>269</cell><cell>351</cell><cell>262</cell><cell>266</cell></row><row><cell>Av.P</cell><cell>544</cell><cell>470</cell><cell>445</cell><cell>388</cell><cell>314</cell></row><row><cell>RelRet</cell><cell>1496</cell><cell>2065</cell><cell>4468</cell><cell>1736</cell><cell>3702</cell></row><row><cell>f20narr (TDN)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>P10</cell><cell>436</cell><cell>425</cell><cell>507</cell><cell>434</cell><cell>413</cell></row><row><cell>P30</cell><cell>276</cell><cell>294</cell><cell>375</cell><cell>296</cell><cell>300</cell></row><row><cell>Av.P</cell><cell>558</cell><cell>504</cell><cell>478</cell><cell>459</cell><cell>357</cell></row><row><cell>RelRet</cell><cell>1508</cell><cell>2091</cell><cell>4413</cell><cell>1779</cell><cell>3856</cell></row><row><cell>Chg RelRet</cell><cell>+12</cell><cell>+26</cell><cell>-55</cell><cell>+43</cell><cell>+154</cell></row><row><cell>f20re-ranked (TDN)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>P10</cell><cell>433</cell><cell>424</cell><cell>509</cell><cell>434</cell><cell>407</cell></row><row><cell>P30</cell><cell>276</cell><cell>295</cell><cell>377</cell><cell>296</cell><cell>298</cell></row><row><cell>Av.P</cell><cell>558</cell><cell>508</cell><cell>480</cell><cell>459</cell><cell>358</cell></row><row><cell>RelRet</cell><cell>1507</cell><cell>2092</cell><cell>4426</cell><cell>1783</cell><cell>3900</cell></row><row><cell>Chg RelRet</cell><cell>+11</cell><cell>+27</cell><cell>-42</cell><cell>+47</cell><cell>+198</cell></row><row><cell>f20desc (TD)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>P10</cell><cell>396</cell><cell>370</cell><cell>450</cell><cell>398</cell><cell>386</cell></row><row><cell>P30</cell><cell>261</cell><cell>272</cell><cell>358</cell><cell>279</cell><cell>288</cell></row><row><cell>Avep</cell><cell>494</cell><cell>452</cell><cell>435</cell><cell>419</cell><cell>343</cell></row><row><cell>RelRet</cell><cell>1493</cell><cell>2074</cell><cell>4474</cell><cell>1778</cell><cell>3759</cell></row><row><cell>Chg RelRet</cell><cell>+3</cell><cell>+9</cell><cell>+6</cell><cell>+42</cell><cell>+57</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,111.78,484.10,389.16,9.06;5,126.00,495.62,200.36,9.06" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,321.64,484.10,179.30,9.06;5,126.00,495.62,25.25,9.06">TREC2004 Robust Track Experiments using PIRCS</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">L</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Grunfeld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,161.28,495.62,108.33,9.06">Proceedings of TREC 2004</title>
		<meeting>TREC 2004</meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,111.78,507.14,407.42,9.07;5,126.00,518.60,102.80,9.06" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,292.33,507.14,139.64,9.06">Fondazione Ugo Bordoni at TREC</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Romano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,459.46,507.14,59.74,9.06;5,126.00,518.60,46.10,9.06">Proceedings of TREC 2004</title>
		<meeting>TREC 2004</meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,111.78,530.12,383.36,9.07;5,126.00,541.64,283.20,9.06" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,371.09,530.12,124.05,9.06;5,126.00,541.64,111.34,9.06">Cost JHU/APL at TREC 2004: Robust and Terabyte Tracks</title>
		<author>
			<persName coords=""><forename type="first">Christine</forename><surname>Piatko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Scott</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,244.02,541.64,108.49,9.06">Proceedings of TREC 2004</title>
		<meeting>TREC 2004</meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,111.78,553.10,310.34,9.07" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,177.44,553.10,130.29,9.06">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,314.00,553.10,32.27,9.06">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="10" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,111.78,564.62,410.17,9.07;5,126.00,576.08,396.08,9.06;5,126.00,587.60,88.07,9.06" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,448.00,564.62,68.74,9.06">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,230.09,576.08,260.50,9.06">Proceedings of the Third Text REtrieval Conference (TREC-3)</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Third Text REtrieval Conference (TREC-3)</meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="109" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,111.78,599.12,410.37,9.07;5,126.00,610.58,187.94,9.06;5,313.86,608.53,5.04,5.83;5,321.84,610.58,200.14,9.06;5,126.00,622.10,366.18,9.06" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,280.30,599.12,241.84,9.06;5,126.00,610.58,81.02,9.06">Applying Summarization Techniques for Term Selection in Relevance Feedback</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Lam-Adesina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,225.71,610.58,88.23,9.06;5,313.86,608.53,5.04,5.83;5,321.84,610.58,200.14,9.06;5,126.00,622.10,208.39,9.06">Proceedings of the 24 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 24 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New Orleans</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
