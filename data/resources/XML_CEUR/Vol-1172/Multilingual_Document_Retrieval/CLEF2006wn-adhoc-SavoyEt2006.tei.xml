<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,75.90,98.15,443.06,12.58;1,208.68,116.15,177.63,12.58">UniNE at CLEF 2006: Experiments with Monolingual, Bilingual, Domain-Specific and Robust Retrieval</title>
				<funder ref="#_xprw9KD">
					<orgName type="full">Swiss National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,245.82,139.83,56.77,8.74"><forename type="first">Jacques</forename><surname>Savoy</surname></persName>
							<email>jacques.savoy@unine.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of Neuchatel</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,309.73,139.83,53.70,8.74"><forename type="first">Samir</forename><surname>Abdou</surname></persName>
							<email>samir.abdou@unine.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of Neuchatel</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,75.90,98.15,443.06,12.58;1,208.68,116.15,177.63,12.58">UniNE at CLEF 2006: Experiments with Monolingual, Bilingual, Domain-Specific and Robust Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EF1D84B2AB900E28F83B57CAC81885DD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.1 [Content Analysis and Indexing]: Indexing methods</term>
					<term>Linguistic processing. I.2.7 [Natural Language Processing]: Language models. H.3.3 [Information Storage and Retrieval]: Retrieval models. H.3.4 [Systems and Software]: Performance evaluation Experimentation</term>
					<term>Performance</term>
					<term>Measurement</term>
					<term>Algorithms Natural Language Processing with European Languages</term>
					<term>Bilingual Information Retrieval</term>
					<term>Digital Libraries</term>
					<term>Hungarian Language</term>
					<term>Bulgarian Language</term>
					<term>Portuguese Language</term>
					<term>French Language</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For our participation in this CLEF evaluation campaign, the first objective was to propose and evaluate various indexing and search strategies for the Hungarian language in order to produce better retrieval effectiveness than language-independent approach (n-gram). Using both a new stemmer including some derivational suffixes removals, and a more aggressive automatic decompounding scheme, we were able to produce better retrieval effectiveness than corresponding 4-gram indexing scheme. Our second objective was to obtain a better picture of the relative merit of various search engines with the French, Brazilian/Portuguese and Bulgarian languages. To do so we evaluated these test-collections using the Okapi, Divergence from Randomness (DFR) and language model (LM) models together with nine vector-processing approaches. After pseudorelevance feedback, either the DFR or the LM approach tends to produce the best IR performance. For the Bulgarian language, we also found that word-based indexing proposes usually better retrieval effectiveness than corresponding 4-gram indexing.</p><p>In the bilingual track, we evaluated the effectiveness of various machine translation systems to automatically translate a query submitted in English into the French and Portuguese languages. After blind query expansion, the MAP achieved by the best single MT system is around 95% of the corresponding monolingual search when French is the target language, or 83% with the Portuguese. Using the GIRT corpora (available in German and English), we investigated variations in retrieval effectiveness when facing with domain-specific collection composed of relatively short bibliographic notices. Finally, in the robust retrieval task we investigated different techniques in order to improve the retrieval performance of difficult topics. In this track, we found that both the mean average precision and the geometric mean are strongly correlated. Moreover, massive query expansion based on a search engine did not provide better retrieval effectiveness than Rocchio's approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>During the last years, the IR group at University of Neuchatel is involved in designing, implementing and evaluating IR systems for various natural languages, including both European <ref type="bibr" coords="1,385.64,720.57,54.26,8.74">(Savoy 2004a</ref>) and popular Asian <ref type="bibr" coords="1,70.92,731.85,54.18,8.74">(Savoy 2005a</ref>) languages (namely, Chinese, Japanese, Korean). In this context, our first objective is to promote effective monolingual IR in those languages. Our second aim is to design and evaluate effective bilingual search (using a query-based translation approach) and finally to propose effective multilingual IR systems.</p><p>During our participation in various evaluation campaigns, the first lesson learned is the fact that the best performing IR models often require relatively small amount of linguistic knowledge and are usually languageindependent (when an IR model performs well with a given natural language, it tends to perform well with another language). Thus, statistical features present in each natural language seem to be appropriate in general to distinguish between relevant and non-relevant information items. As a second point, general languageindependent indexing strategy (i.e., n-gram indexing <ref type="bibr" coords="2,284.09,158.31,119.36,8.74" target="#b10">(McNamee &amp; Mayfield 2004)</ref>) presents a reasonable level of performance. For Korean and Chinese, such indexing strategy seems even to be the best choice. For European languages not previously studied, at least in the IR domain, such an indexing approach usually provides one of the best retrieval effectiveness (i.e., the best runs in both Bulgarian and Hungarian monolingual tracks at CLEF 2005 were obtained using such an indexing scheme <ref type="bibr" coords="2,343.32,203.43,68.47,8.74" target="#b11">(McNamee 2005)</ref>).</p><p>These two findings must be moderated. With a new language or a new corpus written in a known language or a new set of queries against a known test-collection, we are unable to predict precisely which IR model will achieve the best retrieval effectiveness. From our past experiments, the Okapi probabilistic model <ref type="bibr" coords="2,468.03,243.00,54.23,9.02;2,70.92,254.28,36.25,9.02" target="#b12">(Robertson et al. 2000)</ref> presents usually very good retrieval performance. As a second probabilistic approach, models derived from the Divergence from Randomness (DFR) family <ref type="bibr" coords="2,288.44,265.83,127.65,8.74" target="#b3">(Amati &amp; van Rijsbergen 2002)</ref> provide also high retrieval effectiveness. On the other hand, various implementations based on the language model <ref type="bibr" coords="2,464.16,277.11,39.95,8.74;2,70.92,288.39,22.88,8.74" target="#b6">(Hiemstra 2000;</ref><ref type="bibr" coords="2,96.30,288.39,23.43,8.74" target="#b7">2002)</ref> may also produce the best retrieval performance. Finally, random variations may favor a given IR model for a given set of queries without producing an important performance difference that can be viewed as significant by a statistical test.</p><p>When considering the n-gram indexing strategy, the best choice for the parameter n seems to depend on the language. On the one hand, it seems that for popular Asian languages (i.e., Chinese, Japanese or Korean), the bigram indexing approach provides the best choice (sometimes combined with a character-based indexing strategy). For European languages, the situation is less clear. For example, it seems that the best value of n is 4 for the Bulgarian and Hungarian languages and 5 for the English, French and Portuguese languages <ref type="bibr" coords="2,473.23,373.34,44.99,8.74;2,70.92,384.63,21.62,8.74" target="#b11">(McNamee 2005)</ref>. As usual, the performance differences are not always statistically significant.</p><p>Finally, various parameters like difference in stemming strategies, stopword lists, processing of diacritics and uppercase letters, indexing of noun-phrases, etc. differ from participant to participant. Thus comparisons between two runs imply comparing two IR systems with all their components. It is therefore difficult or even impossible to known precisely the impact of each single component when comparing runs provided by two participants.</p><p>The rest of this paper is organized as follows: Section 2 describes the main characteristics of the CLEF-2006 test-collections, Section 3 outlines the main aspects of our stopword lists and light stemming procedures. Section 4 analyses the principal features of different indexing and search strategies, and evaluates their use with the available corpora. The data fusion approaches adapted in our experiments are explained in Section 5, and Section 6 depicts our official results. Our bilingual experiments are presented and evaluated in Section 7 while Section 8 describes our experiments involving the domain-specific GIRT corpus. Section 9 presents the main results of our participation in the robust retrieval task, limited however to the French language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview of the Test-Collections</head><p>The corpora used in our experiments include newspaper and news agency articles, namely Le Monde <ref type="bibr" coords="2,492.48,586.83,22.37,8.74" target="#b5">(1994</ref><ref type="bibr" coords="2,514.85,586.83,4.47,8.74;2,70.92,598.11,18.10,8.74">-1995</ref><ref type="bibr" coords="2,89.02,597.84,207.43,9.02">, French), Schweizerische Depeschenagentur (1994</ref><ref type="bibr" coords="2,296.44,598.11,22.49,8.74">-1995</ref><ref type="bibr" coords="2,318.93,597.84,99.62,9.02">, French), Público (1994</ref><ref type="bibr" coords="2,418.54,598.11,75.87,8.74">-1995, Portuguese)</ref>, Folha <ref type="bibr" coords="2,97.45,609.12,78.60,9.02">de Sãn Paolo (1994</ref><ref type="bibr" coords="2,176.05,609.39,22.45,8.74">-1995</ref><ref type="bibr" coords="2,198.50,609.12,137.50,9.02">, Brazilian), Magyar Hirlap (2002</ref><ref type="bibr" coords="2,335.99,609.12,101.46,9.02">, Hungarian), Sega (2002</ref><ref type="bibr" coords="2,437.46,609.39,52.30,8.74;2,70.92,620.39,106.96,9.02">, Bulgarian), Standart (2002, Bulgarian)</ref>, Los Angeles Times <ref type="bibr" coords="2,262.32,620.67,61.00,8.74">(1994, English)</ref>, Glasgow <ref type="bibr" coords="2,368.14,620.39,91.97,9.02">Herald (1995, English)</ref>. As shown in Table <ref type="table" coords="2,96.29,631.94,3.76,8.74" target="#tab_1">1</ref>, the English corpus (249.08 indexing terms / document) has a larger mean size article than the Portuguese collection (212.9). This mean value is a little bit lower for the French (178) and relatively similar for the Bulgarian (133.7) and Hungarian (142.1) languages. It is interesting to note that even though the Hungarian collection is the smallest (105 MB), it contains the largest number of distinct indexing terms (657,132), computed after stemming.</p><p>During the indexing process in our automatic runs, we retained only the following logical sections from the original documents: &lt;TITLE&gt;, &lt;TEXT&gt;, &lt;LEAD&gt;, &lt;LEAD1&gt;, &lt;TX&gt;, &lt;LD&gt;, &lt;TI&gt; and &lt;ST&gt;. From the topic descriptions we automatically removed certain phrases such as "Relevant document report …", "Finde Dokumente, die über …", "Keressünk olyan cikkeket, amelyek …" or "Trouver des documents qui …", etc.</p><p>As shown in the Appendix, the available topics cover various subjects (e.g., "Consumer Boycotts," "Doping in Sports," "Theft of "The Scream," or "Grand Slam Winners"), and some of them may cover a relative large domain (e.g. Query #316: "Strikes," or Query #311 "Unemployment in Europe"), including both regional ("Hungarian-Bulgarian Relationships," "New Quebec Premier") or international coverage ("Energy Crises"). For the French, English and Portuguese collection, we had to use Topics #301 to #350 in which Topics#326 to #350 are covering more specifically the year 1994-95 (e.g., "Civil War in the Yemen," "Nixon's Death"). For the Hungarian and Bulgarian corpus, the query set is formed by Topics #301 to #325 and Topics #351 to #375 (covering more specifically the year 2002-2003 like "The Harry Potter Phenomenon," "Impact of September 11").  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>French</head><formula xml:id="formula_0" coords="3,93.60,425.91,422.56,20.02">(Q#316) Minimum 1 (Q#336) 2 (Q#334) 2 (Q#301) 4 (Q#367) 1 (Q#306)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Stopword Lists and Stemming Procedures</head><p>During this evaluation campaign, we mainly used the stopword lists and stemmers used in our CLEF 2005 participation <ref type="bibr" coords="3,124.10,524.19,92.76,8.74" target="#b18">(Savoy &amp; Berger 2006)</ref>. However, we corrected some errors in our Bulgarian stopword list (we removed words having a clear meaning and introduced by mistake in the suggested stopword list).</p><p>For the Hungarian language, our suggested stemmer removes only inflectional suffixes for this language. This year, we have tried to be more aggressive and we added 17 rules in our Hungarian stemmer to remove also some derivational suffixes (e.g., "jelent" (to mean) and "jelentés" (meaning), or "tánc" (to dance) and "táncol" (dance)). Moreover, the Hungarian language uses compound constructions (e.g., handgun, worldwide). In order to increase the matching between search keywords and document representations, we automatically decompounding Hungarian words using our decompounding algorithm <ref type="bibr" coords="3,359.05,609.15,57.58,8.74" target="#b15">(Savoy 2004b)</ref>, leaving both compound words and their component parts in the documents and queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IR models and Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Indexing and Searching Strategies</head><p>In order to obtain a broader view of the relative merit of various retrieval models, we may first adopt a binary indexing scheme in which each document (or request) was represented by a set of keywords, without any weight. To measure the similarity between documents and requests, we computed the inner product (retrieval model denoted "doc=bnn, query=bnn" or "bnn-bnn"). In order to weight the presence of each indexing term in a document surrogate (or in a query), we took the term occurrence frequency into account (denoted tf ij for indexing term t j in document D i , and the corresponding retrieval model was denoted: "doc=nnn, query=nnn"). We might also account for their inverse document frequency (denoted idf j ). Moreover, we might normalize each indexing weight using different weighting schemes, as is described in the Appendix.</p><p>In addition to these models based on the vector-space paradigm, we also considered probabilistic models such as the Okapi model (or BM25) <ref type="bibr" coords="4,217.82,124.19,91.31,9.02" target="#b12">(Robertson et al. 2000)</ref>. As a second probabilistic approach, we implemented four variants of the DFR (Divergence from Randomness) family suggested by <ref type="bibr" coords="4,440.23,135.74,52.27,8.74;4,70.92,147.03,71.20,8.74" target="#b3">Amati &amp; van Rijsbergen (2002)</ref>. In this framework, the indexing weight w ij attached to term t j in document D i combines two information measures as follows:</p><formula xml:id="formula_1" coords="4,126.90,173.41,218.38,11.65">w ij = Inf 1 ij • Inf 2 ij = -log 2 [Prob 1 ij (tf)] • (1 -Prob 2 ij (tf))</formula><p>The first model called GL2 is based on the following equations:</p><formula xml:id="formula_2" coords="4,126.90,207.97,396.99,30.01">Prob 2 ij = tfn ij / (tfn ij + 1) with tfn ij = tf ij • log 2 [1 + ((c • mean dl) / l i )] (1) Prob 1 ij = [1 / (1+λ j )] • [λ j / (1+λ j )] tfn ij with λ j = tc j / n (2)</formula><p>For the second model called PL2, only the implementation of Prob 1 ij is modified as:</p><formula xml:id="formula_3" coords="4,126.90,262.63,396.99,12.97">Prob 1 ij = (e -λj • λ tfij ) / tf ij ! with λ j = tc j / n (3)</formula><p>For the third model called I(F)L2, only the implementation of Prob 1 ij is modified as: Prob 1 ij = log 2 [(tc j +0.5) / (n+1)] tf ij (4)</p><p>For the four model called PB2, the implementation of Prob 1 ij is given by Equation <ref type="formula" coords="4,416.48,319.95,3.77,8.74">3</ref>, and</p><formula xml:id="formula_4" coords="4,126.90,335.05,396.98,11.65">Prob 2 ij = 1-[(tc j +1) / (df j • (tf ij +1))]<label>(5)</label></formula><p>where tc j represents the number of occurrences of term t j in the collection, df j the number of documents in which the term t j appears, and n the number of documents in the corpus. In our experiments, the constants b, k 1 , avdl, pivot, slope, c and mean dl were fixed according to the values given in the Appendix).</p><p>Finally, we also considered an approach based on a language model (LM) <ref type="bibr" coords="4,384.23,394.35,65.40,8.74" target="#b6">(Hiemstra 2000;</ref><ref type="bibr" coords="4,452.13,394.35,21.65,8.74" target="#b7">2002)</ref>, known as a non-parametric probabilistic model (the Okapi and DFR are viewed as parametric models). Probability estimates would thus not be based on any known distribution (as in Equation 2, 3 or 4) but rather be estimated directly, based on occurrence frequencies in document D or corpus C. Within this language model paradigm, various implementations and smoothing methods might be considered, and in this study, we adopted a model proposed by <ref type="bibr" coords="4,122.82,450.75,64.28,8.74" target="#b7">Hiemstra (2002)</ref>, as described in Equation 6, which combines an estimate based on document (P[t j | D i ]) and on corpus (P[t j | C]).</p><formula xml:id="formula_5" coords="4,126.90,478.21,396.99,24.91">P[D i | Q] = P[D i ] . ∏ t j ∈Q [λ j . P[t j | D i ] + (1-λ j ) . P[t j | C]] with P[t j | D i ] = tf ij /l i and P[t j | C] = df j /lc with lc = ∑ k df k (6)</formula><p>where λ j is a smoothing factor (constant for all indexing terms t j , and usually fixed at 0.35) and lc the size of the corpus C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Overall Evaluation</head><p>To measure the retrieval performance, we adopted the mean average precision (MAP) (computed on the basis of 1,000 retrieved items per request by the new TREC-EVAL program). Using this evaluation tool, some evaluation difference may occur with the values computed according to the official measure (the latter always takes account for 50 queries). In the following tables, the best performance under the given conditions (with the same indexing scheme and the same collection) is listed in bold type.</p><p>Table <ref type="table" coords="4,110.51,626.31,5.01,8.74" target="#tab_2">2</ref> shows the MAP achieved by four probabilistic models and nine vector-space schemes using the French or the Portuguese collection and three different query formulations (title-only or T, TD, and TDN). In the last lines we have reported the MAP average over these 13 IR models, the average computed over the first ten IR models (ending with "doc=ltc, query=ltc"), and the percentage of improvement over the short (T) query formulation. From this data, we can see that the set of the best performing IR models correspond to the probabilistic one, more the "Lnu-ltc" and "dtu-dtn" vector-space models. As depicted in the last line, increasing the query improves the MAP, but the enhancement is lower than our prior estimation (+15%).</p><p>Table <ref type="table" coords="4,110.50,711.26,5.01,8.74" target="#tab_3">3</ref> reports the evaluations done with the Bulgarian and Hungarian languages (word-based indexing). In this table, the two last lines indicates the MAP average computed over the top-8 IR models (ending with "doc=ltc, query=ltc"), and the percentage of improvement over the short (T) query formulation. Mainly, the same conclusions can be drawn for these two languages. First the probabilistic models expose the best IR performance and secondly the improvement over the short query formulation is usually greater than 10%. It could be a surprise to see that the vector-space model "dtu-dtn" produces the best performance for the Hungarian language. From a statistical point of view, the difference in performance with the Okapi model could be due to random variations and could be not statistically significant.  For this year, we tried to investigate more deeply the Hungarian language on the one hand and, on the other we implemented various IR models based on the Lucene (open source available at lucene.apache.org) search engine. In Table <ref type="table" coords="5,141.52,657.63,3.77,8.74" target="#tab_4">4</ref>, we reported some experiments done with our new Lucene version using a word-based indexing scheme (TD queries) with a stemmer removing some derivational suffixes (see Section 3). In the third column, we automatically decompounded long words (composed by more than 8 characters) using our own algorithm <ref type="bibr" coords="5,112.33,691.47,57.55,8.74" target="#b15">(Savoy 2004b)</ref>. In this experiment, both the compound words and their components were left in documents and queries. Finally, we have also reported the MAP achieved by the 4-gram approach (the best performance of last year, without stemming). The data depicted in Table <ref type="table" coords="5,366.50,714.03,5.01,8.74" target="#tab_4">4</ref> shows that, inside a given indexing strategy, all probabilistic approaches present similar level of performance. Compared to the word-based indexing strategy, the 4-gram indexing approach proposes, in average, an improvement of +6.91%, while the decompounded indexing method exposes the best performance (+8.08% better than the word-based approach).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean average precision</head><p>Finally, some differences could appear between Table <ref type="table" coords="6,290.45,73.35,5.01,8.74" target="#tab_3">3</ref> (computed with our version of the SMART system) and evaluations computed with our Lucene system (Table <ref type="table" coords="6,288.39,84.63,3.62,8.74" target="#tab_4">4</ref>). It was observed that pseudo-relevance feedback (PRF or blind-query expansion) seemed to be a useful technique for enhancing retrieval effectiveness. In this study, we adopted Rocchio's approach <ref type="bibr" coords="6,450.50,291.84,46.38,9.02;6,70.92,304.20,36.32,9.02" target="#b4">(Buckley et al. 1996)</ref> with α = 0.75, β = 0.75, whereby the system was allowed to add m terms extracted from the k best ranked documents from the original query. To evaluate this proposition, we used four probabilistic models and enlarged the query by the 20 to 50 terms retrieved from the 10 best-ranked articles with the French corpus (Table <ref type="table" coords="6,99.64,338.31,4.18,8.74" target="#tab_5">5</ref>) and Brazilian/Portuguese collection (Table <ref type="table" coords="6,284.95,338.31,3.62,8.74" target="#tab_6">6</ref>). For the Bulgarian language, we have evaluated both a word-based indexing (top part of Table <ref type="table" coords="6,463.97,644.01,4.18,8.74">7</ref>) and 4-gram indexing strategy (bottom part of Table <ref type="table" coords="6,231.37,655.29,3.63,8.74">7</ref>). First it is interesting to note that both before and after blind query expansion, the word-based indexing approach provides always a better MAP than the corresponding 4-gram approach. When considering word-based indexing, the use of a blind query expansion improves the MAP from +9.2% (Okapi model, 0.2614 vs. 0.2854) to +21.7% (GL2 model, 0.2734 vs. 0.3327). With the 4-gram indexing approach, the enhancement goes from +12% (Okapi model, 0.2528 vs. 0.2831) to +33% (LM model, 0.2380 vs. 0.3165). Of course, the number of "terms" added with the 4-gram indexing scheme is usually greater than the corresponding run using a word-based indexing strategy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean average precision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean average precision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Some Query-by-Query Analysis</head><p>In order to obtain some explanation of our failures, we decided to inspect the queries achieving a MAP below 0.1 for all IR models defined in Section 4.2. For the French collection, we found three such queries. The most difficult was Query #320 ("Energy Crises") for which the best MAP is 0.0368, obtained by DFR-GL2. In this case, terms in the query ("crises de l'énergie") cannot retrieved in a high position relevant documents where pertinent sentences included formulation like "crise énergétique" (stemming problem: "crise" or "crises" → "cris" but "énergie" → "energ" and "énergétique" → "energet") or "pénurie d'énergie" (synonyms, in the current context between "pénurie" and "crise").</p><p>With the French corpus, in the second position we found Query #336 ("NBA Labour Conflicts") for which the best MAP is 0.0667, obtained by LM model. In this case, the terms used in the query ("Labour Conflicts" or "conflits de travail") retrieved a lot of non-relevant items. The single relevant article for this query used mainly the term "strikes," "base-ball" and "ice-hockey" and the single word in common is "conflicts". Finally, we have Query #309 ("Hard Drugs") for which the best MAP is 0.0726, obtained by Okapi model with blind-query expansion (50 documents / 50 terms). In this case, the first relevant document appears in rank 20. This topic retrieves a relatively large number of documents (having more than one term in common with the query) describing however other non relevant aspects.</p><p>With the Brazilian/Portuguese corpus, we found four queries for which the best MAP is below 0.1. First we have Query #340 ("New Quebec Premier") for which the best MAP is 0.003, obtained by DFR-PL2 (with blindquery expansion; 10 documents / 50 terms). The second most difficult topic was Query #320 ("Energy Crises") for which the best MAP is 0.0066, achieved by the language model. In the third position, we have Query #327 ("Earthquakes in Mexico City") for which the best MAP is 0.0372 (DFR-GL2 &amp; blind-query expansion, 10 documents / 40 terms). Finally, we have Query #344 ("Brazil vs. Sweden World Cup Semifinals") with a best MAP of 0.0434 obtained by DFR-PL" model (with blind-query expansion, 10 documents / 20 terms).</p><p>With the Hungarian collection based on a 4-gram indexing scheme, we found six queries for which the best MAP is below 0.1 <ref type="bibr" coords="8,146.53,135.75,53.60,8.74">(Query #371,</ref><ref type="bibr" coords="8,202.68,135.75,22.63,8.74">#357,</ref><ref type="bibr" coords="8,227.85,135.75,22.63,8.74">#374,</ref><ref type="bibr" coords="8,253.02,135.75,22.63,8.74">#317,</ref><ref type="bibr" coords="8,278.19,135.75,22.63,8.74">#362,</ref><ref type="bibr" coords="8,303.36,135.75,38.71,8.74">and #320)</ref>. With our decompounding approach, the "difficult" queries formed a subset of the previous one (namely <ref type="bibr" coords="8,327.31,147.03,50.21,8.74">Query #357,</ref><ref type="bibr" coords="8,380.06,147.03,22.61,8.74">#320,</ref><ref type="bibr" coords="8,405.21,147.03,22.61,8.74">#374,</ref><ref type="bibr" coords="8,430.36,147.03,38.68,8.74">and #371)</ref>. If we consider also word-based indexing strategy, we can find a set of seven hard topics that includes Query #357, #374, and #371. These three topics are the most difficult queries for the Hungarian corpus, independently of the underlying indexing scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Data Fusion</head><p>It is assumed that combining different search models should improve retrieval effectiveness, due to the fact that different document representations might retrieve different pertinent items and thus increase the overall recall <ref type="bibr" coords="8,95.73,258.27,91.03,8.74" target="#b20">(Vogt &amp; Cottrell 1999)</ref>. In this current study we combine two or three probabilistic models representing both the parametric (Okapi and DFR) and non-parametric (language model) approaches. To achieve this we evaluated various fusion operators (see Table <ref type="table" coords="8,255.50,280.83,5.01,8.74">9</ref> for a list of their precise descriptions). For example, the Sum RSV operator indicates that the combined document score (or the final retrieval status value) is simply the sum of the retrieval status value (RSV k ) of the corresponding document D k computed by each single indexing scheme <ref type="bibr" coords="8,103.41,314.67,78.24,8.74" target="#b5">(Fox &amp; Shaw 1994)</ref>. Table <ref type="table" coords="8,216.40,314.67,5.01,8.74">9</ref> thus illustrates how both the Norm Max and Norm RSV apply a normalization procedure when combining document scores. When combining the retrieval status value (RSV k ) for various indexing schemes and in order to favor some more efficient retrieval schemes, we could multiply the document score by a constant α i (usually equal to 1) reflecting the differences in retrieval performance.</p><formula xml:id="formula_6" coords="8,112.56,368.55,370.00,54.41">Sum RSV SUM (α i . RSV k ) Norm Max SUM (α i . (RSV k / Max i )) Norm RSV SUM [α i . ((RSV k -Min i ) / (Max i -Min i ))] Z-Score α i . [((RSV k -Mean i ) / Stdev i ) + δ i ] with  δ i = [(Mean i -Min i ) / Stdev i ]</formula><p>Table <ref type="table" coords="8,198.84,434.40,3.91,9.02">9</ref>: Data fusion combination operators used in this study</p><p>In addition to using these data fusion operators, we also considered the round-robin approach, wherein we took one document in turn from all individual lists and removed any duplicates, retaining the most highly ranked instance. Finally we suggested merging the retrieved documents according to the Z-Score, computed for each result list. Within this scheme, for the ith result list, we needed to compute the average RSV k value (denoted Mean i and the standard deviation (denoted Stdev i ). Based on these we could then normalize the retrieval status value for each document D k provided by the ith result list by computing the deviation of RSV k with respect to the mean (Mean i ). In Table <ref type="table" coords="8,170.18,519.63,3.76,8.74">9</ref>, Min i (Max i ) denotes the minimal (maximal) RSV value in the ith result list. Of course, we might also weight the relative contribution of each retrieval scheme by assigning a different α i value to each retrieval model.  Table <ref type="table" coords="9,110.50,73.35,10.03,8.74" target="#tab_9">10</ref> depicts the evaluation of various data fusion operators, comparing them to the single approach using the language model (LM), Okapi or DFR probabilistic models. From this data, we can see that combining two or three IR models might improve retrieval effectiveness, slightly for the French collection, moderately for the Portuguese and clearly for the Hungarian corpus. When combining different retrieval models, the Z-Score scheme tended to perform the best. Compared to the best single search model, the performance achieved by the various data fusion approaches seems not to be statistically significant, except for the Hungarian corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean average precision (% of change</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Official Results</head><p>Table <ref type="table" coords="9,110.50,184.59,10.03,8.74" target="#tab_10">11</ref> shows the exact specifications of our 12 official monolingual runs. These experiments were mainly based on the probabilistic models (Okapi, DFR and language model (LM)). All runs are fully automatic using the TD query formulation and using often a data fusion approach (often based on the Z-Score operator).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run name</head><p>Language  From this data, we can see that for the French collection the best translation is obtained by Reverso (95% of the performance level achieved by a monolingual search) and for the Portuguese corpus by Online (84% of the performance level achieved by the corresponding monolingual search). From a more general point of view, both Reverso (Promt) and Online MT systems obtain satisfactory retrieval performances for both languages. For the French language, both the FreeTranslation and BabelFish present also an overall good performance. Starting with queries written in English, data depicted in Table <ref type="table" coords="10,291.06,629.30,10.05,8.74" target="#tab_12">13</ref> indicates also that the automatic translation process performs better with French as target language than with Portuguese.</p><p>Table <ref type="table" coords="10,110.50,657.86,10.03,8.74" target="#tab_14">14</ref> shows the retrieval effectiveness for various query translation combinations (concatenation of the translations produced by two or more MT systems) when using the Okapi probabilistic model with blind query expansion (40 terms extracted from the ten-best retrieved items). The top part of the table indicates the exact query translation combination used while the bottom part shows the MAP obtained with our combined query translation approach. The resulting retrieval performances depicted in Table <ref type="table" coords="10,380.98,702.98,10.03,8.74" target="#tab_14">14</ref> are never better than the best single translation scheme (row labeled "Best single") for the French language and usually slightly better for the Portuguese language.  Finally, Table <ref type="table" coords="11,143.88,584.79,10.03,8.74" target="#tab_15">15</ref> lists the parameter settings used for our 6 official runs in the bilingual task. Each experiment uses queries written in English to retrieve documents in the other target languages. Before combining the result lists using a data fusion operator (see Section 5), we automatically expanded the translated queries using a pseudo-relevance feedback method (Rocchio's approach in the present case).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Monolingual Domain-Specific Retrieval: GIRT</head><p>In the domain-specific retrieval task (called GIRT), the two available corpora are composed of bibliographic records extracted from various sources in the social sciences domain, see <ref type="bibr" coords="11,366.20,684.75,49.15,8.74" target="#b8">(Kluck 2004</ref>) for a more complete description of these corpora. A few statistics on these collections are given in Table <ref type="table" coords="11,411.50,696.03,8.37,8.74" target="#tab_17">16</ref>. The English corpus is a manually translation of the German documents, the whole document is however not always fully translated.</p><p>A typical record in this collection is composed of a title, an abstract, and a set of manually assigned keyword. Additional information such as authors' name, publication date, or the language in which the bibliographic notice is written may of course be less important from an IR perspective but they are made available. As depicted in the Appendix, the topics in this domain-specific collection cover a variety of themes (e.g., "Poverty", "Role of the father", "The computer in the everyday", or "Modernizing of public administration").  Table <ref type="table" coords="12,110.52,396.39,10.04,8.74" target="#tab_19">17</ref> shows the MAP of various query formulations for the German and English. The best retrival models are usually the Okapi probabilistic model or the DFR-GL2. The language model (LM) achieves also a very good retrieval effectiveness with these test-collections. If we see a clear improvement from T query formulation to TD, the performance difference between TD and TDN query formulation is relatively small for both languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>German</head><p>In order to improve the search performance, we have considered a pseudo-relevance feedback using the Rocchio's formulation (see Table <ref type="table" coords="12,208.00,470.07,10.04,8.74" target="#tab_7">18</ref> for the German corpus, and Table <ref type="table" coords="12,358.61,470.07,10.04,8.74" target="#tab_20">19</ref> for the English collection). Such query expansion clearly improves the mean average precision.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean average precision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Robust Retrieval Track</head><p>The aim of this track consists to analyze and to improve IR systems when facing with "difficult" topics <ref type="bibr" coords="13,70.92,693.99,61.69,8.74" target="#b21">(Voorhees 2004</ref>). In the current context, difficult topics means queries having a poor mean average precision in previous evaluation campaigns. We also knew that topic difficulty depend on the underling collection <ref type="bibr" coords="13,70.92,716.55,66.02,8.74">(Voorhees 2005)</ref>. The goal of the robust track is therefore to explore how one can build a search system that can perform "reasonably well" for all queries. This question is a main concern when evaluating real systems with users facing with unexpected search results or "stupid" answers returned by a search engine.</p><p>It is known that determining a priori whether a given topic is difficult of not, seems to be impossible <ref type="bibr" coords="14,70.92,84.63,66.03,8.74" target="#b21">(Voorhees 2004)</ref>. Therefore, the queries created and evaluated during the <ref type="bibr" coords="14,370.38,84.63,121.67,8.74">CLEF-2001 (Query #41 -#90)</ref>, <ref type="bibr" coords="14,70.92,95.91,128.49,8.74">CLEF-2002 (Query #91 -#140)</ref> and <ref type="bibr" coords="14,219.00,95.91,133.46,8.74">CLEF-2003 (Query #141 -#200)</ref> evaluation campaigns have been reused against mainly the same document collection. Moreover, the organizers divided arbitrary this query set into a training set (60 queries) and a test set (100 queries). In fact, in this latter set, 9 queries do not have any relevant items and thus the test set is formed by only 91 remaining queries. When analyzing this sample, we found that the mean number of relevant items per query was 24.066 (median: 14, min: 1, max: 177, standard deviation: 30.78).</p><p>When evaluating an IR system with previously created queries, we need to search into the same documents collection. With the French language, the CLEF-2001 and CLEF-2002 campaigns have used the newspaper Le Monde <ref type="bibr" coords="14,101.34,192.15,46.41,8.74" target="#b5">(1994), and</ref><ref type="bibr" coords="14,150.30,191.87,193.48,9.02">Schweizerische Depeschenagentur (SDA, 1994)</ref> to generate a collection composed of 87,191 documents. During the CLEF-2003 campaign, 42,615 documents extracted from the SDA during the year 1995 have been added (the size of the final corpus is therefore of 129,806 articles). Thus for Query #41 to #140 (corresponding to 59 queries in the test set) we do not have any judgments against SDA 95 (and retrieved items not judged are assumed non relevant). If we remove all references to SDA95 in the result list, the MAP will change and increase. For example, with the Okapi model and TQ queries, the MAP is 0.4816 and after removing the SDA.95 references, the MAP increases to 0.5322 (+10.5%).</p><p>When using the MAP to measure the retrieval effectiveness, all observations (queries) have the same importance or weight. It is known that the average measure may hide irregularities among the observations (but owns, of course, the advantage to resume a large number of observations with a single number). Thus incorrect answers provided by the search engine are not really penalized by the arithmetic mean. Attaching the same importance to all topics has the following problem. If, for example, a search system improves its performance from 0.5 to 0.6 for one query, this enhancement is viewed as similar to a system improving its performance for a difficult query from 0.02 to 0.12. Replacing the arithmetic mean by the geometric mean, the second improvement in our example will have a greater impact than the first. Of course, other evaluation measure could be considered such as the median <ref type="bibr" coords="14,207.53,367.34,52.58,8.74" target="#b13">(Savoy 1997)</ref>.</p><p>As a first experiment, we want to verify the retrieval effectiveness (both the mean average precision (or MAP) and the geometric mean (GMAP)) using short topic formulation (T), medium (TD) or long topic description (TDN). The results are depicted in Table <ref type="table" coords="14,286.69,407.18,8.37,8.74" target="#tab_22">21</ref>. In the third line of this table, we have indicated the mean number of distinct terms in the three different query formulations (ranging from a mean value of 2.91 different terms for title-only formulation, to 16.32 for the TDN query formulation). In the last two lines we have indicated the overall mean and the mean when considering only the first 7 IR models (ending with the line labeled "doc=ltn, query=ntc"). The MAP presents always a higher value than the geometric mean but both evaluations are strongly correlated (r=0.9624). As a single IR system, the Okapi probabilistic model exposes always the best performance (either measured by the MAP or the GMAP). For our investigations, the short query formulation (T) will represent the starting point. With this short query formulation, the three most difficult queries were Query#200 ("Innondationeurs en Hollande et en Allemagne") with the best performance for a single IR system is 0.0002 (Lnu-ltc), Query #91 ("AI en Amérique latine", best MAP: 0.0012, Lnu-ltc), and Query #48 ("Forces de maintien de la paix en Bosnie", best MAP: 0.0077, ltn-ntc). We could note that the term "Innondationeurs" in not a French word and the right term must be "Innondations" that appears in the descriptive part of Query #200.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head><p>If the user may introduce more search terms (comparing T with TD performance), the overall IR performance measure by the geometric mean increases from 0.1898 to 0.3205 (or +69%). With this mediumsize query description, the three most difficult queries are Query #48 (best MAP: 0.028, Lnu-ltc), Query #61 ("Catastrophe petrolière en Sibérie", best MAP: 0.0293, Lnu-ltc), and Query #148 ("Dommages à la couche d'ozone", best MAP: 0.0507, Okapi). It is a surprise to still encounter Query #48 in the top three most difficult queries. The descriptive part of this query adds new and related terms ("Nations Unies", "Kosovo") without providing a positive impact of the performance. The most difficult topic with T query (Query #200) is now in the 6 th rank (MAP: 0.085). The inclusion of the correct term "Innondations" improves the search process the performance is still relatively low. For Query #91 appearing in the second position with T queries, it now occurs in rank 7 th (MAP: 0.0954).</p><p>When comparing short (T) with the longest query formulation (TDN), the IR performance measured by the geometric mean doubles (from 0.1898 to 0.3839, or +102%). With TDN formulation, the most difficult topics are Query #48 (best MAP: 0.048, atn-ntc), Query #148 (best MAP: 0.0507, Okapi), and Query #90 ("Les exportateurs de légumes", best MAP: 0.113, Okapi). The most difficult topic using T formulation (Query #200) appears now in rank 19 th with a MAP of 0.353 while Query #91 (in the second position with short formulation) occurs in rank 6 th (MAP: 0.1343).</p><p>As another way to improve the performance, we may employ a pseudo-relevance feedback procedure (Rocchio in this case). The performance indicated in Table <ref type="table" coords="15,311.97,316.95,10.03,8.74" target="#tab_23">22</ref> shows that the overall performance improves after adding a relatively small number of new terms (e.g., 15) extracted from the best 5 retrieved items. We may also apply a data fusion approach as described in Section 5. Such a procedure has been applied to form two of our official runs, namely UniNEfrr1 with TD query, and UniNEfrr2 with T query (complete description given in Table <ref type="table" coords="15,178.56,542.25,7.96,8.74" target="#tab_24">23</ref>). With the last run (UniNEfrr3), we have exploited the Web, or more precisely the Yahoo! search engine. We have sent to this Web search engine the title of the queries. As an answer, we obtained a page with ten references with, for each, a short description. We have extracted these ten short textual descriptions and added them to the original query. The expanded query has been sent to our search model in order to hopefully obtain a better result list. When using TD query formulation, the mean number of distinct search terms was 7.51. When including the first ten references retrieved by Yahoo.fr, this average value increased to 115.46 (meaning that we have added, in mean, 108 new search terms). Such a massive query expansion was not effective (see results depicted in Table <ref type="table" coords="16,143.10,95.91,8.92,8.74" target="#tab_24">23</ref>) and we need to include a term selection procedure to hopefully improve the geometric mean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run name</head><p>In fact our first intent was to use a French or Swiss newspaper Web site to find related terms. We think that we need to use, of course the same language (French in this case, but differences in meaning exist between the French expressions in Montreal and in Geneva), but also to have similar cultural and regional coverage (e.g, news from Switzerland differ from those in Canada) together with comparable thematic coverage (e.g., a general vs. a business-oriented newspaper) and comparable writing style (e.g., all newspapers do not have the same clients like "The Sun" and "The Times" in England). However, the time difference could also be problematic (the searched corpus is composed of articles written during the year 1994-95).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Conclusion</head><p>In this seventh CLEF evaluation campaign, we proposed a more effective IR model for the Hungarian language. We have considered a more aggressive stemmer that tries to remove some frequent derivational suffixes for this language. We also investigated the relative merit of the word-based and 4-gram indexing scheme. We have also evaluated an automatic decompouding scheme for the Hungarian language. Combining different indexing and retrieval schemes for this language seems to be really effective but requires more processing time and disk space.</p><p>For the French, Brazilian/Portuguese and Bulgarian language, we used the same stopword lists and stemmers developed during the previous years. In order to enhance retrieval performance, we have implemented an IR model based on the language model and have suggested a data fusion approach based on the Z-Score after applying a blind query expansion. Such general search strategy seems also effective for the GIRT corpora (German and English).</p><p>In the bilingual task, the freely available translation tools performed at a reasonable level for both the French and Portuguese languages (based on the best translation tool, the MAP compared to the monolingual search is around 95% for the French language and 83% for the Brazilian/Portuguese). Finally, in the robust retrieval task, we investigated the reasons and means to improve the retrieval effectiveness when facing with difficult topics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,205.92,457.80,183.14,9.02"><head>Table 1 :</head><label>1</label><figDesc>CLEF 2006 test-collection statistics</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,76.56,136.71,441.99,458.62"><head>Table 2 :</head><label>2</label><figDesc>Mean average precision of various IR models and query formulations (French &amp; Brazilian/Portuguese languages)</figDesc><table coords="5,76.56,136.71,441.99,458.62"><row><cell></cell><cell>French</cell><cell>French</cell><cell>French</cell><cell cols="3">Portuguese Portuguese Portuguese</cell></row><row><cell>Query</cell><cell>T</cell><cell>TD</cell><cell>TDN</cell><cell>T</cell><cell>TD</cell><cell>TDN</cell></row><row><cell>Model \ # of queries</cell><cell>49 queries</cell><cell>49 queries</cell><cell>49 queries</cell><cell>50 queries</cell><cell>50 queries</cell><cell>50 queries</cell></row><row><cell>doc=Okapi, query=npn</cell><cell>0.3601</cell><cell>0.4151</cell><cell>0.4489</cell><cell>0.3947</cell><cell>0.4333</cell><cell>0.4388</cell></row><row><cell>DFR GL2</cell><cell>0.3352</cell><cell>0.3988</cell><cell>0.4457</cell><cell>0.3945</cell><cell>0.4033</cell><cell>0.4406</cell></row><row><cell>DFR PL2</cell><cell></cell><cell>0.4101</cell><cell></cell><cell></cell><cell>0.4147</cell><cell></cell></row><row><cell>LM (λ=0.35)</cell><cell></cell><cell>0.3913</cell><cell></cell><cell></cell><cell>0.3909</cell><cell></cell></row><row><cell>doc=Lnu, query=ltc</cell><cell>0.3156</cell><cell>0.3738</cell><cell>0.4059</cell><cell>0.3711</cell><cell>0.4212</cell><cell>0.4335</cell></row><row><cell>doc=dtu, query=dtn</cell><cell>0.3171</cell><cell>0.3781</cell><cell>0.3960</cell><cell>0.3713</cell><cell>0.4172</cell><cell>0.4257</cell></row><row><cell>doc=atn, query=ntc</cell><cell>0.3164</cell><cell>0.3808</cell><cell>0.4134</cell><cell>0.3475</cell><cell>0.3854</cell><cell>0.4043</cell></row><row><cell>doc=ltn, query=ntc</cell><cell>0.3051</cell><cell>0.3453</cell><cell>0.3577</cell><cell>0.3277</cell><cell>0.3567</cell><cell>0.3683</cell></row><row><cell>doc=ntc, query=ntc</cell><cell>0.2151</cell><cell>0.2606</cell><cell>0.2658</cell><cell>0.2664</cell><cell>0.2959</cell><cell>0.3114</cell></row><row><cell>doc=ltc, query=ltc</cell><cell>0.2115</cell><cell>0.2511</cell><cell>0.2703</cell><cell>0.2695</cell><cell>0.3112</cell><cell>0.3395</cell></row><row><cell>doc=lnc, query=ltc</cell><cell>0.2083</cell><cell>0.2602</cell><cell>0.2900</cell><cell>0.2702</cell><cell>0.3227</cell><cell>0.3498</cell></row><row><cell>doc=bnn, query=bnn</cell><cell>0.1594</cell><cell>0.1628</cell><cell>0.1258</cell><cell>0.1904</cell><cell>0.2037</cell><cell>0.1426</cell></row><row><cell>doc=nnn, query=nnn</cell><cell>0.1352</cell><cell>0.1475</cell><cell>0.1412</cell><cell>0.1109</cell><cell>0.1154</cell><cell>0.0929</cell></row><row><cell>Mean</cell><cell>0.2544</cell><cell>0.3061</cell><cell>0.3115</cell><cell>0.2920</cell><cell>0.3321</cell><cell>0.3307</cell></row><row><cell>Mean over top-10 models</cell><cell>0.2916</cell><cell>0.3495</cell><cell>0.3654</cell><cell>0.3355</cell><cell>0.3765</cell><cell>0.3888</cell></row><row><cell>% change over T</cell><cell></cell><cell>+18.05%</cell><cell>+26.41%</cell><cell></cell><cell>+10.68%</cell><cell>+15.29%</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Mean average precision</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Bulgarian</cell><cell>Bulgarian</cell><cell>Bulgarian</cell><cell cols="3">Hungarian Hungarian Hungarian</cell></row><row><cell>Query</cell><cell>T</cell><cell>TD</cell><cell>TDN</cell><cell>T</cell><cell>TD</cell><cell>TDN</cell></row><row><cell>Model \ # of queries</cell><cell>50 queries</cell><cell>50 queries</cell><cell>50 queries</cell><cell>48 queries</cell><cell>48 queries</cell><cell>48 queries</cell></row><row><cell>doc=Okapi, query=npn</cell><cell>0.2661</cell><cell>0.2776</cell><cell>0.3079</cell><cell>0.2838</cell><cell>0.3149</cell><cell>0.3256</cell></row><row><cell>DFR GL2</cell><cell>0.2578</cell><cell>0.2645</cell><cell>0.2964</cell><cell>0.2708</cell><cell>0.3043</cell><cell>0.3306</cell></row><row><cell>doc=Lnu, query=ltc</cell><cell>0.2413</cell><cell>0.2663</cell><cell>0.2848</cell><cell>0.2716</cell><cell>0.3107</cell><cell>0.3216</cell></row><row><cell>doc=dtu, query=dtn</cell><cell>0.2479</cell><cell>0.2527</cell><cell>0.2823</cell><cell>0.2817</cell><cell>0.3224</cell><cell>0.3348</cell></row><row><cell>doc=atn, query=ntc</cell><cell>0.2395</cell><cell>0.2520</cell><cell>0.2777</cell><cell>0.2810</cell><cell>0.3191</cell><cell>0.3320</cell></row><row><cell>doc=ltn, query=ntc</cell><cell>0.2289</cell><cell>0.2375</cell><cell>0.2492</cell><cell>0.2705</cell><cell>0.2956</cell><cell>0.3138</cell></row><row><cell>doc=ntc, query=ntc</cell><cell>0.1743</cell><cell>0.1898</cell><cell>0.2131</cell><cell>0.2297</cell><cell>0.2574</cell><cell>0.2677</cell></row><row><cell>doc=ltc, query=ltc</cell><cell>0.1788</cell><cell>0.1982</cell><cell>0.2271</cell><cell>0.2244</cell><cell>0.2654</cell><cell>0.2904</cell></row><row><cell>doc=lnc, query=ltc</cell><cell>0.1901</cell><cell>0.2089</cell><cell>0.2456</cell><cell>0.2154</cell><cell>0.2547</cell><cell>0.2836</cell></row><row><cell>doc=bnn, query=bnn</cell><cell>0.1258</cell><cell>0.1253</cell><cell>0.0662</cell><cell>0.1553</cell><cell>0.0942</cell><cell>0.0556</cell></row><row><cell>doc=nnn, query=nnn</cell><cell>0.0997</cell><cell>0.1042</cell><cell>0.1047</cell><cell>0.1346</cell><cell>0.1089</cell><cell>0.0900</cell></row><row><cell>Mean</cell><cell>0.2406</cell><cell>0.2161</cell><cell>0.2323</cell><cell>0.2381</cell><cell>0.2589</cell><cell>0.2678</cell></row><row><cell>Mean over top-8 models</cell><cell>0.2293</cell><cell>0.2423</cell><cell>0.2673</cell><cell>0.2642</cell><cell>0.2987</cell><cell>0.3146</cell></row><row><cell>% change over T</cell><cell></cell><cell>+5.67%</cell><cell>+16.56%</cell><cell></cell><cell>+13.07%</cell><cell>+19.07%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,138.12,606.24,318.74,20.29"><head>Table 3 :</head><label>3</label><figDesc>Mean average precision of various IR models and query formulations (Bulgarian &amp; Hungarian language, word-based indexing)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,107.76,115.35,365.09,156.95"><head>Table 4 :</head><label>4</label><figDesc>Mean average precision of best performing IR model (TD query formulation)</figDesc><table coords="6,107.76,115.35,338.83,136.78"><row><cell>Query TD</cell><cell>Hungarian</cell><cell>Hungarian</cell><cell>Hungarian</cell></row><row><cell></cell><cell>48 queries</cell><cell>48 queries</cell><cell>48 queries</cell></row><row><cell></cell><cell>word-based</cell><cell>decompounded</cell><cell>4-gram</cell></row><row><cell>doc=Okapi, query=npn</cell><cell>0.3129</cell><cell>0.3392</cell><cell>0.3496</cell></row><row><cell>DFR GL2</cell><cell>0.3148</cell><cell>0.3396</cell><cell>0.3346</cell></row><row><cell>DFR PB2</cell><cell>0.3233</cell><cell>0.3574</cell><cell>0.3412</cell></row><row><cell>DFR PL2</cell><cell>0.3149</cell><cell>0.3399</cell><cell>0.3262</cell></row><row><cell>DFR I(F)L2</cell><cell>0.3157</cell><cell>0.3415</cell><cell>0.3441</cell></row><row><cell>LM (λ=0.3)</cell><cell>0.3142</cell><cell>0.3354</cell><cell>0.3329</cell></row><row><cell>LM (λ=0.35)</cell><cell>0.3132</cell><cell>0.3344</cell><cell>0.3330</cell></row><row><cell>Mean</cell><cell>0.3156</cell><cell>0.3411</cell><cell>0.3374</cell></row><row><cell>% change over word-based</cell><cell></cell><cell>+8.08%</cell><cell>+6.91%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,79.44,369.75,429.88,205.97"><head>Table 5 :</head><label>5</label><figDesc>Mean average precision using blind-query expansion (French collection)</figDesc><table coords="6,79.44,369.75,429.88,205.97"><row><cell>Query TD</cell><cell>French</cell><cell>French</cell><cell>French</cell><cell>French</cell></row><row><cell></cell><cell>49 queries</cell><cell>49 queries</cell><cell>49 queries</cell><cell>49 queries</cell></row><row><cell>IR Model / MAP</cell><cell>Okapi 0.4151</cell><cell>DFR PL2 0.4101</cell><cell>DFR GL2 0.3988</cell><cell>LM 0.3913</cell></row><row><cell>k doc. / m terms</cell><cell>10/20 0.4222</cell><cell>10/20 0.4255</cell><cell>10/20 0.4182</cell><cell>10/20 0.4388</cell></row><row><cell></cell><cell>10/30 0.4269</cell><cell>10/30 0.4255</cell><cell>10/30 0.4282</cell><cell>10/30 0.4460</cell></row><row><cell></cell><cell>10/40 0.4296</cell><cell>10/40 0.4307</cell><cell>10/40 0.4338</cell><cell>10/40 0.4508</cell></row><row><cell></cell><cell>10/50 0.4261</cell><cell>10/50 0.4311</cell><cell>10/50 0.4356</cell><cell>10/50 0.4509</cell></row><row><cell></cell><cell></cell><cell cols="2">Mean average precision</cell><cell></cell></row><row><cell>Query TD</cell><cell>Portuguese</cell><cell>Portuguese</cell><cell>Portuguese</cell><cell>Portuguese</cell></row><row><cell></cell><cell>50 queries</cell><cell>50 queries</cell><cell>50 queries</cell><cell>50 queries</cell></row><row><cell>IR Model / MAP</cell><cell>Okapi 0.4118</cell><cell>DFR PL2 0.4147</cell><cell>DFR GL2 0.4033</cell><cell>LM 0.3909</cell></row><row><cell>k doc. / m terms</cell><cell>10/20 0.4236</cell><cell>10/20 0.4412</cell><cell>10/20 0.4141</cell><cell>10/20 0.4273</cell></row><row><cell></cell><cell>10/30 0.4361</cell><cell>10/30 0.4414</cell><cell>10/30 0.4129</cell><cell>10/30 0.4286</cell></row><row><cell></cell><cell>10/40 0.4362</cell><cell>10/40 0.4386</cell><cell>10/40 0.4105</cell><cell>10/40 0.4266</cell></row><row><cell></cell><cell>10/50 0.4427</cell><cell>10/50 0.4367</cell><cell>10/50 0.4101</cell><cell>10/50 0.4276</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="6,70.92,586.62,429.08,48.85"><head>Table 6 :</head><label>6</label><figDesc>Mean average precision using blind-query expansion (Brazilian/Portuguese collection)For the French collection, the percentage of improvement varies from +3.5% (Okapi model, 0.4151 vs. 0.4296) to +15.2% (LM model, 0.3913 vs. 0.4509). For the Brazilian/Portuguese corpus, the enhancement raises from +2.7% (GL2 model, 0.4033 vs. 0.4141) to +9.6% (LM model, 0.3905 vs. 0.4286).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="7,171.18,484.62,252.61,20.29"><head>Table 8 :</head><label>8</label><figDesc>Mean average precision using blind-query expansion (English collection, TD query formulation)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="8,90.36,723.60,414.21,9.02"><head>Table 10 :</head><label>10</label><figDesc>Mean</figDesc><table /><note coords="8,160.63,723.87,343.94,8.74"><p>average precision using different combination operators (with blind-query expansion)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="9,76.56,225.87,441.94,477.46"><head>Table 11 :</head><label>11</label><figDesc>Description and mean average precision (MAP) of our official monolingual runs</figDesc><table coords="9,76.56,225.87,441.94,477.46"><row><cell></cell><cell></cell><cell cols="2">Query Index</cell><cell>Model</cell><cell>Query expansion</cell><cell cols="2">Single MAP Comb MAP</cell></row><row><cell>UniNEfr1</cell><cell>French</cell><cell>TD</cell><cell>word</cell><cell>PL2</cell><cell>10 docs / 40 terms</cell><cell>0.4307</cell><cell>Z-ScoreW</cell></row><row><cell></cell><cell></cell><cell>TD</cell><cell>word</cell><cell>LM</cell><cell>10 docs / 30 terms</cell><cell>0.4460</cell><cell></cell></row><row><cell></cell><cell></cell><cell>TD</cell><cell>word</cell><cell>Okapi</cell><cell>10 docs / 40 terms</cell><cell>0.4193</cell><cell>0.4549</cell></row><row><cell>UniNEfr2</cell><cell>French</cell><cell>TD</cell><cell>word</cell><cell>GL2</cell><cell>10 docs / 40 terms</cell><cell>0.4338</cell><cell>Z-ScoreW</cell></row><row><cell></cell><cell></cell><cell>TD</cell><cell>word</cell><cell>Okapi</cell><cell>10 docs / 20 terms</cell><cell>0.4222</cell><cell>0.4430</cell></row><row><cell>UniNEfr3</cell><cell>French</cell><cell>TD</cell><cell>word</cell><cell>Okapi</cell><cell>10 docs / 60 terms</cell><cell>0.4275</cell><cell>Norm RSV</cell></row><row><cell></cell><cell></cell><cell>TD</cell><cell>word</cell><cell>LM</cell><cell>10 docs / 30 terms</cell><cell>0.4460</cell><cell>0.4559</cell></row><row><cell>UniNEpt1</cell><cell cols="2">Portuguese TD</cell><cell>word</cell><cell>LM</cell><cell>10 docs / 50 terms</cell><cell>0.4276</cell><cell>Z-ScoreW</cell></row><row><cell></cell><cell></cell><cell>TD</cell><cell>word</cell><cell>Okapi</cell><cell>10 docs / 80 terms</cell><cell>0.4403</cell><cell>0.4552</cell></row><row><cell>UniNEpt2</cell><cell cols="2">Portuguese TD</cell><cell>word</cell><cell>LM</cell><cell>10 docs / 40 terms</cell><cell>0.4266</cell><cell>Z-Score</cell></row><row><cell></cell><cell></cell><cell>TD</cell><cell>word</cell><cell>GL2</cell><cell>10 docs / 40 terms</cell><cell>0.4105</cell><cell></cell></row><row><cell></cell><cell></cell><cell>TD</cell><cell>word</cell><cell>Okapi</cell><cell>10 docs / 30 terms</cell><cell>0.4361</cell><cell>0.4461</cell></row><row><cell>UniNEpt3</cell><cell cols="2">Portuguese TD</cell><cell>word</cell><cell>LM</cell><cell>10 docs / 100 terms</cell><cell>0.4302</cell><cell>Z-ScoreW</cell></row><row><cell></cell><cell></cell><cell>TD</cell><cell>word</cell><cell>Okapi</cell><cell>10 docs / 30 terms</cell><cell>0.4361</cell><cell>0.4495</cell></row><row><cell>UniNEbg1</cell><cell cols="2">Bulgarian TD</cell><cell>4-gram</cell><cell>IFL2</cell><cell>5 docs / 50 terms</cell><cell>0.2924</cell><cell>Norm RSV</cell></row><row><cell></cell><cell></cell><cell>TD</cell><cell>word</cell><cell>LM</cell><cell>3 docs / 70 terms</cell><cell>0.3300</cell><cell></cell></row><row><cell></cell><cell></cell><cell>TD</cell><cell>4-gram</cell><cell>Okapi</cell><cell>3 docs / 100 terms</cell><cell>0.2943</cell><cell>0.3129</cell></row><row><cell>UniNEbg2</cell><cell cols="2">Bulgarian TD</cell><cell>word</cell><cell>LM</cell><cell>5 docs / 40 terms</cell><cell>0.3201</cell><cell>Z-ScoreW</cell></row><row><cell></cell><cell></cell><cell cols="2">TTTD 4-gram</cell><cell>GL2</cell><cell>10 docs / 90 terms</cell><cell>0.2941</cell><cell>0.3314</cell></row><row><cell>UniNEbg3</cell><cell cols="3">Bulgarian TTTD 4-gram</cell><cell>IFL2</cell><cell>5 docs / 50 terms</cell><cell>0.2924</cell><cell>Norm RSV</cell></row><row><cell></cell><cell></cell><cell cols="2">TTTD word</cell><cell>LM</cell><cell>3 docs / 70 terms</cell><cell>0.3300</cell><cell></cell></row><row><cell></cell><cell></cell><cell>TD</cell><cell>4-gram</cell><cell>LM</cell><cell>3 docs / 100 terms</cell><cell>0.2959</cell><cell></cell></row><row><cell></cell><cell></cell><cell>TD</cell><cell>word</cell><cell>Okapi</cell><cell>5 docs / 80 terms</cell><cell>0.3229</cell><cell>0.3298</cell></row><row><cell>UniNEhu1</cell><cell cols="2">Hungarian TD</cell><cell>word</cell><cell>PB2</cell><cell>5 docs / 20 terms</cell><cell>0.3922</cell><cell>Norm RSV</cell></row><row><cell></cell><cell></cell><cell>TD</cell><cell>4-gram</cell><cell>Okapi</cell><cell>3 docs / 90 terms</cell><cell>0.3927</cell><cell>0.4186</cell></row><row><cell>UniNEhu2</cell><cell cols="3">Hungarian TTTD wordDec</cell><cell>PL2</cell><cell>3 docs / 40 terms</cell><cell>0.3794</cell><cell>Z-Score</cell></row><row><cell></cell><cell></cell><cell cols="2">TTTD word</cell><cell>LM</cell><cell>3 docs / 70 terms</cell><cell>0.3815</cell><cell></cell></row><row><cell></cell><cell></cell><cell>TD</cell><cell>4-gram</cell><cell>Okapi</cell><cell>3 docs / 100 terms</cell><cell>0.3870</cell><cell>0.4308</cell></row><row><cell>UniNEhu3</cell><cell cols="2">Hungarian TD</cell><cell>word</cell><cell>PB2</cell><cell>5 best docs / 20 terms</cell><cell>0.3922</cell><cell>0.3922</cell></row><row><cell>Run name</cell><cell cols="3">Language Query Index</cell><cell>Model</cell><cell>Query expansion</cell><cell cols="2">Single MAP Comb MAP</cell></row><row><cell>UniNEtden</cell><cell>English</cell><cell>TD</cell><cell>word</cell><cell>GL2</cell><cell>3 docs / 10 terms</cell><cell>0.3965</cell><cell>Z-Score</cell></row><row><cell></cell><cell></cell><cell>TD</cell><cell>word</cell><cell>Okapi</cell><cell>3 docs / 10 terms</cell><cell>0.4048</cell><cell></cell></row><row><cell></cell><cell></cell><cell>TD</cell><cell>word</cell><cell>LM</cell><cell>5 docs / 10 terms</cell><cell>0.4250</cell><cell>0.4367</cell></row><row><cell cols="3">UniNEtdnen English TDN</cell><cell>word</cell><cell>GL2</cell><cell>5 docs / 15 terms</cell><cell>0.4195</cell><cell>Sum RSV</cell></row><row><cell></cell><cell></cell><cell>TDN</cell><cell>word</cell><cell>Okapi</cell><cell>3 docs / 10 terms</cell><cell>0.4273</cell><cell></cell></row><row><cell></cell><cell></cell><cell>TDN</cell><cell>word</cell><cell>LM</cell><cell>3 docs / 10 terms</cell><cell>0.4352</cell><cell>0.4444</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="9,165.66,715.20,263.70,20.29"><head>Table 12 :</head><label>12</label><figDesc>Description and mean average precision (MAP) of our unofficial monolingual runs for the English collection Table13shows the mean average precision obtained using the various MT tools and the Okapi probabilistic model with blind query expansion (40 terms extracted from the first 10 retrieved items). Of course, all tools are not always available for each language and thus various entries are missing (as shown in Table13, indicated by the label "N/A").</figDesc><table coords="10,148.86,358.95,280.12,173.26"><row><cell></cell><cell cols="2">Mean average precision (% of monolingual)</cell></row><row><cell>Language</cell><cell>French</cell><cell>Portuguese</cell></row><row><cell>Okapi (TD queries)</cell><cell>49 queries</cell><cell>50 queries</cell></row><row><cell>Manual &amp; PRF (10/40)</cell><cell>0.4296</cell><cell>0.4389</cell></row><row><cell>AlphaWorks</cell><cell>0.3378 (78.6%)</cell><cell>N / A</cell></row><row><cell>AppliedLanguage</cell><cell>0.3726 (86.7%)</cell><cell>0.3077 (70.1%)</cell></row><row><cell>BabelFish (altavista)</cell><cell>0.3771 (87.8%)</cell><cell>0.3092 (70.4%)</cell></row><row><cell>FreeTranslation</cell><cell>0.3813 (88.8%)</cell><cell>0.3356 (76.5%)</cell></row><row><cell>Google</cell><cell>0.3754 (87.4%)</cell><cell>0.3070 (69.9%)</cell></row><row><cell>InterTrans</cell><cell>0.2761 (64.3%)</cell><cell>0.3343 (76.2%)</cell></row><row><cell>Online</cell><cell>0.3942 (91.8%)</cell><cell>0.3677 (83.8%)</cell></row><row><cell>Reverso / Promt</cell><cell>0.4081 (95.0%)</cell><cell>0.3531 (80.5%)</cell></row><row><cell>WorldLingo</cell><cell>0.3832 (89.2%)</cell><cell>0.3091 (70.4%)</cell></row><row><cell>Systran</cell><cell>N / A</cell><cell>0.3077 (70.1%)</cell></row><row><cell>WorldLingo</cell><cell>0.3832 (70.4%)</cell><cell>0.3091 (70.4%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="10,148.02,544.08,299.02,20.29"><head>Table 13 :</head><label>13</label><figDesc>Mean average precision of various machine translation systems (Okapi model with blind query expansion, TD queries)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="11,141.84,271.92,302.76,283.45"><head>Table 14 :</head><label>14</label><figDesc>MAP of various combined translation tools (Okapi model with blind query expansion, TD queries)</figDesc><table coords="11,141.84,302.19,302.76,253.18"><row><cell>From English to …</cell><cell>French</cell><cell>Portuguese</cell></row><row><cell></cell><cell>49 queries</cell><cell>50 queries</cell></row><row><cell>IR 1 (#docs/#terms)</cell><cell>PL2 (10/30)</cell><cell>I(n)L2 (10/40)</cell></row><row><cell>IR 2 (#docs/#terms)</cell><cell>LM (10/30)</cell><cell>LM (10/30)</cell></row><row><cell>Data fusion operator</cell><cell>Z-score</cell><cell>Round-robin</cell></row><row><cell>Translation tools</cell><cell>BabelFish &amp; Reverso</cell><cell>Promt &amp; Free &amp; Online</cell></row><row><cell>MAP</cell><cell>0.4278</cell><cell>0.4114</cell></row><row><cell>Run name</cell><cell>UniNEBifr1</cell><cell>UniNEBipt2</cell></row><row><cell>IR 1 (#docs/#terms)</cell><cell>PL2 (10/30)</cell><cell>GL2 (10/40)</cell></row><row><cell>IR 2 (#docs/#terms)</cell><cell>Okapi (10/60)</cell><cell>Okapi (10/80)</cell></row><row><cell>IR 3 (#docs/#terms)</cell><cell>LM (10/50)</cell><cell>LM (10/40)</cell></row><row><cell>Data fusion operator</cell><cell>Z-scoreW</cell><cell>Z-Score</cell></row><row><cell>Translation tools</cell><cell>Reverso &amp; Online</cell><cell>Promt &amp; Free</cell></row><row><cell>MAP</cell><cell>0.4256</cell><cell>0.4138</cell></row><row><cell>Run name</cell><cell>UniNEBifr2</cell><cell>UniNEBipt1</cell></row><row><cell>IR 1 (#docs/#terms)</cell><cell>PL2 (10/30)</cell><cell>I(n)L2 (10/40)</cell></row><row><cell>IR 2 (#docs/#terms)</cell><cell>Okapi (10/60)</cell><cell>LM (10/30)</cell></row><row><cell>IR 3 (#docs/#terms)</cell><cell>LM (10/30)</cell><cell></cell></row><row><cell>Data fusion operator</cell><cell>Z-score</cell><cell>Norm RSV</cell></row><row><cell>Translation tools</cell><cell>BabelFish &amp; Google &amp; Free</cell><cell>Promt &amp; Online</cell></row><row><cell>MAP</cell><cell>0.4083</cell><cell>0.4062</cell></row><row><cell>Run name</cell><cell>UniNEBifr3</cell><cell>UniNEBipt3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" coords="11,172.86,567.24,249.28,9.02"><head>Table 15 :</head><label>15</label><figDesc>Description and MAP of our official bilingual runs</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17" coords="12,190.86,378.84,213.26,9.02"><head>Table 16 :</head><label>16</label><figDesc>CLEF 2005 GIRT test collection statistics</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19" coords="12,92.94,725.64,409.18,9.02"><head>Table 17 :</head><label>17</label><figDesc>Mean average precision of various single searching strategies(monolingual, GIRT corpus)   </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20" coords="13,76.56,393.72,441.88,222.01"><head>Table 19 :</head><label>19</label><figDesc>Mean average precision using blind-query expansion (English GIRT collection)</figDesc><table coords="13,76.56,411.75,441.88,203.98"><row><cell>Run name</cell><cell cols="3">Language Query Index</cell><cell>Model</cell><cell>Query expansion</cell><cell cols="2">Single MAP Comb. MAP</cell></row><row><cell>UniNEde1</cell><cell>German</cell><cell>TD</cell><cell>word</cell><cell>Okapi</cell><cell>10 best docs / 15 terms</cell><cell>0.4959</cell><cell>Z-ScoreW</cell></row><row><cell></cell><cell></cell><cell>TD</cell><cell cols="3">word DFR GL2 10 best docs / 100 terms</cell><cell>0.4677</cell><cell>0.5015</cell></row><row><cell>UniNEde2</cell><cell>German</cell><cell>TD</cell><cell>word</cell><cell>Okapi</cell><cell>5 best docs / 10 terms</cell><cell>0.4878</cell><cell>Z-ScoreW</cell></row><row><cell></cell><cell></cell><cell>TD</cell><cell cols="3">word DFR GL2 5 best docs / 10 terms</cell><cell>0.4420</cell><cell></cell></row><row><cell></cell><cell></cell><cell>TD</cell><cell>word</cell><cell>LM</cell><cell>10 best docs / 30 terms</cell><cell>0.5011</cell><cell>0.5051</cell></row><row><cell>UniNEde3</cell><cell cols="2">German TDN</cell><cell>word</cell><cell>Okapi</cell><cell>5 best docs / 10 terms</cell><cell>0.4851</cell><cell>Z-ScoreW</cell></row><row><cell></cell><cell></cell><cell>TDN</cell><cell cols="3">word DFR GL2 5 best docs / 10 terms</cell><cell>0.4541</cell><cell></cell></row><row><cell></cell><cell></cell><cell>TDN</cell><cell>word</cell><cell>LM</cell><cell>10 best docs / 30 terms</cell><cell>0.4832</cell><cell>0.5159</cell></row><row><cell>UniNEen1</cell><cell>English</cell><cell>TD</cell><cell>word</cell><cell>LM</cell><cell>10 best docs / 20 terms</cell><cell cols="2">0.4160 Round-robin</cell></row><row><cell></cell><cell></cell><cell>TD</cell><cell cols="3">word DFR GL2 10 best docs / 150 terms</cell><cell>0.4113</cell><cell>0.4292</cell></row><row><cell>UniNEen2</cell><cell>English</cell><cell>TD</cell><cell>word</cell><cell>Okapi</cell><cell>10 best docs / 30 terms</cell><cell>0.3952</cell><cell>Z-scoreW</cell></row><row><cell></cell><cell></cell><cell>TD</cell><cell cols="3">word DFR GL2 10 best docs / 50 terms</cell><cell>0.4065</cell><cell></cell></row><row><cell></cell><cell></cell><cell>TD</cell><cell>word</cell><cell>LM</cell><cell>10 best docs / 10 terms</cell><cell>0.4032</cell><cell>0.4303</cell></row><row><cell>UniNEen3</cell><cell cols="2">English TDN</cell><cell>word</cell><cell>Okapi</cell><cell>10 best docs / 10 terms</cell><cell>0.3981</cell><cell>ZscoreW</cell></row><row><cell></cell><cell></cell><cell>TDN</cell><cell cols="3">word DFR GL2 10 best docs / 50 terms</cell><cell>0.4410</cell><cell></cell></row><row><cell></cell><cell></cell><cell>TDN</cell><cell>word</cell><cell>LM</cell><cell>10 best docs / 20 terms</cell><cell>0.4291</cell><cell>0.4576</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21" coords="13,127.02,627.60,340.90,9.02"><head>Table 20 :</head><label>20</label><figDesc>Description and mean average precision (MAP) of our official GIRT runs</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22" coords="14,76.56,492.62,440.52,223.12"><head>Table 21 :</head><label>21</label><figDesc>Comparing the mean average precision (MAP) with the geometric mean (GMAP) with various query formulations and search models (French corpus)</figDesc><table coords="14,76.56,492.62,440.52,191.92"><row><cell></cell><cell>T</cell><cell>T</cell><cell>TD</cell><cell>TD</cell><cell>TDN</cell><cell>TDN</cell></row><row><cell></cell><cell>MAP</cell><cell>GMAP</cell><cell>MAP</cell><cell>GMAP</cell><cell>MAP</cell><cell>GMAP</cell></row><row><cell>mean distinct terms/query</cell><cell>2.91</cell><cell>2.91</cell><cell>7.51</cell><cell>7.51</cell><cell>16.32</cell><cell>16.32</cell></row><row><cell>Model \ # of queries</cell><cell>91 queries</cell><cell>91 queries</cell><cell>91 queries</cell><cell>91 queries</cell><cell>91 queries</cell><cell>91 queries</cell></row><row><cell>doc=Okapi, query=npn</cell><cell>0.3969</cell><cell>0.2121</cell><cell>0.4816</cell><cell>0.3534</cell><cell>0.5151</cell><cell>0.4146</cell></row><row><cell>DFR GL2</cell><cell>0.3742</cell><cell>0.1833</cell><cell>0.4714</cell><cell>0.3316</cell><cell>0.5088</cell><cell>0.3961</cell></row><row><cell>LM (λ=0.35)</cell><cell>0.3611</cell><cell>0.1745</cell><cell>0.4535</cell><cell>0.3079</cell><cell>0.5003</cell><cell>0.3809</cell></row><row><cell>doc=Lnu, query=ltc</cell><cell>0.3669</cell><cell>0.1941</cell><cell>0.4518</cell><cell>0.3291</cell><cell>0.4958</cell><cell>0.3927</cell></row><row><cell>doc=dtu, query=dtn</cell><cell>0.3765</cell><cell>0.1735</cell><cell>0.4406</cell><cell>0.3015</cell><cell>0.4909</cell><cell>0.3662</cell></row><row><cell>doc=atn, query=ntc</cell><cell>0.3869</cell><cell>0.1952</cell><cell>0.4459</cell><cell>0.3143</cell><cell>0.5001</cell><cell>0.3855</cell></row><row><cell>doc=ltn, query=ntc</cell><cell>0.3705</cell><cell>0.1957</cell><cell>0.4328</cell><cell>0.3056</cell><cell>0.4636</cell><cell>0.3510</cell></row><row><cell>doc=ntc, query=ntc</cell><cell>0.2447</cell><cell>0.0944</cell><cell>0.2988</cell><cell>0.1606</cell><cell>0.3262</cell><cell>0.1901</cell></row><row><cell>doc=ltc, query=ltc</cell><cell>0.2540</cell><cell>0.0916</cell><cell>0.3193</cell><cell>0.1769</cell><cell>0.3581</cell><cell>0.2212</cell></row><row><cell>doc=lnc, query=ltc</cell><cell>0.2604</cell><cell>0.0964</cell><cell>0.3364</cell><cell>0.1917</cell><cell>0.3949</cell><cell>0.2513</cell></row><row><cell>doc=nnn, query=nnn</cell><cell>0.1572</cell><cell>0.0484</cell><cell>0.1379</cell><cell>0.0499</cell><cell>0.1465</cell><cell>0.0563</cell></row><row><cell>Mean</cell><cell>0.3227</cell><cell>0.1508</cell><cell>0.3882</cell><cell>0.2566</cell><cell>0.4273</cell><cell>0.3096</cell></row><row><cell>Mean over 7-best models</cell><cell>0.3761</cell><cell>0.1898</cell><cell>0.4539</cell><cell>0.3205</cell><cell>0.4964</cell><cell>0.3839</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23" coords="15,75.54,345.99,444.00,165.17"><head>Table 22 :</head><label>22</label><figDesc>MAP and geometric mean (GMAP) when applying pseudo-relevance feedback approach (Rocchio)</figDesc><table coords="15,76.56,345.99,440.52,145.24"><row><cell></cell><cell>T</cell><cell>T</cell><cell>TD</cell><cell>TD</cell><cell>TDN</cell><cell>TDN</cell></row><row><cell></cell><cell>MAP</cell><cell>GMAP</cell><cell>MAP</cell><cell>GMAP</cell><cell>MAP</cell><cell>GMAP</cell></row><row><cell>Model \ # of queries</cell><cell>91 queries</cell><cell>91 queries</cell><cell>91 queries</cell><cell>91 queries</cell><cell>91 queries</cell><cell>91 queries</cell></row><row><cell>doc=Okapi, query=npn</cell><cell>0.3969</cell><cell>0.2121</cell><cell>0.4816</cell><cell>0.3534</cell><cell>0.5151</cell><cell>0.4146</cell></row><row><cell>docs / terms 3 / 10</cell><cell>0.4058</cell><cell>0.2186</cell><cell>0.4936</cell><cell>0.3676</cell><cell>0.5226</cell><cell>0.4184</cell></row><row><cell>3 / 15</cell><cell>0.4014</cell><cell>0.2152</cell><cell>0.4993</cell><cell>0.3720</cell><cell>0.5224</cell><cell>0.4179</cell></row><row><cell>3 / 20</cell><cell>0.3981</cell><cell>0.2128</cell><cell>0.4974</cell><cell>0.3716</cell><cell>0.5220</cell><cell>0.4160</cell></row><row><cell>5 / 10</cell><cell>0.4123</cell><cell>0.2318</cell><cell>0.5015</cell><cell>0.3723</cell><cell>0.5348</cell><cell>0.4306</cell></row><row><cell>5 / 15</cell><cell>0.4141</cell><cell>0.2324</cell><cell>0.5035</cell><cell>0.3755</cell><cell>0.5362</cell><cell>0.4319</cell></row><row><cell>5 / 20</cell><cell>0.4120</cell><cell>0.2301</cell><cell>0.5011</cell><cell>0.3733</cell><cell>0.5353</cell><cell>0.4282</cell></row><row><cell>10 / 10</cell><cell>0.3945</cell><cell>0.1991</cell><cell>0.5015</cell><cell>0.3728</cell><cell>0.5305</cell><cell>0.4286</cell></row><row><cell>10 / 15</cell><cell>0.3984</cell><cell>0.2021</cell><cell>0.4953</cell><cell>0.3684</cell><cell>0.5284</cell><cell>0.4261</cell></row><row><cell>10 / 20</cell><cell>0.4059</cell><cell>0.2121</cell><cell>0.4893</cell><cell>0.3641</cell><cell>0.5255</cell><cell>0.4220</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24" coords="15,76.56,560.97,434.11,127.43"><head>Table 23 :</head><label>23</label><figDesc>Description and evaluations (MAP and GMAP) of our official robust runs (91 queries)</figDesc><table coords="15,76.56,560.97,434.11,106.54"><row><cell></cell><cell>Index</cell><cell cols="2">Query Model</cell><cell>Query expansion</cell><cell>MAP</cell><cell>comb. MAP</cell><cell>GMAP</cell></row><row><cell>UniNEfrr1</cell><cell>word</cell><cell>TD</cell><cell cols="2">Okapi 5 best docs / 15 terms</cell><cell>0.5035</cell><cell>Round-robin</cell></row><row><cell></cell><cell>word</cell><cell>TD</cell><cell>GL2</cell><cell>3 best docs / 30 terms</cell><cell>0.5014</cell><cell></cell></row><row><cell></cell><cell>word</cell><cell>TD</cell><cell>LM</cell><cell>10 best docs / 15 terms</cell><cell>0.5095</cell><cell>0.5227</cell><cell>0.3889</cell></row><row><cell>UniNEfrr2</cell><cell>word</cell><cell>T</cell><cell cols="2">Okapi 3 best docs / 10 terms</cell><cell>0.4058</cell><cell>Z-ScoreW</cell></row><row><cell></cell><cell>word</cell><cell>T</cell><cell>GL2</cell><cell>5 best docs / 30 terms</cell><cell>0.4029</cell><cell></cell></row><row><cell></cell><cell>word</cell><cell>T</cell><cell>LM</cell><cell>5 best docs / 10 terms</cell><cell>0.4137</cell><cell>0.4396</cell><cell>0.2376</cell></row><row><cell>UniNEfrr3</cell><cell>word</cell><cell>TD</cell><cell>GL2</cell><cell>3 best docs / 30 terms</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>&amp; Yahoo!.fr</cell><cell>0.4607</cell><cell></cell><cell>0.2935</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors would like to also thank the CLEF-2006 task organizers for their efforts in developing various European language test-collections. The authors would also like to thank <rs type="person">C. Buckley</rs> from <rs type="affiliation">SabIR</rs> for giving us the opportunity to use the SMART system, together with <rs type="person">Pierre-Yves Berger</rs> for his help in translating the English topics and in using the Yahoo.fr search engine. This research was supported in part by the <rs type="funder">Swiss National Science Foundation</rs> under Grant #<rs type="grantNumber">200020-103420</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_xprw9KD">
					<idno type="grant-number">200020-103420</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-</head><p>Table <ref type="table" coords="10,110.49,73.35,10.02,8.74">12</ref> shows the exact specifications of two unofficial monolingual runs submitted to improve the pool for the English monolingual collection. These experiments are based on a combination of three probabilistic models (Okapi, DFR-GL2 and LM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Bilingual Information Retrieval</head><p>Due to time constraint, we have limited our participation in the bilingual track to the French and Portuguese language. Moreover, we chose English as the language for submitting queries to be automatically translated into these two different languages, using ten different freely available machine translation <ref type="bibr" coords="10,415.70,173.31,21.69,8.74">(MT)</ref>  To assign an indexing weight w ij that reflects the importance of each single-term t j in a document D i , we might use the various approaches shown in Table <ref type="table" coords="17,271.65,392.67,4.32,8.74">A</ref>.1, where n indicates the number of documents in the collection, t the number of indexing terms, df j the number of documents in which the term t j appears, the document length (the number of indexing terms) of D i is denoted by nt i , and avdl, b, k 1 , pivot and slope are constants. For the Okapi weighting scheme, K represents the ratio between the length of D i measured by l i (sum of tf ij ) and the collection mean noted by avdl. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="7,87.66,796.29,3.34,8.74;7,299.46,74.79,95.86,8.74;7,109.52,87.51,13.35,8.74;7,194.56,87.51,39.53,8.74;7,283.20,87.51,39.54,8.74;7,371.85,87.51,39.51,8.74;7,460.47,87.51,39.51,8.74;7,193.62,98.79,41.58,8.74;7,282.26,98.79,41.58,8.74;7,370.90,98.79,41.58,8.74;7,459.53,98.79,10.04,8.74" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="7,299.46,74.79,95.86,8.74;7,109.52,87.51,13.35,8.74;7,194.56,87.51,39.53,8.74;7,283.20,87.51,39.54,8.74;7,371.85,87.51,39.51,8.74;7,460.47,87.51,39.51,8.74;7,193.62,98.79,41.58,8.74;7,282.26,98.79,41.58,8.74;7,370.90,98.79,41.58,8.74">Mean average precision TD Bulgarian Bulgarian Bulgarian Bulgarian 50 queries 50 queries 50 queries</title>
		<imprint>
			<biblScope unit="page">50</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,472.11,98.79,29.00,8.74;7,79.44,113.55,38.70,8.74;7,202.14,113.55,24.51,8.74;7,281.92,113.55,42.08,8.74;7,371.34,113.55,40.40,8.74;7,453.63,113.55,53.12,8.74;7,79.44,124.83,46.97,8.74;7,200.59,124.83,27.68,8.74;7,289.20,124.83,27.68,8.74;7,377.80,124.83,27.68,8.74;7,466.41,124.83,27.68,8.74;7,87.00,138.24,63.72,9.02;7,187.83,138.51,53.12,8.74;7,276.43,138.51,53.12,8.74;7,365.03,138.51,53.11,8.74;7,453.63,138.51,53.12,8.74;7,187.80,149.97,53.12,8.74;7,276.41,149.97,53.12,8.74;7,365.02,149.70,53.15,9.02;7,453.66,149.97,53.12,8.74;7,185.28,161.43,58.15,8.74;7,273.88,161.16,58.20,9.02;7,362.52,161.43,58.17,8.74;7,451.14,161.16,58.17,9.02;7,187.80,172.62,53.14,9.02;7,276.42,172.89,53.14,8.74;7,365.05,172.89,53.13,8.74;7,453.67,172.89,53.13,8.74;7,187.80,184.17,53.13,8.74;7,276.43,184.17,53.13,8.74;7,365.04,184.17,53.12,8.74;7,453.65,184.17,53.12,8.74;7,185.28,195.45,58.16,8.74;7,273.89,195.45,58.16,8.74;7,362.49,195.45,58.16,8.74;7,451.10,195.45,58.16,8.74;7,79.44,210.21,38.70,8.74;7,202.14,210.21,24.51,8.74;7,281.92,210.21,42.08,8.74;7,371.34,210.21,40.40,8.74;7,453.63,210.21,53.12,8.74;7,79.44,221.49,29.07,8.74;7,200.55,221.49,27.69,8.74;7,289.16,221.49,27.69,8.74;7,377.78,221.49,27.69,8.74;7,466.39,221.49,27.69,8.74;7,87.00,234.90,63.72,9.02;7,187.83,235.17,53.12,8.74;7,276.43,235.17,53.12,8.74;7,365.03,235.17,53.11,8.74;7,453.63,235.17,53.12,8.74;7,187.80,246.63,53.12,8.74;7,276.41,246.63,53.12,8.74;7,365.02,246.36,53.14,9.02;7,453.66,246.63,53.12,8.74;7,185.28,258.09,58.15,8.74;7,273.88,257.82,58.20,9.02;7,362.52,258.09,58.17,8.74;7,451.14,258.09,58.17,8.74;7,185.28,269.28,58.18,9.02;7,273.90,269.55,58.17,8.74;7,362.52,269.55,58.17,8.74;7,451.14,269.55,58.17,8.74;7,185.28,281.01,58.17,8.74;7,273.90,281.01,58.17,8.74;7,362.52,281.01,58.16,8.74;7,451.12,280.74,58.19,9.02;7,182.76,292.29,63.20,8.74;7,271.36,292.29,63.20,8.74;7,359.97,292.29,63.20,8.74;7,448.57,292.29,63.20,8.74;7,171.18,311.94,252.61,9.02;7,137.82,323.49,319.33,8.74;7,292.38,342.21,95.86,8.74;7,107.76,354.93,43.51,8.74;7,239.97,354.93,30.64,8.74;7,324.99,354.93,30.64,8.74;7,410.07,354.93,30.64,8.74;7,234.48,366.93,41.58,8.74;7,319.52,366.93,41.58,8.74;7,404.62,366.93,41.58,8.74;7,107.76,380.01,38.70,8.74;7,243.00,380.01,24.51,8.74;7,320.02,380.01,40.40,8.74;7,398.76,380.01,53.08,8.74;7,241.38,392.01,27.69,8.74;7,326.40,392.01,27.69,8.74;7,411.48,392.01,27.69,8.74;7,115.32,404.46,63.72,9.02;7,228.68,404.46,53.11,9.02;7,313.68,404.73,53.14,8.74;7,398.76,404.73,53.14,8.74;7,228.66,416.73,53.12,8.74;7,313.66,416.73,53.14,8.74;7,398.75,416.73,53.13,8.74;7,226.14,428.73,58.15,8.74;7,311.14,428.73,58.15,8.74;7,396.19,428.73,58.15,8.74;7,228.66,440.73,53.12,8.74;7,313.67,440.73,53.12,8.74;7,398.74,440.73,53.12,8.74;7,228.66,452.73,53.12,8.74;7,313.67,452.46,53.14,9.02;7,398.76,452.46,53.13,9.02;7,226.14,464.73,58.16,8.74;7,311.15,464.73,58.16,8.74;7,396.21,464.73,35.09,8.74" xml:id="b1">
	<analytic>
		<title/>
		<idno>λ=0.35) 0.4006 0.3999 0.3862 k doc. / m terms 3/10 0.4048 3/10 0.3965 3/10 0.4059 5/10 0.4036 5/10 0.3961 5/10 0.4250 10/10 0.3989 10/10 0.3923 10/10 0.4067 3/15 0.3955 3/40 0.4063 3/15</idno>
	</analytic>
	<monogr>
		<title level="j" coord="7,472.11,98.79,29.00,8.74;7,79.44,113.55,38.70,8.74;7,202.14,113.55,24.51,8.74;7,281.92,113.55,42.08,8.74;7,371.34,113.55,40.40,8.74;7,453.63,113.55,15.04,8.74;7,107.76,380.01,38.70,8.74;7,243.00,380.01,24.51,8.74;7,320.02,380.01,40.40,8.74;7,398.76,380.01,15.04,8.74">queries IR Model Okapi DFR IFL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>IR Model Okapi DFR GL2 LM</note>
</biblStruct>

<biblStruct coords="7,431.30,464.73,23.08,8.74;13,87.66,796.29,3.34,8.74;13,85.14,73.35,425.48,8.74;13,70.92,84.63,440.92,8.74;13,70.92,95.91,287.00,8.74;13,292.38,114.63,95.86,8.74;13,107.76,127.35,43.48,8.74;13,239.16,127.35,32.22,8.74;13,324.19,127.35,32.22,8.74;13,409.29,127.35,32.22,8.74;13,234.48,139.35,41.58,8.74;13,319.52,139.35,41.58,8.74;13,404.62,139.35,41.58,8.74;13,107.76,152.07,68.22,8.74;13,226.64,152.07,57.25,8.74;13,303.65,152.07,73.16,8.74;13,401.43,152.07,47.72,8.74;13,115.32,164.52,63.72,9.02;13,228.69,164.79,53.11,8.74;13,313.68,164.79,53.11,8.74;13,398.73,164.79,53.11,8.74;13,228.66,176.79,53.12,8.74;13,313.67,176.79,53.12,8.74;13,398.74,176.79,53.12,8.74;13,226.14,188.79,58.16,8.74;13,311.15,188.79,58.16,8.74;13,396.21,188.79,58.16,8.74;13,228.66,200.79,53.12,8.74;13,313.66,200.79,53.12,8.74;13,398.72,200.79,53.12,8.74;13,228.66,212.79,53.13,8.74;13,313.68,212.52,53.13,9.02;13,398.76,212.79,53.12,8.74;13,226.14,224.52,58.17,9.02;13,311.16,224.79,58.16,8.74;13,396.23,224.52,58.18,9.02;13,114.00,244.68,367.04,9.02;13,292.38,263.67,95.86,8.74;13,107.76,276.39,43.51,8.74;13,239.97,276.39,30.64,8.74;13,324.99,276.39,30.64,8.74;13,410.07,276.39,30.64,8.74;13,234.48,288.39,41.58,8.74;13,319.52,288.39,41.58,8.74;13,404.62,288.39,41.58,8.74;13,107.76,301.11,68.22,8.74;13,226.64,301.11,57.25,8.74;13,303.65,301.11,73.16,8.74;13,401.43,301.11,47.72,8.74;13,115.32,313.56,63.72,9.02;13,228.69,313.83,53.11,8.74;13,313.68,313.83,53.11,8.74;13,398.73,313.83,53.11,8.74;13,228.66,325.83,53.12,8.74;13,313.67,325.83,53.12,8.74;13,398.74,325.83,53.12,8.74;13,226.14,337.83,58.16,8.74;13,311.15,337.83,58.16,8.74;13,396.21,337.83,58.16,8.74;13,228.66,349.83,53.12,8.74;13,311.14,349.83,58.15,8.74;13,398.72,349.83,53.12,8.74;13,228.66,361.83,53.13,8.74;13,311.16,361.56,58.17,9.02;13,398.76,361.83,53.12,8.74;13,226.14,373.56,58.17,9.02;13,308.64,373.83,63.20,8.74;13,396.22,373.56,58.19,9.02;16,70.92,526.22,56.04,10.80" xml:id="b2">
	<monogr>
		<idno>MAP Okapi 0.3516 DFR GL2 0.3532 LM 0.3541 k doc. / m terms 3/30 0.3781 3/50 0.3845 3/10 0.3436 5/30 0.3910 5/50 0.4073 5/10 0.3541 10/30 0.3952 10/50 0.4065 10/10 0.4032 3/50 0.3757 3/100 0.3884 3/40 0.2948 5/50 0.3921 5/100 0.4144 5/40 0.2991 10/50 0.3955 10/100 0.4106 10/40 0.4203</idno>
		<title level="m" coord="13,107.76,152.07,68.22,8.74;13,226.64,152.07,57.25,8.74">IR Model / MAP Okapi 0.4565</title>
		<imprint>
			<biblScope unit="page">4008</biblScope>
		</imprint>
	</monogr>
	<note>Our 6 official runs in the monolingual GIRT task are described in Table 20. Each run is built using a data fusion operator (&quot;Z-ScoreW&quot; in this case, see Section 5). For all runs, we automatically expanded the queries using a blind relevance feedback method (Rocchio in our experiments). Mean average precision Query TD German German German 25 queries 25 queries 25 queries. Mean average precision using blind-query expansion (German GIRT collection) Mean average precision Query TD English English English 25 queries 25 queries 25 queries IR Model /</note>
</biblStruct>

<biblStruct coords="16,70.92,547.41,446.36,8.74;16,92.16,558.42,365.41,9.02" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="16,241.55,547.41,275.73,8.74;16,92.16,558.69,114.35,8.74">Probabilistic models of information retrieval based on measuring the divergence from randomness</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,216.12,558.42,171.68,9.02">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="357" to="389" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,70.92,571.95,408.27,8.74;16,92.16,582.96,299.01,9.02" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="16,302.91,571.95,157.28,8.74">New retrieval approaches using SMART</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<idno>#500-236</idno>
	</analytic>
	<monogr>
		<title level="m" coord="16,92.16,582.96,93.33,9.02">Proceedings of TREC-4</title>
		<meeting>TREC-4<address><addrLine>Gaithersburg</addrLine></address></meeting>
		<imprint>
			<publisher>NIST Publication</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="25" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,70.92,596.22,432.22,9.02;16,92.16,607.77,152.79,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="16,203.22,596.49,134.14,8.74">Combination of multiple searches</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
		<idno>#500-215</idno>
	</analytic>
	<monogr>
		<title level="m" coord="16,357.30,596.22,83.01,9.02">Proceedings TREC-2</title>
		<meeting>TREC-2<address><addrLine>Gaithersburg</addrLine></address></meeting>
		<imprint>
			<publisher>NIST Publication</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="243" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,70.92,621.03,368.95,8.74" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="16,159.30,621.03,193.01,8.74">Using language models for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note type="report_type">CTIT Ph.D. Thesis</note>
</biblStruct>

<biblStruct coords="16,70.92,634.29,448.76,8.74;16,92.16,645.30,268.66,9.02" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="16,159.25,634.29,343.42,8.74">Term-specific smoothing for the language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,92.16,645.30,124.68,9.02">Proceedings of the ACM-SIGIR</title>
		<meeting>the ACM-SIGIR<address><addrLine>Tempere</addrLine></address></meeting>
		<imprint>
			<publisher>The ACM Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="35" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,70.92,658.83,374.94,8.74" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="16,148.76,658.83,271.98,8.74">The GIRT data in the evaluation of CLIR systems -from 1997 until</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kluck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2004. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,450.91,658.83,58.60,8.74;16,92.16,669.84,408.95,9.02;16,92.16,681.12,259.31,9.02" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="16,258.90,669.84,242.21,9.02;16,92.16,681.12,29.96,9.02">Comparative Evaluation of Multilingual Information Access Systems</title>
	</analytic>
	<monogr>
		<title level="m" coord="16,128.95,681.39,51.21,8.74">LNCS #3237</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="376" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,70.92,694.38,439.61,9.02;16,92.16,705.66,92.54,9.02" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="16,223.58,694.65,268.90,8.74">Character n-gram tokenization for European language text retrieval</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,501.06,694.38,9.47,9.02;16,92.16,705.66,29.54,9.02">IR Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="73" to="97" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,70.92,719.19,453.05,8.74;16,92.16,730.47,31.50,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="16,160.12,719.19,226.96,8.74">Exploring new languages with HAIRCUT at CLEF 2005</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,407.56,719.19,111.89,8.74">Working Notes, CLEF 2005</title>
		<meeting><address><addrLine>Vienna</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,70.92,73.35,410.75,8.74;17,92.16,84.36,223.16,9.02" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="17,283.50,73.35,193.05,8.74">Experimentation as a way of life: Okapi at TREC</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beaulieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,92.16,84.36,157.71,9.02">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="95" to="108" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,70.92,97.62,411.01,9.02;17,92.16,108.90,120.43,9.02" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="17,144.29,97.89,223.16,8.74">Statistical inference in retrieval effectiveness evaluation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,376.50,97.62,105.43,9.02;17,92.16,108.90,49.95,9.02">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="495" to="512" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,70.92,122.16,428.39,9.02;17,92.16,133.44,102.61,9.02" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="17,148.75,122.43,332.44,8.74">Combining multiple strategies for effective monolingual and cross-lingual retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,489.84,122.16,9.47,9.02;17,92.16,133.44,29.54,9.02">IR Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="121" to="148" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,70.92,146.97,430.57,8.74;17,92.16,157.98,431.21,9.02;17,92.16,169.26,392.21,9.02" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="17,149.38,146.97,352.12,8.74;17,92.16,158.25,85.41,8.74">Report on CLEF-2003 monolingual tracks: Fusion of probabilistic models for effective monolingual retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,414.06,157.98,109.31,9.02;17,92.16,169.26,220.89,9.02">Comparative Evaluation of Multilingual Information Access Systems</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2004">2004b. 2004</date>
			<biblScope unit="page" from="322" to="336" />
		</imprint>
	</monogr>
	<note>LNCS #3237</note>
</biblStruct>

<biblStruct coords="17,70.92,182.79,419.80,8.74;17,92.16,193.80,369.02,9.02" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="17,146.26,182.79,344.46,8.74;17,92.16,194.07,38.38,8.74">Comparative study of monolingual and multilingual search models for use with Asian languages</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,139.86,193.80,256.57,9.02">ACM Transactions on Asian Languages Information Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="163" to="189" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,70.92,207.33,419.60,8.74;17,92.16,218.34,410.25,9.02;17,92.16,229.62,327.47,9.02" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="17,146.31,207.33,277.21,8.74">Data fusion for effective European monolingual information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,357.78,218.34,144.63,9.02;17,92.16,229.62,156.15,9.02">Multilingual Information Access for Text, Speech and Images</title>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J F</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><surname>Magnini</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005">2005c. 2005</date>
			<biblScope unit="page" from="233" to="244" />
		</imprint>
	</monogr>
	<note>LNCS #3491</note>
</biblStruct>

<biblStruct coords="17,70.92,243.15,426.06,8.74" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="17,208.33,243.15,263.56,8.74">Monolingual, Bilingual and GIRT Information Retrieval at CLEF</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P.-Y</forename><surname>Berger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2006. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,502.02,243.15,20.07,8.74;17,92.16,254.16,406.42,9.02;17,92.16,265.44,316.61,9.02" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="17,397.92,254.16,100.66,9.02;17,92.16,265.44,141.89,9.02">Multilingual Information Access for Text, Speech and Images</title>
		<editor>C. Peters, P. Clough, J. Gonzalo, G.J.F. Jones, M. Kluck &amp; B. Magnini</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="17,70.92,278.70,429.98,9.02" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="17,221.56,278.97,164.13,8.74">Fusion via a linear combination of scores</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Vogt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,394.68,278.70,41.55,9.02">IR Journal</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="151" to="173" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,70.92,291.96,426.88,9.02;17,92.16,303.51,200.43,8.74" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="17,173.96,292.23,200.90,8.74">Overview of the TREC 2004 robust retrieval track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<idno>#500-261</idno>
	</analytic>
	<monogr>
		<title level="m" coord="17,394.62,291.96,98.14,9.02">Proceedings TREC-2004</title>
		<meeting>TREC-2004<address><addrLine>Gaithersburg</addrLine></address></meeting>
		<imprint>
			<publisher>NIST Publication</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="70" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,70.92,316.77,366.68,8.74" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="17,174.00,316.77,115.37,8.74">The TREC 2005 robust track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,298.19,316.77,79.20,8.74">ACM-SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="48" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
