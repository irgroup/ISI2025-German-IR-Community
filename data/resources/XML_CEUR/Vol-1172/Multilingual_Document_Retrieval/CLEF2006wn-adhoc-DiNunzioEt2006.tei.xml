<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,177.24,115.72,260.83,12.93">CLEF 2006: Ad Hoc Track Overview</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,145.32,153.09,61.11,9.96"><forename type="first">Giorgio</forename><forename type="middle">M</forename><surname>Di</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,251.04,153.09,53.71,9.96"><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
							<email>ferro@dei.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,315.24,153.09,66.14,9.96"><forename type="first">Thomas</forename><surname>Mandl</surname></persName>
							<email>mandl@uni-hildesheim.de</email>
							<affiliation key="aff1">
								<orgName type="department">Information Science</orgName>
								<orgName type="institution">University of Hildesheim</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,411.37,153.09,54.33,9.96"><forename type="first">Carol</forename><surname>Peters</surname></persName>
							<email>carol.peters@isti.cnr.it</email>
							<affiliation key="aff2">
								<orgName type="institution">ISTI-CNR</orgName>
								<address>
									<addrLine>Area di Ricerca</addrLine>
									<postCode>56124</postCode>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,177.24,115.72,260.83,12.93">CLEF 2006: Ad Hoc Track Overview</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">34A7E015A1E6A87C22E53D649EF17CBD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 [Systems and Software]: Performance evaluation Experimentation, Performance, Measurement, Algorithms Multilingual Information Access, Cross-Language Information Retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe the objectives and organization of the CLEF 2006 ad hoc track and discuss the main characteristics of the tasks offered to test monolingual, bilingual, and multilingual textual document retrieval systems. The track was divided into two streams. The main stream offered mono-and bilingual tasks using the same collections as CLEF 2005: Bulgarian, English, French, Hungarian and Portuguese. The second stream, designed for more experienced participants, offered the so-called "robust task" which used test collections from previous years in six languages (Dutch, English, French, German, Italian and Spanish) with the objective of privileging experiments which achieve good stable performance over all queries rather than high average performance. The document collections used were taken from the CLEF multilingual comparable corpus of news documents. The performance achieved for each task is presented and a statistical analysis of results is given.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="31" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="32" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="33" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="34" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="35" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="36" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ad hoc retrieval track is generally considered to be the core track in the Cross-Language Evaluation Forum (CLEF). The aim of this track is to promote the development of monolingual and cross-language textual document retrieval systems. The CLEF 2006 ad hoc track was structured in two streams. The main stream offered monolingual tasks (querying and finding documents in one language) and bilingual tasks (querying in one language and finding documents in another language) using the same collections as CLEF 2005. The second stream, designed for more experienced participants, was the "robust task", aimed at finding documents for very difficult queries. It used test collections developed in previous years.</p><p>The Monolingual and Bilingual tasks were principally offered for Bulgarian, French, Hungarian and Portuguese target collections. Additionally, in the bilingual task only, newcomers (i.e. groups that had not previously participated in a CLEF cross-language task) or groups using a "new-to-CLEF" query language could choose to search the English document collection. The aim in all cases was to retrieve relevant documents from the chosen target collection and submit the results in a ranked list.</p><p>The Robust task offered monolingual, bilingual and multilingual tasks using the test collections built over three years: CLEF 2001 -2003, for six languages: Dutch, English, French, German, Italian and Spanish. Using topics from three years meant that more extensive experiments and a better analysis of the results were possible. The aim of this task was to study and achieve good performance on queries that had proved difficult in the past rather than obtain a high average performance when calculated over all queries.</p><p>In this paper we describe the track setup, the evaluation methodology and the participation in the different tasks (Section 2), present the main characteristics of the experiments and show the results (Sections 3 -5). Statistical testing is discussed in Section 6 and the final section provides a brief summing up. For information on the various approaches and resources used by the groups participating in this track and the issues they focused on, we refer the reader to the other papers in the Ad Hoc section of the Working Notes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Track Setup</head><p>The ad hoc track in CLEF adopts a corpus-based, automatic scoring method for the assessment of system performance, based on ideas first introduced in the Cranfield experiments in the late 1960s. The test collection used consists of a set of "topics" describing information needs and a collection of documents to be searched to find those documents that satisfy these information needs. Evaluation of system performance is then done by judging the documents retrieved in response to a topic with respect to their relevance, and computing the recall and precision measures. The distinguishing feature of CLEF is that it applies this evaluation paradigm in a multilingual setting. This means that the criteria normally adopted to create a test collection, consisting of suitable documents, sample queries and relevance assessments, have been adapted to satisfy the particular requirements of the multilingual context. All language dependent tasks such as topic creation and relevance judgment are performed in a distributed setting by native speakers. Rules are established and a tight central coordina- tion is maintained in order to ensure consistency and coherency of topic and relevance judgment sets over the different collections, languages and tracks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Test Collections</head><p>Different test collections were used in the ad hoc task this year. The main (i.e. non-robust) monolingual and bilingual tasks used the same document collections as in Ad Hoc last year but new topics were created and new relevance assessments made. As has already been stated, the test collection used for the robust task was derived from the test collections previously developed at CLEF. No new relevance assessments were performed for this task.</p><p>Documents. The document collections used for the CLEF 2006 ad hoc tasks are part of the CLEF multilingual corpus of newspaper and news agency documents described in the Introduction to these Proceedings.</p><p>In the main stream monolingual and bilingual tasks, the English, French and Portuguese collections consisted of national newspapers and news agencies for the period 1994 and 1995. Different variants were used for each language. Thus, for English we had both US and British newspapers, for French we had a national newspaper of France plus Swiss French news agencies, and for Portuguese we had national newspapers from both Portugal and Brazil. This means that, for each language, there were significant differences in orthography and lexicon over the sub-collections. This is a real world situation and system components, i.e. stemmers, translation resources, etc., should be sufficiently flexible to handle such variants. The Bulgarian and Hungarian collections used in these tasks were new in CLEF 2005 and consist of national newspapers for the year 2002<ref type="foot" coords="3,450.72,529.39,3.97,6.40" target="#foot_0">1</ref> . This has meant using collections of different time periods for the ad-hoc mono-and bilingual tasks. This had important consequences on topic creation.  Topics Topics in the CLEF ad hoc track are structured statements representing information needs; the systems use the topics to derive their queries. Each topic consists of three parts: a brief "title" statement; a one-sentence "description"; a more complex "narrative" specifying the relevance assessment criteria. Sets of 50 topics were created for the CLEF 2006 ad hoc mono-and bilingual tasks. One of the decisions taken early on in the organization of the CLEF ad hoc tracks was that the same set of topics would be used to query all collections, whatever the task. There were a number of reasons for this: it makes it easier to compare results over different collections, it means that there is a single master set that is rendered in all query languages, and a single set of relevance assessments for each language is sufficient for all tasks. However, in CLEF 2005 the assessors found that the fact that the collections used in the CLEF 2006 ad hoc mono-and bilingual tasks were from two different time periods <ref type="bibr" coords="4,433.56,384.33,47.46,9.96;4,134.76,396.33,38.08,9.96">(1994-1995 and 2002</ref>) made topic creation particularly difficult. It was not possible to create time-dependent topics that referred to particular date-specific events as all topics had to refer to events that could have been reported in any of the collections, regardless of the dates. This meant that the CLEF 2005 topic set is somewhat different from the sets of previous years as the topics all tend to be of broad coverage. In fact, it was difficult to construct topics that would find a limited number of relevant documents in each collection, and consequently a -probably excessive -number of topics used for the 2005 mono-and bilingual tasks have a very large number of relevant documents.</p><p>For this reason, we decided to create separate topic sets for the two different time-periods for the CLEF 2006 ad hoc mono-and bilingual tasks. We thus created two overlapping topic sets, with a common set of time independent topics and sets of time-specific topics. 25 topics were common to both sets while 25 topics were collection-specific, as follows:</p><p>-Topics C301 -C325 were used for all target collections -Topics C326 -C350 were created specifically for the English, French and Portuguese collections <ref type="bibr" coords="4,234.36,592.41,53.08,9.96">(1994/1995)</ref> -Topics C351 -C375 were created specifically for the Bulgarian and Hungarian collections <ref type="bibr" coords="4,213.48,617.97,26.24,9.96">(2002)</ref>.</p><p>This meant that a total of 75 topics were prepared in many different languages (European and non-European): Bulgarian, English, French, German, Hungarian, Italian, Portuguese, and Spanish plus Amharic, Chinese, Hindi, Indonesian, Oromo and Telugu. Participants had to select the necessary topic set according to the target collection to be used. Below we give an example of the English version of a typical CLEF topic: For the robust task, the topic sets used in CLEF 2001, CLEF 2002 and CLEF 2003 were used for evaluation. A total of 160 topics were collected and split into two sets: 60 topics used to train the system, and 100 topics used for the evaluation. Topics were available in the languages of the target collections: English, German, French, Spanish, Italian, Dutch.</p><formula xml:id="formula_0" coords="5,134.76,166.94,89.41,8.27">&lt;top&gt; &lt;num&gt; C302 &lt;/</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Participation Guidelines</head><p>To carry out the retrieval tasks of the CLEF campaign, systems have to build supporting data structures. Allowable data structures include any new structures built automatically (such as inverted files, thesauri, conceptual networks, etc.) or manually (such as thesauri, synonym lists, knowledge bases, rules, etc.) from the documents. They may not, however, be modified in response to the topics, e.g. by adding topic words that are not already in the dictionaries used by their systems in order to extend coverage. Some CLEF data collections contain manually assigned, controlled or uncontrolled index terms. The use of such terms has been limited to specific experiments that have to be declared as "manual" runs.</p><p>Topics can be converted into queries that a system can execute in many different ways. CLEF strongly encourages groups to determine what constitutes a base run for their experiments and to include these runs (officially or unofficially) to allow useful interpretations of the results. Unofficial runs are those not submitted to CLEF but evaluated using the trec eval package. This year we have used the new package written by Chris Buckley for the Text REtrieval Conference (TREC) (trec eval 7. <ref type="figure" coords="5,278.36,581.85,4.18,9.96">3</ref>) and available from the TREC website.</p><p>As a consequence of limited evaluation resources, a maximum of 12 runs each for the mono-and bilingual tasks was allowed (no more than 4 runs for any one language combination -we try to encourage diversity). We accepted a maximum of 4 runs per group and topic language for the multilingual robust task. For biand mono-lingual robust tasks, 4 runs were allowed per language or language pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Relevance Assessment</head><p>The number of documents in large test collections such as CLEF makes it impractical to judge every document for relevance. Instead approximate recall values are calculated using pooling techniques. The results submitted by the groups participating in the ad hoc tasks are used to form a pool of documents for each topic and language by collecting the highly ranked documents from all submissions. This pool is then used for subsequent relevance judgments. The stability of pools constructed in this way and their reliability for post-campaign experiments is discussed in <ref type="bibr" coords="6,227.75,221.73,10.57,9.96" target="#b0">[1]</ref> with respect to the CLEF 2003 pools. After calculating the effectiveness measures, the results are analyzed and run statistics produced and distributed. New pools were formed in CLEF 2006 for the runs submitted for the main stream mono-and bilingual tasks and the relevance assessments were performed by native speakers. Instead, the robust tasks used the original pools and relevance assessments from CLEF 2003.</p><p>The individual results for all official ad hoc experiments in CLEF 2006 are given in the Appendix at the end of the on-line Working Notes prepared for the Workshop <ref type="bibr" coords="6,181.57,317.37,10.00,9.96" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Result Calculation</head><p>Evaluation campaigns such as TREC and CLEF are based on the belief that the effectiveness of Information Retrieval Systems (IRSs) can be objectively evaluated by an analysis of a representative set of sample search results. For this, effectiveness measures are calculated based on the results submitted by the participant and the relevance assessments. Popular measures usually adopted for exercises of this type are Recall and Precision. Details on how they are calculated for CLEF are given in <ref type="bibr" coords="6,235.94,438.57,9.91,9.96" target="#b2">[3]</ref>. For the robust task, we used different measures, see below Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Participants and Experiments</head><p>As shown in Table <ref type="table" coords="6,218.64,500.01,3.90,9.96" target="#tab_4">3</ref>, a total of 25 groups from 15 different countries submitted results for one or more of the ad hoc tasks -a slight increase on the 23 participants of last year. Table <ref type="table" coords="6,221.53,523.89,4.98,9.96" target="#tab_5">4</ref> provides a breakdown of the number of participants by country.</p><p>A total of 296 experiments were submitted with an increase of 16% on the 254 experiments of 2005. On the other hand, the average number of submitted runs per participant is nearly the same: from 11 runs/participant of 2005 to 11.7 runs/participant of this year.</p><p>Participants were required to submit at least one title+description ("TD") run per task in order to increase comparability between experiments. The large majority of runs (172 out of 296, 58.11%) used this combination of topic fields, 78 (26.35%) used all fields, 41 (13.85%) used the title field, and only 5 (1.69%) used the description field. The majority of experiments were conducted using automatic query construction (287 out of 296, 96.96%) and only in a small fraction of the experiments (9 out 296, 3.04%) have queries been manually constructed from topics. A breakdown into the separate tasks is shown in Table <ref type="table" coords="8,432.36,130.29,4.11,9.96" target="#tab_6">5</ref>(a).</p><p>Fourteen different topic languages were used in the ad hoc experiments. As always, the most popular language for queries was English, with French second. The number of runs per topic language is shown in Table <ref type="table" coords="8,388.24,166.29,4.20,9.96" target="#tab_6">5</ref>(b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Main Stream Monolingual Experiments</head><p>Monolingual retrieval was offered for Bulgarian, French, Hungarian, and Portuguese. As can be seen from Table <ref type="table" coords="8,295.91,235.65,4.11,9.96" target="#tab_6">5</ref>(a), the number of participants and runs for each language was quite similar, with the exception of Bulgarian, which had a slightly smaller participation. This year just 6 groups out of 16 (37.5%) submitted monolingual runs only (down from ten groups last year), and 5 of these groups were first time participants in CLEF. This year, most of the groups submitting monolingual runs were doing this as part of their bilingual or multilingual system testing activity. Details on the different approaches used can be found in the papers in this section of the working notes. There was a lot of detailed work with Portuguese language processing; not surprising as we had four new groups from Brazil in Ad Hoc this year. As usual, there was a lot of work on the development of stemmers and morphological analysers ( <ref type="bibr" coords="8,377.36,355.17,10.31,9.96" target="#b3">[4]</ref>, for instance, applies a very deep morphological analysis for Hungarian) and comparisons of the pros and cons of so-called "light" and "heavy" stemming approaches (e.g. <ref type="bibr" coords="8,449.75,379.17,10.31,9.96" target="#b4">[5]</ref>). In contrast to previous years, we note that a number of groups experimented with NLP techniques (see, for example, papers by <ref type="bibr" coords="8,332.89,403.05,9.91,9.96" target="#b5">[6]</ref>, and <ref type="bibr" coords="8,368.91,403.05,10.23,9.96" target="#b6">[7]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Results</head><p>Table <ref type="table" coords="8,162.84,454.41,4.98,9.96" target="#tab_7">6</ref> shows the top five groups for each target collection, ordered by mean average precision. The table reports: the short name of the participating group; the mean average precision achieved by the run; the run identifier; and the performance difference between the first and the last participant. Table <ref type="table" coords="8,441.49,490.29,4.98,9.96" target="#tab_7">6</ref> regards runs using title + description fields only (the mandatory run).</p><p>Figures from 1 to 4 compare the performances of the top participants of the Monolingual tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Main Stream Bilingual Experiments</head><p>The bilingual task was structured in four subtasks (X → BG, FR, HU or PT target collection) plus, as usual, an additional subtask with English as target language restricted to newcomers in a CLEF cross-language task. This year, in this subtask, we focussed in particular on non-European topic languages and in particular languages for which there are still few processing tools or resources were in existence. We thus offered two Ethiopian languages: Amharic and Oromo; two Indian languages: Hindi and Telugu; and Indonesian. Although, as was to   be expected, the results are not particularly good, we feel that experiments of this type with lesser-studied languages are very important (see papers by <ref type="bibr" coords="12,451.32,328.77,9.91,9.96" target="#b7">[8]</ref>, <ref type="bibr" coords="12,467.28,328.77,10.00,9.96" target="#b8">[9]</ref>, <ref type="bibr" coords="12,134.76,340.77,15.49,9.96" target="#b9">[10]</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results</head><p>Table <ref type="table" coords="12,162.24,391.17,4.98,9.96" target="#tab_8">7</ref> shows the best results for this task for runs using the title+description topic fields. The performance difference between the best and the last (up to 5) placed group is given (in terms of average precision. Again both pooled and not pooled runs are included in the best entries for each track, with the exception of Bilingual X → EN.</p><p>For bilingual retrieval evaluation, a common method to evaluate performance is to compare results against monolingual baselines. For the best bilingual systems, we have the following results for CLEF 2006:</p><p>-X → BG: 52.49% of best monolingual Bulgarian IR system; -X → FR: 93.82% of best monolingual French IR system; -X → HU: 53.13% of best monolingual Hungarian IR system. -X → PT: 90.91% of best monolingual Portuguese IR system;</p><p>We can compare these to those for CLEF 2005:</p><p>-X → BG: 85% of best monolingual Bulgarian IR system; -X → FR: 85% of best monolingual French IR system; -X → HU: 73% of best monolingual Hungarian IR system. -X → PT: 88% of best monolingual Portuguese IR system; While these results are very good for the well-established-in-CLEF languages, and can be read as state-of-the-art for this kind of retrieval system, at a first glance they appear very disappointing for Bulgarian and Hungarian. However, we have to point out that, unfortunately, this year only one group submitted cross-language runs for Bulgarian and Hungarian and thus it does not make much sense to draw any conclusions from these, apparently poor, results for these languages. It is interesting to note that when Cross Language Information Retrieval (CLIR) system evaluation began in 1997 at TREC-6 the best CLIR systems had the following results:</p><p>-EN → FR: 49% of best monolingual French IR system; -EN → DE: 64% of best monolingual German IR system.</p><p>Figures from 5 to 9 compare the performances of the top participants of the Bilingual tasks with the following target languages: Bulgarian, French, Hungarian, Portuguese, and English. Although, as usual, English was by far the most popular language for queries, some less common and interesting query to target language pairs were tried, e.g. Amharic, Spanish and German to French, and French to Portuguese.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Robust Experiments</head><p>The robust task was organized for the first time at CLEF 2006. The evaluation of robustness emphasizes stable performance over all topics instead of high average performance <ref type="bibr" coords="13,209.40,595.65,14.60,9.96" target="#b10">[11]</ref>. The perspective of each individual user of an information retrieval system is different from the perspective taken by an evaluation initiative. The user will be disappointed by systems which deliver poor results for some topics whereas an evaluation initiative rewards systems which deliver good average results. A system delivering poor results for hard topics is likely to be considered of low quality by a user although it may reach high average results.  The robust task has been inspired by the robust track at TREC where it ran at <ref type="bibr" coords="16,134.76,461.37,50.14,9.96">TREC 2003</ref><ref type="bibr" coords="16,192.60,461.37,20.10,9.96">TREC , 2004</ref><ref type="bibr" coords="16,235.21,461.37,18.34,9.96">TREC and 2005</ref>. A robust evaluation stresses performance for weak topics. This can be done by using the Geometric Average Precision (GMAP) as a main indicator for performance instead of the Mean Average Precision (MAP) of all topics. Geometric average has proven to be a stable measure for robustness at TREC <ref type="bibr" coords="16,177.72,509.13,14.60,9.96" target="#b10">[11]</ref>. The robust task at CLEF 2006 is concerned with the multilingual aspects of robustness. It is essentially an ad-hoc task which offers mono-lingual and cross-lingual sub tasks.</p><p>During CLEF 2001, CLEF 2002 and CLEF 2003 a set of 160 topics (Topics #41 -#200) was developed for these collections and relevance assessments were made. No additional relevance judgements were made this year for the robust task. However, the data collection was not completely constant over all three CLEF campaigns which led to an inconsistency between relevance judgements and documents. The SDA 95 collection has no relevance judgements for most topics (#41 -#140). This inconsistency was accepted in order to increase the size of the collection. One participant reported that exploiting the knowledge would have resulted in an increase of approximately 10% in MAP <ref type="bibr" coords="16,419.88,643.53,14.60,9.96" target="#b11">[12]</ref>. However, participants were not allowed to use this knowledge. The results of the original submissions for the data sets were analyzed in order to identify the most difficult topics. This turned out to be an impossible task. The difficulty of a topic varies greatly among languages, target collections and tasks. This confirms the finding of the TREC 2005 robust task where the topic difficulty differed greatly even for two different English collections. It was found that topics are not inherently difficult but only in combination with a specific collection <ref type="bibr" coords="17,434.40,178.05,14.60,9.96" target="#b12">[13]</ref>. Topic difficulty is usually defined by low MAP values for a topic. We also considered a low number of relevant documents and high variation between systems as indicators for difficulty. Consequently, the topic set for the robust task at CLEF 2006 was arbitrarily split into two sets. Participants were allowed to use the available relevance assessments for the set of 60 training topics. The remaining 100 topics formed the test set for which results are reported. The participants were encouraged to submit results for training topics as well. These runs will be used to further analyze topic difficulty. The robust task received a total of 133 runs from eight groups listed in Table <ref type="figure" coords="17,303.03,285.69,16.43,9.96">5(a)</ref>.</p><p>Most popular among the participants were the mono-lingual French and English tasks. For the multi-lingual task, four groups submitted ten runs. The bilingual tasks received fewer runs. A run using title and description was mandatory for each group. Participants were encouraged to run their systems with the same setup for all robust tasks in which they participated (except for language specific resources). This way, the robustness of a system across languages could be explored.</p><p>Effectiveness scores for the submissions were calculated with the GMAP which is calculated as the n-th root of a product of n values. GMAP was computed using the version 8.0 of trec eval 2 program. In order to avoid undefined results, all precision score lower than 0.00001 are set to 0.00001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Robust Monolingual Results</head><p>Table <ref type="table" coords="17,162.24,468.69,4.98,9.96" target="#tab_10">8</ref> shows the best results for this task for runs using the title+description topic fields. The performance difference between the best and the last (up to 5) placed group is given (in terms of average precision).</p><p>Figures from 10 to 15 compare the performances of the top participants of the Robust Monolingual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Robust Bilingual Results</head><p>Table <ref type="table" coords="17,162.24,567.93,4.98,9.96" target="#tab_11">9</ref> shows the best results for this task for runs using the title+description topic fields. The performance difference between the best and the last (up to 5) placed group is given (in terms of average precision).</p><p>For bilingual retrieval evaluation, a common method is to compare results against monolingual baselines. We have the following results for CLEF 2006:</p><p>-X → DE: 60.37% of best monolingual German IR system;    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Robust Multilingual Results</head><p>Table <ref type="table" coords="22,161.88,501.21,10.02,9.96" target="#tab_12">10</ref> shows the best results for this task for runs using the title+description topic fields. The performance difference between the best and the last (up to 5) placed group is given (in terms of average precision).</p><p>Figure <ref type="figure" coords="22,181.45,537.09,10.02,9.96" target="#fig_4">19</ref> compares the performances of the top participants of the Robust Multilingual task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comments on Robust Cross Language Experiments</head><p>Some participants relied on the high correlation between the measure and optimized their systems as in previous campaigns. However, several groups worked specifically at optimizing for robustness. The SINAI system took an approach which has proved successful at the TREC robust task, expansion with terms gathered from a web search engine <ref type="bibr" coords="22,292.68,643.53,14.60,9.96" target="#b13">[14]</ref>. The REINA system from the University of Salamanca used a heuristic to determine hard topics during training.  Subsequently, different expansion techniques were applied <ref type="bibr" coords="25,397.08,118.29,14.60,9.96" target="#b14">[15]</ref>. Hummingbird experimented with other evaluation measures than those used in the track <ref type="bibr" coords="25,462.37,130.29,14.60,9.96" target="#b15">[16]</ref>. The MIRACLE system tried to find a fusion scheme which had a positive effect on the robust measure <ref type="bibr" coords="25,235.69,154.17,14.60,9.96" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Statistical Testing</head><p>When the goal is to validate how well results can be expected to hold beyond a particular set of queries, statistical testing can help to determine what differences between runs appear to be real as opposed to differences that are due to sampling issues. We aim to identify runs with results that are significantly different from the results of other runs. "Significantly different" in this context means that the difference between the performance scores for the runs in question appears greater than what might be expected by pure chance. As with all statistical testing, conclusions will be qualified by an error probability, which was chosen to be 0.05 in the following. We have designed our analysis to follow closely the methodology used by similar analyses carried out for TREC <ref type="bibr" coords="25,400.47,321.57,14.67,9.96" target="#b17">[18]</ref>.</p><p>We used the MATLAB Statistics Toolbox, which provides the necessary functionality plus some additional functions and utilities. We use the ANalysis Of VAriance (ANOVA) test. ANOVA makes some assumptions concerning the data be checked. Hull <ref type="bibr" coords="25,209.76,369.69,15.49,9.96" target="#b17">[18]</ref> provides details of these; in particular, the scores in question should be approximately normally distributed and their variance has to be approximately the same for all runs. Two tests for goodness of fit to a normal distribution were chosen using the MATLAB statistical toolbox: the Lilliefors test <ref type="bibr" coords="25,155.15,417.57,15.61,9.96" target="#b19">[19]</ref> and the Jarque-Bera test <ref type="bibr" coords="25,291.36,417.57,14.60,9.96" target="#b20">[20]</ref>. In the case of the CLEF tasks under analysis, both tests indicate that the assumption of normality is violated for most of the data samples (in this case the runs for each participant).</p><p>In such cases, a transformation of data should be performed. The transformation for measures that range from 0 to 1 is the arcsin-root transformation: arcsin √ x which Tague-Sutcliffe <ref type="bibr" coords="25,231.36,511.17,15.49,9.96" target="#b21">[21]</ref> recommends for use with precision/recall measures. Table <ref type="table" coords="25,176.64,523.53,10.02,9.96" target="#tab_1">11</ref> shows the results of both the Lilliefors and Jarque-Bera tests before and after applying the Tague-Sutcliffe transformation. After the transformation the analysis of the normality of samples distribution improves significantly, with some exceptions. The difficulty to transform the data into normally distributed samples derives from the original distribution of run performances which tend towards zero within the interval [0,1].</p><p>In the following sections, two different graphs are presented to summarize the results of this test. All experiments, regardless of topic language or topic fields, are included. Results are therefore only valid for comparison of individual pairs of runs, and not in terms of absolute performance. Both for the ad-hoc and robust tasks, only runs where significant differences exist are shown; the remainder of the graphs can be found in the Appendices <ref type="bibr" coords="25,384.41,655.41,9.91,9.96" target="#b1">[2]</ref>. for many groups it is the first track in which they participate and provides them with an opportunity to test their systems and compare performance between monolingual and cross-language runs, before perhaps moving on to more complex system development and subsequent evaluation. However, the track is certainly not just aimed at beginners. It also gives groups the possibility to measure advances in system performance over time. In addition, each year, we also include a task aimed at examining particular aspects of cross-language text retrieval. This year, the focus was examining the impact of "hard" topics on performance in the "robust" task.</p><p>Thus, although the ad hoc track in CLEF 2006 offered the same target languages for the main mono-and bilingual tasks as in 2005, it also had two new focuses. Groups were encouraged to use non-European languages as topic languages in the bilingual task. We were particularly interested in languages for which few processing tools were readily available, such as Amharic, Oromo and Telugu. In addition, we set up the "robust task" with the objective of providing the more expert groups with the chance to do in-depth failure analysis.</p><p>Finally, it should be remembered that, although over the years we vary the topic and target languages offered in the track, all participating groups also have the possibility of accessing and using the test collections that have been created in previous years for all of the twelve languages included in the CLEF multilingual test collection. The test collections for CLEF 2000 -CLEF 2003 are about to be made publicly available on the Evaluations and Language resources Distribution Agency (ELDA) catalog<ref type="foot" coords="35,296.76,381.67,3.97,6.40" target="#foot_1">3</ref> .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="11,243.48,722.88,128.29,8.97"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Monolingual Portuguese</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="16,257.64,410.04,99.89,8.97"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Bilingual English</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="21,230.16,722.88,155.06,8.97"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Robust Monolingual Spanish.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="22,149.76,442.53,330.89,9.96;22,134.76,454.53,119.07,9.96"><head>Figures from 16</head><label>16</label><figDesc>Figures from 16 to 18 compare the performances of the top participants of the Robust Bilingual tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="24,247.56,722.88,120.02,8.97"><head>Fig. 19 .</head><label>19</label><figDesc>Fig. 19. Robust Multilingual.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,185.04,115.32,245.17,83.25"><head>Table 1 .</head><label>1</label><figDesc>Test collections for the main stream Ad Hoc tasks.</figDesc><table coords="3,210.00,134.79,192.48,63.77"><row><cell>Language</cell><cell>Collections</cell></row><row><cell>Bulgarian</cell><cell>Sega 2002, Standart 2002</cell></row><row><cell>English</cell><cell>LA Times 94, Glasgow Herald 95</cell></row><row><cell>French</cell><cell>ATS (SDA) 94/95, Le Monde 94/95</cell></row><row><cell>Hungarian</cell><cell>Magyar Hirlap 2002</cell></row><row><cell>Portuguese</cell><cell>Público 94/95; Folha 94/95</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,134.76,554.37,346.03,81.72"><head>Table 1</head><label>1</label><figDesc>summarizes the collections used for each language. The robust task used test collections containing data in six languages (Dutch, English, German, French, Italian and Spanish) used at CLEF 2001, CLEF 2002 and CLEF 2003. There are approximately 1.35 million documents and 3.6 gigabytes of text in the CLEF 2006 "robust" collection.Table 2 summarizes the collections used for each language.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,174.48,115.32,263.40,94.17"><head>Table 2 .</head><label>2</label><figDesc>Test collections for the Robust task.</figDesc><table coords="4,174.48,134.79,263.40,74.69"><row><cell>Language</cell><cell>Collections</cell></row><row><cell>English</cell><cell>LA Times 94, Glasgow Herald 95</cell></row><row><cell>French</cell><cell>ATS (SDA) 94/95, Le Monde 94</cell></row><row><cell>Italian</cell><cell>La Stampa 94, AGZ (SDA) 94/95</cell></row><row><cell>Dutch</cell><cell>NRC Handelsblad 94/95, Algemeen Dagblad 94/95</cell></row><row><cell>German</cell><cell>Frankfurter Rundschau 94/95, Spiegel 94/95, SDA 94</cell></row><row><cell>Spanish</cell><cell>EFE 94/95</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,150.96,122.52,310.31,304.17"><head>Table 3 .</head><label>3</label><figDesc>CLEF 2006 ad hoc participants -new groups are indicated by *</figDesc><table coords="7,150.96,143.67,310.31,283.01"><row><cell>Participant</cell><cell>Institution</cell><cell>Country</cell></row><row><cell>alicante</cell><cell>U. Alicante</cell><cell>Spain</cell></row><row><cell>celi</cell><cell>CELI, Torino *</cell><cell>Italy</cell></row><row><cell>colesir</cell><cell>U.Coruna and U.Sunderland</cell><cell>Spain</cell></row><row><cell>daedalus</cell><cell>Daedalus Consortium</cell><cell>Spain</cell></row><row><cell>dcu</cell><cell>Dublin City U.</cell><cell>Ireland</cell></row><row><cell>depok</cell><cell>U.Indonesia</cell><cell>Indonesia</cell></row><row><cell>dsv</cell><cell>U.Stockholm</cell><cell>Sweden</cell></row><row><cell cols="2">erss-toulouse U.Toulouse/CNRS *</cell><cell>France</cell></row><row><cell>hildesheim</cell><cell>U.Hildesheim</cell><cell>Germany</cell></row><row><cell cols="2">hummingbird Hummingbird Core Technology Group</cell><cell>Canada</cell></row><row><cell>indianstat</cell><cell>Indian Statistical Institute *</cell><cell>India</cell></row><row><cell>jaen</cell><cell>U.Jaen</cell><cell>Spain</cell></row><row><cell>ltrc</cell><cell>Int. Inst. IT *</cell><cell>India</cell></row><row><cell>mokk</cell><cell>Budapest U.Tech and Economics</cell><cell>Hungary</cell></row><row><cell>nilc-usp</cell><cell>U.Sao Paulo -Comp.Ling. *</cell><cell>Brazil</cell></row><row><cell>pucrs</cell><cell>U.Catolica Rio Grande do Sul *</cell><cell>Brazil</cell></row><row><cell cols="2">queenmary Queen Mary, U.London *</cell><cell>United Kingdom</cell></row><row><cell>reina</cell><cell>U.Salamanca</cell><cell>Spain</cell></row><row><cell>rim</cell><cell>EMSE -Ecole Sup. des Mines</cell><cell>France</cell></row><row><cell>rsi-jhu</cell><cell>Johns Hopkins U. -APL</cell><cell>United States</cell></row><row><cell>saocarlos</cell><cell>U.Fed Sao Carlos -Comp.Sci *</cell><cell>Brazil</cell></row><row><cell>u.buffalo</cell><cell>SUNY at Buffalo</cell><cell>United States</cell></row><row><cell>ufrgs-usp</cell><cell cols="2">U.Sao Paulo and U.Fed. Rio Grande do Sul * Brazil</cell></row><row><cell>unine</cell><cell>U.Neuchatel -Informatics</cell><cell>Switzerland</cell></row><row><cell>xldb</cell><cell>U.Lisbon -Informatics</cell><cell>Portugal</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,199.80,448.92,215.91,205.45"><head>Table 4 .</head><label>4</label><figDesc>CLEF 2006 ad hoc participants by country.</figDesc><table coords="7,236.88,470.07,138.65,184.29"><row><cell>Country</cell><cell># Participants</cell></row><row><cell>Brazil</cell><cell>4</cell></row><row><cell>Canada</cell><cell>1</cell></row><row><cell>France</cell><cell>2</cell></row><row><cell>Germany</cell><cell>1</cell></row><row><cell>Hungary</cell><cell>1</cell></row><row><cell>India</cell><cell>2</cell></row><row><cell>Indonesia</cell><cell>1</cell></row><row><cell>Ireland</cell><cell>1</cell></row><row><cell>Italy</cell><cell>1</cell></row><row><cell>Portugal</cell><cell>1</cell></row><row><cell>Spain</cell><cell>5</cell></row><row><cell>Sweden</cell><cell>1</cell></row><row><cell>Switzerland</cell><cell>1</cell></row><row><cell>United Kingdom</cell><cell>1</cell></row><row><cell>United States</cell><cell>2</cell></row><row><cell>Total</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,134.76,196.20,342.76,379.09"><head>Table 5 .</head><label>5</label><figDesc>Breakdown of experiments into tracks and topic languages. (a) Number of experiments per track, participant.</figDesc><table coords="9,139.32,220.56,338.20,354.73"><row><cell></cell><cell></cell><cell></cell><cell cols="2">(b) List of experiments by</cell></row><row><cell></cell><cell></cell><cell></cell><cell>topic language.</cell><cell></cell></row><row><cell>Track</cell><cell cols="2"># Part. # Runs</cell><cell cols="2">Topic Lang. # Runs</cell></row><row><cell>Monolingual-BG</cell><cell>4</cell><cell>11</cell><cell>English</cell><cell>65</cell></row><row><cell>Monolingual-FR</cell><cell>8</cell><cell>27</cell><cell>French</cell><cell>60</cell></row><row><cell>Monolingual-HU</cell><cell>6</cell><cell>17</cell><cell>Italian</cell><cell>38</cell></row><row><cell>Monolingual-PT</cell><cell>12</cell><cell>37</cell><cell>Portuguese</cell><cell>37</cell></row><row><cell>Bilingual-X2BG</cell><cell>1</cell><cell>2</cell><cell>Spanish</cell><cell>25</cell></row><row><cell>Bilingual-X2EN</cell><cell>5</cell><cell>33</cell><cell>Hungarian</cell><cell>17</cell></row><row><cell>Bilingual-X2FR</cell><cell>4</cell><cell>12</cell><cell>German</cell><cell>12</cell></row><row><cell>Bilingual-X2HU</cell><cell>1</cell><cell>2</cell><cell>Bulgarian</cell><cell>11</cell></row><row><cell>Bilingual-X2PT</cell><cell>6</cell><cell>22</cell><cell>Indonesian</cell><cell>10</cell></row><row><cell>Robust-Mono-DE</cell><cell>3</cell><cell>7</cell><cell>Dutch</cell><cell>10</cell></row><row><cell>Robust-Mono-EN</cell><cell>6</cell><cell>13</cell><cell>Amharic</cell><cell>4</cell></row><row><cell>Robust-Mono-ES</cell><cell>5</cell><cell>11</cell><cell>Oromo</cell><cell>3</cell></row><row><cell>Robust-Mono-FR</cell><cell>7</cell><cell>18</cell><cell>Hindi</cell><cell>2</cell></row><row><cell>Robust-Mono-IT</cell><cell>5</cell><cell>11</cell><cell>Telugu</cell><cell>2</cell></row><row><cell>Robust-Mono-NL</cell><cell>3</cell><cell>7</cell><cell>Total</cell><cell>296</cell></row><row><cell>Robust-Bili-X2DE</cell><cell>2</cell><cell>5</cell><cell></cell><cell></cell></row><row><cell>Robust-Bili-X2ES</cell><cell>3</cell><cell>8</cell><cell></cell><cell></cell></row><row><cell>Robust-Bili-X2NL</cell><cell>1</cell><cell>4</cell><cell></cell><cell></cell></row><row><cell>Robust-Multi</cell><cell>4</cell><cell>10</cell><cell></cell><cell></cell></row><row><cell>Robust-Training-Mono-DE</cell><cell>2</cell><cell>3</cell><cell></cell><cell></cell></row><row><cell>Robust-Training-Mono-EN</cell><cell>4</cell><cell>7</cell><cell></cell><cell></cell></row><row><cell>Robust-Training-Mono-ES</cell><cell>3</cell><cell>5</cell><cell></cell><cell></cell></row><row><cell>Robust-Training-Mono-FR</cell><cell>5</cell><cell>10</cell><cell></cell><cell></cell></row><row><cell>Robust-Training-Mono-IT</cell><cell>3</cell><cell>5</cell><cell></cell><cell></cell></row><row><cell>Robust-Training-Mono-NL</cell><cell>2</cell><cell>3</cell><cell></cell><cell></cell></row><row><cell>Robust-Training-Bili-X2DE</cell><cell>1</cell><cell>1</cell><cell></cell><cell></cell></row><row><cell>Robust-Training-Bili-X2ES</cell><cell>1</cell><cell>2</cell><cell></cell><cell></cell></row><row><cell>Robust-Training-Multi</cell><cell>2</cell><cell>3</cell><cell></cell><cell></cell></row><row><cell>Total</cell><cell></cell><cell>296</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="12,124.08,115.32,380.24,173.85"><head>Table 6 .</head><label>6</label><figDesc>Best entries for the monolingual track.</figDesc><table coords="12,124.08,136.47,380.24,152.69"><row><cell>Track</cell><cell></cell><cell cols="3">Participant Rank</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1st</cell><cell>2nd</cell><cell>3rd</cell><cell>4th</cell><cell>5th</cell><cell>Diff.</cell></row><row><cell>Bulgarian</cell><cell>unine</cell><cell>rsi-jhu</cell><cell cols="2">hummingbird daedalus</cell><cell></cell><cell>1st vs 4th</cell></row><row><cell cols="2">MAP 33.14%</cell><cell>31.98%</cell><cell>30.47%</cell><cell>27.87%</cell><cell></cell><cell>20.90%</cell></row><row><cell cols="5">Run UniNEbg2 02aplmobgtd4 humBG06tde bgFSbg2S</cell><cell></cell><cell></cell></row><row><cell>French</cell><cell>unine</cell><cell>rsi-jhu</cell><cell>hummingbird</cell><cell>alicante</cell><cell>daedalus</cell><cell>1st vs 5th</cell></row><row><cell cols="2">MAP 44.68%</cell><cell>40.96%</cell><cell>40.77%</cell><cell>38.28%</cell><cell>37.94%</cell><cell>17.76%</cell></row><row><cell cols="4">Run UniNEfr3 95aplmofrtd5s1 humFR06tde</cell><cell>8dfrexp</cell><cell>frFSfr2S</cell><cell></cell></row><row><cell>Hungarian</cell><cell>unine</cell><cell>rsi-jhu</cell><cell>alicante</cell><cell>mokk</cell><cell cols="2">hummingbird 1st vs 5th</cell></row><row><cell cols="2">MAP 41.35%</cell><cell>39.11%</cell><cell>35.32%</cell><cell>34.95%</cell><cell>32.24%</cell><cell>28.26%</cell></row><row><cell cols="3">Run UniNEhu2 02aplmohutd4</cell><cell>30dfrexp</cell><cell>plain2</cell><cell>humHU06tde</cell><cell></cell></row><row><cell>Portuguese</cell><cell>unine</cell><cell>hummingbird</cell><cell>alicante</cell><cell>rsi-jhu</cell><cell>u.buffalo</cell><cell>1st vs 5th</cell></row><row><cell cols="2">MAP 45.52%</cell><cell>45.07%</cell><cell>43.08%</cell><cell>42.42%</cell><cell>40.53%</cell><cell>12.31%</cell></row><row><cell cols="6">Run UniNEpt1 humPT06tde 30okapiexp 95aplmopttd5 UBptTDrf1</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="13,81.60,115.32,446.12,207.45"><head>Table 7 .</head><label>7</label><figDesc>Best entries for the bilingual task.</figDesc><table coords="13,81.60,136.47,446.12,186.29"><row><cell>Track</cell><cell></cell><cell></cell><cell cols="2">Participant Rank</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1st</cell><cell>2nd</cell><cell>3rd</cell><cell>4th</cell><cell>5th</cell><cell>Diff.</cell></row><row><cell>Bulgarian</cell><cell>daedalus</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MAP</cell><cell>17.39%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Run bgFSbgWen2S</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>French</cell><cell>unine</cell><cell>queenmary</cell><cell>rsi-jhu</cell><cell>daedalus</cell><cell></cell><cell>1st vs 4th</cell></row><row><cell>MAP</cell><cell>41.92%</cell><cell>33.96%</cell><cell>33.60%</cell><cell>33.20%</cell><cell></cell><cell>26.27%</cell></row><row><cell cols="3">Run UniNEBifr1 QMUL06e2f10b</cell><cell>aplbienfrd</cell><cell>frFSfrSen2S</cell><cell></cell><cell></cell></row><row><cell>Hungarian</cell><cell>daedalus</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MAP</cell><cell>21.97%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Run huFShuMen2S</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Portuguese</cell><cell>unine</cell><cell>rsi-jhu</cell><cell>queenmary</cell><cell>u.buffalo</cell><cell>daedalus</cell><cell>1st vs 5th</cell></row><row><cell>MAP</cell><cell>41.38%</cell><cell>35.49%</cell><cell>35.26%</cell><cell>29.08%</cell><cell>26.50%</cell><cell>55.85%</cell></row><row><cell cols="2">Run UniNEBipt2</cell><cell>aplbiesptd</cell><cell cols="2">QMUL06e2p10b UBen2ptTDrf2</cell><cell>ptFSptSen2S</cell><cell></cell></row><row><cell>English</cell><cell>rsi-jhu</cell><cell>depok</cell><cell>ltrc</cell><cell>celi</cell><cell>dsv</cell><cell>1st vs 5th</cell></row><row><cell>MAP</cell><cell>32.57%</cell><cell>26.71%</cell><cell>25.04%</cell><cell>23.97%</cell><cell>22.78%</cell><cell>42.98%</cell></row><row><cell cols="2">Run aplbiinen5</cell><cell>UI td mt</cell><cell>OMTD</cell><cell cols="2">CELItitleNOEXPANSION DsvAmhEngFullNofuzz</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="18,109.92,234.72,403.28,306.45"><head>Table 8 .</head><label>8</label><figDesc>Best entries for the robust monolingual task.</figDesc><table coords="18,109.92,255.87,403.28,285.29"><row><cell>Track</cell><cell></cell><cell cols="2">Participant Rank</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1st</cell><cell>2nd</cell><cell>3rd</cell><cell>4th</cell><cell>5th</cell><cell>Diff.</cell></row><row><cell cols="2">Dutch hummingbird</cell><cell>daedalus</cell><cell>colesir</cell><cell></cell><cell></cell><cell>1st vs 3rd</cell></row><row><cell>MAP</cell><cell>51.06%</cell><cell>42.39%</cell><cell>41.60%</cell><cell></cell><cell></cell><cell>22.74%</cell></row><row><cell>GMAP</cell><cell>25.76%</cell><cell>17.57%</cell><cell>16.40%</cell><cell></cell><cell></cell><cell>57.13%</cell></row><row><cell cols="3">Run humNL06Rtde nlFSnlR2S</cell><cell>CoLesIRnlTst</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">English hummingbird</cell><cell>reina</cell><cell>dcu</cell><cell>daedalus</cell><cell>colesir</cell><cell>1st vs 5th</cell></row><row><cell>MAP</cell><cell>47.63%</cell><cell>43.66%</cell><cell>43.48%</cell><cell>39.69%</cell><cell>37.64%</cell><cell>26.54%</cell></row><row><cell>GMAP</cell><cell>11.69%</cell><cell>10.53%</cell><cell>10.11%</cell><cell>8.93%</cell><cell>8.41%</cell><cell>39.00%</cell></row><row><cell cols="6">Run humEN06Rtde reinaENtdtest dcudesceng12075 enFSenR2S CoLesIRenTst</cell><cell></cell></row><row><cell>French</cell><cell>unine</cell><cell>hummingbird</cell><cell>reina</cell><cell>dcu</cell><cell>colesir</cell><cell>1st vs 5th</cell></row><row><cell>MAP</cell><cell>47.57%</cell><cell>45.43%</cell><cell>44.58%</cell><cell>41.08%</cell><cell>39.51%</cell><cell>20.40%</cell></row><row><cell>GMAP</cell><cell>15.02%</cell><cell>14.90%</cell><cell>14.32%</cell><cell>12.00%</cell><cell>11.91%</cell><cell>26.11%</cell></row><row><cell cols="6">Run UniNEfrr1 humFR06Rtde reinaFRtdtest dcudescfr12075 CoLesIRfrTst</cell><cell></cell></row><row><cell cols="2">German hummingbird</cell><cell>colesir</cell><cell>daedalus</cell><cell></cell><cell></cell><cell>1st vs 3rd</cell></row><row><cell>MAP</cell><cell>48.30%</cell><cell>37.21%</cell><cell>34.06%</cell><cell></cell><cell></cell><cell>41.81%</cell></row><row><cell>GMAP</cell><cell>22.53%</cell><cell>14.80%</cell><cell>10.61%</cell><cell></cell><cell></cell><cell>112.35%</cell></row><row><cell cols="3">Run humDE06Rtde CoLesIRdeTst</cell><cell>deFSdeR2S</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Italian hummingbird</cell><cell>reina</cell><cell>dcu</cell><cell>daedalus</cell><cell>colesir</cell><cell>1st vs 5th</cell></row><row><cell>MAP</cell><cell>41.94%</cell><cell>38.45%</cell><cell>37.73%</cell><cell>35.11%</cell><cell>32.23%</cell><cell>30.13%</cell></row><row><cell>GMAP</cell><cell>11.47%</cell><cell>10.55%</cell><cell>9.19%</cell><cell>10.50%</cell><cell>8.23%</cell><cell>39.37%</cell></row><row><cell cols="4">Run humIT06Rtde reinaITtdtest dcudescit1005</cell><cell>itFSitR2S</cell><cell>CoLesIRitTst</cell><cell></cell></row><row><cell cols="2">Spanish hummingbird</cell><cell>reina</cell><cell>dcu</cell><cell>daedalus</cell><cell>colesir</cell><cell>1st vs 5th</cell></row><row><cell>MAP</cell><cell>45.66%</cell><cell>44.01%</cell><cell>42.14%</cell><cell>40.40%</cell><cell>40.17%</cell><cell>13.67%</cell></row><row><cell>GMAP</cell><cell>23.61%</cell><cell>22.65%</cell><cell>21.32%</cell><cell>19.64%</cell><cell>18.84%</cell><cell>25.32%</cell></row><row><cell cols="4">Run humES06Rtde reinaEStdtest dcudescsp12075</cell><cell cols="2">esFSesR2S CoLesIResTst</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="22,139.32,115.32,333.67,173.85"><head>Table 9 .</head><label>9</label><figDesc>Best entries for the robust bilingual task.</figDesc><table coords="22,139.32,136.47,333.67,152.69"><row><cell>Track</cell><cell></cell><cell>Participant Rank</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1st</cell><cell>2nd</cell><cell>3rd</cell><cell>4th 5th</cell><cell>Diff.</cell></row><row><cell>Dutch</cell><cell>daedalus</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MAP</cell><cell>35.37%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>GMAP</cell><cell>9.75%</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Run nlFSnlRLfr2S</cell><cell></cell><cell></cell><cell></cell></row><row><cell>German</cell><cell>daedalus</cell><cell>colesir</cell><cell></cell><cell></cell><cell>1st vs 2nd</cell></row><row><cell>MAP</cell><cell>29.16%</cell><cell>25.24%</cell><cell></cell><cell></cell><cell>15.53%</cell></row><row><cell>GMAP</cell><cell>5.18%</cell><cell>4.31%</cell><cell></cell><cell></cell><cell>20.19%</cell></row><row><cell cols="3">Run deFSdeRSen2S CoLesIRendeTst</cell><cell></cell><cell></cell></row><row><cell>Spanish</cell><cell>reina</cell><cell>dcu</cell><cell>daedalus</cell><cell></cell><cell>1st vs 3rd</cell></row><row><cell>MAP</cell><cell>36.93%</cell><cell>33.22%</cell><cell>26.89%</cell><cell></cell><cell>37.34%</cell></row><row><cell>GMAP</cell><cell>13.42%</cell><cell>10.44%</cell><cell>6.19%</cell><cell></cell><cell>116.80%</cell></row><row><cell cols="4">Run reinaIT2EStdtest dcuitqydescsp12075 esFSesRLit2S</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="22,124.08,301.20,359.00,140.01"><head>Table 10 .</head><label>10</label><figDesc>Best entries for the robust multilingual task.</figDesc><table coords="22,124.08,322.47,359.00,118.74"><row><cell>Track</cell><cell></cell><cell></cell><cell>Participant Rank</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1st</cell><cell>2nd</cell><cell>3rd</cell><cell>4th</cell><cell>5th</cell><cell>Diff.</cell></row><row><cell>Multilingual</cell><cell>jaen</cell><cell>daedalus</cell><cell>colesir</cell><cell>reina</cell><cell></cell><cell>1st vs 4th</cell></row><row><cell cols="2">MAP 27.85%</cell><cell>22.67%</cell><cell>22.63%</cell><cell>19.96%</cell><cell></cell><cell>39.53%</cell></row><row><cell cols="2">GMAP 15.69%</cell><cell>11.04%</cell><cell>11.24%</cell><cell>13.25%</cell><cell></cell><cell>18.42%</cell></row><row><cell cols="5">Run ujamlrsv2 mlRSFSen2S CoLesIRmultTst reinaES2mtdtest</cell><cell></cell></row><row><cell cols="5">-X → ES: 80.88% of best monolingual Spanish IR system; -X → NL: 69.27% of best monolingual Dutch IR system.</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.72,645.24,335.77,8.97;3,144.72,656.16,97.23,8.97"><p>It proved impossible to find national newspapers in electronic form for 1994 and/or 1995 in these languages.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="35,144.72,656.66,93.63,8.27"><p>http://www.elda.org/</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" coords="26,134.76,116.19,38.17,8.13">Table 11</ref><p>. Lilliefors (LF) and Jarque-Bera (JB) test for each Ad-Hoc track with and without Tague-Sutcliffe (TS) arcsin transformation. Each entry is the number of experiments whose performance distribution can be considered drawn from a Gaussian distribution, with respect to the total number of experiment of the track. The value of alpha for this test was set to 5%. The first graph shows participants' runs (y axis) and performance obtained (x axis). The circle indicates the average performance (in terms of Precision) while the segment shows the interval in which the difference in performance is not statistically significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Track</head><p>The second graph shows the overall results where all the runs that are included in the same group do not have a significantly different performance. All runs scoring below a certain group perform significantly worse than at least the top entry of the group. Likewise all the runs scoring above a certain group perform significantly better than at least the bottom entry in that group. To determine all runs that perform significantly worse than a certain run, determine the rightmost group that includes the run, all runs scoring below the bottom entry of that group are significantly worse. Conversely, to determine all runs that perform significantly better than a given run, determine the leftmost group that includes the run. All runs that score better than the top entry of that group perform significantly better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We have reported the results of the ad hoc cross-language textual document retrieval track at CLEF 2006. This track is considered to be central to CLEF as Experiments Experiments Experiments Experiments  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="35,142.88,445.32,337.61,8.97;35,151.56,456.36,328.98,8.97;35,151.56,467.28,329.03,8.97;35,151.56,478.20,329.03,8.97;35,151.56,489.24,203.67,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="35,216.10,445.32,136.34,8.97">CLEF 2003 -Overview of results</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="35,276.31,456.36,204.23,8.97;35,151.56,467.28,329.03,8.97;35,151.56,478.20,148.87,8.97">Comparative Evaluation of Multilingual Information Access Systems: Fourth Workshop of the Cross-Language Evaluation Forum (CLEF 2003) Revised Selected Papers</title>
		<title level="s" coord="35,307.05,478.20,168.26,8.97">Lecture Notes in Computer Science (LNCS</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3237</biblScope>
			<biblScope unit="page" from="44" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="35,142.88,500.88,337.75,8.97;35,151.56,511.80,329.08,8.97;35,151.56,522.84,97.76,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="35,270.56,500.88,163.09,8.97">Appendix A. Results of the Core Tracks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Di Nunzio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="35,291.42,511.80,184.45,8.97">Working Notes for the CLEF 2006 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</editor>
		<imprint>
			<publisher>Published Online</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="35,142.88,534.48,337.61,8.97;35,151.56,545.40,328.82,8.97;35,151.56,556.32,328.87,8.97;35,151.56,567.36,329.05,8.97;35,151.56,578.28,233.79,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="35,261.56,534.48,153.69,8.97">CLEF 2003 Methodology and Metrics</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="35,322.37,545.40,158.01,8.97;35,151.56,556.32,328.87,8.97;35,151.56,567.36,180.07,8.97">Comparative Evaluation of Multilingual Information Access Systems: Fourth Workshop of the Cross-Language Evaluation Forum (CLEF 2003) Revised Selected Papers</title>
		<title level="s" coord="35,338.73,567.36,141.88,8.97;35,151.56,578.28,26.39,8.97">Lecture Notes in Computer Science (LNCS</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3237</biblScope>
			<biblScope unit="page" from="7" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="35,142.88,589.92,337.74,8.97;35,151.56,600.96,328.85,8.97;35,151.56,611.88,143.59,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="35,200.74,589.92,276.00,8.97">Benefits of Deep NLP-based Lemmatization for Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halácsy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="35,335.93,600.96,144.47,8.97;35,151.56,611.88,38.11,8.97">Working Notes for the CLEF 2006 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</editor>
		<imprint>
			<publisher>Published Online</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="35,142.88,623.52,337.86,8.97;35,151.56,634.44,166.53,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="35,240.58,623.52,240.16,8.97;35,151.56,634.44,42.68,8.97">A Study on the use of Stemming for Monolingual Ad-Hoc Portuguese</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Moreira Orengo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="35,202.67,634.44,86.90,8.97">Information Retrieval</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="36,142.88,119.04,337.79,8.97;36,151.56,130.08,328.94,8.97;36,151.56,141.00,328.98,8.97;36,151.56,151.92,55.06,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="36,433.07,119.04,47.60,8.97;36,151.56,130.08,236.26,8.97">Using Noun Phrases for Local Analysis in Automatic Query Expansion</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Azevedo Arcoverde</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Das Gracas Volpe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Nunes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Scardua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="36,245.71,141.00,186.47,8.97">Working Notes for the CLEF 2006 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</editor>
		<imprint>
			<publisher>Published Online</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="36,142.88,162.24,337.61,8.97;36,151.56,173.16,328.93,8.97;36,151.56,184.08,165.08,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="36,283.39,162.24,197.10,8.97;36,151.56,173.16,16.64,8.97">The PUCRS-PLN Group participation at CLEF 2006</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">L S</forename><surname>De Lima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="36,360.05,173.16,120.44,8.97;36,151.56,184.08,59.59,8.97">Working Notes for the CLEF 2006 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</editor>
		<imprint>
			<publisher>Published Online</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="36,142.88,194.40,337.68,8.97;36,151.56,205.32,329.06,8.97;36,151.56,216.24,192.91,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="36,259.28,194.40,221.28,8.97;36,151.56,205.32,24.61,8.97">Oromo-English Information Retrieval Experiments at CLEF</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">K</forename><surname>Tune</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="36,388.49,205.32,92.13,8.97;36,151.56,216.24,87.30,8.97">Working Notes for the CLEF 2006 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</editor>
		<imprint>
			<publisher>Published Online</publisher>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="36,142.88,226.56,337.56,8.97;36,151.56,237.48,329.05,8.97;36,151.56,248.40,222.91,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="36,247.76,226.56,232.68,8.97;36,151.56,237.48,92.70,8.97">Hindi and Telugu to English Cross Language Information Retrieval at CLEF 2006</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pingali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="36,421.30,237.48,59.31,8.97;36,151.56,248.40,117.31,8.97">Working Notes for the CLEF 2006 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</editor>
		<imprint>
			<publisher>Published Online</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="36,142.54,258.60,337.87,8.97;36,151.56,269.64,328.91,8.97;36,151.56,280.56,208.75,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="36,301.15,258.60,179.26,8.97;36,151.56,269.64,66.64,8.97">Evaluating Language Resources for English-Indonesian CLIR</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hayurani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Adriani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="36,405.95,269.64,74.52,8.97;36,151.56,280.56,103.26,8.97">Working Notes for the CLEF 2006 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</editor>
		<imprint>
			<publisher>Published Online</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="36,142.54,290.76,337.93,8.97;36,151.56,301.80,22.85,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="36,223.90,290.76,144.69,8.97">The TREC Robust Retrieval Track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="36,379.97,290.76,56.22,8.97">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="11" to="20" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="36,142.54,312.00,337.87,8.97;36,151.56,322.92,328.80,8.97;36,151.56,333.96,306.94,8.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="36,237.45,312.00,242.96,8.97;36,151.56,322.92,173.37,8.97">UniNE at CLEF 2006: Experiments with Monolingual, Bilingual, Domain-Specific and Robust Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Abdou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="36,172.55,333.96,180.42,8.97">Working Notes for the CLEF 2006 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</editor>
		<imprint>
			<publisher>Published Online</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="36,142.54,344.16,337.96,8.97;36,151.56,355.08,328.82,8.97;36,151.56,366.12,328.90,8.97;36,151.56,377.04,121.18,8.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="36,218.02,344.16,205.71,8.97">Overview of the TREC 2005 Robust Retrieval Track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec14/t14_proceedings.html[lastvisited2006" />
	</analytic>
	<monogr>
		<title level="m" coord="36,260.10,355.08,220.28,8.97;36,151.56,366.12,52.97,8.97">The Fourteenth Text REtrieval Conference Proceedings (TREC 2005)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2005-08-04">August 4. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="36,142.54,387.24,338.08,8.97;36,151.56,398.28,329.07,8.97;36,151.56,409.20,329.05,8.97;36,151.56,420.12,248.84,8.97" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="36,151.56,398.28,329.07,8.97;36,151.56,409.20,104.06,8.97">SINAI at CLEF 2006 Ad-hoc Robust Multilingual Track: Query Expansion using the Google Search Engine</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Martinez-Santiago</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Montejo-Ráez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Garcia-Cumbreras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ureña-Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="36,446.49,409.20,34.12,8.97;36,151.56,420.12,143.23,8.97">Working Notes for the CLEF 2006 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</editor>
		<imprint>
			<publisher>Published Online</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="36,142.54,430.44,337.96,8.97;36,151.56,441.36,328.94,8.97;36,151.56,452.28,328.98,8.97;36,151.56,463.32,55.06,8.97" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="36,305.96,430.44,174.55,8.97;36,151.56,441.36,240.54,8.97">REINA at CLEF 2006 Robust Task: Local Query Expansion Using Term Windows for Robust Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zazo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Figuerola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Berrocal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="36,245.71,452.28,186.47,8.97">Working Notes for the CLEF 2006 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</editor>
		<imprint>
			<publisher>Published Online</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="36,142.54,473.52,338.07,8.97;36,151.56,484.44,328.93,8.97;36,151.56,495.48,165.08,8.97" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="36,212.73,473.52,267.89,8.97;36,151.56,484.44,35.22,8.97">Comparing the Robustness of Expansion Techniques and Retrieval Measures</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tomlinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="36,365.32,484.44,115.17,8.97;36,151.56,495.48,59.59,8.97">Working Notes for the CLEF 2006 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</editor>
		<imprint>
			<publisher>Published Online</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="36,142.54,505.68,338.12,8.97;36,151.56,516.60,328.94,8.97;36,151.56,527.64,328.18,8.97" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="36,395.03,505.68,85.63,8.97;36,151.56,516.60,168.50,8.97">Report of the MIRA-CLE teach for the Ad-hoc track in CLEF</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Goni-Menoyo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalez-Cristobal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vilena-Román</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="36,193.77,527.64,180.42,8.97">Working Notes for the CLEF 2006 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</editor>
		<imprint>
			<publisher>Published Online</publisher>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="36,142.54,537.84,338.10,8.97" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="36,190.42,537.84,273.27,8.97">Using Statistical Testing in the Evaluation of Retrieval Experiments</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hull</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="36,151.56,548.76,329.08,8.97;36,151.56,559.68,329.06,8.97;36,151.56,570.72,240.05,8.97" xml:id="b18">
	<monogr>
		<title level="m" coord="36,347.91,548.76,132.73,8.97;36,151.56,559.68,329.06,8.97;36,151.56,570.72,52.73,8.97">Proc. 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1993)</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Korfhage</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Rasmussen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Willett</surname></persName>
		</editor>
		<meeting>16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1993)<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="329" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="36,142.54,580.92,338.07,8.97;36,151.56,591.84,92.99,8.97" xml:id="b19">
	<monogr>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Conover</surname></persName>
		</author>
		<title level="m" coord="36,215.01,580.92,137.33,8.97">Practical Nonparametric Statistics</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley and Sons</publisher>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
	<note>1st edn</note>
</biblStruct>

<biblStruct coords="36,142.54,602.16,338.11,8.97;36,151.56,613.08,329.08,8.97;36,151.56,624.00,72.24,8.97" xml:id="b20">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">G</forename><surname>Judge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">C</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">E</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lütkepohl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<title level="m" coord="36,430.17,602.16,50.49,8.97;36,151.56,613.08,174.62,8.97">Introduction to the Theory and Practice of Econometrics</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley and Sons</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
	<note>2nd edn</note>
</biblStruct>

<biblStruct coords="36,142.54,634.32,337.94,8.97;36,151.56,645.24,329.06,8.97;36,151.56,656.16,328.75,8.97" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="36,236.14,634.32,244.34,8.97;36,151.56,645.24,35.87,8.97">The Pragmatics of Information Retrieval Experimentation, Revisited</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tague-Sutcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="36,341.92,645.24,134.82,8.97">Readings in Information Retrieval</title>
		<editor>
			<persName><forename type="first">Spack</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Willett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename></persName>
		</editor>
		<meeting><address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publisher, Inc</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="205" to="216" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
