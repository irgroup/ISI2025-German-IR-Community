<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,94.68,148.63,413.43,15.51;1,252.24,170.59,98.68,15.51">GeoCLEF Text Retrieval and Manual Expansion Approaches</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,223.44,203.97,63.84,9.96"><forename type="first">Ray</forename><forename type="middle">R</forename><surname>Larson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,314.68,203.97,64.74,9.96"><forename type="first">Fredric</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,94.68,148.63,413.43,15.51;1,252.24,170.59,98.68,15.51">GeoCLEF Text Retrieval and Manual Expansion Approaches</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6AAF6784C356E550BD36D80398AAFCD6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.7 Digital Libraries Algorithms, Performance, Measurement Cheshire II, Logistic Regression, Data Fusion 1 Introduction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we will describe the Berkeley approaches to the GeoCLEF tasks for CLEF 2006. This year we used two separate systems for different tasks. Although of the systems both use versions of the same primary retrieval algorithm they differ in the supporting text pre-processing tools used.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>LR model of probabilistic IR attempts to estimate the probability of relevance for each document based on a set of statistics about a document collection and a set of queries in combination with a set of weighting coefficients for those statistics. The statistics to be used and the values of the coefficients are obtained from regression analysis of a sample of a collection (or similar test collection) for some set of queries where relevance and non-relevance has been determined. More formally, given a particular query and a particular document in a collection P (R | Q, D) is calculated and the documents or components are presented to the user ranked in order of decreasing values of that probability. To avoid invalid probability values, the usual calculation of P (R | Q, D) uses the "log odds" of relevance given a set of S statistics, s i , derived from the query and database, such that:</p><formula xml:id="formula_0" coords="2,235.08,230.18,277.90,30.50">log O(R | Q, D) = b 0 + S i=1 b i s i (1)</formula><p>where b 0 is the intercept term and the b i are the coefficients obtained from the regression analysis of the sample collection and relevance judgements. The final ranking is determined by the conversion of the log odds form to probabilities:</p><formula xml:id="formula_1" coords="2,232.44,310.85,136.40,25.12">P (R | Q, D) = e log O(R|Q,D) 1 + e log O(R|Q,D)</formula><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">TREC2 Logistic Regression Algorithm</head><p>For GeoCLEF we used a version the Logistic Regression (LR) algorithm that has been used very successfully in Cross-Language IR by Berkeley researchers for a number of years <ref type="bibr" coords="2,450.92,380.37,11.83,9.96" target="#b2">[3]</ref>. We used two different implementations of the algorithm. One was in stand-alone experimental software developed by Aitao Chen, and the other in the Cheshire II information retrieval system. Although the basic behaviour of the algorithm is the same for both systems, there are differences in the sets of pre-processing and indexing elements used in retrieval. One of the primary differences is the lack of decompounding for German documents and query terms in the Cheshire II system. The formal definition of the TREC2 Logistic Regression algorithm used is:</p><formula xml:id="formula_2" coords="2,184.08,482.94,328.90,147.36">log O(R|C, Q) = log p(R|C, Q) 1 -p(R|C, Q) = log p(R|C, Q) p(R|C, Q) = c 0 + c 1 * 1 |Q c | + 1 |Qc| i=1 qtf i ql + 35 + c 2 * 1 |Q c | + 1 |Qc| i=1 log tf i cl + 80 (3) -c 3 * 1 |Q c | + 1 |Qc| i=1 log ctf i N t + c 4 * |Q c |</formula><p>where C denotes a document component (i.e., an indexed part of a document which may be the entire document) and Q a query, R is a relevance variable, c k are the k coefficients obtained though the regression analysis.</p><formula xml:id="formula_3" coords="2,90.00,673.29,422.65,62.10">p(R|C, Q) is the probability that document component C is relevant to query Q, p(R|C, Q) the probability that document component C is not relevant to query Q, which is 1.0 - p(R|C, Q) |Q c |</formula><p>If stopwords are removed from indexing, then ql, cl, and N t are the query length, document length, and collection length, respectively. If the query terms are re-weighted (in feedback, for example), then qtf i is no longer the original term frequency, but the new weight, and ql is the sum of the new weight values for the query terms. Note that, unlike the document and collection lengths, query length is the "optimized" relative frequency without first taking the log over the matching terms.</p><p>The coefficients were determined by fitting the logistic regression model specified in log O(R|C, Q) to TREC training data using a statistical software package. The coefficients, c k , used for our official runs are the same as those described by Chen <ref type="bibr" coords="3,320.47,322.41,13.49,9.96" target="#b0">[1]</ref>. These were: c 0 = -3.51, c 1 = 37.4, c 2 = 0.330, c 3 = 0.1937 and c 4 = 0.0929. Further details on the TREC2 version of the Logistic Regression algorithm may be found in Cooper et al. <ref type="bibr" coords="3,320.93,346.41,9.98,9.96" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Blind Relevance Feedback</head><p>In addition to the direct retrieval of documents using the TREC2 logistic regression algorithm described above, we have implemented a form of "blind relevance feedback" as a supplement to the basic algorithm. The algorithm used for blind feedback was originally developed and described by Chen <ref type="bibr" coords="3,115.43,428.25,9.98,9.96" target="#b1">[2]</ref>. Blind relevance feedback has become established in the information retrieval community due to its consistent improvement of initial search results as seen in TREC, CLEF and other retrieval evaluations <ref type="bibr" coords="3,179.66,452.13,9.98,9.96">[6]</ref>. The blind feedback algorithm is based on the probabilistic term relevance weighting formula developed by Robertson and Sparck Jones <ref type="bibr" coords="3,358.69,464.13,9.89,9.96">[8]</ref>.</p><p>Blind relevance feedback is typically performed in two stages. First, an initial search using the original topic statement is performed, after which a number of terms are selected from some number of the top-ranked documents (which are presumed to be relevant). The selected terms are then weighted and then merged with the initial query to formulate a new query. Finally the reweighted and expanded query is submitted against the same collection to produce a final ranked list of documents. Obviously there are important choices to be made regarding the number of top-ranked documents to consider, and the number of terms to extract from those documents. For ImageCLEF this year, having no prior data to guide us, we chose to use the top 10 terms from 10 top-ranked documents. The terms were chosen by extracting the document vectors for each of the 10 and computing the Robertson and Sparck Jones term relevance weight for each document. This weight is based on a contingency table where the counts of 4 different conditions for combinations of (assumed) relevance and whether or not the term is, or is not in a document. Table <ref type="table" coords="3,478.63,607.53,5.03,9.96">1</ref> shows this contingency table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relevant Not Relevant</head><formula xml:id="formula_4" coords="3,187.08,653.61,228.36,34.68">In doc R t N t -R t N t Not in doc R -R t N -N t -R + R t N -N t R N -R N</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1: Contingency table for term relevance weighting</head><p>The relevance weight is calculated using the assumption that the first 10 documents are relevant and all others are not. For each term in these documents the following weight is calculated:</p><formula xml:id="formula_5" coords="4,255.24,119.66,257.74,30.29">w t = log Rt R-Rt Nt-Rt N -Nt-R+Rt (4)</formula><p>The 10 terms (including those that appeared in the original query) with the highest w t are selected and added to the original query terms. For the terms not in the original query, the new "term frequency" (qtf i in main LR equation above) is set to 0.5. Terms that were in the original query, but are not in the top 10 terms are left with their original qtf i . For terms in the top 10 and in the original query the new qtf i is set to 1.5 times the original qtf i for the query. The new query is then processed using the same LR algorithm as shown in Equation <ref type="formula" coords="4,403.80,215.97,5.03,9.96">4</ref>and the ranked results returned as the response for that topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approaches for GeoCLEF</head><p>In this section we describe the specific approaches taken for our submitted runs for the GeoCLEF task. First we describe the indexing and term extraction methods used, and then the search features we used for the submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Indexing and Term Extraction</head><p>The standalone version treats all text as a single "bag of words" that is extracted and indexed. For German documents it uses a custom "decompounding" algorithm to extract component terms from German compounds. The Cheshire II system uses the XML structure and extracts selected portions of the record for indexing and retrieval.   <ref type="table" coords="4,131.98,613.77,5.03,9.96" target="#tab_2">2</ref> lists the indexes created by the Cheshire II system for the GeoCLEF database and the document elements from which the contents of those indexes were extracted. The "Used" column in Table <ref type="table" coords="4,128.74,637.77,5.03,9.96" target="#tab_2">2</ref> indicates whether or not a particular index was used in the submitted GeoCLEF runs. The geotext, geopoint, and geobox indexes were not created on the Cheshire2 for the Spanish and Portuguese sub-collections due to the lack of a suitable gazetteer in those languages.</p><p>Because there was no explicit tagging of location-related terms in the collections used for GeoCLEF, we applied the above approach to the "TEXT", "LD", and "TX" elements of the records of the various collections. The part of news articles normally called the "dateline" indicating the location of the news story was not separately tagged in any of the GeoCLEF collection, but often appeared as the first part of the text for the story. For all indexing we used language-specific stoplists to exclude function words and very common words from the indexing and searching. The German language runs used decompounding in the indexing and querying processes to generate simple word forms from compounds.</p><p>The Snowball stemmer was used by both systems for language-specific stemming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Search Processing</head><p>All of the runs for Monolingual English and German, and the runs for Bilingual English⇒German used the standalone retrieval programs developed by Aitao Chen. The Monolingual Spanish and Portuguese, and the Bilingual English⇒Spanish and English⇒Portuguese runs all used the Cheshire II system. The English and German Monolingual runs used language-specific decompounding of German compound words. The Bilingual English⇒German also used decompounding.</p><p>Searching the GeoCLEF collection using the Cheshire II system involved using TCL scripts to parse the topics and submit the title and description or the title, description, and narrative from the topics. For monolingual search tasks we used the topics in the appropriate language (Spanish and Portuguese), for bilingual tasks the topics were translated from the source language to the target language using the L&amp;H PC-based machine translation system. In all cases the various topic elements were combined into a single probabilistic query.</p><p>We tried two main approaches for searching, the first used only the topic text from the title and desc elements (TD), the second included the narrative elements as well (TDN). In all cases only the full-text "topic" index was used for Cheshire II searching.</p><p>Two of our English Monolingual runs used manual modification for topics 27, 43, and 50 by adding manually selected place names to the topics, in addition, one of these (which turned out to be our best performing English Monolingual run) also manually eliminated country names from topic 50.</p><p>Also after two initial runs for Portuguese Monolingual were submitted (BKGeoP1 aand BKGeoP2), a revised and corrected version of the topics was released, and two additional runs (BKGeoP3 and BKGeoP4) were submitted using the revised topics, retaining the original submissions for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results for Submitted Runs</head><p>The summary results (as Mean Average Precision) for the submitted bilingual and monolingual runs for both English and German are shown in Table <ref type="table" coords="5,339.07,728.13,3.90,9.96" target="#tab_4">3</ref>, the Recall-Precision curves for these runs are also shown in Figures <ref type="figure" coords="5,227.05,740.13,5.03,9.96" target="#fig_0">1</ref> and<ref type="figure" coords="5,255.11,740.13,5.03,9.96">2</ref> (for monolingual) and 3 and 4 (for bilingual). In Figures  1-4 the names for the individual runs represent the language code and type of run, which can be compared with full names and descriptions in Table <ref type="table" coords="6,319.17,525.21,3.90,9.96" target="#tab_4">3</ref>.</p><p>Table <ref type="table" coords="6,132.34,537.21,5.03,9.96" target="#tab_4">3</ref> indicates runs that had the highest overall MAP for the task by asterisks next to the run name. Single asterisks indicate the the highest MAP values among our own runs, while double asterisks indicate the runs where the MAP is the maximum recorded among official submissions.</p><p>As can be seen from the table, Berkeley's cross-language submissions using titles, descriptions, and narratives from the topics were the best performing runs for the Bilingual tasks overall. Our Monolingual submissions, on the other hand did not fare as well, but still all ranked within the top quartile of results for each language except Portuguese where we fell below the mean. This result was surprising, given the good performance for Spanish. We now suspect that errors in mapping the topic encoding to the stored document encoding, or possibly problems with the Snowball stemmer for Portuguese may be responsible for this relatively poor performance. Last year's GeoCLEF results (see <ref type="bibr" coords="6,253.65,656.73,10.70,9.96">[7]</ref>) also reported on runs using different systems (as Berke-ley1 and Berkeley2), but both systems did all or most of the tasks. Table <ref type="table" coords="6,408.32,668.61,5.03,9.96" target="#tab_5">4</ref> shows a comparison of Average precision (MAP) for the best performing German and English runs for this year and for the two systems from last year. The German language performance of the system this year for both Bilingual and Monolingual tasks shows a definite improvement, while the English Monolingual performance is somewhat worse that either system last year. The "Berk2" system is essentially the same system as used this year for English and German runs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Manual expansion of selected topics shows a clear, if small, improvement in performance over fully automatic methods. In comparing to Berkeley's best performing English and German runs for last year, it would appear that either the English queries this year were much more difficult, or that there were problems in the English runs. This year, while we did not use automatic expansion of toponyms in the topic texts, this was done explicitly in some of the topic narratives which may explain the improvements in runs using the narratives. It is also apparent that this kind of explicit toponym inclusion in queries, as might be expected, leads to better performance when compared to using titles and descriptions alone in retrieval.</p><p>Although we did not do any explicit geographic processing for this year, we plan to do so in the future. The challenge for next year is to be able to obtain the kind of effectiveness improvement seen with manual query expansion, in automatic queries using geographic processing.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,140.28,283.89,322.18,9.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Berkeley Monolingual Runs -English (left) and German (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,132.96,283.89,336.80,9.96"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Berkeley Monolingual Runs -Spanish (left) and Portuguese (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,173.28,293.73,256.46,9.96"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Berkeley Bilingual Runs -English to Portuguese</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,90.00,725.13,365.86,30.06"><head></head><label></label><figDesc>tf i is the within-document frequency of the ith matching term, ctf i is the occurrence frequency in a collection of the ith matching term,</figDesc><table coords="3,90.00,150.09,385.61,48.91"><row><cell>ql is query length (i.e., number of terms in a query like |Q| for non-feedback situations),</cell></row><row><cell>cl is component length (i.e., number of terms in a component), and</cell></row><row><cell>N t is collection length (i.e., number of terms in a test collection).</cell></row></table><note coords="2,112.44,725.13,343.42,9.96;2,90.00,745.38,16.07,9.82;2,111.60,744.93,242.21,9.96"><p>is the number of matching terms between a document component and a query, qtf i is the within-query frequency of the ith matching term,</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,105.00,581.85,301.48,41.88"><head>Table 2 :</head><label>2</label><figDesc>Cheshire II Indexes for GeoCLEF 2006</figDesc><table coords="4,105.00,613.77,24.10,9.96"><row><cell>Table</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,96.00,358.41,432.27,92.00"><head>Table 3 :</head><label>3</label><figDesc>Submitted GeoCLEF Runs</figDesc><table coords="8,344.76,390.84,183.51,8.57"><row><cell>Berk1</cell><cell>Berk2</cell><cell>Pct. Diff Pct. Diff</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,90.00,462.93,422.86,151.32"><head>Table 4 :</head><label>4</label><figDesc>Comparison of Berkeley's best 2005 and 2006 runs for English and German [6] Ray R. Larson. Probabilistic retrieval, component fusion and blind feedback for xml retrieval. In INEX 2005, pages 225-239. Springer (Lecture Notes in Computer Science, LNCS 3977), 2006. [7] Ray R. Larson, Fredric C. Gey, and Vivien Petras. Berkeley at GeoCLEF: Logistic regression and fusion for geographic information retrieval. In Cross-Language Evaluation Forum: CLEF 2005, pages 963-976. Springer (Lecture Notes in Computer Science LNCS 4022), 2006. [8] S. E. Robertson and K. Sparck Jones. Relevance weighting of search terms. Journal of the American Society for Information Science, pages 129-146, May-June 1976.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,105.47,521.73,407.33,9.96;7,105.48,533.73,407.16,9.96;7,105.48,545.61,406.97,9.96;7,105.48,557.61,407.13,9.96;7,105.48,569.61,141.70,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,166.38,521.73,297.92,9.96">Multilingual information retrieval using english and chinese queries</title>
		<author>
			<persName coords=""><forename type="first">Aitao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,421.06,533.73,91.58,9.96;7,105.48,545.61,406.97,9.96;7,105.48,557.61,83.04,9.96">Evaluation of Cross-Language Information Retrieval Systems: Second Workshop of the Cross-Language Evaluation Forum, CLEF-2001</title>
		<title level="s" coord="7,429.03,557.61,83.58,9.96;7,105.48,569.61,89.57,9.96">Springer Computer Scinece Series LNCS</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Martin</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Kluck</surname></persName>
		</editor>
		<meeting><address><addrLine>Darmstadt, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-09">September 2001. 2002</date>
			<biblScope unit="volume">2406</biblScope>
			<biblScope unit="page" from="44" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.47,589.53,407.24,9.96;7,105.48,601.41,94.70,9.96" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="7,166.86,589.53,213.38,9.96">Cross-Language Retrieval Experiments at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Aitao</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002. 2003</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">2785</biblScope>
			<biblScope unit="page" from="28" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.47,621.33,407.32,9.96;7,105.48,633.33,350.28,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,248.63,621.33,264.16,9.96;7,105.48,633.33,168.89,9.96">Multilingual information retrieval using machine translation, relevance feedback and decompounding</title>
		<author>
			<persName coords=""><forename type="first">Aitao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fredric</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,283.89,633.33,92.84,9.96">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="149" to="182" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.47,653.25,407.32,9.96;7,105.48,665.25,407.25,9.96;7,105.48,677.13,53.80,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,283.19,653.25,229.60,9.96;7,105.48,665.25,195.51,9.96">Full Text Retrieval based on Probabilistic Equations with Coefficients fitted by Logistic Regression</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">S</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,320.70,665.25,159.76,9.96">Text REtrieval Conference (TREC-2)</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.47,697.05,407.20,9.96;7,105.48,709.05,407.11,9.96;7,105.48,720.93,407.43,9.96;7,105.48,732.93,101.53,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,374.79,697.05,137.88,9.96;7,105.48,709.05,106.48,9.96">Probabilistic retrieval based on staged logistic regression</title>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">S</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fredric</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><forename type="middle">P</forename><surname>Dabney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,232.79,709.05,279.79,9.96;7,105.48,720.93,180.21,9.96">15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>Copenhagen, Denmark; New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1992">June 21-24. 1992</date>
			<biblScope unit="page" from="198" to="210" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
