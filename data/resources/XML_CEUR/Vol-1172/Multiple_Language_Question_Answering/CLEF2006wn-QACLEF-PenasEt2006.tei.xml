<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,149.76,74.45,295.78,12.58">Overview of the Answer Validation Exercise 2006</title>
				<funder ref="#_zhTTFRn">
					<orgName type="full">Spanish Ministry of Science and Technology</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,168.42,100.86,60.27,9.02"><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
							<email>anselmo@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="department">Sistemas Informáticos</orgName>
								<orgName type="institution">UNED</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,235.55,100.86,61.35,9.02"><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
							<email>alvarory@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="department">Sistemas Informáticos</orgName>
								<orgName type="institution">UNED</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,303.86,100.86,56.76,9.02"><forename type="first">Valentín</forename><surname>Sama</surname></persName>
							<email>vsama@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="department">Sistemas Informáticos</orgName>
								<orgName type="institution">UNED</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,368.09,100.86,58.82,9.02"><forename type="first">Felisa</forename><surname>Verdejo</surname></persName>
							<email>felisa@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="department">Sistemas Informáticos</orgName>
								<orgName type="institution">UNED</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,198.36,123.84,66.40,9.02"><forename type="first">Dpto</forename><surname>Lenguajes</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sistemas Informáticos</orgName>
								<orgName type="institution">UNED</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,149.76,74.45,295.78,12.58">Overview of the Answer Validation Exercise 2006</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E6B8AE21CED340FBD16864D9C9DF53B6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Question Answering</term>
					<term>Evaluation</term>
					<term>Textual Entailment</term>
					<term>Answer Validation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The first Answer Validation Exercise (AVE) has been launched at the Cross Language Evaluation Forum 2006. This task is aimed at developing systems able to decide whether the answer of a Question Answering system is correct or not. The exercise is described here together with the evaluation methodology and the systems results. The starting point for the AVE 2006 was the reformulation of the Answer Validation as a Recognizing Textual Entailment problem, under the assumption that hypothesis can be automatically generated instantiating hypothesis patterns with the QA systems' answers. 11 groups have participated with 38 runs in 7 different languages. Systems that reported the use of logic have obtained the best results in their respective subtasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The first Answer Validation Exercise <ref type="bibr" coords="1,225.74,383.40,45.72,9.02">(AVE 2006</ref>) was activated to promote the development and evaluation of subsystems aimed at validating the correctness of the answers given by QA systems. This automatic Answer Validation is expected to be useful for improving QA systems performance, help humans in the assessment of QA systems output, improve systems confidence self-score, and to develop better criteria for collaborative systems.</p><p>Systems must emulate human assessment of QA responses and decide whether an answer is correct or not according to a given snippet. The first AVE has been reformulated as Textual Entailment problem <ref type="bibr" coords="1,500.98,452.40,11.68,9.02" target="#b0">[1]</ref> <ref type="bibr" coords="1,512.66,452.40,11.68,9.02" target="#b1">[2]</ref> where the hypotheses have been built semi-automatically turning the questions plus the answers into an affirmative form.</p><p>Participant systems received a set of pairs text-hypothesis built from the QA main track responses of the CLEF 2006, following the methodology described in <ref type="bibr" coords="1,298.76,498.42,10.66,9.02" target="#b5">[6]</ref>. Development collections were built from the QA assessments of last campaigns <ref type="bibr" coords="1,197.34,509.88,11.69,9.02" target="#b2">[3]</ref>[4] <ref type="bibr" coords="1,220.72,509.88,11.69,9.02" target="#b4">[5]</ref> <ref type="bibr" coords="1,232.41,509.88,11.69,9.02" target="#b6">[7]</ref> in English and Spanish. A subtask per language has been activated: English, Spanish, French, German, Dutch, Italian, Portuguese and Bulgarian.</p><p>Participant systems must return a value YES or NO for each pair text-hypothesis to indicate if the text entails the hypothesis or not (i.e. the answer is correct according to the text). Systems results are evaluated against the QA human assessments.</p><p>The training collections together with the 8 testing collections (one per language) resulting from the first AVE 2006 are available at http://nlp.uned.es/QA/ave for researchers registered at CLEF. Section 2 describe the test collections. Section 3 motivates the evaluation measures. Section 4 presents the results in each language and Section 5 present some conclusions and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Test Collections</head><p>As a difference with the previous campaigns of the QA track, a text snippet was requested to support the correctness of the answers. The QA assessments were done considering the given snippet, so the direct relation between QA assessments and RTE judges was preserved: Pairs corresponding to answers judged as Correct have an entailment value equal to YES; pairs corresponding to answers judged as Wrong or Unsupported have an entailment value equal to NO; and pairs corresponding to answers judged as Inexact have an entailment value equal to UNKNOWN and are ignored for evaluation purposes. Pairs coming from answers not evaluated at the QA Track are also tagged as UNKNOWN and they are also ignored in the evaluation.</p><p>Figure <ref type="figure" coords="2,136.02,73.26,5.01,9.02" target="#fig_0">1</ref> resumes the process followed in each language to build the test collection. Starting with the 200 questions, a hypothesis pattern was created for each one, and instantiated with all the answers of all systems for the corresponding question. The pairs were completed with the text snippet given by the system for supporting the answer.  Table <ref type="table" coords="2,96.38,357.00,5.01,9.02" target="#tab_1">1</ref> shows the number of pairs for each language obtained as the result of the processing. This pairs conform the test collections for each language and a benchmark for future evaluations. Percentages of YES, NO and UNKNOWN pairs are similar in all languages except for the percentage of UNKNOWN pairs in English and Portuguese, in which up to 5 runs weren't finally assessed in the QA task and therefore, the corresponding pairs couldn't be used to evaluate the systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Evaluation of the Answer Validation Exercise</head><p>The evaluation is based on the detection of the correct answers and only them. There are two reasons for this.</p><p>First, an answer will be validated if there is enough evidence to affirm its correctness. Figure <ref type="figure" coords="2,473.68,616.26,5.01,9.02" target="#fig_1">2</ref> shows the decision flow that involves an Answer Validation module after searching for candidate answers: In the cases where there is not enough evidence of correctness (according to the AV module), the system must request another candidate answer. Thus, the Answer Validation must focus on detecting that there is enough evidence of the answer correctness. Second, in a real exploitation environment, there is no balance between correct and incorrect candidate answers, that is to say, a system that validates QA responses does not receive correct and incorrect answers in the same proportion. In fact, the experiences at CLEF during the last years showed that only 23% of all the answers given by all the systems were correct (results for the Spanish as target, see <ref type="bibr" coords="2,376.02,708.30,10.52,9.02" target="#b5">[6]</ref>). Although numbers are expected to change, the important thing is that the evaluation of Answer Validation modules must consider the real output of Question Answering systems, which is not balanced. We think this leads to different development strategies closer to the real AV Exercise that, anyway, must be evaluated with this unbalanced nature. Therefore, instead of using an overall accuracy as the evaluation measure, we proposed to use precision (1), recall (2) and a F-measure (3) (harmonic mean) over pairs with entailment value equals to YES. In other words, we proposed to quantify systems ability to detect the pairs with entailment or to detect whether there is enough evidence to accept an answer. If we would had considered the accuracy over all pairs then a baseline AV system that always answers NO (rejects all answers) would obtain an accuracy value of 0.77, which seems too high for evaluation purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Question Answerin</head><p>Answer Validation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Candidate answer</head><p>Answer is correct Answer Answer is not correct or not enough evidence </p><formula xml:id="formula_0" coords="3,234.78,560.51,139.53,26.92">recall precision recall precision F + ⋅ ⋅ = 2<label>(3)</label></formula><p>In the other hand, the higher the proportion of YES pairs is, the higher the baselines are. Thus, results can be compared between systems and always taking as reference the baseline of a system that accept all answers (return YES in 100% of cases). Since UNKNOWN pairs are ignored in the evaluation (though they were present in the test collection), the precision formula (2) was modify to ignore the cases were systems assessed a YES value to the UNKNOWN pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>Eleven groups have participated in seven different languages at this first AVE 2006. Table <ref type="table" coords="3,472.02,720.06,5.01,9.02" target="#tab_2">2</ref> shows the participant groups and the number of runs they submitted per language. Al least two different groups participated for each language, so the comparison between different approaches is possible. English and Spanish were the most popular with 11 and 9 runs respectively. We expect that in a near future the QA systems will take advantage of this communities working in the kind of reasoning needed for the Answer Validation. Tables <ref type="table" coords="4,113.25,449.16,4.45,9.02" target="#tab_3">3</ref><ref type="table" coords="4,117.71,449.16,4.45,9.02" target="#tab_4">4</ref><ref type="table" coords="4,117.71,449.16,4.45,9.02" target="#tab_5">5</ref><ref type="table" coords="4,117.71,449.16,4.45,9.02" target="#tab_6">6</ref><ref type="table" coords="4,117.71,449.16,4.45,9.02" target="#tab_7">7</ref><ref type="table" coords="4,117.71,449.16,4.45,9.02">8</ref><ref type="table" coords="4,122.16,449.16,4.45,9.02" target="#tab_8">9</ref>show the results for all participant system in each language. Since the number of pairs and the proportion of the YES pairs is different for each language (due to the real submission of the QA systems), results can't be compared between languages. Together with the systems precision, recall and F-measure, two baselines values are shown: the results of a system that always accept all answers (returns YES in 100% of the pairs), and the results of a hypothetical system that returns YES for the 50% of pairs.</p><p>In the languages where at least one system reported the use of Logic (Spanish, English and German) the best performing system was one of them. Although the use of Logic doesn't guarantee a good result, the best systems used it. However, the most extensively used techniques were Machine Learning and overlapping measures between text and hypothesis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and future work</head><p>The starting point for the AVE 2006 was the reformulation of the Answer Validation as a Recognizing Textual Entailment problem, under the assumption that hypothesis can be automatically generated instantiating hypothesis patterns with the QA systems answers. Thus, the collections developed in AVE are specially oriented to the development and evaluation of Answer Validation systems. We have also proposed a methodology for the evaluation in chain with a QA Track.</p><p>11 groups have participated with 38 runs in 7 different languages. Systems that reported the use of logic have obtained the best results in their respective subtasks.</p><p>Future work aims at developing an Answer Validation model where the hypotheses can include the type of answer requested by the question in order to reformulate the Answer Validation Exercise for the next campaign. Finally, we want to quantify the gain in performance that the Answer Validation systems give in chain with the Question Answering ones.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,203.52,299.46,188.24,9.02;2,159.72,310.98,275.89,9.02"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Text-hypothesis pairs for the Answer Validation Exercise from the pool of answers of the main QA Track.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,202.14,406.74,202.99,9.02"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Decision flow for the Answer Validation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,78.00,402.96,437.26,98.38"><head>Table 1 .</head><label>1</label><figDesc>YES, NO and UNKNOWN pairs in the testing collections of AVE 2006</figDesc><table coords="2,78.00,432.03,437.26,69.30"><row><cell></cell><cell>German</cell><cell>English</cell><cell>Spanish</cell><cell>French</cell><cell>Italian</cell><cell>Dutch</cell><cell>Portuguese</cell></row><row><cell>YES pairs</cell><cell>344(24%)</cell><cell>198(9.5%)</cell><cell>671(28%)</cell><cell>705(22%)</cell><cell>187(16%)</cell><cell>81(10%)</cell><cell>188(14%)</cell></row><row><cell>NO pairs</cell><cell>1064(74%)</cell><cell>1048(50%)</cell><cell>1615(68%)</cell><cell>2359(72%)</cell><cell>901(79%)</cell><cell>696(86%)</cell><cell>604(46%)</cell></row><row><cell>UNKNO</cell><cell>35(3%)</cell><cell>842(40.5%)</cell><cell>83(4%)</cell><cell>202(6%)</cell><cell>52(5%)</cell><cell>30(4%)</cell><cell>532(40%)</cell></row><row><cell>WN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Total</cell><cell>1443</cell><cell>2088</cell><cell>2369</cell><cell>3266</cell><cell>1140</cell><cell>807</cell><cell>1324</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,70.92,113.46,453.49,310.22"><head>Table 2 .</head><label>2</label><figDesc>Participants and runs per language in AVE 2006</figDesc><table coords="4,129.30,142.50,335.54,211.81"><row><cell></cell><cell>German</cell><cell>English</cell><cell>Spanish</cell><cell>French</cell><cell>Italian</cell><cell>Dutch</cell><cell>Portuguese</cell><cell>Total</cell></row><row><cell>Fernuniversität in Hagen</cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell></row><row><cell>Language Computer Corporation</cell><cell></cell><cell>1</cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell></row><row><cell>U. Rome "Tor Vergata"</cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell></row><row><cell>U. Alicante (Kozareva)</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>1</cell><cell>13</cell></row><row><cell>U. Politecnica de Valencia</cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell></row><row><cell>U. Alicante (Ferrández)</cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell></row><row><cell>LIMSI-CNRS</cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell>1</cell></row><row><cell>U. Twente</cell><cell>1</cell><cell>2</cell><cell>2</cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>1</cell><cell>10</cell></row><row><cell>UNED (Herrera)</cell><cell></cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell></row><row><cell>UNED (Rodrigo)</cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell></row><row><cell>ITC-irst</cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell></row><row><cell>R2D2 project</cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell></row><row><cell cols="2">Total 5</cell><cell>11</cell><cell>9</cell><cell>4</cell><cell>3</cell><cell>4</cell><cell>2</cell><cell>38</cell></row></table><note coords="4,70.92,391.68,453.49,9.02;4,70.92,403.14,453.46,9.02;4,70.92,414.66,323.32,9.02"><p>Only 3 of the 12 groups (FUH, LCC and ITC-IRST) have participated in the Question Answering Track showing the chance for new-comers to start developing a single QA module and, at the same time, open a place for experienced groups in RTE and KR to apply their research to the QA problem.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,87.36,564.18,418.53,186.56"><head>Table 3 .</head><label>3</label><figDesc>AVE 2006 Results for English</figDesc><table coords="4,87.36,585.84,418.53,164.90"><row><cell>System Id</cell><cell>Group</cell><cell cols="2">F-measure Precision</cell><cell>Recall</cell><cell>Techniques</cell></row><row><cell>COGEX</cell><cell>LCC</cell><cell>0.4559</cell><cell>0.3261</cell><cell>0.7576</cell><cell>Logic</cell></row><row><cell cols="2">ZNZ -TV_2 U. Rome</cell><cell>0.4106</cell><cell>0.2838</cell><cell>0.7424</cell><cell>ML</cell></row><row><cell>itc-irst</cell><cell>ITC-irst</cell><cell>0.3919</cell><cell>0.3090</cell><cell>0.5354</cell><cell>Lexical, Syntax, Corpus, ML</cell></row><row><cell cols="2">ZNZ -TV_1 U. Rome</cell><cell>0.3780</cell><cell>0.2707</cell><cell>0.6263</cell><cell>ML</cell></row><row><cell>MLEnt_2</cell><cell>U. Alicante</cell><cell>0.3720</cell><cell>0.2487</cell><cell>0.7374</cell><cell>Overlap, Corpus, ML</cell></row><row><cell>uaofe_2</cell><cell>U. Alicante</cell><cell>0.3177</cell><cell>0.2040</cell><cell>0.7172</cell><cell>Lexical, Syntax, Logic</cell></row><row><cell>MLEnt_1</cell><cell>U. Alicante</cell><cell>0.3174</cell><cell>0.2114</cell><cell>0.6364</cell><cell>Overlap, Logic, ML</cell></row><row><cell>uaofe_1</cell><cell>U. Alicante</cell><cell>0.3070</cell><cell>0.2144</cell><cell>0.5404</cell><cell>Lexical, Syntax, Logic</cell></row><row><cell>utwente.ta</cell><cell>U. Twente</cell><cell>0.3022</cell><cell>0.3313</cell><cell>0.2778</cell><cell>Syntax, ML</cell></row><row><cell>utwente.lcs</cell><cell>U. Twente</cell><cell>0.2759</cell><cell>0.2692</cell><cell>0.2828</cell><cell>Overlap, Paraphrase</cell></row><row><cell cols="2">100% YES Baseline</cell><cell>0.2742</cell><cell>0.1589</cell><cell>1</cell><cell></cell></row><row><cell cols="2">50% YES Baseline</cell><cell>0.2412</cell><cell>0.1589</cell><cell>0.5</cell><cell></cell></row><row><cell>ebisbal</cell><cell>U.P. Valencia</cell><cell>0.075</cell><cell>0.2143</cell><cell>0.0455</cell><cell>ML</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,96.36,84.72,400.97,102.62"><head>Table 4 .</head><label>4</label><figDesc>AVE 2006 Results for French</figDesc><table coords="5,96.36,106.43,400.97,80.90"><row><cell>System Id</cell><cell>Group</cell><cell cols="3">F-measure Precision Recall</cell><cell>Techniques</cell></row><row><cell>MLEnt_2</cell><cell>U. Alicante</cell><cell>0.4693</cell><cell>0.3444</cell><cell>0.7362</cell><cell>Overlap, ML</cell></row><row><cell>MLEnt_1</cell><cell>U. Alicante</cell><cell>0.4085</cell><cell>0.3836</cell><cell>0.4369</cell><cell>Overlap, Corpus, ML</cell></row><row><cell cols="2">100% YES Baseline</cell><cell>0.3741</cell><cell>0.2301</cell><cell>1</cell><cell></cell></row><row><cell cols="2">50% YES Baseline</cell><cell>0.3152</cell><cell>0.2301</cell><cell>0.5</cell><cell></cell></row><row><cell>LIRAVE</cell><cell>LIMSI-CNRS</cell><cell>0.1112</cell><cell>0.4327</cell><cell cols="2">0.0638 Lexical, Syntax, Paraphrase</cell></row><row><cell cols="2">utwente.lcs U. Twente</cell><cell>0.0943</cell><cell>0.4625</cell><cell>0.0525</cell><cell>Overlap</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="5,108.36,201.84,376.22,158.30"><head>Table 5 .</head><label>5</label><figDesc>AVE 2006  Results for Spanish</figDesc><table coords="5,108.36,219.30,376.22,140.84"><row><cell cols="2">System Id Group</cell><cell cols="2">F-measure Precision</cell><cell>Recall</cell><cell>Techniques</cell></row><row><cell>COGEX</cell><cell>LCC</cell><cell>0.6063</cell><cell>0.527</cell><cell>0.7139</cell><cell>Logic</cell></row><row><cell>UNED_1</cell><cell>UNED</cell><cell>0.5655</cell><cell>0.467</cell><cell>0.7168</cell><cell>Overlap, ML</cell></row><row><cell>UNED_2</cell><cell>UNED</cell><cell>0.5615</cell><cell>0.4652</cell><cell>0.7079</cell><cell>Overlap, ML</cell></row><row><cell>NED</cell><cell>UNED</cell><cell>0.5315</cell><cell>0.4364</cell><cell>0.6796</cell><cell>NE recognition</cell></row><row><cell>MLEnt_2</cell><cell>U. Alicante</cell><cell>0.5301</cell><cell>0.4065</cell><cell>0.7615</cell><cell>Overlap, ML</cell></row><row><cell>R2D2</cell><cell>R2D2 Project</cell><cell>0.4938</cell><cell>0.4387</cell><cell>0.5648</cell><cell>Voting, Overlap, ML</cell></row><row><cell cols="2">utwente.ta U. Twente</cell><cell>0.4682</cell><cell>0.4811</cell><cell>0.4560</cell><cell>Syntax, ML</cell></row><row><cell cols="2">100% YES Baseline</cell><cell>0.4538</cell><cell>0.2935</cell><cell>1</cell><cell></cell></row><row><cell cols="2">utwente.lcs U. Twente</cell><cell>0.4326</cell><cell>0.5507</cell><cell>0.3562</cell><cell>Overlap, Paraphrase</cell></row><row><cell>MLEnt_1</cell><cell>U. Alicante</cell><cell>0.4303</cell><cell>0.4748</cell><cell>0.3934</cell><cell>Overlap, Corpus, ML</cell></row><row><cell cols="2">50% YES Baseline</cell><cell>0.3699</cell><cell>0.2935</cell><cell>0.5</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="5,96.36,374.64,397.99,137.60"><head>Table 6 .</head><label>6</label><figDesc>AVE 2006 Results for German</figDesc><table coords="5,96.36,396.36,397.99,115.88"><row><cell>System Id</cell><cell>Group</cell><cell cols="3">F measure Precision Recall</cell><cell>Techniques</cell></row><row><cell>FUH_1</cell><cell>Fernuniversität</cell><cell>0.5420</cell><cell>0.5839</cell><cell>0.5058</cell><cell>Lexical, Syntax, Semantics,</cell></row><row><cell></cell><cell>in Hagen</cell><cell></cell><cell></cell><cell></cell><cell>Logic, Corpus</cell></row><row><cell>FUH_2</cell><cell>Fernuniversität</cell><cell>0.5029</cell><cell>0.7293</cell><cell>0.3837</cell><cell>Lexical, Syntax, Semantics,</cell></row><row><cell></cell><cell>in Hagen</cell><cell></cell><cell></cell><cell></cell><cell>Logic, Corpus, Paraphrase</cell></row><row><cell>MLEnt_2</cell><cell>U. Alicante</cell><cell>0.4685</cell><cell>0.3573</cell><cell>0.6802</cell><cell>Overlap, ML</cell></row><row><cell cols="2">100% YES Baseline</cell><cell>0.3927</cell><cell>0.2443</cell><cell>1</cell><cell></cell></row><row><cell>MLEnt_1</cell><cell>U. Alicante</cell><cell>0.3874</cell><cell>0.4006</cell><cell>0.375</cell><cell>Overlap, Corpus, ML</cell></row><row><cell cols="2">50% YES Baseline</cell><cell>0.3282</cell><cell>0.2443</cell><cell>0.5</cell><cell></cell></row><row><cell cols="2">utwente.lcs U. Twente</cell><cell>0.1432</cell><cell>0.4</cell><cell>0.0872</cell><cell>Overlap</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="5,96.36,526.74,388.22,195.68"><head>Table 7 .</head><label>7</label><figDesc>AVE 2006 Results for Dutch</figDesc><table coords="5,96.36,548.46,388.22,173.96"><row><cell>System Id</cell><cell cols="2">Group</cell><cell cols="4">F measure Precision Recall</cell><cell>Techniques</cell></row><row><cell>utwente.ta</cell><cell cols="2">U. Twente</cell><cell>0.3871</cell><cell cols="2">0.2874</cell><cell>0.5926</cell><cell>Syntax, ML</cell></row><row><cell>MLEnt_1</cell><cell cols="2">U. Alicante</cell><cell>0.2957</cell><cell>0.189</cell><cell></cell><cell>0.6790</cell><cell>Overlap, Corpus, ML</cell></row><row><cell>MLEnt_2</cell><cell cols="2">U. Alicante</cell><cell>0.2548</cell><cell cols="2">0.1484</cell><cell>0.9012</cell><cell>Overlap, ML</cell></row><row><cell>utwente.lcs</cell><cell cols="2">U. Twente</cell><cell>0.2201</cell><cell>0.2</cell><cell></cell><cell>0.2469</cell><cell>Overlap, Paraphrase</cell></row><row><cell cols="2">100% YES Baseline</cell><cell></cell><cell>0.1887</cell><cell>0.1042</cell><cell></cell><cell>1</cell></row><row><cell cols="2">50% YES Baseline</cell><cell></cell><cell>0.1725</cell><cell>0.1042</cell><cell></cell><cell>0.5</cell></row><row><cell></cell><cell></cell><cell cols="5">Table 8. AVE 2006 Results for Portuguese</cell></row><row><cell cols="2">System Id</cell><cell>Group</cell><cell cols="4">F measure Precision Recall</cell><cell>Techniques</cell></row><row><cell cols="3">100% YES Baseline</cell><cell cols="2">0.3837</cell><cell cols="2">0.2374</cell><cell>1</cell></row><row><cell cols="2">utwente.lcs</cell><cell cols="2">U. Twente</cell><cell>0.3542</cell><cell></cell><cell>0.5783</cell><cell>0.2553</cell><cell>Overlap</cell></row><row><cell cols="3">50% YES Baseline</cell><cell cols="2">0.3219</cell><cell cols="2">0.2374</cell><cell>0.5</cell></row><row><cell>MLEnt</cell><cell></cell><cell cols="2">U. Alicante</cell><cell>0.1529</cell><cell></cell><cell>0.1904</cell><cell>0.1277</cell><cell>Corpus</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="6,96.36,73.26,388.22,90.56"><head>Table 9 .</head><label>9</label><figDesc>AVE 2006 Results for Italian</figDesc><table coords="6,96.36,94.92,388.22,68.90"><row><cell>System Id</cell><cell>Group</cell><cell cols="3">F measure Precision Recall</cell><cell>Techniques</cell></row><row><cell>MLEnt_2</cell><cell>U. Alicante</cell><cell>0.4066</cell><cell>0.2830</cell><cell>0.7219</cell><cell>Overlap, ML</cell></row><row><cell>MLEnt_1</cell><cell>U. Alicante</cell><cell>0.3480</cell><cell>0.2164</cell><cell>0.8877</cell><cell>Overlap, Corpus, ML</cell></row><row><cell cols="2">100% YES Baseline</cell><cell>0.2934</cell><cell>0.1719</cell><cell>1</cell><cell></cell></row><row><cell cols="2">50% YES Baseline</cell><cell>0.2558</cell><cell>0.1719</cell><cell>0.5</cell><cell></cell></row><row><cell>utwente.lcs</cell><cell>U. Twente</cell><cell>0.1673</cell><cell>0.3281</cell><cell>0.1123</cell><cell>Overlap</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work has been partially supported by the <rs type="funder">Spanish Ministry of Science and Technology</rs> within the <rs type="projectName">R2D2-SyEMBRA</rs> project (<rs type="grantNumber">TIC-2003-07158-C04-02</rs>). We are grateful to all the people involved in the organization of the QA track (specially to the coordinators at CELCT, <rs type="person">Danilo Giampiccolo</rs> and <rs type="person">Pamela Forner</rs>) and to the people that built the patterns for the hypotheses: <rs type="person">Juan Feu</rs> (Dutch), <rs type="person">Petya Osenova</rs> (<rs type="affiliation">Bulgarian</rs>), <rs type="person">Christelle Ayache</rs> (<rs type="affiliation">French</rs>), <rs type="person">Bodgan Sacaleanu</rs> (German) and <rs type="person">Diana Santos</rs> (Portuguese).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_zhTTFRn">
					<idno type="grant-number">TIC-2003-07158-C04-02</idno>
					<orgName type="project" subtype="full">R2D2-SyEMBRA</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,74.69,485.94,449.76,9.02;6,88.92,497.40,435.52,9.02;6,88.92,508.92,46.36,9.02" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,435.48,485.94,88.96,9.02;6,88.92,497.40,171.34,9.02">The Second PASCAL Recognising Textual Entailment Challenge</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bar-Haim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Szpektor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,278.00,497.40,164.12,9.02">Proceedings of the Challenges Workshop</title>
		<meeting>the Challenges Workshop<address><addrLine>Venice</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-04">April 2006</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.69,520.44,449.75,9.02;6,88.92,531.90,435.46,9.02;6,88.92,543.42,91.88,9.02" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,267.25,520.44,240.00,9.02">The PASCAL Recognising Textual Entailment Challenge</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,88.92,531.90,352.17,9.02">Proceedings of the PASCAL Challenges Workshop on Recognising TextualEntailment</title>
		<meeting>the PASCAL Challenges Workshop on Recognising TextualEntailment<address><addrLine>Southampton, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-04">April 2005</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.68,554.88,449.72,9.02;6,88.92,566.40,435.50,9.02;6,88.92,577.92,124.55,9.02" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,254.94,554.88,197.01,9.02">Question Answering Pilot Task at CLEF 2004</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,474.26,554.88,50.15,9.02;6,88.92,566.40,196.31,9.02">Multilingual Information Access for Text, Speech and Images</title>
		<title level="s" coord="6,416.26,566.40,108.15,9.02;6,88.92,577.92,29.51,9.02">LectureNotes in Computer Science</title>
		<meeting><address><addrLine>CLEF</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004. 2005</date>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page" from="581" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.69,589.38,449.70,9.02;6,88.92,600.90,435.51,9.02;6,88.92,612.42,435.46,9.02;6,88.92,623.88,301.52,9.02" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,508.78,589.38,15.61,9.02;6,88.92,600.90,250.73,9.02">The Multiple Language Question Answering Track at CLEF 2003</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Romagnoli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vallin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Peinado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,171.96,612.42,288.81,9.02">Comparative Evaluation of Multilingual Information Access Systems</title>
		<title level="s" coord="6,155.04,623.88,140.39,9.02">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<meeting><address><addrLine>CLEF</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003. 2004</date>
			<biblScope unit="volume">3237</biblScope>
			<biblScope unit="page" from="471" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.69,635.40,449.73,9.02;6,88.92,646.92,435.51,9.02;6,88.92,658.38,435.57,9.02;6,88.92,669.90,390.32,9.02" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,88.92,646.92,281.79,9.02">Overview of the CLEF 2004 Multilingual Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vallin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Erbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sutcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,302.52,658.38,221.97,9.02;6,88.92,669.90,27.04,9.02">Multilingual Information Access for Text, Speech and Images</title>
		<title level="s" coord="6,243.84,669.90,140.39,9.02">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<meeting><address><addrLine>CLEF</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003. 2004</date>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page" from="371" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.69,681.42,449.80,9.02;6,88.92,692.88,424.89,9.02" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,288.36,681.42,236.13,9.02;6,88.92,692.88,40.36,9.02">SPARTE, a Test Suite for Recognising Textual Entailment in Spanish</title>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felisa</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,305.83,692.88,46.32,9.02">CiCLing&apos;06</title>
		<title level="s" coord="6,136.08,692.88,142.08,9.02">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3878</biblScope>
			<biblScope unit="page" from="275" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.68,704.40,449.75,9.02;6,88.92,715.92,435.49,9.02;6,88.92,727.38,177.21,9.02" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,258.59,715.92,265.82,9.02;6,88.92,727.38,21.61,9.02">Overview of the CLEF 2005 Multilingual Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vallin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Aunimo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Osenova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sacaleanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sutcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,128.28,727.38,108.15,9.02">Proceedings of CLEF 2005</title>
		<meeting>CLEF 2005</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
