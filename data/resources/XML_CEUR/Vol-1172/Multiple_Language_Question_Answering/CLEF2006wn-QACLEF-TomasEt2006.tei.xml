<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,100.04,148.86,402.93,15.15;1,204.88,170.78,193.24,15.15">Experiments with LSA for Passage Re-Ranking in Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,151.36,204.67,57.75,8.74"><forename type="first">David</forename><surname>Tomás</surname></persName>
							<email>dtomas@dlsi.ua.es</email>
							<affiliation key="aff0">
								<orgName type="department">Departamento de Lenguajes y Sistemas Informáticos</orgName>
								<orgName type="institution">Universidad de Alicante</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,227.76,204.67,63.70,8.74"><forename type="first">José</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
							<email>vicedo@dlsi.ua.es</email>
							<affiliation key="aff0">
								<orgName type="department">Departamento de Lenguajes y Sistemas Informáticos</orgName>
								<orgName type="institution">Universidad de Alicante</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,310.11,204.67,59.86,8.74"><forename type="first">Empar</forename><surname>Bisbal</surname></persName>
							<email>ebisbal@dsic.upv.es</email>
							<affiliation key="aff1">
								<orgName type="department">Departamento de Sistemas Informáticos</orgName>
								<orgName type="institution">Computación</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Universidad Politécnica de Valencia</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,388.61,204.67,58.56,8.74"><forename type="first">Lidia</forename><surname>Moreno</surname></persName>
							<email>lmoreno@dsic.upv.es</email>
							<affiliation key="aff1">
								<orgName type="department">Departamento de Sistemas Informáticos</orgName>
								<orgName type="institution">Computación</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Universidad Politécnica de Valencia</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,100.04,148.86,402.93,15.15;1,204.88,170.78,193.24,15.15">Experiments with LSA for Passage Re-Ranking in Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">93C03178F5BCDEB5EAC0F7C5604885EA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.2 [Information Storage and Retrieval]: Information Storage-File organization</term>
					<term>H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval</term>
					<term>H.5.2 [Information Interfaces and Presentation]: User Interfaces-Natural Language</term>
					<term>I.2.7 [Artificial Intelligence]: Natural Language Processing-Language parsing and understanding Experimentation, Performance, Design Question answering, Information Retrieval, Multilingual, Machine Learning, LSA</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As in the previous QA@CLEF track, two separate groups at the University of Alicante participated this year using different approaches. This paper describes the work of Alicante 1 group. We have continued with the research line established in the past competition, where the main goal was to obtain a fully data-driven system based on machine learning techniques. Last year an XML framework was established in order to obtain a modular system where each component could be easily replaced or upgraded. In this framework, a question classification system based on Support Vector Machines (SVM) and surface text features was included, achieving remarkable performance in this stage. The main novelties introduced this year are focused on the information retrieval stage. First, we employed Indri as our search engine for passage retrieval. Secondly, we developed a module for passage re-ranking based on Latent Semantic Analysis (LSA). This technique provides a method for determining the similarity of meaning between words by analysis of large text corpora. In our experiments, every question was compared with every passage returned by the search engine by means of LSA in order to re-rank them. Looking at the results, this technique increased the retrieval accuracy for definition questions but it decreased accuracy on factoid ones. To take advantage of the flexibility and adaptability of our machine learning based proposal, this year we extended our participation to monolingual Spanish task and bilingual Spanish-English task. We reach a best overall accuracy of 29.47% in the first task and 20.00% in the second one.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper is focused on the work of Alicante 1 group at the University of Alicante. We have continued with the research line established in the past competition <ref type="bibr" coords="2,383.89,197.13,9.96,8.74" target="#b0">[1]</ref>, where an XML framework was defined with the aim of easily integrate machine learning based modules on every stage of the Question Answering (QA) process: question analysis, information retrieval and answer extraction. In this framework and inside the question analysis stage, a question classification system based on Support Vector Machines (SVM) and surface text features <ref type="bibr" coords="2,343.07,244.95,10.51,8.74" target="#b1">[2]</ref> was included the last year, achieving remarkable performance at this point.</p><p>The main novelties introduced this year are focused on the information retrieval stage. First, we moved from Xapian<ref type="foot" coords="2,189.62,279.24,3.97,6.12" target="#foot_0">1</ref> to Indri<ref type="foot" coords="2,230.17,279.24,3.97,6.12" target="#foot_1">2</ref> as passage retrieval engine. Secondly, we developed a procedure for passage re-ranking based on Latent Semantic Analysis (LSA) <ref type="bibr" coords="2,380.39,292.77,9.96,8.74" target="#b2">[3]</ref>. This technique provides a method for determining the similarity of meaning between words by analysis of large text corpora. We employed LSA to compare every question formulated to the system with every passage returned by the Indri search engine. With this similarity measure our system can automatically re-rank the passages retrieved.</p><p>One of the main advantages that systems based on machine learning techniques offer is their flexibility and adaptability to new languages and domains. Thus, this year we extended our participation from the original monolingual Spanish task to bilingual Spanish-English task, as we wanted to test the system in a different language that the one it was originally intended for. For every task we submitted two runs, one with the baseline system and another with LSA passage re-ranking.</p><p>This paper is organized as follows: in section 2 we describe the system architecture detailing the novelties included this year; section 3 outlines the runs submitted; section 4 presents and analyzes the results obtained at QA@CLEF 2006 monolingual Spanish and bilingual Spanish-English task, paying special attention at the passage re-ranking module; finally, in section 5 we discuss the conclusions and main challenges for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head><p>The main architecture of the system remains unchanged since the past competition <ref type="bibr" coords="2,458.12,526.87,9.96,8.74" target="#b0">[1]</ref>. We have a modularized structure where each stage can be isolated in an independent component so as to easily integrate and substitute new ones in an XML framework. This way, we want to evolve the system originally described in <ref type="bibr" coords="2,222.72,562.73,10.52,8.74" target="#b3">[4]</ref> and <ref type="bibr" coords="2,255.90,562.73,10.51,8.74" target="#b4">[5]</ref> to a fully data-driven approach, adapting every stage of the system with machine learning techniques.</p><p>The system follows the classical three-stage pipeline architecture mentioned above. Next paragraphs describe each module in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Question Analysis</head><p>This first stage carries out two processes: question classification and query construction. The first one detects what type of information is claimed in the question, mapping it into a previously defined taxonomy. Otherwise, query construction detects meaningful terms from the question that help the search engine to locate the documents or paragraphs that are likely to contain the answer.</p><p>The question classification system is based on Support Vector Machines (SVM). The system was trained with a parallel annotated corpus in Spanish and English made up of question from Question Answering Track in TREC<ref type="foot" coords="3,244.60,110.45,3.97,6.12" target="#foot_2">3</ref> 1999 to 2003 and CLEF 2003 to 2004, to sum up 2739 training questions. Thus, there is no need to manually tune the question classification system from one language to another since all the knowledge necessary to classify the questions is automatically acquired changing the training corpus. Anyway, as Spanish was the only source language in the tasks we took part, only classification in this language was carried out. Despite the corpus initially presented fifteen different categories, we reduced them to five so as to fit the needs of the answer extraction process. There is a more detailed description of this module in <ref type="bibr" coords="3,414.21,183.75,9.96,8.74" target="#b1">[2]</ref>.</p><p>On the other hand, the query construction module remains the same for this year competition <ref type="bibr" coords="3,90.00,207.66,9.97,8.74" target="#b4">[5]</ref>. It employs hand made lexical patters in order to obtain the information needed to build the query for the Indri search engine. We have already developed a fully data-driven keyword detection module <ref type="bibr" coords="3,170.90,231.57,10.52,8.74" target="#b5">[6]</ref> to help on query construction although it has not being included in the global QA system yet. In the bilingual Spanish-English task, we just translated the question into English through SysTran<ref type="foot" coords="3,199.21,253.91,3.97,6.12" target="#foot_3">4</ref> online translation service, and applied adapted patterns to obtain the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Information Retrieval</head><p>The main novelties introduced this year in our system are focused on this stage. First, we moved the search engine from Xapian to Indri due to the computational cost of Xapian indexing module. Indri offers a powerful and flexible query language, but as our keyword extraction and query construction approach are quite simple, we limited the queries to ad hoc retrieval with query likelihood ranking approach. The search engine performs passage retrieval over the entire Spanish or English corpora depending on the task. It returns the 50 topmost relevant passages with a length of 200 words which are provided to the answer extraction module.</p><p>On the other hand, we have developed a re-ranking component for the passages retrieved, trying to weigh them beyond query likelihood criterion. This component is based on LSA. This is a corpus-based statistical method for inducing and representing aspects of the meaning of words and passages reflected in their usage. In LSA a representative sample of text is converted to a term-by-passage matrix in which each cell indicates the frequency with which each term occurs in each passage. After a preliminary information-theoretic weighting of cell entries, the matrix is submitted to Singular Value Decomposition (SVD) and a 100 to 1500 dimensional abstract semantic space is constructed in which each original word and each original passage are represented as vectors. Thus, the similarities derived by LSA are not simple co-occurrence statistics, as the dimension-reduction step constitutes a form of induction that can extract a great deal of added information from mutual constraints among a large number of words occurring in a large number of contexts.</p><p>In order to test this module, Indri first retrieves the best 1000 passages extracted from the corpora. These passages are employed to construct the term-by-passage matrix with Infomap NLP Software<ref type="foot" coords="3,127.39,563.20,3.97,6.12" target="#foot_4">5</ref> reducing the original matrix to 100 dimensions (the default value) by means of SVD. Then, the similarity between the question and every passage is calculated. All the passages are sorted in decreasing order of this value, keeping the 50 best valued passages. This process tries to better rank the passages semantically related to the question, overriding the problem of exact match of keywords from the query.</p><p>Unlike the previous competition, we removed the additional search performed in Google<ref type="foot" coords="3,495.71,622.98,3.97,6.12" target="#foot_5">6</ref> as statistical indicator of answer correctness. We wanted to avoid introducing noise in the retrieval process in order to better evaluate the new search engine and the re-ranking module.</p><p>As Indri performs multilingual search without additional tuning and so does the re-ranking module, no special modification was necessary when working on Spanish or English corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Answer Extraction</head><p>This module remains almost unchanged since the past competition. The only remarkable change is due to Google search removal, as we described in the previous section. We had to modify the frequency value from the original formula <ref type="bibr" coords="4,294.36,154.32,10.51,8.74" target="#b0">[1]</ref> as we do not include the frequency of the web summaries retrieved by Google anymore.</p><p>At this point of the process the system keeps the following information: keywords and definitions terms from the query, and the set of relevant passages retrieved from de corpora. The system employs this information to extract a single answer from the list of relevant passages retrieved by the search engine. The set of possible answers is formed extracting all the n-grams (unigrams, bigrams and trigrams in our experiments) from these relevant passages. First, some heuristics are applied in order to filter non probable answers, like those that contains query terms or do not fit the question type. Next, remaining candidate answers are scored taking into account the sentence where they appear, the frequency of appearance, the distance to the keywords and the size of the answer.</p><p>As all these heuristics and features are language independent, the answer extraction module can be either applied to Spanish or English passages without additional changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Runs Submitted</head><p>As in the past three years, we have participated in the monolingual Spanish task. As a novelty, we also took part in the bilingual Spanish-English task. We were interested in testing the skills of our system retrieving and extracting answers in another language. We applied no special cross-lingual techniques as we just automatically translated the questions from Spanish to English to set a monolingual English environment to test the system.</p><p>We performed two different runs for every task. The difference between each run is established in the information retrieval stage. The first run represents the baseline experiment, where Indri is employed to retrieve the 50 topmost relevant passages related to the query. The second run introduces the re-ranking module described in section 2. Indri retrieves the best 1000 passages, which are then re-ranked via LSA to finally select the 50 best ones. The rest of the system remains the same for both runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>This year we submitted four different runs, two for the monolingual Spanish task and two more for the bilingual Spanish-English task. As a novelty, the QA@CLEF track presents this year the inclusion of list questions, where the systems are required to return not just one answer but a list of possible answers.</p><p>There were a total of 200 questions per run. Table <ref type="table" coords="4,337.48,574.69,4.98,8.74" target="#tab_0">1</ref> shows the overall and detailed results obtained for factoid, definition and temporally restricted factoid questions. This last type is not really representative as there were only two questions of this kind in the monolingual Spanish track, and none in the bilingual Spanish-English track. Experiment aliv061eses refers to the first run (baseline experiment) in the monolingual Spanish task. Experiment aliv062eses refers to the second run (passage re-ranking) in the monolingual Spanish task. Otherwise, experiment aliv061esen refers to the first run in the bilingual Spanish-English task, while aliv062esen refers to the second run.</p><p>Both runs in the monolingual Spanish task present the same overall accuracy (29.47%). Nevertheless, there are differences in the performance depending on the question type. For factoid answers, the first run offers better accuracy, while the second run results more precise for definition questions. This tendency is repeated in the bilingual Spanish-English task. So, the re-ranking module seems to perform better for definition questions while decreases the accuracy of the baseline system over factoid ones. The experiments in bilingual Spanish-English task reflect a significant loss of performance with respect to the monolingual Spanish runs (20.00% of accuracy in the best run). As for this task we just employed automatic translations of the Spanish questions into English, the noise introduced in this early stage the question answer process heavily damages the global performance of the system. Anyway, the query construction module relies on lexical extraction patterns so that grammatical mistakes in translations do not significantly affect the performance of the system. These problems provoke that the accuracy can not be correctly evaluated. We could override this problem by using perfect hand-made translations only for evaluation purposes.</p><p>With respect to list questions, there were ten different questions on every run. We did nothing special to deal with this type of questions in our system. We followed the normal procedure established for every question type, returning up to a maximum of ten answers as indicated in the guidelines. We obtained a P@N of 0.0100, 0, 0.0411 and 0.0200 for aliv61eses, aliv62eses, aliv61esen and aliv62esen respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>In this paper we have described the novelties introduce in our Question Answering system for QA@CLEF 2006 competition. This year we have continued with the research line established in 2005, where the main goal is to obtain a fully data-driven system based on machine learning techniques. The main novelties this year are focused on the information retrieval stage. We defined a new passage re-ranking module based on LSA that seems to improve the performance of the system on definition questions, while decreasing the results for factoid ones. This suggest that we could set a good scheme for future work applying the baseline approach for factoid questions while employing re-ranking for definition ones.</p><p>Anyway, the results obtained with the re-ranking module were not completely satisfactory. The corpus employed to build de term-by-passage matrix (the 1000 best passages retrieve by Indri on every question) seems inadequate for this task. Probably a bigger amount of text would improve the results. Additionally, different tests varying the reduction of the dimensional space should be done in order to better tune the re-ranking module. The module is still in a preliminary stage and deserves deeper research.</p><p>This year we participated in the bilingual Spanish-English task to take advantage of the flexibility of machine learning based systems. The loss of performance with respect to the monolingual Spanish task may be due to the noise introduced by the automatic translation of questions, which makes a bit difficult to correctly evaluate the system.</p><p>As a future work, we plan to better test and tune the individual performance of the reranking procedure and continue integrating machine learning based modules in our system, going on with the answer extraction stage. Another challenge for the future is to test the system on new languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgements</head><p>This work has been developed in the framework of the project CICYT R2D2 (TIC2003-07158-C04).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,114.06,118.94,374.87,82.21"><head>Table 1 :</head><label>1</label><figDesc>Detailed results for monolingual Spanish and bilingual Spanish-English tasks</figDesc><table coords="5,132.57,130.61,337.85,70.54"><row><cell></cell><cell></cell><cell>Accuracy (%)</cell><cell></cell><cell></cell></row><row><cell cols="5">Experiment Factoid Definition Temporally restricted Overall</cell></row><row><cell>aliv061eses</cell><cell>28.08</cell><cell>35.71</cell><cell>0.00</cell><cell>29.47</cell></row><row><cell>aliv062eses</cell><cell>26.71</cell><cell>40.48</cell><cell>0.00</cell><cell>29.47</cell></row><row><cell>aliv061esen</cell><cell>19.33</cell><cell>22.50</cell><cell>-</cell><cell>20.00</cell></row><row><cell>aliv062esen</cell><cell>12.00</cell><cell>27.50</cell><cell>-</cell><cell>15.26</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,105.24,723.58,86.10,6.99"><p>http://www.xapian.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,105.24,733.09,130.11,6.99"><p>http://www.lemurproject.org/indri</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,105.24,691.30,175.55,6.99"><p>Text REtrieval Conference, http://trec.nist.gov</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,105.24,700.80,104.82,6.99"><p>http://www.systransoft.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="3,105.24,710.31,128.73,6.99"><p>http://infomap-nlp.sourceforge.net</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="3,105.24,719.81,87.97,6.99"><p>http://www.google.com</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,105.50,133.84,407.51,8.74;6,105.50,145.80,407.51,8.74;6,105.50,157.75,22.70,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,351.57,133.84,161.44,8.74;6,105.50,145.80,86.72,8.74">An XML-Based System for Spanish Question Answering</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tomás</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Saiz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Izquierdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="6,204.99,145.80,157.62,8.74">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page" from="347" to="350" />
			<date type="published" when="2006">4022. 2006</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.50,177.68,407.51,8.74;6,105.50,189.63,407.51,8.74;6,105.50,201.59,177.14,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,347.95,177.68,165.06,8.74;6,105.50,189.63,91.84,8.74">A Multilingual SVM-Based Question Classification System</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Bisbal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tomás</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,211.44,189.63,216.94,8.74">MICAI 2005: Advances in Artificial Intelligence</title>
		<title level="s" coord="6,437.31,189.63,75.69,8.74;6,105.50,201.59,77.10,8.74">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2005">3789. 2005</date>
			<biblScope unit="page" from="806" to="815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.50,221.51,407.50,8.74;6,105.50,233.47,169.98,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,322.91,221.51,185.72,8.74">Introduction to Latent Semantic Analysis</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">W</forename><surname>Foltz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Laham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,105.50,233.47,84.23,8.74">Discourse Processes</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="259" to="284" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.50,253.39,407.50,8.74;6,105.50,265.35,407.51,8.74;6,105.50,277.30,57.35,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,353.37,253.39,135.41,8.74">Question answering in Spanish</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Izquierdo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Llopis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Muñoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,168.19,265.35,255.88,8.74">Proceedings CLEF-2003 Lecture Notes in Computer Science</title>
		<editor>
			<persName><surname>Clef</surname></persName>
		</editor>
		<meeting>CLEF-2003 Lecture Notes in Computer Science<address><addrLine>Tronheim, Norwey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-08">August 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.50,297.23,407.50,8.74;6,105.50,309.18,407.51,8.74;6,105.50,321.14,71.98,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,287.99,297.23,225.01,8.74">Does English help Question Answering in Spanish?</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Saiz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Izquierdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,190.27,309.18,267.20,8.74">Proceedings CLEF-2004 Lecture Notes in Computer Science</title>
		<editor>
			<persName><surname>Clef</surname></persName>
		</editor>
		<meeting>CLEF-2004 Lecture Notes in Computer Science<address><addrLine>Bath, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-09">September 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.50,341.06,407.51,8.74;6,105.50,353.02,407.51,8.74;6,105.50,364.97,386.74,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,325.65,341.06,187.35,8.74;6,105.50,353.02,274.81,8.74">Automatic Feature Extraction for Question Answering Based on Dissimilarity of Probability Distributions</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tomás</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Bisbal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,459.49,353.02,53.51,8.74;6,105.50,364.97,126.09,8.74">Advances in Natural Language Processing</title>
		<title level="s" coord="6,239.33,364.97,152.88,8.74">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2006">2006. 4139. 2006</date>
			<biblScope unit="page" from="133" to="140" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
