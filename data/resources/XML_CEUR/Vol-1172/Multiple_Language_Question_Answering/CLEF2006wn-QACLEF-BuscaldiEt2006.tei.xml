<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,175.47,148.86,252.06,15.15">The UPV at QA@CLEF 2006</title>
				<funder ref="#_TEbec2M">
					<orgName type="full">ICT EU-India</orgName>
				</funder>
				<funder ref="#_e5abwJg">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2006-08-19">August 19, 2006</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,130.16,182.75,70.63,8.74"><forename type="first">Davide</forename><surname>Buscaldi</surname></persName>
							<email>dbuscaldi@dsic.upv.es</email>
							<affiliation key="aff0">
								<orgName type="laboratory">de Sistemas Informticos y Computación (DSIC)</orgName>
								<orgName type="institution">Universidad Politcnica de Valencia</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,223.48,182.75,87.16,8.74"><forename type="first">José</forename><forename type="middle">Manuel</forename><surname>Gomez</surname></persName>
							<email>jogomez@dsic.upv.es</email>
							<affiliation key="aff0">
								<orgName type="laboratory">de Sistemas Informticos y Computación (DSIC)</orgName>
								<orgName type="institution">Universidad Politcnica de Valencia</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,333.33,182.75,52.69,8.74"><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
							<email>prosso@dsic.upv.es</email>
							<affiliation key="aff0">
								<orgName type="laboratory">de Sistemas Informticos y Computación (DSIC)</orgName>
								<orgName type="institution">Universidad Politcnica de Valencia</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,408.72,182.75,64.11,8.74"><forename type="first">Emilio</forename><surname>Sanchis</surname></persName>
							<email>esanchis@dsic.upv.es</email>
							<affiliation key="aff0">
								<orgName type="laboratory">de Sistemas Informticos y Computación (DSIC)</orgName>
								<orgName type="institution">Universidad Politcnica de Valencia</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,175.47,148.86,252.06,15.15">The UPV at QA@CLEF 2006</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2006-08-19">August 19, 2006</date>
						</imprint>
					</monogr>
					<idno type="MD5">DA2C258440897EB85BD0C7448E5ED9A7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software Measurement, Algorithms, Performance, Experimentation Question Answering, Passage Retrieval, Answer Extraction and Analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report describes the work done by the RFIA group at the Departamento de Sistemas Informáticos y Computación of the Universidad Politécnica of Valencia for the 2006 edition of the CLEF Question Answering task. We participated in three monolingual tasks: Spanish, Italian and French. The system used is a slightly revised version of the one we developed for the past year. The most interesting aspect of the work is the comparison between a Passage Retrieval engine (JIRS) specifically aimed to the Question Answering task and a standard, general use search engine such as Lucene. Results show that JIRS is able to return high quality passages.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>QUASAR is the mono/cross-lingual Question Answering (QA) System we developed for our first participation to the past edition of the CLEF QA task. It is based on the JIRS Passage Retrieval (PR) system, specifically oriented to this task, in contrast to most QA systems that use classical PR methods <ref type="bibr" coords="1,146.78,621.13,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="1,160.20,621.13,7.75,8.74" target="#b0">1,</ref><ref type="bibr" coords="1,170.86,621.13,7.75,8.74" target="#b6">7,</ref><ref type="bibr" coords="1,181.51,621.13,7.01,8.74" target="#b4">5]</ref>. JIRS can be considered as a language-independent PR system, because it does not use any knowledge about lexicon and syntax of the language during question and passage processing phases. One of the objectives of this participation was the comparison of JIRS with a classical PR engine (in this case, Lucene<ref type="foot" coords="1,278.04,655.42,3.97,6.12" target="#foot_0">1</ref> ). In order to do this, we actually implemented two versions of QUASAR, which differ only for the PR engine used. With regard to the improvements over last year's system, our efforts were focused on the Question Analysis module, which in contrast to the one used in 2005 does not use Support Vector Machines in order to classify the questions. Moreover, we moved towards a stronger integration of the modules.</p><p>The 2006 CLEF QA task introduced some challenges with respect to the previous edition: list questions, the lack of a label distinguishing "definition" questions from other ones, and the introduction of another kind of "definitions", that we named object definitions. This forced us to change slightly the class ontology we used in 2005. This year we did not participate to the cross-language tasks.</p><p>In the next section, we describe the structure and the building blocks of our QA system. In section 3 we discuss the results of QUASAR in the 2006 CLEF QA task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Architecture of QUASAR</head><p>The architecture of QUASAR is shown in Fig. <ref type="figure" coords="2,291.68,214.61,4.13,8.74">1</ref>.</p><p>Figure <ref type="figure" coords="2,252.32,567.67,3.88,8.74">1</ref>: Diagram of the QA system Given a user question, this will be handed over to the Question Analysis module, which is composed by a Question Analyzer that extracts some constraints to be used in the answer extraction phase, and by a Question Classifier that determines the class of the input question. At the same time, the question is passed to the Passage Retrieval module, which generates the passages used by the Answer Extraction (AE) module together with the information collected in the question analysis phase in order to extract the final answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Question Analysis Module</head><p>This module obtains both the expected answer type (or class) and some constraints from the question. Question classification is a crucial step of the processing since the Answer Extraction module uses a different strategy depending on the expected answer type; as reported by Moldovan et al. <ref type="bibr" coords="3,119.61,112.02,9.97,8.74" target="#b3">[4]</ref>, errors in this phase account for the 36.4% of the total number of errors in Question Answering.</p><p>The different answer types that can be treated by our system are shown in Table <ref type="table" coords="3,459.79,135.93,3.88,8.74" target="#tab_0">1</ref>. We introduced the "FIRSTNAME" subcategory for "NAME" type questions, because we defined a pattern for this kind of questions in the AE module. We also specialized the "DEFINITION" class into three subcategories: "PERSON", "ORGANIZATION" and "OBJECT", which was introduced this year (e.g.: What is a router? ). With respect to CLEF 2005, the Question Classifier does not use a SVM classifier. Each category is defined by one or more patterns written as regular expressions. The questions that do not match any defined pattern are labeled with OTHER. If a question matches more than one pattern, it is assigned the label of the longest matching pattern (i.e., we consider longest patterns to be less generic than shorter ones).</p><p>The Question Analyzer has the purpose of identifying the constraints to be used in the AE phase. These constraints are made by sequences of words extracted from the POS-tagged query by means of POS patterns and rules. For instance, any sequence of nouns (such as ozone hole) is considered as a relevant pattern. The POS-taggers used were the SVMtool<ref type="foot" coords="3,436.09,561.04,3.97,6.12" target="#foot_1">2</ref> for English and Spanish, and the TreeTagger<ref type="foot" coords="3,215.20,573.00,3.97,6.12" target="#foot_2">3</ref> for Italian and French.</p><p>There are two classes of constraints: a target constraint, which is the word of the question that should appear closest to the answer string in a passage, and zero or more contextual constraints, keeping the information that has to be included in the retrieved passage in order to have a chance of success in extracting the correct answer. For example, in the following question: "Dónde se celebraron los Juegos Olímpicos de Invierno de 1994? " (Where did the Winter Olympic games of 1994 take place? ) celebraron is the target constraint, while Juegos Olímpicos de Invierno and 1994 are the contextual constraints. There is always only one target constraint for each question, but the number of contextual constraint is not fixed. For instance, in "Quién es Neil Armstrong? " the target constraint is Neil Armstrong but there are no contextual constraints.</p><p>We did not implement</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Passage Retrieval Module (JIRS)</head><p>Passages with the relevant terms (i.e., without stopwords) are found by the Search Engine using the classical IR system. This year, the module was modified in order to rank better the passages which contain an answer pattern matching the question type. Therefore, this module is not as language-independent as in 2005 because it uses informations from the Question Classifier and the patterns used in the Answer Extraction phase. Sets of unigrams, bigrams, ..., n-grams are extracted from the extended passages and from the user question. In both cases, n will be the number of question terms. With the n-gram sets of the passages and the user question we will make a comparison in order to obtain the weight of each passage. The weight of a passage will be heavier if the passage contains greater n-gram structures of the question.</p><p>For instance, if we ask "Who is the President of Mexico? " the system could retrieve two passages: one with the expression "...Vicente Fox is the President of Mexico...", and the other one with the expression "...Giorgio Napolitano is the President of Italy...". Of course, the first passage must have more importance because it contains the 5-gram "is the President of Mexico", whereas the second passage only contains the 4-gram "is the President of ", since the "is the President of Italy" 5-gram is not in the original question. To calculate the weight of n-grams of every passage, first the greatest relevance of n-gram in the passage is identify and we assign to this a weight equal to the sum of all term weights. Next, other n-grams less relevant are searched. These n-grams are not composed by terms of found n-grams. The weight of these n-grams will be the sum of all their weight terms divided by two, in order to avoid that its weight will be the same of the complete n-gram. The weight of every term comes fixed by (refeqtermweight):</p><formula xml:id="formula_0" coords="4,251.76,389.94,257.00,22.31">w k = 1 - log(n k ) 1 + log(N ) . (<label>1</label></formula><formula xml:id="formula_1" coords="4,508.76,396.68,4.24,8.74">)</formula><p>Where n k is the number of passages in which the associated term to the weight w k appears and N is the number of system passages. We make the assumption that stopwords occur in every passage (i.e., n k takes the value of N ). For instance, if the term appears once in the passage collection, its weight will be equal to 1 (the greatest weight). Whereas if it is a stopword its weight will be the lowest.</p><p>Depending on the style used to submit a question, sometimes a term unrelated to the question can obtain a greater weight than those assigned to the Named Entities (NEs), such as names of persons, organizations and places, dates. The NEs are the most important terms of the question and it does not make sense to return passages which do not contain them. Therefore, the equation (1) has been changed in order to give more weight to the NE than the rest of question terms and so to force its presence in the first passages of the ranking. In order to identify a NE a natural language processing is not used. We assumed that in most questions the NEs start with either an uppercase letter or they are a number. Once the terms are weighted, these are normalized for the sum of all terms are equal to 1.</p><p>JIRS can be obtained at the following URL: http://leto.dsic.upv.es:8080/jirs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Answer Extraction</head><p>The input of this module is constituted by the n passages returned by the PR module and the constraints (including the expected type of the answer) obtained through the Question Analysis module. A TextCrawler is instantiated for each of the n passages with a set of patterns for the expected type of the answer and a pre-processed version of the passage text. For CLEF 2006, we corrected some errors in the patterns and we also introduced new ones. Some patterns can be used for all languages; for instance, when looking for proper names, the pattern is the same for all languages. The pre-processing of passage text consists in separating all the punctuation characters from the words and in stripping off the annotations of the passage. It is important to keep the punctuation symbols because we observed that they usually offer important clues for the individuation of the answer (this is true especially for definition questions):</p><p>for instance, it is more frequent to observe a passage containing "The president of Italy, Giorgio Napolitano" than one containing "The president of Italy IS Giorgio Napolitano" ; moreover, movie and book titles are often put between apices.</p><p>The positions of the passages in which occur the constraints are marked before passing them to the TextCrawlers. A difference with 2005 is that now we do not use the Levenshtein-based spell-checker to compare strings in this phase now.</p><p>The TextCrawler begins its work by searching all the passage's substrings matching the expected answer pattern. Then a weight is assigned to each found substring s, depending on the positions of s with respect to the constraints, if s does not include any of the constraint words. If in the passage are present both the target constraint and one or more of the contextual constraints, then the product of the weights obtained for every constraint is used; otherwise, it is used only the weight obtained for the constraints found in the passage.</p><p>The Filter module takes advantage of a mini knowledge base in order to discard the candidate answers which do not match with an allowed pattern or that do match with a forbidden pattern. For instance, a list of country names in the four languages has been included in the knowledge base in order to filter country names when looking for countries. When the Filter module rejects a candidate, the TextCrawler provide it with the next best-weighted candidate, if there is one.</p><p>Finally, when all TextCrawlers end their analysis of the text, the Answer Selection module selects the answer to be returned by the system. The following strategies apply:</p><p>• Simple voting (SV): The returned answer corresponds to the candidate that occurs most frequently as passage candidate.</p><p>• Weighted voting (WV): Each vote is multiplied for the weight assigned to the candidate by the TextCrawler and for the passage weight as returned by the PR module.</p><p>• Maximum weight (MW): The candidate with the highest weight and occurring in the best ranked passage is returned.</p><p>• Double voting (DV): As simple voting, but taking into account the second best candidates of each passage.</p><p>• Top (TOP): The candidate elected by the best weighted passage is returned.</p><p>This year we used the Confidence Weighted Score (CWS) to select the answer to be returned to the system, relying on the fact that in 2005 our system was the one returning the best values for CWS <ref type="bibr" coords="5,134.61,522.48,9.96,8.74" target="#b5">[6]</ref>. For each candidate answer we calculated the CWS by dividing the number of strategies giving the same answer by the total number of strategies <ref type="bibr" coords="5,403.68,534.44,11.62,8.74" target="#b4">(5)</ref>, multiplied for other measures depending on the number of passages returned (n p /N , where N is the maximum number of passages that can be returned by the PR module and n p is the number of passages actually returned) and the averaged passage weight. The final answer returned by the system is the one with the best CWS. Our system always return only one answer (or NIL), although 2006 rules allowed to return more answers per question. The weighting of NIL answers is slightly different, since is obtained as 1 -n p /N if n p &gt; 0, 0 elsewhere.</p><p>The snippet for answer justification is obtained from the portion of text surrounding the first occurrence of the answer string. The snippet size is always 300 characters (150 before and 150 after the answer) + the number of characters of the answer string.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>We submitted two runs for each of the following monolingual task: Spanish, Italian and French. The first runs (labelled upv 061 ) use the system with JIRS as PR engine, whereas for the other runs we used Lucene, adapted to the QA task with the implementation of a weighting scheme that privileges long passages and is similar to the word-overlap scheme of the MITRE system <ref type="bibr" coords="5,484.97,732.66,9.96,8.74" target="#b1">[2]</ref>. In Table <ref type="table" coords="5,117.40,744.62,4.98,8.74" target="#tab_1">2</ref> we show the overall accuracy obtained in all the runs. With respect to 2005, the overall accuracy increased by ∼ 3% in Spanish and Italian, and by ∼ 7% in French. We suppose that the improvement in French is due to the fact that the target collection was larger this year. Spanish is still the language in which we obtain the best results, even if we are not sure about the reason: a possibility is that this can be due to the better quality of the POS-tagger used in the analysis phase for the Spanish language.</p><p>We obtained an improvement over the 2005 system in factoid questions, but also worse results in definition ones, probably because of the introduction of the object definitions.</p><p>The JIRS-based systems performed better than the Lucene-based ones in Spanish and French, whereas in Italian they obtained almost the same results. The difference in the CWS values obtained in both Spanish and French is consistent and weights in favour of JIRS. This prove that the quality of passages returned by JIRS for these two languages is considerably better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Further Work</head><p>We obtained a slight improvement over the results of our 2005 system. This is consistent with the small amount of modifications introduced, principally because of the new rules defined for the 2006 CLEF QA task. The most interesting result is that JIRS demonstrated to be more effective for the QA task than a standard search engine such as Lucene in two languages over three. Our further works on the QUASAR system will concern the implementation of a specialized strategy for definition questions, and probably a major revision of the Answer Extraction module.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" coords="2,90.00,235.30,423.01,317.26"><head></head><label></label><figDesc></figDesc><graphic coords="2,90.00,235.30,423.01,317.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,176.95,224.54,249.11,225.27"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="3,176.95,224.54,249.11,225.27"><row><cell cols="3">: QC pattern classification categories.</cell></row><row><cell>L0</cell><cell>L1</cell><cell>L2</cell></row><row><cell>NAME</cell><cell>ACRONYM</cell><cell></cell></row><row><cell></cell><cell>PERSON</cell><cell></cell></row><row><cell></cell><cell>TITLE</cell><cell></cell></row><row><cell></cell><cell>FIRSTNAME</cell><cell></cell></row><row><cell></cell><cell>LOCATION</cell><cell>COUNTRY</cell></row><row><cell></cell><cell></cell><cell>CITY</cell></row><row><cell></cell><cell></cell><cell>GEOGRAPHICAL</cell></row><row><cell cols="2">DEFINITION PERSON</cell><cell></cell></row><row><cell></cell><cell>ORGANIZATION</cell><cell></cell></row><row><cell></cell><cell>OBJECT</cell><cell></cell></row><row><cell>DATE</cell><cell>DAY</cell><cell></cell></row><row><cell></cell><cell>MONTH</cell><cell></cell></row><row><cell></cell><cell>YEAR</cell><cell></cell></row><row><cell></cell><cell>WEEKDAY</cell><cell></cell></row><row><cell>QUANTITY</cell><cell>MONEY</cell><cell></cell></row><row><cell></cell><cell>DIMENSION</cell><cell></cell></row><row><cell></cell><cell>AGE</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,90.00,118.94,423.00,117.83"><head>Table 2 :</head><label>2</label><figDesc>Accuracy results for the submitted runs. Overall: overall accuracy, factoid: accuracy over factoid questions; definition: accuracy over definition questions; nil: precision over nil questions (correctly answered nil/times returned nil); CWS: confidence-weighted score.</figDesc><table coords="6,166.51,155.10,269.98,81.66"><row><cell cols="2">task run</cell><cell cols="2">overall factoid definition</cell><cell>nil CWS</cell></row><row><cell cols="3">es-es upv 061 36.84% 34.25%</cell><cell>47.62% 0.33 0.225</cell></row><row><cell></cell><cell cols="2">upv 062 30.00% 27.40%</cell><cell>40.48% 0.32 0.148</cell></row><row><cell>it-it</cell><cell cols="2">upv 061 28.19% 28.47%</cell><cell>26.83% 0.23 0.123</cell></row><row><cell></cell><cell cols="2">upv 062 28.19% 27.78%</cell><cell>29.27% 0.23 0.132</cell></row><row><cell cols="3">fr-fr upv 061 31.58% 31.08%</cell><cell>33.33% 0.36 0.163</cell></row><row><cell></cell><cell cols="2">upv 062 24.74% 26.35%</cell><cell>19.05% 0.18 0.108</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,105.24,747.00,91.27,6.99"><p>http://lucene.apache.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,105.24,713.06,149.17,6.99"><p>http://www.lsi.upc.edu/ nlp/SVMTool/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,105.24,722.56,328.21,6.99"><p>http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/DecisionTreeTagger.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank <rs type="person">R2D2 CICYT</rs> (<rs type="grantNumber">TIC2003-07158-C04-03</rs>) and <rs type="funder">ICT EU-India</rs> (<rs type="grantNumber">ALA/95/23/2003/077-054</rs>) research projects.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_TEbec2M">
					<idno type="grant-number">TIC2003-07158-C04-03</idno>
				</org>
				<org type="funding" xml:id="_e5abwJg">
					<idno type="grant-number">ALA/95/23/2003/077-054</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,105.50,627.45,407.50,8.74;6,105.50,639.40,407.51,8.74;6,105.50,651.36,71.54,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,346.95,627.45,166.05,8.74;6,105.50,639.40,105.83,8.74">Cross-language question answering at the university of helsinki</title>
		<author>
			<persName coords=""><forename type="first">Lili</forename><surname>Aunimo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Reeta</forename><surname>Kuuskoski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Juha</forename><surname>Makkonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,231.75,639.40,276.72,8.74">Workshop of the Cross-Lingual Evaluation Forum (CLEF 2004)</title>
		<meeting><address><addrLine>Bath, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.50,671.28,407.50,8.74;6,105.50,683.24,299.05,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,372.31,671.28,140.69,8.74;6,105.50,683.24,129.96,8.74">Analyses for elucidating current question answering technology</title>
		<author>
			<persName coords=""><forename type="first">Marc</forename><surname>Light</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gideon</forename><forename type="middle">S</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Breck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,244.27,683.24,68.28,8.74">Nat. Lang. Eng</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="325" to="342" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.50,703.16,407.50,8.74;6,105.50,715.12,376.65,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,432.41,703.16,80.59,8.74;6,105.50,715.12,168.44,8.74">Multilingual question/answering: the DIOGENE system</title>
		<author>
			<persName coords=""><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roberto</forename><surname>Prevete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hristo</forename><surname>Tanev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,295.46,715.12,156.07,8.74">The 10th Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,112.02,407.50,8.74;7,105.50,123.98,407.50,8.74;7,105.50,135.93,389.36,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,429.63,112.02,83.37,8.74;7,105.50,123.98,283.22,8.74">Performance issues and error analysis in an open-domain question answering system</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marius</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,411.83,123.98,101.17,8.74;7,105.50,135.93,283.34,8.74">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,155.86,407.51,8.74;7,105.50,167.81,407.51,8.74;7,105.50,179.77,357.14,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,297.60,155.86,215.41,8.74;7,105.50,167.81,387.06,8.74">Experiments on robust nl question interpretation and multi-layered document annotation for a cross-language question/answering system</title>
		<author>
			<persName coords=""><forename type="first">Gunther</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Sacaleanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,105.50,179.77,277.75,8.74">Workshop of the Cross-Lingual Evaluation Forum (CLEF 2004)</title>
		<meeting><address><addrLine>Bath, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,199.69,407.50,8.74;7,105.50,211.65,407.51,8.74;7,105.50,223.60,407.51,8.74;7,105.50,235.56,22.70,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,105.50,223.60,278.31,8.74">Overview of the clef 2005 multilingual question answering track</title>
		<author>
			<persName coords=""><forename type="first">Alessandro</forename><surname>Vallin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lili</forename><surname>Aunimo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christelle</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Petya</forename><surname>Osenova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Sacaleanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Sutcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,405.93,223.60,102.70,8.74">CLEF 2005 Proceedings</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,255.48,407.50,8.74;7,105.50,267.44,407.50,8.74;7,105.50,279.39,62.02,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,427.11,255.48,85.89,8.74;7,105.50,267.44,42.89,8.74">Question answering in spanish</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruben</forename><surname>Vicedo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Izquierdo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rafael</forename><surname>Llopis</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Munoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,172.24,267.44,281.97,8.74">Workshop of the Cross-Lingual Evaluation Forum (CLEF 2003)</title>
		<meeting><address><addrLine>Trondheim, Norway</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
