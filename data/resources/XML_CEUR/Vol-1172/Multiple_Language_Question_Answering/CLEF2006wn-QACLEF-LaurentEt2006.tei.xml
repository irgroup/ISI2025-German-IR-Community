<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,144.96,136.75,305.36,12.65;1,252.96,153.31,89.52,12.65">Cross Lingual Question Answer ing using QRISTAL for CLEF 2006</title>
				<funder>
					<orgName type="full">TRUST</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,193.44,192.69,77.93,9.16"><forename type="first">Dominique</forename><surname>Laurent</surname></persName>
							<email>dlaurent@synapsefr.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Synapse Développement</orgName>
								<address>
									<addrLine>33 rue Maynard</addrLine>
									<postCode>31000</postCode>
									<settlement>Toulouse</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,278.40,192.69,61.37,9.16"><forename type="first">Patrick</forename><surname>Séguéla</surname></persName>
							<email>p.seguela@synapsefr.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Synapse Développement</orgName>
								<address>
									<addrLine>33 rue Maynard</addrLine>
									<postCode>31000</postCode>
									<settlement>Toulouse</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,346.80,192.69,55.12,9.16"><forename type="first">Sophie</forename><surname>Nègre</surname></persName>
							<email>sophie.negre@synapsefr.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Synapse Développement</orgName>
								<address>
									<addrLine>33 rue Maynard</addrLine>
									<postCode>31000</postCode>
									<settlement>Toulouse</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,144.96,136.75,305.36,12.65;1,252.96,153.31,89.52,12.65">Cross Lingual Question Answer ing using QRISTAL for CLEF 2006</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">897EB507B7902A651F6847952C577832</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> QRISTAL [9]  <p>is a question answering system making intensive use of natural language processing both for indexing documents and extracting answers. It ranked first in the EQueR evaluation campaign (Evalda, Technolangue [3]) and in CLEF 2005 for monolingual task (FrenchFrench) and multilingual task (EnglishFrench and PortugueseFrench). This article describes the improvements of the system since last year. Then, it presents our benchmarked results for the CLEF 2006 campaign and a critical description of the system. Since Synapse Développement is participating to Quaero project, QRISTAL is most likely to be integrated in a mass market search engine in the forthcoming years.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 Introduction QRISTAL (French acronym for "Question Answering Integrating Natural Language Processing Techniques") is a cross lingual question answering system for French, English, Italian, Portuguese, Polish and Czech. It was designed to extract answers both from documents stored on a hard disk and from Web pages by using traditional search engines (Google, MSN, AOL, etc.). Qristal is currently used in the MCAST European project of Econtent (22249, Multilingual Content Aggregation System based on TRUST Search Engine). Anyone can assess the Qristal technology for French at www.qristal.fr. Note that the testing corpus for the testing web page is the grammar handbook proposed at http://www.synapsefr.com.</p><p>For each language, a linguistic module analyzes questions and searches for potential answers. For CLEF 2006, the French, English and Portuguese modules were used for question analysis. Only the French module was used for answers extraction. The linguistic modules were developed by different companies. They share however a common architecture and similar resources (general taxonomy, typology of questions and answers and terminological fields).</p><p>For French, our system is based on the Cordial technology. It massively uses NLP tools, such as syntactic analysis, semantic disambiguation, anaphora resolution, metaphor detection, handling of converses, named entities extraction as well as conceptual and domain recognition. As the product is being marketed, the linguistic resources need to be permanently updated and it required a constant optimization of the various modules so that the software remains extremely fast. Users are now accustomed to obtain something that looks like an answer within a very short time, not exceeding two seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Architecture</head><p>The architecture of the Qristal system is described in different articles (see <ref type="bibr" coords="2,384.96,113.73,10.70,9.16" target="#b0">[1]</ref>, <ref type="bibr" coords="2,402.72,113.73,10.70,9.16" target="#b1">[2]</ref>, <ref type="bibr" coords="2,420.48,113.73,10.70,9.16" target="#b7">[8]</ref>, <ref type="bibr" coords="2,438.24,113.73,10.70,9.16" target="#b8">[9]</ref>, <ref type="bibr" coords="2,456.00,113.73,15.45,9.16" target="#b9">[10]</ref>, <ref type="bibr" coords="2,478.80,113.73,15.45,9.16" target="#b10">[11]</ref>, <ref type="bibr" coords="2,501.60,113.73,15.11,9.16" target="#b11">[12]</ref>). Qristal is a complete engine for indexation and answers extraction. However, it doesn't index the Web. Indexing is processed only for documents based on disks. Web search uses a metasearch engine we have implemented. As we will see in the conclusion, our participation to Quaero project should change this way of use by tagging semantically the Web pages.</p><p>Our company is responsible for the indexing process of Qristal. Moreover, it ensures the integration and interoperability between all linguistic modules. Both English and Italian modules were developed by Expert System Company. The Portuguese module was developed by the Priberam Company which also takes part in CLEF 2005 for Portuguese monolingual and in CLEF 2006 for Spanish and Portuguese monolingual, and for SpanishPortuguese and PortugueseSpanish multilingual tasks. The Polish module was developed by the TiP Company. The Czech module is developed by the University of Economics of Prague (UEP). These modules were developed within the European projects TRUST <ref type="bibr" coords="2,289.44,251.73,11.75,9.16" target="#b7">[8]</ref> (Text Retrieval Using Semantic Technologies) and M CAST (Multilingual Content Aggregation System based on TRUST Search Engine).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multicriteria indexing</head><p>While indexing documents, the technology automatically identifies the document language of and the system calls the corresponding language module. There are as many indexes as languages identified in the corpus. Documents are treated per blocks. The size of each block is approximately 1 kilobyte. Block limits are settled on the end of sentences or paragraphs. This size of block (1 kb) appeared to be optimal during our tests. Some indexes relate to blocks like fields or taxonomy whereas other relate to words, like idioms or named entities.</p><p>Each linguistic module processes a syntactic and semantic analysis for each block to be indexed. It fills a complete structure of data for each sentence. This structure is passed to the general processor that uses it to increment the various indexes. This description is accurate for the French module. Other language modules are very close to that framework but don't always include all its elements. For example, English and Italian modules do not include an indexing based on heads of derivation.</p><p>Texts are converted into Unicode. Then, they are divided into one kilobyte blocks. This reduces the index size as only the number of occurrences per block is stored for a given lemma. This number of occurrences is used to infer the relevance of each block while searching a given lemma in the index. In fact we here use lemmas but the system stores heads of derivation and not lemmas. For example, symmetric, symmetrical, asymmetry, dissymmetrical or symmetrize will be indexed in the same entry : symmetry. Each text block is analyzed syntactically and semantically. Considering results of this analysis, 8 different indexes are built for:</p><p>• heads of derivation. A head of derivation can be a sense for a word. In French, the verb voler has 2 different meanings (to steal or to fly). The meaning "dérober" (to steal) will lead to vol (robbery), voleur (thief) or voleuse (female thief). The second meaning, "se mouvoir dans l'air" (to fly), will lead to vol (flight), volant (flying as an adjective), voleter ( to flutter) or envol (taking flight) and all its forms.</p><p>• proper names. If they appear in our dictionaries.</p><p>• idioms. Those idioms are listed in our idioms dictionaries. They encompass approximately 50 000 entries, like word processing, fly blind or as good as your word. • named entities. Named entities are extracted from texts. George W. Bush or Defense Advanced Research Project Agency are named entities.</p><p>• concepts. Concepts are nodes of our general taxonomy. 2 levels of concepts are indexed. The first level lists 256 categories, like "visibility". The second level, actually the leaves of our taxonomy, lists 3387 subcategories, like "lighting" or "transparency",</p><p>• fields. 186 fields, like "aeronautics", "agriculture", etc.,</p><p>• question and answer types for categories like "distance", "speed", "definition", "causality", etc.,</p><p>• keywords of the text.</p><p>For each language, the indexing process is similar. Extracted data are the same. Thus, the handling of those data is independent of their original language. This is particularly important for cross language question answering.</p><p>For the French language, the rate of correct grammatical disambiguation (distinction between nameverb adjectiveadverb) is higher than 99%. The rate of semantic disambiguation is approximately 90% for 9 000 polysemous words and approximately 30 000 senses for these words. Note that this number of senses is markedly inferior to the Larousse one (Larousse is one of the most famous French dictionaries). Note however that our idioms dictionary covers a large number of the senses mentioned in this kind of dictionaries. The indexing speed varies between 200 and 400 Mo per hour with a Pentium 3 GHz, according to the size and number of indexed files.</p><p>Indexing question types is undoubtedly one of the most original aspects of our system. While the analysis of the blocks is being made, possible answers are located. For example, a name of function for a person (like baker, minister, director of public prosecutions), a date of birth (like born on April 28, 1958), a causality (like due to snow drift or because of freezing), a consequence (like leading to serious disruption or facilitating the management of the traffic). This caused the block to be indexed like being able to provide an answer for a given question type.</p><p>Currently, our question typology includes 86 types of questions. These rates are very close to CLEF 2005 results, because we only improved the French module and elaborated a new English module. The "English 1" corresponds to the Synapse English module and the "English 2" corresponds to the Expert Systems English module.</p><p>Building a keyword index for each text is also peculiar to our system. Dividing text into blocks made it compulsory. Isolated blocks cannot explicitly mention main subjects of the original text although sentences of these blocks relate to these subjects. The keyword index makes it possible to add contextual information about the main subjects of the text for blocks. Keywords can be a concept, a person, an event, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Answer extraction</head><p>After the user has keyed in his/her question, it is syntactically and semantically analyzed by the system. Question type is inferred. We would like here to draw the attention to the fact that questions are shorter than texts. This lack of context makes the semantic analysis of the question more dubious. That's why the semantic analysis processed on the question is more comprehensive than the analysis processed on texts. Moreover, users have the possibility to interactively force a sense. This possibility, however, was not used for CLEF as the entire process was automatic.</p><p>The result of the semantic analysis of the question is a weight for each sense of each word recognized as a pivot. For example, sense 1 is recognized with 20%, sense 2 with 65% and sense 3 with 15%. This weight, together with synonyms, question and answer types or concepts, is considered while searching the index. Thus all senses of a word are taken into account during the index search. This prevents from dramatic consequences due to errors in the semantic disambiguation while making the most of good analysis.</p><p>After question analysis, all indexes are searched and the best ranked blocks are analyzed again. As one can notice on figure <ref type="figure" coords="4,143.04,99.57,3.78,9.16" target="#fig_1">2</ref>, the analysis of the selected blocks is close to the analysis processed while indexing or question analyzing. On top of this "classic" analysis, a weight for each sentence is inferred. This weight is based on the number of words, synonyms and named entities found in this sentence, the presence of an answer corresponding to the question type and a correspondence between the fields and domain.</p><p>After this analysis, sentences are ranked. Then, an additional analysis is processed to extract named entities, idioms or lists that match the answer. This extraction relies on the syntactic characteristics of those groups.</p><p>For a question on a corpus located on a hard disk, the response time is approximately 1,3 seconds with a Pentium 3 GHz. On the Web, first answers are provided after 2 seconds. Then the system computes a progressive refining during ten seconds, according to user's parameters like the number of words, the number of analyzed pages, etc.</p><p>We tested many answer justification modules, mostly implemented from Web <ref type="bibr" coords="4,394.32,255.57,10.70,9.16" target="#b3">[4]</ref>, <ref type="bibr" coords="4,412.08,255.57,11.75,9.16" target="#b6">[7]</ref> or <ref type="bibr" coords="4,438.72,255.57,15.45,9.16" target="#b14">[15]</ref>. Our technology enables, as an option, to use such a module of justification. It consists in searching the web with the words of the question looking for potential answers the system inferred. However this process is seldom selected by users as it increases the response time of a few seconds. It was not used in CLEF 2006 either. The only justification module we used was an internal module which makes the most of the semantic information for proper names enclosed in our dictionaries. For more than 40 000 proper names, we possess information about the country of origin, the year of birth and death, the function for people, country, the area and population for a city, etc. We think this justification module is at the origin of some "unjustified" answers. As a matter of fact, it caused the system to rank first a text including the answer even if the system did not find any clear justification of that answer in the text.</p><p>For cross language question answering, English is used as pivot language. The fact that most users are only interested in documents in their own language and English motivated that choice. Thus, for cross language answering, the system processes generally only one translation. For this evaluation, both Portuguese to French and Italian to French runs required two translations: from source language to English and then from English to French. QRISTAL does not use any Web Services for translation because of response time. Only words or idioms recognized as pivots are translated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Improvements since CLEF 2005</head><p>For CLEF 2006, we used our same technology and system, in mono and multilingual mode <ref type="bibr" coords="4,443.94,509.73,13.63,9.16" target="#b8">[9]</ref>, but with a few improvements, such as :</p><p>The ontology has been revised all through the year, mainly to emend errors or categorisation. To maintain compatibility with other language ontology, no new category has been added nor deleted.</p><p>The dictionaries have been updated, in particular proper names and expressions. This updating effort, while being continuous, has been increased last year, but not the extent to be ready, with other resources, for the said evaluation. This being the case for the dictionary of nominal expressions, leaping from 55 000 expressions to over 100 000 and not integrated in the assessed Qristal version.</p><p>A multilingual (French,English,Spanish,Italian,Portuguese) lexicon of translated proper names has been implemented. It includes more than 5 000 proper names and acronyms, mainly toponyms, (countries, provinces, towns) but also name of people, in particular names in Arabic, Russian, Chinese, which spelling differs according to the languages. This lexicon has played an important role in the improvement of the CLEF 2006 results over CLEF 2005, for the PortugeseFrench pair, as the English translation was avoided.</p><p>The EnglishFrench and FrenchEnglish dictionaries have also been revised and increased with now more than 200 000 translations of words or expressions. The impact of this improvement has been measured in comparing the CLEF 2005 results with the former dictionaries versus the CLEF 2006 results using the improved ones.</p><p>Only one question in Portuguese to French and two questions in English to French find an additional answer with the new dictionaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Improvement of the algorithms</head><p>Our syntactic analyser has not noticeably been improved. However, according to the nonyet final results of another benchmark evaluation, our analyser is given as the most performing and , above all, robust for the French language. It appears that it is still yet very far from the complete detection of the complex syntactic structures. So, for the SubjectVerb relation, it detects exactly the subject and the verb in only 9 cases out of 10. But this must be moderated by the fact , the evaluation is carried out on all types of corpora, including emails and chats, somehow more difficult to analyse.</p><p>The module « search of the category of the question» has been improved but this impacts only on the monolingual FrenchFrench part. For the MCAST European project, our engine was enriched of numerous utilities to manage very large volumes of data and to satisfy "clientphp server" structures, but these have no impact on the performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">New English module</head><p>The main difference between the engine assessed at CLEF 2005 and CLEF 2006, is due to the inhouse development of a new English language processing module. On the basis of our ancient syntactic analyser and englishbased linguistic resources not yet completed, we used a beta version of our new English module. It carries out the syntactic and semantic analysis of the question, determines the type of the question, the pivot words and synonyms, then transfers the results to the French module that implements the requested translations of the same, to finish with the use of the language independent data (type of the question, categories of the ontology, etc.).  Results per category are as follows: As last year, our system is highly performing for questions of the type « Definition ». It is to be noted that this type of question , the "loss" of performance in a multilingual context, is less than for the other types of questions. This is due to the fact the "Definition" type of question relates most of the times, to acronyms or surnames of people, for which the translation is simpler and less ambiguous. The contribution of the PortugueseFrench proper names lexicon seems obvious as the percentage of found definitions in this pair has moved from 68 % (CLEF 2005) to 77 % (CLEF 2006).</p><p>The « listtype » questions were only identified by our French module, thus none of this type of question were exact from the Portuguese, and the questions assessed as exact for EnglishFrench were , in fact, lists of one single element. For French, the proportion of exactly identified lists was honourable (50%), but this type of question remains difficult to process by our system.</p><p>Specific developments for the NIL questions had been implemented in CLEF 2006. In monolingual mode, the improvement is spectacular since the "precision rate" is now 0.56 versus 0.23 in 2005 and the "recall rate" 0.66 versus 0.25. For EnglishFrench they were respectively 0.29 v 0.14 and 0.66 v 0.30. Finally, for PortugueseFrench, the "precision rate" was 0.26 v. 0.13 and "recall rate" was 0.70 v 0.15.</p><p>Figure <ref type="figure" coords="6,101.28,687.81,5.04,9.16">4</ref> presents statistics for answers evaluated as 'R' that stands for right. But CLEF proposed two other qualifications for answers that is 'U' for unjustified and 'X' for inexact. We think 'U' and 'X' answers would be often accepted by users, even 'X' answers if they are presented with their context. For question 57 Qui est Flavio Briatore ? (Who is Flavio Briatore?), the answer provided by our system was directeur général de Benetton Formula (general manager of Benetton Formula), whereas the awaited answer was directeur général de Benetton Formula 1 (general manager of Benetton Formula 1). Likewise, for question 96 A quel parti politique Jacques Santer appartientil ? (Which political party does Jacques Santer belong to ?), the provided answer by Qristal was Parti chrétiensocial dès 1966 (Christian Social Party since 1966) whereas the awaited answer was Parti chrétiensocial (Christian Social party). This lead us to consider statistics for all answers considered as "not wrong", that is right (R), unjustified (U) or inaccurate (X): Then we had a closer look to questions where the monolingual process finds the answer but the cross language does not. This leads us to the following remark. Questions are defined by reading the corpus and, deliberately or not, people formulating questions tend to reuse words or expressions mentioned in the text of the identified answer. On one hand, this influences the capacity of the system and the importance of each module in the overall process. For example, the use of synonyms is not that important for CLEF as it normally is. On the other hand, for cross language question answering, translations can be fuzzy and potentially quite far from the targeted word or expression especially when one uses English as an intermediate language. In this way, translated words are quite often different from the terms mentioned by both the question and the answer.</p><formula xml:id="formula_0" coords="7,182.40,139.17,59.76,9.16">FrenchFrench</formula><p>For question 1 "Qu'estce qu'Atlantis ?" ("What is Atlantis ?"), the question is translated from the English sentence "What is Atlantis ?" but in fact, the word "Atlantis" is normally translated by "Atlantide" in French and "Atlantis" is only kept when you want to speak of the spatial shuttle.</p><p>More generally, in comparing the monolingual and multilingual results, one observes that the longer the question is, the less proper names are present, and the less the results are satisfying. The quality loss is estimated to about 15 % for the questions of the "definition" type, and near 50 % for factual questions with temporal anchor. On the 200 questions used for the evaluation, 17 had no proper names and no dates. The following table provides the results of our runs for these questions :</p><formula xml:id="formula_1" coords="7,151.44,443.25,280.32,216.52">FRFR ENFR 1 ENFR 2 PTFR 18 R W R W 24 R R R R 59 W W W W 64 R W W W 79 R W W W 100 R W W X 104 W W W W 109 W W W W 117 R W W W 118 R W W W 133 X W W W 144 W R R R 164 R W W W 166 R R R R 188 W W W W 189 R R W R 199 W W W W 58 % 24 % 24 % 24 %</formula><p>The table shows that for theses questions which quality of the translation is a crucial issue, the results are heavily deteriorated. Question 144, for which only the monolingual run returns an error, was a NIL question for which the FrenchFrench module returns, in despite, an answer while the other modules are returning a NIL.</p><p>Priberam, the company responsible for the Portuguese module in our engine, participated in CLEF 2006 for Portuguese and Spanish evaluation tracks. It is interesting to note that they obtained results very similar to our results for the Portuguese monolingual run <ref type="bibr" coords="7,247.44,746.85,11.75,9.16" target="#b0">[1]</ref> [2] and have similar degradations of results from monolingual to multilingual runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Outlines</head><p>Our CLEF 2006 results are noticeably better than those of our CLEF 2005 campaign, furthermore if one consider that "list" questions had been added this year. Notable too is our EnglishFrench run , using our Italian partner's English module, returned less good results in 2006 versus 2005 ((32,5% v 39,5%), this confirming the overall greater difficulty attached to the questions this year.</p><p>The following modifications have generated the following improvements:</p><p>• Revised processing of the NIL questions. Although this is of little interest to the user, often requesting replies even inexact ones, the revision was implemented for CLEF and its evaluations. The end result has been to reach far better "precision" and "recall"rates for this type of questions.</p><p>• Slightly improved translations and primarily the use of the multilingual lexicon of translated proper names and acronyms. • Betterment of the resources and the algorithm to detect the type of the question.</p><p>In respect of the deployed efforts, the development costs engaged and the resources upgrading, the global improvement of the results is not astonishing. It seems that the algorithms used by our modules find their limits around 70% of satisfying answers ! However, noticing that only 20 % of the answers are marked "wrong" in FrenchFrench, one may think that a revision of the delimitation of the extracted replies could , in the near future, allow the reach of success rates around 80 %.</p><p>Numerous other developments and improvements have been incorporated into our system, specially initiated in the framework of our MCAST European project, but are not visible into the CLEF assessed results. For instance, the speed to return an answer has greatly improved, from an usual 3 seconds in 2005 to less than 1 second in 2006. This was obtained thanks to a preliminary fast analysis of the index returned sentences, which screens more than 80% of the sentences with no pivotwords of the question, hence a very weak probability to contain an answer.</p><p>After the CLEF 2005 evaluation, we had identified a few leads for improvement of our system. A few of them have been implemented this year, but a lot remains to be done. We have started the elaboration of a " knowledge base", from a Webbased data extraction, and currently targeting geographical data (country, province, town names) , the whole to be extended, in the forthcoming months, to people names and events. We still not take into account the presentation of the document, and the answer extraction should be revised as still too imprecise.</p><p>As for the next coming years, our technology and system should evolve considerably as our firm is a partner of the FrancoGerman Quaero project, taking in charge the Question Aswering issue for the French and English languages. Within this project, in partnership with the firm -Exalead having developed the search engine "Eponyme", we should market a general consumer and a professional version of our system, on closed corpora but more rewardingly on the billion Web pages. In this respect, our strategy should evolve towards a semantic tagging of the Web pages with indexation of the tagged items, in view to find the answers to a question within a timeframe of 1/10 th or 2/10 th of a second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Despite the introduction of the questions of the « list » type , more difficult to process than the « definition » or « factual » ones, our system improved its results both in mono and multilingual mode. For factual questions, QRISTAL returns around 70 % of exact answers in monolingual mode and almost 50 % in multilingual mode.</p><p>A finetuned analysis of the results shows that the quality of the answer extraction still can be markedly improved as nearly 10 % of the answers have been assessed as "inexact" ("X") or nonjustified ("U") in French French mode, while the proportion of answers qualified as "false" was barely above 20 % in the same mode.</p><p>In the time elapsing between the 2 campaigns, our firm has invested almost 4 man/years in its system, but most developments were carried out on other areas that the quality of the said system. This was true for the answer delivering speed, the multitask operations, the Web accessibility, and the new English language module. We consider the real system improvement as having requested 1 man/year, an important investment for a comparatively reduced improvement.</p><p>The incorporation of our system into a Webfocused search engine through the Quaero project, is to necessitate in the coming years, a comprehensive revision of our methods in view to return the most precise answers in less than 2/10th of a second. So, the core of the syntactic and semantic analysis processing will be batch processed and will produce a semantic tagging of the Web pages (or of the documents in closed corpora), along with the indexation of these tags (named entities, possible answers per type of question, keywords, etc.). Furthermore, the presence of a knowledge base fed and updated permanently via the Web, should permit the development of verification procedures of the answers, hence reducing the related errors. These procedures are of the most importance as, beside the delivered improvements, they avoid the delivery of nonsensical or absurd answers that correspondingly generate a user's suspicion on the reliability of the system.</p><p>At a time when the Question Answering Systems find their first business use in firms or for the general audience, it is vital for these QA systems to avoid replicating the errors made while introducing the first grammar checking or voice recognition systems on the markets. As the average quality of these later systems in their initial versions has largely deceived their users, to the extent that any further satisfying development did not compensate for the deception and are still viewed as "unusable and of no interest"!</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,70.80,404.48,6.00,10.91;5,92.40,404.48,116.88,10.91;5,70.80,430.05,453.52,9.16;5,70.80,442.05,453.36,9.16;5,70.80,454.05,453.36,9.16;5,70.80,466.05,293.88,9.16"><head>4</head><label></label><figDesc>Results for CLEF 2006 QRISTAL was evaluated for CLEF 2006 for French to French, English to French (Synapse Module and Expert Systems module), and Portuguese to French. That is 1 monolingual and 3 multilingual campaigns. For each one of these tasks, we processed only one run. Note that results obtained in CLEF 2006 could have been obtained with our June 2006 commercial version of our Qristal software.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,224.64,737.25,145.92,9.16;5,71.04,487.44,452.88,247.20"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Results of the general task</figDesc><graphic coords="5,71.04,487.44,452.88,247.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,188.64,459.81,217.84,9.16;6,71.04,174.96,452.88,270.24"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3 : Results of our 4 runs for each question type</figDesc><graphic coords="6,71.04,174.96,452.88,270.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,106.56,240.69,409.68,45.16"><head></head><label></label><figDesc>Thanks to these dictionaries, the deterioration between the FrenchFrench and FrenchPortuguese pairs has dropped significantly ie 43 % (2005) to 31 % (2006) detailed in the following : (a -CLEF 2005 ) from 64 % in FrenchFrench and 36.5% in PortugueseFrench to (b CLEF 2006) 68% in FrenchFrench and 47% in PortugeseFrench).</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors thank all the engineers and linguists that took part in the development of QRISTAL. They also thank the <rs type="institution">Italian company Expert System</rs> and the <rs type="institution">Portuguese company Priberam</rs> for allowing them to use their modules for question analysis in English and Portuguese. They finally thank the <rs type="institution">European Commission</rs> which supported and still supports our development efforts through <rs type="funder">TRUST</rs> and MCAST projects, and our coordinator <rs type="person">Christian Gronoff</rs> from Semiosphere.</p><p>Last but not least, authors thank <rs type="person">Carol Peters</rs>, <rs type="person">Danilo Giampiccolo</rs> and <rs type="person">Christelle Ayache</rs> for the remarkable organization of CLEF.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,87.36,583.89,436.96,9.16;9,70.80,595.49,453.60,9.69;9,70.80,607.49,47.16,9.69" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,390.24,583.89,134.08,9.16;9,70.80,595.89,163.85,9.16">Design &amp; Implementation of a Semantic Search Engine for Portuguese</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Amaral</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Pinto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,242.88,595.49,281.52,9.69;9,70.80,607.49,42.87,9.69">Proceedings of the Fourth Conference on Language Resources and Evaluation</title>
		<meeting>the Fourth Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,86.88,631.89,437.28,9.16;9,70.80,643.49,453.48,9.69;9,70.80,655.89,32.52,9.16" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="9,442.56,631.89,81.60,9.16;9,70.80,643.49,340.93,9.69">Priberam&apos;s question answering system for Portuguese, Working Notes for the CLEF 2005 Workshop</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Amaral</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Figueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Pinto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005. 2123 September</date>
			<pubPlace>Wien, Austria</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.36,679.89,436.80,9.16;9,70.80,691.49,430.92,9.69" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,274.80,679.89,249.36,9.16;9,70.80,691.89,66.71,9.16">Campagne d&apos;évaluation EQueREVALDA : Évaluation en questionréponse</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vilnat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,144.72,691.49,24.56,9.69">TALN</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6372</biblScope>
			<date type="published" when="2005">2005. 2005. juin 2005</date>
			<publisher>Ateliers &amp; Tutoriels</publisher>
			<pubPlace>Dourdan, France</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,85.68,715.89,438.60,9.16;9,70.80,727.49,379.56,9.69" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,324.72,715.89,194.87,9.16">Exploiting Redundancy in Question Answering</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">R</forename><surname>Lynam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,70.80,727.49,326.06,9.69">Proceedings of 24th Annual International ACM SIGIR Conference (SIGIR 2001)</title>
		<meeting>24th Annual International ACM SIGIR Conference (SIGIR 2001)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page">358365</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,84.96,87.17,439.36,9.69;10,70.80,99.17,178.68,9.69" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="10,161.28,87.17,363.04,9.69;10,70.80,99.17,51.37,9.69">L&apos;évaluation des systèmes de questionréponse, Évaluation des systèmes de traitement de l&apos;information</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Grau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Lavoisier</publisher>
			<biblScope unit="page">7798</biblScope>
		</imprint>
		<respStmt>
			<orgName>TSTI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,85.68,123.57,438.72,9.16;10,70.80,135.17,453.52,9.69;10,70.80,147.17,105.72,9.69" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,459.36,123.57,65.04,9.16;10,70.80,135.57,267.54,9.16">Answer Mining by Combining Extraction Techniques with Abductive Reasoning</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bensley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,346.80,135.17,177.52,9.69;10,70.80,147.17,77.28,9.69">Proceedings of The Twelfth Text Retrieval Conference (TREC</title>
		<meeting>The Twelfth Text Retrieval Conference (TREC</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,87.60,171.57,437.04,9.16;10,70.80,183.17,432.12,9.69" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,447.60,171.57,77.04,9.16;10,70.80,183.57,92.24,9.16">The University of Amsterdam at QALEF</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>De Rijke M</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schlobach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,191.04,183.17,187.58,9.69">Working Notes of the Workshop of CLEF 2004</title>
		<meeting><address><addrLine>Bath</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1517-09">2004. 2004. 1517 september 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,84.96,207.57,439.20,9.16;10,70.80,219.17,453.60,9.69;10,70.80,231.17,158.76,9.69" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,342.00,207.57,182.16,9.16;10,70.80,219.57,229.77,9.16">Multilingual Semantic and Cognitive Search Engine for Text Retrieval Using Semantic Technologies</title>
		<author>
			<persName coords=""><forename type="first">Laurent</forename><forename type="middle">D</forename><surname>Varone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Amaral</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fuglewicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,308.40,219.17,216.00,9.69;10,70.80,231.17,95.13,9.69">First International Workshop on Proofing Tools and Language Technologies</title>
		<meeting><address><addrLine>Patras, Grèce</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,84.96,255.17,439.32,9.69;10,70.80,267.57,250.44,9.16" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,226.32,255.57,174.26,9.16">QRISTAL, système de QuestionsRéponses</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Seguela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,408.48,255.17,24.56,9.69">TALN</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5362</biblScope>
			<date type="published" when="2005">2005. 2005. juin 2005</date>
			<publisher>Conférences principales</publisher>
			<pubPlace>Dourdan, France</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,89.52,291.57,434.96,9.16;10,70.80,303.17,238.20,9.69" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,270.00,291.57,254.48,9.16;10,70.80,303.57,18.14,9.16">CrossLingual Question Answering using QRISTAL for CLEF 2005</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nègre</forename><forename type="middle">S</forename><surname>Séguéla P</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,96.24,303.17,45.26,9.69">CLEF 2005</title>
		<meeting><address><addrLine>Wien, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005. 2123 september 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.84,327.17,430.56,9.69;10,70.80,339.17,145.08,9.69" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,186.48,327.57,231.90,9.16">Industrial concerns of a QuestionAnswering system ?</title>
		<author>
			<persName coords=""><forename type="first">Laurent</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,426.72,327.17,97.68,9.69;10,70.80,339.17,23.14,9.69">EACL 2006, Workshop KRAQ</title>
		<meeting><address><addrLine>Trento, Italia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-04-03">2006. April 3 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,363.17,434.32,9.69;10,70.80,375.57,89.16,9.16" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,272.40,363.57,80.94,9.16">QA better than IR ?</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nègre</forename><forename type="middle">S</forename><surname>Séguéla P</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,360.00,363.17,134.29,9.69">EACL 2006, Workshop MLQA&apos;06</title>
		<meeting><address><addrLine>Trento, Italia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-04-04">2006. April 4 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.24,399.57,434.20,9.16;10,70.80,411.17,453.52,9.69;10,70.80,423.17,223.32,9.69" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,122.64,411.57,299.10,9.16">Overview of the CLEF 2004 Multilingual Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vallin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Erbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>De Rijke M</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simov</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sutcliffe R</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,431.28,411.17,93.04,9.69;10,70.80,423.17,99.02,9.69">Working Notes of the Workshop of CLEF 2004</title>
		<meeting><address><addrLine>Bath</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1517-09">2004. 1517 september 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,91.20,447.17,433.08,9.69;10,70.80,459.57,77.16,9.16" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,169.44,447.57,206.87,9.16">From Document Retrieval to Question Answering</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="10,385.20,447.17,104.00,9.69">ILLC Dissertation Series</title>
		<imprint>
			<biblScope unit="volume">20034</biblScope>
			<date type="published" when="2003">2003</date>
			<publisher>ILLC</publisher>
			<pubPlace>Amsterdam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.16,483.57,432.24,9.16;10,70.80,495.57,221.16,9.16" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="10,207.60,483.57,244.62,9.16">Overview of the TREC 2003 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="middle">M</forename><surname>Voorhees E</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec12/t12_proceedings.html" />
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>NIST</publisher>
			<biblScope unit="volume">5468</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
