<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,100.04,148.86,402.93,15.15;1,174.39,170.78,254.21,15.15">CLEF2006 Question Answering Experiments at Tokyo Institute of Technology</title>
				<funder ref="#_gTW2uWh">
					<orgName type="full">Japanese government</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,125.95,204.67,78.87,8.74"><forename type="first">E</forename><forename type="middle">W D</forename><surname>Whittaker</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<addrLine>2-12-1, Ookayama, Meguro-ku</addrLine>
									<postCode>152-8552</postCode>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,212.88,204.67,46.22,8.74"><forename type="first">J</forename><forename type="middle">R</forename><surname>Novak</surname></persName>
							<email>novakj@furui.cs.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<addrLine>2-12-1, Ookayama, Meguro-ku</addrLine>
									<postCode>152-8552</postCode>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,267.39,204.67,44.97,8.74"><forename type="first">P</forename><surname>Chatain</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<addrLine>2-12-1, Ookayama, Meguro-ku</addrLine>
									<postCode>152-8552</postCode>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,320.39,204.67,46.24,8.74"><forename type="first">P</forename><forename type="middle">R</forename><surname>Dixon</surname></persName>
							<email>dixonp@furui.cs.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<addrLine>2-12-1, Ookayama, Meguro-ku</addrLine>
									<postCode>152-8552</postCode>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,374.77,204.67,44.55,8.74"><forename type="first">M</forename><forename type="middle">H</forename><surname>Heie</surname></persName>
							<email>heie@furui.cs.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<addrLine>2-12-1, Ookayama, Meguro-ku</addrLine>
									<postCode>152-8552</postCode>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,442.02,204.67,35.04,8.74"><forename type="first">S</forename><surname>Furui</surname></persName>
							<email>furui@furui.cs.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<addrLine>2-12-1, Ookayama, Meguro-ku</addrLine>
									<postCode>152-8552</postCode>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,100.04,148.86,402.93,15.15;1,174.39,170.78,254.21,15.15">CLEF2006 Question Answering Experiments at Tokyo Institute of Technology</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F3EFEA9C3F1E5F3A189FDD400071303C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>Measurement, Performance, Experimentation Question answering, Statistical classification, Cross-language, Spanish, French</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present the experiments performed at Tokyo Institute of Technology for the CLEF2006 Multiple Language Question Answering (QA@CLEF) track. Our approach to question answering centres on a non-linguistic, data-driven, statistical classification model that uses the redundancy of the web to find correct answers. Using this approach a system can be trained in a matter of days to perform question answering in each of the target languages we considered-English, French and Spanish. For the cross-language aspect we employed publicly available web-based text translation tools to translate the question from the source into the corresponding target language, then used the corresponding mono-lingual QA system to find the answers. The hypothesised correct answers were then projected back on to the appropriate closed-domain corpus.</p><p>Correct and supported answer performance on the mono-lingual tasks was around 14% for both Spanish and French. Performance on the cross-lingual tasks ranged from 5% for Spanish-English, to 12% for French-Spanish. Our projection method was shown not to work well: in the worst case on the French-English task we lost 84% of our otherwise correct answers. Ignoring the need for correct support information the exact answer accuracy increased to 29% and 21% correct on the Spanish and French mono-lingual tasks, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper we describe how we applied our recently developed statistical, data-driven approach to question answering (QA) to the task of multiple language question answering in the CLEF2006 QA evaluation. The approach that we used, described in detail in previous publications <ref type="bibr" coords="1,464.54,745.64,15.50,8.74" target="#b11">[14,</ref><ref type="bibr" coords="1,482.40,745.64,12.73,8.74" target="#b12">15,</ref><ref type="bibr" coords="1,497.51,745.64,11.62,8.74" target="#b13">16]</ref>, uses a noisy-channel formulation of the question answering problem and the redundancy of data on the web to answer questions. Our approach permits the rapid development of factoid QA systems in new languages given the availability of suitable question and answer training examples and a large corpus of text data such as web-pages or newspaper text as described in <ref type="bibr" coords="2,432.86,147.89,14.61,8.74" target="#b14">[17]</ref>.</p><p>Although we had previously developed systems in five different languages using the same method none of these languages except English were included in the CLEF2006 evaluation. We therefore chose to build French and Spanish systems from scratch and participate in the French and Spanish mono-lingual tasks and the cross-language combinations of both languages together with English. Using the procedure applied successfully in <ref type="bibr" coords="2,345.79,207.66,15.50,8.74" target="#b14">[17]</ref> we developed first-cut French and Spanish systems in a couple of days and used the remaining time before the actual evaluation for system optimisation on previous years' CLEF evaluation questions <ref type="foot" coords="2,380.61,230.00,3.97,6.12" target="#foot_0">1</ref> .</p><p>Our approach is substantially different to conventional approaches to QA though it shares elements of other statistical, data-driven approaches to factoid question answering found in the literature <ref type="bibr" coords="2,132.86,267.44,10.51,8.74" target="#b0">[1,</ref><ref type="bibr" coords="2,145.77,267.44,7.75,8.74" target="#b1">2,</ref><ref type="bibr" coords="2,155.92,267.44,7.75,8.74" target="#b2">3,</ref><ref type="bibr" coords="2,166.07,267.44,7.75,8.74" target="#b4">7,</ref><ref type="bibr" coords="2,176.21,267.44,12.73,8.74" target="#b8">11,</ref><ref type="bibr" coords="2,191.34,267.44,12.73,8.74" target="#b9">12,</ref><ref type="bibr" coords="2,206.47,267.44,11.63,8.74" target="#b10">13]</ref>. More recently, similar approaches to the answer typing employed in our system have appeared <ref type="bibr" coords="2,203.17,279.39,10.52,8.74" target="#b6">[9]</ref> although they still use linguistic notions that our approach eschews in favour of data-driven classifications. While this approach results in a small number of parameters that must be optimised to minimise the effects of data sparsity and to make the search space tractable, we largely remove the need for numerous ad-hoc weights and heuristics that are an inevitable feature of many rule-based systems. The software for each language-specific QA system is identical; only the training data differs. All model parameters are determined when the data is loaded at system initialisation; this typically only takes a couple of minutes to compute and they do not change in between questions. New data or system settings can therefore be easily applied without the need for time-consuming model re-training.</p><p>Many contemporary approaches to QA require the specialised skills of native speaking linguistic experts for the construction of rules and databases that are often used by other QA systems. In contrast, our method allows us to include all kinds of dependencies in a consistent manner and has the important benefits that it is fully trainable and requires minimal human intervention once sufficient data is collected. This was particularly important in the case of our participation in CLEF this year since although our French QA system was developed by a native French-speaker, our Spanish system was built by a student with a conversational level of Spanish learnt in school.</p><p>Our QA systems tend to work well when there are numerous (redundant) sentences that contain the correct answer which is why a web search engine is used to obtain nominally relevant documents. In particular, it is advantageous for good performance that the correct answer co-occurs more frequently (roughly speaking) with words from the question than other candidate answers of the same answer type do. If this is not the case, the QA system has no other information with which to differentiate the correct answer from competing alternatives of the same answer type. By using a large amount of text data that contain the question words we are essentially replacing query expansion (as performed by most QA systems) with what might be called document expansion: the documents are expanded to match the query rather than expanding the query to match the documents. Due to the evaluation requirement that support from a fixed document collection be provided for each question, our answers must subsequently be projected on to the appropriate collection. Inevitably this is a lossy operation as will be discussed in Section 5 and also means we never attempt to predict "unanswerable" questions by giving "NIL" as an answer.</p><p>The rest of this paper is organised as follows: we first present a summary in Section 2 of the mathematical framework for factoid QA as a classification task that was presented in <ref type="bibr" coords="2,474.40,638.05,14.61,8.74" target="#b12">[15]</ref>. We describe the experimental setup specific to the CLEF2006 evaluation in Section 3 and present the results on each task that were obtained in the evaluation in Section 4. A discussion and conclusion are given in Sections 5 and 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">QA as statistical classification with non-linguistic features</head><p>This section is re-produced verbatim from the paper "TREC2005 Question Answering Experiments at Tokyo Institute of Technology" <ref type="bibr" coords="3,239.25,145.80,15.49,8.74" target="#b11">[14]</ref>.</p><p>It is clear that the answer to a question depends primarily on the question itself but also on many other factors such as the person asking the question, the location of the person, what questions the person has asked before, and so on. Although such factors are clearly relevant in a real-world scenario they are difficult to model and also to test in an off-line mode, for example, in the context of the TREC evaluations. We therefore choose to consider only the dependence of an answer A on the question Q, where each is considered to be a string of l A words A = a 1 , . . . , a l A and l Q words Q = q 1 , . . . , q l Q , respectively. In particular, we hypothesise that the answer A depends on two sets of features W = W(Q) and X = X (Q) as follows:</p><formula xml:id="formula_0" coords="3,246.94,265.35,266.07,8.74">P (A | Q) = P (A | W, X),<label>(1)</label></formula><p>where W = w 1 , . . . , w l W can be thought of as a set of l W features describing the "question-type" part of Q such as when, why, how, etc. and X = x 1 , . . . , x l X is a set of l X features comprising the "information-bearing" part of Q i.e. what the question is actually about and what it refers to. For example, in the questions, Where was Tom Cruise married? and When was Tom Cruise married? the information-bearing component is identical in both cases whereas the question-type component is different.</p><p>Finding the best answer Â involves a search over all A for the one which maximises the probability of the above model:</p><formula xml:id="formula_1" coords="3,247.49,400.31,110.66,17.10">Â = arg max A P (A | W, X).</formula><p>(</p><formula xml:id="formula_2" coords="3,504.51,402.83,8.49,8.74">)<label>2</label></formula><p>This is guaranteed to give us the optimal answer in a maximum likelihood sense if the probability distribution is the correct one. We don't know this and it's still difficult to model so we make various modelling assumptions to simplify things. Using Bayes' rule this can be rearranged as</p><formula xml:id="formula_3" coords="3,238.94,489.01,274.06,22.31">arg max A P (W, X | A) • P (A) P (W, X) .<label>(3)</label></formula><p>The denominator can be ignored since it is common to all possible answer sequences and does not change. Further, to facilitate modelling we make the assumption that X is conditionally independent of W given A to obtain:</p><formula xml:id="formula_4" coords="3,222.27,573.46,286.49,14.58">arg max A P (X | A) • P (W | A) • P (A). (<label>4</label></formula><formula xml:id="formula_5" coords="3,508.76,573.46,4.24,8.74">)</formula><p>Using Bayes rule, making further conditional independence assumptions and assuming uniform prior probabilities, which therefore do not affect the optimisation criterion, we obtain the final optimisation criterion:</p><formula xml:id="formula_6" coords="3,237.37,651.17,271.39,33.18">arg max A P (A | X) retrieval model • P (W | A) f ilter model . (<label>5</label></formula><formula xml:id="formula_7" coords="3,508.76,651.17,4.24,8.74">)</formula><p>The P (A | X) model is essentially a language model which models the probability of an answer sequence A given a set of information-bearing features X, similar to the work of <ref type="bibr" coords="3,448.00,708.07,14.61,8.74" target="#b7">[10]</ref>. It models the proximity of A to features in X. We call this model the retrieval model and do not examine it further-please refer to <ref type="bibr" coords="3,204.73,731.98,15.50,8.74" target="#b11">[14,</ref><ref type="bibr" coords="3,223.55,731.98,12.73,8.74" target="#b12">15,</ref><ref type="bibr" coords="3,239.60,731.98,12.73,8.74" target="#b13">16]</ref> for more details.</p><p>The P (W | A) model matches an answer A with features in the question-type set W . Roughly speaking this model relates ways of asking a question with classes of valid answers. For example, it associates dates, or days of the week with when-type questions. In general, there are many valid and equiprobable A for a given W so this component can only re-rank candidate answers retrieved by the retrieval model. If the filter model were perfect and the retrieval model were to assign the correct answer a higher probability than any other answers of the same type the correct answer should always be ranked first. Conversely, if an incorrect answer, in the same class of answers as the correct answer, is assigned a higher probability by the retrieval model we cannot recover from this error. Consequently, we call it the filter model and examine it further in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Filter model</head><p>The question-type mapping function W(Q) extracts n-tuples (n = 1, 2, . . .) of question-type features from the question Q, such as How, How many and When were. A set of |V W | = 2522 single-word features is extracted based on frequency of occurrence in questions in previous TREC question sets. Some examples include: when, where, who, whose, how, many, high, deep, long etc.</p><p>Modelling the complex relationship between W and A directly is non-trivial. We therefore introduce an intermediate variable representing classes of example questions-and-answers (q-anda) c e for e = 1 . . . |C E | drawn from the set C E , and to facilitate modelling we say that W is conditionally independent of c e given A as follows:</p><formula xml:id="formula_8" coords="4,211.14,368.81,301.86,31.06">P (W | A) = |C E | e=1 P (W, c e | A)<label>(6)</label></formula><p>=</p><formula xml:id="formula_9" coords="4,280.91,404.96,232.09,31.06">|C E | e=1 P (W | c e ) • P (c e | A).<label>(7)</label></formula><p>Given a set E of example q-and-a t j for j = 1 . . . |E| where t j = (q j 1 , . . . , q j l Q j , a j 1 , . . . , a j l A j ) we define a mapping function f : E → C E by f (t j ) = e. Each class c e = (w e 1 , . . . , w e l W e , a e 1 , . . . , a e l A e ) is then obtained by c e = j:f (tj )=e</p><formula xml:id="formula_10" coords="4,234.51,479.55,40.77,25.88">W(t j ) l A j i=1</formula><p>a j i , so that:</p><formula xml:id="formula_11" coords="4,174.30,511.11,338.71,31.06">P (W | A) = |C E | e=1 P (W | w e 1 , . . . , w e l W e ) • P (a e 1 , . . . , a e l A e | A).<label>(8)</label></formula><p>Assuming conditional independence of the answer words in class c e given A, and making the modelling assumption that the jth answer word a e j in the example class c e is dependent only on the jth answer word in A we obtain:</p><formula xml:id="formula_12" coords="4,209.14,599.07,299.62,31.18">P (W | A) = |C E | e=1 P (W | c e ) • l A e j=1 P (a e j | a j ). (<label>9</label></formula><formula xml:id="formula_13" coords="4,508.76,610.34,4.24,8.74">)</formula><p>Since our set of example q-and-a cannot be expected to cover all the possible answers to questions that may be asked we perform a similar operation to that above to give us the following:</p><formula xml:id="formula_14" coords="4,181.98,676.55,331.02,31.18">P (W | A) = |C E | e=1 P (W | c e ) l A e j=1 |C A | a=1 P (a e j | c a )P (c a | a j ),<label>(10)</label></formula><p>where c a is a concrete class in the set of |C A | answer classes C A . The independence assumption leads to underestimating the probabilities of multi-word answers so we take the geometric mean of the length of the answer (not shown in Equation ( <ref type="formula" coords="4,321.24,744.77,8.64,8.74" target="#formula_14">10</ref>)) and normalise P (W | A) accordingly.</p><p>The system using the above formulation of filter model given by Equation ( <ref type="formula" coords="5,439.66,112.02,8.86,8.74" target="#formula_14">10</ref>) is referred to as model ONE. Systems using the model given by Equation ( <ref type="formula" coords="5,363.17,123.98,4.24,8.74" target="#formula_11">8</ref>) are referred to as model TWO.</p><p>Only systems based on model ONE were used in the CLEF2006 evaluation systems described in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental setup for CLEF2006</head><p>The CLEF2006 tasks we took part in are as follows: F-F, S-S, F-S, E-S, E-F, F-E and S-E where F=French, S=Spanish and E=English. For each task we submitted only one run and up to ten ranked answers for each question. No classification as to whether a question was more likely to be a factoid, definition or list question was performed prior to answering a question. Therefore all questions were treated as factoid questions since the QA systems we developed were trained using only factoid questions and the features extracted from factoid questions.</p><p>For the two mono-lingual tasks (French and Spanish) questions were passed as-is to the appropriate mono-lingual system after minimal query normalisation and upper-casing of all question terms. For the cross-lingual tasks questions were first translated into the target language using web-based text translation tools: Altavista's Babelfish <ref type="bibr" coords="5,331.97,310.25,10.52,8.74">[5]</ref> for French-Spanish and Google Translate [6] for all other combinations. The translated question was then normalised and upper-cased and passed to the appropriate mono-lingual system <ref type="foot" coords="5,313.61,332.58,3.97,6.12" target="#foot_1">2</ref>For each question input to our QA system the question was passed to Google after removing stop words and the (up to) top 500 documents were downloaded for each question. For answering a specific question only that question's downloaded data was used. Document processing involved the removal of any document markup, conversion to UTF-8, and the same language-specific normalisation and upper-casing as applied to the questions.</p><p>For answering questions in a given language the corpus in which answers were to be located was not used. However, once a set of answers to a question had been obtained the final step was to project the answers on to the appropriate corpus. Due to a lack of time and resources for a full development of the projection system we relied on using Lucene <ref type="bibr" coords="5,424.14,441.76,10.52,8.74" target="#b3">[4]</ref> to determine the document which had the highest ranking when the answer and question were used as a Boolean query. Snippets were likewise determined using Lucene's summary function. If an answer could be found somewhere in the document collection the snippets were further filtered to ensure that the snippet always included the answer string (though possibly none of the question words).</p><p>Due to time limitations we chose only to implement the French and Spanish system using model ONE, given by Equation <ref type="bibr" coords="5,231.81,513.49,16.39,8.74" target="#b7">(10)</ref>. Although we have implementations for English using both models ONE and TWO, for consistency we used the English system that implemented model ONE only. In the TREC2005 QA evaluation model TWO outperformed model ONE by approximately 50% relative-our aim is therefore to implement model TWO for the Spanish real-time task and in other languages in time for future CLEF evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>All tasks were composed of a set of 190 factoid and definition questions and 10 list questions. For all tasks up to 10 answers were given by our QA systems to all questions. In general the top 3 answers for the factoid/definition questions were assessed and all list answers were assessed for exactness and support.</p><p>In Table <ref type="table" coords="5,145.05,663.89,4.98,8.74" target="#tab_0">1</ref> a breakdown is given of the results obtained on the two mono-lingual (French and Spanish) tasks for factoid/definition questions with answers in first place and for all answers to list questions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>There were four main factors in our submissions to CLEF2006 that were expected to have a large impact on performance: (1) the mis-match in time period between the document collection and the web documents used for answering questions; (2) the use of factoid QA systems to also answer definition and list questions; (3) the effect of the machine translation tools for the cross-language tasks; and (4) the projection method of mapping answers back on to the appropriate document collection. Since all our QA systems relied on web data to answer questions for all languages there was an inevitable mis-match in the time period of documents used for answering questions and the time-frame that was meant to be used i.e. 1994-1995. Although web documents exist which cover the same period, web search engines typically return more recent documents. However, it turned out that this was not a major problem although there were inconsistencies for questions such as "¿Quién es el presidente de Letonia?"/"Qui est le président de la Létonie<ref type="foot" coords="6,401.10,571.87,3.97,6.12" target="#foot_3">4</ref> ?"/"Who is the president of Latvia?" and "¿Quién es el secretario general de la Interpol?"/"Qui est le secrétaire général d'Interpol?"/"Who is the secretary general of Interpol?" the answers to which have changed in the intervening period.</p><p>It was observed during our participation in the TREC2005 evaluations that simply using a factoid QA system to output the top so many answers for list questions was not a very promising approach, even when list questions were used for training. Part of the problem was due to a paucity of list question training examples compared to the number of factoid questions available. Another problem lay in how to determine the threshold for outputting answers: whether simply to output a fixed number of answers each time, or to base it on some function of the answer score. In the CLEF2006 evaluations the problem was further compounded by not knowing in advance which questions would be factoid, definition and list questions. We therefore decided to assume all questions were factoids and output ten answers in all cases. Our poor performance on all list questions for all tasks can be attributed mostly to there being very few list question examples in our training data and very few list question features (such as plurals) used in the filter model. As a consequence, answer typing for list questions was not very effective. For definition questions the independence assumptions made by model ONE render very poor answer typing of definition questions unless an answer is able to be defined in one or two words.</p><p>The substantially lower answer accuracies (between 3.7% and 12.0%) obtained on the crosslanguage tasks where Babelfish and Google Translate were used for question translation were generally expected due to the well documented quality of such translation tools. It was deemed unlikely that the highest result that was obtained, for French-Spanish, was due to using Babelfish rather than Google Translate and was instead due more to the relative similarity of the two languages (see Section 5.1). In any case, further improvements in machine-translation techniques will almost certainly result in considerable improvements in our cross-language QA performance and multi-language combination experiments.</p><p>We were far more surprised and disappointed by the loss incurred by our projection method which reduced our set of correct answers by 47% and 31% on the Spanish and French monolingual tasks, respectively. If we were to ignore the need for correct support information the performance would increase to 29% and 21% correct on the Spanish and French mono-lingual tasks, respectively. In the worst case on the French-English task we lost 84% of our otherwise correct answers; equivalently we would have obtained an exact answer accuracy of 23% if the support requirement were ignored. Our previous experience with projection onto the AQUAINT document collection for English language answers on the TREC2005 QA task using the algorithm included in the open-source Aranea system <ref type="bibr" coords="7,282.12,375.03,10.52,8.74" target="#b5">[8]</ref> had shown fairly consistent losses of around 20%. While the algorithm that we applied in CLEF2006 was far simpler than that employed by Aranea, it did have access to the full document collection for finding documents containing answers whereas for TREC we relied only on the (up to) top 1000 documents supplied by NIST that were obtained using the PRISE retrieval system. This prevented any errors from only retrieving documents that were selected using only question features however the increased recall of documents containing the answers might have been offset by lower precision.</p><p>The Spanish system, like the French system, was developed in a very short period of time. Making further refinements, increasing the amount of training data used, and implementing model TWO are expected to bring accuracy into line with the English system. The possible advantages of applying a refined cross-language approach to mono-lingual tasks, e.g. using the combined results of multiple mono-lingual systems to answer questions in a particular language, are also being investigated. This will provide a means of further exploiting the redundancy of the web, as well as a method to improve the results for languages which are still under-represented on the web.</p><p>In the next two sections we present brief language-specific discussions of the results that were obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Spanish</head><p>As indicated in Table <ref type="table" coords="7,188.48,612.59,4.98,8.74" target="#tab_1">2</ref> results for the mono-lingual Spanish task were considerably better than those obtained for the cross-lingual tasks (English-Spanish, French-Spanish). The discrepancy between the results for the mono-lingual and cross-language Spanish test sets can be almost entirely explained in terms of the relative accuracy of the automatic translation tools used as an intermediate step to obtaining results for the latter. Furthermore, the difference between the results for the French-Spanish task and those for the English-Spanish task is almost certainly due to the relative closeness of the language pair, with Spanish and French both being members of the Romance family of languages, rather than the use of different automatic translation tools. These differences aside, results for all three Spanish tasks exhibited similar characteristics.</p><p>The results on all three tasks that included Spanish as a source or target language were by far the best for factoid questions, especially those whose answers could be categorised as names or dates. Of the 26 exactly correct and supported answers obtained for the Spanish mono-lingual task, a total of 20 consisted of proper names or dates, (11 dates, 9 proper names). If the 29 correct but unsupported answers are also taken into account, this total rises to 41, and accounts for approximately 75% of all correct answers obtained for this particular task.</p><p>Definition questions, in addition to being ambiguous in the evaluation sense, are much more difficult than factoids for our QA system to answer. Yet, despite treating all questions as inherently factoid, some interesting results were obtained for the Spanish mono-lingual task. In particular, these results included 2 exactly correct and supported answers, and 8 correct but unsupported answers in the definition category. A cursory analysis of the data revealed that each of these correct answers could be construed as the result of a categorisation process, whereby the subject of the question had been classified into a larger category, and this category was then returned as the answer, e.g. "¿Qué es la Quinua?" ("What is Quinua?") answer: [a] cereal, and "¿Quién fue Alexander Graham Bell?" ("Who was Alexander Graham Bell?") answer: [an] inventor. The system gives these 'category' words high scores due to the fact that they often appear in the context of proper nouns, where they are used as definitions, or as noun-qualifiers. However, because the system uses no explicit linguistic typology or categories, this results in occasional mismatches such as: "¿Quién es Nick Leeson?" ("Who is Nick Leeson?") (answer: Barings). This answer would be categorised as a retrieval error since 'Barings' is a valid answer type for a Who-question but its high co-occurrence with the subject of the question results in an overly high retrieval model score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">French</head><p>For the French mono-lingual task, unsupported answers were not as much an issue as for the Spanish mono-lingual task, although there were still 12 unsupported answers for the factoid questions, 10 or 11 of which would have also been exact. For the English-French task, there were 8 unsupported answers for factoid questions, almost all of which were also exact. Projection onto French documents, however imperfect, seems to have been less of a problem than for the other languages though it is unlikely that the differences are significant.</p><p>Out of our 27 correct and supported answers on the mono-lingual task, 23 were places, dates or names. For those types of questions the answer types that were returned were usually correct. Questions involving numbers, however, were a serious problem: out of 15 "How many..." questions we got only one correct, and the answer types which were given were not consistent instead being dates, numbers, names or nouns. The same observation holds for "How old was X when..." questions, which were all incorrectly answered, with varying answer types for the answers given. With a rule-based or regular-expressions-based system it is difficult to make such errors. However, with our probabilistic approach, in which no hard decisions are made and all types of answers are valid but with varying probabilities, it is entirely possible to incur such filter model errors. Although some cases would be trivially remedied with a simple regular-expression this is against our philosophy; instead we feel the problem should be solved through better parameter estimation and better modelling of the training data, rather than ad-hoc heuristics.</p><p>Another interesting observation on the mono-lingual task was that for 19 questions where the first answer was inexact, wrong, or unsupported we got an exact and supported answer in second place. For answers in third place the number of exact and supported answers was only 3. In most of these cases, the answer types were the same. This is untypical of our results obtained previously on English and Japanese where there is typically a significant drop in the number of correct answers at each increase in the rank of answers considered.</p><p>As with Spanish, automatic translation of the questions into other languages was far from perfect. One common problem was words with several meanings which were (correctly) translated into French using the wrong meaning, thus radically changing the meaning of keywords in the question. For example, in the English question "In which settlement was a mass slaughter of Muslims committed in 1995?" "settlement", is translated into "règlement". Consequently, answers given for this question related to the French legal system rather than a location. Moreover, it was apparent that Google Translate was far from optimal for translating questions presumably because source sentences are expected to be in the affirmative. Thus, "What is..." and "Which is..." became "Ce qui..." which our QA system tended to interpret as "Qui..." thus favouring a person or company as the answer type. Similarly "How old..." often became "Comment vieux..." rather than "Quel âge..." and so was answered as if it were a regular "How..." question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>With the results obtained in the CLEF2006 QA evaluation we feel we have proven the language independence and generality of our statistical data-driven approach. Comparable performance using model ONE has been obtained under evaluation conditions on the three languages of English, French and Spanish in both this evaluation and TREC2005. In addition, post-evaluation experimentation with Japanese <ref type="bibr" coords="9,201.66,238.52,15.50,8.74" target="#b13">[16]</ref> has confirmed the efficacy of the approach for an Asian language as well.</p><p>While the absolute performance of our QA systems falls short of that obtained by state-ofthe-art linguistics-based systems both the French and Spanish systems were developed only over the two months prior to the evaluation and use an absolute minimum of linguistic knowledge to answer questions in favour of using the redundancy of the web.</p><p>Further work will concentrate on how to answer questions using less redundant data through data-driven query expansion methods and also look at removing the independence assumptions made in the formulation of the filter model to improve question and answer typing accuracy. We expect that improvements made on language-specific systems will feed through to improvements in all systems and we hope to be able to compete in more and different language combinations in CLEF evaluations in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Online demonstration</head><p>A demonstration of the system using model ONE supporting questions in English, Japanese, Chinese, Russian, French, Spanish and Swedish can be found online at http://www.inferret.com</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,90.00,110.82,423.01,229.62"><head>Table 1 :</head><label>1</label><figDesc>Breakdown of performance on the French and Spanish mono-lingual tasks by type of question and assessment of answer, where right means exactly correct and supported.A similar breakdown for the five 3 cross-lingual tasks is given in Table2for factoid/definition questions with answers in first place and for all answers to list questions.</figDesc><table coords="6,108.20,110.82,384.62,229.62"><row><cell></cell><cell></cell><cell cols="3">Factoid/definition questions</cell><cell></cell><cell>List questions</cell><cell></cell><cell></cell></row><row><cell>Task</cell><cell></cell><cell>Right</cell><cell cols="6">ineXact Unsupp. CWS Right ineXact Unsupp. P@N</cell></row><row><cell>S-S</cell><cell cols="2">26 (13.7%)</cell><cell>1</cell><cell>29 0.035</cell><cell>3</cell><cell>0</cell><cell>0</cell><cell>0.03</cell></row><row><cell>F-F</cell><cell cols="2">27 (14.2%)</cell><cell>12</cell><cell>12 0.142</cell><cell>9</cell><cell>2</cell><cell>0</cell><cell>0.09</cell></row><row><cell></cell><cell></cell><cell cols="3">Factoid/definition questions</cell><cell></cell><cell>List questions</cell><cell></cell><cell></cell></row><row><cell>Task</cell><cell></cell><cell>Right</cell><cell cols="6">ineXact Unsupp. CWS Right ineXact Unsupp. P@N</cell></row><row><cell>E-F</cell><cell cols="2">19 (10.0%)</cell><cell>6</cell><cell>8 0.017</cell><cell>4</cell><cell>1</cell><cell>1</cell><cell>0.06</cell></row><row><cell>E-S</cell><cell>11</cell><cell>(5.8%)</cell><cell>0</cell><cell>10 0.005</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0.01</cell></row><row><cell>F-E</cell><cell>7</cell><cell>(3.7%)</cell><cell>10</cell><cell>37 0.003</cell><cell>1</cell><cell>3</cell><cell>15</cell><cell>0.01</cell></row><row><cell>S-E</cell><cell>10</cell><cell>(5.3%)</cell><cell>11</cell><cell>34 0.008</cell><cell>0</cell><cell>1</cell><cell>18</cell><cell>0.00</cell></row><row><cell>F-S</cell><cell cols="2">22 (11.6%)</cell><cell>0</cell><cell>15 0.037</cell><cell>2</cell><cell>0</cell><cell>0</cell><cell>0.02</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,90.00,353.29,423.00,20.69"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note coords="6,128.16,353.29,384.84,8.74;6,90.00,365.25,420.16,8.74"><p>Breakdown of performance on the English, French and Spanish cross-lingual combinations by type of question and assessment of answer, where right means exactly correct and supported.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,105.24,692.84,407.75,6.99;2,90.00,702.31,418.46,6.99"><p>Development of the QA system itself is relatively fast and straightforward-by far the most time-consuming part is the development of robust text download, extraction and text normalisation tools for any given language.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,105.24,706.73,407.75,6.99;5,90.00,716.20,423.00,6.99;5,90.00,725.66,270.92,6.99"><p>One alternative would have been to use the mono-lingual system of the source language to obtain answers then translate its answers into the target language. A combination of these two approaches could also have been used to try to minimise the effects of poor automatic translation performance.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="6,105.24,723.88,278.04,6.99"><p>Note that the Spanish-French cross-lingual task was not run in CLEF2006.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="6,105.24,733.38,407.75,6.99;6,90.00,742.84,225.72,6.99"><p>"Létonie" should have been written as "Lettonie" in the French mono-lingual test set; it was, however, written correctly in the French-Spanish and French-English test sets.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgements</head><p>This research was supported by the <rs type="funder">Japanese government</rs>'s <rs type="programName">21st century COE programme</rs>: "<rs type="projectName">Framework for Systematization and Application of Large-scale Knowledge Resources</rs>".</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_gTW2uWh">
					<orgName type="project" subtype="full">Framework for Systematization and Application of Large-scale Knowledge Resources</orgName>
					<orgName type="program" subtype="full">21st century COE programme</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,110.48,557.92,402.52,7.86;9,110.48,568.88,402.52,7.86;9,110.48,579.84,354.91,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,364.37,557.92,148.63,7.86;9,110.48,568.88,145.00,7.86">Bridging the Lexical Chasm: Statistical Approaches to Answer-Finding</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,277.44,568.88,235.56,7.86;9,110.48,579.84,260.60,7.86">Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 23rd annual international ACM SIGIR conference on Research and development in information retrieval<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,590.49,402.52,7.86;9,110.48,601.45,402.52,7.86;9,110.48,612.41,21.00,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,263.60,590.49,230.29,7.86">An Analysis of the AskMSR Question-answering System</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,110.48,601.45,397.09,7.86">Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,623.06,402.51,7.86;9,110.48,634.02,173.60,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,227.49,623.06,206.58,7.86">A Noisy-Channel Approach to Question Answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Echihabi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,455.89,623.06,57.10,7.86;9,110.48,634.02,144.15,7.86">Proceedings of the 41st Annual Meeting of the ACL</title>
		<meeting>the 41st Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,644.68,272.31,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="9,244.87,644.68,67.50,7.86">Lucene in Action</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Gospodnetic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hatcher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Manning</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,676.64,402.51,7.86;9,110.48,687.60,167.90,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,232.42,676.64,227.63,7.86">IBM&apos;s Statistical Question Answering System-TREC-11</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,479.82,676.64,33.17,7.86;9,110.48,687.60,139.58,7.86">Proceedings of the TREC 2002 Conference</title>
		<meeting>the TREC 2002 Conference</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,698.26,402.52,7.86;9,110.48,709.21,402.52,7.86;9,110.48,720.17,182.45,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,197.13,698.26,315.87,7.86;9,110.48,709.21,97.01,7.86">Question Answering from the Web Using Knowledge Annotation and Knowledge Mining Techniques</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,231.09,709.21,281.91,7.86;9,110.48,720.17,154.19,7.86">Proceedings of Twelfth International Conference on Information and Knowledge Management (CIKM 2003)</title>
		<meeting>Twelfth International Conference on Information and Knowledge Management (CIKM 2003)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,730.83,402.52,7.86;9,110.48,741.79,77.08,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,215.26,730.83,147.28,7.86">A Probabilistic Answer Type Model</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Pinchak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,386.38,730.83,121.25,7.86">European Chapter of the ACL</title>
		<meeting><address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,112.70,402.53,7.86;10,110.48,123.66,402.52,7.86;10,110.48,134.62,151.04,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,207.23,112.70,230.09,7.86">A Language Modeling Approach to Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,456.43,112.70,56.58,7.86;10,110.48,123.66,402.52,7.86;10,110.48,134.62,31.95,7.86">Proceedings of the 21st annual international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the 21st annual international ACM SIGIR conference on research and development in information retrieval<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,145.27,402.53,7.86;10,110.48,156.23,288.85,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,314.10,145.27,181.45,7.86">Probabilistic Question Answering on the Web</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Grewal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,110.48,156.23,206.57,7.86">Proc. of the 11th international conference on WWW</title>
		<meeting>of the 11th international conference on WWW<address><addrLine>Hawaii, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,166.89,402.52,7.86;10,110.48,177.85,402.52,7.86;10,110.48,188.80,68.64,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,310.87,166.89,202.12,7.86;10,110.48,177.85,54.83,7.86">Statistical QA-Classifier vs. Re-ranker: What&apos;s the difference</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,187.59,177.85,325.40,7.86;10,110.48,188.80,40.12,7.86">Proceedings of the ACL Workshop on Multilingual Summarization and Question Answering</title>
		<meeting>the ACL Workshop on Multilingual Summarization and Question Answering</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,199.46,402.52,7.86;10,110.48,210.42,180.23,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,209.51,199.46,211.36,7.86">Automatic Question Answering: Beyond the Factoid</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,440.86,199.46,72.14,7.86;10,110.48,210.42,151.90,7.86">Proceedings of the HLT/NAACL 2004: Main Conference</title>
		<meeting>the HLT/NAACL 2004: Main Conference</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,221.07,402.53,7.86;10,110.48,232.03,365.79,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,320.48,221.07,192.53,7.86;10,110.48,232.03,119.35,7.86">TREC2005 Question Answering Experiments at Tokyo Institute of Technology</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Whittaker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Chatain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,249.78,232.03,198.16,7.86">Proceedings of the 14th Text Retrieval Conference</title>
		<meeting>the 14th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,242.69,402.53,7.86;10,110.48,253.64,263.31,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,281.40,242.69,231.60,7.86;10,110.48,253.64,108.15,7.86">A Statistical Pattern Recognition Approach to Question Answering using Web Data</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Whittaker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,238.71,253.64,106.76,7.86">Proceedings of Cyberworlds</title>
		<meeting>Cyberworlds</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,264.30,402.53,7.86;10,110.48,275.26,185.11,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,285.91,264.30,227.09,7.86;10,110.48,275.26,40.15,7.86">A Unified Approach to Japanese and English Question Answering</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Whittaker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hamonic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,170.70,275.26,95.73,7.86">Proceedings of NTCIR-5</title>
		<meeting>NTCIR-5</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,285.91,402.53,7.86;10,110.48,296.87,402.51,7.86;10,110.48,307.83,199.80,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,382.38,285.91,130.63,7.86;10,110.48,296.87,266.08,7.86">Monolingual Web-based Factoid Question Answering in Chinese, Swedish, English and Japanese</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Whittaker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hamonic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Klingberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,402.53,296.87,110.46,7.86;10,110.48,307.83,114.61,7.86">Workshop on Multi-lingual Question Answering (EACL)</title>
		<meeting><address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
