<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,345.17,148.73,163.46,15.51;1,152.64,170.69,297.48,15.51;1,450.36,167.41,5.99,11.96">at QA@CLEF 2006 Using Syntactic Knowledge for QA *</title>
				<funder>
					<orgName type="full">Dutch Organisation for Scientific Research</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,216.60,204.07,56.48,9.96"><forename type="first">Gosse</forename><surname>Bouma</surname></persName>
							<email>g.bouma@rug.nl</email>
						</author>
						<author>
							<persName coords="1,281.94,204.07,54.61,9.96"><forename type="first">Ismail</forename><surname>Fahmi</surname></persName>
						</author>
						<author>
							<persName coords="1,344.90,204.07,36.03,9.96"><forename type="first">Jori</forename><surname>Mur</surname></persName>
						</author>
						<author>
							<persName coords="1,163.44,218.11,80.48,9.96"><forename type="first">Gertjan</forename><surname>Van Noord</surname></persName>
						</author>
						<author>
							<persName coords="1,252.03,218.11,93.75,9.96"><forename type="first">Lonneke</forename><surname>Van Der Plas</surname></persName>
						</author>
						<author>
							<persName coords="1,368.33,218.11,70.97,9.96"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The University of Groningen</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Information Science</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Groningen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,345.17,148.73,163.46,15.51;1,152.64,170.69,297.48,15.51;1,450.36,167.41,5.99,11.96">at QA@CLEF 2006 Using Syntactic Knowledge for QA *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8548A851626ABFFEC8685451CA470C21</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>J.5 [Arts and Humanities]: Language translation</term>
					<term>Linguistics Algorithms, Measurement, Performance, Experimentation Question answering, Dutch, Lexical Equivalences, Coreference Resolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe our system for the monolingual Dutch and multilingual English to Dutch QA tasks. First, we give a brief outline of the architecture of our QA-system, which makes heavy use of syntactic information. Next, we describe the modules that were improved or developed esepcially for the CLEF tasks, i.e. (1) incorporation of syntactic knowledge in the IR-engine, (2) incorporation of lexical equivalences, (3) incorporation of coreference resolution for off-line answer extraction, (4) treatment of temporally restricted questions, (5) treatment of definition questions, and (6) a baseline multilingual (English to Dutch) QA system, which uses a combination of Systran and Wikipedia (for term recognition and translation) for question translation. For non-list questions, 31% of the highest ranked answers returned by the monolingual system were correct and 20% of the answers returned by the multilingual system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Joost is a question answering system for Dutch which is characterized full syntactic analysis of the question and of all text returned by the IR engine for a given query. Answers are extracted by pattern matching over dependency relations, and potential answers are ranked, among others, by computing the syntactic similarity between the question and the sentence from which the answer is extracted. A brief overview of the system architecture is given in section 2. More detailed descriptions of the system can be found in <ref type="bibr" coords="1,284.39,706.27,92.79,9.96">Bouma et al. (2005)</ref> and <ref type="bibr" coords="1,401.29,706.27,91.06,9.96" target="#b0">Bouma et al. (2006)</ref>. In the rest of the paper, we focus on components of the system that were revised or developed for clef 2006, and on discussion of the results. Section 3 discusses the IR system, which tries to use various linguistic features to improve precision. In section 4, we discuss the effect of incorporating coreference resolution into the module which extracts answers to frequently asked question-types off-line. Section 5 contains an overview of techniques we implemented to identify (near) synonyms, spelling variants, etc. Sections 6 and 7 present our treatment of definition and temporally restricted questions. A description of our baseline multilingual QA system (based on Systran and Wikipedia) is given in section 8. The results of the evaluation are presented in section 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Architecture</head><p>We briefly describe the general architecture of our QA system Joost. The architecture of our system is depicted in figure <ref type="figure" coords="2,220.11,249.43,3.90,9.96" target="#fig_0">1</ref>. Apart from the three classical components question analysis, passage retrieval and answer extraction, the system also contains a component called Qatar, which is based on the technique of extracting answers off-line. All components in our system rely heavily on syntactic analysis, which is provided by Alpino <ref type="bibr" coords="2,321.30,285.31,177.31,9.96" target="#b3">(Bouma, van Noord, and Malouf, 2001)</ref>, a wide-coverage dependency parser for Dutch. Alpino is used to parse questions as well as the full document collection from which answers need to be extracted. A brief overview of the components of our QA system follows below. The first processing stage is question analysis. The input to this component is a natural language question in Dutch, which is parsed by Alpino. The goal of question analysis is to determine the question type and to identify keywords in the question.</p><p>Depending on the question type the next stage is either passage retrieval or table look-up (using Qatar). If the question type matches one of the table categories, it will be answered by Qatar. Tables are created off-line for facts that frequently occur in fixed patterns. We store these facts as potential answers together with the IDs of the paragraphs in which they were found. During the question answering process the question type determines which table is selected (if any).</p><p>For all questions that cannot be answered by Qatar, we follow the other path through the QAsystem to the passage retrieval component. Previous experiments have shown that a segmentation of the corpus into paragraphs is most efficient for information retrieval (IR) performance in QA. Hence, IR passes relevant paragraphs to subsequent modules for extracting the actual answers from these text passages.</p><p>The final processing stage in our QA-system is answer extraction and selection. The input to this component is a set of paragraph IDs, either provided by Qatar or by the IR system. We then retrieve all sentences from the text collection included in these paragraphs. For questions that are answered by means of table look-up, the tables provide an exact answer string. In this case the context is used only for ranking the answers. For other questions, answer strings have to be extracted from the paragraphs returned by IR. The features that are used to rank the extracted answers will be explained in detail below. Finally, the answer ranked first is returned to the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Linguistically Informed Information Retrieval</head><p>The information retrieval component in our system is used to identify relevant paragraphs from the CLEF corpus to narrow down the search for subsequent answer extraction modules. Accurate IR is crucial for the success of this approach. Answer containing paragraphs that have been missed by IR are lost for the entire system. Hence, IR performance in terms of recall is essential. Furthermore, high precision is also desirable as IR scores are used for ranking potential answers.</p><p>Given a full syntactic analysis of the CLEF text collection, it becomes feasible to exploit linguistic information as a knowledge source for IR. Using Apache's IR system Lucene <ref type="bibr" coords="3,430.25,369.43,64.09,9.96" target="#b5">(Jakarta, 2004)</ref>, we can index the document collection along various linguistic dimensions, such as part of speech tags, named entity classes, and dependency relations. We defined several layers of linguistic features and feature combinations extracted from syntactically analysed sentences and included them as index fields. In our current system we use 12 layers containing the following features: text (stemmed plain text tokens), root (linguistic root forms), RootPos (root forms concatenated with wordclass labels), RootRel (root forms concatenated with the name of the dependency relation to their head words), RootHead (dependent-head bigrams using root forms), RootRelHead (dependent-head bigrams with the type of relation between them), compound (compositional compounds identified by Alpino), ne (named entities), neLOC (location names), nePER (person names), neORG (organisation names), and neTypes (labels of named entities identified in the paragraph). The layers are filled with appropriate data extracted from the analysed corpus.</p><p>Each of the index fields defined above can be accessed using Lucene's query language. Complex queries combining keywords for several layers can be constructed. Queries to be used in our system are constructed from the syntactically analysed question. We extract linguistic features in the same way as done for building the index. The task now is to use this rich information appropriately. The selection of keywords is not straightforward. Keywords that are too specific might harm the retrieval performance. It is important to carefully select features and feature combinations to actually improve the results compared to standard plain text retrieval.</p><p>For the selection and weighting of keywords we applied a genetic algorithm trained on previously collected question answer pairs. For constructing a query we defined further keyword restrictions to make an even more fine-grained selection. We can select keywords based on their wordclass, their relation to the head word and based on a combination of the two. For example, we can select RootHead keywords from the question which have been tagged as nouns. Each of these (possibly restricted) keyword selections can be weighted with a numeric value according to their importance for retrieval. They can also be marked as "required" using the '+' character in Lucene's query syntax. All keyword selections are then concatenated in a disjunctive way to form the final query. Look at the example query in figure <ref type="figure" coords="3,321.16,692.23,5.03,9.96" target="#fig_1">2</ref> to get an impression of possible queries in the system.</p><p>Note that the question type provided by the question analysis module is used to query the neTypes layer with a corresponding named entity label.</p><p>The optimsation procedure using the genetic algorithm works essentially as follows: First we  start with initial settings using only one type of keyword selection. These settings are applied to construct queries from our given collection of questions. The queries are then used to retrieve a fixed number of paragraphs for each question and the retrieval performance is measured in terms of mean reciprocal rank scores. We used the answer string provided in the training data to determine if a paragraph is relevant or not. After the initial step two preferable settings (according to the scores) are selected and their settings are combined to test new parameters.</p><p>Additionally we apply simple mutation operations to alter parameters at random from time to time. The process of selecting and combining is then repeated until no significant improvement can be measured anymore. Details of the genetic optimisation process are given in <ref type="bibr" coords="4,457.36,361.87,55.28,9.96;4,90.00,373.87,22.27,9.96" target="#b8">(Tiedemann, 2005)</ref>. As the result of the optimisation we obtain an improvement of about 19% over the baseline using standard plain text retrieval (i.e. the text layer only) on unseen evaluation data. It should be noted that this improvement is not solely an effect of using root forms or named entity labels, but that many of the features that are assigned a high weight by the genetic algorithm refer to layers that make use of dependency information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Coreference Resolution for Off-line Question Answering</head><p>The system component Qatar extracts potential answers from the corpus off-line using dependency based patterns. Off-line answer extraction has proven to be very effective. The results typically show a high precision score. However, the main problem with this technique is the lack of coverage of the extracted answers. One way to increase the coverage is to apply coreference resolution.</p><p>For instance, the age of a person may be extracted from snippets such as:</p><p>( We adjusted the basic patterns by replacing the slot for the named entity with a slot for a pronoun. Similarly, we adjusted the patterns to match sentences with a definite noun. We considered noun phrases preceded by a definite determiner as definite noun phrases.</p><p>Our strategy for resolving definite NPs is based on knowledge about the categories of named entities, so-called instances (or categorised named entities). Examples are Van Gogh is-a painter, Seles is-a tennis player. We acquired instances by scanning the corpus for apposition relations and predicate complement relations<ref type="foot" coords="5,245.52,182.35,3.95,4.85" target="#foot_0">1</ref> .</p><p>We scan the left context of the definite NP for named entities from right to left. For each named entity we encounter, we check whether it occurs together with the definite NP as a pair on the instance list. If so, the named entity is selected as the antecedent of the NP. As long as no suitable named entity is found we select the next named entity and so on until we reach the beginning of the document. If no named entity is found that forms an instance pair with the definite NP, we select simply the first preceding named entity.</p><p>We applied a similar technique for resolving pronouns. The pronouns we tried to resolve were the nominative forms of the singular pronouns hij (he), zij/ze (she), het (it) and the plural pronoun zij/ze (they). We chose to resolve only the nominative case, as in almost all patterns the slot for the name was the slot in subject position. The number of both the anaphor and the antecedent was determined by the number of the main verb. Since we find the anaphors by matching patterns, we knew what the named entity (NE) tag of the antecedent should be.</p><p>Again we scan the left context of the anaphor (now a pronoun) for named entities from right to left. We implemented a preference for proper nouns in the subject position. For each named entity we encounter, we check whether it has the correct NE-tag and number. If so and if it concerns a non-person NE-tag, the named entity is selected as the antecedent. If we are looking for a person name, we have to do another check to see if the gender is correct. To determine the gender of the selected name we created a list of boy's names and girl's names by downloading such lists from the Internet<ref type="foot" coords="5,124.92,409.51,3.95,4.85" target="#foot_1">2</ref> . The female list contained 12,691 names and the male list 11,854 names. To be accepted as the correct antecedent, the proper name should not occur on the name list of the opposite sex of the pronoun. After having resolved the anaphor, the fact was added to the appropriate table.</p><p>For both extraction modules we randomly selected a sample of around 200 extracted facts and we manually evaluated these facts on the following two criteria: (1) correctness of the fact and (2) in the case of coreference resolution, correctness of the selected antecedent.</p><p>We estimated the number of additional fact types we found using the estimated precision scores. If we had only used the pronoun patterns we would have found 3,627 (5.6%) new facts. On the other hand, if we had only used the definite noun patterns we would have found 35,687 (55.2%) new facts. Using both we extracted 39,208 (60.7%) additional facts.</p><p>The number of facts we extracted by the pronoun patterns is quite low. We did a corpus investigation on a subset of the corpus which consisted of sentences containing terms relevant to the 12 selected question types<ref type="foot" coords="5,217.08,552.91,3.95,4.85" target="#foot_2">3</ref> . In only 10% of the sentences one or more pronouns appeared. This outcome indicates that the possibilities of increasing coverage by pronoun resolution are inherently limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Lexical Equivalences</head><p>One of the features that is used to rank potential answers to a question is the amount of syntactic similarity between the question and the sentence from which the answer is taken. Syntactic similarity is computed as the proportion of dependency relations from the question which have a match in the dependency relations of the answer sentence. In <ref type="bibr" coords="5,355.23,668.35,153.17,9.96">Bouma, Mur, and van Noord (2005)</ref>, we showed that taking syntactic equivalences into account (such as the fact that a by-phrase in a passive is equivalent to the subject in the active, etc.) makes the syntactic similarity score more effective.</p><p>In the current system, we also take lexical equivalences into account. That is, given two dependency relations Head, Rel, Dependent and Head , Rel, Dependent , we assume that they are equivalent if both Head and Head and Dependent and Dependent are near-synonyms.</p><p>Two roots R and R are considered near synonyms in the following cases:</p><formula xml:id="formula_0" coords="6,105.00,193.15,44.52,9.96">• R = R ,</formula><p>• R and R are synonyms,</p><p>• R and R are spelling variants,</p><p>• R is an abbreviation of R , or vice versa,</p><p>• R is the genitive form of R , or vice versa,</p><p>• R is the adjectival form of the country name R , or vice versa,</p><p>• R matches with a part of the compound R , or vice versa A list of synonyms (containing 118K root forms in total) was constructed by merging information from EuroWordNet, the dictionary website mijnwoordenboek.nl, and various encyclopedias (which often provide alternative terms for a given lemma keyword).</p><p>The spelling of person and geographical names entities tends to be subject to a fair amount of variation. For instance, the 1994 Spanish prime minister is referred to as either Felipe Gonzalez, Felippe Gonzales, Felipe Gonzales or Felipe González. The spelling used in a question is not necessarily the same as the one used in a parapgraph which provides the answer:</p><p>(2) a. Hoe heet de dochter van Deng Xiaopeng (What is the name of the daughter of Deng Xiaopeng?) (2) Deng Rong, de dochter van de Chinese leider Deng Xiaoping (Deng Rong, the daughter of the Chinese leader Deng Xiaoping).</p><p>One might consider two named entities spelling variants if the edit distance between the two is less than a certain threshold, or if one is a word suffix of the other (i.e. Maradona and Diego Armando Maradona). However, this method tends to be very noisy. To improve the precision of the method, we restricted ourselves to person names, and imposed the additional constraint that the two names must occur with the same function in our database of functions (used for off-line question answering). Thus, Felipe Gonzalez and Felippe Gonzales are considered to be variants only if they are known to have the same function (e.g. prime-minister of Spain). Currently, we recognize 4500 pairs of spelling variants. The compound rule applies when one of the words contains a hyphen (Fiat-topman) or a space (i.e. Latin phrases like colitis ulcerosa are analyzed as a single word by our parser) and the other word matches with either part of it, or when the lexical analyzer of the parser analyzes a word as a compound (i.e. chromosoomafwijking (chromosome deficit)), and the other word matches with the suffix (afwijking).</p><p>We tested the effect of incorporating lexical equivalences on questions from previous clef tasks. Although approximately 8% of the questions receives a different answer when lexical equivalences are incorporated, the effect on the overall score is negligible. We suspect that this is due to the fact that in the definition of synonyms, no distinction is made between various senses of a word, and the equivalences defined for compounds tend to introduce a fair amount of noise (e.g. the Calypso-queen of the Netherlands is not the same as the queen of the Netherlands). It should also be noted that most lexical equivalences are not taken into consideration by the IR-component. This probably means that some relevant documents (especially those containing spelling variants of proper names) are missed.</p><p>Definition questions can ask either for a definition of a named entity (What is Lusa?) or a concept (What is a cincinatto). We used the following answer patterns to find potential answers:</p><p>• Appositions (the Portugese press agency Lusa)</p><p>• Nominal modifiers (milk sugar ( saccharum lactis ) )</p><p>• or (ofwel) disjunctions ( milk sugar or saccharum lactis )</p><p>• Predicative complements (milk sugar is (called/known as) saccharum lactis)</p><p>• Predicative modifiers (composers such as Joonas Kookonen)</p><p>As some of these patterns tend to be very noisy, we also check whether there exists an isarelation between the head noun of the definition, and the term to be defined. isa-relations are collected from:</p><p>• All Named Entity -Noun appositions (48K) extracted from an automatically parsed version of the Dutch Wikipedia</p><p>• All head noun -concept pairs (136K) extracted from definition sentences found in Dutch Wikipedia .</p><p>Definition sentences were identified automatically (see <ref type="bibr" coords="7,340.84,375.31,108.30,9.96" target="#b4">Fahmi and Bouma (2006)</ref>). Answers for which a corresponding isa-relation exists in Wikipedia are given a higher score.</p><p>For the 40 definition questions in the test set, 18 received a correct first answer (45%), which is considerably better than the overall performance on non-list questions (31%). We consider 7 of the 40 definition questions to be concept definition questions. Of those, only 1 was answered correct. Thus, answering concept definitions correctly remains a challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Temporally Restricted Questions</head><p>Sometimes, questions contain an explicit date:</p><p>(3) a. Which Russian Tsar died in 1584? b. Who was the chancellor of Germany from 1974 to 1982?</p><p>To provide the correct answer to such questions, it must be ensured that there is no conflict between the date mentioned in the question and temporal information present in the text from which the answer was extracted.</p><p>To answer temporally restricted questions, we try to assign a date to sentences containing a potential answer to the question. If a sentence contains an explicit date expression, this is used as answer date. A sentence is considered to contain an explicit date if it contains a temporal expression referring to a date (2nd of August, 1991) or a relative date (last year). The denotation of the latter type of expression is computed relative to the date of the newspaper article from which the sentence is taken. Sentences which do not contain an explicit date are assigned an answer date which corresponds to the date of the newspaper from which the sentence is extracted.</p><p>For questions which contain an explicit date, this is used as the question date. For all other questions, the question date is nil.</p><p>The date score of a potential answer is:</p><p>• 0 if the question date is nil,</p><p>• 1 if answer and question date match,</p><p>• -1 otherwise.</p><p>There are 31 questions in the CLEF 2006 test set which contain an explicit date, and which we consider to be temporally restricted questions. Our monolingual QA system returned 11 correct first answers for these questions (10 of correctly answered questions ask explicitly for a fact from 1994 or 1995). The performance of the system on temporally restricted questions is similar to the performance achieved for (non-list) questions in general (31%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Multilingual QA</head><p>We have developed a baseline English to Dutch QA-sytem which is based on two freely avaiable resources: Systran and Wikipedia. For development, we used the CLEF 2004 multieight corpus. <ref type="bibr" coords="8,90.00,237.91,95.07,9.96" target="#b7">(Magnini et al., 2005)</ref> The English source questions are converted into an HTML file, which is translated automatically into Dutch by Systran.<ref type="foot" coords="8,211.08,260.95,3.95,4.85" target="#foot_3">4</ref> These translations are used as input for the monolingual QA-system described above. <ref type="foot" coords="8,161.88,272.95,3.95,4.85" target="#foot_4">5</ref>This scenario has a number of obvious drawbacks:</p><p>• Translations often result in grammatically incorrect sentences, for which no (correct) grammatical analysis can be given.</p><p>• Even if a translation can be analyzed syntactically, it may contain words or phrases that were not anticipated by the question analysis module.</p><p>• Named entities and (multiword) terms are not recognized.</p><p>We did not spend any time on fixing the first and second potential problem. While testing the system, it seemed that the parser was relatively robust against grammatical irregularities. We did notice that question analysis could be improved, so as to take into account peculiarities of the translated questions.</p><p>The third problem seemed most serious to us. It seems Systran fails to recognize many named entities and multiword terms. The result is that these are translated on a word by word basis, which typically leads to errors that are almost certainly fatal for any component (starting with IR) which takes the translated string as starting point.</p><p>To improve on the treatment of named entities and terms, we extracted from English Wikipedia all pairs of lemma titles and their cross-links to the corresponding link in Dutch Wikipedia. Terms in the English input which are found in the Wikipedia list are escaped from (4) a. Who is Jan Tinbergen b. Wie is Januari Tinbergen? c. Wie is Jan Tinbergen?</p><p>(5) a. In which country do people sleep with their feet on the pillow, according to Pippi Longstocking? b. In welk land slapen de mensen met hun voeten op het hoofdkussen, volgens Pippi Longstocking? c. In welk land slapen de mensen met hun voeten op het hoofdkussen, volgens Pippi Langkous?</p><p>(6) a. How large is the Pacific Ocean? b. Hoe groot is de Vreedzame Oceaan? c. Hoe groot is Grote Oceaan?</p><p>Three cases can arise: the term should not be translated, but it is by Systran (Jan Tinbergen),</p><p>(2) the term is not translated by Systran, but it should (Pippi Longstocking), (3) the term should be translated, but it is translated wrongly by Systran (Pacific Ocean) 48 of the 200 input questions contained terms that matched an entry in the bilingual term database extracted from Wikipedia. 4 of the marked terms are incorrect (Martin Luther instead of Martin Luther King is marked as a term, nuclear power instead of nuclear power plants is marked as a term, prime-minister is translated as minister-voorzitter rather than as minister-president or premier, and the game is incorrectly recognized as a term (it matches the name of a movie in Wikipedia) and not translated).</p><p>Although the precision of recognizing terms is high, it should be noted that recall could be much better. Terms such as Olympic Winter Games, World Heritage Sites, and proper names such as Jack Soden and Chad Rowan are not recognized, leading to word by word translations (Olympische Spelen van de Winter, De Plaatsen van de Erfenis van de Wereld) that sometimes are highly cryptical (Hefboom Soden, de Lijsterbes van Tsjaad). In addition, many unrecognized proper names show up as discontinuous strings in the translation (i.e. What did Yogi Bear steal is translated as Wat Yogi stal de Beer).</p><p>Although the performance of the multilingual system is a good deal less than that of the monolingual system, there actually are a few questions which are answered correctly by the bilingual system, but not by the monolingual system. In ( <ref type="formula" coords="9,121.46,441.91,3.87,9.96">7</ref>), the translated sentence uses elementaire deeltjes, which also occurs in the answer sentence. The monolingual question, however, uses the equivalent phrase fundamentele deeltjes, but this equivalence is not detected by the QA system. In (8) the translated question uses the noun auteur, which also occurs in the sentence providing the answer, whereas the monolingual version uses the verb schrijven (to write).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Evaluation and Error Analysis</head><p>The results from the CLEF evaluation are given in figure <ref type="figure" coords="9,342.51,544.51,3.90,9.96">3</ref>.</p><p>The monolingual system assigned only 13 questions a question type for which a table with potential answers was extracted off-line. For only 5 of those, an answer is found off-line. This suggests that the effect of off-line techniques on the overall result is relatively small. As off-line answer extraction tends to be more accurate than IR-based answer extraction, it may also explain why the results for the CLEF 2006 task are relatively modest. <ref type="foot" coords="9,361.08,603.43,3.95,4.85" target="#foot_5">7</ref>If we look at the scores per question type for the most frequent question types (as they were assigned by the question analysis component) , we see that definition questions are answered relatively well (18 out of 40 of the first answers correct), that the scores for general wh-questions and location questions are in line with the overall score (16 out of 52 and 8 out of 25 correct), but that measure and date questions are answered poorly (3 out of 20 and 3 out of 15 correct). On the development-set (of 800 questions from previous CLEF tasks), all of these question types perform considerably better (the worst scoring question type are measure questions, which still finds a correct first answer in 44% of the cases). A few questions are not answered correctly because the question type was unexpected. This is true in particular for the (3) questions of the type When did Gottlob Frege live?.</p><p>Attachment errors of the parser are the source of some mistakes. For instance, Joost replies that O.J. Simpson was accused of murder on his ex-wife, where this should have been murder on his ex-wife and a friend. As the conjunction is misparsed, the system fails to find this constituent. Different attachments also cause problems for the question Who was the German chancellor between 1974 and 1982?. It has an almost verbatim answer in the corpus <ref type="bibr" coords="10,392.47,418.27,120.22,9.96;10,90.00,430.27,164.20,9.96">(the social-democrat Helmut Schmidt, chancellor between 1974 and</ref><ref type="bibr" coords="10,257.56,430.27,22.27,9.96">1982)</ref>, but since the temporal restriction is attached to the verb in the question, and the noun social-democrat in the answer, this answer is not found.</p><p>The performance loss between the bilingual and the monolingual system is approximately 33%. This is somewhat more than the differences between multilingual and monolingual QA reported for many other systems (see <ref type="bibr" coords="10,216.24,478.03,90.85,9.96" target="#b6">Ligozat et al. (2006)</ref> for an overview). However, we do believe that it demonstrates that the syntactic analysis module is relatively robust against the grammatical anomalies present in automatically translated input. It should be noted, however, that 19 out of 200 questions cannot be assigned a question type, whereas this is the case for only 4 questions in the monolingual system. Adapting the question analysis module to typical output produced by automatic translation, and improvement of the term recognition module (by incorporating a named entity recognizer and/or more term lists) seems relatively straightforward, and might lead to somewhat better results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,216.12,638.59,170.72,9.96;2,188.21,341.75,226.42,286.62"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: System architecture of Joost.</figDesc><graphic coords="2,188.21,341.75,226.42,286.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,90.00,174.07,422.45,9.96;4,90.00,186.07,422.66,9.96;4,90.00,197.95,422.61,9.96;4,90.00,209.95,422.60,9.96;4,90.00,221.95,422.74,9.96;4,90.00,233.83,79.47,9.96"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example IR query from the question Wanneer stelde de Verenigde Naties een embargo in tegen Irak ? (When did the United Nations declare the embargo against Iraq?) using the following keyword selections: (1) all plain text tokens (except stop words), (2) Named entities weighted with boost factor 2, (3) RootHead bigrams for all words tagged as noun, (4) the question type transformed into a named entity class, (5) plain text keywords of words in an object relation (embargo &amp; Irak).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,418.94,512.95,93.87,9.96;8,90.00,524.83,422.64,9.96;8,90.00,536.83,416.78,9.96"><head></head><label></label><figDesc>automatic translation and replaced by their Dutch counterparts directly. The following examples compare the effect of direct translation (b-examples) and translation combined with Wikipedia look-up (c-examples).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,90.00,346.51,12.70,9.96;9,120.70,346.51,391.94,9.96;9,120.72,358.39,391.94,9.96;9,120.72,370.39,392.00,9.96;9,140.64,382.39,60.91,9.96;9,90.00,398.11,12.70,9.96;9,120.70,398.11,219.64,9.96;9,120.72,410.11,277.68,9.96;9,120.72,421.99,246.38,9.96"><head></head><label></label><figDesc>are the three elementary particles of physics according to the Standard Model? b. Wat zijn de drie elementaire deeltjes van fysica volgens Standaardmodel? (translated) c. Wat zijn de drie fundamentele deeltjes in het Standaardmodel uit de deeltjesfysica? is the author of the book Jurassic Park? b. Wie is de auteur van het boek Jurassic Park ? (translated) c. Wie schreef het boek Jurassic Park ? (monolingual)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,90.00,544.15,422.73,184.20"><head>Table 1 :</head><label>1</label><figDesc>We selected 12 answer types that we expect to benefit from coreference resolution. They are shown in table1. Applying the basic patterns to extract facts for these categories we extracted Answer types for which coreference resolution was applied 64,627 fact types.</figDesc><table coords="4,143.52,658.27,315.74,48.24"><row><cell>Answer Type</cell><cell>Answer Type</cell><cell>Answer Type</cell><cell>Answer Type</cell></row><row><cell>Age</cell><cell>Age of Death</cell><cell cols="2">Cause of Death Founder</cell></row><row><cell>Date of Birth</cell><cell>Date of Death</cell><cell>Capital</cell><cell>Function</cell></row><row><cell cols="3">Location of Birth Location of Death Inhabitants</cell><cell>Winner</cell></row></table><note coords="4,94.24,544.15,8.47,9.96;4,120.71,544.15,243.47,9.96;4,120.72,556.03,361.93,9.96;4,120.72,568.03,284.86,9.96;4,90.00,587.83,422.69,9.96;4,90.00,599.83,422.61,9.96;4,90.00,611.71,399.53,9.96"><p><p>1) a. de 26-jarige Steffi Graf (the 26-year old Steffi Graf) b. Steffi Graf....de 26-jarige tennisster (Steffi Graf...the 26-year old tennis player) c. Steffi Graf....Ze is 26 jaar. (Steffi Graf...She is 26 years old )</p>If no coreference resolution is applied, only patterns in which a named entity is present, such as (1-a) will match. Using coreference resolution, we can also extract the age of a person from snippets such as (1-b) and (1-c), where the named entity is present in a preceding sentence.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,90.00,110.23,422.68,213.96"><head></head><label></label><figDesc>Official CLEF scores for the monolingual Dutch task (top) and bilingual English to Dutch task (bottom).</figDesc><table coords="10,90.00,110.23,360.57,201.96"><row><cell>Q type</cell><cell>#</cell><cell cols="3"># correct % correct MRR</cell></row><row><cell>Factoid Questions</cell><cell cols="2">146 40</cell><cell>27.4</cell></row><row><cell>Definition Questions</cell><cell>40</cell><cell>18</cell><cell>45</cell></row><row><cell cols="2">Temporally Restricted 6 1</cell><cell>0</cell><cell>0</cell></row><row><cell>Non-list questions</cell><cell cols="2">187 58</cell><cell>31</cell><cell>0.346</cell></row><row><cell>List Questions</cell><cell>13</cell><cell cols="3">15/65 answers correct (P@5 = 0.23)</cell></row><row><cell>Q type</cell><cell>#</cell><cell cols="3"># correct % correct MRR</cell></row><row><cell>Factoid Questions</cell><cell cols="2">147 27</cell><cell>18.4</cell></row><row><cell>Definition Questions</cell><cell>39</cell><cell>11</cell><cell>28.2</cell></row><row><cell cols="2">Temporally Restricted 1</cell><cell>0</cell><cell>0</cell></row><row><cell>Non-list questions</cell><cell cols="2">187 38</cell><cell>20.3</cell><cell>0.223</cell></row><row><cell>List Questions</cell><cell>13</cell><cell cols="3">4/37 answers correct (P@5 = 0.06)</cell></row><row><cell>Figure 3:</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,105.24,699.96,407.17,7.97;5,90.00,709.32,87.04,7.97"><p>We limited our search to the predicate complement relation between named entities and a noun and excluded examples with negation</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,105.24,718.92,90.16,7.97;5,224.20,718.92,116.77,7.97;5,370.01,718.92,105.26,7.97;5,499.03,718.92,13.55,7.97;5,90.00,728.28,81.64,7.97"><p>http://www.namen.info, http://www.voornamenboek.nl, http://www.babynames.com and http://prenoms.free.fr</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,105.24,737.88,274.17,7.97"><p>terms such as "geboren" (born), "stierf" (died ), "hoofdstad" (capital ) etc.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="8,105.24,714.84,361.08,7.97"><p>Actually, we used the Babelfish interface to Systran, http://babelfish.altavista.digital.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="8,105.24,724.44,407.15,7.97;8,90.00,733.92,422.45,7.97;8,90.00,743.28,212.94,7.97"><p>For English to Dutch, the only alternative on-line translation service seems to be Freetranslation (www. freetranslation.com). When testing the system on questions from the multieight corpus, the results from Systran seemed slightly better, so we decided to use Systran only.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5" coords="9,105.24,719.52,124.33,7.97"><p>For development, we used almost</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="800" xml:id="foot_6" coords="9,248.51,719.52,264.12,7.97;9,90.00,729.00,422.73,7.97;9,90.00,738.48,256.55,7.97"><p>questions from previous CLEF tasks. For those questions, almost 30% of the questions are answered by answers that were found off-line. 75% of the first answers for those questions is correct. Overall, the system finds well-over 50% correct first answers.</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>* This research was carried out as part of the research program for Interactive Multimedia Information Extraction, imix, financed by nwo, the <rs type="funder">Dutch Organisation for Scientific Research</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,90.00,616.51,422.88,9.96;10,105.00,628.51,407.53,9.96;10,105.00,640.39,86.80,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,183.77,628.51,195.40,9.96">Linguistic knowledge and question answering</title>
		<author>
			<persName coords=""><forename type="first">Gosse</forename><surname>Bouma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ismail</forename><surname>Fahmi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jori</forename><surname>Mur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gertjan</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lonneke</forename><surname>Van Der Plas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jörg</forename><surname>Tiedeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,388.68,628.51,123.85,9.96;10,105.00,640.39,33.05,9.96">Traitement Automatique des Langues</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="10,90.00,660.31,422.72,9.96;10,105.00,672.31,407.35,9.96;10,105.00,684.31,392.74,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,337.69,660.31,175.03,9.96;10,105.00,672.31,11.99,9.96">Reasoning over dependency relations for QA</title>
		<author>
			<persName coords=""><forename type="first">Gosse</forename><surname>Bouma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jori</forename><surname>Mur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gertjan</forename><surname>Van Noord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,361.22,672.31,151.13,9.96;10,105.00,684.31,276.98,9.96">Proceedings of the IJCAI workshop on Knowledge and Reasoning for Answering Questions (KRAQ)</title>
		<editor>
			<persName><forename type="first">Farah</forename><surname>Benamara</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Patrick</forename><surname>Saint-Dizier</surname></persName>
		</editor>
		<meeting>the IJCAI workshop on Knowledge and Reasoning for Answering Questions (KRAQ)<address><addrLine>Edinburgh</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="15" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,704.23,422.85,9.96;10,105.00,716.11,407.54,9.96;10,105.00,728.11,106.56,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,105.00,716.11,254.03,9.96">Question answering for Dutch using dependency relations</title>
		<author>
			<persName coords=""><forename type="first">Gosse</forename><surname>Bouma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jori</forename><surname>Mur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gertjan</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lonneke</forename><surname>Van Der Plas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,382.47,716.11,130.08,9.96;10,105.00,728.11,64.43,9.96">Working Notes for the CLEF 2005 Workshop</title>
		<meeting><address><addrLine>Vienna</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,90.00,111.43,422.63,9.96;11,105.00,123.43,407.62,9.96;11,105.00,135.31,53.48,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,375.47,111.43,137.16,9.96;11,105.00,123.43,112.48,9.96">Alpino: Wide-coverage computational analysis of Dutch</title>
		<author>
			<persName coords=""><forename type="first">Gosse</forename><surname>Bouma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gertjan</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Malouf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,242.29,123.43,204.64,9.96">Computational Linguistics in The Netherlands</title>
		<meeting><address><addrLine>Rodopi, Amsterdam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2001. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,90.00,155.35,422.78,9.96;11,105.00,167.23,407.62,9.96;11,105.00,179.23,354.26,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,268.06,155.35,240.62,9.96">Learning to identify definitions using syntactic features</title>
		<author>
			<persName coords=""><forename type="first">Ismail</forename><surname>Fahmi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gosse</forename><surname>Bouma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,342.53,167.23,170.10,9.96;11,105.00,179.23,289.17,9.96">Proceedings of the EACL workshop on Learning Structured Information in Natural Language Applications</title>
		<editor>
			<persName><forename type="first">Roberto</forename><surname>Basili</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</editor>
		<meeting>the EACL workshop on Learning Structured Information in Natural Language Applications<address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,90.00,199.15,422.62,9.96;11,105.00,211.03,243.96,9.96" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Apache</forename><surname>Jakarta</surname></persName>
		</author>
		<ptr target="http://lucene.apache.org/java/docs/index.html" />
		<title level="m" coord="11,202.68,199.15,309.95,9.96;11,105.00,211.03,27.27,9.96">Apache Lucene -a high-performance, full-featured text search engine library</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,90.00,230.95,422.65,9.96;11,105.00,242.95,407.77,9.96;11,105.00,254.95,371.55,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,445.03,230.95,67.62,9.96;11,105.00,242.95,258.20,9.96">Evaluation and improvement of cross-lingual question answering strategies</title>
		<author>
			<persName coords=""><forename type="first">Anne-Laure</forename><surname>Ligozat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brigitte</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Isabella</forename><surname>Robba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anne</forename><surname>Vilat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,181.63,254.95,229.56,9.96">EACL workshop on Multilingual Question Answering</title>
		<editor>
			<persName><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Richard</forename><surname>Sutcliffe</surname></persName>
		</editor>
		<meeting><address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,90.00,274.87,422.70,9.96;11,105.00,286.75,407.76,9.96;11,105.00,298.75,407.55,9.96;11,105.00,310.75,407.60,9.96;11,105.00,322.63,303.32,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,187.98,286.75,276.12,9.96">Overview of the clef 2004 multilingual question answering track</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vallin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Erbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Simov</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sutcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,460.77,298.75,51.79,9.96;11,105.00,310.75,407.60,9.96;11,105.00,322.63,21.28,9.96">Multilingual Information Access for Text, Speech and Images: Results of the Fifth CLEF Evaluation Campaign</title>
		<title level="s" coord="11,133.89,322.63,154.31,9.96">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3491</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,90.00,342.55,422.59,9.96;11,105.00,354.55,407.82,9.96;11,105.00,366.55,98.57,9.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,196.81,342.55,258.30,9.96">Improving passage retrieval in question answering using NLP</title>
		<author>
			<persName coords=""><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,476.77,342.55,35.83,9.96;11,105.00,354.55,318.34,9.96">Proceedings of the 12th Portuguese Conference on Artificial Intelligence (EPIA)</title>
		<title level="s" coord="11,105.00,366.55,51.90,9.96">LNAI Series</title>
		<meeting>the 12th Portuguese Conference on Artificial Intelligence (EPIA)<address><addrLine>Covilhã, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
