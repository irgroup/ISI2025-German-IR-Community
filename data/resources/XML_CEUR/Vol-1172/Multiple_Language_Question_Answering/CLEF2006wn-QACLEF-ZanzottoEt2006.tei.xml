<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,101.52,98.26,400.29,15.49;1,246.48,120.22,110.12,15.49">Experimenting a &quot;general purpose&quot; textual entailment learner in AVE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,135.48,152.61,74.02,10.76"><forename type="first">Fabio</forename><surname>Massimo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Milano-Bicocca Milan</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,212.40,152.61,42.58,10.76;1,178.56,166.65,33.10,10.76"><forename type="first">Zanzotto</forename><surname>Disco</surname></persName>
							<email>zanzotto@disco.unimib.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Milano-Bicocca Milan</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,349.32,152.61,102.80,10.76"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Rome &quot;Tor Vergata&quot;</orgName>
								<address>
									<settlement>Rome</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,101.52,98.26,400.29,15.49;1,246.48,120.22,110.12,15.49">Experimenting a &quot;general purpose&quot; textual entailment learner in AVE</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D9C1FE96E645BAB40CF5F1354785E40B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.2 [ARTIFICIAL INTELLIGENCE]: I.2.7 Natural Language Processing</term>
					<term>I.2.6 Learning Measurement</term>
					<term>Performance</term>
					<term>Experimentation Question answering</term>
					<term>Textual Entailment Recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present the use of a "general purpose" textual entaiment recognizer in the Answer Validation Exercise (AVE) task. Our system has been developed to learn entailment rules from annotated examples. The main idea of the system is the cross-pair similirity measure we defined. This similarity allows us to define an implicit feature space using kernel functions in SVM learners. We experimented with our system using different training and testing sets: RTE data sets and AVE data sets. The comparative results show that entailment rules can be learned from data sets, e.g. RTE, that are different from AVE. Moreover, it seems that better results are obtained using more controlled training data (the RTE set) that less controlled ones (the AVE development set). Although, the high variability of the outcome prevents us to derive definitive conclusions, the results of our system show that our approach is quite promising and improvable in the future.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Textual entailment recognition is a common task performed in several natural language applications <ref type="bibr" coords="1,498.84,558.62,10.69,8.97" target="#b7">[8]</ref>, e.g. Question Answering and Information Extraction. The Recognizing Textual Entaliment PASCAL Challenges <ref type="bibr" coords="1,137.64,582.50,10.89,8.97" target="#b8">[9,</ref><ref type="bibr" coords="1,151.92,582.50,8.24,8.97" target="#b1">2]</ref> fostered the development of several "general purpose" textual entailment recognizers. CLEF 2006 instead provides an opportunity to show that those systems are useful for Question Answering. The voluntary exercise track aims to study the application of textual entailment recognition systems to the validation of correctness of answers given by QA systems. The basic idea is that once a pair answer/snippet is returned by a QA system, a hypothesis is built by turning the pair question/answer into an affirmative form. If the related text (a snippet or a document) semantically entails this hypothesis, then the answer is expected to be correct. The task of deciding this entailment is named here automatic Answer Validation Exercise (AVE).</p><p>We applied our entailment system <ref type="bibr" coords="1,248.88,678.14,15.34,8.97" target="#b20">[21]</ref>, developed for the second automatic entailment recognition challenge (RTE) <ref type="bibr" coords="1,158.04,690.14,10.60,8.97" target="#b1">[2]</ref>, to AVE. Our system has been shown to be one of the state-of-the-art systems on both RTE data sets <ref type="bibr" coords="1,146.52,702.14,10.77,8.97" target="#b8">[9,</ref><ref type="bibr" coords="1,159.48,702.14,7.26,8.97" target="#b1">2]</ref>. It determines whether or not a text T entails a hypothesis H by automatically learning rewriting rules from training positive and negative entailment pairs (T, H). For example given a text T 1 : "At the end of the year, all solid companies pay dividends." and two hypothesis: a) H 1 : "At the end of the year, all solid insurance companies pay dividends" and b) H 2 : "At the end of the year, all solid companies pay cash dividends", we can built two examples: (T 1 , H 1 ) which is an evidence of a true entailment (positive instance) and (T 1 , H 2 ) which is a negative evidence.</p><p>Our system extract rules from them to solve apparently not related entailments. For example, given the following text and hypothesis: T3 ⇒ H3? T3 "All wild animals eat plants that have scientifically proven medicinal properties." H3 "All wild mountain animals eat plants that have scientifically proven medicinal properties."</p><p>we note that T 3 is structurally (and somehow lexically similar) to T 1 and H 3 is more similar to H 1 than to H 2 . Thus, from T 1 ⇒ H 1 , we may extract rules to derive that T 3 ⇒ H 3 .</p><p>The main idea of our model is that it relies not only on a intra-pair similarity between T and H but also on a cross-pair similarity between two pairs (T , H ) and (T , H ). The latter similarity measure along with a set of annotated examples allows the leaning model to automatically derive syntactic and lexical rules that can solve complex entailment cases.</p><p>In this paper, we experimented with our entailment recognition system <ref type="bibr" coords="2,393.48,297.74,16.64,8.97" target="#b20">[21]</ref> and the CLEF AVE. The comparative results show that entailment rules can be learned from data sets, e.g. RTE, that are different from AVE. Although, the high variability of the outcome prevents us to derive definitive conclusions, the results of our system show that our approach is quite promising and improvable in the future.</p><p>In the remainder of this paper, Sec. 2 illustrates the related work, Sec. 3 introduces the complexity of learning entailment rules from examples, Sec. 4 describes our models, Sec. 6 shows the experimental results, and, finally, Sec. 7 derives the conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Although the textual entailment recognition problem is not new, most of the automatic approaches have been proposed only recently. This has been mainly due to the RTE challenge events <ref type="bibr" coords="2,422.64,434.30,10.77,8.97" target="#b8">[9,</ref><ref type="bibr" coords="2,435.72,434.30,7.18,8.97" target="#b1">2]</ref>. In the following we report some of such researches.</p><p>A first class of methods defines measures of the distance or similarity between T and H either assuming the independence between words <ref type="bibr" coords="2,227.28,470.18,10.77,8.97" target="#b6">[7,</ref><ref type="bibr" coords="2,241.32,470.18,13.28,8.97" target="#b10">11]</ref> in a bag-of-word fashion or exploiting syntactic interpretations <ref type="bibr" coords="2,90.00,482.06,15.34,8.97" target="#b15">[16]</ref>. A pair (T, H) is then in entailment when sim(T, H) &gt; α. These approaches can hardly determine whether the entailment holds in the examples of the previous section. From the point of view of bag-ofword methods, the pairs (T 1 , H 1 ) and (T 1 , H 2 ) have both the same intra-pair similarity since the sentences of T 1 and H 1 as well as those of T 1 and H 2 differ by a noun, insurance and cash, respectively. At syntactic level, also, we cannot capture the required information as such nouns are both noun modifiers: insurance modifies companies and cash modifies dividends.</p><p>A second class of methods can give a solution to the previous problem. These methods generally combine a similarity measure with a set of possible transformations T applied over syntactic and semantic interpretations. The entailment between T and H is detected when there is a transformation r ∈ T so that sim(r(T ), H) &gt; α. These transformations are logical rules in <ref type="bibr" coords="2,368.64,589.70,11.60,8.97" target="#b2">[3]</ref> or sequences of allowed rewrite rules in <ref type="bibr" coords="2,123.00,601.70,15.24,8.97" target="#b9">[10]</ref>. The disadvantage is that such rules have to be manually designed. Moreover, they generally model better positive implications than negative ones and they do not consider errors in syntactic parsing and semantic analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Challenges in learning from examples</head><p>In the introductory section, we have shown that, to carry out automatic learning from examples, we need to define a cross-pair similarity measure. Its definition is not straightforward as it should detect whether two pairs (T , H ) and (T , H ) realize the same rewrite rules. This measure should consider pairs similar when: (1) T and H are structurally similar to T and H , respectively and (2) the lexical relations within the pair (T , H ) are compatible with those in (T , H ). Typically, T and H show a certain degree of overlapping, thus, lexical relations (e.g., between the same words) determine word movements from T to H (or vice versa). This is important to model the syntactic/lexical similarity between example pairs.  Indeed, if we encode such movements in the syntactic parse trees of texts and hypotheses, we can use interesting similarity measures defined for syntactic parsing, e.g., the tree kernel devised in <ref type="bibr" coords="3,454.56,466.58,10.69,8.97" target="#b5">[6]</ref>.</p><p>To consider structural and lexical relation similarity, we augment syntactic trees with placeholders which identify linked words. More in detail: -We detect links between words w t in T that are equal, similar, or semantically dependent on words w h in H. We call anchors the pairs (w t , w h ) and we associate them with placeholders. For example, in Fig. <ref type="figure" coords="3,505.56,514.34,3.77,8.97" target="#fig_1">1</ref>, the placeholder 2" indicates the (companies,companies) anchor between T 1 and H 1 . This allows us to derive the word movements between text and hypothesis.</p><p>-We align the trees of the two texts T and T as well as the tree of the two hypotheses H and H by considering the word movements. We find a correct mapping between placeholders of the two hypothesis H and H and apply it to the tree of H to substitute its placeholders. The same mapping is used to substitute the placeholders in T . This mapping should maximize the structural similarity between the four trees by considering that placeholders augment the node labels. Hence, the cross-pair similarity computation is reduced to the tree similarity computation.</p><p>The above steps define an effective cross-pair similarity that can be applied to the example in Fig. <ref type="figure" coords="3,505.20,621.98,3.90,8.97" target="#fig_1">1</ref>: T 1 and T 3 share the subtree in bold starting with S → NP VP. The lexicals in T 3 and H 3 are quite different from those T 1 and H 1 , but we can rely on the structural properties expressed by their bold subtrees. These are more similar to the subtrees of T 1 and H 1 than those of T 1 and H 2 , respectively. Indeed, H 1 and H 3 share the production NP → DT JJ NN NNS while H 2 and H 3 do not. Consequently, to decide if (T 3 ,H 3 ) is a valid entailment, we should rely on the decision made for (T 1 , H 1 ). Note also that the dashed lines connecting placeholders of two texts (hypotheses) indicate structurally equivalent nodes. For instance, the dashed line between 3 and b links the main verbs both in the texts T 1 and T 3 and in the hypotheses H 1 and H 3 . After substituting 3 with b and 2 with a , we can detect if T 1 and T 3 share the bold subtree S → NP 2 VP 3 . As such subtree is shared also by H 1 and H 3 , the words within the pair (T 1 , H 1 ) are correlated similarly to the words in (T 3 , H 3 ).</p><p>The above example emphasizes that we need to derive the best mapping between placeholder sets. It can be obtained as follows: let A and A be the placeholders of (T , H ) and (T , H ), respectively, without loss of generality, we consider |A | ≥ |A | and we align a subset of A to A . The best alignment is the one that maximizes the syntactic and lexical overlapping of the two subtrees induced by the aligned set of anchors. More precisely, let C be the set of all bijective mappings from a ⊆ A : |a | = |A | to A , an element c ∈ C is a substitution function. We define as the best alignment the one determined by</p><formula xml:id="formula_0" coords="4,160.80,142.39,352.28,10.34">c max = argmax c∈C (K T (t(H , c), t(H , i)) + K T (t(T , c), t(T , i))<label>(1)</label></formula><p>where (a) t(S, c) returns the syntactic tree of the hypothesis (text) S with placeholders replaced by means of the substitution c, (b) i is the identity substitution and (c) K T (t 1 , t 2 ) is a function that measures the similarity between the two trees t 1 and t 2 (for more details see Sec. 4.2). For example, the c max between (T 1 , H 1 ) and</p><formula xml:id="formula_1" coords="4,144.60,200.23,195.93,10.65">(T 3 , H 3 ) is {( 2' , a' ), ( 2" , a" ), ( 3 , b ), ( 4 , c )}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Similarity Models</head><p>In this section we describe how anchors are found at the level of a single pair (T, H) (Sec. 4.1). The anchoring process gives the direct possibility of implementing an inter-pair similarity that can be used as a baseline approach or in combination with the cross-pair similarity. This latter will be implemented with tree kernel functions over syntactic structures (Sec. 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Anchoring and Lexical Similarity</head><p>The algorithm that we design to find the anchors is based on similarity functions between words or more complex expressions. Our approach is in line with many other researches (e.g., <ref type="bibr" coords="4,406.68,351.74,10.77,8.97" target="#b6">[7,</ref><ref type="bibr" coords="4,420.00,351.74,11.50,8.97" target="#b10">11]</ref>).</p><p>Given the set of content words (verbs, nouns, adjectives, and adverbs) W T and W H of the two sentences T and H, respectively, the set of anchors A ⊂ W T × W H is built using a similarity measure between two words sim w (w t , w h ). Each element w h ∈ W H will be part of a pair (w t , w h ) ∈ A if:</p><formula xml:id="formula_2" coords="4,102.48,418.87,190.72,43.21">1. sim w (w t , w h ) = 0 2. sim w (w t , w h ) = max w t ∈WT sim w (w t , w h )</formula><p>According to these properties, elements in W H can participate in more than one anchor and conversely more than one element in W H can be linked to a single element w ∈ W T .</p><p>The similarity sim w (w t , w h ) can be defined using different indicators and resources. First of all, two words are maximally similar if these have the same surface form w t = w h . Second, we can use one of the WordNet <ref type="bibr" coords="4,128.40,531.14,16.52,8.97" target="#b16">[17]</ref> similarities indicated with d(l w , l w ) (in line with what was done in <ref type="bibr" coords="4,412.92,531.14,11.22,8.97" target="#b6">[7]</ref>) and different relation between words such as the lexical entailment between verbs (Ent) and derivationally relation between words (Der). Finally, we use the edit distance measure lev(w t , w h ) to capture the similarity between words that are missed by the previous analysis for misspelling errors or for the lack of derivationally forms not coded in WordNet.</p><p>As result, given the syntactic category c w ∈ {noun, verb, adjective, adverb} and the lemmatized form l w of a word w, the similarity measure between two words w and w is defined as follows:</p><formula xml:id="formula_3" coords="4,172.80,622.38,340.28,93.07">sim w (w, w ) =                    1 if w = w ∨ l w = l w ∧ c w = c w ∨ ((l w , c w ), (l w , c w )) ∈ Ent∨ ((l w , c w ), (l w , c w )) ∈ Der∨ lev(w, w ) = 1 d(l w , l w ) if c w = c w ∧ d(l w , l w ) &gt; 0.2 0 otherwise<label>(2)</label></formula><p>It is worth noticing that, the above measure is not a pure similarity measure as it includes the entailment relation that does not represent synonymy or similarity between verbs. To emphasize the contribution of each used resource, in the experimental section, we will compare Eq. 2 with some versions that exclude some word relations.</p><p>The above word similarity measure can be used to compute the similarity between T and H. In line with <ref type="bibr" coords="5,110.16,73.34,10.69,8.97" target="#b6">[7]</ref>, we define it as:</p><formula xml:id="formula_4" coords="5,205.92,95.59,307.16,49.53">s(T, H) = (wt,w h )∈A sim w (w t , w h ) × idf (w h ) w h ∈WH idf (w h ) (3)</formula><p>where idf (w) is the inverse document frequency of the word w.</p><p>From the above intra-pair similarity, we can obtain the baseline cross-pair similarity based on only lexical information:</p><formula xml:id="formula_5" coords="5,195.36,192.31,317.72,10.33">K lex ((T , H ), (T , H )) = s(T , H ) × s(T , H )<label>(4)</label></formula><p>In the next section we define a novel cross-pair similarity that takes into account syntactic evidence by means of tree kernel functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Cross-pair syntactic kernels</head><p>Section 3 has shown that to measure the syntactic similarity between two pairs, (T , H ) and (T , H ), we should capture the number of common subtrees between texts and hypotheses that share the same anchoring scheme. The best alignment between anchor sets, i.e. the best substitution c max , can be found with Eq. 1.</p><p>As the corresponding maximum quantifies the alignment degree, we could define a cross-pair similarity as follows:</p><formula xml:id="formula_6" coords="5,126.36,339.07,386.72,15.37">K struct ((T , H ), (T , H )) = max c∈C K T (t(H , c), t(H , i)) + K T (t(T , c), t(T , i) ,<label>(5)</label></formula><p>where as K T (t 1 , t 2 ) we use the tree kernel function defined in <ref type="bibr" coords="5,343.32,366.14,10.60,8.97" target="#b5">[6]</ref>. This evaluates the number of subtrees shared by t 1 and t 2 , thus defining an implicit substructure space. Formally, given a subtree space F = {f 1 , f 2 , . . . , f |F | }, the indicator function I i (n) is equal to 1 if the target f i is rooted at node n and equal to 0 otherwise. A tree-kernel function over t 1 and t 2 is</p><formula xml:id="formula_7" coords="5,90.00,413.23,180.04,12.98">K T (t 1 , t 2 ) = n1∈Nt 1 n2∈Nt 2 ∆(n 1 , n 2 )</formula><p>, where N t1 and N t2 are the sets of the t 1 's and t 2 's nodes, respectively. In turn</p><formula xml:id="formula_8" coords="5,173.40,427.23,154.49,14.69">∆(n 1 , n 2 ) = |F | i=1 λ l(fi) I i (n 1 )I i (n 2 )</formula><p>, where 0 ≤ λ ≤ 1 and l(f i ) is the number of levels of the subtree f i . Thus λ l(fi) assigns a lower weight to larger fragments. When λ = 1, ∆ is equal to the number of common fragments rooted at nodes n 1 and n 2 . As described in <ref type="bibr" coords="5,406.32,454.58,10.60,8.97" target="#b5">[6]</ref>, ∆ can be computed in</p><formula xml:id="formula_9" coords="5,90.00,465.91,72.09,10.33">O(|N t1 | × |N t2 |).</formula><p>The K T function has been proven to be a valid kernel, i.e. its associated Gram matrix is positivesemidefinite. Some basic operations on kernel functions, e.g. the sum, are closed with respect to the set of valid kernels. Thus, if the maximum held such property, Eq. 5 would be a valid kernel and we could use it in kernel based machines like SVMs. Unfortunately, a counterexample illustrated in <ref type="bibr" coords="5,440.88,514.34,11.60,8.97" target="#b3">[4]</ref> shows that the max function does not produce valid kernels in general.</p><p>However, we observe that: (1) K struct ((T , H ), (T , H )) is a symmetric function since the set of transformation C are always computed with respect to the pair that has the largest anchor set; (2) in <ref type="bibr" coords="5,493.92,550.22,15.34,8.97" target="#b11">[12]</ref>, it is shown that when kernel functions are not positive semidefinite, SVMs still solve a data separation problem in pseudo Euclidean spaces. The drawback is that the solution may be only a local optimum. Therefore, we can experiment Eq. 5 with SVMs and observe if the empirical results are satisfactory. Section 6 shows that the solutions found by Eq. 5 produce accuracy higher than those evaluated on previous automatic textual entailment recognition approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Refining cross-pair syntactic similarity</head><p>In the previous section we have defined the intra and the cross pair similarity. The former does not show relevant implementation issues whereas the latter should be optimized to favor its applicability with SVMs. The Eq. 5 improvement depends on two factors: (1) its computation complexity; (2) the pruning of irrelevant information in large syntactic trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Controlling the computational cost</head><p>The computational cost of cross-pair similarity between two tree pairs (Eq. 5) depends on the size of C. This is combinatorial in the size of A and A , i. To reduce the number of placeholders, we consider the notion of chunk defined in <ref type="bibr" coords="6,428.28,115.94,10.60,8.97" target="#b0">[1]</ref>, i.e., not recursive kernels of noun, verb, adjective, and adverb phrases. When placeholders are in a single chunk both in the text and hypothesis we assign them the same name. For example, Fig. <ref type="figure" coords="6,369.72,139.82,4.98,8.97" target="#fig_1">1</ref> shows the placeholders 2' and 2" that are substituted by the placeholder 2 . The placeholder reduction procedure also gives the possibility of resolving the ambiguity still present in the anchor set A (see Sec. 4.1). A way to eliminate the ambiguous anchors is to select the ones that reduce the final number of placeholders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Pruning irrelevant information in large text trees</head><p>Often only a portion of the parse trees is relevant to detect entailments. For instance, let us consider the following pair from the RTE 2005 corpus:</p><p>T ⇒ H (id: 929) T "Ron Gainsford, chief executive of the TSI, said: "It is a major concern to us that parents could be unwittingly exposing their children to the risk of sun damage, thinking they are better protected than they actually are." H "Ron Gainsford is the chief executive of the TSI."</p><p>Only the bold part of T supports the implication; the rest is useless and also misleading: if we used it to compute the similarity it would reduce the importance of the relevant part. Moreover, as we normalize the syntactic tree kernel (K T ) with respect to the size of the two trees, we need to focus only on the part relevant to the implication. The anchored leaves are good indicators of relevant parts but also some other parts may be very relevant. For example, the function word not plays an important role. Another example is given by the word insurance in H 1 and mountain in H 3 (see Fig. <ref type="figure" coords="6,275.16,437.30,3.63,8.97" target="#fig_1">1</ref>). They support the implication T 1 ⇒ H 1 and T 1 ⇒ H 3 as well as cash supports T 1 H 2 . By removing these words and the related structures, we cannot determine the correct implications of the first two and the incorrect implication of the second one. Thus, we keep all the words that are immediately related to relevant constituents.</p><p>The reduction procedure can be formally expressed as follows: given a syntactic tree t, the set of its nodes N (t), and a set of anchors, we build a tree t with all the nodes N that are anchors or ancestors of any anchor. Moreover, we add to t the leaf nodes of the original tree t that are direct children of the nodes in N . We apply such procedure only to the syntactic trees of texts before the computation of the kernel function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental investigation</head><p>The experiments aim at determining if our system can learn rules required to solve the entailment cases contained in the AVE data set. Although, we have already shown that our system can learn entailment <ref type="bibr" coords="6,90.00,612.74,10.77,8.97" target="#b8">[9,</ref><ref type="bibr" coords="6,104.16,612.74,7.26,8.97" target="#b1">2]</ref>, the task here appears to be more complex as: (a) texts are automatically built from answers and questions; this necessarily introduces some degree of noise; and (b) often question answering systems provide a correct answer but the supporting text is not adequate to carry out a correctness inference, e.g. a lot background knowledge is required or the answer was selected by chance.</p><p>Our approach to study the above points is to train and experiment with our system and several data sets proposed in AVE as well as RTE1 and RTE2. The combination of training and testing based on such sets can give an indication on the learnability of general rules valid for different domain and different applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental settings</head><p>For the experiments, we used the following data sets: We also created new sets by merging groups of the above four collections. For example, AV E a ∪ RT E1 ∪ RT E2 stands for the set obtained as union of AV E a, RT E1 and RT E2. Moreover, to implement our model (described in sections 4 and 5), we used the following resources:</p><p>• The Charniak parser <ref type="bibr" coords="7,198.60,450.74,11.60,8.97" target="#b4">[5]</ref> and the morpha lemmatiser <ref type="bibr" coords="7,329.28,450.74,16.52,8.97" target="#b17">[18]</ref> to carry out the syntactic and morphological analysis.</p><p>• WordNet 2.0 <ref type="bibr" coords="7,170.88,481.58,16.64,8.97" target="#b16">[17]</ref> to extract both the verbs in entailment, Ent set, and the derivationally related words, Der set.</p><p>• The wn::similarity package <ref type="bibr" coords="7,253.68,512.30,16.64,8.97" target="#b19">[20]</ref> to compute the Jiang&amp;Conrath (J&amp;C) distance <ref type="bibr" coords="7,459.24,512.30,16.64,8.97" target="#b13">[14]</ref> as in <ref type="bibr" coords="7,498.84,512.30,10.69,8.97" target="#b6">[7]</ref>. This is one of the best figure method which provides a similarity score in the [0, 1] interval. We used it to implement the d(l w , l w ) function.</p><p>• A selected portion of the British National Corpus<ref type="foot" coords="7,309.72,553.46,3.49,6.28" target="#foot_0">1</ref> to compute the inverse document frequency (idf ).</p><p>We assigned the maximum idf to words not found in the BNC.</p><p>• SVM-light-TK<ref type="foot" coords="7,174.96,584.30,3.49,6.28" target="#foot_1">2</ref>  <ref type="bibr" coords="7,181.08,585.86,16.64,8.97" target="#b18">[19]</ref> which encodes the basic tree kernel function, K T , in SVM-light <ref type="bibr" coords="7,457.08,585.86,15.34,8.97" target="#b14">[15]</ref>. We used such software to implement the overall kernel K overall = K lex + K struct (see equations 4 and 5).</p><p>In all the experiments we used K overall which combines the lexical and structural cross similarities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results and analysis</head><p>Table <ref type="table" coords="7,114.72,656.42,4.98,8.97" target="#tab_0">1</ref> reports the results of our system trained with data from RTE1, RTE2 and AVE tested on the AVE split (AV E a and AV E b ). Columns 1 and 2 denote the data set used for training and testing, respectively whereas column 2, 3 and 4 illustrated the F1 measure of the systems with respect to 3 different values of the j parameters, 1, 10 and 0.9, respectively. Such parameter tunes the trade-off between Precision and Recall.</p><p>Higher values cause the system to retrieve more positive examples. When the system has a Recall of 0, the table shows the "x" symbol while the symbol "-" indicates that the experiment has not been performed. Following aspects should be noted:</p><p>• Training on AV E a and testing on AV E b provides almost 4% more in F1 than training on AV E b and testing on AV E a . This suggests the high variability of the results due to few training data; also testified by the high impact of the j parameter (about 24% of difference between j = 1 and j = 10). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Qualitative analysis</head><p>The system we presented strongly uses syntactic interpretations of the example pairs. Then, its major bottleneck is the standard AVE process used to produce the affirmative form of the question given the answer provided by a the QA system. This process frequently generates ungrammatical sentences. The problem is clear just reading the first instances of the AVE development set. We report hereafter some of these examples. Each table reports the original question (Q), the text snippet (T ), and the affirmative form of the question used as hypothesis (H). We can observe that these examples have highly ungrammatical hypothesis. In the example (id 1), Nixon is repeated at the end of H. In the example (id 2), Halley is used as subject and as predicate. Finally, in the example (id 6) there is a large part of the hypothesis that is unnecessary and creates an ungrammatical sentence.</p><p>In this paper, we experimented with our entailment system <ref type="bibr" coords="9,337.20,83.66,16.52,8.97" target="#b20">[21]</ref> and the CLEF AVE. The comparative results show that entailment rules can be learned from data sets, e.g. RTE, that are different from AVE. The experiments show that few training examples and data sparseness produce a high variability of the results. In this scenario the parameterization is very critical and necessitates of accurate cross-validation techniques. The AVE results also show that our model can learn entailments from the RTE data sets (with a higher F1 than using only AVE data). This suggests that there are some general rules, valid cross domains and collections. The importance of such rules is still more evident if we consider that the distribution of positive and negative examples in the RTE and AVE data sets is quite different. This usually prevents statistical learning algorithms to carry out a correct generalization of the data.</p><p>In the future, we would like to carry out a throughout parameterization and continue investigating approaches to exploit data from difference sources of entailments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,176.40,421.51,250.17,10.65"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Relations between (T 1 , H 1 ), (T 1 , H 2 ), and (T 3 , H 3 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,290.63,91.39,222.36,9.96;6,90.00,103.61,212.01,9.30"><head></head><label></label><figDesc>e. |C| = (|A | -|A |)!|A |! if |A | ≥ |A |. Thus we should keep the sizes of A and A reasonably small.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,90.00,60.22,423.21,337.33"><head>Table 1 :</head><label>1</label><figDesc>F1 measure of our entailment system trained with data from RTE1, RTE2 and AVE tested on the AVE split (AV Ea and AV E b ).• RT E1 and RT E2, i.e. the sets (development and test data) of the first<ref type="bibr" coords="7,394.44,309.98,11.60,8.97" target="#b8">[9]</ref> and second<ref type="bibr" coords="7,454.68,309.98,11.60,8.97" target="#b1">[2]</ref> challenges, respectively. RT E1 contains 1,367 examples whereas RT E2 contain 1,600 instances. The positive and negative examples are equally distributed in the collection, i.e. 50% of the data. • AV E a and AV E b come from a random split of the AVE development set, we created it to homogeneously learn and test our model on the AVE data. The AVE development set contains 2870 instances. Here, the positive and negative examples are not equally distributed. It contains 436 positive 2434 negative examples.</figDesc><table coords="7,183.48,60.22,235.96,185.91"><row><cell>Training set</cell><cell>Test sets</cell><cell cols="3">Trade-off parameter</cell></row><row><cell></cell><cell></cell><cell>j=1</cell><cell>j=10</cell><cell>j=0.9</cell></row><row><cell>AV Ea</cell><cell>AV E b</cell><cell cols="2">11.55 35.36</cell><cell>-</cell></row><row><cell>AV E b</cell><cell>AV Ea</cell><cell>x</cell><cell>31.85</cell><cell>-</cell></row><row><cell>AV E b ∪ RT E1</cell><cell>AV Ea</cell><cell cols="2">12.20 37.14</cell><cell>-</cell></row><row><cell>AV E b ∪ RT E1 ∪ RT E2</cell><cell>AV Ea</cell><cell cols="2">28.57 35.89</cell><cell>-</cell></row><row><cell>AV E b ∪ RT E2</cell><cell>AV Ea</cell><cell cols="2">25.68 38.98</cell><cell>-</cell></row><row><cell>AV Ea ∪ RT E1</cell><cell>AV E b</cell><cell cols="2">22.57 32.05</cell><cell>-</cell></row><row><cell>AV Ea ∪ RT E1 ∪ RT E2</cell><cell>AV E b</cell><cell cols="2">31.76 30.85</cell><cell>-</cell></row><row><cell>AV Ea ∪ RT E2</cell><cell>AV E b</cell><cell cols="2">34.07 32.38</cell><cell>-</cell></row><row><cell>RT E1</cell><cell>AV Ea</cell><cell cols="2">39.81 30.64</cell><cell>-</cell></row><row><cell>RT E1</cell><cell>AV E b</cell><cell cols="2">35.58 28.31</cell><cell>-</cell></row><row><cell>RT E1 ∪ RT E2</cell><cell>AV Ea</cell><cell cols="3">38.27 31.58 40.85</cell></row><row><cell>RT E1 ∪ RT E2</cell><cell>AV E b</cell><cell cols="3">33.42 28.29 36.20</cell></row><row><cell>RT E2</cell><cell>AV Ea</cell><cell cols="2">37.46 33.04</cell><cell>-</cell></row><row><cell>RT E2</cell><cell>AV E b</cell><cell cols="2">35.57 29.72</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,90.00,104.23,423.21,224.41"><head>•</head><label></label><figDesc>If we add the examples from the RTE challenges to the AV E b training data, we obtain a good improvement, e.g. the system trained on AV E b ∪ RT E2 improves the one trained on AV E b of about 7% (38.98% vs. 31.85%). Adding RT E1 to the training data causes a decrease. This could be explained by the high impact of parameters. It is possible that the good setting for AV E b ∪ RT E2 is not very good for AV E b ∪ RT E1 ∪ RT E2.• Training on AV E a and RTE data sets seems not helpful as the result using only AV E a is higher, e.g. 35.36% vs. 32.38%. • Finally, training on RT E1 provides higher performance than training on RT E2 on both AV E a and AV E b test sets (see rows 10 and 11 vs 14 and 15). Moreover, their combined use (RT E1 ∪ RT E2) is helpful only if we select an opportune parameter j=0.9. This leads to the highest performance on AV E a and AV E b , i.e. 40.85% and 36.20%, respectively. Given these preliminary results, we decided to use the best model obtained on RT E1 ∪ RT E2 to generate data of our CLEF submission. Moreover, as the AVE test set may have been statistically similar to the development set, we also submitted a run of the model trained on AV E a ∪ AV E b . The official results were 39.95% and 36.69%, respectively. These are quite in line with the analogous experiments shown in Table 1, i.e. training on RT E1 ∪ RT E2 and testing on AV E a (40.85%) and training on AV E a and testing on AV E b (35.36).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="7,104.40,746.46,92.34,7.17"><p>http://www.natcorp.ox.ac.uk/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="7,104.40,755.98,318.22,7.37"><p>SVM-light-TK is available at http://ai-nlp.info .uniroma2.it/moschitti/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,111.60,259.10,401.61,8.97;9,111.60,271.10,379.53,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,170.88,259.10,163.37,8.97">Part-of-speech tagging and partial parsing</title>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Abney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,111.60,271.10,186.54,8.97">Corpus-based methods in language and speech</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Bloothooft</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Church</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Young</surname></persName>
		</editor>
		<meeting><address><addrLine>Dordrecht</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer academic publishers</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,111.60,291.02,401.46,8.97;9,111.60,302.90,389.61,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,152.52,302.90,124.40,8.97">The II PASCAL RTE challenge</title>
		<author>
			<persName coords=""><forename type="first">Roy</forename><surname>Bar Haim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lisa</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,295.20,302.90,121.65,8.97">PASCAL Challenges Workshop</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,111.60,322.82,401.41,8.97;9,111.60,334.82,401.36,8.97;9,111.60,346.82,161.13,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,241.32,322.82,216.91,8.97">Recognising textual entailment with logical inference</title>
		<author>
			<persName coords=""><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Katja</forename><surname>Markert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,480.36,322.82,32.65,8.97;9,111.60,334.82,100.35,8.97">Proc. of HLT-EMNLP Conference</title>
		<meeting>of HLT-EMNLP Conference<address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-10">October 2005</date>
			<biblScope unit="page" from="628" to="635" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct coords="9,111.60,366.74,401.48,8.97;9,111.60,378.62,171.45,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,276.84,366.74,182.07,8.97">Non-mercer kernel for svm object recognition</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Boughorbel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J-P</forename><surname>Tarel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,476.88,366.74,36.20,8.97;9,111.60,378.62,77.04,8.97">Proceedings of BMVC 2004</title>
		<meeting>BMVC 2004</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="137" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,111.60,398.54,401.85,8.97;9,111.60,410.54,107.01,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,187.44,398.54,144.64,8.97">A maximum-entropy-inspired parser</title>
		<author>
			<persName coords=""><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,350.88,398.54,91.19,8.97">Proc. of the 1st NAACL</title>
		<meeting>of the 1st NAACL<address><addrLine>Seattle, Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="132" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,111.60,430.46,401.60,8.97;9,111.60,442.46,310.29,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,255.72,430.46,257.48,8.97;9,111.60,442.46,174.40,8.97">New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nigel</forename><surname>Duffy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,304.20,442.46,87.52,8.97">Proceedings of ACL02</title>
		<meeting>ACL02</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,111.60,462.38,401.42,8.97;9,111.60,474.26,401.61,8.97;9,111.60,486.26,260.97,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,262.80,462.38,166.24,8.97">Measuring the semantic similarity of texts</title>
		<author>
			<persName coords=""><forename type="first">Courtney</forename><surname>Corley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,446.52,462.38,66.50,8.97;9,111.60,474.26,295.51,8.97">Proc. of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment</title>
		<meeting>of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005-06">June 2005</date>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,111.60,506.18,401.48,8.97;9,111.60,518.18,401.46,8.97;9,111.60,530.06,130.53,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,245.16,506.18,267.92,8.97;9,111.60,518.18,65.64,8.97">Probabilistic textual entailment: Generic applied modeling of language variability</title>
		<author>
			<persName coords=""><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oren</forename><surname>Glickman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,195.24,518.18,317.82,8.97;9,111.60,530.06,26.92,8.97">Proceedings of the Workshop on Learning Methods for Text Understanding and Mining</title>
		<meeting>the Workshop on Learning Methods for Text Understanding and Mining<address><addrLine>Grenoble, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,111.60,549.98,401.54,8.97;9,111.60,561.98,194.25,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,334.80,549.98,119.48,8.97">The PASCAL RTE challenge</title>
		<author>
			<persName coords=""><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oren</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,478.44,549.98,34.70,8.97;9,111.60,561.98,84.57,8.97">PASCAL Challenges Workshop</title>
		<meeting><address><addrLine>Southampton, U.K</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,111.60,581.90,401.48,8.97;9,111.60,593.78,401.62,8.97;9,111.60,605.78,147.09,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,486.60,581.90,26.48,8.97;9,111.60,593.78,231.71,8.97">An inference model for semantic entailment in natural language</title>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Salvo</forename><surname>Braz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vasin</forename><surname>Punyakanok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Sammons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,362.52,593.78,150.70,8.97;9,111.60,605.78,37.41,8.97">Proc. of The PASCAL RTE Challenge Workshop</title>
		<meeting>of The PASCAL RTE Challenge Workshop<address><addrLine>Southampton, U.K</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,111.60,625.70,401.46,8.97;9,111.60,637.70,307.65,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,319.44,625.70,173.50,8.97">Web based probabilistic textual entailment</title>
		<author>
			<persName coords=""><forename type="first">Oren</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Moshe</forename><surname>Koppel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,111.60,637.70,200.37,8.97">Proceedings of the 1st Pascal Challenge Workshop</title>
		<meeting>the 1st Pascal Challenge Workshop<address><addrLine>Southampton, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,111.60,657.62,401.46,8.97;9,111.60,669.50,172.17,8.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,190.80,657.62,236.90,8.97">Feature space interpretation of SVMs with indefinite kernels</title>
		<author>
			<persName coords=""><forename type="first">Bernard</forename><surname>Haasdonk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,434.28,657.62,78.78,8.97;9,111.60,669.50,66.56,8.97">IEEE Trans Pattern Anal Mach Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="482" to="492" />
			<date type="published" when="2005-04">Apr 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,111.60,689.42,401.46,8.97;9,111.60,701.42,122.25,8.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,183.72,689.42,240.02,8.97">Automatic acquisition of hyponyms from large text corpora</title>
		<author>
			<persName coords=""><forename type="first">Marti</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,444.60,689.42,68.46,8.97;9,111.60,701.42,27.85,8.97">Proc. of the 15th CoLing</title>
		<meeting>of the 15th CoLing<address><addrLine>Nantes, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,111.60,721.34,401.60,8.97;9,111.60,733.34,298.65,8.97" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,256.56,721.34,256.64,8.97;9,111.60,733.34,14.83,8.97">Semantic similarity based on corpus statistics and lexical taxonomy</title>
		<author>
			<persName coords=""><forename type="first">Jay</forename><forename type="middle">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">W</forename><surname>Conrath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,145.56,733.34,108.14,8.97">Proc. of the 10th ROCLING</title>
		<meeting>of the 10th ROCLING<address><addrLine>Tapei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="132" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.60,61.34,401.46,8.97;10,111.60,73.34,365.37,8.97" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,198.72,61.34,172.20,8.97">Making large-scale svm learning practical</title>
		<author>
			<persName coords=""><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,186.24,73.34,214.73,8.97">Advances in Kernel Methods-Support Vector Learning</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Schlkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.60,93.26,401.50,8.97;10,111.60,105.26,161.25,8.97" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,284.64,93.26,160.06,8.97">Tree edit distance for textual entailment</title>
		<author>
			<persName coords=""><forename type="first">Milen</forename><surname>Kouylekov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,465.48,93.26,47.62,8.97;10,111.60,105.26,51.36,8.97">Proc. of the RANLP-2005</title>
		<meeting>of the RANLP-2005<address><addrLine>Borovets, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.60,125.18,401.82,8.97;10,111.60,137.06,81.81,8.97" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,184.20,125.18,158.79,8.97">WordNet: A lexical database for English</title>
		<author>
			<persName coords=""><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,349.68,125.18,111.16,8.97">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995-11">November 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.60,156.98,401.36,8.97;10,111.60,168.98,197.13,8.97" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,309.24,156.98,178.68,8.97">Applied morphological processing of english</title>
		<author>
			<persName coords=""><forename type="first">Guido</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Darren</forename><surname>Pearce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,495.24,156.98,17.72,8.97;10,111.60,168.98,108.79,8.97">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="207" to="223" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.60,188.90,401.41,8.97;10,111.60,200.90,118.41,8.97" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,202.92,188.90,232.68,8.97">Making tree kernels practical for natural language learning</title>
		<author>
			<persName coords=""><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,453.48,188.90,59.53,8.97;10,111.60,200.90,34.73,8.97">Proceedings of EACL&apos;06</title>
		<meeting>EACL&apos;06<address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.60,220.82,401.50,8.97;10,111.60,232.70,270.93,8.97" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="10,366.60,220.82,146.50,8.97;10,111.60,232.70,90.84,8.97">Wordnet::similarity -measuring the relatedness of concepts</title>
		<author>
			<persName coords=""><forename type="first">Ted</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Siddharth</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Michelizzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,220.80,232.70,76.92,8.97">Proc. of 5th NAACL</title>
		<meeting>of 5th NAACL<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.60,252.62,401.34,8.97;10,111.60,264.62,401.48,8.97;10,111.60,276.62,401.85,8.97;10,111.60,288.50,291.81,8.97" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="10,325.08,252.62,187.86,8.97;10,111.60,264.62,85.53,8.97">Automatic learning of textual entailments with cross-pair similarities</title>
		<author>
			<persName coords=""><forename type="first">Fabio</forename><surname>Massimo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zanzotto</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,217.68,264.62,295.40,8.97;10,111.60,276.62,331.99,8.97">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006-07">July 2006</date>
			<biblScope unit="page" from="401" to="408" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
