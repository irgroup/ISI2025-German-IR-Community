<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,119.69,148.86,363.63,15.15;1,184.47,170.78,234.07,15.15">Priberam&apos;s question answering system in a cross-language environment</title>
				<funder ref="#_pVPDfya">
					<orgName type="full">M-CAST</orgName>
				</funder>
				<funder ref="#_reVnHWY">
					<orgName type="full">European Commission in TRUST</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,233.98,204.67,55.41,8.74"><forename type="first">Adán</forename><surname>Cassan</surname></persName>
							<affiliation key="aff0">
								<address>
									<postCode>1000-123</postCode>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,297.46,204.67,67.27,8.74"><forename type="first">Helena</forename><surname>Figueira</surname></persName>
							<affiliation key="aff0">
								<address>
									<postCode>1000-123</postCode>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,135.37,218.62,62.47,8.74"><forename type="first">André</forename><surname>Martins</surname></persName>
							<affiliation key="aff0">
								<address>
									<postCode>1000-123</postCode>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,205.77,218.62,63.92,8.74"><forename type="first">Afonso</forename><surname>Mendes</surname></persName>
							<affiliation key="aff0">
								<address>
									<postCode>1000-123</postCode>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,278.11,218.62,59.32,8.74"><forename type="first">Pedro</forename><surname>Mendes</surname></persName>
							<affiliation key="aff0">
								<address>
									<postCode>1000-123</postCode>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,345.86,218.62,59.11,8.74"><forename type="first">Cláudia</forename><surname>Pinto</surname></persName>
							<affiliation key="aff0">
								<address>
									<postCode>1000-123</postCode>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,412.69,218.62,54.94,8.74"><forename type="first">Daniel</forename><surname>Vidal</surname></persName>
							<affiliation key="aff0">
								<address>
									<postCode>1000-123</postCode>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,254.21,240.68,94.59,8.74"><forename type="first">Priberam</forename><surname>Informática</surname></persName>
							<affiliation key="aff0">
								<address>
									<postCode>1000-123</postCode>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,206.74,252.64,85.43,8.74"><forename type="first">Alameda</forename><forename type="middle">D</forename><surname>Afonso</surname></persName>
							<affiliation key="aff0">
								<address>
									<postCode>1000-123</postCode>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,119.69,148.86,363.63,15.15;1,184.47,170.78,234.07,15.15">Priberam&apos;s question answering system in a cross-language environment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">08BE1DCF6A82BC75200EAC96106BC1D6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2 [Database Management]: H.2.3 Languages-Query Languages Measurement, Performance, Experimentation, Languages Question answering, Questions beyond factoids</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Following last year's participation in the monolingual question answering (QA) track of CLEF, where Priberam's QA system achieved state-of-the-art results, this year we decided to take part in both Portuguese and Spanish monolingual tasks, as well as in two bilingual (Portuguese-Spanish and Spanish-Portuguese) tasks of QA@CLEF. The architecture of our QA system relies on previous work done for a multilingual semantic search engine developed in the framework of TRUST 1 , where Priberam was responsible for the Portuguese module. Unlike TRUST, however, which used a third party indexation engine, our QA system is based on the indexing technology of LegiX, Priberam's legal information tool 2 , whose indexing engine was adapted to index semantic information, ontology domains, question categories and other specificities for QA. Given the multilingual platform where the system has been developed and tested, as well as the results obtained so far, it seemed natural to assess its language independence. To that intent, we have extended it to another language, Spanish, thus demonstrating the applicability of the system architecture. This paper describes the improvements and changes implemented in Priberam's QA system since last CLEF participation, detailing the work involved in its cross-lingual extension and discussing the results of the runs submitted to evaluation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Priberam took part in the 2005 CLEF campaign, introducing its QA system for Portuguese in the monolingual task. This year, encouraged by last year's results <ref type="bibr" coords="2,385.01,145.80,9.97,8.74" target="#b0">[1]</ref>, we decided to participate in the same track, but we extended the participation to the Spanish monolingual and bilingual (Portuguese-Spanish and Spanish-Portuguese) tasks.</p><p>The architecture of our QA system remains roughly the same, as detailed in <ref type="bibr" coords="2,459.76,181.66,10.52,8.74" target="#b1">[2,</ref><ref type="bibr" coords="2,475.00,181.66,7.01,8.74" target="#b2">3]</ref>: after the question is submitted, it is categorised according to a question typology. An internal query retrieves a set of potentially relevant documents, containing a list of sentences related with the question. Sentences are weighted according to their semantic relevance and similarity with the question. Next, through specific answer patterns, these sentences are examined once again and the parts containing possible answers are extracted and weighted. Finally, a single answer is chosen among all candidates.</p><p>This year, we focused on the handling of temporally restricted questions, the addition of another language, and the adaptation of the system to a cross-language environment. Priberam relied on the scalability of the system's architecture to cope with the inclusion of another language module, Spanish. Currently, two M-CAST<ref type="foot" coords="2,235.71,299.64,3.97,6.12" target="#foot_0">3</ref> partners, the University of Economics, Prague (UEP) and TiP, are also using this framework to develop Czech and Polish language modules. The purpose of this year's participation in QA@CLEF was to evaluate the language independence of the system, as well as its performance in a bilingual context.</p><p>The next section gives an overview of the work done for the inclusion of another language, describing the development of the Spanish module. Section 3 addresses the various improvements in Priberam's QA system architecture and depicts the cross-language scenario. Section 4 discusses both the monolingual and bilingual results of the system at this year's QA@CLEF campaign, and section 5 concludes with future guidelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Addition of Spanish</head><p>Taking advantage of the company's natural language processing (NLP) technology and workbench <ref type="bibr" coords="2,90.00,463.42,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="2,104.46,463.42,7.01,8.74" target="#b3">4]</ref>, it was somewhat straightforward to build a new language module for Spanish, similar to the Portuguese one. The QA system architecture was designed to be language independent, and by using the same software tools, like Priberam's SintaGest, new language modules can easily be implemented and tested. This means that only the language resources (lexicon, thesaurus, ontology, QA patterns) have to be adapted or imported to be in conformity with the existing NLP tools. With a team of four people, the work on the lexicon took us about three months to complete, because manual work was involved, and the adaptation and development of the QA rules took about two months, as it had to be tested while it was being developed.</p><p>The Spanish ontology was added to the common multilingual ontology, in a joint work of Priberam and Synapse Développement. As detailed in <ref type="bibr" coords="2,327.16,571.02,9.96,8.74" target="#b1">[2]</ref>, this multilingual taxonomy groups over 160 000 words and expressions through their conceptual domains organised in a four level tree with 3 387 terminal nodes.</p><p>Priberam started by acquiring a Spanish lexicon and converting it to the same format and specifications of the Portuguese one. After loading the existent lexical information in a database, we had to establish equivalencies between POS categories, uniform them and classify all the lexical entries, so the Spanish lexicon could be used with the tools that were used to build the Portuguese module. New entries in the lexicon were also inserted, mainly proper nouns, such as toponyms and anthroponyms. This was particularly important for the recognition of named entities (NEs).</p><p>Unlike the Portuguese lexicon, the Spanish one does not contain any semantic features, nor sense definitions connected to the ontology levels. The semantic classification has already been started but we still have to define senses for each entry in the lexicon and link each one of them to the levels of the ontology. This future work in the Spanish lexicon will hopefully lead to similar results of both language modules. There is also still no thesaurus for Spanish, which may influence the retrieval of documents and sentences that contain synonyms of the question's keywords. Some relations between words were semi-automatically established, such as derivation relations (e.g. caracterizar /caracterización), names of toponyms and their gentilics (e.g. Australia/australiano), currencies (e.g. Grecia/euro) and cities (e.g. España/Madrid ). Other relations, such as hyperonymy, hyponymy, antonymy and synonymy, still need to be implemented in the Spanish lexicon and improved in the Portuguese one.</p><p>The system's overall design remained unchanged for Spanish; the question patterns, answer patterns and question answering patterns <ref type="bibr" coords="3,275.21,231.57,10.52,8.74" target="#b2">[3]</ref> were adapted to fit the new language module. The typology of the 86 question categories was not altered, since it is language independent. Due to the syntactical similarities between the two Romance languages, many of the Portuguese patterns remained applicable. Groups of semantically related words and question identifiers had to be translated and revised, which was very helpful to improve and fix some errors of the Portuguese language module.</p><p>For the Spanish module, some of the Portuguese contextual rules, such as the ones for morphological disambiguation and for NEs recognition, were rewritten and adapted as well. Again, the similarity between Portuguese and Spanish allowed us to adapt much of the work done previously for Portuguese without having to start everything from scratch. Here the work was mostly done with constants and entity identifiers <ref type="bibr" coords="3,253.89,351.12,9.97,8.74" target="#b2">[3]</ref>, whereas the detection rules were just adapted in some cases.</p><p>Like in Portuguese, Spanish morphological disambiguation is done in two stages: first, the contextual rules defined in SintaGest are applied; then, remaining ambiguities are suppressed with a statistical POS tagger based on a second-order hidden Markov model <ref type="bibr" coords="3,428.57,398.94,10.52,8.74" target="#b4">[5,</ref><ref type="bibr" coords="3,442.54,398.94,7.01,8.74" target="#b5">6]</ref>. For training, we used a corpus previously disambiguated with SVMTool<ref type="foot" coords="3,345.43,409.32,3.97,6.12" target="#foot_1">4</ref>  <ref type="bibr" coords="3,353.22,410.90,9.97,8.74" target="#b6">[7]</ref>.</p><p>3 Cross-language and improvements of the system architecture</p><p>The five main steps of Priberam's QA system are:</p><p>• The indexing process, in which a large set of files in text format is analysed and index keys for morphologically disambiguated lemmas, question categories and ontology domains are created;</p><p>• The question analysis, in which a question is parsed, categories for the question are determined and pivots and other search keys are extracted;</p><p>• The document retrieval, in which a query is made to the index database and a set of document sentences is retrieved;</p><p>• The sentence retrieval, in which each sentence previously retrieved is parsed and given a score to express its likelihood of containing an answer;</p><p>• The answer extraction, in which a unique answer is selected from the best-scored sentences, by means of extraction patterns.</p><p>This year some minor changes were introduced regarding: (i ) the handling of temporally restricted questions, (ii ) the final validation of the extracted answers, and (iii ) the adaptation of the system to a cross-language environment. The next subsections describe these three topics in more detail.</p><p>Additionally, other small improvements were made, such as the implementation of a priority based scheme to make question categorisation more assertive, and the use of inexact matching techniques (based on the Levenshtein distance) for partial matching of proper nouns in the sentence retrieval module. The latter was done adapting existing technology for the Portuguese spell checker <ref type="foot" coords="4,121.58,158.27,3.97,6.12" target="#foot_2">5</ref> . Future work will address extending the use of inexact matching techniques in the document retrieval module, which will prevent the exclusion of documents where a proper noun is misspelled or spelled differently from the question, which is included in the causes of failure in the cross-language task (cf. section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Handling of temporally restricted questions</head><p>Improving the system's ability to handle temporally restricted questions was one of the goals for this year's QA@CLEF. The organisation distinguishes three types of temporal restrictions:</p><p>• Restriction by date: e.g., "Who was the US president in 1962?";</p><p>• Restriction by period : e.g., "How many cars were sold in Spain between 1980 and 1995?";</p><p>• Restriction by event: e.g., "Where did Michael Milken study before enrolling in the University of Pennsylvania?".</p><p>Our approach focuses on the two first types of restrictions. We take into account two sources of information: (i ) the dates of documents, and (ii ) temporal expressions in the documents' text.</p><p>The documents' dates in the collections EFE (for Spanish), Público (for European Portuguese) and Folha de São Paulo (for Brazilian Portuguese) are an instance of metadata. To exploit this source, we provided our system with the ability to deal with metadata information. This is a common requirement of real-world systems in most domains (the Web, digital libraries and local archives). Our procedure to deal with dates is extensible to other kinds of metadata, such as the document's title, subject, author information, etc. This is currently being done for M-CAST project, applied to digital libraries. In many cases, questions with restrictions require a hybrid search procedure, using both metadata Boolean-like queries and NLP techniques. The same hybrid behaviour is required to handle temporally restricted questions. During indexation, our system finds the adequate piece of metadata containing the date of each document which is then indexed. This makes the system able to handle Boolean-like date queries (such as "select all documents dated above 1994-10-01 and below 1994-10-15").</p><p>A more difficult issue is the recognition of temporal expressions in natural language text. While absolute dates like "25th April, 1974" are easy to recognise and convert to a numeric format, the same does not happen when the dates are incomplete ("25th April", "April 1974", "1974"), when the temporal expressions are less conventional ("spring 1974", "2nd quarter of 1974"), or when we consider temporal deictics (like "yesterday", "last Monday", "20th of the current month") that require knowledge about the discourse date.</p><p>As a first approach, we were only concerned with temporal expressions that refer to possibly incomplete absolute dates or periods. To answer temporally restricted questions, we need to perform operations over numeric representations of dates and periods, like date comparison (is 1974-04-25 greater, equal or lower than 1974-05-01?), translation of time units (add 8 days to 1974-04-25), or checking if a date lies in a period (is 1974-04-28 in the interval [1974-04-25, 1974-05-01]?). Although with a full numeric representation of dates these operations are exact and well defined, the need to deal with incomplete dates requires some fuzziness.</p><p>The procedure for answering a temporally restricted question is as follows: during question analysis, the temporal expression corresponding to the restriction is recognised and converted to the above numeric format. The document retriever module relaxes the restriction into a larger period (starting a few days before, and ending a few days after), and applies a metadata query to retrieve a set T 1 of those documents whose dates are relevant. This "relaxation" approach seems reasonably appropriate for newspaper corpora, since it works both for news about events that already occurred, and for announces of incoming events. As an example, consider the question "Contra que clube jogou o Marítimo a 2 de Abril de 1995?" [Against what team did Marítimo play on the 2nd April 1995?]. Here T 1 will contain all documents from the 29th March until the 5th April 1995. After this, another query is applied to retrieve a set T 2 of documents containing temporal expressions that match the original (non-relaxed) temporal restriction of the question. Here, fuzzy matches are accepted (although having a lower score) to deal with incomplete dates. An OR-operation of these two queries retrieves a set T = T 1 ∪ T 2 of documents that possibly satisfy the restriction either because of their date or because of the temporal expressions they contain. Finally, the usual document retrieval procedure continues as in the unrestricted case, with the difference that the top-30 retrieved documents are constrained to belong to the set T . These documents are then analysed at sentence level by the sentence retrieval module and only those sentences that either have some temporal expression satisfying the restriction, or belong to a properly dated document, are kept for answer extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Answer validation</head><p>Last year the absence of a strategy for final answer validation was considered one of the major causes of failure of our system. This year we have made a naïve approach to this problem. In order to strengthen the match between the question and the sentence containing the answer, we demand the following: (i ) the answering sentence must match (at least partially) all the proper nouns and NEs in the question, and (ii ) it must match a given amount of nominal and verbal pivots in the question. Here, a match is any correspondence of lemmas, heads of derivation, or synonyms. If a candidate answer is extracted from a sentence that fails one of these two criteria, it is discarded, although it can still be used for coherence analysis of other answers.</p><p>Notice that this approach ignores any relation among the question pivots and the extracted answer; it merely checks for matches of each pivot individually. Current work is addressing a more sophisticated strategy for answer validation through syntactic parsing. The idea is to capture the argument structure of the question, by setting a syntactical role to each pivot or group of pivots in the sentence, requiring that the answer to be extracted is contained in a specific phrase, and checking if the answering sentence actually offers enough support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adaptation of the system to a cross-language environment</head><p>Multilingual question answering has been introduced in this year Priberam's participation, namely in the Portuguese-Spanish and Spanish-Portuguese tasks. The adaptation of the system to a cross-language environment required few modifications. Moreover, unlike most approaches, it is self-contained, in the sense that it does not require using any external software or Web searches to perform translations. Instead, we use an ontology based direct translation, refined by means of a statistical corpora based approach.</p><p>The central piece of our system for the cross-language tasks is the multilingual ontology, also used in last year's CLEF for monolingual <ref type="bibr" coords="5,274.96,610.78,10.52,8.74" target="#b2">[3]</ref> and bilingual <ref type="bibr" coords="5,349.85,610.78,10.51,8.74" target="#b7">[8]</ref> purposes. The combination of the ontology information of all TRUST languages provides a bidirectional word/expression translation mechanism. Some language pairs are directly connected, as is the case of Portuguese and Spanish; others are connectable using the English language as an intermediate. This allows operating in a cross-language scenario for any pair of languages in the ontology (among Portuguese, Spanish, French, English, Polish and Czech 6 ). For each language pair that is directly connected, translation scores are used to reflect the likelihood of each translation. By using this method, the system is capable of selecting the preferential equivalent among the available translations. For instance, in the case of the Spanish word hijo in the ontological domain [family/lineage], which has the Portuguese translations filho (son) and criança (child), the selected translation is filho, since it has a higher score. These scores are computed in a background task using the Europarl parallel corpus <ref type="bibr" coords="6,90.00,123.98,9.97,8.74" target="#b8">[9]</ref>, a paragraph aligned corpus in the official languages of the European Union, containing the proceedings of the European Parliament. After a preliminary sentence alignment step, each aligned sentence is processed simultaneously for the two languages under consideration. Once processing is finished, words from one language are associated to their translation in the second language corresponding sentence. The number of times one word is associated to another is recorded and then used to calculate the translation score. This parallel corpus was also used to improve the ontology translation database. Translations were extracted based on the co-occurrence of words in aligned sentences using likelihood and Chi-squared criteria and then added to the database. Presently, we are also exploiting Europarl for other purposes, like word sense disambiguation.</p><p>In a cross-language environment, our QA system starts by instantiating independent language modules for each language (in this case, Portuguese and Spanish). Suppose that a question is asked in language X, while language Y is chosen as target. First, the question analyser performs pivot extraction and question categorisation, using the language module for X. Then, if the ontology has a direct connection from X to Y , each pivot is translated to language Y . Otherwise, each pivot is translated to English, and then the language module for Y is used to translate from English to Y . Whenever multiple translations are available, the one with the highest translation score is chosen as default, while the others are kept as synonyms (Table <ref type="table" coords="6,402.17,315.26,4.98,8.74" target="#tab_0">1</ref> illustrates this process). This is done for pivots' lemmas, heads of derivation and synonyms; in the end, equivalent lemmas, heads of derivation and synonyms are selected for language Y . Notice that this strategy does not translate the whole question from X to Y , which would discard alternative translations of some words. At this point, the language module for Y has pivots in its own language, and question categories, which are language independent. It only needs to activate the question answering patterns (QAPs) to be later used by the answer extraction module <ref type="bibr" coords="6,399.19,386.99,9.96,8.74" target="#b2">[3]</ref>. Remember that in a monolingual environment, the QAP activation is done via question patterns (QP), which allows taking profit of finer relations between the question and the answer structures that go beyond question categories. However, this is not possible in a cross-language environment, since QPs are connected to the language module for X and QAPs to the one for Y . For this reason, our current approach does cross-language QAP activation via question categories: we look in language Y for all the QAPs associated with those categories. Finally, the system proceeds as in the Y monolingual environment, with the document retrieval module, the sentence extractor and the answer extractor. ]. Notice that the ontology includes the expression salto em altura/salto de altura, and that although momento [moment] is chosen as the preferential translation for the polysemic word altura, the most correct altura and altitud are also taken into account as pivot synonyms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>This year the CLEF organization did not provide the sets of questions classified according to the CLEF question typology (factoid, definition, temporally restricted factoid and list). Therefore, we divided the sets of 200 questions into five major categories regarding the type of the expected answer: denomination/designation (DEN), definition (DEF), location/space (LOC), quantification (QUANT) and duration/time (TEMP).</p><p>Ans  The general results of Table <ref type="table" coords="7,237.69,301.59,4.98,8.74" target="#tab_2">2</ref> show that there was a slight increase regarding last year's Portuguese monolingual task. As for Priberam's first participation in the Spanish monolingual task, we achieved better results than last year's best performing system <ref type="bibr" coords="7,417.54,325.50,9.97,8.74" target="#b0">[1]</ref>. As for the crosslanguage environment, the accuracy is significantly lower, and relatively similar for the Portuguese-Spanish and Spanish-Portuguese tasks.</p><p>Despite the fairly satisfying results in the Spanish monolingual task, there is still a considerable gap between the performance of the Spanish and the Portuguese modules. A few reasons contributed to this. The semantic classification, the addition of proper nouns and the NEs detection rules for Spanish were just in an early stage, which led to a higher percentage of errors related to the extraction of candidate answers. The absence of a thesaurus also played a part in the document and sentence retrieval stages. Finally, one has to bear in mind that, since the addition of the Spanish language module is recent, it is not so extensively tested and fine-tuned.</p><p>Table <ref type="table" coords="7,132.96,445.05,4.98,8.74" target="#tab_3">3</ref> displays the distribution of errors along the main stages of our QA system, both in the monolingual and bilingual runs. In the monolingual runs, most errors occur during the extraction of candidate answers. This is related with several issues: QAPs that are badly tuned or erroneously applied due to errors in question categorisation, difficulties in writing QAPs to extract the right answer in long sentences, and a few errors due to the handling of morphological and lexical relations (for instance, no connection was found between festival de cinema of PT question 82 "Que festival de cinema atribui o 'Urso de Ouro' ?" [What film festival awards the 'Golden Bear' ?] and the NE Festival de Cinema de Berlim of the retrieved sentence), as well as anaphoric relations (e.g. the ES question 80 "¿En qué año murió Bernard Montgomery?" [In what year did Bernard Montgomery die?] did not retrieve the answer "(...) su muerte, ocurrida en 1976, motivó un auténtico duelo nacional"). The inclusion of other kinds of metadata besides the date may improve the answer extraction performance, as suggested by PT question 133 "Onde ganharam os Abba o Festival da Eurovisão?" [Where did Abba win the Eurovision Song Contest?], whose answer, Brighton, should be inferred from the title of the news.</p><p>As for the document retrieval stage, many errors are connected with long questions with many pivots, which may have the effect of filtering out relevant documents while keeping others that contain more but less important pivots. There were also issues of matching proper nouns and NEs, which are usually the core pivots (e.g. in PT question 53 "Quem venceu a Volta a França em 1988?" [Who won the Tour of France in 1988?] the right document was not retrieved, because the sentence of the answer contained Tour instead of Volta a França); tokenization issues (e.g. in PT question 166 "Qual foi o resultado do Itália-Nigéria no Campeonato do Mundo de 1994?" [What was the score of Italy-Nigeria in the World Cup of 1994?] Itália-Nigéria was considered a single token, hence not matching Itália or Nigéria); difficulties with ambiguous NEs (e.g. in PT question 189 "Quem escreveu 'A Capital' ?" [Who wrote 'A Capital' ?], 'A Capital' occurs frequently in the corpus as the name of a newspaper and less often as the title of a book). Furthermore, some questions could only be answered if a more sophisticated strategy of indexation was made (e.g. the answer to ES question 124 "¿Qué zar ruso murió en 1584?" lies in a document with a list of events: "EFEMERIDES DEL 17 DE MARZO (...) Defunciones (...) 1584.-Ivan 'El Terrible', zar ruso.", and could only be retrieved with a different indexation strategy to deal with items in a list). Table <ref type="table" coords="8,141.96,315.26,4.98,8.74" target="#tab_3">3</ref> shows also that the Spanish document retrieval stage outperformed the Portuguese one, although we were expecting the opposite behaviour since the system did not use a Spanish thesaurus. The only explanation we could find is that the Portuguese questions were, on average, harder to retrieve than the Spanish ones, more of them requiring anaphora resolution and difficult matches between NEs (e.g., Isabel II /Elizabeth 2 a ).</p><p>Our simple strategy for answer validation led to fair results. Although this year's CLEF organization did not provide us with the information of which questions were considered NIL, we estimated our system's NIL recall/precision as respectively 65%/43% (PT), 60%/34% (ES), 30%/29% (PT-ES) and 30%/18% (ES-PT).</p><p>The work done for temporally restricted questions, described in the subsection 3.1, allowed the system to retrieve the right answer in 40.7% of the questions, in the Portuguese run, and 32.1%, in the Spanish one. The justification for these results is related with several different issues, mostly in the document retrieval and answer selection stages, and not necessarily with our procedure to handle this type of restriction. For instance, in PT question 155 "Contra que clube jogou o Marítimo a 2 de Abril de 1995?" [Against what team did Marítimo play on the 2nd April 1995?] the answer was not extracted because, due to tokenization, we could not retrieve the document containing the answer, Marítimo-Tirsense, although the system correctly matched the date of the document within the relaxed interval set by the temporal restriction.</p><p>In the bilingual runs, one of the major causes of failure is related with translation errors, mostly with proper nouns and NEs equivalencies. For instance, in the questions "Quantos Óscares ganhou 'A Guerra das Estrelas' ?"; "O que é a Eurovisão?" and "Cuándo se suicidó Cleopatra?", the system missed the translations A Guerra das Estrelas/La guerra de las Galaxias, Eurovisão/Eurovisión and Cleópatra/Cleopatra. Since NEs and proper nouns are generally the most important elements for the right extraction of answers, if one fails to translate them correctly, they are bound to have a negative impact in the results. Notice that the ontology was not designed to contain proper nouns and NEs except those belonging to closed domains, like names of countries and other geographic entities. Frequently (like in the two last examples above) there are only slight spelling differences between a word and its translation. This suggests dealing with this issue by extending the use of inexact matching techniques to the document retrieval module, hence preventing the exclusion of documents by misspelling of proper nouns or NEs. This technique may also improve the monolingual usage, since in "real-world" systems it is frequent to have different spellings of proper nouns in the question and in the target documents.</p><p>Despite the positive results presented in the previous section, there are still a few issues that need to be solved, in order to improve the general performance of the system. With that purpose in mind, we are currently enriching the Spanish lexicon with the inclusion of proper nouns, semantic features, synonyms and lexical relations.</p><p>In addition, we are implementing syntactical processing to question categorisation to capture the argument structure of the question. This will allow a more tuned answer extraction, by matching the specific roles of the question arguments in the answer. This strategy will also lead to an improvement of the cross-language performance, taking profit of the language independence of that argument structure.</p><p>Current work is being done in M-CAST project for dealing with other metadata information besides dates, like the document title, its author, and the subject. A task to be addressed in the future is the application of inexact matching techniques, currently used only in the sentence retrieval stage, in the document retrieval module as well. Of course, this would imply some changes in the indexation scheme. As said above, we expect this to improve not only the translation of proper nouns and NEs in cross-language environments, but also the monolingual robustness. Finally, research is being done to address word sense disambiguation. Europarl corpus <ref type="bibr" coords="9,490.98,313.17,10.51,8.74" target="#b8">[9]</ref> is being used with the aim of building a Portuguese sense disambiguated corpus, taking profit of the multilingual ontology, which enables domain specific translation, hence allowing discriminating word senses.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,90.00,502.91,423.00,133.26"><head>Table 1 :</head><label>1</label><figDesc>Example of the Portuguese-Spanish translation for the question "Qual é o recorde mundial do salto em altura?" [What is the world record for high jump?</figDesc><table coords="6,101.53,502.91,399.94,99.55"><row><cell cols="3">Original pivots Translated pivots Other translations kept as synonyms</cell></row><row><cell>recorde</cell><cell>récord</cell><cell></cell></row><row><cell>mundial</cell><cell>mundial</cell><cell>universal</cell></row><row><cell>salto em altura</cell><cell>salto de altura</cell><cell></cell></row><row><cell>salto</cell><cell>salto</cell><cell>brinco, salto mortal, sobresalto, carrerilla, bronco,</cell></row><row><cell></cell><cell></cell><cell>voltereta, rebote, tacón, talón, cambio brusco</cell></row><row><cell>altura</cell><cell>momento</cell><cell>porte, tamaño, talla, estatura, nivel, niveles, altitud,</cell></row><row><cell></cell><cell></cell><cell>cumbre, cima, tope, pináculo, grandeza, altura, instante,</cell></row><row><cell></cell><cell></cell><cell>segundo</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,95.98,157.54,415.71,109.02"><head></head><label></label><figDesc>The differences in the number of categorized questions between ES and PT-ES are due to an inexact translation of question 178, in which the Spanish LOC question "¿Dónde está el Hermitage?" [Where is the Hermitage located?] was incorrectly translated to a Portuguese DEF question "O que é o Hermitage?" [What is the Hermitage?].</figDesc><table coords="7,95.98,157.54,415.71,89.69"><row><cell>. →</cell><cell>Right</cell><cell>Wrong</cell><cell>Inexact</cell><cell>Unsupported</cell><cell>Total</cell><cell>Accuracy (%)</cell></row><row><cell>Quest. ↓</cell><cell>PT ES PT ES ES PT</cell><cell>PT ES PT ES ES PT</cell><cell>PT ES PT ES ES PT</cell><cell>PT ES PT ES ES PT</cell><cell>PT ES PT ES ES PT</cell><cell>PT ES PT ES ES PT</cell></row><row><cell>DEN</cell><cell>69 49 33 29</cell><cell>29 44 62 68</cell><cell>2 1 0 2</cell><cell>0 2 1 1</cell><cell>100 96 96 100</cell><cell>69.0 51.0 34.4 29.0</cell></row><row><cell>DEF</cell><cell>19 18 9 14</cell><cell>8 9 18 14</cell><cell>4 0 1 1</cell><cell>0 0 0 2</cell><cell>31 27 28  † 31</cell><cell>61.3 66.7 32.1 45.1</cell></row><row><cell>LOC</cell><cell>19 13 10 8</cell><cell>9 11 15 20</cell><cell>1 0 0 0</cell><cell>1 3 1 2</cell><cell>30 27 26  † 30</cell><cell>63.3 48.2 38.5 26.7</cell></row><row><cell>QUANT</cell><cell>11 11 9 9</cell><cell>7 13 15 9</cell><cell>0 0 0 0</cell><cell>0 0 0 0</cell><cell>18 24 24 18</cell><cell>61.1 45.8 37.5 50.0</cell></row><row><cell>TEMP</cell><cell>16 14 11 7</cell><cell>5 9 13 13</cell><cell>0 3 2 1</cell><cell>0 0 0 0</cell><cell>21 26 26 21</cell><cell>76.2 53.8 42.3 33.3</cell></row><row><cell>Total</cell><cell>134 105 72 67</cell><cell>61 86 123 124</cell><cell>7 4 3 4</cell><cell>1 5 2 5</cell><cell>200 200 200 200</cell><cell>67.0 52.5 36.0 33.5</cell></row><row><cell>†</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,211.95,278.67,179.10,8.74"><head>Table 2 :</head><label>2</label><figDesc>Results by category of question.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,120.94,478.79,361.12,132.02"><head>Table 3 :</head><label>3</label><figDesc>Reasons for wrong (W), inexact (X) and unsupported (U) answers.</figDesc><table coords="7,120.94,478.79,361.12,109.71"><row><cell>Question →</cell><cell></cell><cell cols="2">W+X+U</cell><cell></cell><cell></cell><cell cols="2">Failure (%)</cell><cell></cell></row><row><cell>Stage ↓</cell><cell cols="4">PT ES PT ES PT ES</cell><cell>PT</cell><cell>ES</cell><cell>PT ES</cell><cell>ES PT</cell></row><row><cell>Document retrieval</cell><cell>21</cell><cell>16</cell><cell>28</cell><cell>27</cell><cell>31.8</cell><cell>16.8</cell><cell>21.9</cell><cell>20.3</cell></row><row><cell>Extraction of candidate answers</cell><cell>23</cell><cell>41</cell><cell>19</cell><cell>26</cell><cell>34.9</cell><cell>43.2</cell><cell>14.8</cell><cell>19.5</cell></row><row><cell>Choice of the final answer</cell><cell>15</cell><cell>30</cell><cell>36</cell><cell>37</cell><cell>22.7</cell><cell>31.6</cell><cell>28.1</cell><cell>27.8</cell></row><row><cell>NIL validation</cell><cell>7</cell><cell>8</cell><cell>14</cell><cell>12</cell><cell>10.6</cell><cell>8.4</cell><cell>10.9</cell><cell>9.0</cell></row><row><cell>Translation</cell><cell>-</cell><cell>-</cell><cell>29</cell><cell>28</cell><cell>-</cell><cell>-</cell><cell>22.7</cell><cell>21.1</cell></row><row><cell>Other</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>3</cell><cell>0.0</cell><cell>0.0</cell><cell>1.6</cell><cell>2.3</cell></row><row><cell>Total</cell><cell>66</cell><cell cols="7">95 128 133 100.0 100.0 100.0 100.0</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,105.24,709.15,407.75,6.99;2,90.00,718.61,423.00,6.99;2,90.00,728.08,423.00,6.99;2,90.00,737.54,423.00,6.99;2,90.00,747.00,131.73,6.99"><p>M-CAST -Multilingual Content Aggregation System based on TRUST Search Engine -is an European Commission co-financed project (EDC 22249 M-CAST), whose aim is the development of a multilingual infrastructure enabling content producers to access, search and integrate the assets of large multilingual text (and multimedia) collections, such as internet libraries, resources of publishing houses, press agencies and scientific databases (http://www.m-cast.infovide.pl).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="3,105.24,727.74,407.75,6.99;3,90.00,737.20,157.13,6.99"><p>SVMTool is developed by TALP Research Center NLP group, of Universitat Politècnica de Catalunya (http://www.lsi.upc.es/~nlp/SVMTool/).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="4,105.24,723.73,407.75,6.99;4,90.00,733.20,423.00,6.99;4,90.00,742.66,372.17,7.21"><p>The Portuguese spell checker is included in FLiP, together with a grammar checker, a thesaurus, a hyphenator, a verb conjugator and a translation assistant that enable different proofing level -word, sentence, paragraph and text -of European and Brazilian Portuguese. An online version is available at http://www.flip.pt.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3" coords="5,105.24,737.54,407.76,6.99;5,90.00,747.00,166.85,6.99"><p>The French, Polish and Czech parts of the ontology are property respectively of Synapse Développement, TiP and University of Economics, Prague (UEP).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>Priberam <rs type="institution">Informática</rs> would like to thank the partners of the NLUC consortium 7 , especially <rs type="institution">Synapse Développement</rs>, the CLEF organization and Linguateca. We would also like to acknowledge the support of the <rs type="funder">European Commission in TRUST</rs> (<rs type="grantNumber">IST-1999-56416</rs>) and <rs type="funder">M-CAST</rs> (<rs type="grantNumber">EDC 22249 M-CAST</rs>) projects.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_reVnHWY">
					<idno type="grant-number">IST-1999-56416</idno>
				</org>
				<org type="funding" xml:id="_pVPDfya">
					<idno type="grant-number">EDC 22249 M-CAST</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,105.50,494.43,407.51,8.74;9,105.50,506.39,407.50,8.74;9,105.50,518.34,407.50,8.74;9,105.50,530.30,407.51,8.74;9,105.50,542.97,377.89,8.30" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,362.10,506.39,150.89,8.74;9,105.50,518.34,150.17,8.74">Overview of the CLEF 2005 multilingual question answering track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vallin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Aunimo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Osenova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Penas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sacaleanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sutcliffe</surname></persName>
		</author>
		<ptr target="http://clef.isti.cnr.it/2005/workingnotes/WorkingNotes2005/vallin05.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="9,282.23,518.34,230.77,8.74;9,105.50,530.30,138.72,8.74">Cross Language Evaluation Forum: Working Notes for the CLEF 2005 Workshop</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09-23">21-23 September), 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,562.18,407.51,8.74;9,105.50,574.13,407.50,8.74;9,105.50,586.09,407.51,8.74;9,105.50,598.04,385.29,9.02" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,381.10,562.18,131.90,8.74;9,105.50,574.13,179.13,8.74">Design and Implementation of a Semantic Search Engine for Portuguese</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Amaral</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Pinto</surname></persName>
		</author>
		<ptr target="http://www.priberam.pt/docs/LREC2004.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="9,307.12,574.13,205.88,8.74;9,105.50,586.09,221.13,8.74">Proceedings of 4th International Conference on Language Resources and Evaluation (LREC 2004)</title>
		<meeting>4th International Conference on Language Resources and Evaluation (LREC 2004)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-05-28">26-28 May. 2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="247" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,617.97,407.50,8.74;9,105.50,629.92,407.51,8.74;9,105.50,641.88,407.50,8.74;9,105.50,654.55,380.35,8.30" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,465.43,617.97,47.56,8.74;9,105.50,629.92,183.80,8.74">Priberam&apos;s question answering system for Portuguese</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Amaral</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Figueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Pinto</surname></persName>
		</author>
		<ptr target="http://clef.isti.cnr.it/2005/workingnotes/WorkingNotes2005/amaral05.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="9,314.09,629.92,198.91,8.74;9,105.50,641.88,159.03,8.74">Cross Language Evaluation Forum: Working Notes for the CLEF 2005 Workshop</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09-23">21-23 September), 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,673.76,407.51,8.74;9,105.50,685.72,407.50,8.74;9,105.50,697.67,407.51,8.74;9,105.50,710.34,232.90,8.30" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,385.45,673.76,127.56,8.74;9,105.50,685.72,153.06,8.74">A Workbench for Developing Natural Language Processing Tools</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Amaral</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Figueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Pinto</surname></persName>
		</author>
		<ptr target="http://www.priberam.pt/docs/WorkbenchNLP.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="9,280.13,685.72,232.87,8.74;9,105.50,697.67,184.02,8.74">Pre-proceedings of the 1st Workshop on International Proofing Tools and Language Technologies</title>
		<meeting><address><addrLine>Patras, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07-02">1-2 July), 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,112.02,407.51,8.74;10,105.50,123.98,407.51,8.74;10,105.50,135.93,22.70,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,236.13,112.02,272.51,8.74">A second-order hidden Markov model for part-of-speech tagging</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Thede</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Harper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,117.01,123.98,215.22,8.74">Proceedings of the 37th Annual Meeting of the ACL</title>
		<meeting>the 37th Annual Meeting of the ACL<address><addrLine>Maryland; College Park</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="175" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,155.86,407.51,8.74;10,105.50,167.81,334.76,8.74" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<title level="m" coord="10,318.57,155.86,194.44,8.74;10,105.50,167.81,105.14,8.74">Foundations of Statistical Natural Language Processing (2nd printing</title>
		<meeting><address><addrLine>Cambridge, Massachusetts</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,187.74,407.51,8.74;10,105.50,199.69,407.51,8.74;10,105.50,211.65,276.20,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,271.42,187.74,241.58,8.74;10,105.50,199.69,110.41,8.74">SVMTool: A general POS tagger generator based on Support Vector Machines</title>
		<author>
			<persName coords=""><forename type="first">Jesús</forename><surname>Giménez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,241.38,199.69,271.62,8.74;10,105.50,211.65,164.02,8.74">Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC&apos;04)</title>
		<meeting>the 4th International Conference on Language Resources and Evaluation (LREC&apos;04)<address><addrLine>Lisbon, Portugal)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,231.57,193.79,8.74;10,317.48,231.57,195.52,8.74;10,105.50,243.53,407.51,8.74;10,105.50,255.48,334.86,8.74;10,456.49,255.48,56.51,8.74;10,105.50,268.15,385.58,8.30" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,317.48,231.57,195.52,8.74;10,105.50,243.53,94.93,8.74">Cross Lingual Question Answering using QRISTAL for CLEF</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Séguéla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nègre</surname></persName>
		</author>
		<ptr target="http://clef.isti.cnr.it/2005/workingnotes/WorkingNotes2005/laurent05.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="10,256.68,243.53,256.33,8.74;10,105.50,255.48,124.71,8.74">Cross Language Evaluation Forum: Working Notes for the CLEF 2005 Workshop</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09-23">2005. 21-23 September), 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,287.36,407.51,8.74;10,105.50,299.32,407.50,8.74;10,105.50,311.99,387.04,8.30" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,163.70,287.36,298.48,8.74">Europarl: A Parallel Corpus for Statistical Machine Translation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<ptr target="http://www.iccs.inf.ed.ac.uk/~pkoehn/publications/europarl-mtsummit05.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="10,493.89,287.36,19.11,8.74;10,105.50,299.32,124.52,8.74">Proceedings of MT Summit X</title>
		<meeting>MT Summit X<address><addrLine>Phuket, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
