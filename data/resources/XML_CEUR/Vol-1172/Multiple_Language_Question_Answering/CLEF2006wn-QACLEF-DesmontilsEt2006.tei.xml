<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,98.22,148.78,406.57,15.22">Prodicos experiment feedback for QA@CLEF2006</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,209.17,182.69,58.87,8.80"><forename type="first">E</forename><surname>Desmontils</surname></persName>
							<email>emmanuel.desmontils@univ-nantes.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Nantes</orgName>
								<address>
									<addrLine>2, rue de la Houssinière</addrLine>
									<postCode>BP92208, F-44322</postCode>
									<settlement>Nantes CEDEX 3</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,275.97,182.69,46.89,8.80"><forename type="first">C</forename><surname>Jacquin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Nantes</orgName>
								<address>
									<addrLine>2, rue de la Houssinière</addrLine>
									<postCode>BP92208, F-44322</postCode>
									<settlement>Nantes CEDEX 3</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,337.25,182.69,56.58,8.80"><forename type="first">L</forename><surname>Monceaux</surname></persName>
							<email>laura.monceaux@univ-nantes.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Nantes</orgName>
								<address>
									<addrLine>2, rue de la Houssinière</addrLine>
									<postCode>BP92208, F-44322</postCode>
									<settlement>Nantes CEDEX 3</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,98.22,148.78,406.57,15.22">Prodicos experiment feedback for QA@CLEF2006</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EC58C37ECD544E145EF8A68074A65882</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software Measurement, Performance, Experimentation Question answering, Question analysis, Passage selection, Answer extraction, Pattern-based answer extraction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present the second version of the Prodicos query answering system which was developed by the TALN team from the LINA institute. We have participated to the monolingual evaluation task dedicated to the French language. We firstly present the question analysis step which makes it possible to extract many features from the questions (question category, question type, question focus, answer type, ...). For this new campaign, new features are extracted from the questions in order to improve the passage selection process (named entities, noun phrases and dates). We also determine four different strategies that will be used during the answer extraction step (entity named strategy, numerical entity strategy, acronym definition strategy, pattern-based strategy). We also take into account a new category of question (lists). We then present the passage selection process whose goal is to extract from the journalistic corpora the most relevant passages which answer to the question. This year, we present a new strategy applied to definitional queries. We use external knowledge (Wikipedia encyclopedia) to add information to these kinds of questions. Then, we discuss, in details, the major improvements made on our system at the answer extraction module level. According to the strategies determined during the question analysis stage, we present the 4 different strategies applied to this step. We present, in details and independently, each strategy and their use context. Afterwards, for the passage selection and answer extraction modules, the evaluation is put forward to justify the results obtained.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper, we present the second version of the Prodicos query answering system which was developed by the TALN team from the LINA institute. It was our second participation to the QA@CLEF evaluation campaign. We have decided to participate to the monolingual evaluation task dedicated to the French language. This campaign enables us to analyse the performances of our system. Firstly, we present the various modules constituting our system and for two of them (passage extraction module and answer extraction module) , the evaluation is put forward to justify the results obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview of the system architecture</head><p>The Prodicos query answering system is divided into three parts (figure <ref type="figure" coords="2,406.32,202.59,3.88,8.80">1</ref>):</p><p>• question analysis module;</p><p>• passage extraction module (extracts passages which might contain the answer);</p><p>• answer extraction module (extracts the answer according to the results provided by the previous module).</p><p>Figure <ref type="figure" coords="2,260.33,515.02,3.88,8.80">1</ref>: The global architecture</p><p>The modules of the Prodicos system are based on the use of linguistic knowledge, in particular lexical knowledge coming from the EuroWordnet thesaurus <ref type="bibr" coords="2,355.80,550.33,10.52,8.80" target="#b5">[6]</ref> and syntactic knowledge coming from a syntactic chunker which has been developed by our team (by the use of the TreeTagger tool <ref type="bibr" coords="2,110.20,574.24,10.29,8.80" target="#b4">[5]</ref>).</p><p>The system has participated to the QA@CLEF 2006 evaluation campaign for the monolingual query answering task dedicated to the French language. This campaign enables us to make an evaluation of the system. We present, in the next sections, in greater detail, the various modules which belong to the Prodicos system and the linguistic tools used to implement them. In parallel, we analyse in detail the results for the passage extraction module and the answer extraction module. The question analysis module was been evaluated last year during the QA@CLEF 2005 campaign <ref type="bibr" coords="2,134.82,657.93,9.96,8.80" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Question analysis module</head><p>The question analysis module aims to extract relevant features from questions that will make it possible to guide the passage selection and the answer search. We extract many features from the questions <ref type="bibr" coords="2,133.55,736.60,9.96,8.80" target="#b0">[1]</ref>: question category, question type, question focus, answer type, principal verb, etc. The question category is determined according to specific syntactic rules. The main feature which comes from the question analysis is then the question type. It will not only help to determine the strategy to perform an answer search but also it will make it possible to select rules to extract other important features from questions (answer type, question focus). We defined twenty question types which correspond to a simplified syntactic form of the question<ref type="foot" coords="3,433.46,158.22,3.97,6.16" target="#foot_0">1</ref> (for example the type QuiVerbeGN). The question type makes also it possible to verify the answer type that will be retrieved. The answer type may be a named entity (Person, Location-State, Location-City, Organization...), or a numerical entity (Date, Length, Weight, Financial-Amount...). The question focus corresponds to a word or a word group involved in the question. Its main particularity is that, generally around it, the answer is present within the passages which may contain the answer. These different features are extracted by using the TreeTagger tool and then, according to the part-of-speech tags, by building some rules to determine the question chunks (noun phrase, adjective phrase, adverb phrase, prepositional phrase, verb phrase). Then, according to the previous syntactic chunks, we have written rules which make it possible to extract, from the questions, information like question focus, principal verb,... For determining answer type, we use semantic knowledge (EuroWordnet Thesaurus). We build lists of words which are hyponyms of some predefined words which are considered like categories and we use them in order to generate the answer type <ref type="bibr" coords="3,162.57,315.20,9.96,8.80" target="#b2">[3]</ref>, <ref type="bibr" coords="3,179.17,315.20,9.97,8.80" target="#b0">[1]</ref>.</p><p>For this new campaign new features are extracted from the questions in order to improve the passage selection process (named entities, noun phrases and dates). For example, for the queries: "Qui est Boris Becker ?" (the 90th question: "who is Boris Becker?") and "Qu'est-ce que l'effet de serre ?" (189th question: "what is the greenhouse effect?"), we now consider "Boris Becker" and "effet de serre" as a single entity. We also determine a new feature which is the strategy to use to search the right answer. It is determined according to the question focus and the question type. These strategies are either an entity named strategy, either a numerical entity strategy, either an acronym definition strategy or a pattern-based strategy (Figure <ref type="figure" coords="3,372.86,410.84,3.88,8.80" target="#fig_1">2</ref>). For example, if we take into account the first case, this means that the answer extraction module must use a named entity recognizer in order to extract the answer ...  For questions whose answer is of type list, we generate a new feature which is the number of answer which we are waiting for. In this context, we have three kinds of question, classified according to the number of awaited answers. Answer of:</p><p>• one answer type. For example "Qui est Boris Becker ?" ("Who is Boris Becker?"),</p><p>• precise number answer type (often extracted from the noun phrase corresponding to the question focus). For example, for the question: "Qui sont les deux principaux responsables de l'attentat d'Oklahoma City ? (the 92 th question: "who are the two persons in charge for the terrorist attack of Oklahoma City?"), the question focus is "les deux principaux responsables de l'attentat d'Oklahoma City" ("two persons in charge for the terrorist attack of Oklahoma City"),</p><p>• several answers type (undefined number,often extracted from the noun phrase corresponding to the question focus). For example, for the question: "Citer le nom de tous les aéroports de Londres, en Angleterre." (the 88th question: "give the name of all London's airport, in England), the question focus is "le nom de tous les aéroports de Londres, en Angleterre" ("the name of all London's airport").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Passage selection module</head><p>The goal of this module is to extract from the journalistic corpora the most relevant passages which answer to the question (ie, the passages which might contain the answer). Firstly, the corpora are processed and marked with XML annotation in order to locate the passages. The corpora are then annotated with part-of-speech and lemma by using the TreeTagger tool. A passage is often a sentence excepted for example for a person's citation which is a set of sentences whose union is regarded as a single passage.</p><p>Then, the corpora are indexed by the Lucene search engine<ref type="foot" coords="4,368.61,340.51,3.97,6.16" target="#foot_1">2</ref> . The indexing unit used is the passage. For each question, we then build a Lucene request according to the data generated by the question analysis step. The request is built according to a combination of some elements linked with the "or" boolean operator. The elements are: question focus, named entities, principal verbs, common nouns, adjectives, dates and other numerical entities. For a particular request, the passage extraction module provides a sorted passage list which answers to the request. The sort criterion is a confidence coefficient associated with each passage in the list. It is determined according to the number and the category of the question elements which are found in passages. For example, if the question focus belongs to a passage, the confidence coefficient of this passage is high, because the question focus is very important for the answer extraction step <ref type="bibr" coords="4,452.80,449.66,9.96,8.80" target="#b0">[1]</ref>. When the passage extraction module stops, only the 50 passages with the highest confidence coefficient are kept.</p><p>We have a particular strategy for the definitional questions. We use external knowledge to add information to these kinds of questions. The external source used is the Wikipedia encyclopedia. We expand the question focus (which is either a simple noun or a complex noun phrase) with pieces of information extracted from Wikipedia articles. In this aim, we add to lucene request the noun phrases or simple nouns which belong to the first sentence of the corresponding Wikipedia article (if the question focus exists in the encyclopedia or if the noun phrase whose we search the definition is not polysemous). The added noun phrases are determined with the help of the TreeTagger tool. For example for the 4th question "Qui est Radovan Karadzic ? " (Who is Radovan Karadzic?) , the query sent to lucene search engine is : "Radovan" OR "Karadzic" OR "Radovan Karadzic" OR "homme politique" ("politician") OR "psychiatre" ("psychiatrist").</p><p>After the CLEF 2006 evaluation campaign, we have made the study, for the definitional queries, of the position of the first passage belonging to the list of returned passage, which contains the right answer (table <ref type="table" coords="4,175.88,628.99,3.87,8.80">1</ref>).</p><p>The set of Clef evaluation queries comprises 40 definitional queries. For only eight of them, the question focus does not belong to the Wikipedia encyclopedia. This shows that the queries are often general and in this context, the French version of the encyclopedia has a good coverage. Three of them belong to the "wrong tagged" category. For these questions, the TreeTagger tool gives a wrong part-of-speech for the question focus and the consequence is, that the passage extraction process does not correctly detect the right passage for the answer. These question focuses are: "Euro Disney", "Crédit Suisse" and "Javier Clemente". These named entities are each divided into two single words and some of these words have a wrong part-of-speech associated to Table <ref type="table" coords="5,191.47,201.31,3.88,8.80">1</ref>: Passage extraction evaluation for definitionnal queries them. Indeed, for French language, some words constituting these named entities are ambiguous and represent either an adjective or a common noun. In table <ref type="table" coords="5,364.63,253.94,4.98,8.80">1</ref> , we can see that for 67% of the definitional questions, the passages containing the right answer take the value 1. Moreover, for 83% of them the answer belongs to a passage selected during this step. This are good results and this shows that the use of encyclopedic knowledge helps the selection passage process. The nature of the resource (Wikipedia) is also very interesting because of the recurrent problem for French language to have such kind of resource at one's disposal. The multilingual property can also be used in a cross-language evaluation context.</p><p>5 Answer extraction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Global process</head><p>This step comes at the end of our process. After the question analysis and the passage selection, we have to extract correct answers corresponding to questions. To this end, we use on the one hand elements coming from the question analysis like, for instance, the question's category, the strategy to use it, the number of answers, and so on (see figure <ref type="figure" coords="5,366.17,436.44,4.98,8.80" target="#fig_1">2</ref> for an example of a part of such an analysis, element shown are used in this step) and, on the other hand, a list of passages selected and evaluated by our previous step according to this question. The goal of this step is to find the precise answer(s) to a question. An answer is built with the answer itself, the passage used to answer and, a trust value. This ending process can be divided into 4 local steps (figure <ref type="figure" coords="5,198.60,496.22,3.87,8.80" target="#fig_2">3</ref>):</p><p>1. according to the question's strategy, the convenient entity extraction module is selected, 2. candidate answers are detected and selected by the previous selected module, <ref type="bibr" coords="5,102.18,553.77,3.88,8.80" target="#b2">3</ref>. answers are evaluated and the answer(s) with the highest trust coefficient is (are) kept, 4. passages where each answer has been found are also associated to the selected answer. The question's analysis can give 4 groups of categories which correspond to 4 possible strategies: numerical entities extraction, named entities extraction, acronym definitions extraction and pattern-based extraction (the default one). Now, we will present processes associated to each strategy and the build of final answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Numerical entities extraction</head><p>For locating numerical entities, we use a set of dedicated regular expressions. These expressions make it possible to the system to extract numerical information namely: dates, duration, times, periods, ages, financial amounts, lengths, weights, numbers and ratios. It uses the MUC (Message Understanding Conference) categories ("TIMEX" and "NUMEX") to annotate texts. For example, lets take the 13th question: «En quelle année la catastrophe de Tchernobyl a -t-elle eu lieu ?» (the year of the Tchernobyl's nuclear explosion). Our numerical extraction tool gives results as for the 7th sentence of "LEMONDE95-041936" shown in figure <ref type="figure" coords="5,336.38,745.58,3.88,8.80" target="#fig_3">4</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Named entities extraction</head><p>For locating named entities, NEMESIS tool <ref type="bibr" coords="6,286.62,527.15,10.52,8.80" target="#b1">[2]</ref> is used. It was developed by our research team. Nemesis is a French proper name recognizer for large-scale information extraction, whose specifications have been elaborated through corpus investigation both in terms of referential categories and graphical structures. The graphical criteria are used to identify proper names and the referential classification to categorize them. The system is a classical one: it is rule-based and uses specialized lexicons without any linguistic preprocessing. Its originality consists on a modular architecture which includes a learning process. For example, lets take the 7th question: «Quel pays l' Irak a -t-il envahi en 1990 ?» ("Which country Iraq did it invade in 1990?"). Figure <ref type="figure" coords="6,483.77,610.84,4.98,8.80" target="#fig_4">5</ref> show what NEMESIS gives as results for the 21th sentence of "LEMONDE95-040819". It detects two country names ("Irak" and "Koweït") and a people's proper name ("Yasser Arafat").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Acronym definition extraction</head><p>For acronym's definition search, we use a tool developed by E. Morin <ref type="bibr" coords="6,393.55,681.02,10.52,8.80" target="#b3">[4]</ref> based on regular expressions. It detects acronyms and links them to their definition (if it exists). For example, lets take the 28th question: «Qu'est-ce que l' OMS ?» ("What means OMS?"). This tool gives results as the one of the figure <ref type="figure" coords="6,181.61,716.89,4.98,8.80" target="#fig_5">6</ref> that shows the analysis the 20th sentence of "ATS.941027.0143" 3 . &lt;enonce num="21" num_doc="LEMONDE95-040819" coef="0.85714287"&gt; &lt;texte&gt;1990 . 2 août : invasion du Koweït par l' Irak , soutenu par Yasser Arafat .&lt;/texte&gt; &lt;texte-EN&gt; 1990 . 2 août : invasion du &lt;NP Categorie="Pays" Classe="Toponyme" Id="1"&gt;Koweït&lt;/NP&gt; par l' &lt;NP Categorie="Pays" Classe="Toponyme" Id="2"&gt;Irak&lt;/NP&gt; , soutenu par &lt;NP Categorie="Patronyme" Classe="Anthroponyme" Id="3"&gt;Yasser Arafat&lt;/NP&gt; . &lt;/texte-EN&gt; &lt;/enonce&gt;  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Pattern-based answer extraction</head><p>For the pattern-based answer extraction process, we developed our own tool. According to question categories, syntactic patterns were defined in order to extract answer(s) (see figure <ref type="figure" coords="7,442.75,433.88,4.98,8.80" target="#fig_7">7</ref> for an example of a pattern set associated with the question category called "Definition"). These patterns are based on the question focus and makes it possible to the system to extract the answer. Patterns are sorted according to their priority, ie answers extracted by a pattern with an higher priority are considered as better answers than the ones extracted by patterns with a lower priority.  As a result, for a given question, patterns associated with the question category are applied to all selected passages. Thus, we obtain a set of candidate answers for this question. For example, lets take the 2nd question: «Qu'est ce que Hubble ?» ("What is Hubble?"). This question corresponds to the category "Definition" (see figure <ref type="figure" coords="7,295.44,649.94,3.87,8.80" target="#fig_7">7</ref>). Pattern-based extraction process gives a set of candidate answers as the one presented in figure <ref type="figure" coords="7,303.90,661.90,3.88,8.80">8</ref>.</p><p>Patterns (syntactic patterns) are based on the noun phrase that contains the focus of the question. Therefore, the first step consists in selecting only passages which could contain the answer and which contain the focus of the question. To apply syntactic patterns, passages are parsed and divided into basic phrases such as noun phrase (GN), adjectival phrase (GA), adverbial phrase (GR), verb phrase (NV), etc. We use a parser which is based on TreeTagger tool for annotating text with part-of-speech and lemma information. Subsequently, passages are studied to detect the focus noun phrase and to apply each pattern of the question's category. The figure <ref type="figure" coords="7,508.02,745.58,4.98,8.80" target="#fig_8">9</ref> &lt;QUESTION num="0002" nbrep="1" categorie="Definition"&gt; &lt;REP num="1" doc="LEMONDE95-040794-1.0"&gt; l' objectif &lt;/REP&gt; &lt;REP num="1" doc="LEMONDE95-023629-1.0"&gt; le télescope &lt;/REP&gt; &lt;REP num="1" doc="LEMONDE95-033842-1.0"&gt; par le télescope &lt;/REP&gt; &lt;REP num="1" doc="ATS.940217.0089-1.0"&gt;la réparation&lt;/REP&gt; &lt;REP num="1" doc="LEMONDE95-023628-1.0"&gt; le télescope sa position&lt;/REP&gt;...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;/QUESTION&gt;</head><p>Figure <ref type="figure" coords="8,134.35,206.90,3.88,8.80">8</ref>: Example of noun phrases extracted using our pattern-based extraction algorithm ... &lt;Groupe type="PV" id="E-3G8"&gt; &lt;F id="E-3F0"&gt;à&lt;/F&gt; &lt;F id="E-3F1"&gt;équiper&lt;/F&gt; &lt;/Groupe&gt; &lt;Groupe type="GR" id="E-3G9"&gt; &lt;F id="E-3F0"&gt;aussi&lt;/F&gt; &lt;/Groupe&gt; &lt;Groupe type="GN" id="E-3G10"&gt; &lt;F id="E-3F0"&gt;le&lt;/F&gt; &lt;F id="E-3F1"&gt;télescope&lt;/F&gt; &lt;/Groupe&gt; &lt;Groupe type="GP" id="E-3G11"&gt; &lt;F id="E-3F0"&gt;de&lt;/F&gt; &lt;F id="E-3F1"&gt;Hubble&lt;/F&gt; &lt;/Groupe&gt; &lt;Groupe type="GP" id="E-3G12"&gt; &lt;F id="E-3F0"&gt;d'&lt;/F&gt; &lt;F id="E-3F1"&gt;une&lt;/F&gt; &lt;F id="E-3F2"&gt;optique&lt;/F&gt; &lt;/Groupe&gt; ... gives an example of an annotated sentence for the question 2 («Qu'est ce que Hubble ?»). In this case, all patterns, for the category "Definition" (see figure <ref type="figure" coords="8,347.97,444.76,3.88,8.80" target="#fig_7">7</ref>), are applied. The pattern "GNRep GNFocus" can be applied (the answer focus is "Hubble"). Thus, the noun phrase "E-3G10" is a candidate answer.</p><p>For the "Definition" category, the pattern strategy gives good results. Nevertheless, for more complex questions, a semantic process could improve the answer search. Indeed, sometimes the build of powerful patterns is a difficult task (such as for the question «Dans quel lieu des massacres de Musulmans ont-ils été commis en 1995 ?») knowing our patterns are based on the question's focus and the focus is not always easy to find. In addition, the answer type is not always easy to define without semantic information. Another improvement can take into account verb categorization. Indeed, the verb in the question is quite important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Answer selection</head><p>When the answer type was been determined by the question analysis step, the process extracts, from the list of passages provided by the previous step, the candidate answers. Named entities, acronym definitions or numerical entities closest to the question focus (if this last is detected) are supported. Indeed, in such cases, the answer is often situated close to the question focus.</p><p>The answer selection process depends on the question category. For numerical entities, named entities and acronym definitions, the right answer is the one with the best frequency. This frequency is weighted according to several heuristics such as: the distance (in words) between this answer and the question focus, the presence in the sentence of named entities or dates from the question, etc. For answers extracted by the pattern-based selection, two strategies are used according to the question category:</p><p>• the selection of the first selected answer obtained by the first applicable pattern,</p><p>• the selection of the most frequent answer (the candidate answer frequency).</p><p>Most of the time, the first heuristic is the better one. Indeed, the selected answer is the first one obtained by the first applicable pattern (patterns sorted according to their convenience) and into the first passage (sorted by the passage selection step according to their convenience). Nevertheless, for definitionnal questions such as the 90th question «Qui est Boris Becker ?» ("Who is Boris Becker?") or the first question «Qu'est ce qu'Atlantis ?» ("What is Atlantis?"), we noted that the better strategy is the candidate phrase frequency. Indeed, for this question category where the number of question's terms is low, the passage selection step does not make it possible to the system to select with precision passages containing the answer. Therefore, the frequency-based strategy generally selects the right answer. For example, for the second question, the answer «télescope» is selected for the definitionnal question (figure <ref type="figure" coords="9,354.97,219.56,4.43,8.80" target="#fig_8">9</ref>) because of its frequency. Our process find at least 23 right answers. 71 questions (35.5%) was considered as pattern-based answer search type. Our process finds at least 16 right answers.</p><p>5 questions (2.5%) were analyzed as acronym definition search: question 28 (OMS), question 48 (OUA), question 95 (RKA), question 129 (KMT) and question 145 (TDRS). Our system found 4 good answers (28 with "Organisation Mondiale pour la Santé", 48 with "Organisation de l' unité africaine", 129 with "Kouomintang" and 145 with "Tracking and Data Relay Satellite"). Only "RKA" (for "Agence Spatiale Russe") was not found. This is a specific case. In fact, the definition does not contain the letter "K". Actually, "RKA" is based on the russian definition that does not appear in the corpus.</p><p>For 4 questions (5 questions awaiting a list were undetected), a list of answers is awaited:</p><p>• question 88 (Pattern-based search strategy), «Citer le nom de tous les aéroports de Londres , en Angleterre .» which demands an unlimited list,</p><p>• question 92 (Named entities extraction strategy), «Qui sont les deux principaux responsables de l' attentat d' Oklahoma City ?», waiting for 2 answers,</p><p>• question 100 (Named entities extraction strategy), «Donner le nom des neuf planètes qui constituent le système solaire .», 9 answers awaited.</p><p>• question 117 (Named entities extraction strategy), «Quels sont les sept pays les plus industrialisés du monde ?», 7 answers awaited.</p><p>In such cases, our process does not produce satisfactory results. For the first "question" (88), a process problem due to the unlimited list causes no answer ! For the second one (92), we give the right answers (with the same associated sentence). For the third one (100), we give only one wrong answer. For the last one (117), we have found 3 of the seven answers, ie (Canada, Russia, Bosnia, Italy, USA, Ukraine, Uruguay) instead of (USA, Canada, Japan, United Kingdom, France, Germany and Italy).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this experiment report, we have studied our second version of the Prodicos QA system on QA@CLEF2006 question set. The comparison between this version and the first one studied on QA@CLEF2005 is not an easy task. Indeed, question types are quite different. For instance, in the CLEF'2005's session, 21 questions were acronym definition search. Conversely, in the CLEF'2006's session, only 5 questions were acronym definition search. Consequently, we have not presented our result relatively to the preceding system.</p><p>The hard result of the evaluation of the 2006 session is a rate of good answers (overall accuracy) of 29% (14.5% at the 2005 session). We regard this result as encouraging (although definitely perfectible). In addition, acronym definition questions are less numerous while our tool is more powerful. The rate of good answers is definitely low but some improvements were made. For instance, the pattern-based answer extraction found 16 answers whereas it found only 2 answers last year. In addition, in this process, 11 answers are "inexacts" (int the set "X"). As a result, it is obvious that the syntactic parser has to be improved to reduce the set "X" and, consequently, to increase the set "R".</p><p>Positive points are: (1) a good study of the question type, (2) a correct passage search and (3) an improvement of the answer extraction process. Nevertheless, some improvements have to be done concerning: (1) the question focus identification, (2) the use of semantic resources in French language for all process steps and (3) the answers extraction processes. Furthermore, we have to improve our French semantic ressource. Indeed, EuroWordnet in its French version has some defaults like the lack of definitions for concepts, relations between some concepts are unavailable, etc.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,93.39,459.60,119.20,7.38;3,95.60,469.07,44.96,7.38;3,97.82,478.53,356.31,7.38;3,97.82,487.99,141.60,7.38;3,97.82,497.46,164.19,7.38;3,97.82,506.92,78.52,7.38;3,97.82,516.39,156.36,7.38;3,97.82,525.85,202.66,7.38;3,97.82,535.32,152.31,7.38;3,97.82,544.78,101.60,7.38;3,102.25,554.25,182.26,7.38;3,102.25,563.71,282.45,7.38;3,100.03,573.17,39.42,7.38;3,97.82,582.64,47.17,7.38;3,93.39,592.10,53.07,7.38"><head>&lt;QUESTION</head><label></label><figDesc>NumQuest="0007"&gt; &lt;ANALYSE&gt; &lt;TEXTE_QUESTION lang="F"&gt;Quel pays l' Irak a -t-elle envahit en 1990 ?&lt;/TEXTE_QUESTION&gt; ... /EN&gt;&lt;/TYPE_EN&gt; &lt;LISTE_NP&gt;&lt;NP&gt;Irak&lt;/NP&gt;&lt;/LISTE_NP&gt; &lt;FOCUS lang="F" trad="N"&gt; &lt;LEMMES&gt;&lt;LEMME&gt;pays&lt;/LEMME&gt;&lt;/LEMMES&gt; &lt;TETE forme="pays" eti="NN" leNum="F2"&gt;&lt;LEMME&gt;pays&lt;/LEMME&gt;&lt;/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,212.32,629.03,178.37,8.80"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of a question analysis</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,213.59,320.28,175.83,8.80;6,159.77,108.86,283.46,186.41"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The answer extraction process</figDesc><graphic coords="6,159.77,108.86,283.46,186.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,195.58,476.94,211.84,8.80"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example of a numerical entities search</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,202.37,216.37,198.26,8.80"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Example of a named entities search</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="7,190.62,372.59,221.75,8.80"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Example of an acronym definition search</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="7,161.49,590.60,280.02,8.80"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Example of patterns associated to a question category</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="8,99.70,401.27,403.61,8.80"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: The first syntactically annotated sentence containing the answer of the question 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="2,159.77,303.60,283.46,186.41"><head></head><label></label><figDesc></figDesc><graphic coords="2,159.77,303.60,283.46,186.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,90.00,219.56,423.00,174.81"><head>Table 2 :</head><label>2</label><figDesc>Table 2 presents all results of our run according to question types. Results synthesis 43 questions (21.5%) was considered as named entities extraction strategy. Our process find at least 15 right answers. 81 questions (40.5%) was considered as named entities extraction strategy.</figDesc><table coords="9,166.77,254.16,269.46,72.57"><row><cell>Question type</cell><cell cols="5">R U X W Total</cell></row><row><cell>Named entities extraction</cell><cell cols="2">23 2</cell><cell>3</cell><cell>53</cell><cell>81</cell></row><row><cell>Numerical entities extraction</cell><cell cols="2">15 0</cell><cell>4</cell><cell>24</cell><cell>43</cell></row><row><cell>Acronym definition search</cell><cell>4</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>5</cell></row><row><cell cols="5">Pattern-based answer extraction 16 0 11 44</cell><cell>71</cell></row><row><cell>Total</cell><cell cols="4">58 2 18 122</cell><cell>200</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,105.24,746.96,138.14,7.04"><p>excepted for definitional questions<ref type="bibr" coords="3,234.45,746.96,8.93,7.04" target="#b2">[3]</ref> </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,105.24,743.55,135.04,7.04"><p>http://lucene.apache.org/java/docs/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="6,105.24,735.81,388.66,7.04"><p>In this example "SIGLE" describes an acronym and "DEF" (with the same identifier "no") its definition</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,105.50,427.64,407.51,8.80;10,105.50,439.60,407.51,8.80;10,105.50,451.55,407.50,8.80;10,105.50,463.51,407.51,8.80;10,105.50,475.46,296.88,8.80" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,323.12,427.64,166.14,8.80">The query answering system Prodicos</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Monceaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Jacquin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Desmontils</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,152.48,451.55,360.51,8.80;10,105.50,463.51,219.12,8.80">Proceedings of Accessing Multilingual Information Repositories: 6th Workshop of the Cross-Language Evaluation Forum, CLEF 2005</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müllern</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</editor>
		<meeting>Accessing Multilingual Information Repositories: 6th Workshop of the Cross-Language Evaluation Forum, CLEF 2005<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2005-09">September 2005</date>
			<biblScope unit="volume">4022</biblScope>
		</imprint>
	</monogr>
	<note>Revised Selected Papers. forthcoming</note>
</biblStruct>

<biblStruct coords="10,105.50,495.39,407.51,8.80;10,105.50,507.34,298.97,8.80" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="10,163.07,495.39,349.93,8.80;10,105.50,507.34,31.72,8.80">Identification et catégorisation automatiques des entités nommées dans les textes français</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Fourour</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<pubPlace>LINA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Université de Nantes</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">These en informatique</note>
</biblStruct>

<biblStruct coords="10,105.50,527.27,407.50,8.80;10,105.50,539.22,407.51,8.80" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="10,171.61,527.27,341.39,8.80;10,105.50,539.22,147.35,8.80">Adaptation du niveau d&apos;analyse des interventions dans un dialogue -application à un système de question -réponse</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Monceaux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>LIMSI</publisher>
			<pubPlace>Paris Sud; ORSAY</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">These en informatique</note>
</biblStruct>

<biblStruct coords="10,105.50,559.15,407.51,8.80;10,105.50,571.10,407.51,8.80;10,105.50,583.06,402.77,8.80" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="10,161.81,559.15,351.19,8.80;10,105.50,571.10,44.32,8.80">Extraction de liens sémantiques entre termes à partir de corpus de textes techniques</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Morin</surname></persName>
		</author>
		<ptr target="http://www.sciences.univ-nantes.fr/info/perso/permanents/morin/article/morin-these99.pdf" />
		<imprint>
			<date type="published" when="1999-12">Décembre 1999</date>
		</imprint>
		<respStmt>
			<orgName>Université de Nantes, LINA</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Thèse en Informatique</note>
</biblStruct>

<biblStruct coords="10,105.50,602.99,407.50,8.80;10,105.50,614.94,407.51,8.80;10,105.50,626.90,407.51,8.80" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,164.50,602.99,324.64,8.80">Improvements in Part-of-Speech Tagging with an Application To German</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,479.50,614.94,33.51,8.80;10,105.50,626.90,204.89,8.80">Natural Language Processing Using Very Large Corpora</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Armstrong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Chuch</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Isabelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Tzoukermann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Yarowski</surname></persName>
		</editor>
		<meeting><address><addrLine>Dordrecht</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic Publisher</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,646.82,407.51,8.80;10,105.50,658.78,194.92,8.80" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,156.35,646.82,273.89,8.80">EuroWordNet: A Multilingual Database with Lexical Semantic</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Vossen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,472.52,646.82,40.48,8.80;10,105.50,658.78,50.15,8.80">Networks Piek Vossen</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>university of Amsterdam</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
