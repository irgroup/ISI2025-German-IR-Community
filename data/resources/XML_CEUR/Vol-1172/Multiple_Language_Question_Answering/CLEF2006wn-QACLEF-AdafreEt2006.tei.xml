<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,370.51,148.86,123.16,15.15">at WiQA 2006</title>
				<funder ref="#_gga4FRM">
					<orgName type="full">Netherlands Organisation for Scientific Research (NWO)</orgName>
				</funder>
				<funder ref="#_wtsQJvd #_zuAPpKq #_VdmZEAe">
					<orgName type="full">NWO</orgName>
				</funder>
				<funder ref="#_ytyTrhg #_sdFqV58 #_5H3s2Tk #_jHsftj8 #_g4bvzNG #_nY92Xuk #_DFG3n2c #_ZMbstts #_3u3t3AN #_yZGqc4n #_vAHxpuC">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,162.40,182.75,58.14,8.74"><forename type="first">Sisay</forename><surname>Fissaha</surname></persName>
							<email>sfissaha@science.uva.nl</email>
						</author>
						<author>
							<persName coords="1,223.86,182.75,29.36,8.74;1,273.14,182.75,36.27,8.74"><forename type="first">Adafre</forename><surname>Valentin</surname></persName>
						</author>
						<author>
							<persName coords="1,312.72,182.75,31.97,8.74;1,364.62,182.75,75.99,8.74"><forename type="first">Jijkoun</forename><surname>Maarten De Rijke</surname></persName>
							<email>jijkoun@science.uva.nl</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">ISLA</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,370.51,148.86,123.16,15.15">at WiQA 2006</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F3BDB94F75E3A9D4F7AAD568791DF221</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Managment]: Languages-Query Languages Measurement, Performance, Experimentation Question answering, Questions beyond factoids, Importance ranking, Novelty detection, Duplicate removal</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation in the WiQA 2006 pilot on question answering using Wikipedia. We present an analysis of the results of our system for both monolingual and bilingual runs. The system currently works for Dutch and English.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The WiQA 2006 pilot deals with access to the content of Wikipedia, the free online encyclopedia. At WiQA, information access is considered both from a reader's point of view and from an author's point of view-WiQA derives much of its motivation from the observation that, in the Wikipedia context, the distinction between reader and author has become blurred; see <ref type="bibr" coords="1,430.38,552.82,10.52,8.74" target="#b2">[3]</ref> for an overview of the task. The overlap in the roles of the different types of users in Wikipedia motivates new modes of information access, ones that can support the emerging dual roles identified above.</p><p>WiQA 2006 attempts to address this issue by formulating the task definition as follows. Given a topic (and associated article) in one language, identify relevant snippets on the topic from other articles in the same language or even in other languages. In the context of Wikipedia, having a system that offers this type of functionality capability is important both for using Wikipedia as a reader and as an author. It provides effective access to additional relevant information in Wikipedia that is not found in the main article on the topic, which, we believe, is important for both use cases.</p><p>In this report, we describe our participation in the WiQA 2006 pilot. We took part both in the monolingual and bilingual tasks. Section 2 presents an overview of the system. Section 3 describes the runs that we submitted. Section 1 provides an analysis of the results. Finally, Section 5 presents some concluding remarks.</p><p>We briefly describe our mono-and bilingual systems for the WiQA 2006 pilot; for a far more detailed description of our monolingual system, please consult <ref type="bibr" coords="2,363.75,145.80,10.52,8.74" target="#b1">[2]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Monolingual Task</head><p>The main components of the monolingual system are aimed at addressing the following subtasks: identifying relevant snippets, estimating sentence importance, and removing redundant snippets. We now discuss each of these in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Identifying relevant snippets</head><p>We apply two approaches to identifying relevant sentences. The first exploits the peculiar characteristics of Wikipedia, i.e., the dense link structure, to identify relevant snippets. The second approach makes use of standard document retrieval techniques. We briefly review the two approaches below.</p><p>Link based retrieval. The link structure, particularly, the structure of the incoming links (similar to Wikipedia's 'What links here' feature), provides a simple mechanism for identifying relevant articles. If an article contains a citation to the topic then it is likely to contain relevant bits of information about the topic since the hyperlinks in Wikipedia are part of the content of the article. Since hyperlinks are created manually by humans, this approach tends to produce little noise. However, due to inconsistencies in manual processing and also editing requirements, not all mentions of a topic may be hyperlinked which may cause some recall problems. Furthermore, coreferences may also need to be resolved in order to improve recall.</p><p>We devised a simple coreference resolution method for a particular class of coreferences, i.e., last name resolution for a person. This method checks whether a person name appears at least once in the article as hyperlink. Then the person's name will be tokenized into words and the last word will be taken as last name of the person. The last name will then be replaced by the full name. This ommits a large class of coreferences such as pronouns and definite descriptions. However, the use of pronouns in Wikipedia is relatively low. Furthermore, current coreference resolution techniques mostly use deep linguistic analysis which is hard to scale up to corpora of the size of Wikipedia.</p><p>Lucene based retrieval. We indexed the Wikipedia articles using the Lucene retrieval engine <ref type="bibr" coords="2,111.87,539.21,9.96,8.74" target="#b3">[4]</ref>. We assumed each article to constitute a single document in the index. The resulting index is used to retrieve articles that contain information about a topic. We used the title of the topic, which corresponds to the title of a Wikipedia article, as a query to retrieve articles. Since the aim is to identify smaller snippets about the topic, we split articles into sentences and keep only those sentences that contain an occurrence of the topic at hand. For non-person topics, we require the snippets to contain all the tokens of the topic. On the other hand, for person topics, we also consider snippets that contain one of the tokens in the name. Unlike the link based retrieval approach outlined above, which requires strict hyperlink relation, this method only checks whether a snippet contains the title of the topic which may not necessarly imply a hyperlink relation. As a result, the method tends to be recall oriented, identifying more relevant snippets. However, the relaxed criteria also means that the method is likely to pick up more noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Estimating Sentence Importance</head><p>Once we have an initial set of relevant sentences, the next phase of the processing step is to rank the sentences based on their importance to the topic. For this, we combine several types of evidence. These include retrieval scores, position in the article, and graph-based ranking scores.</p><p>Retrieval scores. The topic is used as a query to retrieve the initial set of relevant articles. Articles containing multiple occurrences of the topic will be ranked higher. The assumption is that such higher ranking articles are likely to contain larger number of relevant sentences, which in turn increases the likelihood of getting important and novel sentences. This provides for a relatively weak source of evidence of importance of a sentence in an article. We convert the retrieval scores to values between 0 and 1 by dividing each retrieval score with the maximum value.</p><p>Position in article. The position of a sentence in an article is used as an extra indication for its importance for the topic of the article from which the sentence is extracted. The earlier the sentence appears in the article, the more important it is assumed to be. The sentence positions are converted into a value between 0 and 1 in which the sentence at position 1 receives the maximum graph-based score (explained below), and subsequent sentences receive a score which is a fraction of the maximum graph-based score using the formula proposed in <ref type="bibr" coords="3,379.80,257.27,9.96,8.74" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph-based scoring</head><p>The previous methods make use of local context in assigning relative importance to sentences. Graph-based scoring allows us to rank sentences by taking into account a more global context of the sentences. This global context consists of a representative sample of articles that belong to the same categories as the topic. The resulting corpus serves as our importance model by which we assign each sentence a score. Once we have our representative corpus, we rank sentences based on their centrality with respect to this corpus. For more detailed discussion of the method, see <ref type="bibr" coords="3,220.29,354.69,9.97,8.74" target="#b1">[2]</ref>.</p><p>The overall score of a sentence is the sum of the above scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Filtering</head><p>After we compute the scores for the snippets, the next step involves computing a redundancy penalty <ref type="bibr" coords="3,126.02,422.69,9.96,8.74" target="#b4">[5]</ref>. For this, we sort the snippets by decreasing order of their scores. We compare each candidate sentence with the sentences in the main article, and sentences that appear before it in the ranked list. We used simple word overlap for measuring sentence similarity. We keep the maximum similarity score and subtract it from the sentence score. In all our similarity computations, we remove stopwords. We sort the list again in decreasing order of the resulting scores. The top 10 sentences constitutes the snippets in our submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multilingual Task</head><p>The multilingual task extends the problem of finding important snippets in one language to multiple languages in Wikipedia: given a topic in one language, find important snippets in other Wikipedia articles of the same language or other languages. The major challenge in this task concerns ensuring relevance and novelity acrose multiple languages. This in turn means that we need to have some mechanism of measuring similarity among snippets from different languages. This may be achieved using different approaches, i.e., using a biligual dictionary or machine translation system. For this task, we only used a bilingual dictionary generated from Wikipedia itself. We applied the method on Dutch-English language pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Steps</head><p>Identifying important snippets in a multilingual task consists of several monolingual extraction plus multilingual filtering. Specifically, given a topic in one language, we</p><p>• identify important snippets in the language of the topic (primary language).</p><p>• translate the topic into other languages</p><p>• identify important snippets in the translated topics (secondary languages)</p><p>• filter the resulting multilingual list for redundant information</p><p>The ranked list consists of three types of snippets. The first set contains the snippets extracted from the primary language. These snippets are extracted using the monolingual procedure presented in the Section 2.1 (primary snippets). The second set contains snippets coming from the main article in the secondary language (secondary main article snippets). These are automatically considered as relevant and are included in the main list. The third type of snippets are extracted from other articles in secondary language (secondary article snippets), again using the monolingual algorithm, but this time on the secondary language. The snippets are ordered such that the primary snippets comes at the top followed by secondary main article snippets which are then followed by the secondary other article snippets.</p><p>The above ranked list and the main article in the primary language are input to the filtering module, which goes through the list and removes duplicates. In the next section we briefly summarize the method we adopt for multilingual similarity which forms the core of the filtering module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Using a Bilingual Lexicon</head><p>We now describe the method we used for identifying similar text across different languages. It relies on a bilingual lexicon which is generated from Wikipedia using the link structure. The bilingual lexicon consists of the Wikipedia page titles that typically represents concepts or entities that have entries in Wikipedia. Therefore, similarity measure is based on concept or article title overlap. We used page title translations as our primitives (as main features) for the computation of multilingual similarity. For each Wikipedia page in one language, translations of the title in other languages, for which there are separate entries, are given. This information can be used to generate a bilingual translation lexicon. Most of these titles are noun phrases and are very useful in multilingual similarity computations. Most of these noun phrases are already disambuiguated. They are mostly content bearing terms and may consist of single word or multiword units.</p><p>We used the Wikipedia redirect feature to identify synonymous expression. In Wikipedia, the redirect feature is used to map several titles into a canonical form. For a more detailed description see <ref type="bibr" coords="4,106.11,443.23,9.96,8.74" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Runs</head><p>We submitted a total of seven runs: six monoligual runs for Dutch and English (three runs per language), and one is a Dutch-English bilingual run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Monolingual Runs</head><p>All the monolingual runs use the approach outlined in Section 2. Observe that except for the use of stopwords lists, the method is generic and can be ported to a new language with relative ease. The runs differ in the methods adopted for acquiring the initial set of relevant sentences. As mentioned in Section 2.1.1, we identified two ways of identifying relevant sentences. The first run uses the link based approach whereas the second uses retrieval based approach. The third run combines the retrieval based approach with link based filtering in identifying relevant sentences. The latter two runs use the retrieval score in computing the overall score of sentence importance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bilingual Run</head><p>The bilingual run considers Dutch and English language pairs. The source topics can be in any of these languages. As mentioned in Section 2.2, the method is built on top of our monolingual approach. We used the output of the third monoligual run as an input for the bilingual filtering. The bilingual run makes use of the bilingual lexicon generated automatically from Wikipedia for identifying duplicates across different languages.</p><p>We present the results of our seven runs. The results are assessed based on the following attributes: "supported", "important", "novel" and "not repeated". The summary statistics used as evaluation measures are:</p><p>• Yield at top 10 snippets: total number of supported important novel non-repeated snippets for all topics among the top 10 snippets (yield );</p><p>• Average yield per topic (top 10 snippets): previous, devided by the number of topics (Avg. Yield ) ;</p><p>• MRR (top 10 snippets): Mean Reciprocal Rank of the first supported important novel nonrepeated snippet among top 10, according to the system's ranking(MRR);</p><p>• Precision at top 10 snippets: the number of supported important novel non-repeated snippets among top 10 snippets per topic, divided by the total number of top 10 snippets per topic (Precision).</p><p>The test files for the English and Dutch monolingual tasks consist of 65 and 60 topics respectively. The corresponding test file for bilingual task consists of 60 topics.</p><p>Table <ref type="table" coords="5,132.33,341.06,4.98,8.74" target="#tab_0">1</ref> shows the results of the seven runs. As mentioned in Section 3, our monolingual runs differ on how the initial set of relevent sentences are acquired. In Table <ref type="table" coords="5,419.96,353.02,3.88,8.74" target="#tab_0">1</ref>, the three different approaches are indicated by Ret (retrieval only approach), Link (link only based approach), and LinkRet (the combination of the two methods). The columns in Table <ref type="table" coords="5,399.72,376.93,4.98,8.74" target="#tab_0">1</ref>  Overall the scores for link based monolingual runs are the best. This shows that link based retrieval approach provide a more accurate initial set of relevant sentences on which the performance of the whole system largely depends. The retrieval based approach seems to introduce more noise as shown by the scores of the Ret and LinkRet based monolingual runs for both languages. Contrary to our expectation, the combination of the two methods performed worse for English.</p><p>In order to see whether the three approaches return similar sets of snippets, we counted the number of snippets that are found in the result sets of the three methods for English monolingual runs. The three methods have 305 snippets in common in their result sets. Of these snippets, 103 snippets are judged good snippets where the number of good snippets returned by the three methods are; 220-Link, 191-ret, and 188-linkret.</p><p>Furthermore, the three methods return no good snippets for the following topics, Brooks Williams, Wing Commander (film), Christian County, Illinois, White nationalism, Telenovela database, Oxygen depletion. Although the potential cause for the poor performance on these topics may vary acrose the topics, a brief look at the outputs for these topics reveals some possible sources of problems. For some of these topics, the retrieval component returned very few candidate snippets, e.g. Brooks Williams and Oxygen depletion. For others, it returned a large number of similar snippets, e.g. towns and cities for Christian County, Illinois and different entries of Telenovela database, which are judged irrelevant or redundant by the assessors. Some topics have ambiguous titles, e.g. Wing Commander (film), as indicated by its result set which contains snippets from articles with similar titles, e.g Wing Commander (computer game).</p><p>On the other hand, the three methods return more than 5 good snippets for the following topics: Center for American Progress, Atyrau, Saitama Prefecture, Kang Youwei, Philips Records. Further examination of the outputs of the three methods for these topics shows that the methods tend to return similar sets of good snippets. Overall most of the scores for our English monolingual runs are above the median score. The scores for our best run is close to the maximum score.</p><p>A similar analysis on the output of the Dutch monolingual runs shows similar patterns. Some topics are very hard for all methods. For example, all methods returned no snippets at all for the following topics; Vclav Havel, MG (auto), Tsjetsjeni, Caro, Socit Nationale des Chemins de fer Franais, TVR (auto). This is mainly due to the special characters in the titles of the topics which we did not anticipate and made it impossible to identify candidate snippets. On the other hand, all methods performed well on the following topics; Gilera, Columbia-universiteit, Albert Einstein, Zwarte rat, NSU, Slangendrager .</p><p>The scores for the billingual run is based on the output of the LinkRet retrieval component. The scores tend to be higher than the monolingual scores. This may be due to the fact that most of the topics are Dutch topics that are mostly short and additional snippets are likely to new. Furthermore, the snippets can come from both Dutch and English encyclopedias which also contributes to finding good snippets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper described our participation in the WiQA 2006 pilot. Our approaches consist basically of the following three steps: retrieving candidate snippets, reranking the snippets, and removing duplicates. We compared two approaches for identifying candidate snippets, i.e. link based retrieval and the traditional document based retrieval. The result showed that the link based approach performed better than the document retrieval based approach. The results showed that overall our system performed well. However, there is a lot of room for improvements. In the future, we want to compare different reranking methods and also perform detailed error analysis of the output of our system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,108.06,376.93,400.59,213.21"><head>Table 1 :</head><label>1</label><figDesc>are as described above. Results for English and Dutch monolingual tasks; Dutch-English bilingual task.</figDesc><table coords="5,210.26,400.14,182.49,155.79"><row><cell></cell><cell>English</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Avg. Yield MRR Precision</cell></row><row><cell>Ret</cell><cell>2.938</cell><cell>0.523</cell><cell>0.329</cell></row><row><cell>Link</cell><cell>3.385</cell><cell>0.579</cell><cell>0.358</cell></row><row><cell>LinkRet</cell><cell>2.892</cell><cell>0.516</cell><cell>0.330</cell></row><row><cell></cell><cell>Dutch</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Avg. Yield MRR Precision</cell></row><row><cell>Ret</cell><cell>3.200</cell><cell>0.459</cell><cell>0.427</cell></row><row><cell>Link</cell><cell>3.800</cell><cell>0.532</cell><cell>0.501</cell></row><row><cell>LinkRet</cell><cell>3.500</cell><cell>0.532</cell><cell>0.494</cell></row><row><cell></cell><cell cols="2">English-Dutch</cell><cell></cell></row><row><cell></cell><cell cols="3">Avg. Yield MRR Precision</cell></row><row><cell>LinkRet</cell><cell>5.03</cell><cell>0.518</cell><cell>0.535</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>6 Acknowledgments <rs type="person">Sisay Fissaha Adafre</rs> was supported by the <rs type="funder">Netherlands Organisation for Scientific Research (NWO)</rs> under project number <rs type="grantNumber">220-80-001</rs>. <rs type="person">Valentin Jijkoun</rs> was supported by <rs type="funder">NWO</rs> under project numbers <rs type="grantNumber">220-80-001</rs>, <rs type="grantNumber">600.-065.-120</rs> and <rs type="grantNumber">612.000.106</rs>. Maarten de Rijke was supported by <rs type="funder">NWO</rs> under project numbers <rs type="grantNumber">017.001.190</rs>, <rs type="grantNumber">220-80-001</rs>, <rs type="grantNumber">264-70-050</rs>, <rs type="grantNumber">354-20-005</rs>, <rs type="grantNumber">600.-065.-120</rs>, <rs type="grantNumber">612-13-001</rs>, <rs type="grantNumber">612.000.106</rs>, <rs type="grantNumber">612.066.302</rs>, <rs type="grantNumber">612.069.006</rs>, <rs type="grantNumber">640.001.501</rs>, and <rs type="grantNumber">640.002.501</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_gga4FRM">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_wtsQJvd">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_ytyTrhg">
					<idno type="grant-number">600.-065.-120</idno>
				</org>
				<org type="funding" xml:id="_zuAPpKq">
					<idno type="grant-number">612.000.106</idno>
				</org>
				<org type="funding" xml:id="_VdmZEAe">
					<idno type="grant-number">017.001.190</idno>
				</org>
				<org type="funding" xml:id="_sdFqV58">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_5H3s2Tk">
					<idno type="grant-number">264-70-050</idno>
				</org>
				<org type="funding" xml:id="_jHsftj8">
					<idno type="grant-number">354-20-005</idno>
				</org>
				<org type="funding" xml:id="_g4bvzNG">
					<idno type="grant-number">600.-065.-120</idno>
				</org>
				<org type="funding" xml:id="_nY92Xuk">
					<idno type="grant-number">612-13-001</idno>
				</org>
				<org type="funding" xml:id="_DFG3n2c">
					<idno type="grant-number">612.000.106</idno>
				</org>
				<org type="funding" xml:id="_ZMbstts">
					<idno type="grant-number">612.066.302</idno>
				</org>
				<org type="funding" xml:id="_3u3t3AN">
					<idno type="grant-number">612.069.006</idno>
				</org>
				<org type="funding" xml:id="_yZGqc4n">
					<idno type="grant-number">640.001.501</idno>
				</org>
				<org type="funding" xml:id="_vAHxpuC">
					<idno type="grant-number">640.002.501</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,105.50,694.75,407.50,8.74;6,105.50,706.71,407.50,8.74;6,105.50,718.66,83.69,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,294.31,694.75,218.69,8.74;6,105.50,706.71,52.17,8.74">Finding similar sentences across multiple languages in wikipedia</title>
		<author>
			<persName coords=""><forename type="first">Sisay</forename><surname>Fissaha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adafre</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,181.75,706.71,331.25,8.74;6,105.50,718.66,53.27,8.74">EACL 2006 Workshop on New Text, Wikis and Blogs and Other Dynamic Text Sources</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,112.02,407.51,8.74;7,105.50,123.98,83.75,8.74" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="7,298.64,112.02,210.53,8.74">Learning to identify important biographical facts</title>
		<author>
			<persName coords=""><forename type="first">Sisay</forename><surname>Fissaha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adafre</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>Submitted</note>
</biblStruct>

<biblStruct coords="7,105.50,143.90,385.89,8.74" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="7,282.91,143.90,83.60,8.74">Overview of WiQA</title>
		<author>
			<persName coords=""><forename type="first">Valentin</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
	<note>In This volume</note>
</biblStruct>

<biblStruct coords="7,105.50,163.83,290.78,9.02" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><surname>Lucene</surname></persName>
		</author>
		<ptr target="http://lucene.apache.org/" />
		<title level="m" coord="7,143.26,163.83,110.72,8.74">The Lucene search engine</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,183.75,407.50,8.74;7,105.50,195.71,407.51,8.74;7,105.50,207.66,22.70,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,422.01,183.75,90.98,8.74;7,105.50,195.71,146.16,8.74">Centroid-based summarization of multiple documents</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hongyan</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Magorzata</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Sty</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Tam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,262.56,195.71,178.08,8.74">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="919" to="938" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
