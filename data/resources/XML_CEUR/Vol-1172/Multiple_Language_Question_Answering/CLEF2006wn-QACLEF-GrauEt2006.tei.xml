<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,162.57,146.21,277.85,18.08;1,219.83,168.13,163.33,18.08">The bilingual system MUSCLEF at QA@CLEF 2006</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,158.87,203.19,56.97,10.46"><forename type="first">Brigitte</forename><surname>Grau</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIR group</orgName>
								<orgName type="institution">LIMSI-CNRS</orgName>
								<address>
									<postBox>BP 133</postBox>
									<postCode>91403</postCode>
									<settlement>Orsay Cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,224.17,203.19,85.32,10.46"><forename type="first">Anne-Laure</forename><surname>Ligozat</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIR group</orgName>
								<orgName type="institution">LIMSI-CNRS</orgName>
								<address>
									<postBox>BP 133</postBox>
									<postCode>91403</postCode>
									<settlement>Orsay Cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,317.18,203.19,61.98,10.46"><forename type="first">Isabelle</forename><surname>Robba</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIR group</orgName>
								<orgName type="institution">LIMSI-CNRS</orgName>
								<address>
									<postBox>BP 133</postBox>
									<postCode>91403</postCode>
									<settlement>Orsay Cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,387.67,203.19,52.15,10.46"><forename type="first">Anne</forename><surname>Vilnat</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIR group</orgName>
								<orgName type="institution">LIMSI-CNRS</orgName>
								<address>
									<postBox>BP 133</postBox>
									<postCode>91403</postCode>
									<settlement>Orsay Cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,225.16,217.13,63.54,10.46"><forename type="first">Michael</forename><surname>Bagur</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIR group</orgName>
								<orgName type="institution">LIMSI-CNRS</orgName>
								<address>
									<postBox>BP 133</postBox>
									<postCode>91403</postCode>
									<settlement>Orsay Cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.40,217.13,66.46,10.46"><forename type="first">Kevin</forename><surname>Séjourné</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIR group</orgName>
								<orgName type="institution">LIMSI-CNRS</orgName>
								<address>
									<postBox>BP 133</postBox>
									<postCode>91403</postCode>
									<settlement>Orsay Cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,162.57,146.21,277.85,18.08;1,219.83,168.13,163.33,18.08">The bilingual system MUSCLEF at QA@CLEF 2006</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BEB4BD1C25F42D612795337E0BFA5E5E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>I.2 [Artificial Intelligence]: I.2.7 Natural Language Processing Measurement, Performance, Experimentation Question answering, evaluation, multi-word expressions</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents our bilingual question-answering system MUSCLEF. We underline the difficulties encountered when shifting from a mono to a cross-lingual system, then we focus on the evaluation of three modules of MUSCLEF: question analysis, answer extraction and fusion. We finally present how we re-use different modules of MUSCLEF to participate to AVE (Answer Validation Exercise).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper presents our cross-lingual question answering system, called MUSCLEF. This year we participated to the French-English cross-language task for which we submitted two runs. Like the past two years, we used two strategies: the first one consists in translating only a set of terms selected by the question analysis module, this strategy being implemented in a system called MUSQAT; the second one consists in translating the whole question and then applying our mono-lingual system named QALC.</p><p>To our knowledge, none of the systems participating to CLEF continues to use the first strategy (term translation), which seems indeed to give lower results than the first one. Nevertheless, we think this approach remains interesting for several reasons: some languages may not dispose of good enough translation tools so this approach would be the only means to build cross-language systems; translation tools when they exist are not efficient for all types of questions; finally, our system MUSQAT could be used when the translation of a question is too difficult to obtain.</p><p>The paper is organized according to the following plan: first we describe the architecture of MUSCLEF (section 2), then we underline some difficulties when shifting from a mono to a crosslingual system (3), after we focus on evaluation and give results obtained by three particular modules of MUSCLEF (4), we give also the general results of our participation to CLEF <ref type="bibr" coords="1,497.51,742.62,11.62,10.46" target="#b4">(5)</ref>.</p><p>Lastly, before concluding, we present how we re-used different modules of MUSCLEF to build a first system for the Answer Validation Exercise <ref type="bibr" coords="2,298.91,122.49,11.62,10.46" target="#b5">(6)</ref>.</p><p>2 System overview QALC, our mono-lingual system, is composed of four modules described below, the first three of them begin classical modules of question answering systems:</p><p>• the first module analyzes the question and detects characteristics that will enable us to finally get the answer: the expected answer type, the focus, the main verb and some syntactic features;</p><p>• the second module is the processing of the collection: a search engine, named MG<ref type="foot" coords="2,494.53,253.90,3.97,7.32" target="#foot_0">1</ref> , is applied; then the returned documents are reindexed according to the presence of the question terms. Next a module recognizes the named entities and each sentence is weighted according to the information extracted from the question;</p><p>• the third module is the answer extraction which applies two different strategies depending on whether the expected answer is a named entity or not;</p><p>• the fourth module is the fusion. Indeed our system QALC is applied on the Web as well as on the closed collection of the CLEF evaluation, then a comparison of both set of answers is done; this way, we increase the score of answers that are present in both sets.</p><p>To build MUSCLEF, our cross-lingual question anwering system, we added several modules to QALC, corresponding to both possible strategies to deal with cross-lingualism: question translation and term-by-term translation. In MUSCLEF, the first strategy uses Reverso<ref type="foot" coords="2,453.83,411.30,3.97,7.32" target="#foot_1">2</ref> to translate the questions then our mono-lingual system QALC is applied. The second strategy, that we named MUSQAT, uses different dictionnaries to translate the selected terms (a description and an evaluation of this translation are given section 3).</p><p>Finally, we apply the fusion module to the different sets of answers: a first one corresponds to MUSQAT, a second one corresponds to the application of QALC on the translated questions, both these sets of answers coming from the CLEF collection of documents, and a third one corresponds to the application of QALC on the translated questions using the Web. MUSCLEF is presented Figure <ref type="figure" coords="2,124.61,508.02,3.87,10.46">1</ref>, where the first line of modules corresponds to our mono-lingual system QALC and the second line contains the modules necessary to deal with cross-lingualism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English terms</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fusion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English answers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English questions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>French questions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Named entity tagging</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer extraction</head><p>Reindexing and ranking 3 Shifting from a mono-lingual to a cross-lingual system</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence weighting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document processing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Performance comparison</head><p>After CLEF 2005 evaluation, CLEF organizers gave the original set of question written in good English to the participants, from which all sets of question were derived. Thanks to this new set of questions we could compare the behaviour of our different implementations: mono-lingual QALC, cross-lingual QALC (using Reverso), and cross-lingual MUSQAT. The results are given table <ref type="table" coords="3,116.40,336.74,3.87,10.46" target="#tab_0">1</ref>. The results of document selection and document processing were calculated for 180 questions instead of 200 because of the 20 NIL questions. Each number in this table represents the percentage of questions for which a good document/sentence/answer is returned.</p><p>Concerning the first three lines, we observe a big difference between the mono-lingual and the cross-lingual systems (from to 6 to 17 %). This difference is due to missing translations: for instance acronyms or proper names (which original alphabet can be different from ours) are often not correctly translated. In the last two lines, the differences are more surprising (and we could not explain them yet): the mono-lingual system lost 40% of good answers during answer extraction, while the best cross-lingual system, QALC+Reverso lost 32.5%, and MUSQAT lost 27.7%.</p><p>On the same data of CLEF 2005, <ref type="bibr" coords="3,254.21,444.34,10.52,10.46" target="#b2">[3]</ref> made also this kind of comparison: they report a loss of 24.5% of good answers between their mono-lingual French system QRISTAL (which obtains very high results: 64%) and their English-to-French system<ref type="foot" coords="3,329.74,467.18,3.97,7.32" target="#foot_2">3</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Corpus-based translation validation</head><p>In this section, we present how in MUSQAT, we proceeded to the term and multi-term translation and to the validation of this translation. The translation is achieved using two dictionaries, Magic-Dic<ref type="foot" coords="3,108.86,537.37,3.97,7.32" target="#foot_3">4</ref> and FreeDict<ref type="foot" coords="3,178.64,537.37,3.97,7.32" target="#foot_4">5</ref> , both being under GPL licence. Thus, the system MUSQAT gets several translations for each French word, which can be either synonyms or different translations when the term is polysemic.</p><p>The evaluation made last year (reported in <ref type="bibr" coords="3,296.75,574.30,10.52,10.46" target="#b1">[2]</ref> and <ref type="bibr" coords="3,330.18,574.30,10.79,10.46" target="#b3">[4]</ref>) on term translation in MUSQAT lead us to enhance our approach by the validation of these translations. To proceed to this validation, we used Fastr<ref type="foot" coords="3,151.72,597.13,3.97,7.32" target="#foot_5">6</ref> and searched in a subset of documents (from 700 to 1000 documents per question) of the CLEF collection either the bi-terms or syntactic variants of them. When neither a bi-term translation nor a variant was found, we discarded the corresponding translated terms.</p><p>For example, to the French bi-term cancer du sein corresponded the three following translations: breast cancer, chest cancer and bosom cancer. In the retained document only the first translation is present, this lead us to discard the terms chest, bosom and their corresponding bi-term.</p><p>We hoped this way to decrease the noise due to the presence of wrong translations. Unfortunately, this first experience in translation validation was not convincing for we obtained nearly the same results in MUSQAT with or without it. (22% of good answers without the validation, 23.5% with it).</p><p>Undoubtedly this approach needs to be enhanced but also evaluated on larger corpora. Indeed, we only evaluated it on the corpus of CLEF 2005 questions, on which we obtained the following figures: from the 199<ref type="foot" coords="4,180.72,193.15,3.97,7.32" target="#foot_6">7</ref> questions, we extracted 998 bi-terms from 167 questions and 1657 non empty mono-terms; only 121 bi-terms were retrieved in documents, which invalidated 121 mono-terms and reduced the number of questions with at least one bi-term to 98. The number of invalidated mono-terms (121) is certainly not high enough in this first experiment to enable MUSQAT to reduce the noise due to wrong translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">On-line term translation</head><p>Yet, after the translation and its validation, some terms are absent from these dictionaries, and thus remain untranslated. A module was developed to try and translate these terms using the Web.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Description</head><p>Since some terms are absent from our dictionaries, we decided to look for them in Web resources. These resources can be on-line dictionaries like Mediaco, Ultralingua or other dictionaries from Lexilogos<ref type="foot" coords="4,134.20,378.62,3.97,7.32" target="#foot_7">8</ref> , but not necessarily: for example, we also use the free encyclopedia Wikipedia, and the web site for European languages and cultures Eurocosm.</p><p>Many of the terms that remain untranslated are multi-word terms, which require a special strategy because it is not always possible to search directly for multi-words expressions in the Web resources. The translation is thus composed of three steps. First, all the multi-word terms are cut into single words. Then we browse the Web to get pages from all the on-line resources that contain these words. Each page is mapped into a common format which gives for each term its translations (there can be several ones). Finally, for each term of the original list, we look for all exact matches in the Web pages, and the most frequent translation is chosen. Table <ref type="table" coords="4,131.52,487.30,4.98,10.46" target="#tab_1">2</ref> shows an example of a mapping for the French term "voiture" and table 3 the frequency of each of its translations. To avoid incorrect translations, we only consider the translations of the exact term. For the term "voiture", the most frequent translation is "car" and thus this translation is chosen. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>French term</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The results of this module are summed up in table <ref type="table" coords="5,314.99,228.50,3.87,10.46" target="#tab_3">4</ref>.</p><p>For each corpus, about 30% of the originally untranslated terms were translated by this module. Most of the terms that can't be translated are actually incorrect multi-word terms in French, mostly because the words are lemmatized, which leads to incorrect terms like "second guerre" (instead of "seconde guerre") or "seigneur de anneau" (instead of "seigneur des anneaux").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation of MUSCLEF modules 4.1 Question analysis</head><p>The question analysis module determines several characteristics of the question among which its category, expected answer type (named entity or not) and focus. We conducted a corpus study in order to validate our choice concerning these characteristics, and the focus in particular, on the corpus of English questions and collection. For the focus, we found that 54% of the correct answers contain the focus of the question, while only 32% of the incorrect answers do (against 20% and 11% for an non-empty word chosen by chance in the question), which tends to validate the choice we made for the focus.</p><p>The performance of this module was evaluated in <ref type="bibr" coords="5,325.65,435.16,10.52,10.46" target="#b4">[5]</ref> which estimated its precision and recall at about 90% in monolingual tasks. The performance is lower on translated questions, since the question words or the structure of the question can be incorrectly translated. For example, the question "Quel montant Selten, Nash et Harsanyi ont-ils reçu pour le Prix Nobel d'Economie ?" ("How much money did Selten, Nash and Harsanyi receive for the Nobel Prize for Economics?") is translated into "What going up Selten, Nash and Harsanyi did they receive for the Nobel prize of economy?", which prevents us from determining the right expected answer type "FINAN-CIAL AMOUNT".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Answer extraction</head><p>In MUSQAT and QALC, we use the same method to extract the final short answer from the candidate sentence. And in both these systems, this last step of the question-answering process entails an important loss of performance. Indeed, in MUSQAT and QALC the percentage of questions for which a candidate sentence containing the correct answer is ranked first is around 35%, and as seen in section 3 the percentage of questions for which a correct short answer is ranked first falls to around 25%. During this step, we lose about one third of good answers.</p><p>In <ref type="bibr" coords="5,119.00,636.85,9.96,10.46" target="#b4">[5]</ref>, we exposed the reasons of the low performances of our answer extraction module. The patterns used to extract the answer when the expected type is not a named entity have been improved for the definition questions. In our last test, indeed, 21 questions among the 48 definition questions of CLEF 2005 were correctly tagged by the patterns. But in other cases, patterns still show a very low efficiency, for here linguistic variations are more important and remains usually difficult to manage. QALC + Reverso MUSQAT Run 1 Run 2 26% 22.5% 25% 27% </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Fusion</head><p>Since we now have three sets of results to merge, we proceeded in two steps: we first merged the results of QALC+Reverso and MUSQAT, which gave us our first run. And, as a second run, we merged our first run and the set obtained with QALC+web system. Those tests were done on the CLEF 2005 data, and we can see table <ref type="table" coords="6,413.30,351.98,4.98,10.46" target="#tab_4">5</ref> that neither the first fusion nor the second enabled us to increase our results. Nevertheless, as we can see it section 5, on CLEF 2006 results the second fusion using the web gave better results since we obtained 25 % of good answers with the web and 22 % without.</p><p>[6] report a different experience using the web: for each answer candidate they build a query made of the initial question plus the answer. The query is sent to Google and then they use the total frequency count returned to sort their set of answers. This first approach lead them to a loss of performance, but like us they are confident in this idea of using the web, and will further enhance their approach.</p><p>Concerning the first fusion, both systems (QALC+Reverso and MUSQAT) giving similar results, it is not surprising that the fusion does not increase the number of good answers. However, our fusion algorithm (described in details in <ref type="bibr" coords="6,286.97,483.49,10.79,10.46" target="#b0">[1]</ref>) is mainly based on the scores attributed by the different systems to their answers, and does not take into account the performances of the systems themselves, which could be a interesting way to improve it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Table <ref type="table" coords="6,116.44,562.16,4.98,10.46" target="#tab_5">6</ref> reports the results we obtained at the CLEF 2006 evaluation. As described just above (subsection 4.3), we remind that the first run is the result of the fusion of two systems: QALC+Reverso and MUSQAT, while the second run is the result of the fusion of this first run and of QALC+Web. Last year, the best of our runs obtained a score of 19%, so the improvements brought to our systems can be considered as encouraging. The difference of results between both runs strengthens the idea that the use of an external source of knowledge is an interesting track to follow. We underline, that at the time of writing this paper, only the first answer of each question has been assessed, so the MRR score does not bring more information than the number of good first short answers. The four first lines of results concern 190 questions, the 10 remaining questions were list questions for which the score is on the last line.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Answer validation</head><p>In order to build the Answer Validation system, we used our QA system, applied to the hypotheses and justifications rather than to the questions and the collection, and we added a decision module.</p><p>Our goal was to obtain the information needed to decide whether the answer was entailed by the text proposed to validate it.</p><p>First the initial corpus file goes through a formatting step transforming it into a file which may be treated by our system, then the QA system is used to extract needed information from it, like tagged hypothesis, tagged justification snippet or terms extracted from the question for example. They are written in a pseudo-xml file passed to the decision algorithm. We also get the answer our QA system would have extracted from the proposed justification, which is used to see if the answer to judge is likely to be true.</p><p>Then, the decision algorithm proceeds in two main steps. During the first one, we try to detect quite evident mistakes, such as the answers which are completely enclosed in the question, or which are not part of the justification.</p><p>The second step proceeds to more sophisticated verifications : (a) verifying the adequate type of the expected named entity if there is one; (b) looking the justification for terms judged as important during the question analysis; (c) confirming the decision with an extern-justification module using the latest version of Lucene to execute a number of coupled queries on the collection, like proximity queries (checks if a number of terms can be found close to one another within a text); the top results of each couples queries are compared in order to decide whether the answer is likely to be true or not; (d) comparing the results that our answer-extraction module (part of our QA system) would provide from the justification text.</p><p>The results obtained by these different verifications are combined to decide if the answer is justified or not and to give a confidence score to this decision. Some errors have been corrected after submitting our results to the AVE campaign (which were rather bad, with very few positive answers). We proceeded to a partial evaluation on our positive answers. We found 363 "YES" among more than 3,000 hypothesis-snippet pairs. About 80% of them are "good" ones. When we only consider the "YES" with a confidence score of 1, we obtained 146 answers, with 90% of good answers. So our algorithm has a good precision, but we have not evaluated the recall result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Our cross-lingual system MUSCLEF presents the particularity to use three strategies in parallel: question translation, term-by-term translation and the use of another source of knowledge (limited actually to the Web). The three sets of answers are finally merged thanks to a fusion algorithm proceeding on two set of answers at the same time. The term-by-term strategy gives lower results than the most widely used strategy consisting in translating the question into the target source then applying a mono-lingual strategy. Nevertheless, we think it remains interesting from the multilingualism point of view, and we try to improve it by using of different techniques of translation (use of several dictionaries and on-line resources) and validation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,118.05,543.51,366.91,159.80"><head>Table 1 :</head><label>1</label><figDesc>Comparison of mono-lingual QALC, cross-lingual QALC and MUSQAT</figDesc><table coords="2,118.05,543.51,366.91,159.80"><row><cell></cell><cell>Web / Collection</cell><cell></cell></row><row><cell>Question analysis</cell><cell></cell><cell></cell></row><row><cell>Focus Answer type Semantically linked words</cell><cell>Search engine</cell><cell>2 lists of ranked answers</cell></row><row><cell>Syntactic relations</cell><cell></cell><cell></cell></row><row><cell>Main verb</cell><cell></cell><cell></cell></row><row><cell>Terms</cell><cell></cell><cell></cell></row><row><cell>translation</cell><cell></cell><cell></cell></row><row><cell cols="3">Figure 1: Architecture of MUSCLEF, our cross-language question answering system</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,90.01,543.04,386.82,176.19"><head>Table 2 :</head><label>2</label><figDesc>Translations of the French term voiture</figDesc><table coords="4,119.30,543.04,357.53,154.32"><row><cell></cell><cell>Translation</cell></row><row><cell>voiture</cell><cell>car</cell></row><row><cell>voiture</cell><cell>carriage</cell></row><row><cell cols="2">voiture d'enfant ... voiture voiture voiture de fonction company car baby-carriage ... car automobile ... ...</cell><cell>Translation # of occurrences car 3 automobile 2 coach 1 carriage 1</cell></row><row><cell>voiture</cell><cell>car</cell></row><row><cell>...</cell><cell>...</cell></row><row><cell>clé de voiture</cell><cell>car key</cell></row><row><cell>voiture</cell><cell>automobile</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,303.71,655.56,211.50,22.42"><head>Table 3 :</head><label>3</label><figDesc>Frequency of the different translations of voiture</figDesc><table coords="5,175.86,109.34,251.28,49.51"><row><cell>Corpus</cell><cell cols="2">CLEF 2005 CLEF 2006</cell></row><row><cell># of translated terms</cell><cell>195 (33%)</cell><cell>408 (32%)</cell></row><row><cell># of terms still untranslated</cell><cell>394 (67%)</cell><cell>858 (68%)</cell></row><row><cell>Total # of terms to translate</cell><cell>589</cell><cell>1266</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,212.68,180.22,177.64,10.46"><head>Table 4 :</head><label>4</label><figDesc>On-line term translation results</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,176.33,143.55,250.35,102.92"><head>Table 5 :</head><label>5</label><figDesc>Fusion results on CLEF 2005 data</figDesc><table coords="6,352.38,174.24,71.32,10.46"><row><cell>Run 1</cell><cell>Run 2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,208.72,257.88,185.56,10.46"><head>Table 6 :</head><label>6</label><figDesc>MUSCLEF results at CLEF 2006</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,105.24,724.08,221.15,8.37"><p>MG for Managing Gigabytes, http://www.cs.mu.oz.au/mg/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,105.24,733.58,87.36,8.37"><p>http://www.reverso.net</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,105.24,687.27,407.88,8.37;3,90.00,696.73,265.71,8.37"><p>It is the best of their cross-lingual systems with a percentage of 39.5 of good answers, while their Portugueseto-French system gets 36.5 and their Italien-to-French system gets 25.5.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,101.09,705.11,106.93,9.49"><p><ref type="bibr" coords="3,101.09,705.11,3.66,6.28" target="#b3">4</ref> http://magic-dic.homeunix.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="3,208.02,706.23,15.81,8.37;3,101.09,714.62,63.42,9.49"><p>net/ 5 http://freedict.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="3,164.51,715.74,25.93,8.37;3,101.09,724.12,412.02,9.49;3,90.00,734.71,281.09,8.37"><p>org/en/<ref type="bibr" coords="3,101.09,724.12,3.66,6.28" target="#b5">6</ref> Fastr was developed by Christian Jacquemin, it is a transformational shallow parser for the recognition of term occurrences and variants , http://www.limsi.fr/Individu/jacquemi/FASTR/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="4,105.24,736.35,248.96,8.37"><p>199 instead of 200 because one has been thrown out by the process</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="4,105.24,745.85,69.46,8.37"><p>www.lexilogos.com</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,105.49,602.63,407.51,10.46;7,105.50,614.60,407.51,10.46;7,105.50,626.55,407.50,10.46;7,105.50,638.50,407.52,10.46;7,105.50,650.46,145.32,10.46" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,130.18,626.55,361.70,10.46">Getting reliable answers by exploiting results from several sources of information</title>
		<author>
			<persName coords=""><forename type="first">Jean-Baptiste</forename><surname>Berthelin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gaël</forename><surname>De Chalendar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Faïza</forename><surname>Elkateb-Gara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olivier</forename><surname>Ferret</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brigitte</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martine</forename><surname>Hurault-Plantet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gabriel</forename><surname>Illouz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laura</forename><surname>Monceaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Isabelle</forename><surname>Robba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anne</forename><surname>Vilnat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,105.50,638.50,407.52,10.46;7,105.50,650.46,18.64,10.46">CoLogNET-ElsNET Symposium, Question and Answers : Theoretical and Applied Perspectives</title>
		<meeting><address><addrLine>Amsterdam, Holland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.49,670.38,407.53,10.46;7,105.50,682.33,407.50,10.46;7,105.50,694.29,163.72,10.46" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,490.01,670.38,23.01,10.46;7,105.50,682.33,190.13,10.46">Term translation validation by retrieving bi-terms</title>
		<author>
			<persName coords=""><forename type="first">Brigitte</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anne-Laure</forename><surname>Ligozat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Isabelle</forename><surname>Robba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Madeleine</forename><surname>Sialeu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anne</forename><surname>Vilnat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,316.78,682.33,196.22,10.46;7,105.50,694.29,57.14,10.46">Working Notes, CLEF Cross-Language Evaluation Forum</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.49,714.21,407.51,10.46;7,105.50,726.18,407.50,10.46;7,105.50,738.13,61.24,10.46" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,349.50,714.21,163.51,10.46;7,105.50,726.18,62.56,10.46">Cross lngual question answering using qristal for clef</title>
		<author>
			<persName coords=""><forename type="first">Dominique</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Séguéla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sophie</forename><surname>Nègre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,215.53,726.18,254.29,10.46">Working Notes, CLEF Cross-Language Evaluation Forum</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.49,110.53,407.52,10.46;8,105.50,122.49,407.50,10.46;8,105.50,134.45,168.78,10.46" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,407.11,110.53,105.91,10.46;8,105.50,122.49,224.56,10.46">Evaluation and improvement of cross-lingual question answering strategies</title>
		<author>
			<persName coords=""><forename type="first">Anne-Laure</forename><surname>Ligozat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brigitte</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Isabelle</forename><surname>Robba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anne</forename><surname>Vilnat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,354.97,122.49,158.03,10.46;8,105.50,134.45,43.68,10.46">Workshop on Multilingual Question Answering</title>
		<meeting><address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>EACL</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.49,154.37,407.51,10.46;8,105.50,166.33,367.63,10.46" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,404.68,154.37,108.33,10.46;8,105.50,166.33,159.41,10.46">L&apos;extraction des réponses dans un système de question-réponse</title>
		<author>
			<persName coords=""><forename type="first">Anne-Laure</forename><surname>Ligozat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brigitte</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Isabelle</forename><surname>Robba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anne</forename><surname>Vilnat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,286.23,166.33,77.13,10.46">TALN Conference</title>
		<meeting><address><addrLine>Leuven, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.49,186.26,407.51,10.46;8,105.50,198.21,407.50,10.46;8,105.50,210.16,61.24,10.46" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,287.93,186.26,225.07,10.46;8,105.50,198.21,88.73,10.46">Dfki&apos;s lt-lab at the clef 2005 multiple language question answering track</title>
		<author>
			<persName coords=""><forename type="first">Günter</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Sacaleanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,217.58,198.21,252.67,10.46">Working Notes, CLEF Cross-Language Evaluation Forum</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
