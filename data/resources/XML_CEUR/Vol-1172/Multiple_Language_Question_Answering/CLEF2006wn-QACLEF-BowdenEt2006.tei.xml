<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,129.48,148.63,344.05,15.51">LCC&apos;s PowerAnswer at QA@CLEF 2006</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,106.44,182.13,70.70,9.96"><forename type="first">Mitchell</forename><surname>Bowden</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Computer Corporation Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<settlement>Texas</settlement>
									<country>United States of America</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,185.71,182.13,67.26,9.96"><forename type="first">Marian</forename><surname>Olteanu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Computer Corporation Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<settlement>Texas</settlement>
									<country>United States of America</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,260.89,182.13,90.21,9.96"><forename type="first">Pasin</forename><surname>Suriyentrakorn</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Computer Corporation Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<settlement>Texas</settlement>
									<country>United States of America</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,358.48,182.13,66.35,9.96"><forename type="first">Jonathan</forename><surname>Clark</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Computer Corporation Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<settlement>Texas</settlement>
									<country>United States of America</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,432.45,182.13,63.90,9.96"><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Computer Corporation Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<settlement>Texas</settlement>
									<country>United States of America</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,129.48,148.63,344.05,15.51">LCC&apos;s PowerAnswer at QA@CLEF 2006</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9CDF2EC834B1FC199C958A44B50D6FCE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Managment]: Languages-Query Languages</term>
					<term>I.2 [Artificial Intelligence]: I.2.7 Natural Language Processing Measurement, Performance, Experimentation Open-domain Question Answering, Questions beyond factoids, Statistical machine translation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper reports on Language Computer Corporation's first QA@CLEF participation. For this exercise, we integrated our open-domain PowerAnswer question answering system with our statistical machine translation engine. For 2006, we participated in the English-to-Spanish, French and Portuguese cross-language tasks. We took the approach of intermediate translation, only processing English within the QA system regardless of the input or source languages. The output snippets were then mapped back into the source language documents for the final output of the system and submission. What follows is a description of our system and methodology.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For 2006, Language Computer's open-domain question answering system PowerAnswer <ref type="bibr" coords="1,480.25,603.93,10.55,9.96" target="#b4">[5]</ref> participated in QA@CLEF for the first time. PowerAnswer has previously participated in many other evaluations, notably TREC <ref type="bibr" coords="1,242.38,627.81,9.98,9.96" target="#b0">[1]</ref>, however, this is the first Multilingual QA evaluation the system has entered. We have developed our own statistical machine translation system, which we integrated with PowerAnswer for this evaluation. Since PowerAnswer is a very modular and extensible system, we were able to make a minimum of modifications for this integration for our initial approach.</p><p>Our goals for this year's participation were <ref type="bibr" coords="1,299.70,687.69,12.70,9.96" target="#b0">(1)</ref> to examine how well the current QA system performs when given noisy data, such as that from automatic translation and (2) to examine the performance of the machine translation system in a question answering environment. To that end, we adopted an approach of intermediate translation instead of adapting the QA system to process target languages natively.</p><p>The paper presents a summary of the PowerAnswer system, our machine translation engine, the integration of the two for QA@CLEF 2006, and then follows with a discussion of our results and challenges in this year's CLEF question topics. Automatic question answering requires a system that has a wide range of tools available. There is no one monolithic solution for all question types or even data sources. In realization of this, LCC developed PowerAnswer 2 as a fully-modular and distributed multi-strategy question answering system that integrates semantic relations, advanced inferencing abilities, syntactically constrained lexical chains, and temporal contexts. This section presents an outline of the system and how it was modified to meet the challenges of QA@CLEF 2006.</p><p>PowerAnswer comprises a set of strategies that are selected based on advanced question processing, and each strategy is developed to solve a specific class of questions either independently or together. A Strategy Selection module automatically analyzes the question and chooses a set of strategies with the algorithms and tools that are tailored to the class of the given question. PowerAnswer can distribute the strategies across workers in the case of multiple strategies being selected, alleviating the increase in the complexity of the question answering process by splitting the workload across machines and processors.  Each strategy is a collection of components, (1) Question Processing, (2) Passage Retrieval, and (3) Answer Processing. Each of these components constitute one or more modules, which interface to a library of generic NLP tools. These NLP tools are the building blocks of the PowerAnswer 2 system that, through a well-defined set of interfaces, allow for rapid integration and testing of new tools and third-party software such as IR systems, syntactic parsers, named entity recognizers, logic provers, semantic parsers, ontologies, word sense disambiguation modules, and more. Furthermore, the components that make up each strategy can be interchanged to quickly create new strategies, if needed, they can also be distributed <ref type="bibr" coords="2,357.13,733.77,14.58,9.96" target="#b9">[10]</ref>.</p><p>As illustrated in Figure <ref type="figure" coords="3,210.40,111.33,3.90,9.96" target="#fig_0">1</ref>, the role of the QP module is to determine (1) the expected answer type, (2) to select the keywords used in retrieving relevant passages, and (3) perform any preliminary questions as necessary for resolving question ambiguity. The PR module ranks passages that are retrieved by the IR system, while the AP module extracts and scores the candidate answers. All modules have access to a syntactic parser, a named entity recognizer and a reference resolution system through LCC's generic NLP tool libraries. To improve the answer selection, we take advantage of redundancy in large corpora, specifically in this case, the Internet. As the size of a document collection grows, a question answering system is more likely to pinpoint a candidate answer that closely resembles the surface structure of the question. These features have the role of correcting the errors in answer processing that are produced by the selection of keywords, by syntactic and semantic processing and by the absence of pragmatic information. Usually, the final decision for selecting answers is based on logical proofs from our inference engine COGEX. For this year's QA@CLEF, however, we disabled the logic prover in order to better evaluate the individual components of this QA architecture. COGEX's evaluation on multilingual data was performed in the CLEF Answer Validation Exercise <ref type="bibr" coords="3,259.89,278.73,14.58,9.96" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Overview of Translation Engine</head><p>The translation system used at LCC implements phrase-based statistical machine translation <ref type="bibr" coords="3,499.35,333.45,9.98,9.96" target="#b1">[2]</ref>, the core translation engine is the open-source Phramer <ref type="bibr" coords="3,337.35,345.45,15.46,9.96" target="#b11">[12]</ref> system, developed by one of LCC's engineers. Phramer in turn implements and extends the phrase-based machine translation algorithms implemented by Pharaoh <ref type="bibr" coords="3,238.71,369.33,9.98,9.96" target="#b3">[4]</ref>. A more detailed description of the MT solution that we adopted for Multilingual QA@CLEF can be found in <ref type="bibr" coords="3,333.44,381.33,14.58,9.96" target="#b10">[11]</ref>. We trained the translation system using the European Parliament Proceedings Parallel Corpus 1996-2003 (EUROPARL) <ref type="bibr" coords="3,470.64,393.21,9.89,9.96" target="#b2">[3]</ref>, which provides between 600k and 800k pairs of sentences (sentences in English paired with the translation in another European language). We followed the training procedure described in the Pharaoh training manual<ref type="foot" coords="3,159.60,427.73,3.95,6.97" target="#foot_0">1</ref> to generate the phrase table required for translation.</p><p>In order to translate entire documents, we augmented the core translation engine with (1) tokenization, (2) capitalization, and (3) de-tokenization.</p><p>The tokenization process was performed on the original documents (in French, Portuguese or Spanish), in order to convert the sentences to space-separated entities, in which the punctuation and the words are isolated. The step was required because the statistical machine translation core engine accepts only lowercased tokenized input.</p><p>The capitalization process follows the translation process and it restores the casing of the words. The capitalization tool uses three-gram statistics extracted from 150 million words from the English GigaWord Second Edition<ref type="foot" coords="3,256.56,532.37,3.95,6.97" target="#foot_1">2</ref> corpus, augmented with two heuristics:</p><p>1. first word will always be uppercased 2. if the words appear also in the foreign documents, the casing is preserved (this rule is very effective for proper nouns and named entities)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PowerAnswer-Phramer Integration</head><p>Our cross-language solution for Question Answering was based on automatic translation of the documents in the source language (English). QA was performed on a collection consisting only of English documents. The answers were converted back into the target language (the original language of the documents) by aligning the translation with the original document (checking to see what was the original phrase in the original document that generated the answer in English); when this method failed, the system falls back to machine translation (source → target).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Passage Retrieval</head><p>Making use of PowerAnswer's modular design, we developed three different retrieval methods, settling on the first for our final experiment.</p><p>1. use an index of English words, created from the translated documents 2. use an index of foreign words (French, Spanish or Portuguese), created from the original documents 3. use an index of English words, created from the original documents in correlation with the translation table</p><p>The first solution is the default solution. The entire target language document collection is translated into English, processed through the set of NLP tools and indexed for querying. Its major disadvantage is the computational effort required to translate the entire collection. It also requires updating the English version of the collection when one improves the quality of the translation. Its major advantage is that there are no additional costs during question answering (the documents are already translated). This passage retrieval method is illustrated in Figure <ref type="figure" coords="4,431.34,264.81,3.90,9.96" target="#fig_2">2</ref>.  The second solution, as seen in Figure <ref type="figure" coords="4,278.51,430.05,3.90,9.96" target="#fig_3">3</ref>, requires minimum effort during indexing (the document collection is indexed in its native language). In order to retrieve the relevant documents, we translate the keywords of the IR query (the query submitted by PowerAnswer to the Lucenebased 3 IR system) with alternations as the new IR query (step 1). The translation of keywords is performed using Phramer, by generating n-best translations. This translated query is submitted to the target language index (step 2). The documents retrieved by this query are then dynamically translated into English using Phramer (step 3). We use a cache to store translated documents so that IR query reformulations and other questions that might retrieve the same documents will not need to be translated again. The set of translated documents is indexed into a mini-collection (step 4) and the mini-collection is re-queried using the original English-based IR query (step 5).  For example, the boolean IR query in English ("poem" AND "love" AND "1922") is translated into French as ("poeme" AND ("aiment" OR "aimer" OR "aimez" OR "amour") AND "1922") with the alternations. This new query will return 85 French documents. Some of them do not contain "love" in their automatic translation (but the original document contains "aiment", "aimer", "aimez" or "amour"). Thus, by re-querying the translated sub-collection (that contains only the translation of those 85 documents) we retrieve only 72 English documents that will be passed to PowerAnswer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PowerAnswer Phramer</head><formula xml:id="formula_0" coords="4,95.00,559.45,329.37,81.53">(s6) (s5) (s8) (s7) (s3) (s2) (s1) (s0)</formula><p>The advantage of the second method is that minimum effort is required during collection preparation. Also, the collection preparation might not be under the control of the QA system (i.e. it can be web-based). Also, improvements in the MT engine can be reflected immediately in the output of the integrated system. The disadvantage is that more computation is required at run-time for translating the IR query and the documents dynamically.</p><p>The third alternative extracts during indexing the English words that might be part of the translation and indexes the collection accordingly. The process doesn't involve lexical choice -all choices are considered possible. The set of keywords is determined using the translation table, and collects all words that are part of the translation lattice ( <ref type="bibr" coords="5,365.54,290.73,10.49,9.96" target="#b3">[4]</ref>). Determining only the words according to the translation table (semi-translation) is approximately 10 times faster than the full translation. The index is queried using the original IR query generated by PowerAnswer (with English keywords). After the initial retrieval, the algorithm is similar to the second method: translate the retrieved documents, re-query the mini-collection. The advantage is the much smaller indexing time when compared with the first method, besides all the advantages of the second method. Also, it has all the disadvantages of the second method, except that it doesn't require IR query translation.</p><p>Because preliminary testing proved that there aren't significant differences in recall between the three methods and because the first method is fastest after the document collection is prepared, we used only the first method for the final evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer Processing</head><p>For each of the above methods, PowerAnswer returns the exact answer and the supporting sentence answer the exact was extracted from (all in English). Then, these answers are then aligned to the corresponding text in the target language documents. The final output of the system is the converted responses in the target language with the appropriate supporting snippet. If the alignment method fails, the English answer is converted into the target language as the final response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Our integrated multilingual PowerAnswer system was tested on 190 English → Spanish, 190 English → French and 188 English → Portuguese factoid and definition questions and 10 English → Spanish, 10 English → French and 12 English → Portuguese list questions. For QA@CLEF, the main score is the overall accuracy, the average of SCORE(q), where SCORE(q) is defined for factoids and definition questions as 1 if the top answer for q is assessed as correct, 0 otherwise. Also included are the Mean Reciprocal Rank (MRR) and the Confidence Weighted Score (CWS) that judges how well a system returns correct answers higher in the ranked list of answers. There were several sources of errors in LCC's submission, including one major source that accounts for a 50% loss in accuracy. The sources of errors include: translation misalignments, tokenization errors, and data processing errors -questions and passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Translation misalignments</head><p>Because the version of PowerAnswer used this year is monolingual, the design we used for multilingual question answering involved translating documents dynamically for processing through the QA system and mapping the responses back into the source language documents. This resulted in many places where errors could occur. While the translation of the documents into English did introduce noise into the data such as mistranslations, words that were not translated and should have been or words that should not have been translated and were, it did not affect the QA system as much as we suspected. By far the greatest source of errors was the alignment between the English answers and the source documents which produced the final response list. This source accounts for roughly a 50% loss of accuracy for the tasks we participated in, as Table <ref type="table" coords="6,404.10,284.61,5.03,9.96" target="#tab_6">3</ref> shows. For Portuguese, there was also an error in the submission that accounts for the great difference in accuracy when compared to the Spanish and French results.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data errors</head><p>Questions that contained common nouns in the source language question were one example of data processing errors. Question 83 in the EN-PT task is Who is the director of the film "Caro diario"?.</p><p>Here, the noun "Caro" was translated "expensive" when being indexed in English. Hence, the query keyword "Caro" as it is in the question could not be found in the translated collection by the IR system, causing the passage to receive a lowered score when retrieved on less keywords. Sometimes the wording of the questions also affected the system in a negative way. There were some spelling changes that were unrecoverable by our system, such "Huan Karlos" for Juan Carlos (EN-PT #5). There were also a few instances of keywords for which PowerAnswer was unable to generate the correct alternation, for example "celebrated" in EN-ES #75 In which year was the Football World Cup celebrated in the United States?, where the correct alternation would have been a synonym for "to host".</p><p>The scoring of definition questions tends to be subjective, so there were cases where we believe an answer returned warranted at worst an inexact but was judged wrong, such as EN-PT #84 What is the Unhcr?. PowerAnswer responded with "O Acnur (Alto Comissariado das Nações Unidas para Refugiados) consultou o Brasil e mais 30 países sobre a possibilidade de acolher um grupo de 5.000 refugiados da ex-Iugoslávia", which was judged as wrong. Additionally, the definition question strategy often returned answer snippets that were a full sentence and were judged as inexact because of their length. One example is EN-PT question 34 Who was Alexander Graham Bell?. PowerAnswer returns the full sentence containing the important nugget "A empresa foi fundada em 1885 e entre os sócios estava Alexander Graham Bell, o inventor do telefone", where the final part "inventor of the telephone" was all that was necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>While this year's performance was not what we hoped due to some major errors in the final stages of processing answers, we look forward to better results in the following years. One large step we will be taking is to make the core of PowerAnswer more language-independent. The English dependency comes from the NLP tools more than the modules of PowerAnswer, where the language dependence occurs primarily in Question Processing. By developing a set of multilingual NLP tools, including syntactic parsers and semantic relations extractors, and abstracting the Englishdependent components of PowerAnswer out and building language-independent as well as some language-specific question processing, we hope to see great improvement in our multilingual QA capabilities. This work will be eased by the flexible design of PowerAnswer and our libraries of generic NLP tool interfaces. For next year's CLEF, we plan on submitting results from an improved and corrected system using the same method we have discussed in this paper, as well as from a more language-independent PowerAnswer that can process the source language text natively without intermediate translation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,217.44,626.13,168.08,9.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: PowerAnswer 2 Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,171.24,406.17,260.17,9.96"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Passage Retrieval on English documents (default)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,171.36,716.49,260.14,9.96;4,101.04,734.74,3.60,4.10;4,105.24,735.62,94.93,7.97"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Passage Retrieval on Target Language documents 3 http://lucene.apache.org/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,90.00,135.21,422.65,146.07"><head>Table 1 :</head><label>1</label><figDesc>Table 1 lists the cross-lingual tasks in which we participated. LCC's QA@CLEF tasks 2 Overview of LCC's PowerAnswer</figDesc><table coords="2,256.20,170.36,87.41,43.79"><row><cell>Source Target</cell></row><row><cell>English French</cell></row><row><cell>English Spanish</cell></row><row><cell>English Portuguese</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,90.00,640.05,422.66,78.82"><head>Table 2</head><label>2</label><figDesc>illustrates the final results of Language Computer's efforts in our first participation at QA@CLEF for 2006.</figDesc><table coords="5,210.12,674.36,179.52,44.51"><row><cell>Source</cell><cell>Accuracy</cell><cell>CWS</cell><cell>MRR</cell></row><row><cell>Spanish</cell><cell>20.00%</cell><cell cols="2">0.04916 0.2000</cell></row><row><cell>French</cell><cell>21.05%</cell><cell cols="2">0.04856 0.2623</cell></row><row><cell>Portuguese</cell><cell>8.51%</cell><cell cols="2">0.01494 0.1328</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,170.64,731.37,261.48,9.96"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note coords="5,210.09,731.37,222.03,9.96"><p>LCC's QA@CLEF 2006 Factoid/Definition Results</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="6,185.04,388.65,232.74,9.96"><head>Table 3 :</head><label>3</label><figDesc>LCC's Factoid/Definition Results in English</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,105.24,708.74,185.34,7.97"><p>http://www.iccs.inf.ed.ac.uk/ pkoehn/training.tgz</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,105.24,718.22,291.27,7.97"><p>http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2005T12</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,110.51,285.69,402.39,9.96;7,110.52,297.57,402.01,9.96;7,110.52,309.57,76.83,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,147.22,297.57,271.97,9.96">Employing Two Question Answering Systems in TREC-2005</title>
		<author>
			<persName coords=""><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christine</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mitchell</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andy</forename><surname>Hickl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,447.46,297.57,65.08,9.96;7,110.52,309.57,45.99,9.96">Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.51,329.49,402.29,9.96;7,110.52,341.49,265.99,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,353.70,329.49,154.93,9.96">Statistical phrase-based translation</title>
		<author>
			<persName coords=""><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Franz</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,110.52,341.49,148.87,9.96">Proceedings of HLT/NAACL 2003</title>
		<meeting>HLT/NAACL 2003<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.51,361.41,402.31,9.96;7,110.52,373.29,49.25,9.96" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="7,179.48,361.41,273.06,9.96">Europarl: A Parallel Corpus for Statistical Machine Translation</title>
		<author>
			<persName coords=""><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
			<pubPlace>MT Summit</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.51,393.21,402.27,9.96;7,110.52,405.21,225.86,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,180.80,393.21,331.98,9.96;7,110.52,405.21,56.44,9.96">Pharaoh: A beam search decoder for phrase-based statistical machine translation models</title>
		<author>
			<persName coords=""><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,188.47,405.21,116.85,9.96">Proceedings of AMTA 2004</title>
		<meeting>AMTA 2004</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.51,425.13,402.15,9.96;7,110.52,437.13,354.12,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,442.06,425.13,70.61,9.96;7,110.52,437.13,188.68,9.96">PowerAnswer 2: Experiments and Analysis over TREC 2004</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christine</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mitchell</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,320.45,437.13,113.23,9.96">Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.51,457.05,402.25,9.96;7,110.52,468.93,300.22,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,362.91,457.05,149.85,9.96;7,110.52,468.93,61.93,9.96">Temporal Context Representation and Reasoning</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christine</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,193.86,468.93,90.02,9.96">Proceedings of IJCAI</title>
		<meeting>IJCAI<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.51,488.97,402.25,9.96;7,110.52,500.85,325.11,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,436.88,488.97,75.88,9.96;7,110.52,500.85,132.80,9.96">COGEX A Logic Prover for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christine</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steve</forename><surname>Maiorano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,264.82,500.85,138.25,9.96">Proceedings of the HLT/NAACL</title>
		<meeting>the HLT/NAACL</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.51,520.77,402.01,9.96;7,110.52,532.77,185.95,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,275.14,520.77,165.78,9.96">Lexical chains for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Novischi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,463.02,520.77,49.50,9.96;7,110.52,532.77,48.21,9.96">Proceedings of COLING</title>
		<meeting>COLING<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-08">August 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.51,552.69,402.14,9.96;7,110.52,564.57,268.66,9.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,247.92,552.69,264.73,9.96;7,110.52,564.57,97.68,9.96">Logic Form Transformation of WordNet and its Applicability to Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vasile</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,229.83,564.57,82.29,9.96">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.50,584.61,402.18,9.96;7,110.52,596.49,345.04,9.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,493.01,584.61,19.67,9.96;7,110.52,596.49,160.46,9.96">Synergist: Tools for Intelligence Analysis</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Munirathnam</forename><surname>Srikanth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abraham</forename><surname>Fowler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Altaf</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Jean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,279.68,596.49,77.16,9.96">NIMD Conference</title>
		<meeting><address><addrLine>Arlington, VA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.50,616.41,402.25,9.96;7,110.52,628.41,401.98,9.96;7,110.52,640.29,22.88,9.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,369.73,616.41,143.03,9.96;7,110.52,628.41,105.45,9.96">Language Models and Reranking for Machine Translation</title>
		<author>
			<persName coords=""><forename type="first">Marian</forename><surname>Olteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pasin</forename><surname>Suriyentrakorn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,241.47,628.41,266.69,9.96">NAACL 2006 Workshop On Statistical Machine Translation</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.50,660.21,402.27,9.96;7,110.52,672.21,401.85,9.96;7,110.52,684.21,78.38,9.96" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,393.95,660.21,118.83,9.96;7,110.52,672.21,156.50,9.96">Phramer -An Open Source Statistical Phrase-Based Translator</title>
		<author>
			<persName coords=""><forename type="first">Marian</forename><surname>Olteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ionut</forename><surname>Volosen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,295.16,672.21,217.21,9.96;7,110.52,684.21,47.80,9.96">NAACL 2006 Workshop On Statistical Machine Translation</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.50,704.13,402.15,9.96;7,110.52,716.01,217.46,9.96" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,309.30,704.13,196.72,9.96">Automatic Answer Validation using COGEX</title>
		<author>
			<persName coords=""><forename type="first">Marta</forename><surname>Tatu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brandon</forename><surname>Iles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,110.52,716.01,185.75,9.96">Cross-Language Evaluation Forum (CLEF)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
